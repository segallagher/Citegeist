[
    {
        "url": "https://arxiv.org/abs/1501.00601",
        "title": "Ultimate Intelligence Part I: Physical Completeness and Objectivity of Induction",
        "authors": [
            "Eray \u00d6zkural"
        ],
        "abstract": "We propose that Solomonoff induction is complete in the physical sense via several strong physical arguments. We also argue that Solomonoff induction is fully applicable to quantum mechanics. We show how to choose an objective reference machine for universal induction by defining a physical message complexity and physical message probability, and argue that this choice dissolves some well-known objections to universal induction. We also introduce many more variants of physical message complexity based on energy and action, and discuss the ramifications of our proposals.\n    ",
        "submission_date": "2015-01-03T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00653",
        "title": "Hostile Intent Identification by Movement Pattern Analysis: Using Artificial Neural Networks",
        "authors": [
            "Souham Biswas",
            "Manisha J. Nene"
        ],
        "abstract": "In the recent years, the problem of identifying suspicious behavior has gained importance and identifying this behavior using computational systems and autonomous algorithms is highly desirable in a tactical scenario. So far, the solutions have been primarily manual which elicit human observation of entities to discern the hostility of the situation. To cater to this problem statement, a number of fully automated and partially automated solutions exist. But, these solutions lack the capability of learning from experiences and work in conjunction with human supervision which is extremely prone to error. In this paper, a generalized methodology to predict the hostility of a given object based on its movement patterns is proposed which has the ability to learn and is based upon the mechanism of humans of learning from experiences. The methodology so proposed has been implemented in a computer simulation. The results show that the posited methodology has the potential to be applied in real world tactical scenarios.\n    ",
        "submission_date": "2015-01-04T00:00:00",
        "last_modified_date": "2015-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01178",
        "title": "Constraint-based sequence mining using constraint programming",
        "authors": [
            "Benjamin Negrevergne",
            "Tias Guns"
        ],
        "abstract": "The goal of constraint-based sequence mining is to find sequences of symbols that are included in a large number of input sequences and that satisfy some constraints specified by the user. Many constraints have been proposed in the literature, but a general framework is still missing. We investigate the use of constraint programming as general framework for this task. We first identify four categories of constraints that are applicable to sequence mining. We then propose two constraint programming formulations. The first formulation introduces a new global constraint called exists-embedding. This formulation is the most efficient but does not support one type of constraint. To support such constraints, we develop a second formulation that is more general but incurs more overhead. Both formulations can use the projected database technique used in specialised algorithms. Experiments demonstrate the flexibility towards constraint-based settings and compare the approach to existing methods.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01239",
        "title": "On the Relationship between Sum-Product Networks and Bayesian Networks",
        "authors": [
            "Han Zhao",
            "Mazen Melibari",
            "Pascal Poupart"
        ],
        "abstract": "In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of {\\em normal} SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01252",
        "title": "Optimisation using Natural Language Processing: Personalized Tour Recommendation for Museums",
        "authors": [
            "Mayeul Mathias",
            "Assema Moussa",
            "Fen Zhou",
            "Juan-Manuel Torres-Moreno",
            "Marie-Sylvie Poli",
            "Didier Josselin",
            "Marc El-B\u00e8ze",
            "Andr\u00e9a Carneiro Linhares",
            "Francoise Rigat"
        ],
        "abstract": "This paper proposes a new method to provide personalized tour recommendation for museum visits. It combines an optimization of preference criteria of visitors with an automatic extraction of artwork importance from museum information based on Natural Language Processing using textual energy. This project includes researchers from computer and social sciences. Some results are obtained with numerical experiments. They show that our model clearly improves the satisfaction of the visitor who follows the proposed tour. This work foreshadows some interesting outcomes and applications about on-demand personalized visit of museums in a very near future.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01432",
        "title": "Evidential-EM Algorithm Applied to Progressively Censored Observations",
        "authors": [
            "Kuang Zhou",
            "Arnaud Martin",
            "Quan Pan"
        ],
        "abstract": "Evidential-EM (E2M) algorithm is an effective approach for computing maximum likelihood estimations under finite mixture models, especially when there is uncertain information about data. In this paper we present an extension of the E2M method in a particular case of incom-plete data, where the loss of information is due to both mixture models and censored observations. The prior uncertain information is expressed by belief functions, while the pseudo-likelihood function is derived based on imprecise observations and prior knowledge. Then E2M method is evoked to maximize the generalized likelihood function to obtain the optimal estimation of parameters. Numerical examples show that the proposed method could effectively integrate the uncertain prior infor-mation with the current imprecise knowledge conveyed by the observed data.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01457",
        "title": "Comparison of Selection Methods in On-line Distributed Evolutionary Robotics",
        "authors": [
            "I\u00f1aki Fern\u00e1ndez P\u00e9rez",
            "Amine Boumaza",
            "Fran\u00e7ois Charpillet"
        ],
        "abstract": "In this paper, we study the impact of selection methods in the context of on-line on-board distributed evolutionary algorithms. We propose a variant of the mEDEA algorithm in which we add a selection operator, and we apply it in a taskdriven scenario. We evaluate four selection methods that induce different intensity of selection pressure in a multi-robot navigation with obstacle avoidance task and a collective foraging task. Experiments show that a small intensity of selection pressure is sufficient to rapidly obtain good performances on the tasks at hand. We introduce different measures to compare the selection methods, and show that the higher the selection pressure, the better the performances obtained, especially for the more challenging food foraging task.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01460",
        "title": "Median evidential c-means algorithm and its application to community detection",
        "authors": [
            "Kuang Zhou",
            "Arnaud Martin",
            "Quan Pan",
            "Zhun-Ga Liu"
        ],
        "abstract": "Median clustering is of great value for partitioning relational data. In this paper, a new prototype-based clustering method, called Median Evidential C-Means (MECM), which is an extension of median c-means and median fuzzy c-means on the theoretical framework of belief functions is proposed. The median variant relaxes the restriction of a metric space embedding for the objects but constrains the prototypes to be in the original data set. Due to these properties, MECM could be applied to graph clustering problems. A community detection scheme for social networks based on MECM is investigated and the obtained credal partitions of graphs, which are more refined than crisp and fuzzy ones, enable us to have a better understanding of the graph structures. An initial prototype-selection scheme based on evidential semi-centrality is presented to avoid local premature convergence and an evidential modularity function is defined to choose the optimal number of communities. Finally, experiments in synthetic and real data sets illustrate the performance of MECM and show its difference to other methods.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01501",
        "title": "Autonomous Fault Detection in Self-Healing Systems using Restricted Boltzmann Machines",
        "authors": [
            "Chris Schneider",
            "Adam Barker",
            "Simon Dobson"
        ],
        "abstract": "Autonomously detecting and recovering from faults is one approach for reducing the operational complexity and costs associated with managing computing environments. We present a novel methodology for autonomously generating investigation leads that help identify systems faults, and extends our previous work in this area by leveraging Restricted Boltzmann Machines (RBMs) and contrastive divergence learning to analyse changes in historical feature data. This allows us to heuristically identify the root cause of a fault, and demonstrate an improvement to the state of the art by showing feature data can be predicted heuristically beyond a single instance to include entire sequences of information.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02560",
        "title": "Belief Hierarchical Clustering",
        "authors": [
            "Wiem Maalel",
            "Kuang Zhou",
            "Arnaud Martin",
            "Zied Elouedi"
        ],
        "abstract": "In the data mining field many clustering methods have been proposed, yet standard versions do not take into account uncertain databases. This paper deals with a new approach to cluster uncertain data by using a hierarchical clustering defined within the belief function framework. The main objective of the belief hierarchical clustering is to allow an object to belong to one or several clusters. To each belonging, a degree of belief is associated, and clusters are combined based on the pignistic properties. Experiments with real uncertain data show that our proposed method can be considered as a propitious tool.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02732",
        "title": "Predicting Performance During Tutoring with Models of Recent Performance",
        "authors": [
            "April Galyardt",
            "Ilya Goldin"
        ],
        "abstract": "In educational technology and learning sciences, there are multiple uses for a predictive model of whether a student will perform a task correctly or not. For example, an intelligent tutoring system may use such a model to estimate whether or not a student has mastered a skill. We analyze the significance of data recency in making such predictions, i.e., asking whether relatively more recent observations of a student's performance matter more than relatively older observations. We develop a new Recent-Performance Factors Analysis model that takes data recency into account. The new model significantly improves predictive accuracy over both existing logistic-regression performance models and over novel baseline models in evaluations on real-world and synthetic datasets. As a secondary contribution, we demonstrate how the widely used cross-validation with 0-1 loss is inferior to AIC and to cross-validation with L1 prediction error loss as a measure of model performance.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03093",
        "title": "MultiGain: A controller synthesis tool for MDPs with multiple mean-payoff objectives",
        "authors": [
            "Tom\u00e1\u0161 Br\u00e1zdil",
            "Krishnendu Chatterjee",
            "Vojt\u011bch Forejt",
            "Anton\u00edn Ku\u010dera"
        ],
        "abstract": "We present MultiGain, a tool to synthesize strategies for Markov decision processes (MDPs) with multiple mean-payoff objectives. Our models are described in PRISM, and our tool uses the existing interface and simulator of PRISM. Our tool extends PRISM by adding novel algorithms for multiple mean-payoff objectives, and also provides features such as (i)~generating strategies and exploring them for simulation, and checking them with respect to other properties; and (ii)~generating an approximate Pareto curve for two mean-payoff objectives. In addition, we present a new practical algorithm for the analysis of MDPs with multiple mean-payoff objectives under memoryless strategies.\n    ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03302",
        "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for \"question answering about images\" as a particular appealing instance of such a holistic task. In particular, we point out that it is a version of a Turing Test that is likely to be more robust to over-interpretations and contrast it with tasks like grounding and generation of descriptions. Finally, we discuss tools to measure progress in this field.\n    ",
        "submission_date": "2015-01-14T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03784",
        "title": "Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing",
        "authors": [
            "Denis Kleyko",
            "Evgeny Osipov",
            "Alexander Senior",
            "Asad I. Khan",
            "Y. Ahmet \u015eekercio\u011flu"
        ],
        "abstract": "This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.\n    ",
        "submission_date": "2015-01-15T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03959",
        "title": "Value Iteration with Options and State Aggregation",
        "authors": [
            "Kamil Ciosek",
            "David Silver"
        ],
        "abstract": "This paper presents a way of solving Markov Decision Processes that combines state abstraction and temporal abstraction. Specifically, we combine state aggregation with the options framework and demonstrate that they work well together and indeed it is only after one combines the two that the full benefit of each is realized. We introduce a hierarchical value iteration algorithm where we first coarsely solve subgoals and then use these approximate solutions to exactly solve the MDP. This algorithm solved several problems faster than vanilla value iteration.\n    ",
        "submission_date": "2015-01-16T00:00:00",
        "last_modified_date": "2015-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04177",
        "title": "Second International Nurse Rostering Competition (INRC-II) --- Problem Description and Rules ---",
        "authors": [
            "Sara Ceschia",
            "Nguyen Thi Thanh Dang",
            "Patrick De Causmaecker",
            "Stefaan Haspeslagh",
            "Andrea Schaerf"
        ],
        "abstract": "In this paper, we provide all information to participate to the Second International Nurse Rostering Competition (INRC-II). First, we describe the problem formulation, which, differently from INRC-I, is a multi-stage procedure. Second, we illustrate all the necessary infrastructure do be used together with the participant's solver, including the testbed, the file formats, and the validation/simulation tools. Finally, we state the rules of the competition. All update-to-date information about the competition is available at ",
        "submission_date": "2015-01-17T00:00:00",
        "last_modified_date": "2015-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04242",
        "title": "The Information-theoretic and Algorithmic Approach to Human, Animal and Artificial Cognition",
        "authors": [
            "Nicolas Gauvrit",
            "Hector Zenil",
            "Jesper Tegn\u00e9r"
        ],
        "abstract": "We survey concepts at the frontier of research connecting artificial, animal and human cognition to computation and information processing---from the Turing test to Searle's Chinese Room argument, from Integrated Information Theory to computational and algorithmic complexity. We start by arguing that passing the Turing test is a trivial computational problem and that its pragmatic difficulty sheds light on the computational nature of the human mind more than it does on the challenge of artificial intelligence. We then review our proposed algorithmic information-theoretic measures for quantifying and characterizing cognition in various forms. These are capable of accounting for known biases in human behavior, thus vindicating a computational algorithmic view of cognition as first suggested by Turing, but this time rooted in the concept of algorithmic probability, which in turn is based on computational universality while being independent of computational model, and which has the virtue of being predictive and testable as a model theory of cognitive behavior.\n    ",
        "submission_date": "2015-01-17T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04370",
        "title": "Structure Learning in Bayesian Networks of Moderate Size by Efficient Sampling",
        "authors": [
            "Ru He",
            "Jin Tian",
            "Huaiqing Wu"
        ],
        "abstract": "We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state-of-the-art methods.\n    ",
        "submission_date": "2015-01-19T00:00:00",
        "last_modified_date": "2015-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04684",
        "title": "Slice Sampling for Probabilistic Programming",
        "authors": [
            "Razvan Ranca",
            "Zoubin Ghahramani"
        ],
        "abstract": "We introduce the first, general purpose, slice sampling inference engine for probabilistic programs. This engine is released as part of StocPy, a new Turing-Complete probabilistic programming language, available as a Python library. We present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables. We show that StocPy compares favourably to other PPLs in terms of flexibility and usability, and that slice sampling can outperform previously introduced inference methods. Our experiments include a logistic regression, HMM, and Bayesian Neural Net.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04786",
        "title": "Consid{\u00e9}rant la d{\u00e9}pendance dans la th{\u00e9}orie des fonctions de croyance",
        "authors": [
            "Mouna Chebbah",
            "Mouloud Kharoune",
            "Arnaud Martin",
            "Boutheina Ben Yaghlane"
        ],
        "abstract": "In this paper, we propose to learn sources independence in order to choose the appropriate type of combination rules when aggregating their beliefs. Some combination rules are used with the assumption of their sources independence whereas others combine beliefs of dependent sources. Therefore, the choice of the combination rule depends on the independence of sources involved in the combination. \nIn this paper, we propose also a measure of independence, positive and negative dependence to integrate in mass functions before the combinaision with the independence assumption.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04795",
        "title": "Belief Approach for Social Networks",
        "authors": [
            "Salma Ben Dhaou",
            "Mouloud Kharoune",
            "Arnaud Martin",
            "Boutheina Ben Yaghlane"
        ],
        "abstract": "Nowadays, social networks became essential in information exchange between individuals. Indeed, as users of these networks, we can send messages to other people according to the links connecting us. Moreover, given the large volume of exchanged messages, detecting the true nature of the received message becomes a challenge. For this purpose, it is interesting to consider this new tendency with reasoning under uncertainty by using the theory of belief functions. In this paper, we tried to model a social network as being a network of fusion of information and determine the true nature of the received message in a well-defined node by proposing a new model: the belief social network.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04796",
        "title": "What do we learn about development from baby robots?",
        "authors": [
            "Pierre-Yves Oudeyer"
        ],
        "abstract": "Understanding infant development is one of the greatest scientific challenges of contemporary science. A large source of difficulty comes from the fact that the development of skills in infants results from the interactions of multiple mechanisms at multiple spatio-temporal scales. The concepts of \"innate\" or \"acquired\" are not any more adequate tools for explanations, which call for a shift from reductionist to systemic accounts. To address this challenge, building and experimenting with robots modeling the growing infant brain and body is crucial. Systemic explanations of pattern formation in sensorimotor, cognitive and social development, viewed as a complex dynamical system, require the use of formal models based on mathematics, algorithms and robots. Formulating hypothesis about development using such models, and exploring them through experiments, allows us to consider in detail the interaction between many mechanisms and parameters. This complements traditional experimental methods in psychology and neuroscience where only a few variables can be studied at the same time. Furthermore, the use of robots is of particular importance. The laws of physics generate everywhere around us spontaneous patterns in the inorganic world. They also strongly impact the living, and in particular constrain and guide infant development through the properties of its (changing) body in interaction with the physical environment. Being able to consider the body as an experimental variable, something that can be systematically changed in order to study the impact on skill formation, has been a dream to many developmental scientists. This is today becoming possible with developmental robotics.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05031",
        "title": "Maximin Safety: When Failing to Lose is Preferable to Trying to Win",
        "authors": [
            "Brad Gulko",
            "Samantha Leung"
        ],
        "abstract": "We present a new decision rule, \\emph{maximin safety}, that seeks to maintain a large margin from the worst outcome, in much the same way minimax regret seeks to minimize distance from the best. We argue that maximin safety is valuable both descriptively and normatively. Descriptively, maximin safety explains the well-known \\emph{decoy effect}, in which the introduction of a dominated option changes preferences among the other options. Normatively, we provide an axiomatization that characterizes preferences induced by maximin safety, and show that maximin safety shares much of the same behavioral basis with minimax regret.\n    ",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2015-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05272",
        "title": "Trolls Identification within an Uncertain Framework",
        "authors": [
            "Imen Ouled Dlala",
            "Dorra Attiaoui",
            "Arnaud Martin",
            "Boutheina Ben Yaghlane"
        ],
        "abstract": "The web plays an important role in people's social lives since the emergence of Web 2.0. It facilitates the interaction between users, gives them the possibility to freely interact, share and collaborate through social networks, online communities forums, blogs, wikis and other online collaborative media. However, an other side of the web is negatively taken such as posting inflammatory messages. Thus, when dealing with the online communities forums, the managers seek to always enhance the performance of such platforms. In fact, to keep the serenity and prohibit the disturbance of the normal atmosphere, managers always try to novice users against these malicious persons by posting such message (DO NOT FEED TROLLS). But, this kind of warning is not enough to reduce this phenomenon. In this context we propose a new approach for detecting malicious people also called 'Trolls' in order to allow community managers to take their ability to post online. To be more realistic, our proposal is defined within an uncertain framework. Based on the assumption consisting on the trolls' integration in the successful discussion threads, we try to detect the presence of such malicious users. Indeed, this method is based on a conflict measure of the belief function theory applied between the different messages of the thread. In order to show the feasibility and the result of our approach, we test it in different simulated data.\n    ",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2015-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05530",
        "title": "Belief Hidden Markov Model for speech recognition",
        "authors": [
            "Siwar Jendoubi",
            "Boutheina Ben Yaghlane",
            "Arnaud Martin"
        ],
        "abstract": "Speech Recognition searches to predict the spoken words automatically. These systems are known to be very expensive because of using several pre-recorded hours of speech. Hence, building a model that minimizes the cost of the recognizer will be very interesting. In this paper, we present a new approach for recognizing speech based on belief HMMs instead of proba-bilistic HMMs. Experiments shows that our belief recognizer is insensitive to the lack of the data and it can be trained using only one exemplary of each acoustic unit and it gives a good recognition rates. Consequently, using the belief HMM recognizer can greatly minimize the cost of these systems.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05612",
        "title": "Features modeling with an $\u03b1$-stable distribution: Application to pattern recognition based on continuous belief functions",
        "authors": [
            "Anthony Fiche",
            "Jean-Christophe Cexus",
            "Arnaud Martin",
            "Ali Khenchaf"
        ],
        "abstract": "The aim of this paper is to show the interest in fitting features with an $\\alpha$-stable distribution to classify imperfect data. The supervised pattern recognition is thus based on the theory of continuous belief functions, which is a way to consider imprecision and uncertainty of data. The distributions of features are supposed to be unimodal and estimated by a single Gaussian and $\\alpha$-stable model. Experimental results are first obtained from synthetic data by combining two features of one dimension and by considering a vector of two features. Mass functions are calculated from plausibility functions by using the generalized Bayes theorem. The same study is applied to the automatic classification of three types of sea floor (rock, silt and sand) with features acquired by a mono-beam echo-sounder. We evaluate the quality of the $\\alpha$-stable model and the Gaussian model by analyzing qualitative results, using a Kolmogorov-Smirnov test (K-S test), and quantitative results with classification rates. The performances of the belief classifier are compared with a Bayesian approach.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05613",
        "title": "Second-Order Belief Hidden Markov Models",
        "authors": [
            "Jungyeul Park",
            "Mouna Chebbah",
            "Siwar Jendoubi",
            "Arnaud Martin"
        ],
        "abstract": "Hidden Markov Models (HMMs) are learning methods for pattern recognition. The probabilistic HMMs have been one of the most used techniques based on the Bayesian model. First-order probabilistic HMMs were adapted to the theory of belief functions such that Bayesian probabilities were replaced with mass functions. In this paper, we present a second-order Hidden Markov Model using belief functions. Previous works in belief HMMs have been focused on the first-order HMMs. We extend them to the second-order model.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05614",
        "title": "Int{\u00e9}gration d'une mesure d'ind{\u00e9}pendance pour la fusion d'informations",
        "authors": [
            "Mouloud Kharoune",
            "Arnaud Martin"
        ],
        "abstract": "Many information sources are considered into data fusion in order to improve the decision in terms of uncertainty and imprecision. For each technique used for data fusion, the asumption on independance is usually made. We propose in this article an approach to take into acount an independance measure befor to make the combination of information in the context of the theory of belief functions.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05677",
        "title": "Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs",
        "authors": [
            "David Tolpin",
            "Jan Willem van de Meent",
            "Brooks Paige",
            "Frank Wood"
        ],
        "abstract": "We introduce an adaptive output-sensitive Metropolis-Hastings algorithm for probabilistic models expressed as programs, Adaptive Lightweight Metropolis-Hastings (AdLMH). The algorithm extends Lightweight Metropolis-Hastings (LMH) by adjusting the probabilities of proposing random variables for modification to improve convergence of the program output. We show that AdLMH converges to the correct equilibrium distribution and compare convergence of AdLMH to that of LMH on several test problems to highlight different aspects of the adaptation scheme. We observe consistent improvement in convergence on the test problems.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05724",
        "title": "Uncertainty in Ontology Matching: A Decision Rule-Based Approach",
        "authors": [
            "Amira Essaid",
            "Arnaud Martin",
            "Gr\u00e9gory Smits",
            "Boutheina Ben Yaghlane"
        ],
        "abstract": "Considering the high heterogeneity of the ontologies pub-lished on the web, ontology matching is a crucial issue whose aim is to establish links between an entity of a source ontology and one or several entities from a target ontology. Perfectible similarity measures, consid-ered as sources of information, are combined to establish these links. The theory of belief functions is a powerful mathematical tool for combining such uncertain information. In this paper, we introduce a decision pro-cess based on a distance measure to identify the best possible matching entities for a given source entity.\n    ",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05882",
        "title": "Efficient local search limitation strategy for single machine total weighted tardiness scheduling with sequence-dependent setup times",
        "authors": [
            "Anand Subramanian",
            "Katyanne Farias"
        ],
        "abstract": "This paper concerns the single machine total weighted tardiness scheduling with sequence-dependent setup times, usually referred as $1|s_{ij}|\\sum w_jT_j$. In this $\\mathcal{NP}$-hard problem, each job has an associated processing time, due date and a weight. For each pair of jobs $i$ and $j$, there may be a setup time before starting to process $j$ in case this job is scheduled immediately after $i$. The objective is to determine a schedule that minimizes the total weighted tardiness, where the tardiness of a job is equal to its completion time minus its due date, in case the job is completely processed only after its due date, and is equal to zero otherwise. Due to its complexity, this problem is most commonly solved by heuristics. The aim of this work is to develop a simple yet effective limitation strategy that speeds up the local search procedure without a significant loss in the solution quality. Such strategy consists of a filtering mechanism that prevents unpromising moves to be evaluated. The proposed strategy has been embedded in a local search based metaheuristic from the literature and tested in classical benchmark instances. Computational experiments revealed that the limitation strategy enabled the metaheuristic to be extremely competitive when compared to other algorithms from the literature, since it allowed the use of a large number of neighborhood structures without a significant increase in the CPU time and, consequently, high quality solutions could be achieved in a matter of seconds. In addition, we analyzed the effectiveness of the proposed strategy in two other well-known metaheuristics. Further experiments were also carried out on benchmark instances of problem $1|s_{ij}|\\sum T_j$.\n    ",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05917",
        "title": "On Generalized Rectangular Fuzzy Model for Assessment",
        "authors": [
            "Igor Yakov Subbotin"
        ],
        "abstract": "The article is dedicated to the analysis of the existing models for assessment based of the fuzzy logic centroid technique. A new Generalized Rectangular Model were developed. Some generalizations of the existing models are offered.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05940",
        "title": "A New Efficient Method for Calculating Similarity Between Web Services",
        "authors": [
            "T. Rachad",
            "J. Boutahar",
            "S. El ghazi"
        ],
        "abstract": "Web services allow communication between heterogeneous systems in a distributed environment. Their enormous success and their increased use led to the fact that thousands of Web services are present on the Internet. This significant number of Web services which not cease to increase has led to problems of the difficulty in locating and classifying web services, these problems are encountered mainly during the operations of web services discovery and substitution. Traditional ways of search based on keywords are not successful in this context, their results do not support the structure of Web services and they consider in their search only the identifiers of the web service description language (WSDL) interface elements. The methods based on semantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web service with a semantic description allow raising partially this problem, but their complexity and difficulty delays their adoption in real cases. Measuring the similarity between the web services interfaces is the most suitable solution for this kind of problems, it will classify available web services so as to know those that best match the searched profile and those that do not match. Thus, the main goal of this work is to study the degree of similarity between any two web services by offering a new method that is more effective than existing works.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06595",
        "title": "User Clustering in Online Advertising via Topic Models",
        "authors": [
            "Sahin Cem Geyik",
            "Ali Dasdan",
            "Kuang-Chih Lee"
        ],
        "abstract": "In the domain of online advertising, our aim is to serve the best ad to a user who visits a certain webpage, to maximize the chance of a desired action to be performed by this user after seeing the ad. While it is possible to generate a different prediction model for each user to tell if he/she will act on a given ad, the prediction result typically will be quite unreliable with huge variance, since the desired actions are extremely sparse, and the set of users is huge (hundreds of millions) and extremely volatile, i.e., a lot of new users are introduced everyday, or are no longer valid. In this paper we aim to improve the accuracy in finding users who will perform the desired action, by assigning each user to a cluster, where the number of clusters is much smaller than the number of users (in the order of hundreds). Each user will fall into the same cluster with another user if their event history are similar. For this purpose, we modify the probabilistic latent semantic analysis (pLSA) model by assuming the independence of the user and the cluster id, given the history of events. This assumption helps us to identify a cluster of a new user without re-clustering all the users. We present the details of the algorithm we employed as well as the distributed implementation on Hadoop, and some initial results on the clusters that were generated by the algorithm.\n    ",
        "submission_date": "2015-01-26T00:00:00",
        "last_modified_date": "2015-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06705",
        "title": "Inclusion within Continuous Belief Functions",
        "authors": [
            "Dorra Attiaoui",
            "Pierre-Emmanuel Dor\u00e9",
            "Arnaud Martin",
            "Boutheina Ben Yaghlane"
        ],
        "abstract": "Defining and modeling the relation of inclusion between continuous belief function may be considered as an important operation in order to study their behaviors. Within this paper we will propose and present two forms of inclusion: The strict and the partial one. In order to develop this relation, we will study the case of consonant belief function. To do so, we will simulate normal distributions allowing us to model and analyze these relations. Based on that, we will determine the parameters influencing and characterizing the two forms of inclusion.\n    ",
        "submission_date": "2015-01-27T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07008",
        "title": "A Distance-Based Decision in the Credal Level",
        "authors": [
            "Amira Essaid",
            "Arnaud Martin",
            "Gr\u00e9gory Smits",
            "Boutheina Ben Yaghlane"
        ],
        "abstract": "Belief function theory provides a flexible way to combine information provided by different sources. This combination is usually followed by a decision making which can be handled by a range of decision rules. Some rules help to choose the most likely hypothesis. Others allow that a decision is made on a set of hypotheses. In [6], we proposed a decision rule based on a distance measure. First, in this paper, we aim to demonstrate that our proposed decision rule is a particular case of the rule proposed in [4]. Second, we give experiments showing that our rule is able to decide on a set of hypotheses. Some experiments are handled on a set of mass functions generated randomly, others on real databases.\n    ",
        "submission_date": "2015-01-28T00:00:00",
        "last_modified_date": "2015-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07250",
        "title": "FMAP: Distributed Cooperative Multi-Agent Planning",
        "authors": [
            "Alejandro Torre\u00f1o",
            "Eva Onaindia",
            "\u00d3scar Sapena"
        ],
        "abstract": "This paper proposes FMAP (Forward Multi-Agent Planning), a fully-distributed multi-agent planning method that integrates planning and coordination. Although FMAP is specifically aimed at solving problems that require cooperation among agents, the flexibility of the domain-independent planning model allows FMAP to tackle multi-agent planning tasks of any type. In FMAP, agents jointly explore the plan space by building up refinement plans through a complete and flexible forward-chaining partial-order planner. The search is guided by $h_{DTG}$, a novel heuristic function that is based on the concepts of Domain Transition Graph and frontier state and is optimized to evaluate plans in distributed environments. Agents in FMAP apply an advanced privacy model that allows them to adequately keep private information while communicating only the data of the refinement plans that is relevant to each of the participating agents. Experimental results show that FMAP is a general-purpose approach that efficiently solves tightly-coupled domains that have specialized agents and cooperative goals as well as loosely-coupled problems. Specifically, the empirical evaluation shows that FMAP outperforms current MAP systems at solving complex planning tasks that are adapted from the International Planning Competition benchmarks.\n    ",
        "submission_date": "2015-01-28T00:00:00",
        "last_modified_date": "2015-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07256",
        "title": "An approach to multi-agent planning with incomplete information",
        "authors": [
            "Alejandro Torre\u00f1o",
            "Eva Onaindia",
            "\u00d3scar Sapena"
        ],
        "abstract": "Multi-agent planning (MAP) approaches have been typically conceived for independent or loosely-coupled problems to enhance the benefits of distributed planning between autonomous agents as solving this type of problems require less coordination between the agents' sub-plans. However, when it comes to tightly-coupled agents' tasks, MAP has been relegated in favour of centralized approaches and little work has been done in this direction. In this paper, we present a general-purpose MAP capable to efficiently handle planning problems with any level of coupling between agents. We propose a cooperative refinement planning approach, built upon the partial-order planning paradigm, that allows agents to work with incomplete information and to have incomplete views of the world, i.e. being ignorant of other agents' information, as well as maintaining their own private information. We show various experiments to compare the performance of our system with a distributed CSP-based MAP approach over a suite of problems.\n    ",
        "submission_date": "2015-01-28T00:00:00",
        "last_modified_date": "2015-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07423",
        "title": "A Flexible Coupling Approach to Multi-Agent Planning under Incomplete Information",
        "authors": [
            "Alejandro Torre\u00f1o",
            "Eva Onaindia",
            "\u00d3scar Sapena"
        ],
        "abstract": "Multi-agent planning (MAP) approaches are typically oriented at solving loosely-coupled problems, being ineffective to deal with more complex, strongly-related problems. In most cases, agents work under complete information, building complete knowledge bases. The present article introduces a general-purpose MAP framework designed to tackle problems of any coupling levels under incomplete information. Agents in our MAP model are partially unaware of the information managed by the rest of agents and share only the critical information that affects other agents, thus maintaining a distributed vision of the task.\n    ",
        "submission_date": "2015-01-29T00:00:00",
        "last_modified_date": "2015-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00152",
        "title": "Minimizing Regret in Dynamic Decision Problems",
        "authors": [
            "Joseph Y. Halpern",
            "Samantha Leung"
        ],
        "abstract": "The menu-dependent nature of regret-minimization creates subtleties when it is applied to dynamic decision problems. Firstly, it is not clear whether \\emph{forgone opportunities} should be included in the \\emph{menu}, with respect to which regrets are computed, at different points of the decision problem. If forgone opportunities are included, however, we can characterize when a form of dynamic consistency is guaranteed. Secondly, more subtleties arise when sophistication is used to deal with dynamic inconsistency. In the full version of this paper, we examine, axiomatically and by common examples, the implications of different menu definitions for sophisticated, regret-minimizing agents.\n    ",
        "submission_date": "2015-01-31T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01075",
        "title": "Classificatory Sorites, Probabilistic Supervenience, and Rule-Making",
        "authors": [
            "Damir D. Dzhafarov",
            "Ehtibar N. Dzhafarov"
        ],
        "abstract": "We view sorites in terms of stimuli acting upon a system and evoking this system's responses. Supervenience of responses on stimuli implies that they either lack tolerance (i.e., they change in every vicinity of some of the stimuli), or stimuli are not always connectable by finite chains of stimuli in which successive members are `very similar'. If supervenience does not hold, the properties of tolerance and connectedness cannot be formulated and therefore soritical sequences cannot be constructed. We hypothesize that supervenience in empirical systems (such as people answering questions) is fundamentally probabilistic. The supervenience of probabilities of responses on stimuli is stable, in the sense that `higher-order' probability distributions can always be reduced to `ordinary' ones. In making rules about which stimuli ought to correspond to which responses, the main characterization of choices in soritical situations is their arbitrariness. We argue that arbitrariness poses no problems for classical logic.\n    ",
        "submission_date": "2015-02-04T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01497",
        "title": "Using temporal abduction for biosignal interpretation: A case study on QRS detection",
        "authors": [
            "Tom\u00e1s Teijeiro",
            "Paulo F\u00e9lix",
            "Jes\u00fas Presedo"
        ],
        "abstract": "In this work, we propose an abductive framework for biosignal interpretation, based on the concept of Temporal Abstraction Patterns. A temporal abstraction pattern defines an abstraction relation between an observation hypothesis and a set of observations constituting its evidence support. New observations are generated abductively from any subset of the evidence of a pattern, building an abstraction hierarchy of observations in which higher levels contain those observations with greater interpretative value of the physiological processes underlying a given signal. Non-monotonic reasoning techniques have been applied to this model in order to find the best interpretation of a set of initial observations, permitting even to correct these observations by removing, adding or modifying them in order to make them consistent with the available domain knowledge. Some preliminary experiments have been conducted to apply this framework to a well known and bounded problem: the QRS detection on ECG signals. The objective is not to provide a new better QRS detector, but to test the validity of an abductive paradigm. These experiments show that a knowledge base comprising just a few very simple rhythm abstraction patterns can enhance the results of a state of the art algorithm by significantly improving its detection F1-score, besides proving the ability of the abductive framework to correct both sensitivity and specificity failures.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01972",
        "title": "A Multistage Stochastic Programming Approach to the Dynamic and Stochastic VRPTW - Extended version",
        "authors": [
            "Michael Saint-Guillain",
            "Yves Deville",
            "Christine Solnon"
        ],
        "abstract": "We consider a dynamic vehicle routing problem with time windows and stochastic customers (DS-VRPTW), such that customers may request for services as vehicles have already started their tours. To solve this problem, the goal is to provide a decision rule for choosing, at each time step, the next action to perform in light of known requests and probabilistic knowledge on requests likelihood. We introduce a new decision rule, called Global Stochastic Assessment (GSA) rule for the DS-VRPTW, and we compare it with existing decision rules, such as MSA. In particular, we show that GSA fully integrates nonanticipativity constraints so that it leads to better decisions in our stochastic context. We describe a new heuristic approach for efficiently approximating our GSA rule. We introduce a new waiting strategy. Experiments on dynamic and stochastic benchmarks, which include instances of different degrees of dynamism, show that not only our approach is competitive with state-of-the-art methods, but also enables to compute meaningful offline solutions to fully dynamic problems where absolutely no a priori customer request is provided.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02029",
        "title": "A Quantum Production Model",
        "authors": [
            "Lu\u00eds Tarrataca",
            "Andreas Wichert"
        ],
        "abstract": "The production system is a theoretical model of computation relevant to the artificial intelligence field allowing for problem solving procedures such as hierarchical tree search. In this work we explore some of the connections between artificial intelligence and quantum computation by presenting a model for a quantum production system. Our approach focuses on initially developing a model for a reversible production system which is a simple mapping of Bennett's reversible Turing machine. We then expand on this result in order to accommodate for the requirements of quantum computation. We present the details of how our proposition can be used alongside Grover's algorithm in order to yield a speedup comparatively to its classical counterpart. We discuss the requirements associated with such a speedup and how it compares against a similar quantum hierarchical search approach.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02193",
        "title": "The Silver Lining Around Fearful Living",
        "authors": [
            "Liane Gabora"
        ],
        "abstract": "This paper discusses in layperson's terms human and computational studies of the impact of threat and fear on exploration and creativity. A first study showed that both killifish from a lake with predators and from a lake without predators explore a new environment to the same degree and plotting number of new spaces covered over time generates a hump-shaped curve. However, for the fish from the lake with predators the curve is shifted to the right; they take longer. This pattern was replicated by a computer model of exploratory behavior varying only one parameter, the fear parameter. A second study showed that stories inspired by threatening photographs were rated as more creative than stories inspired by non-threatening photographs. Various explanations for the findings are discussed.\n    ",
        "submission_date": "2015-02-07T00:00:00",
        "last_modified_date": "2015-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02298",
        "title": "Belief Revision, Minimal Change and Relaxation: A General Framework based on Satisfaction Systems, and Applications to Description Logics",
        "authors": [
            "Marc Aiguier",
            "Jamal Atif",
            "Isabelle Bloch",
            "C\u00e9line Hudelot"
        ],
        "abstract": "Belief revision of knowledge bases represented by a set of sentences in a given logic has been extensively studied but for specific logics, mainly propositional, and also recently Horn and description logics. Here, we propose to generalize this operation from a model-theoretic point of view, by defining revision in an abstract model theory known under the name of satisfaction systems. In this framework, we generalize to any satisfaction systems the characterization of the well known AGM postulates given by Katsuno and Mendelzon for propositional logic in terms of minimal change among interpretations. Moreover, we study how to define revision, satisfying the AGM postulates, from relaxation notions that have been first introduced in description logics to define dissimilarity measures between concepts, and the consequence of which is to relax the set of models of the old belief until it becomes consistent with the new pieces of knowledge. We show how the proposed general framework can be instantiated in different logics such as propositional, first-order, description and Horn logics. In particular for description logics, we introduce several concrete relaxation operators tailored for the description logic $\\ALC{}$ and its fragments $\\EL{}$ and $\\ELext{}$, discuss their properties and provide some illustrative examples.\n    ",
        "submission_date": "2015-02-08T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02414",
        "title": "Tractability and Decompositions of Global Cost Functions",
        "authors": [
            "David Allouche",
            "Christian Bessiere",
            "Patrice Boizumault",
            "Simon de Givry",
            "Patricia Gutierrez",
            "Jimmy H.M. Lee",
            "Kam Lun Leung",
            "Samir Loudni",
            "Jean-Philippe M\u00e9tivier",
            "Thomas Schiex",
            "Yi Wu"
        ],
        "abstract": "Enforcing local consistencies in cost function networks is performed by applying so-called Equivalent Preserving Transformations (EPTs) to the cost functions. As EPTs transform the cost functions, they may break the property that was making local consistency enforcement tractable on a global cost function. A global cost function is called tractable projection-safe when applying an EPT to it is tractable and does not break the tractability property. In this paper, we prove that depending on the size r of the smallest scopes used for performing EPTs, the tractability of global cost functions can be preserved (r = 0) or destroyed (r > 1). When r = 1, the answer is indefinite. We show that on a large family of cost functions, EPTs can be computed via dynamic programming-based algorithms, leading to tractable projection-safety. We also show that when a global cost function can be decomposed into a Berge acyclic network of bounded arity cost functions, soft local consistencies such as soft Directed or Virtual Arc Consistency can directly emulate dynamic programming. These different approaches to decomposable cost functions are then embedded in a solver for extensive experiments that confirm the feasibility and efficiency of our proposal.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02417",
        "title": "Semantics-based services for a low carbon society: An application on emissions trading system data and scenarios management",
        "authors": [
            "Cecilia Camporeale",
            "Antonio De Nicola",
            "Maria Luisa Villani"
        ],
        "abstract": "A low carbon society aims at fighting global warming by stimulating synergic efforts from governments, industry and scientific communities. Decision support systems should be adopted to provide policy makers with possible scenarios, options for prompt countermeasures in case of side effects on environment, economy and society due to low carbon society policies, and also options for information management. A necessary precondition to fulfill this agenda is to face the complexity of this multi-disciplinary domain and to reach a common understanding on it as a formal specification. Ontologies are widely accepted means to share knowledge. Together with semantic rules, they enable advanced semantic services to manage knowledge in a smarter way. Here we address the European Emissions Trading System (EU-ETS) and we present a knowledge base consisting of the EREON ontology and a catalogue of rules. Then we describe two innovative semantic services to manage ETS data and information on ETS scenarios.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02454",
        "title": "A fast PC algorithm for high dimensional causal discovery with multi-core PCs",
        "authors": [
            "Thuc Duy Le",
            "Tao Hoang",
            "Jiuyong Li",
            "Lin Liu",
            "Huawen Liu"
        ],
        "abstract": "Discovering causal relationships from observational data is a crucial problem and it has applications in many research areas. The PC algorithm is the state-of-the-art constraint based method for causal discovery. However, runtime of the PC algorithm, in the worst-case, is exponential to the number of nodes (variables), and thus it is inefficient when being applied to high dimensional data, e.g. gene expression datasets. On another note, the advancement of computer hardware in the last decade has resulted in the widespread availability of multi-core personal computers. There is a significant motivation for designing a parallelised PC algorithm that is suitable for personal computers and does not require end users' parallel computing knowledge beyond their competency in using the PC algorithm. In this paper, we develop parallel-PC, a fast and memory efficient PC algorithm using the parallel computing technique. We apply our method to a range of synthetic and real-world high dimensional datasets. Experimental results on a dataset from the DREAM 5 challenge show that the original PC algorithm could not produce any results after running more than 24 hours; meanwhile, our parallel-PC algorithm managed to finish within around 12 hours with a 4-core CPU computer, and less than 6 hours with a 8-core CPU computer. Furthermore, we integrate parallel-PC into a causal inference method for inferring miRNA-mRNA regulatory relationships. The experimental results show that parallel-PC helps improve both the efficiency and accuracy of the causal inference algorithm.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02467",
        "title": "Structural Decompositions for Problems with Global Constraints",
        "authors": [
            "Evgenij Thorstensen"
        ],
        "abstract": "A wide range of problems can be modelled as constraint satisfaction problems (CSPs), that is, a set of constraints that must be satisfied simultaneously. Constraints can either be represented extensionally, by explicitly listing allowed combinations of values, or implicitly, by special-purpose algorithms provided by a solver.\n",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02535",
        "title": "On First-Order Model-Based Reasoning",
        "authors": [
            "Maria Paola Bonacina",
            "Ulrich Furbach",
            "Viorica Sofronie-Stokkermans"
        ],
        "abstract": "Reasoning semantically in first-order logic is notoriously a challenge. This paper surveys a selection of semantically-guided or model-based methods that aim at meeting aspects of this challenge. For first-order logic we touch upon resolution-based methods, tableaux-based methods, DPLL-inspired methods, and we give a preview of a new method called SGGS, for Semantically-Guided Goal-Sensitive reasoning. For first-order theories we highlight hierarchical and locality-based methods, concluding with the recent Model-Constructing satisfiability calculus.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2019-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02799",
        "title": "On Forgetting in Tractable Propositional Fragments",
        "authors": [
            "Yisong Wang"
        ],
        "abstract": "Distilling from a knowledge base only the part that is relevant to a subset of alphabet, which is recognized as forgetting, has attracted extensive interests in AI community. In standard propositional logic, a general algorithm of forgetting and its computation-oriented investigation in various fragments whose satisfiability are tractable are still lacking. The paper aims at filling the gap. After exploring some basic properties of forgetting in propositional logic, we present a resolution-based algorithm of forgetting for CNF fragment, and some complexity results about forgetting in Horn, renamable Horn, q-Horn, Krom, DNF and CNF fragments of propositional logic.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02840",
        "title": "An Integrated Semantic Web Service Discovery and Composition Framework",
        "authors": [
            "Pablo Rodriguez-Mier",
            "Carlos Pedrinaci",
            "Manuel Lama",
            "Manuel Mucientes"
        ],
        "abstract": "In this paper we present a theoretical analysis of graph-based service composition in terms of its dependency with service discovery. Driven by this analysis we define a composition framework by means of integration with fine-grained I/O service discovery that enables the generation of a graph-based composition which contains the set of services that are semantically relevant for an input-output request. The proposed framework also includes an optimal composition search algorithm to extract the best composition from the graph minimising the length and the number of services, and different graph optimisations to improve the scalability of the system. A practical implementation used for the empirical analysis is also provided. This analysis proves the scalability and flexibility of our proposal and provides insights on how integrated composition systems can be designed in order to achieve good performance in real scenarios for the Web.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03248",
        "title": "Off-Policy Reward Shaping with Ensembles",
        "authors": [
            "Anna Harutyunyan",
            "Tim Brys",
            "Peter Vrancx",
            "Ann Nowe"
        ],
        "abstract": "Potential-based reward shaping (PBRS) is an effective and popular technique to speed up reinforcement learning by leveraging domain knowledge. While PBRS is proven to always preserve optimal policies, its effect on learning speed is determined by the quality of its potential function, which, in turn, depends on both the underlying heuristic and the scale. Knowing which heuristic will prove effective requires testing the options beforehand, and determining the appropriate scale requires tuning, both of which introduce additional sample complexity. We formulate a PBRS framework that reduces learning speed, but does not incur extra sample complexity. For this, we propose to simultaneously learn an ensemble of policies, shaped w.r.t. many heuristics and on a range of scales. The target policy is then obtained by voting. The ensemble needs to be able to efficiently and reliably learn off-policy: requirements fulfilled by the recent Horde architecture, which we take as our basis. We demonstrate empirically that (1) our ensemble policy outperforms both the base policy, and its single-heuristic components, and (2) an ensemble over a general range of scales performs at least as well as one with optimally tuned components.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03552",
        "title": "Applications of Artificial Intelligence Techniques to Combating Cyber Crimes: A Review",
        "authors": [
            "Selma Dilek",
            "H\u00fcseyin \u00c7ak\u0131r",
            "Mustafa Ayd\u0131n"
        ],
        "abstract": "With the advances in information technology (IT) criminals are using cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly vulnerable to intrusions and other threats. Physical devices and human intervention are not sufficient for monitoring and protection of these infrastructures; hence, there is a need for more sophisticated cyber defense systems that need to be flexible, adaptable and robust, and able to detect a wide variety of threats and make intelligent real-time decisions. Numerous bio-inspired computing methods of Artificial Intelligence have been increasingly playing an important role in cyber crime detection and prevention. The purpose of this study is to present advances made so far in the field of applying AI techniques for combating cyber crimes, to demonstrate how these techniques can be an effective tool for detection and prevention of cyber attacks, as well as to give the scope for future work.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03556",
        "title": "An Efficient Metric of Automatic Weight Generation for Properties in Instance Matching Technique",
        "authors": [
            "Md. Hanif Seddiqui",
            "Rudra Pratap Deb Nath",
            "Masaki Aono"
        ],
        "abstract": "The proliferation of heterogeneous data sources of semantic knowledge base intensifies the need of an automatic instance matching technique. However, the efficiency of instance matching is often influenced by the weight of a property associated to instances. Automatic weight generation is a non-trivial, however an important task in instance matching technique. Therefore, identifying an appropriate metric for generating weight for a property automatically is nevertheless a formidable task. In this paper, we investigate an approach of generating weights automatically by considering hypotheses: (1) the weight of a property is directly proportional to the ratio of the number of its distinct values to the number of instances contain the property, and (2) the weight is also proportional to the ratio of the number of distinct values of a property to the number of instances in a training dataset. The basic intuition behind the use of our approach is the classical theory of information content that infrequent words are more informative than frequent ones. Our mathematical model derives a metric for generating property weights automatically, which is applied in instance matching system to produce re-conciliated instances efficiently. Our experiments and evaluations show the effectiveness of our proposed metric of automatic weight generation for properties in an instance matching technique.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03683",
        "title": "Computing rational decisions in extensive games with limited foresight",
        "authors": [
            "Paolo Turrini"
        ],
        "abstract": "We introduce a class of extensive form games where players might not be able to foresee the possible consequences of their decisions and form a model of their opponents which they exploit to achieve a more profitable outcome. We improve upon existing models of games with limited foresight, endowing players with the ability of higher-order reasoning and proposing a novel solution concept to address intuitions coming from real game play. We analyse the resulting equilibria, devising an effective procedure to compute them.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2016-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03890",
        "title": "Decision Maker using Coupled Incompressible-Fluid Cylinders",
        "authors": [
            "Song-Ju Kim",
            "Masashi Aono"
        ],
        "abstract": "The multi-armed bandit problem (MBP) is the problem of finding, as accurately and quickly as possible, the most profitable option from a set of options that gives stochastic rewards by referring to past experiences. Inspired by fluctuated movements of a rigid body in a tug-of-war game, we formulated a unique search algorithm that we call the `tug-of-war (TOW) dynamics' for solving the MBP efficiently. The cognitive medium access, which refers to multi-user channel allocations in cognitive radio, can be interpreted as the competitive multi-armed bandit problem (CMBP); the problem is to determine the optimal strategy for allocating channels to users which yields maximum total rewards gained by all users. Here we show that it is possible to construct a physical device for solving the CMBP, which we call the `TOW Bombe', by exploiting the TOW dynamics existed in coupled incompressible-fluid cylinders. This analog computing device achieves the `socially-maximum' resource allocation that maximizes the total rewards in cognitive medium access without paying a huge computational cost that grows exponentially as a function of the problem size.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03919",
        "title": "Policy Gradient for Coherent Risk Measures",
        "authors": [
            "Aviv Tamar",
            "Yinlam Chow",
            "Mohammad Ghavamzadeh",
            "Shie Mannor"
        ],
        "abstract": "Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03986",
        "title": "A Multicore Tool for Constraint Solving",
        "authors": [
            "Roberto Amadini",
            "Maurizio Gabbrielli",
            "Jacopo Mauro"
        ],
        "abstract": "*** To appear in IJCAI 2015 proceedings *** In Constraint Programming (CP), a portfolio solver uses a variety of different solvers for solving a given Constraint Satisfaction / Optimization Problem. In this paper we introduce sunny-cp2: the first parallel CP portfolio solver that enables a dynamic, cooperative, and simultaneous execution of its solvers in a multicore setting. It incorporates state-of-the-art solvers, providing also a usable and configurable framework. Empirical results are very promising. sunny-cp2 can even outperform the performance of the oracle solver which always selects the best solver of the portfolio for a given problem.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04013",
        "title": "Deep Neural Programs for Adaptive Control in Cyber-Physical Systems",
        "authors": [
            "Konstantin Selyunin",
            "Denise Ratasich",
            "Ezio Bartocci",
            "Radu Grosu"
        ],
        "abstract": "We introduce Deep Neural Programs (DNP), a novel programming paradigm for writing adaptive controllers for cy-ber-physical systems (CPS). DNP replace if and while statements, whose discontinuity is responsible for undecidability in CPS analysis, intractability in CPS design, and frailness in CPS implementation, with their smooth, neural nif and nwhile counterparts. This not only makes CPS analysis decidable and CPS design tractable, but also allows to write robust and adaptive CPS code. In DNP the connection between the sigmoidal guards of the nif and nwhile statements has to be given as a Gaussian Bayesian network, which reflects the partial knowledge, the CPS program has about its environment. To the best of our knowledge, DNP are the first approach linking neural networks to programs, in a way that makes explicit the meaning of the network. In order to prove and validate the usefulness of DNP, we use them to write and learn an adaptive CPS controller for the parallel parking of the Pioneer rovers available in our CPS lab.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04120",
        "title": "About Tau-Chain",
        "authors": [
            "Ohad Asor"
        ],
        "abstract": "Tau-chain is a decentralized peer-to-peer network having three unified faces: Rules, Proofs, and Computer Programs, allowing a generalization of virtually any centralized or decentralized P2P network, together with many new abilities, as we present on this note.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04495",
        "title": "A Generalization of Gustafson-Kessel Algorithm Using a New Constraint Parameter",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "In this paper one presents a new fuzzy clustering algorithm based on a dissimilarity function determined by three parameters. This algorithm can be considered a generalization of the Gustafson-Kessel algorithm for fuzzy clustering.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04593",
        "title": "Explaining robust additive utility models by sequences of preference swaps",
        "authors": [
            "K. Belahcene",
            "C. Labreuche",
            "N. Maudet",
            "V. Mousseau",
            "W. Ouerdane"
        ],
        "abstract": "Multicriteria decision analysis aims at supporting a person facing a decision problem involving conflicting criteria. We consider an additive utility model which provides robust conclusions based on preferences elicited from the decision maker. The recommendations based on these robust conclusions are even more convincing if they are complemented by explanations. We propose a general scheme, based on sequence of preference swaps, in which explanations can be computed. We show first that the length of explanations can be unbounded in the general case. However, in the case of binary reference scales, this length is bounded and we provide an algorithm to compute the corresponding explanation.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04665",
        "title": "Optimizations for Decision Making and Planning in Description Logic Dynamic Knowledge Bases",
        "authors": [
            "Michele Stawowy"
        ],
        "abstract": "Artifact-centric models for business processes recently raised a lot of attention, as they manage to combine structural (i.e. data related) with dynamical (i.e. process related) aspects in a seamless way. Many frameworks developed under this approach, although, are not built explicitly for planning, one of the most prominent operations related to business processes. In this paper, we try to overcome this by proposing a framework named Dynamic Knowledge Bases, aimed at describing rich business domains through Description Logic-based ontologies, and where a set of actions allows the system to evolve by modifying such ontologies. This framework, by offering action rewriting and knowledge partialization, represents a viable and formal environment to develop decision making and planning techniques for DL-based artifact-centric business domains.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04780",
        "title": "Computational Curiosity (A Book Draft)",
        "authors": [
            "Qiong Wu"
        ],
        "abstract": "This book discusses computational curiosity, from the psychology of curiosity to the computational models of curiosity, and then showcases several interesting applications of computational curiosity. A brief overview of the book is given as follows. Chapter 1 discusses the underpinnings of curiosity in human beings, including the major categories of curiosity, curiosity-related emotions and behaviors, and the benefits of curiosity. Chapter 2 reviews the arousal theories of curiosity in psychology and summarizes a general two-step process model for computational curiosity. Base on the perspective of the two-step process model, Chapter 3 reviews and analyzes some of the traditional computational models of curiosity. Chapter 4 introduces a novel generic computational model of curiosity, which is developed based on the arousal theories of curiosity. After the discussion of computational models of curiosity, we outline the important applications where computational curiosity may bring significant impacts in Chapter 5. Chapter 6 discusses the application of the generic computational model of curiosity in a machine learning framework. Chapter 7 discusses the application of the generic computational model of curiosity in a recommender system. In Chapter 8 and Chapter 9, the generic computational model of curiosity is studied in two types of pedagogical agents. In Chapter 8, a curious peer learner is studied. It is a non-player character that aims to provide a believable virtual learning environment for users. In Chapter 9, a curious learning companion is studied. It aims to enhance users' learning experience through providing meaningful interactions with them. Chapter 10 discusses open questions in the research field of computation curiosity.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04791",
        "title": "Advances in Artificial Intelligence: Are you sure, we are on the right track?",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "Over the past decade, AI has made a remarkable progress. It is agreed that this is due to the recently revived Deep Learning technology. Deep Learning enables to process large amounts of data using simplified neuron networks that simulate the way in which the brain works. However, there is a different point of view, which posits that the brain is processing information, not data. This unresolved duality hampered AI progress for years. In this paper, I propose a notion of Integrated information that hopefully will resolve the problem. I consider integrated information as a coupling between two separate entities - physical information (that implies data processing) and semantic information (that provides physical information interpretation). In this regard, intelligence becomes a product of information processing. Extending further this line of thinking, it can be said that information processing does not require more a human brain for its implementation. Indeed, bacteria and amoebas exhibit intelligent behavior without any sign of a brain. That dramatically removes the need for AI systems to emulate the human brain complexity! The paper tries to explore this shift in AI systems design philosophy.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04956",
        "title": "The Linearization of Belief Propagation on Pairwise Markov Networks",
        "authors": [
            "Wolfgang Gatterbauer"
        ],
        "abstract": "Belief Propagation (BP) is a widely used approximation for exact probabilistic inference in graphical models, such as Markov Random Fields (MRFs). In graphs with cycles, however, no exact convergence guarantees for BP are known, in general. For the case when all edges in the MRF carry the same symmetric, doubly stochastic potential, recent works have proposed to approximate BP by linearizing the update equations around default values, which was shown to work well for the problem of node classification. The present paper generalizes all prior work and derives an approach that approximates loopy BP on any pairwise MRF with the problem of solving a linear equation system. This approach combines exact convergence guarantees and a fast matrix implementation with the ability to model heterogenous networks. Experiments on synthetic graphs with planted edge potentials show that the linearization has comparable labeling accuracy as BP for graphs with weak potentials, while speeding-up inference by orders of magnitude.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05021",
        "title": "Inductive Learning for Rule Generation from Ontology",
        "authors": [
            "Olegs Verhodubs"
        ],
        "abstract": "This paper presents an idea of inductive learning use for rule generation from ontologies. The main purpose of the paper is to evaluate the possibility of inductive learning use in rule generation from ontologies and to develop the way how this can be done. Generated rules are necessary to supplement or even to develop the Semantic Web Expert System (SWES) knowledge base. The SWES emerges as the result of evolution of expert system concept toward the Web, and the SWES is based on the Semantic Web technologies. Available publications show that the problem of rule generation from ontologies based on inductive learning is not investigated deeply enough.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05040",
        "title": "Design of a Framework to Facilitate Decisions Using Information Fusion",
        "authors": [
            "Tamer M. Abo Neama",
            "Ismail A. Ismail",
            "Tarek S. Sobh",
            "M. Zaki"
        ],
        "abstract": "Information fusion is an advanced research area which can assist decision makers in enhancing their decisions. This paper aims at designing a new multi-layer framework that can support the process of performing decisions from the obtained beliefs using information fusion. Since it is not an easy task to cross the gap between computed beliefs of certain hypothesis and decisions, the proposed framework consists of the following layers in order to provide a suitable architecture (ordered bottom up): 1. A layer for combination of basic belief assignments using an information fusion approach. Such approach exploits Dezert-Smarandache Theory, DSmT, and proportional conflict redistribution to provide more realistic final beliefs. 2. A layer for computation of pignistic probability of the underlying propositions from the corresponding final beliefs. 3. A layer for performing probabilistic reasoning using a Bayesian network that can obtain the probable reason of a proposition from its pignistic probability. 4. Ranking the system decisions is ultimately used to support decision making. A case study has been accomplished at various operational conditions in order to prove the concept, in addition it pointed out that: 1. The use of DSmT for information fusion yields not only more realistic beliefs but also reliable pignistic probabilities for the underlying propositions. 2. Exploiting the pignistic probability for the integration of the information fusion with the Bayesian network provides probabilistic inference and enable decision making on the basis of both belief based probabilities for the underlying propositions and Bayesian based probabilities for the corresponding reasons. A comparative study of the proposed framework with respect to other information fusion systems confirms its superiority to support decision making.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05443",
        "title": "Influence-Optimistic Local Values for Multiagent Planning --- Extended Version",
        "authors": [
            "Frans A. Oliehoek",
            "Matthijs T.J. Spaan",
            "Stefan Witwicki"
        ],
        "abstract": "Recent years have seen the development of methods for multiagent planning under uncertainty that scale to tens or even hundreds of agents. However, most of these methods either make restrictive assumptions on the problem domain, or provide approximate solutions without any guarantees on quality. Methods in the former category typically build on heuristic search using upper bounds on the value function. Unfortunately, no techniques exist to compute such upper bounds for problems with non-factored value functions. To allow for meaningful benchmarking through measurable quality guarantees on a very general class of problems, this paper introduces a family of influence-optimistic upper bounds for factored decentralized partially observable Markov decision processes (Dec-POMDPs) that do not have factored value functions. Intuitively, we derive bounds on very large multiagent planning problems by subdividing them in sub-problems, and at each of these sub-problems making optimistic assumptions with respect to the influence that will be exerted by the rest of the system. We numerically compare the different upper bounds and demonstrate how we can achieve a non-trivial guarantee that a heuristic solution for problems with hundreds of agents is close to optimal. Furthermore, we provide evidence that the upper bounds may improve the effectiveness of heuristic influence search, and discuss further potential applications to multiagent planning.\n    ",
        "submission_date": "2015-02-18T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05450",
        "title": "The (Final) countdown",
        "authors": [
            "Jean-Marc Alliot"
        ],
        "abstract": "The Countdown game is one of the oldest TV show running in the world. It started broadcasting in 1972 on the french television and in 1982 on British channel 4, and it has been running since in both countries. The game, while extremely popular, never received any serious scientific attention, probably because it seems too simple at first sight. We present in this article an in-depth analysis of the numbers round of the countdown game. This includes a complexity analysis of the game, an analysis of existing algorithms, the presentation of a new algorithm that increases resolution speed by a factor of 20. It also includes some leads on how to turn the game into a more difficult one, both for a human player and for a computer, and even to transform it into a probably undecidable problem.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05562",
        "title": "A New Penta-valued Logic Based Knowledge Representation",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "In this paper a knowledge representation model are proposed, FP5, which combine the ideas from fuzzy sets and penta-valued logic. FP5 represents imprecise properties whose accomplished degree is undefined, contradictory or indeterminate for some objects. Basic operations of conjunction, disjunction and negation are introduced. Relations to other representation models like fuzzy sets, intuitionistic, paraconsistent and bipolar fuzzy sets are discussed.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05615",
        "title": "Forgetting and consolidation for incremental and cumulative knowledge acquisition systems",
        "authors": [
            "Fernando Mart\u00ednez-Plumed",
            "C\u00e8sar Ferri",
            "Jos\u00e9 Hern\u00e1ndez-Orallo",
            "Mar\u00eda Jos\u00e9 Ram\u00edrez-Quintana"
        ],
        "abstract": "The application of cognitive mechanisms to support knowledge acquisition is, from our point of view, crucial for making the resulting models coherent, efficient, credible, easy to use and understandable. In particular, there are two characteristic features of intelligence that are essential for knowledge development: forgetting and consolidation. Both plays an important role in knowledge bases and learning systems to avoid possible information overflow and redundancy, and in order to preserve and strengthen important or frequently used rules and remove (or forget) useless ones. We present an incremental, long-life view of knowledge acquisition which tries to improve task after task by determining what to keep, what to consolidate and what to forget, overcoming The Stability-Plasticity dilemma. In order to do that, we rate rules by introducing several metrics through the first adaptation, to our knowledge, of the Minimum Message Length (MML) principle to a coverage graph, a hierarchical assessment structure which treats evidence and rules in a unified way. The metrics are not only used to forget some of the worst rules, but also to set a consolidation process to promote those selected rules to the knowledge base, which is also mirrored by a demotion system. We evaluate the framework with a series of tasks in a chess rule learning domain.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05698",
        "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
        "authors": [
            "Jason Weston",
            "Antoine Bordes",
            "Sumit Chopra",
            "Alexander M. Rush",
            "Bart van Merri\u00ebnboer",
            "Armand Joulin",
            "Tomas Mikolov"
        ],
        "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05838",
        "title": "Automated Reasoning for Robot Ethics",
        "authors": [
            "Ulrich Furbach",
            "Claudia Schon",
            "Frieder Stolzenburg"
        ],
        "abstract": "Deontic logic is a very well researched branch of mathematical logic and philosophy. Various kinds of deontic logics are considered for different application domains like argumentation theory, legal reasoning, and acts in multi-agent systems. In this paper, we show how standard deontic logic can be used to model ethical codes for multi-agent systems. Furthermore we show how Hyper, a high performance theorem prover, can be used to prove properties of these ethical codes.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05864",
        "title": "Pseudo Fuzzy Set",
        "authors": [
            "Sukanta Nayak",
            "Snehashish Chakraverty"
        ],
        "abstract": "Here a novel idea to handle imprecise or vague set viz. Pseudo fuzzy set has been proposed. Pseudo fuzzy set is a triplet of element and its two membership functions. Both the membership functions may or may not be dependent. The hypothesis is that every positive sense has some negative sense. So, one membership function has been considered as positive and another as negative. Considering this concept, here the development of Pseudo fuzzy set and its property along with Pseudo fuzzy numbers has been discussed.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05888",
        "title": "A partial taxonomy of judgment aggregation rules, and their properties",
        "authors": [
            "Jer\u00f4me Lang",
            "Gabriella Pigozzi",
            "Marija Slavkovik",
            "Leendert van der Torre",
            "Srdjan Vesic"
        ],
        "abstract": "The literature on judgment aggregation is moving from studying impossibility results regarding aggregation rules towards studying specific judgment aggregation rules. Here we give a structured list of most rules that have been proposed and studied recently in the literature, together with various properties of such rules. We first focus on the majority-preservation property, which generalizes Condorcet-consistency, and identify which of the rules satisfy it. We study the inclusion relationships that hold between the rules. Finally, we consider two forms of unanimity, monotonicity, homogeneity, and reinforcement, and we identify which of the rules satisfy these properties.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06124",
        "title": "Unified vector space mapping for knowledge representation systems",
        "authors": [
            "Dmytro Filatov",
            "Taras Filatov"
        ],
        "abstract": "One of the most significant problems which inhibits further developments in the areas of Knowledge Representation and Artificial Intelligence is a problem of semantic alignment or knowledge mapping. The progress in its solution will be greatly beneficial for further advances of information retrieval, ontology alignment, relevance calculation, text mining, natural language processing etc. In the paper the concept of multidimensional global knowledge map, elaborated through unsupervised extraction of dependencies from large documents corpus, is proposed. In addition, the problem of direct Human - Knowledge Representation System interface is addressed and a concept of adaptive decoder proposed for the purpose of interaction with previously described unified mapping model. In combination these two approaches are suggested as basis for a development of a new generation of knowledge representation systems.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2015-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06132",
        "title": "Universal Memory Architectures for Autonomous Machines",
        "authors": [
            "Dan P. Guralnik",
            "Daniel E. Koditschek"
        ],
        "abstract": "We propose a self-organizing memory architecture for perceptual experience, capable of supporting autonomous learning and goal-directed problem solving in the absence of any prior information about the agent's environment. The architecture is simple enough to ensure (1) a quadratic bound (in the number of available sensors) on space requirements, and (2) a quadratic bound on the time-complexity of the update-execute cycle. At the same time, it is sufficiently complex to provide the agent with an internal representation which is (3) minimal among all representations of its class which account for every sensory equivalence class subject to the agent's belief state; (4) capable, in principle, of recovering the homotopy type of the system's state space; (5) learnable with arbitrary precision through a random application of the available actions. The provable properties of an effectively trained memory structure exploit a duality between weak poc sets -- a symbolic (discrete) representation of subset nesting relations -- and non-positively curved cubical complexes, whose rich convexity theory underlies the planning cycle of the proposed architecture.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2015-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06512",
        "title": "From Seed AI to Technological Singularity via Recursively Self-Improving Software",
        "authors": [
            "Roman V. Yampolskiy"
        ],
        "abstract": "Software capable of improving itself has been a dream of computer scientists since the inception of the field. In this work we provide definitions for Recursively Self-Improving software, survey different types of self-improving software, review the relevant literature, analyze limits on computation restricting recursive self-improvement and introduce RSI Convergence Theory which aims to predict general behavior of RSI systems. Finally, we address security implications from self-improving intelligent software.\n    ",
        "submission_date": "2015-02-23T00:00:00",
        "last_modified_date": "2015-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06657",
        "title": "Multi-Touch Attribution Based Budget Allocation in Online Advertising",
        "authors": [
            "Sahin Cem Geyik",
            "Abhishek Saxena",
            "Ali Dasdan"
        ],
        "abstract": "Budget allocation in online advertising deals with distributing the campaign (insertion order) level budgets to different sub-campaigns which employ different targeting criteria and may perform differently in terms of return-on-investment (ROI). In this paper, we present the efforts at Turn on how to best allocate campaign budget so that the advertiser or campaign-level ROI is maximized. To do this, it is crucial to be able to correctly determine the performance of sub-campaigns. This determination is highly related to the action-attribution problem, i.e. to be able to find out the set of ads, and hence the sub-campaigns that provided them to a user, that an action should be attributed to. For this purpose, we employ both last-touch (last ad gets all credit) and multi-touch (many ads share the credit) attribution methodologies. We present the algorithms deployed at Turn for the attribution problem, as well as their parallel implementation on the large advertiser performance datasets. We conclude the paper with our empirical comparison of last-touch and multi-touch attribution-based budget allocation in a real online advertising setting.\n    ",
        "submission_date": "2015-02-24T00:00:00",
        "last_modified_date": "2015-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06818",
        "title": "Tensor SimRank for Heterogeneous Information Networks",
        "authors": [
            "Ben Usman",
            "Ivan Oseledets"
        ],
        "abstract": "We propose a generalization of SimRank similarity measure for heterogeneous information networks. Given the information network, the intraclass similarity score s(a, b) is high if the set of objects that are related with a and the set of objects that are related with b are pair-wise similar according to all imposed relations.\n    ",
        "submission_date": "2015-02-24T00:00:00",
        "last_modified_date": "2015-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06956",
        "title": "Transformation of basic probability assignments to probabilities based on a new entropy measure",
        "authors": [
            "Xinyang Deng",
            "Yong Deng"
        ],
        "abstract": "Dempster-Shafer evidence theory is an efficient mathematical tool to deal with uncertain information. In that theory, basic probability assignment (BPA) is the basic element for the expression and inference of uncertainty. Decision-making based on BPA is still an open issue in Dempster-Shafer evidence theory. In this paper, a novel approach of transforming basic probability assignments to probabilities is proposed based on Deng entropy which is a new measure for the uncertainty of BPA. The principle of the proposed method is to minimize the difference of uncertainties involving in the given BPA and obtained probability distribution. Numerical examples are given to show the proposed approach.\n    ",
        "submission_date": "2015-02-24T00:00:00",
        "last_modified_date": "2015-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07314",
        "title": "Path Finding under Uncertainty through Probabilistic Inference",
        "authors": [
            "David Tolpin",
            "Brooks Paige",
            "Jan Willem van de Meent",
            "Frank Wood"
        ],
        "abstract": "We introduce a new approach to solving path-finding problems under uncertainty by representing them as probabilistic models and applying domain-independent inference algorithms to the models. This approach separates problem representation from the inference algorithm and provides a framework for efficient learning of path-finding policies. We evaluate the new approach on the Canadian Traveler Problem, which we formulate as a probabilistic model, and show how probabilistic inference allows high performance stochastic policies to be obtained for this problem.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07428",
        "title": "Representative Selection in Non Metric Datasets",
        "authors": [
            "Elad Liebman",
            "Benny Chor",
            "Peter Stone"
        ],
        "abstract": "This paper considers the problem of representative selection: choosing a subset of data points from a dataset that best represents its overall set of elements. This subset needs to inherently reflect the type of information contained in the entire set, while minimizing redundancy. For such purposes, clustering may seem like a natural approach. However, existing clustering methods are not ideally suited for representative selection, especially when dealing with non-metric data, where only a pairwise similarity measure exists. In this paper we propose $\\delta$-medoids, a novel approach that can be viewed as an extension to the $k$-medoids algorithm and is specifically suited for sample representative selection from non-metric data. We empirically validate $\\delta$-medoids in two domains, namely music analysis and motion analysis. We also show some theoretical bounds on the performance of $\\delta$-medoids and the hardness of representative selection in general.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07628",
        "title": "Relaxation-based revision operators in description logics",
        "authors": [
            "Marc Aiguier",
            "Jamal Atif",
            "Isabelle Bloch",
            "C\u00e9line Hudelot"
        ],
        "abstract": "As ontologies and description logics (DLs) reach out to a broader audience, several reasoning services are developed in this context. Belief revision is one of them, of prime importance when knowledge is prone to change and inconsistency. In this paper we address both the generalization of the well-known AGM postulates, and the definition of concrete and well-founded revision operators in different DL families. We introduce a model-theoretic version of the AGM postulates with a general definition of inconsistency, hence enlarging their scope to a wide family of non-classical logics, in particular negation-free DL families. We propose a general framework for defining revision operators based on the notion of relaxation, introduced recently for defining dissimilarity measures between DL concepts. A revision operator in this framework amounts to relax the set of models of the old belief until it reaches the sets of models of the new piece of knowledge. We demonstrate that such a relaxation-based revision operator defines a faithful assignment and satisfies the generalized AGM postulates. Another important contribution concerns the definition of several concrete relaxation operators suited to the syntax of some DLs (ALC and its fragments EL and ELU).\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00038",
        "title": "Sequential Feature Explanations for Anomaly Detection",
        "authors": [
            "Md Amran Siddiqui",
            "Alan Fern",
            "Thomas G. Dietterich",
            "Weng-Keen Wong"
        ],
        "abstract": "In many applications, an anomaly detection system presents the most anomalous data instance to a human analyst, who then must determine whether the instance is truly of interest (e.g. a threat in a security setting). Unfortunately, most anomaly detectors provide no explanation about why an instance was considered anomalous, leaving the analyst with no guidance about where to begin the investigation. To address this issue, we study the problems of computing and evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE of an anomaly is a sequence of features, which are presented to the analyst one at a time (in order) until the information contained in the highlighted features is enough for the analyst to make a confident judgement about the anomaly. Since analyst effort is related to the amount of information that they consider in an investigation, an explanation's quality is related to the number of features that must be revealed to attain confidence. One of our main contributions is to present a novel framework for large scale quantitative evaluations of SFEs, where the quality measure is based on analyst effort. To do this we construct anomaly detection benchmarks from real data sets along with artificial experts that can be simulated for evaluation. Our second contribution is to evaluate several novel explanation approaches within the framework and on traditional anomaly detection benchmarks, offering several insights into the approaches.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00185",
        "title": "When Are Tree Structures Necessary for Deep Learning of Representations?",
        "authors": [
            "Jiwei Li",
            "Minh-Thang Luong",
            "Dan Jurafsky",
            "Eudard Hovy"
        ],
        "abstract": "Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. But there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper we benchmark {\\bf recursive} neural models against sequential {\\bf recurrent} neural models (simple recurrent and LSTM models), enforcing apples-to-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answer-phrases; (3) discourse parsing; (4) semantic relation extraction (e.g., {\\em component-whole} between nouns).\n",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00806",
        "title": "An Introduction to Logics of Knowledge and Belief",
        "authors": [
            "Hans van Ditmarsch",
            "Joseph Y. Halpern",
            "Wiebe van der Hoek",
            "Barteld Kooi"
        ],
        "abstract": "This chapter provides an introduction to some basic concepts of epistemic logic, basic formal languages, their semantics, and proof systems. It also contains an overview of the handbook, and a brief history of epistemic logic and pointers to the literature.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00899",
        "title": "An Ant Colony Optimization Algorithm for Partitioning Graphs with Supply and Demand",
        "authors": [
            "Raka Jovanovic",
            "Milan Tuba",
            "Stefan Voss"
        ],
        "abstract": "In this paper we focus on finding high quality solutions for the problem of maximum partitioning of graphs with supply and demand (MPGSD). There is a growing interest for the MPGSD due to its close connection to problems appearing in the field of electrical distribution systems, especially for the optimization of self-adequacy of interconnected microgrids. We propose an ant colony optimization algorithm for the problem. With the goal of further improving the algorithm we combine it with a previously developed correction procedure. In our computational experiments we evaluate the performance of the proposed algorithm on both trees and general graphs. The tests show that the method manages to find optimal solutions in more than 50% of the problem instances, and has an average relative error of less than 0.5% when compared to known optimal solutions.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00980",
        "title": "On memetic search for the max-mean dispersion problem",
        "authors": [
            "Xiangjing Lai",
            "Jin-Kao Hao"
        ],
        "abstract": "Given a set $V$ of $n$ elements and a distance matrix $[d_{ij}]_{n\\times n}$ among elements, the max-mean dispersion problem (MaxMeanDP) consists in selecting a subset $M$ from $V$ such that the mean dispersion (or distance) among the selected elements is maximized. Being a useful model to formulate several relevant applications, MaxMeanDP is known to be NP-hard and thus computationally difficult. In this paper, we present a highly effective memetic algorithm for MaxMeanDP which relies on solution recombination and local optimization to find high quality solutions. Computational experiments on the set of 160 benchmark instances with up to 1000 elements commonly used in the literature show that the proposed algorithm improves or matches the published best known results for all instances in a short computing time, with only one exception, while achieving a high success rate of 100\\%. In particular, we improve 59 previous best results out of the 60 most challenging instances. Results on a set of 40 new large instances with 3000 and 5000 elements are also presented. The key ingredients of the proposed algorithm are investigated to shed light on how they affect the performance of the algorithm.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01051",
        "title": "Combining Probabilistic, Causal, and Normative Reasoning in CP-logic",
        "authors": [
            "Sander Beckers",
            "Joost Vennekens"
        ],
        "abstract": "In recent years the search for a proper formal definition of actual causation -- i.e., the relation of cause-effect as it is instantiated in specific observations, rather than general causal relations -- has taken on impressive proportions. In part this is due to the insight that this concept plays a fundamental role in many different fields, such as legal theory, engineering, medicine, ethics, etc. Because of this diversity in applications, some researchers have shifted focus from a single idealized definition towards a more pragmatic, context-based account. For instance, recent work by Halpern and Hitchcock draws on empirical research regarding people's causal judgments, to suggest a graded and context-sensitive notion of causation. Although we sympathize with many of their observations, their restriction to a merely qualitative ordering runs into trouble for more complex examples. Therefore we aim to improve on their approach, by using the formal language of CP-logic (Causal Probabilistic logic), and the framework for defining actual causation that was developed by the current authors using it. First we rephrase their ideas into our quantitative, probabilistic setting, after which we modify it to accommodate a greater class of examples. Further, we introduce a formal distinction between statistical and normative considerations.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01158",
        "title": "A Meta-Analysis of the Anomaly Detection Problem",
        "authors": [
            "Andrew Emmott",
            "Shubhomoy Das",
            "Thomas Dietterich",
            "Alan Fern",
            "Weng-Keen Wong"
        ],
        "abstract": "This article provides a thorough meta-analysis of the anomaly detection problem. To accomplish this we first identify approaches to benchmarking anomaly detection algorithms across the literature and produce a large corpus of anomaly detection benchmarks that vary in their construction across several dimensions we deem important to real-world applications: (a) point difficulty, (b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d) relevance of features. We apply a representative set of anomaly detection algorithms to this corpus, yielding a very large collection of experimental results. We analyze these results to understand many phenomena observed in previous work. First we observe the effects of experimental design on experimental results. Second, results are evaluated with two metrics, ROC Area Under the Curve and Average Precision. We employ statistical hypothesis testing to demonstrate the value (or lack thereof) of our benchmarks. We then offer several approaches to summarizing our experimental results, drawing several conclusions about the impact of our methodology as well as the strengths and weaknesses of some algorithms. Last, we compare results against a trivial solution as an alternate means of normalizing the reported performance of algorithms. The intended contributions of this article are many; in addition to providing a large publicly-available corpus of anomaly detection benchmarks, we provide an ontology for describing anomaly detection contexts, a methodology for controlling various aspects of benchmark creation, guidelines for future experimental design and a discussion of the many potential pitfalls of trying to measure success in this field.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01288",
        "title": "Game-theoretic Approach for Non-Cooperative Planning",
        "authors": [
            "Jaume Jord\u00e1n",
            "Eva Onaindia"
        ],
        "abstract": "When two or more self-interested agents put their plans to execution in the same environment, conflicts may arise as a consequence, for instance, of a common utilization of resources. In this case, an agent can postpone the execution of a particular action, if this punctually solves the conflict, or it can resort to execute a different plan if the agent's payoff significantly diminishes due to the action deferral. In this paper, we present a game-theoretic approach to non-cooperative planning that helps predict before execution what plan schedules agents will adopt so that the set of strategies of all agents constitute a Nash equilibrium. We perform some experiments and discuss the solutions obtained with our game-theoretical approach, analyzing how the conflicts between the plans determine the strategic behavior of the agents.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01299",
        "title": "Telling cause from effect in deterministic linear dynamical systems",
        "authors": [
            "Naji Shajarisales",
            "Dominik Janzing",
            "Bernhard Shoelkopf",
            "Michel Besserve"
        ],
        "abstract": "Inferring a cause from its effect using observed time series data is a major challenge in natural and social sciences. Assuming the effect is generated by the cause trough a linear system, we propose a new approach based on the hypothesis that nature chooses the \"cause\" and the \"mechanism that generates the effect from the cause\" independent of each other. We therefore postulate that the power spectrum of the time series being the cause is uncorrelated with the square of the transfer function of the linear filter generating the effect. While most causal discovery methods for time series mainly rely on the noise, our method relies on asymmetries of the power spectral density properties that can be exploited even in the context of deterministic systems. We describe mathematical assumptions in a deterministic model under which the causal direction is identifiable with this approach. We also discuss the method's performance under the additive noise model and its relationship to Granger causality. Experiments show encouraging results on synthetic as well as real-world data. Overall, this suggests that the postulate of Independence of Cause and Mechanism is a promising principle for causal inference on empirical time series.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01327",
        "title": "Estimating the Probability of Meeting a Deadline in Hierarchical Plans",
        "authors": [
            "Liat Cohen",
            "Solomon Eyal Shimony",
            "Gera Weiss"
        ],
        "abstract": "Given a hierarchical plan (or schedule) with uncertain task times, we propose a deterministic polynomial (time and memory) algorithm for estimating the probability that its meets a deadline, or, alternately, that its {\\em makespan} is less than a given duration. Approximation is needed as it is known that this problem is NP-hard even for sequential plans (just, a sum of random variables). In addition, we show two new complexity results: (1) Counting the number of events that do not cross deadline is \\#P-hard; (2)~Computing the expected makespan of a hierarchical plan is NP-hard. For the proposed approximation algorithm, we establish formal approximation bounds and show that the time and memory complexities grow polynomially with the required accuracy, the number of nodes in the plan, and with the size of the support of the random variables that represent the durations of the primitive tasks. We examine these approximation bounds empirically and demonstrate, using task networks taken from the literature, how our scheme outperforms sampling techniques and exact computation in terms of accuracy and run-time. As the empirical data shows much better error bounds than guaranteed, we also suggest a method for tightening the bounds in some cases.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2017-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01446",
        "title": "Predicting opponent team activity in a RoboCup environment",
        "authors": [
            "Selene Baez"
        ],
        "abstract": "The goal of this project is to predict the opponent's configuration in a RoboCup SSL environment. For simplicity, a Markov model assumption is made such that the predicted formation of the opponent team only depends on its current formation. The field is divided into a grid and a robot state per player is created with information about its position and its velocity. To gather a more general sense of what the opposing team is doing, the state also incorporates the team's average position (centroid). All possible state transitions are stored in a hash table that requires minimum storage space. The table is populated with transition probabilities that are learned by reading vision packages and counting the state transitions regardless of the specific robot player. Therefore, the computation during the game is reduced to interpreting a given vision package to assign each player to a state, and looking for the most likely state it will transition to. The confidence of the predicted team's formation is the product of each individual player's probability. The project is noteworthy in that it minimizes the time and space complexity requirements for opponent's moves prediction.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02521",
        "title": "A Single-Pass Classifier for Categorical Data",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper describes a new method for classifying a dataset that partitions elements into their categories. It has relations with neural networks but a slightly different structure, requiring only a single pass through the classifier to generate the weight sets. A grid-like structure is required as part of a novel idea of converting a 1-D row of real values into a 2-D structure of value bands. Each cell in any band then stores a distinct set of weights, to represent its own importance and its relation to each output category. During classification, all of the output weight lists can be retrieved and summed to produce a probability for what the correct output category is. The bands possibly work like hidden layers of neurons, but they are variable specific, making the process orthogonal. The construction process can be a single update process without iterations, making it potentially much faster. It can also be compared with k-NN and may be practical for partial or competitive updating.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02626",
        "title": "On the Intrinsic Limits to Representationally-Adaptive Machine-Learning",
        "authors": [
            "David Windridge"
        ],
        "abstract": "Online learning is a familiar problem setting within Machine-Learning in which data is presented serially in time to a learning agent, requiring it to progressively adapt within the constraints of the learning algorithm. More sophisticated variants may involve concepts such as transfer-learning which increase this adaptive capability, enhancing the learner's cognitive capacities in a manner that can begin to imitate the open-ended learning capabilities of human beings.\n",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02917",
        "title": "A Case Based Reasoning Approach for Answer Reranking in Question Answering",
        "authors": [
            "Karl-Heinz Weis"
        ],
        "abstract": "In this document I present an approach to answer validation and reranking for question answering (QA) systems. A cased-based reasoning (CBR) system judges answer candidates for questions from annotated answer candidates for earlier questions. The promise of this approach is that user feedback will result in improved answers of the QA system, due to the growing case base. In the paper, I present the adequate structuring of the case base and the appropriate selection of relevant similarity measures, in order to solve the answer validation problem. The structural case base is built from annotated MultiNet graphs, which provide representations for natural language expressions, and corresponding graph similarity measures. I cover a priori relations to experienced answer candidates for former questions. I compare the CBR System results to current approaches in an experiment integrating CBR into an existing framework for answer validation and reranking. This integration is achieved by adding CBR-related features to the input of a learned ranking model that determines the final answer ranking. In the experiments based on QA@CLEF questions, the best learned models make heavy use of CBR features. Observing the results with a continually growing case base, I present a positive effect of the size of the case base on the accuracy of the CBR subsystem.\n    ",
        "submission_date": "2015-03-10T00:00:00",
        "last_modified_date": "2015-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02994",
        "title": "Quantum Structure in Cognition, Origins, Developments, Successes and Expectations",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "We provide an overview of the results we have attained in the last decade on the identification of quantum structures in cognition and, more specifically, in the formalization and representation of natural concepts. We firstly discuss the quantum foundational reasons that led us to investigate the mechanisms of formation and combination of concepts in human reasoning, starting from the empirically observed deviations from classical logical and probabilistic structures. We then develop our quantum-theoretic perspective in Fock space which allows successful modeling of various sets of cognitive experiments collected by different scientists, including ourselves. In addition, we formulate a unified explanatory hypothesis for the presence of quantum structures in cognitive processes, and discuss our recent discovery of further quantum aspects in concept combinations, namely, 'entanglement' and 'indistinguishability'. We finally illustrate perspectives for future research.\n    ",
        "submission_date": "2015-03-10T00:00:00",
        "last_modified_date": "2015-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03787",
        "title": "Are there intelligent Turing machines?",
        "authors": [
            "Norbert B\u00e1tfai"
        ],
        "abstract": "This paper introduces a new computing model based on the cooperation among Turing machines called orchestrated machines. Like universal Turing machines, orchestrated machines are also designed to simulate Turing machines but they can also modify the original operation of the included Turing machines to create a new layer of some kind of collective behavior. Using this new model we can define some interested notions related to cooperation ability of Turing machines such as the intelligence quotient or the emotional intelligence quotient for Turing machines.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03964",
        "title": "Interactive Restless Multi-armed Bandit Game and Swarm Intelligence Effect",
        "authors": [
            "Shunsuke Yoshida",
            "Masato Hisakado",
            "Shintaro Mori"
        ],
        "abstract": "We obtain the conditions for the emergence of the swarm intelligence effect in an interactive game of restless multi-armed bandit (rMAB). A player competes with multiple agents. Each bandit has a payoff that changes with a probability $p_{c}$ per round. The agents and player choose one of three options: (1) Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among $n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good bandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision: (i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability for Observe in learning. The parameters $(c,p_{obs})$ are uniformly distributed. We determine the optimal strategies for the player using complete knowledge about the rMAB. We show whether or not social or asocial learning is more optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence effect. We conduct a laboratory experiment (67 subjects) and observe the swarm intelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning is far more optimal than asocial learning.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04187",
        "title": "A Minimal Active Inference Agent",
        "authors": [
            "Simon McGregor",
            "Manuel Baltieri",
            "Christopher L. Buckley"
        ],
        "abstract": "Research on the so-called \"free-energy principle'' (FEP) in cognitive neuroscience is becoming increasingly high-profile. To date, introductions to this theory have proved difficult for many readers to follow, but it depends mainly upon two relatively simple ideas: firstly that normative or teleological values can be expressed as probability distributions (active inference), and secondly that approximate Bayesian reasoning can be effectively performed by gradient descent on model parameters (the free-energy principle). The notion of active inference is of great interest for a number of disciplines including cognitive science and artificial intelligence, as well as cognitive neuroscience, and deserves to be more widely known.\n",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04220",
        "title": "Fuzzy Mixed Integer Optimization Model for Regression Approach",
        "authors": [
            "Arindam Chaudhuri",
            "Dipak Chatterjee"
        ],
        "abstract": "Mixed Integer Optimization has been a topic of active research in past decades. It has been used to solve Statistical problems of classification and regression involving massive data. However, there is an inherent degree of vagueness present in huge real life data. This impreciseness is handled by Fuzzy Sets. In this Paper, Fuzzy Mixed Integer Optimization Method (FMIOM) is used to find solution to Regression problem. The methodology exploits discrete character of problem. In this way large scale problems are solved within practical limits. The data points are separated into different polyhedral regions and each region has its own distinct regression coefficients. In this attempt, an attention is drawn to Statistics and Data Mining community that Integer Optimization can be significantly used to revisit different Statistical problems. Computational experimentations with generated and real data sets show that FMIOM is comparable to and often outperforms current leading methods. The results illustrate potential for significant impact of Fuzzy Integer Optimization methods on Computational Statistics and Data Mining.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04222",
        "title": "Fuzzy Mixed Integer Linear Programming for Air Vehicles Operations Optimization",
        "authors": [
            "Arindam Chaudhuri",
            "Dipak Chatterjee",
            "Ritesh Rajput"
        ],
        "abstract": "Multiple Air Vehicles (AVs) to prosecute geographically dispersed targets is an important optimization problem. Associated multiple tasks viz., target classification, attack and verification are successively performed on each target. The optimal minimum time performance of these tasks requires cooperation among vehicles such that critical time constraints are satisfied i.e. target must be classified before it can be attacked and AV is sent to target area to verify its destruction after target has been attacked. Here, optimal task scheduling problem from Indian Air Force is formulated as Fuzzy Mixed Integer Linear Programming (FMILP) problem. The solution assigns all tasks to vehicles and performs scheduling in an optimal manner including scheduled staged departure times. Coupled tasks involving time and task order constraints are addressed. When AVs have sufficient endurance, existence of optimal solution is guaranteed. The solution developed can serve as an effective heuristic for different categories of AV optimization problems.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04260",
        "title": "Quantum Structure of Negation and Conjunction in Human Thought",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "We analyse in this paper the data collected in a set of experiments performed on human subjects on the combination of natural concepts. We investigate the mutual influence of conceptual conjunction and negation by measuring the membership weights of a list of exemplars with respect to two concepts, e.g., 'Fruits' and 'Vegetables', and their conjunction 'Fruits And Vegetables', but also their conjunction when one or both concepts are negated, namely, 'Fruits And Not Vegetables', 'Not Fruits And Vegetables' and 'Not Fruits And Not Vegetables'. Our findings sharpen existing analysis on conceptual combinations, revealing systematic and remarkable deviations from classical (fuzzy set) logic and probability theory. And, more important, our results give further considerable evidence to the validity of our quantum-theoretic framework for the combination of two concepts. Indeed, the representation of conceptual negation naturally arises from the general assumptions of our two-sector Fock space model, and this representation faithfully agrees with the collected data. In addition, we find a further significant deviation and a priori unexpected from classicality, which can exactly be explained by assuming that human reasoning is the superposition of an 'emergent reasoning' and a 'logical reasoning', and that these two processes can be successfully represented in a Fock space algebraic structure.\n    ",
        "submission_date": "2015-03-14T00:00:00",
        "last_modified_date": "2015-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04333",
        "title": "A More Human Way to Play Computer Chess",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper suggests a forward-pruning technique for computer chess that uses 'Move Tables', which are like Transposition Tables, but for moves not positions. They use an efficient memory structure and has put the design into the context of long and short-term memories. The long-term memory updates a play path with weight reinforcement, while the short-term memory can be immediately added or removed. With this, 'long branches' can play a short path, before returning to a full search at the resulting leaf nodes. Re-using an earlier search path allows the tree to be forward-pruned, which is known to be dangerous, because it removes part of the search process. Additional checks are therefore made and moves can even be re-added when the search result is unsatisfactory. Automatic feature analysis is now central to the algorithm, where key squares and related squares can be generated automatically and used to guide the search process. Using this analysis, if a search result is inferior, it can re-insert un-played moves that cover these key squares only. On the tactical side, a type of move that the forward-pruning will fail on is recognised and a pattern-based solution to that problem is suggested. This has completed the theory of an earlier paper and resulted in a more human-like approach to searching for a chess move. Tests demonstrate that the obvious blunders associated with forward pruning are no longer present and that it can compete at the top level with regard to playing strength.\n    ",
        "submission_date": "2015-03-14T00:00:00",
        "last_modified_date": "2019-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04941",
        "title": "How the symbol grounding of living organisms can be realized in artificial agents",
        "authors": [
            "J.H. van Hateren"
        ],
        "abstract": "A system with artificial intelligence usually relies on symbol manipulation, at least partly and implicitly. However, the interpretation of the symbols - what they represent and what they are about - is ultimately left to humans, as designers and users of the system. How symbols can acquire meaning for the system itself, independent of external interpretation, is an unsolved problem. Some grounding of symbols can be obtained by embodiment, that is, by causally connecting symbols (or sub-symbolic variables) to the physical environment, such as in a robot with sensors and effectors. However, a causal connection as such does not produce representation and aboutness of the kind that symbols have for humans. Here I present a theory that explains how humans and other living organisms have acquired the capability to have symbols and sub-symbolic variables that represent, refer to, and are about something else. The theory shows how reference can be to physical objects, but also to abstract objects, and even how it can be misguided (errors in reference) or be about non-existing objects. I subsequently abstract the primary components of the theory from their biological context, and discuss how and under what conditions the theory could be implemented in artificial agents. A major component of the theory is the strong nonlinearity associated with (potentially unlimited) self-reproduction. The latter is likely not acceptable in artificial systems. It remains unclear if goals other than those inherently serving self-reproduction can have aboutness and if such goals could be stabilized.\n    ",
        "submission_date": "2015-03-17T00:00:00",
        "last_modified_date": "2015-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05055",
        "title": "Combining partially independent belief functions",
        "authors": [
            "Mouna Chebbah",
            "Arnaud Martin",
            "Boutheina Ben Yaghlane"
        ],
        "abstract": "The theory of belief functions manages uncertainty and also proposes a set of combination rules to aggregate opinions of several sources. Some combination rules mix evidential information where sources are independent; other rules are suited to combine evidential information held by dependent sources. In this paper we have two main contributions: First we suggest a method to quantify sources' degree of independence that may guide the choice of the more appropriate set of combination rules. Second, we propose a new combination rule that takes consideration of sources' degree of independence. The proposed method is illustrated on generated mass functions.\n    ",
        "submission_date": "2015-03-17T00:00:00",
        "last_modified_date": "2015-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05113",
        "title": "Quantifying Morphological Computation based on an Information Decomposition of the Sensorimotor Loop",
        "authors": [
            "Keyan Ghazi-Zahedi",
            "Johannes Rauh"
        ],
        "abstract": "The question how an agent is affected by its embodiment has attracted growing attention in recent years. A new field of artificial intelligence has emerged, which is based on the idea that intelligence cannot be understood without taking into account embodiment. We believe that a formal approach to quantifying the embodiment's effect on the agent's behaviour is beneficial to the fields of artificial life and artificial intelligence. The contribution of an agent's body and environment to its behaviour is also known as morphological computation. Therefore, in this work, we propose a quantification of morphological computation, which is based on an information decomposition of the sensorimotor loop into shared, unique and synergistic information. In numerical simulation based on a formal representation of the sensorimotor loop, we show that the unique information of the body and environment is a good measure for morphological computation. The results are compared to our previously derived quantification of morphological computation.\n    ",
        "submission_date": "2015-03-17T00:00:00",
        "last_modified_date": "2015-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05501",
        "title": "Probabilistic Argumentation. An Equational Approach",
        "authors": [
            "D. M. Gabbay",
            "O. Rodrigues"
        ],
        "abstract": "There is a generic way to add any new feature to a system. It involves 1) identifying the basic units which build up the system and 2) introducing the new feature to each of these basic units.\n",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05508",
        "title": "Exploration of the scalability of LocFaults approach for error localization with While-loops programs",
        "authors": [
            "Mohammed Bekkouche"
        ],
        "abstract": "A model checker can produce a trace of counterexample, for an erroneous program, which is often long and difficult to understand. In general, the part about the loops is the largest among the instructions in this trace. This makes the location of errors in loops critical, to analyze errors in the overall program. In this paper, we explore the scala-bility capabilities of LocFaults, our error localization approach exploiting paths of CFG(Control Flow Graph) from a counterexample to calculate the MCDs (Minimal Correction Deviations), and MCSs (Minimal Correction Subsets) from each found MCD. We present the times of our approach on programs with While-loops unfolded b times, and a number of deviated conditions ranging from 0 to n. Our preliminary results show that the times of our approach, constraint-based and flow-driven, are better compared to BugAssist which is based on SAT and transforms the entire program to a Boolean formula, and further the information provided by LocFaults is more expressive for the user.\n    ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05530",
        "title": "Exploration of the scalability of LocFaults",
        "authors": [
            "Mohammed Bekkouche"
        ],
        "abstract": "A model checker can produce a trace of counterexample, for an erroneous program, which is often long and difficult to understand. In general, the part about the loops is the largest among the instructions in this trace. This makes the location of errors in loops critical, to analyze errors in the overall program. In this paper, we explore the scalability capabilities of LocFaults, our error localization approach exploiting paths of CFG(Control Flow Graph) from a counterexample to calculate the MCDs (Minimal Correction Deviations), and MCSs (Minimal Correction Subsets) from each found MCD. We present the times of our approach on programs with While-loops unfolded b times, and a number of deviated conditions ranging from 0 to n. Our preliminary results show that the times of our approach, constraint-based and flow-driven, are better compared to BugAssist which is based on SAT and transforms the entire program to a Boolean formula, and further the information provided by LocFaults is more expressive for the user.\n    ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05667",
        "title": "BitSim: An Algebraic Similarity Measure for Description Logics Concepts",
        "authors": [
            "Sourish Dasgupta",
            "Gaurav Maheshwari",
            "Priyansh Trivedi"
        ],
        "abstract": "In this paper, we propose an algebraic similarity measure {\\sigma}BS (BS stands for BitSim) for assigning semantic similarity score to concept definitions in ALCH+ an expressive fragment of Description Logics (DL). We define an algebraic interpretation function, I_B, that maps a concept definition to a unique string ({\\omega}_B) called bit-code) over an alphabet {\\Sigma}_B of 11 symbols belonging to L_B - the language over P B. IB has semantic correspondence with conventional model-theoretic interpretation of DL. We then define {\\sigma}_BS on L_B. A detailed analysis of I_B and {\\sigma}_BS has been given.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06087",
        "title": "The RatioLog Project: Rational Extensions of Logical Reasoning",
        "authors": [
            "Ulrich Furbach",
            "Claudia Schon",
            "Frieder Stolzenburg",
            "Karl-Heinz Weis",
            "Claus-Peter Wirth"
        ],
        "abstract": "Higher-level cognition includes logical reasoning and the ability of question answering with common sense. The RatioLog project addresses the problem of rational reasoning in deep question answering by methods from automated deduction and cognitive computing. In a first phase, we combine techniques from information retrieval and machine learning to find appropriate answer candidates from the huge amount of text in the German version of the free encyclopedia \"Wikipedia\". In a second phase, an automated theorem prover tries to verify the answer candidates on the basis of their logical representations. In a third phase - because the knowledge may be incomplete and inconsistent -, we consider extensions of logical reasoning to improve the results. In this context, we work toward the application of techniques from human reasoning: We employ defeasible reasoning to compare the answers w.r.t. specificity, deontic logic, normative reasoning, and model construction. Moreover, we use integrated case-based reasoning and machine learning techniques on the basis of the semantic structure of the questions and answer candidates to learn giving the right answers.\n    ",
        "submission_date": "2015-03-20T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06201",
        "title": "Data Science as a New Frontier for Design",
        "authors": [
            "Akin Osman",
            "Kazak\u00e7i Mines"
        ],
        "abstract": "The purpose of this paper is to contribute to the challenge of transferring know-how, theories and methods from design research to the design processes in information science and technologies. More specifically, we shall consider a domain, namely data-science, that is becoming rapidly a globally invested research and development axis with strong imperatives for innovation given the data deluge we are currently facing. We argue that, in order to rise to the data-related challenges that the society is facing, data-science initiatives should ensure a renewal of traditional research methodologies that are still largely based on trial-error processes depending on the talent and insights of a single (or a restricted group of) researchers. It is our claim that design theories and methods can provide, at least to some extent, the much-needed framework. We will use a worldwide data-science challenge organized to study a technical problem in physics, namely the detection of Higgs boson, as a use case to demonstrate some of the ways in which design theory and methods can help in analyzing and shaping the innovation dynamics in such projects.\n    ",
        "submission_date": "2015-03-20T00:00:00",
        "last_modified_date": "2015-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07341",
        "title": "An Experiment on Using Bayesian Networks for Process Mining",
        "authors": [
            "Catarina Moreira"
        ],
        "abstract": "Process mining is a technique that performs an automatic analysis of business processes from a log of events with the promise of understanding how processes are executed in an organisation.\n",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07587",
        "title": "Universal Psychometrics Tasks: difficulty, composition and decomposition",
        "authors": [
            "Jose Hernandez-Orallo"
        ],
        "abstract": "This note revisits the concepts of task and difficulty. The notion of cognitive task and its use for the evaluation of intelligent systems is still replete with issues. The view of tasks as MDP in the context of reinforcement learning has been especially useful for the formalisation of learning tasks. However, this alternate interaction does not accommodate well for some other tasks that are usual in artificial intelligence and, most especially, in animal and human evaluation. In particular, we want to have a more general account of episodes, rewards and responses, and, most especially, the computational complexity of the algorithm behind an agent solving a task. This is crucial for the determination of the difficulty of a task as the (logarithm of the) number of computational steps required to acquire an acceptable policy for the task, which includes the exploration of policies and their verification. We introduce a notion of asynchronous-time stochastic tasks. Based on this interpretation, we can see what task difficulty is, what instance difficulty is (relative to a task) and also what task compositions and decompositions are.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07609",
        "title": "An Evolutionary Algorithm for Error-Driven Learning via Reinforcement",
        "authors": [
            "Yanping Liu",
            "Erik D. Reichle"
        ],
        "abstract": "Although different learning systems are coordinated to afford complex behavior, little is known about how this occurs. This article describes a theoretical framework that specifies how complex behaviors that might be thought to require error-driven learning might instead be acquired through simple reinforcement. This framework includes specific assumptions about the mechanisms that contribute to the evolution of (artificial) neural networks to generate topologies that allow the networks to learn large-scale complex problems using only information about the quality of their performance. The practical and theoretical implications of the framework are discussed, as are possible biological analogs of the approach.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07715",
        "title": "The Computational Theory of Intelligence: Data Aggregation",
        "authors": [
            "Daniel Kovach"
        ],
        "abstract": "In this paper, we will expound upon the concepts proffered in [1], where we proposed an information theoretic approach to intelligence in the computational sense. We will examine data and meme aggregation, and study the effect of limited resources on the resulting meme amplitudes.\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2014-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07845",
        "title": "Averaged Hausdorff Approximations of Pareto Fronts based on Multiobjective Estimation of Distribution Algorithms",
        "authors": [
            "Luis Marti",
            "Christian Grimme",
            "Pascal Kerschke",
            "Heike Trautmann",
            "G\u00fcnter Rudolph"
        ],
        "abstract": "In the a posteriori approach of multiobjective optimization the Pareto front is approximated by a finite set of solutions in the objective space. The quality of the approximation can be measured by different indicators that take into account the approximation's closeness to the Pareto front and its distribution along the Pareto front. In particular, the averaged Hausdorff indicator prefers an almost uniform distribution. An observed drawback of multiobjective estimation of distribution algorithms (MEDAs) is that - as common for randomized metaheuristics - the final population usually is not uniformly distributed along the Pareto front. Therefore, we propose a postprocessing strategy which consists of applying the averaged Hausdorff indicator to the complete archive of generated solutions after optimization in order to select a uniformly distributed subset of nondominated solutions from the archive. In this paper, we put forward a strategy for extracting the above described subset. The effectiveness of the proposal is contrasted in a series of experiments that involve different MEDAs and filtering techniques.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08155",
        "title": "Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories",
        "authors": [
            "Miao Fan",
            "Qiang Zhou",
            "Thomas Fang Zheng"
        ],
        "abstract": "This paper considers the problem of knowledge inference on large-scale imperfect repositories with incomplete coverage by means of embedding entities and relations at the first attempt. We propose IIKE (Imperfect and Incomplete Knowledge Embedding), a probabilistic model which measures the probability of each belief, i.e. $\\langle h,r,t\\rangle$, in large-scale knowledge bases such as NELL and Freebase, and our objective is to learn a better low-dimensional vector representation for each entity ($h$ and $t$) and relation ($r$) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning (NELL) or crowdsouring (Freebase), so that we can use $||{\\bf h} + {\\bf r} - {\\bf t}||$ to assess the plausibility of a belief when conducting inference. We use subsets of those inexact knowledge bases to train our model and test the performances of link prediction and triplet classification on ground truth beliefs, respectively. The results of extensive experiments show that IIKE achieves significant improvement compared with the baseline and state-of-the-art approaches.\n    ",
        "submission_date": "2015-03-27T00:00:00",
        "last_modified_date": "2015-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08275",
        "title": "Energy Management in Storage-Augmented, Grid-Connected Prosumer Buildings and Neighbourhoods Using a Modified Simulated Annealing Optimization",
        "authors": [
            "Rosemarie Velik",
            "Pascal Nicolay"
        ],
        "abstract": "This article introduces a modified simulated annealing optimization approach for automatically determining optimal energy management strategies in grid-connected, storage-augmented, photovoltaics-supplied prosumer buildings and neighbourhoods based on user-specific goals. For evaluating the modified simulated annealing optimizer, a number of test scenarios in the field of energy self-consumption maximization are defined and results are compared to a gradient descent and a total state space search approach. The benchmarking against these two reference methods demonstrates that the modified simulated annealing approach is able to find significantly better solutions than the gradient descent algorithm - being equal or very close to the global optimum - with significantly less computational effort and processing time than the total state space search approach.\n    ",
        "submission_date": "2015-03-28T00:00:00",
        "last_modified_date": "2015-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08289",
        "title": "Recent advances on inconsistency indices for pairwise comparisons - a commentary",
        "authors": [
            "Matteo Brunelli"
        ],
        "abstract": "This paper recalls the definition of consistency for pairwise comparison matrices and briefly presents the concept of inconsistency index in connection to other aspects of the theory of pairwise comparisons. By commenting on a recent contribution by Koczkodaj and Szwarc, it will be shown that the discussion on inconsistency indices is far from being over, and the ground is still fertile for debates.\n    ",
        "submission_date": "2015-03-28T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08345",
        "title": "Implementing an intelligent version of the classical sliding-puzzle game for unix terminals using Golang's concurrency primitives",
        "authors": [
            "Pravendra Singh"
        ],
        "abstract": "An intelligent version of the sliding-puzzle game is developed using the new Go programming language, which uses a concurrent version of the A* Informed Search Algorithm to power solver-bot that runs in the background. The game runs in computer system's terminals. Mainly, it was developed for UNIX-type systems but it works pretty well in nearly all the operating systems because of cross-platform compatibility of the programming language used. The game uses language's concurrency primitives to simplify most of the hefty parts of the game. A real-time notification delivery architecture is developed using language's built-in concurrency support, which performs similar to event based context aware invocations like we see on the web platform.\n    ",
        "submission_date": "2015-03-28T00:00:00",
        "last_modified_date": "2015-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.09137",
        "title": "Formalising Hypothesis Virtues in Knowledge Graphs: A General Theoretical Framework and its Validation in Literature-Based Discovery Experiments",
        "authors": [
            "Vit Novacek"
        ],
        "abstract": "We introduce an approach to discovery informatics that uses so called knowledge graphs as the essential representation structure. Knowledge graph is an umbrella term that subsumes various approaches to tractable representation of large volumes of loosely structured knowledge in a graph form. It has been used primarily in the Web and Linked Open Data contexts, but is applicable to any other area dealing with knowledge representation. In the perspective of our approach motivated by the challenges of discovery informatics, knowledge graphs correspond to hypotheses. We present a framework for formalising so called hypothesis virtues within knowledge graphs. The framework is based on a classic work in philosophy of science, and naturally progresses from mostly informative foundational notions to actionable specifications of measures corresponding to particular virtues. These measures can consequently be used to determine refined sub-sets of knowledge graphs that have large relative potential for making discoveries. We validate the proposed framework by experiments in literature-based discovery. The experiments have demonstrated the utility of our work and its superiority w.r.t. related approaches.\n    ",
        "submission_date": "2015-03-31T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00136",
        "title": "Knowledge reduction of dynamic covering decision information systems with immigration of more objects",
        "authors": [
            "Guangming Lang"
        ],
        "abstract": "In practical situations, it is of interest to investigate computing approximations of sets as an important step of knowledge reduction of dynamic covering decision information systems. In this paper, we present incremental approaches to computing the type-1 and type-2 characteristic matrices of dynamic coverings whose cardinalities increase with immigration of more objects. We also present the incremental algorithms of computing the second and sixth lower and upper approximations of sets in dynamic covering approximation spaces.\n    ",
        "submission_date": "2015-04-01T00:00:00",
        "last_modified_date": "2015-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00854",
        "title": "Evaluation Evaluation a Monte Carlo study",
        "authors": [
            "David M. W. Powers"
        ],
        "abstract": "Over the last decade there has been increasing concern about the biases embodied in traditional evaluation methods for Natural Language Processing/Learning, particularly methods borrowed from Information Retrieval. Without knowledge of the Bias and Prevalence of the contingency being tested, or equivalently the expectation due to chance, the simple conditional probabilities Recall, Precision and Accuracy are not meaningful as evaluation measures, either individually or in combinations such as F-factor. The existence of bias in NLP measures leads to the 'improvement' of systems by increasing their bias, such as the practice of improving tagging and parsing scores by using most common value (e.g. water is always a Noun) rather than the attempting to discover the correct one. The measures Cohen Kappa and Powers Informedness are discussed as unbiased alternative to Recall and related to the psychologically significant measure DeltaP. In this paper we will analyze both biased and unbiased measures theoretically, characterizing the precise relationship between all these measures as well as evaluating the evaluation measures themselves empirically using a Monte Carlo simulation.\n    ",
        "submission_date": "2015-04-03T00:00:00",
        "last_modified_date": "2015-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01004",
        "title": "Managing Multi-Granular Linguistic Distribution Assessments in Large-Scale Multi-Attribute Group Decision Making",
        "authors": [
            "Zhen Zhang",
            "Chonghui Guo",
            "Luis Mart\u00ednez"
        ],
        "abstract": "Linguistic large-scale group decision making (LGDM) problems are more and more common nowadays. In such problems a large group of decision makers are involved in the decision process and elicit linguistic information that are usually assessed in different linguistic scales with diverse granularity because of decision makers' distinct knowledge and background. To keep maximum information in initial stages of the linguistic LGDM problems, the use of multi-granular linguistic distribution assessments seems a suitable choice, however to manage such multigranular linguistic distribution assessments, it is necessary the development of a new linguistic computational approach. In this paper it is proposed a novel computational model based on the use of extended linguistic hierarchies, which not only can be used to operate with multi-granular linguistic distribution assessments, but also can provide interpretable linguistic results to decision makers. Based on this new linguistic computational model, an approach to linguistic large-scale multi-attribute group decision making is proposed and applied to a talent selection process in universities.\n    ",
        "submission_date": "2015-04-04T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01173",
        "title": "Dual Decomposition from the Perspective of Relax, Compensate and then Recover",
        "authors": [
            "Arthur Choi",
            "Adnan Darwiche"
        ],
        "abstract": "Relax, Compensate and then Recover (RCR) is a paradigm for approximate inference in probabilistic graphical models that has previously provided theoretical and practical insights on iterative belief propagation and some of its generalizations. In this paper, we characterize the technique of dual decomposition in the terms of RCR, viewing it as a specific way to compensate for relaxed equivalence constraints. Among other insights gathered from this perspective, we propose novel heuristics for recovering relaxed equivalence constraints with the goal of incrementally tightening dual decomposition approximations, all the way to reaching exact solutions. We also show empirically that recovering equivalence constraints can sometimes tighten the corresponding approximation (and obtaining exact results), without increasing much the complexity of inference.\n    ",
        "submission_date": "2015-04-05T00:00:00",
        "last_modified_date": "2015-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01684",
        "title": "Large Margin Nearest Neighbor Embedding for Knowledge Representation",
        "authors": [
            "Miao Fan",
            "Qiang Zhou",
            "Thomas Fang Zheng",
            "Ralph Grishman"
        ],
        "abstract": "Traditional way of storing facts in triplets ({\\it head\\_entity, relation, tail\\_entity}), abbreviated as ({\\it h, r, t}), makes the knowledge intuitively displayed and easily acquired by mankind, but hardly computed or even reasoned by AI machines. Inspired by the success in applying {\\it Distributed Representations} to AI-related fields, recent studies expect to represent each entity and relation with a unique low-dimensional embedding, which is different from the symbolic and atomic framework of displaying knowledge in triplets. In this way, the knowledge computing and reasoning can be essentially facilitated by means of a simple {\\it vector calculation}, i.e. ${\\bf h} + {\\bf r} \\approx {\\bf t}$. We thus contribute an effective model to learn better embeddings satisfying the formula by pulling the positive tail entities ${\\bf t^{+}}$ to get together and close to {\\bf h} + {\\bf r} ({\\it Nearest Neighbor}), and simultaneously pushing the negatives ${\\bf t^{-}}$ away from the positives ${\\bf t^{+}}$ via keeping a {\\it Large Margin}. We also design a corresponding learning algorithm to efficiently find the optimal solution based on {\\it Stochastic Gradient Descent} in iterative fashion. Quantitative experiments illustrate that our approach can achieve the state-of-the-art performance, compared with several latest methods on some benchmark datasets for two classical applications, i.e. {\\it Link prediction} and {\\it Triplet classification}. Moreover, we analyze the parameter complexities among all the evaluated models, and analytical results indicate that our model needs fewer computational resources on outperforming the other methods.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02027",
        "title": "The Neutrosophic Entropy and its Five Components",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "This paper presents two variants of penta-valued representation for neutrosophic entropy. The first is an extension of Kaufmann's formula and the second is an extension of Kosko's formula.\n",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02059",
        "title": "Supporting Language Learners with the Meanings Of Closed Class Items",
        "authors": [
            "Hayat Alrefaie",
            "Allan Ramsay"
        ],
        "abstract": "The process of language learning involves the mastery of countless tasks: making the constituent sounds of the language being learned, learning the grammatical patterns, and acquiring the requisite vocabulary for reception and production. While a plethora of computational tools exist to facilitate the first and second of these tasks, a number of challenges arise with respect to enabling the third. This paper describes a tool that has been designed to support language learners with the challenge of understanding the use of closed-class lexical items. The process of learning the Arabic for office is (mktb) is relatively simple and should be possible by means of simple repetition of the word. However, it is much more difficult to learn and correctly use the Arabic equivalent of the word on. The current paper describes a mechanism for the delivery of diagnostic information regarding specific lexical examples, with the aim of clearly demonstrating why a particular translation of a given closed-class item may be appropriate in certain situations but not others, thereby helping learners to understand and use the term correctly.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02247",
        "title": "Projective simulation with generalization",
        "authors": [
            "Alexey A. Melnikov",
            "Adi Makmal",
            "Vedran Dunjko",
            "Hans J. Briegel"
        ],
        "abstract": "The ability to generalize is an important feature of any intelligent agent. Not only because it may allow the agent to cope with large amounts of data, but also because in some environments, an agent with no generalization capabilities cannot learn. In this work we outline several criteria for generalization, and present a dynamic and autonomous machinery that enables projective simulation agents to meaningfully generalize. Projective simulation, a novel, physical approach to artificial intelligence, was recently shown to perform well in standard reinforcement learning problems, with applications in advanced robotics as well as quantum experiments. Both the basic projective simulation model and the presented generalization machinery are based on very simple principles. This allows us to provide a full analytical analysis of the agent's performance and to illustrate the benefit the agent gains by generalizing. Specifically, we show that already in basic (but extreme) environments, learning without generalization may be impossible, and demonstrate how the presented generalization machinery enables the projective simulation agent to learn.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2017-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02255",
        "title": "On mining complex sequential data by means of FCA and pattern structures",
        "authors": [
            "Aleksey Buzmakov",
            "Elias Egho",
            "Nicolas Jay",
            "Sergei O. Kuznetsov",
            "Amedeo Napoli",
            "Chedy Ra\u00efssi"
        ],
        "abstract": "Nowadays data sets are available in very complex and heterogeneous ways. Mining of such data collections is essential to support many real-world applications ranging from healthcare to marketing. In this work, we focus on the analysis of \"complex\" sequential data by means of interesting sequential patterns. We approach the problem using the elegant mathematical framework of Formal Concept Analysis (FCA) and its extension based on \"pattern structures\". Pattern structures are used for mining complex data (such as sequences or graphs) and are based on a subsumption operation, which in our case is defined with respect to the partial order on sequences. We show how pattern structures along with projections (i.e., a data reduction of sequential structures), are able to enumerate more meaningful patterns and increase the computing efficiency of the approach. Finally, we show the applicability of the presented method for discovering and analyzing interesting patient patterns from a French healthcare data set on cancer. The quantitative and qualitative results (with annotations and analysis from a physician) are reported in this use case which is the main motivation for this work.\n",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02281",
        "title": "An Optimized Hybrid Approach for Path Finding",
        "authors": [
            "Ahlam Ansari",
            "Mohd Amin Sayyed",
            "Khatija Ratlamwala",
            "Parvin Shaikh"
        ],
        "abstract": "Path finding algorithm addresses problem of finding shortest path from source to destination avoiding obstacles. There exist various search algorithms namely A*, Dijkstra's and ant colony optimization. Unlike most path finding algorithms which require destination co-ordinates to compute path, the proposed algorithm comprises of a new method which finds path using backtracking without requiring destination co-ordinates. Moreover, in existing path finding algorithm, the number of iterations required to find path is large. Hence, to overcome this, an algorithm is proposed which reduces number of iterations required to traverse the path. The proposed algorithm is hybrid of backtracking and a new technique(modified 8- neighbor approach). The proposed algorithm can become essential part in location based, network, gaming applications. grid traversal, navigation, gaming applications, mobile robot and Artificial Intelligence.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02358",
        "title": "RDF annotation of Second Life objects: Knowledge Representation meets Social Virtual reality",
        "authors": [
            "Carlo Bernava",
            "Giacomo Fiumara",
            "Dario Maggiorini",
            "Alessandro Provetti",
            "Laura Ripamonti"
        ],
        "abstract": "We have designed and implemented an application running inside Second Life that supports user annotation of graphical objects and graphical visualization of concept ontologies, thus providing a formal, machine-accessible description of objects. As a result, we offer a platform that combines the graphical knowledge representation that is expected from a MUVE artifact with the semantic structure given by the Resource Framework Description (RDF) representation of information.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02878",
        "title": "Data Science and Ebola",
        "authors": [
            "Aske Plaat"
        ],
        "abstract": "Data Science---Today, everybody and everything produces data. People produce large amounts of data in social networks and in commercial transactions. Medical, corporate, and government databases continue to grow. Sensors continue to get cheaper and are increasingly connected, creating an Internet of Things, and generating even more data. In every discipline, large, diverse, and rich data sets are emerging, from astrophysics, to the life sciences, to the behavioral sciences, to finance and commerce, to the humanities and to the arts. In every discipline people want to organize, analyze, optimize and understand their data to answer questions and to deepen insights. The science that is transforming this ocean of data into a sea of knowledge is called data science. This lecture will discuss how data science has changed the way in which one of the most visible challenges to public health is handled, the 2014 Ebola outbreak in West Africa.\n    ",
        "submission_date": "2015-04-11T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02882",
        "title": "Quantitative Analysis of Whether Machine Intelligence Can Surpass Human Intelligence",
        "authors": [
            "Feng Liu",
            "Yong Shi"
        ],
        "abstract": "Whether the machine intelligence can surpass the human intelligence is a controversial issue. On the basis of traditional IQ, this article presents the Universal IQ test method suitable for both the machine intelligence and the human intelligence. With the method, machine and human intelligences were divided into 4 major categories and 15 subcategories. A total of 50 search engines across the world and 150 persons at different ages were subject to the relevant test. And then, the Universal IQ ranking list of 2014 for the test objects was obtained.\n    ",
        "submission_date": "2015-04-11T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03303",
        "title": "Ultimate Intelligence Part II: Physical Measure and Complexity of Intelligence",
        "authors": [
            "Eray \u00d6zkural"
        ],
        "abstract": "We continue our analysis of volume and energy measures that are appropriate for quantifying inductive inference systems. We extend logical depth and conceptual jump size measures in AIT to stochastic problems, and physical measures that involve volume and energy. We introduce a graphical model of computational complexity that we believe to be appropriate for intelligent machines. We show several asymptotic relations between energy, logical depth and volume of computation for inductive inference. In particular, we arrive at a \"black-hole equation\" of inductive inference, which relates energy, volume, space, and algorithmic information for an optimal inductive inference solution. We introduce energy-bounded algorithmic entropy. We briefly apply our ideas to the physical limits of intelligent computation in our universe.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03451",
        "title": "Harnessing Natural Fluctuations: Analogue Computer for Efficient Socially Maximal Decision Making",
        "authors": [
            "Song-Ju Kim",
            "Makoto Naruse",
            "Masashi Aono"
        ],
        "abstract": "Each individual handles many tasks of finding the most profitable option from a set of options that stochastically provide rewards. Our society comprises a collection of such individuals, and the society is expected to maximise the total rewards, while the individuals compete for common rewards. Such collective decision making is formulated as the `competitive multi-armed bandit problem (CBP)', requiring a huge computational cost. Herein, we demonstrate a prototype of an analog computer that efficiently solves CBPs by exploiting the physical dynamics of numerous fluids in coupled cylinders. This device enables the maximisation of the total rewards for the society without paying the conventionally required computational cost; this is because the fluids estimate the reward probabilities of the options for the exploitation of past knowledge and generate random fluctuations for the exploration of new knowledge. Our results suggest that to optimise the social rewards, the utilisation of fluid-derived natural fluctuations is more advantageous than applying artificial external fluctuations. Our analog computing scheme is expected to trigger further studies for harnessing the huge computational power of natural phenomena for resolving a wide variety of complex problems in modern information society.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03558",
        "title": "Fuzzy approaches to context variable in fuzzy geographically weighted clustering",
        "authors": [
            "Nguyen Van Minh",
            "Le Hoang Son"
        ],
        "abstract": "Fuzzy Geographically Weighted Clustering (FGWC) is considered as a suitable tool for the analysis of geo-demographic data that assists the provision and planning of products and services to local people. Context variables were attached to FGWC in order to accelerate the computing speed of the algorithm and to focus the results on the domain of interests. Nonetheless, the determination of exact, crisp values of the context variable is a hard task. In this paper, we propose two novel methods using fuzzy approaches for that determination. A numerical example is given to illustrate the uses of the proposed methods.\n    ",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03592",
        "title": "Towards Verifiably Ethical Robot Behaviour",
        "authors": [
            "Louise A. Dennis",
            "Michael Fisher",
            "Alan F. T. Winfield"
        ],
        "abstract": "Ensuring that autonomous systems work ethically is both complex and difficult. However, the idea of having an additional `governor' that assesses options the system has, and prunes them to select the most ethical choices is well understood. Recent work has produced such a governor consisting of a `consequence engine' that assesses the likely future outcomes of actions then applies a Safety/Ethical logic to select actions. Although this is appealing, it is impossible to be certain that the most ethical options are actually taken. In this paper we extend and apply a well-known agent verification approach to our consequence engine, allowing us to verify the correctness of its ethical decision-making.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03874",
        "title": "Bridging belief function theory to modern machine learning",
        "authors": [
            "Thomas Burger"
        ],
        "abstract": "Machine learning is a quickly evolving field which now looks really different from what it was 15 years ago, when classification and clustering were major issues. This document proposes several trends to explore the new questions of modern machine learning, with the strong afterthought that the belief function framework has a major role to play.\n    ",
        "submission_date": "2015-04-15T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04802",
        "title": "Gradual Classical Logic for Attributed Objects - Extended in Re-Presentation",
        "authors": [
            "Ryuta Arisaka"
        ],
        "abstract": "Our understanding about things is conceptual. By stating that we reason about objects, it is in fact not the objects but concepts referring to them that we manipulate. Now, so long just as we acknowledge infinitely extending notions such as space, time, size, colour, etc, - in short, any reasonable quality - into which an object is subjected, it becomes infeasible to affirm atomicity in the concept referring to the object. However, formal/symbolic logics typically presume atomic entities upon which other expressions are built. Can we reflect our intuition about the concept onto formal/symbolic logics at all? I assure that we can, but the usual perspective about the atomicity needs inspected. In this work, I present gradual logic which materialises the observation that we cannot tell apart whether a so-regarded atomic entity is atomic or is just atomic enough not to be considered non-atomic. The motivation is to capture certain phenomena that naturally occur around concepts with attributes, including presupposition and contraries. I present logical particulars of the logic, which is then mapped onto formal semantics. Two linguistically interesting semantics will be considered. Decidability is shown.\n    ",
        "submission_date": "2015-04-19T00:00:00",
        "last_modified_date": "2015-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04909",
        "title": "Illuminating search spaces by mapping elites",
        "authors": [
            "Jean-Baptiste Mouret",
            "Jeff Clune"
        ],
        "abstract": "Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05095",
        "title": "Hybrid Genetic Algorithm and Lasso Test Approach for Inferring Well Supported Phylogenetic Trees based on Subsets of Chloroplastic Core Genes",
        "authors": [
            "Bassam AlKindy",
            "Christophe Guyeux",
            "Jean-Fran\u00e7ois Couchot",
            "Michel Salomon",
            "Christian Parisod",
            "Jacques M. Bahi"
        ],
        "abstract": "The amount of completely sequenced chloroplast genomes increases rapidly every day, leading to the possibility to build large scale phylogenetic trees of plant species. Considering a subset of close plant species defined according to their chloroplasts, the phylogenetic tree that can be inferred by their core genes is not necessarily well supported, due to the possible occurrence of \"problematic\" genes (i.e., homoplasy, incomplete lineage sorting, horizontal gene transfers, etc.) which may blur phylogenetic signal. However, a trustworthy phylogenetic tree can still be obtained if the number of problematic genes is low, the problem being to determine the largest subset of core genes that produces the best supported tree. To discard problematic genes and due to the overwhelming number of possible combinations, we propose an hybrid approach that embeds both genetic algorithms and statistical tests. Given a set of organisms, the result is a pipeline of many stages for the production of well supported phylogenetic trees. The proposal has been applied to different cases of plant families, leading to encouraging results for these families.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05150",
        "title": "Computing Horn Rewritings of Description Logics Ontologies",
        "authors": [
            "Mark Kaminski",
            "Bernardo Cuenca Grau"
        ],
        "abstract": "We study the problem of rewriting an ontology O1 expressed in a DL L1 into an ontology O2 in a Horn DL L2 such that O1 and O2 are equisatisfiable when extended with an arbitrary dataset. Ontologies that admit such rewritings are amenable to reasoning techniques ensuring tractability in data complexity. After showing undecidability whenever L1 extends ALCF, we focus on devising efficiently checkable conditions that ensure existence of a Horn rewriting. By lifting existing techniques for rewriting Disjunctive Datalog programs into plain Datalog to the case of arbitrary first-order programs with function symbols, we identify a class of ontologies that admit Horn rewritings of polynomial size. Our experiments indicate that many real-world ontologies satisfy our sufficient conditions and thus admit polynomial Horn rewritings.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05381",
        "title": "How do you revise your belief set with %$;@*?",
        "authors": [
            "Ryuta Arisaka"
        ],
        "abstract": "In the classic AGM belief revision theory, beliefs are static and do not change their own shape. For instance, if p is accepted by a rational agent, it will remain p to the agent. But such rarely happens to us. Often, when we accept some information p, what is actually accepted is not the whole p, but only a portion of it; not necessarily because we select the portion but because p must be perceived. Only the perceived p is accepted; and the perception is subject to what we already believe (know). What may, however, happen to the rest of p that initially escaped our attention? In this work we argue that the invisible part is also accepted to the agent, if only unconsciously. Hence some parts of p are accepted as visible, while some other parts as latent, beliefs. The division is not static. As the set of beliefs changes, what were hidden may become visible. We present a perception-based belief theory that incorporates latent beliefs.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05411",
        "title": "Reasoning about Unmodelled Concepts - Incorporating Class Taxonomies in Probabilistic Relational Models",
        "authors": [
            "Daniel Nyga",
            "Michael Beetz"
        ],
        "abstract": "A key problem in the application of first-order probabilistic methods is the enormous size of graphical models they imply. The size results from the possible worlds that can be generated by a domain of objects and relations. One of the reasons for this explosion is that so far the approaches do not sufficiently exploit the structure and similarity of possible worlds in order to encode the models more compactly. We propose fuzzy inference in Markov logic networks, which enables the use of taxonomic knowledge as a source of imposing structure onto possible worlds. We show that by exploiting this structure, probability distributions can be represented more compactly and that the reasoning systems become capable of reasoning about concepts not contained in the probabilistic knowledge base.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05651",
        "title": "Distinguishing Cause from Effect Based on Exogeneity",
        "authors": [
            "Kun Zhang",
            "Jiji Zhang",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "Recent developments in structural equation modeling have produced several methods that can usually distinguish cause from effect in the two-variable case. For that purpose, however, one has to impose substantial structural constraints or smoothness assumptions on the functional causal models. In this paper, we consider the problem of determining the causal direction from a related but different point of view, and propose a new framework for causal direction determination. We show that it is possible to perform causal inference based on the condition that the cause is \"exogenous\" for the parameters involved in the generating process from the cause to the effect. In this way, we avoid the structural constraints required by the SEM-based approaches. In particular, we exploit nonparametric methods to estimate marginal and conditional distributions, and propose a bootstrap-based approach to test for the exogeneity condition; the testing results indicate the causal direction between two variables. The proposed method is validated on both synthetic and real data.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05696",
        "title": "Ascribing Consciousness to Artificial Intelligence",
        "authors": [
            "Murray Shanahan"
        ],
        "abstract": "This paper critically assesses the anti-functionalist stance on consciousness adopted by certain advocates of integrated information theory (IIT), a corollary of which is that human-level artificial intelligence implemented on conventional computing hardware is necessarily not conscious. The critique draws on variations of a well-known gradual neuronal replacement thought experiment, as well as bringing out tensions in IIT's treatment of self-knowledge. The aim, though, is neither to reject IIT outright nor to champion functionalism in particular. Rather, it is suggested that both ideas have something to offer a scientific understanding of consciousness, as long as they are not dressed up as solutions to illusory metaphysical problems. As for human-level AI, we must await its development before we can decide whether or not to ascribe consciousness to it.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05846",
        "title": "Generalized Support and Formal Development of Constraint Propagators",
        "authors": [
            "James Caldwell",
            "Ian P. Gent",
            "Peter Nightingale"
        ],
        "abstract": "Constraint programming is a family of techniques for solving combinatorial problems, where the problem is modelled as a set of decision variables (typically with finite domains) and a set of constraints that express relations among the decision variables. One key concept in constraint programming is propagation: reasoning on a constraint or set of constraints to derive new facts, typically to remove values from the domains of decision variables. Specialised propagation algorithms (propagators) exist for many classes of constraints.\n",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2016-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05895",
        "title": "Semantic Enrichment of Mobile Phone Data Records Using Background Knowledge",
        "authors": [
            "Zolzaya Dashdorj",
            "Stanislav Sobolevsky",
            "Luciano Serafini",
            "Fabrizio Antonelli",
            "Carlo Ratti"
        ],
        "abstract": "Every day, billions of mobile network events (i.e. CDRs) are generated by cellular phone operator companies. Latent in this data are inspiring insights about human actions and behaviors, the discovery of which is important because context-aware applications and services hold the key to user-driven, intelligent services, which can enhance our everyday lives such as social and economic development, urban planning, and health prevention. The major challenge in this area is that interpreting such a big stream of data requires a deep understanding of mobile network events' context through available background knowledge. This article addresses the issues in context awareness given heterogeneous and uncertain data of mobile network events missing reliable information on the context of this activity. The contribution of this research is a model from a combination of logical and statistical reasoning standpoints for enabling human activity inference in qualitative terms from open geographical data that aimed at improving the quality of human behaviors recognition tasks from CDRs. We use open geographical data, Openstreetmap (OSM), as a proxy for predicting the content of human activity in the area. The user study performed in Trento shows that predicted human activities (top level) match the survey data with around 93% overall accuracy. The extensive validation for predicting a more specific economic type of human activity performed in Barcelona, by employing credit card transaction data. The analysis identifies that appropriately normalized data on points of interest (POI) is a good proxy for predicting human economical activities, with 84% accuracy on average. So the model is proven to be efficient for predicting the context of human activity, when its total level could be efficiently observed from cell phone data records, missing contextual information however.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06366",
        "title": "Use of Ensembles of Fourier Spectra in Capturing Recurrent Concepts in Data Streams",
        "authors": [
            "Sripirakas Sakthithasan",
            "Russel Pears",
            "Albert Bifet",
            "Bernhard Pfahringer"
        ],
        "abstract": "In this research, we apply ensembles of Fourier encoded spectra to capture and mine recurring concepts in a data stream environment. Previous research showed that compact versions of Decision Trees can be obtained by applying the Discrete Fourier Transform to accurately capture recurrent concepts in a data stream. However, in highly volatile environments where new concepts emerge often, the approach of encoding each concept in a separate spectrum is no longer viable due to memory overload and thus in this research we present an ensemble approach that addresses this problem. Our empirical results on real world data and synthetic data exhibiting varying degrees of recurrence reveal that the ensemble approach outperforms the single spectrum approach in terms of classification accuracy, memory and execution time.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06374",
        "title": "Logical Conditional Preference Theories",
        "authors": [
            "Cristina Cornelio",
            "Andrea Loreggia",
            "Vijay Saraswat"
        ],
        "abstract": "CP-nets represent the dominant existing framework for expressing qualitative conditional preferences between alternatives, and are used in a variety of areas including constraint solving. Over the last fifteen years, a significant literature has developed exploring semantics, algorithms, implementation and use of CP-nets. This paper introduces a comprehensive new framework for conditional preferences: logical conditional preference theories (LCP theories). To express preferences, the user specifies arbitrary (constraint) Datalog programs over a binary ordering relation on outcomes. We show how LCP theories unify and generalize existing conditional preference proposals, and leverage the rich semantic, algorithmic and implementation frameworks of Datalog.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06423",
        "title": "Information Gathering in Networks via Active Exploration",
        "authors": [
            "Adish Singla",
            "Eric Horvitz",
            "Pushmeet Kohli",
            "Ryen White",
            "Andreas Krause"
        ],
        "abstract": "How should we gather information in a network, where each node's visibility is limited to its local neighborhood? This problem arises in numerous real-world applications, such as surveying and task routing in social networks, team formation in collaborative networks and experimental design with dependency constraints. Often the informativeness of a set of nodes can be quantified via a submodular utility function. Existing approaches for submodular optimization, however, require that the set of all nodes that can be selected is known ahead of time, which is often unrealistic. In contrast, we propose a novel model where we start our exploration from an initial node, and new nodes become visible and available for selection only once one of their neighbors has been chosen. We then present a general algorithm NetExp for this problem, and provide theoretical bounds on its performance dependent on structural properties of the underlying network. We evaluate our methodology on various simulated problem instances as well as on data collected from social question answering system deployed within a large enterprise.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06529",
        "title": "Controlled Query Evaluation for Datalog and OWL 2 Profile Ontologies",
        "authors": [
            "Bernardo Cuenca Grau",
            "Evgeny Kharlamov",
            "Egor V. Kostylev",
            "Dmitriy Zheleznyakov"
        ],
        "abstract": "We study confidentiality enforcement in ontologies under the Controlled Query Evaluation framework, where a policy specifies the sensitive information and a censor ensures that query answers that may compromise the policy are not returned. We focus on censors that ensure confidentiality while maximising information access, and consider both Datalog and the OWL 2 profiles as ontology languages.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06700",
        "title": "Preferential Multi-Context Systems",
        "authors": [
            "Kedian Mu",
            "Kewen Wang",
            "Lian Wen"
        ],
        "abstract": "Multi-context systems (MCS) presented by Brewka and Eiter can be considered as a promising way to interlink decentralized and heterogeneous knowledge contexts. In this paper, we propose preferential multi-context systems (PMCS), which provide a framework for incorporating a total preorder relation over contexts in a multi-context system. In a given PMCS, its contexts are divided into several parts according to the total preorder relation over them, moreover, only information flows from a context to ones of the same part or less preferred parts are allowed to occur. As such, the first $l$ preferred parts of an PMCS always fully capture the information exchange between contexts of these parts, and then compose another meaningful PMCS, termed the $l$-section of that PMCS. We generalize the equilibrium semantics for an MCS to the (maximal) $l_{\\leq}$-equilibrium which represents belief states at least acceptable for the $l$-section of an PMCS. We also investigate inconsistency analysis in PMCS and related computational complexity issues.\n    ",
        "submission_date": "2015-04-25T00:00:00",
        "last_modified_date": "2015-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06848",
        "title": "Maximum a Posteriori Estimation by Search in Probabilistic Programs",
        "authors": [
            "David Tolpin",
            "Frank Wood"
        ],
        "abstract": "We introduce an approximate search algorithm for fast maximum a posteriori probability estimation in probabilistic programs, which we call Bayesian ascent Monte Carlo (BaMC). Probabilistic programs represent probabilistic models with varying number of mutually dependent finite, countable, and continuous random variables. BaMC is an anytime MAP search algorithm applicable to any combination of random variables and dependencies. We compare BaMC to other MAP estimation algorithms and show that BaMC is faster and more robust on a range of probabilistic models.\n    ",
        "submission_date": "2015-04-26T00:00:00",
        "last_modified_date": "2015-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06936",
        "title": "Concept Extraction to Identify Adverse Drug Reactions in Medical Forums: A Comparison of Algorithms",
        "authors": [
            "Alejandro Metke-Jimenez",
            "Sarvnaz Karimi"
        ],
        "abstract": "Social media is becoming an increasingly important source of information to complement traditional pharmacovigilance methods. In order to identify signals of potential adverse drug reactions, it is necessary to first identify medical concepts in the social media text. Most of the existing studies use dictionary-based methods which are not evaluated independently from the overall signal detection task.\n",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07020",
        "title": "Theory of Semi-Instantiation in Abstract Argumentation",
        "authors": [
            "D. M. Gabbay"
        ],
        "abstract": "We study instantiated abstract argumentation frames of the form $(S,R,I)$, where $(S,R)$ is an abstract argumentation frame and where the arguments $x$ of $S$ are instantiated by $I(x)$ as well formed formulas of a well known logic, for example as Boolean formulas or as predicate logic formulas or as modal logic formulas. We use the method of conceptual analysis to derive the properties of our proposed system. We seek to define the notion of complete extensions for such systems and provide algorithms for finding such extensions. We further develop a theory of instantiation in the abstract, using the framework of Boolean attack formations and of conjunctive and disjunctive attacks. We discuss applications and compare critically with the existing related literature.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07168",
        "title": "Further Connections Between Contract-Scheduling and Ray-Searching Problems",
        "authors": [
            "Spyros Angelopoulos"
        ],
        "abstract": "This paper addresses two classes of different, yet interrelated optimization problems. The first class of problems involves a robot that must locate a hidden target in an environment that consists of a set of concurrent rays. The second class pertains to the design of interruptible algorithms by means of a schedule of contract algorithms. We study several variants of these families of problems, such as searching and scheduling with probabilistic considerations, redundancy and fault-tolerance issues, randomized strategies, and trade-offs between performance and preemptions. For many of these problems we present the first known results that apply to multi-ray and multi-problem domains. Our objective is to demonstrate that several well-motivated settings can be addressed using the same underlying approach.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07182",
        "title": "A Probabilistic Framework for Representing Dialog Systems and Entropy-Based Dialog Management through Dynamic Stochastic State Evolution",
        "authors": [
            "Ji Wu",
            "Miao Li",
            "Chin-Hui Lee"
        ],
        "abstract": "In this paper, we present a probabilistic framework for goal-driven spoken dialog systems. A new dynamic stochastic state (DS-state) is then defined to characterize the goal set of a dialog state at different stages of the dialog process. Furthermore, an entropy minimization dialog management(EMDM) strategy is also proposed to combine with the DS-states to facilitate a robust and efficient solution in reaching a user's goals. A Song-On-Demand task, with a total of 38117 songs and 12 attributes corresponding to each song, is used to test the performance of the proposed approach. In an ideal simulation, assuming no errors, the EMDM strategy is the most efficient goal-seeking method among all tested approaches, returning the correct song within 3.3 dialog turns on average. Furthermore, in a practical scenario, with top five candidates to handle the unavoidable automatic speech recognition (ASR) and natural language understanding (NLU) errors, the results show that only 61.7\\% of the dialog goals can be successfully obtained in 6.23 dialog turns on average when random questions are asked by the system, whereas if the proposed DS-states are updated with the top 5 candidates from the SLU output using the proposed EMDM strategy executed at every DS-state, then a 86.7\\% dialog success rate can be accomplished effectively within 5.17 dialog turns on average. We also demonstrate that entropy-based DM strategies are more efficient than non-entropy based DM. Moreover, using the goal set distributions in EMDM, the results are better than those without them, such as in sate-of-the-art database summary DM.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07302",
        "title": "Building Hierarchies of Concepts via Crowdsourcing",
        "authors": [
            "Yuyin Sun",
            "Adish Singla",
            "Dieter Fox",
            "Andreas Krause"
        ],
        "abstract": "Hierarchies of concepts are useful in many applications from navigation to organization of objects. Usually, a hierarchy is created in a centralized manner by employing a group of domain experts, a time-consuming and expensive process. The experts often design one single hierarchy to best explain the semantic relationships among the concepts, and ignore the natural uncertainty that may exist in the process. In this paper, we propose a crowdsourcing system to build a hierarchy and furthermore capture the underlying uncertainty. Our system maintains a distribution over possible hierarchies and actively selects questions to ask using an information gain criterion. We evaluate our methodology on simulated data and on a set of real world application domains. Experimental results show that our system is robust to noise, efficient in picking questions, cost-effective and builds high quality hierarchies.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07443",
        "title": "Combining Existential Rules and Transitivity: Next Steps",
        "authors": [
            "Jean-Fran\u00e7ois Baget",
            "Meghyn Bienvenu",
            "Marie-Laure Mugnier",
            "Swan Rocher"
        ],
        "abstract": "We consider existential rules (aka Datalog+) as a formalism for specifying ontologies. In recent years, many classes of existential rules have been exhibited for which conjunctive query (CQ) entailment is decidable. However, most of these classes cannot express transitivity of binary relations, a frequently used modelling construct. In this paper, we address the issue of whether transitivity can be safely combined with decidable classes of existential rules.\n",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2017-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07571",
        "title": "Can Machines Truly Think",
        "authors": [
            "Murat Okandan"
        ],
        "abstract": "Can machines truly think? This question and its answer have many implications that depend, in large part, on any number of assumptions underlying how the issue has been addressed or considered previously. A crucial question, and one that is almost taken for granted, is the starting point for this discussion: Can \"thought\" be achieved or emulated by algorithmic procedures?\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07877",
        "title": "Prefix-Projection Global Constraint for Sequential Pattern Mining",
        "authors": [
            "Amina Kemmar",
            "Samir Loudni",
            "Yahia Lebbah",
            "Patrice Boizumault",
            "Thierry Charnois"
        ],
        "abstract": "Sequential pattern mining under constraints is a challenging data mining task. Many efficient ad hoc methods have been developed for mining sequential patterns, but they are all suffering from a lack of genericity. Recent works have investigated Constraint Programming (CP) methods, but they are not still effective because of their encoding. In this paper, we propose a global constraint based on the projected databases principle which remedies to this drawback. Experiments show that our approach clearly outperforms CP approaches and competes well with ad hoc methods on large datasets.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08027",
        "title": "Information-theoretic Interestingness Measures for Cross-Ontology Data Mining",
        "authors": [
            "Prashanti Manda",
            "Fiona McCarthy",
            "Bindu Nanduri",
            "Hui Wang",
            "Susan M. Bridges"
        ],
        "abstract": "Community annotation of biological entities with concepts from multiple bio-ontologies has created large and growing repositories of ontology-based annotation data with embedded implicit relationships among orthogonal ontologies. Development of efficient data mining methods and metrics to mine and assess the quality of the mined relationships has not kept pace with the growth of annotation data. In this study, we present a data mining method that uses ontology-guided generalization to discover relationships across ontologies along with a new interestingness metric based on information theory. We apply our data mining algorithm and interestingness measures to datasets from the Gene Expression Database at the Mouse Genome Informatics as a preliminary proof of concept to mine relationships between developmental stages in the mouse anatomy ontology and Gene Ontology concepts (biological process, molecular function and cellular component). In addition, we present a comparison of our interestingness metric to four existing metrics. Ontology-based annotation datasets provide a valuable resource for discovery of relationships across ontologies. The use of efficient data mining methods and appropriate interestingness metrics enables the identification of high quality relationships.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08108",
        "title": "Verification of Generalized Inconsistency-Aware Knowledge and Action Bases (Extended Version)",
        "authors": [
            "Diego Calvanese",
            "Marco Montali",
            "Ario Santoso"
        ],
        "abstract": "Knowledge and Action Bases (KABs) have been put forward as a semantically rich representation of a domain, using a DL KB to account for its static aspects, and actions to evolve its extensional part over time, possibly introducing new objects. Recently, KABs have been extended to manage inconsistency, with ad-hoc verification techniques geared towards specific semantics. This work provides a twofold contribution along this line of research. On the one hand, we enrich KABs with a high-level, compact action language inspired by Golog, obtaining so called Golog-KABs (GKABs). On the other hand, we introduce a parametric execution semantics for GKABs, so as to elegantly accomodate a plethora of inconsistency-aware semantics based on the notion of repair. We then provide several reductions for the verification of sophisticated first-order temporal properties over inconsistency-aware GKABs, and show that it can be addressed using known techniques, developed for standard KABs.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08241",
        "title": "Explanation of Stagnation at Points that are not Local Optima in Particle Swarm Optimization by Potential Analysis",
        "authors": [
            "Alexander Ra\u00df",
            "Manuel Schmitt",
            "Rolf Wanka"
        ],
        "abstract": "Particle Swarm Optimization (PSO) is a nature-inspired meta-heuristic for solving continuous optimization problems. In the literature, the potential of the particles of swarm has been used to show that slightly modified PSO guarantees convergence to local optima. Here we show that under specific circumstances the unmodified PSO, even with swarm parameters known (from the literature) to be good, almost surely does not yield convergence to a local optimum is provided. This undesirable phenomenon is called stagnation. For this purpose, the particles' potential in each dimension is analyzed mathematically. Additionally, some reasonable assumptions on the behavior if the particles' potential are made. Depending on the objective function and, interestingly, the number of particles, the potential in some dimensions may decrease much faster than in other dimensions. Therefore, these dimensions lose relevance, i.e., the contribution of their entries to the decisions about attractor updates becomes insignificant and, with positive probability, they never regain relevance. If Brownian Motion is assumed to be an approximation of the time-dependent drop of potential, practical, i.e., large values for this probability are calculated. Finally, on chosen multidimensional polynomials of degree two, experiments are provided showing that the required circumstances occur quite frequently. Furthermore, experiments are provided showing that even when the very simple sphere function is processed the described stagnation phenomenon occurs. Consequently, unmodified PSO does not converge to any local optimum of the chosen functions for tested parameter settings.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08248",
        "title": "Frugal Bribery in Voting",
        "authors": [
            "Palash Dey",
            "Neeldhara Misra",
            "Y. Narahari"
        ],
        "abstract": "Bribery in elections is an important problem in computational social choice theory. However, bribery with money is often illegal in elections. Motivated by this, we introduce the notion of frugal bribery and formulate two new pertinent computational problems which we call Frugal-bribery and Frugal- $bribery to capture bribery without money in elections. In the proposed model, the briber is frugal in nature and this is captured by her inability to bribe votes of a certain kind, namely, non-vulnerable votes. In the Frugal-bribery problem, the goal is to make a certain candidate win the election by changing only vulnerable votes. In the Frugal-{dollar}bribery problem, the vulnerable votes have prices and the goal is to make a certain candidate win the election by changing only vulnerable votes, subject to a budget constraint of the briber. We further formulate two natural variants of the Frugal-{dollar}bribery problem namely Uniform-frugal-{dollar}bribery and Nonuniform-frugal-{dollar}bribery where the prices of the vulnerable votes are, respectively, all the same or different.\n",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2017-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08256",
        "title": "Manipulation is Harder with Incomplete Votes",
        "authors": [
            "Palash Dey",
            "Neeldhara Misra",
            "Y. Narahari"
        ],
        "abstract": "The Coalitional Manipulation (CM) problem has been studied extensively in the literature for many voting rules. The CM problem, however, has been studied only in the complete information setting, that is, when the manipulators know the votes of the non-manipulators. A more realistic scenario is an incomplete information setting where the manipulators do not know the exact votes of the non- manipulators but may have some partial knowledge of the votes. In this paper, we study a setting where the manipulators know a partial order for each voter that is consistent with the vote of that voter. In this setting, we introduce and study two natural computational problems - (1) Weak Manipulation (WM) problem where the manipulators wish to vote in a way that makes their preferred candidate win in at least one extension of the partial votes of the non-manipulators; (2) Strong Manipulation (SM) problem where the manipulators wish to vote in a way that makes their preferred candidate win in all possible extensions of the partial votes of the non-manipulators. We study the computational complexity of the WM and the SM problems for commonly used voting rules such as plurality, veto, k-approval, k-veto, maximin, Copeland, and Bucklin. Our key finding is that, barring a few exceptions, manipulation becomes a significantly harder problem in the setting of incomplete votes.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00002",
        "title": "FIFTH system for general-purpose connectionist computation",
        "authors": [
            "Anthony Di Franco"
        ],
        "abstract": "To date, work on formalizing connectionist computation in a way that is at least Turing-complete has focused on recurrent architectures and developed equivalences to Turing machines or similar super-Turing models, which are of more theoretical than practical significance. We instead develop connectionist computation within the framework of information propagation networks extended with unbounded recursion, which is related to constraint logic programming and is more declarative than the semantics typically used in practical programming, but is still formally known to be Turing-complete. This approach yields contributions to the theory and practice of both connectionist computation and programming languages. Connectionist computations are carried out in a way that lets them communicate with, and be understood and interrogated directly in terms of the high-level semantics of a general-purpose programming language. Meanwhile, difficult (unbounded-dimension, NP-hard) search problems in programming that have previously been left to the programmer to solve in a heuristic, domain-specific way are solved uniformly a priori in a way that approximately achieves information-theoretic limits on performance.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00162",
        "title": "A Modification of the Halpern-Pearl Definition of Causality",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "The original Halpern-Pearl definition of causality [Halpern and Pearl, 2001] was updated in the journal version of the paper [Halpern and Pearl, 2005] to deal with some problems pointed out by Hopkins and Pearl [2003]. Here the definition is modified yet again, in a way that (a) leads to a simpler definition, (b) handles the problems pointed out by Hopkins and Pearl, and many others, (c) gives reasonable answers (that agree with those of the original and updated definition) in the standard problematic examples of causality, and (d) has lower complexity than either the original or updated definitions.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00274",
        "title": "Stick-Breaking Policy Learning in Dec-POMDPs",
        "authors": [
            "Miao Liu",
            "Christopher Amato",
            "Xuejun Liao",
            "Lawrence Carin",
            "Jonathan P. How"
        ],
        "abstract": "Expectation maximization (EM) has recently been shown to be an efficient algorithm for learning finite-state controllers (FSCs) in large decentralized POMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often converge to maxima that are far from optimal. This paper considers a variable-size FSC to represent the local policy of each agent. These variable-size FSCs are constructed using a stick-breaking prior, leading to a new framework called \\emph{decentralized stick-breaking policy representation} (Dec-SBPR). This approach learns the controller parameters with a variational Bayesian algorithm without having to assume that the Dec-POMDP model is available. The performance of Dec-SBPR is demonstrated on several benchmark problems, showing that the algorithm scales to large problems while outperforming other state-of-the-art methods.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00278",
        "title": "Automatic Observer Script for StarCraft: Brood War Bot Games (technical report)",
        "authors": [
            "Bj\u00f6rn Persson Mattsson",
            "Tom\u00e1\u0161 Vajda",
            "Michal \u010certick\u00fd"
        ],
        "abstract": "This short report describes an automated BWAPI-based script developed for live streams of a StarCraft Brood War bot tournament, SSCAIT. The script controls the in-game camera in order to follow the relevant events and improve the viewer experience. We enumerate its novel features and provide a few implementation notes.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00284",
        "title": "Bayesian Policy Reuse",
        "authors": [
            "Benjamin Rosman",
            "Majd Hawasly",
            "Subramanian Ramamoorthy"
        ],
        "abstract": "A long-lived autonomous agent should be able to respond online to novel instances of tasks from a familiar domain. Acting online requires 'fast' responses, in terms of rapid convergence, especially when the task instance has a short duration, such as in applications involving interactions with humans. These requirements can be problematic for many established methods for learning to act. In domains where the agent knows that the task instance is drawn from a family of related tasks, albeit without access to the label of any given instance, it can choose to act through a process of policy reuse from a library, rather than policy learning from scratch. In policy reuse, the agent has prior knowledge of the class of tasks in the form of a library of policies that were learnt from sample task instances during an offline training phase. We formalise the problem of policy reuse, and present an algorithm for efficiently responding to a novel task instance by reusing a policy from the library of existing policies, where the choice is based on observed 'signals' which correlate to policy performance. We achieve this by posing the problem as a Bayesian choice problem with a corresponding notion of an optimal response, but the computation of that response is in many cases intractable. Therefore, to reduce the computation cost of the posterior, we follow a Bayesian optimisation approach and define a set of policy selection functions, which balance exploration in the policy library against exploitation of previously tried policies, together with a model of expected performance of the policy library on their corresponding task instances. We validate our method in several simulated domains of interactive, short-duration episodic tasks, showing rapid convergence in unknown task variations.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00399",
        "title": "Metareasoning for Planning Under Uncertainty",
        "authors": [
            "Christopher H. Lin",
            "Andrey Kolobov",
            "Ece Kamar",
            "Eric Horvitz"
        ],
        "abstract": "The conventional model for online planning under uncertainty assumes that an agent can stop and plan without incurring costs for the time spent planning. However, planning time is not free in most real-world settings. For example, an autonomous drone is subject to nature's forces, like gravity, even while it thinks, and must either pay a price for counteracting these forces to stay in place, or grapple with the state change caused by acquiescing to them. Policy optimization in these settings requires metareasoning---a process that trades off the cost of planning and the potential policy improvement that can be achieved. We formalize and analyze the metareasoning problem for Markov Decision Processes (MDPs). Our work subsumes previously studied special cases of metareasoning and shows that in the general case, metareasoning is at most polynomially harder than solving MDPs with any given algorithm that disregards the cost of thinking. For reasons we discuss, optimal general metareasoning turns out to be impractical, motivating approximations. We present approximate metareasoning procedures which rely on special properties of the BRTDP planning algorithm and explore the effectiveness of our methods on a variety of problems.\n    ",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00423",
        "title": "Optimal Time-Series Motifs",
        "authors": [
            "Josif Grabocka",
            "Nicolas Schilling",
            "Lars Schmidt-Thieme"
        ],
        "abstract": "Motifs are the most repetitive/frequent patterns of a time-series. The discovery of motifs is crucial for practitioners in order to understand and interpret the phenomena occurring in sequential data. Currently, motifs are searched among series sub-sequences, aiming at selecting the most frequently occurring ones. Search-based methods, which try out series sub-sequence as motif candidates, are currently believed to be the best methods in finding the most frequent patterns.\n",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00566",
        "title": "Estimating the Margin of Victory of an Election using Sampling",
        "authors": [
            "Palash Dey",
            "Y. Narahari"
        ],
        "abstract": "The margin of victory of an election is a useful measure to capture the robustness of an election outcome. It also plays a crucial role in determining the sample size of various algorithms in post election audit, polling etc. In this work, we present efficient sampling based algorithms for estimating the margin of victory of elections.\n",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01221",
        "title": "The Configurable SAT Solver Challenge (CSSC)",
        "authors": [
            "Frank Hutter",
            "Marius Lindauer",
            "Adrian Balint",
            "Sam Bayless",
            "Holger Hoos",
            "Kevin Leyton-Brown"
        ],
        "abstract": "It is well known that different solution strategies work well for different types of instances of hard combinatorial problems. As a consequence, most solvers for the propositional satisfiability problem (SAT) expose parameters that allow them to be customized to a particular family of instances. In the international SAT competition series, these parameters are ignored: solvers are run using a single default parameter setting (supplied by the authors) for all benchmark instances in a given track. While this competition format rewards solvers with robust default settings, it does not reflect the situation faced by a practitioner who only cares about performance on one particular application and can invest some time into tuning solver parameters for this application. The new Configurable SAT Solver Competition (CSSC) compares solvers in this latter setting, scoring each solver by the performance it achieved after a fully automated configuration step. This article describes the CSSC in more detail, and reports the results obtained in its two instantiations so far, CSSC 2013 and 2014.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01603",
        "title": "Best-First and Depth-First Minimax Search in Practice",
        "authors": [
            "Aske Plaat",
            "Jonathan Schaeffer",
            "Wim Pijls",
            "Arie de Bruin"
        ],
        "abstract": "Most practitioners use a variant of the Alpha-Beta algorithm, a simple depth-first pro- cedure, for searching minimax trees. SSS*, with its best-first search strategy, reportedly offers the potential for more efficient search. However, the complex formulation of the al- gorithm and its alleged excessive memory requirements preclude its use in practice. For two decades, the search efficiency of \"smart\" best-first SSS* has cast doubt on the effectiveness of \"dumb\" depth-first Alpha-Beta. This paper presents a simple framework for calling Alpha-Beta that allows us to create a variety of algorithms, including SSS* and DUAL*. In effect, we formulate a best-first algorithm using depth-first search. Expressed in this framework SSS* is just a special case of Alpha-Beta, solving all of the perceived drawbacks of the algorithm. In practice, Alpha-Beta variants typically evaluate less nodes than SSS*. A new instance of this framework, MTD(f), out-performs SSS* and NegaScout, the Alpha-Beta variant of choice by practitioners.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01825",
        "title": "Effects of Nonparanormal Transform on PC and GES Search Accuracies",
        "authors": [
            "Joseph D. Ramsey"
        ],
        "abstract": "Liu, et al., 2009 developed a transformation of a class of non-Gaussian univariate distributions into Gaussian distributions. Liu and collaborators (2012) subsequently applied the transform to search for graphical causal models for a number of empirical data sets. To our knowledge, there has been no published investigation by simulation of the conditions under which the transform aids, or harms, standard graphical model search procedures. We consider here how the transform affects the performance of two search algorithms in particular, PC (Spirtes et al., 2000; Meek 1995) and GES (Meek 1997; Chickering 2002). We find that the transform is harmless but ineffective for most cases but quite effective in very special cases for GES, namely, for moderate non-Gaussianity and moderate non-linearity. For strong-linearity, another algorithm, PC-GES (a combination of PC with GES), is equally effective.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02070",
        "title": "Short Portfolio Training for CSP Solving",
        "authors": [
            "Mirko Stojadinovi\u0107",
            "Mladen Nikoli\u0107",
            "Filip Mari\u0107"
        ],
        "abstract": "Many different approaches for solving Constraint Satisfaction Problems (CSPs) and related Constraint Optimization Problems (COPs) exist. However, there is no single solver (nor approach) that performs well on all classes of problems and many portfolio approaches for selecting a suitable solver based on simple syntactic features of the input CSP instance have been developed. In this paper we first present a simple portfolio method for CSP based on k-nearest neighbors method. Then, we propose a new way of using portfolio systems --- training them shortly in the exploitation time, specifically for the set of instances to be solved and using them on that set. Thorough evaluation has been performed and has shown that the approach yields good results. We evaluated several machine learning techniques for our portfolio. Due to its simplicity and efficiency, the selected k-nearest neighbors method is especially suited for our short training approach and it also yields the best results among the tested methods. We also confirm that our approach yields good results on SAT domain.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02405",
        "title": "Exploiting Resolution-based Representations for MaxSAT Solving",
        "authors": [
            "Miguel Neves",
            "Ruben Martins",
            "Mikol\u00e1\u0161 Janota",
            "In\u00eas Lynce",
            "Vasco Manquinho"
        ],
        "abstract": "Most recent MaxSAT algorithms rely on a succession of calls to a SAT solver in order to find an optimal solution. In particular, several algorithms take advantage of the ability of SAT solvers to identify unsatisfiable subformulas. Usually, these MaxSAT algorithms perform better when small unsatisfiable subformulas are found early. However, this is not the case in many problem instances, since the whole formula is given to the SAT solver in each call. In this paper, we propose to partition the MaxSAT formula using a resolution-based graph representation. Partitions are then iteratively joined by using a proximity measure extracted from the graph representation of the formula. The algorithm ends when only one partition remains and the optimal solution is found. Experimental results show that this new approach further enhances a state of the art MaxSAT solver to optimally solve a larger set of industrial problem instances.\n    ",
        "submission_date": "2015-05-10T00:00:00",
        "last_modified_date": "2015-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02433",
        "title": "Probabilistic Belief Embedding for Knowledge Base Completion",
        "authors": [
            "Miao Fan",
            "Qiang Zhou",
            "Andrew Abel",
            "Thomas Fang Zheng",
            "Ralph Grishman"
        ],
        "abstract": "This paper contributes a novel embedding model which measures the probability of each belief $\\langle h,r,t,m\\rangle$ in a large-scale knowledge repository via simultaneously learning distributed representations for entities ($h$ and $t$), relations ($r$), and the words in relation mentions ($m$). It facilitates knowledge completion by means of simple vector operations to discover new beliefs. Given an imperfect belief, we can not only infer the missing entities, predict the unknown relations, but also tell the plausibility of the belief, just leveraging the learnt embeddings of remaining evidences. To demonstrate the scalability and the effectiveness of our model, we conduct experiments on several large-scale repositories which contain millions of beliefs from WordNet, Freebase and NELL, and compare it with other cutting-edge approaches via competing the performances assessed by the tasks of entity inference, relation prediction and triplet classification with respective metrics. Extensive experimental results show that the proposed model outperforms the state-of-the-arts with significant improvements.\n    ",
        "submission_date": "2015-05-10T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02449",
        "title": "Automating change of representation for proofs in discrete mathematics",
        "authors": [
            "Daniel Raggi",
            "Alan Bundy",
            "Gudmund Grov",
            "Alison Pease"
        ],
        "abstract": "Representation determines how we can reason about a specific problem. Sometimes one representation helps us find a proof more easily than others. Most current automated reasoning tools focus on reasoning within one representation. There is, therefore, a need for the development of better tools to mechanise and automate formal and logically sound changes of representation.\n",
        "submission_date": "2015-05-10T00:00:00",
        "last_modified_date": "2015-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02487",
        "title": "A Constraint Programming Approach for Non-Preemptive Evacuation Scheduling",
        "authors": [
            "Caroline Even",
            "Andreas Schutt",
            "Pascal Van Hentenryck"
        ],
        "abstract": "Large-scale controlled evacuations require emergency services to select evacuation routes, decide departure times, and mobilize resources to issue orders, all under strict time constraints. Existing algorithms almost always allow for preemptive evacuation schedules, which are less desirable in practice. This paper proposes, for the first time, a constraint-based scheduling model that optimizes the evacuation flow rate (number of vehicles sent at regular time intervals) and evacuation phasing of widely populated areas, while ensuring a nonpreemptive evacuation for each residential zone. Two optimization objectives are considered: (1) to maximize the number of evacuees reaching safety and (2) to minimize the overall duration of the evacuation. Preliminary results on a set of real-world instances show that the approach can produce, within a few seconds, a non-preemptive evacuation schedule which is either optimal or at most 6% away of the optimal preemptive solution.\n    ",
        "submission_date": "2015-05-11T00:00:00",
        "last_modified_date": "2015-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02552",
        "title": "Relations between MDDs and Tuples and Dynamic Modifications of MDDs based constraints",
        "authors": [
            "Guillaume Perez",
            "Jean-Charles R\u00e9gin"
        ],
        "abstract": "We study the relations between Multi-valued Decision Diagrams (MDD) and tuples (i.e. elements of the Cartesian Product of variables). First, we improve the existing methods for transforming a set of tuples, Global Cut Seeds, sequences of tuples into MDDs. Then, we present some in-place algorithms for adding and deleting tuples from an MDD. Next, we consider an MDD constraint which is modified during the search by deleting some tuples. We give an algorithm which adapts MDD-4R to these dynamic and persistent modifications. Some experiments show that MDD constraints are competitive with Table constraints.\n    ",
        "submission_date": "2015-05-11T00:00:00",
        "last_modified_date": "2015-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02830",
        "title": "Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search",
        "authors": [
            "Yun-Ching Liu",
            "Yoshimasa Tsuruoka"
        ],
        "abstract": "The UCT algorithm, which combines the UCB algorithm and Monte-Carlo Tree Search (MCTS), is currently the most widely used variant of MCTS. Recently, a number of investigations into applying other bandit algorithms to MCTS have produced interesting results. In this research, we will investigate the possibility of combining the improved UCB algorithm, proposed by Auer et al. (2010), with MCTS. However, various characteristics and properties of the improved UCB algorithm may not be ideal for a direct application to MCTS. Therefore, some modifications were made to the improved UCB algorithm, making it more suitable for the task of game tree search. The Mi-UCT algorithm is the application of the modified UCB algorithm applied to trees. The performance of Mi-UCT is demonstrated on the games of $9\\times 9$ Go and $9\\times 9$ NoGo, and has shown to outperform the plain UCT algorithm when only a small number of playouts are given, and rougly on the same level when more playouts are available.\n    ",
        "submission_date": "2015-05-11T00:00:00",
        "last_modified_date": "2015-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03101",
        "title": "Release Early, Release Often: Predicting Change in Versioned Knowledge Organization Systems on the Web",
        "authors": [
            "Albert Mero\u00f1o-Pe\u00f1uela",
            "Christophe Gu\u00e9ret",
            "Stefan Schlobach"
        ],
        "abstract": "The Semantic Web is built on top of Knowledge Organization Systems (KOS) (vocabularies, ontologies, concept schemes) that provide a structured, interoperable and distributed access to Linked Data on the Web. The maintenance of these KOS over time has produced a number of KOS version chains: subsequent unique version identifiers to unique states of a KOS. However, the release of new KOS versions pose challenges to both KOS publishers and users. For publishers, updating a KOS is a knowledge intensive task that requires a lot of manual effort, often implying deep deliberation on the set of changes to introduce. For users that link their datasets to these KOS, a new version compromises the validity of their links, often creating ramifications. In this paper we describe a method to automatically detect which parts of a Web KOS are likely to change in a next version, using supervised learning on past versions in the KOS version chain. We use a set of ontology change features to model and predict change in arbitrary Web KOS. We apply our method on 139 varied datasets systematically retrieved from the Semantic Web, obtaining robust results at correctly predicting change. To illustrate the accuracy, genericity and domain independence of the method, we study the relationship between its effectiveness and several characterizations of the evaluated datasets, finding that predictors like the number of versions in a chain and their release frequency have a fundamental impact in predictability of change in Web KOS. Consequently, we argue for adopting a release early, release often philosophy in Web KOS development cycles.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03662",
        "title": "Predicting Occupancy Trends in Barcelona's Bicycle Service Stations Using Open Data",
        "authors": [
            "Gabriel Martins Dias",
            "Boris Bellalta",
            "Simon Oechsner"
        ],
        "abstract": "In 2008, the CEO of the company that manages and maintains the public bicycle service in Barcelona recognized that one may not expect to always find a place to leave the rented bike nearby their destination, similarly to the case when, driving a car, people may not find a parking lot. In this work, we make predictions about the statuses of the stations of the public bicycle service in Barcelona. We show that it is feasible to correctly predict nearly half of the times when the stations are either completely full of bikes or completely empty, up to 2 days before they actually happen. That is, users might avoid stations at times when they could not return a bicycle that they have rented before, or when they would not find a bike to rent. To achieve that, we apply the Random Forest algorithm to classify the status of the stations and improve the lifetime of the models using publicly available data, such as information about the weather forecast. Finally, we expect that the results of the predictions can be used to improve the quality of the service and make it more reliable for the users.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03953",
        "title": "A Theory of Formal Synthesis via Inductive Learning",
        "authors": [
            "Susmit Jha",
            "Sanjit A. Seshia"
        ],
        "abstract": "Formal synthesis is the process of generating a program satisfying a high-level formal specification. In recent times, effective formal synthesis methods have been proposed based on the use of inductive learning. We refer to this class of methods that learn programs from examples as formal inductive synthesis. In this paper, we present a theoretical framework for formal inductive synthesis. We discuss how formal inductive synthesis differs from traditional machine learning. We then describe oracle-guided inductive synthesis (OGIS), a framework that captures a family of synthesizers that operate by iteratively querying an oracle. An instance of OGIS that has had much practical impact is counterexample-guided inductive synthesis (CEGIS). We present a theoretical characterization of CEGIS for learning any program that computes a recursive language. In particular, we analyze the relative power of CEGIS variants where the types of counterexamples generated by the oracle varies. We also consider the impact of bounded versus unbounded memory available to the learning algorithm. In the special case where the universe of candidate programs is finite, we relate the speed of convergence to the notion of teaching dimension studied in machine learning theory. Altogether, the results of the paper take a first step towards a theoretical foundation for the emerging field of formal inductive synthesis.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2016-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04097",
        "title": "MCODE: Multivariate Conditional Outlier Detection",
        "authors": [
            "Charmgil Hong",
            "Milos Hauskrecht"
        ],
        "abstract": "Outlier detection aims to identify unusual data instances that deviate from expected patterns. The outlier detection is particularly challenging when outliers are context dependent and when they are defined by unusual combinations of multiple outcome variable values. In this paper, we develop and study a new conditional outlier detection approach for multivariate outcome spaces that works by (1) transforming the conditional detection to the outlier detection problem in a new (unconditional) space and (2) defining outlier scores by analyzing the data in the new space. Our approach relies on the classifier chain decomposition of the multi-dimensional classification problem that lets us transform the output space into a probability vector, one probability for each dimension of the output space. Outlier scores applied to these transformed vectors are then used to detect the outliers. Experiments on multiple multi-dimensional classification problems with the different outlier injection rates show that our methodology is robust and able to successfully identify outliers when outliers are either sparse (manifested in one or very few dimensions) or dense (affecting multiple dimensions).\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04107",
        "title": "OntoSOC: Sociocultural Knowledge Ontology",
        "authors": [
            "Guidedi Kaladzavi",
            "Papa Fary Diallo",
            "Kolyang",
            "Moussa Lo"
        ],
        "abstract": "This paper presents a sociocultural knowledge ontology (OntoSOC) modeling approach. OntoSOC modeling approach is based on Engestrom Human Activity Theory (HAT). That Theory allowed us to identify fundamental concepts and relationships between them. The top-down precess has been used to define differents sub-concepts. The modeled vocabulary permits us to organise data, to facilitate information retrieval by introducing a semantic layer in social web platform architecture, we project to implement. This platform can be considered as a collective memory and Participative and Distributed Information System (PDIS) which will allow Cameroonian communities to share an co-construct knowledge on permanent organized activities.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04112",
        "title": "How, What and Why to test an ontology",
        "authors": [
            "Jennifer D. Warrender",
            "Phillip Lord"
        ],
        "abstract": "Ontology development relates to software development in that they both involve the production of formal computational knowledge. It is possible, therefore, that some of the techniques used in software engineering could also be used for ontologies; for example, in software engineering testing is a well-established process, and part of many different methodologies.\n",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04150",
        "title": "Reinforcement Learning applied to Single Neuron",
        "authors": [
            "Zhipeng Wang",
            "Mingbo Cai"
        ],
        "abstract": "This paper extends the reinforcement learning ideas into the multi-agents system, which is far more complicated than the previously studied single-agent system. We studied two different multi-agents systems. One is the fully-connected neural network consists of multiple single neurons. Another one is the simplified mechanical arm system which is controlled by multiple neurons. We suppose that each neuron is like an agent and it can do Gibbs sampling of the posterior probability of stimulus features. The policy is optimized in a way that the cumulative global rewards are maximized. The algorithm for the second system is based on the same idea but we incorporate the physics model into the constraints. The simulation results show that for the first system our algorithm converges well. For the second system it does not converge well in a reasonable simulation time length. In summary, we took the initial endeavor to study the reinforcement learning for multi-agents system. The computational complexity is always an issue and significant amount of works have to be done in order to better understand the problem.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04265",
        "title": "Cognitive Development of the Web",
        "authors": [
            "Viktoras Veitas",
            "David Weinbaum"
        ],
        "abstract": "The sociotechnological system is a system constituted of human individuals and their artifacts: technological artifacts, institutions, conceptual and representational systems, worldviews, knowledge systems, culture and the whole biosphere as a volutionary niche. In our view the sociotechnological system as a super-organism is shaped and determined both by the characteristics of the agents involved and the characteristics emergent in their interactions at multiple scales. Our approach to sociotechnological dynamics will maintain a balance between perspectives: the individual and the collective. Accordingly, we analyze dynamics of the Web as a sociotechnological system made of people, computers and digital artifacts (Web pages, databases, search engines, etc.). Making sense of the sociotechnological system while being part of it, is also a constant interplay between pragmatic and value based approaches. The first is focusing on the actualities of the system while the second highlights the observer's projections. In our attempt to model sociotechnological dynamics and envision its future, we take special care to make explicit our values as part of the analysis. In sociotechnological systems with a high degree of reflexivity (coupling between the perception of the system and the system's behavior), highlighting values is of critical importance. In this essay, we choose to see the future evolution of the web as facilitating a basic value, that is, continuous open-ended intelligence expansion. By that we mean that we see intelligence expansion as the determinant of the 'greater good' and 'well being' of both of individuals and collectives at all scales. Our working definition of intelligence here is the progressive process of sense-making of self, other, environment and universe. Intelligence expansion, therefore, means an increasing ability of sense-making.\n    ",
        "submission_date": "2015-05-16T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04497",
        "title": "A Definition of Happiness for Reinforcement Learning Agents",
        "authors": [
            "Mayank Daswani",
            "Jan Leike"
        ],
        "abstract": "What is happiness for reinforcement learning agents? We seek a formal definition satisfying a list of desiderata. Our proposed definition of happiness is the temporal difference error, i.e. the difference between the value of the obtained reward and observation and the agent's expectation of this value. This definition satisfies most of our desiderata and is compatible with empirical research on humans. We state several implications and discuss examples.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04578",
        "title": "Advances in Artificial Intelligence: Deep Intentions, Shallow Achievements",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "Over the past decade, AI has made a remarkable progress due to recently revived Deep Learning technology. Deep Learning enables to process large amounts of data using simplified neuron networks that simulate the way in which the brain works. At the same time, there is another point of view that posits that brain is processing information, not data. This duality hampered AI progress for years. To provide a remedy for this situation, I propose a new definition of information that considers it as a coupling between two separate entities - physical information (that implies data processing) and semantic information (that provides physical information interpretation). In such a case, intelligence arises as a result of information processing. The paper points on the consequences of this turn for the AI design philosophy.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04677",
        "title": "On sets of graded attribute implications with witnessed non-redundancy",
        "authors": [
            "Vilem Vychodil"
        ],
        "abstract": "We study properties of particular non-redundant sets of if-then rules describing dependencies between graded attributes. We introduce notions of saturation and witnessed non-redundancy of sets of graded attribute implications are show that bases of graded attribute implications given by systems of pseudo-intents correspond to non-redundant sets of graded attribute implications with saturated consequents where the non-redundancy is witnessed by antecedents of the contained graded attribute implications. We introduce an algorithm which transforms any complete set of graded attribute implications parameterized by globalization into a base given by pseudo-intents. Experimental evaluation is provided to compare the method of obtaining bases for general parameterizations by hedges with earlier graph-based approaches.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04813",
        "title": "What is Learning? A primary discussion about information and Representation",
        "authors": [
            "Hao Wu"
        ],
        "abstract": "Nowadays, represented by Deep Learning techniques, the field of machine learning is experiencing unprecedented prosperity and its influence is demonstrated in academia, industry and civil society. \"Intelligent\" has become a label which could not be neglected for most applications; celebrities and scientists also warned that the development of full artificial intelligence may spell the end of the human race. It seems that the answer to building a computer system that could automatically improve with experience is right on the next corner. While for AI and machine learning researchers, it is a consensus that we are not anywhere near the core technique which could bring the Terminator, Number 5 or R2D2 into real life, and there is not even a formal definition about what is intelligence, or one of its basic properties: Learning. Therefore, even though researchers know these concerns are not necessary currently, there is no generalized explanation about why these concerns are not necessary, and what properties people should take into account that would make these concerns to be necessary. In this paper, starts from analysing the relation between information and its representation, a necessary condition for a model to be a learning model is proposed. This condition and related future works could be used to verify whether a system is able to learn or not, and enrich our understanding of learning: one important property of Intelligence.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04981",
        "title": "A New Fundamental Evidence of Non-Classical Structure in the Combination of Natural Concepts",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "We recently performed cognitive experiments on conjunctions and negations of two concepts with the aim of investigating the combination problem of concepts. Our experiments confirmed the deviations (conceptual vagueness, underextension, overextension, etc.) from the rules of classical (fuzzy) logic and probability theory observed by several scholars in concept theory, while our data were successfully modeled in a quantum-theoretic framework developed by ourselves. In this paper, we isolate a new, very stable and systematic pattern of violation of classicality that occurs in concept combinations. In addition, the strength and regularity of this non-classical effect leads us to believe that it occurs at a more fundamental level than the deviations observed up to now. It is our opinion that we have identified a deep non-classical mechanism determining not only how concepts are combined but, rather, how they are formed. We show that this effect can be faithfully modeled in a two-sector Fock space structure, and that it can be exactly explained by assuming that human thought is the supersposition of two processes, a 'logical reasoning', guided by 'logic', and a 'conceptual reasoning' guided by 'emergence', and that the latter generally prevails over the former. All these findings provide a new fundamental support to our quantum-theoretic approach to human cognition.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05063",
        "title": "Necessary and Sufficient Conditions for Surrogate Functions of Pareto Frontiers and Their Synthesis Using Gaussian Processes",
        "authors": [
            "Conrado Silva Miranda",
            "Fernando Jos\u00e9 Von Zuben"
        ],
        "abstract": "This paper introduces the necessary and sufficient conditions that surrogate functions must satisfy to properly define frontiers of non-dominated solutions in multi-objective optimization problems. These new conditions work directly on the objective space, thus being agnostic about how the solutions are evaluated. Therefore, real objectives or user-designed objectives' surrogates are allowed, opening the possibility of linking independent objective surrogates. To illustrate the practical consequences of adopting the proposed conditions, we use Gaussian processes as surrogates endowed with monotonicity soft constraints and with an adjustable degree of flexibility, and compare them to regular Gaussian processes and to a frontier surrogate method in the literature that is the closest to the method proposed in this paper. Results show that the necessary and sufficient conditions proposed here are finely managed by the constrained Gaussian process, guiding to high-quality surrogates capable of suitably synthesizing an approximation to the Pareto frontier in challenging instances of multi-objective optimization, while an existing approach that does not take the theory proposed in consideration defines surrogates which greatly violate the conditions to describe a valid frontier.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05312",
        "title": "A New Oscillating-Error Technique for Classifiers",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05364",
        "title": "Reactive Reasoning with the Event Calculus",
        "authors": [
            "Alexander Artikis",
            "Marek Sergot",
            "Georgios Paliouras"
        ],
        "abstract": "Systems for symbolic event recognition accept as input a stream of time-stamped events from sensors and other computational devices, and seek to identify high-level composite events, collections of events that satisfy some pattern. RTEC is an Event Calculus dialect with novel implementation and 'windowing' techniques that allow for efficient event recognition, scalable to large data streams. RTEC can deal with applications where event data arrive with a (variable) delay from, and are revised by, the underlying sources. RTEC can update already recognised events and recognise new events when data arrive with a delay or following data revision. Our evaluation shows that RTEC can support real-time event recognition and is capable of meeting the performance requirements identified in a recent survey of event processing use cases.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05365",
        "title": "Towards Ideal Semantics for Analyzing Stream Reasoning",
        "authors": [
            "Harald Beck",
            "Minh Dao-Tran",
            "Thomas Eiter",
            "Michael Fink"
        ],
        "abstract": "The rise of smart applications has drawn interest to logical reasoning over data streams. Recently, different query languages and stream processing/reasoning engines were proposed in different communities. However, due to a lack of theoretical foundations, the expressivity and semantics of these diverse approaches are given only informally. Towards clear specifications and means for analytic study, a formal framework is needed to define their semantics in precise terms. To this end, we present a first step towards an ideal semantics that allows for exact descriptions and comparisons of stream reasoning systems.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05366",
        "title": "Multi-Context Systems for Reactive Reasoning in Dynamic Environments",
        "authors": [
            "Gerhard Brewka",
            "Stefan Ellmauthaler",
            "J\u00f6rg P\u00fchrer"
        ],
        "abstract": "We show in this paper how managed multi-context systems (mMCSs) can be turned into a reactive formalism suitable for continuous reasoning in dynamic environments. We extend mMCSs with (abstract) sensors and define the notion of a run of the extended systems. We then show how typical problems arising in online reasoning can be addressed: handling potentially inconsistent sensor input, modeling intelligent forms of forgetting, selective integration of knowledge, and controlling the reasoning effort spent by contexts, like setting contexts to an idle mode. We also investigate the complexity of some important related decision problems and discuss different design choices which are given to the knowledge engineer.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05367",
        "title": "Asynchronous Multi-Context Systems",
        "authors": [
            "Stefan Ellmauthaler",
            "J\u00f6rg P\u00fchrer"
        ],
        "abstract": "In this work, we present asynchronous multi-context systems (aMCSs), which provide a framework for loosely coupling different knowledge representation formalisms that allows for online reasoning in a dynamic environment. Systems of this kind may interact with the outside world via input and output streams and may therefore react to a continuous flow of external information. In contrast to recent proposals, contexts in an aMCS communicate with each other in an asynchronous way which fits the needs of many application domains and is beneficial for scalability. The federal semantics of aMCSs renders our framework an integration approach rather than a knowledge representation formalism itself. We illustrate the introduced concepts by means of an example scenario dealing with rescue services. In addition, we compare aMCSs to reactive multi-context systems and describe how to simulate the latter with our novel approach.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05368",
        "title": "On Minimal Change in Evolving Multi-Context Systems (Preliminary Report)",
        "authors": [
            "Ricardo Gon\u00e7alves",
            "Matthias Knorr",
            "Jo\u00e3o Leite"
        ],
        "abstract": "Managed Multi-Context Systems (mMCSs) provide a general framework for integrating knowledge represented in heterogeneous KR formalisms. However, mMCSs are essentially static as they were not designed to run in a dynamic scenario. Some recent approaches, among them evolving Multi-Context Systems (eMCSs), extend mMCSs by allowing not only the ability to integrate knowledge represented in heterogeneous KR formalisms, but at the same time to both react to, and reason in the presence of commonly temporary dynamic observations, and evolve by incorporating new knowledge. The notion of minimal change is a central notion in dynamic scenarios, specially in those that admit several possible alternative evolutions. Since eMCSs combine heterogeneous KR formalisms, each of which may require different notions of minimal change, the study of minimal change in eMCSs is an interesting and highly non-trivial problem. In this paper, we study the notion of minimal change in eMCSs, and discuss some alternative minimal change criteria.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05373",
        "title": "Towards a Simulation-Based Programming Paradigm for AI applications",
        "authors": [
            "J\u00f6rg P\u00fchrer"
        ],
        "abstract": "We present initial ideas for a programming paradigm based on simulation that is targeted towards applications of artificial intelligence (AI). The approach aims at integrating techniques from different areas of AI and is based on the idea that simulated entities may freely exchange data and behavioural patterns. We define basic notions of a simulation-based programming paradigm and show how it can be used for implementing AI applications.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05375",
        "title": "Towards Large-scale Inconsistency Measurement",
        "authors": [
            "Matthias Thimm"
        ],
        "abstract": "We investigate the problem of inconsistency measurement on large knowledge bases by considering stream-based inconsistency measurement, i.e., we investigate inconsistency measures that cannot consider a knowledge base as a whole but process it within a stream. For that, we present, first, a novel inconsistency measure that is apt to be applied to the streaming case and, second, stream-based approximations for the new and some existing inconsistency measures. We conduct an extensive empirical analysis on the behavior of these inconsistency measures on large knowledge bases, in terms of runtime, accuracy, and scalability. We conclude that for two of these measures, the approximation of the new inconsistency measure and an approximation of the contension inconsistency measure, large-scale inconsistency measurement is feasible.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05451",
        "title": "Fuzzy Least Squares Twin Support Vector Machines",
        "authors": [
            "Javad Salimi Sartakhti",
            "Homayun Afrabandpey",
            "Nasser Ghadiri"
        ],
        "abstract": "Least Squares Twin Support Vector Machine (LST-SVM) has been shown to be an efficient and fast algorithm for binary classification. It combines the operating principles of Least Squares SVM (LS-SVM) and Twin SVM (T-SVM); it constructs two non-parallel hyperplanes (as in T-SVM) by solving two systems of linear equations (as in LS-SVM). Despite its efficiency, LST-SVM is still unable to cope with two features of real-world problems. First, in many real-world applications, labels of samples are not deterministic; they come naturally with their associated membership degrees. Second, samples in real-world applications may not be equally important and their importance degrees affect the classification. In this paper, we propose Fuzzy LST-SVM (FLST-SVM) to deal with these two characteristics of real-world data. Two models are introduced for FLST-SVM: the first model builds up crisp hyperplanes using training samples and their corresponding membership degrees. The second model, on the other hand, constructs fuzzy hyperplanes using training samples and their membership degrees. Numerical evaluation of the proposed method with synthetic and real datasets demonstrate significant improvement in the classification accuracy of FLST-SVM when compared to well-known existing versions of SVM.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2018-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05502",
        "title": "Towards Efficient Evolving Multi-Context Systems (Preliminary Report)",
        "authors": [
            "Ricardo Gon\u00e7alves",
            "Matthias Knorr",
            "Jo\u00e3o Leite"
        ],
        "abstract": "Managed Multi-Context Systems (mMCSs) provide a general framework for integrating knowledge represented in heterogeneous KR formalisms. Recently, evolving Multi-Context Systems (eMCSs) have been introduced as an extension of mMCSs that add the ability to both react to, and reason in the presence of commonly temporary dynamic observations, and evolve by incorporating new knowledge. However, the general complexity of such an expressive formalism may simply be too high in cases where huge amounts of information have to be processed within a limited short amount of time, or even instantaneously. In this paper, we investigate under which conditions eMCSs may scale in such situations and we show that such polynomial eMCSs can be applied in a practical use case.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05947",
        "title": "A Pareto Front-Based Multiobjective Path Planning Algorithm",
        "authors": [
            "Alexander Lavin"
        ],
        "abstract": "Path planning is one of the most vital elements of mobile robotics. With a priori knowledge of the environment, global path planning provides a collision-free route through the workspace. The global path plan can be calculated with a variety of informed search algorithms, most notably the A* search method, guaranteed to deliver a complete and optimal solution that minimizes the path cost. Path planning optimization typically looks to minimize the distance traversed from start to goal, yet many mobile robot applications call for additional path planning objectives, presenting a multiobjective optimization (MOO) problem. Past studies have applied genetic algorithms to MOO path planning problems, but these may have the disadvantages of computational complexity and suboptimal solutions. Alternatively, the algorithm in this paper approaches MOO path planning with the use of Pareto fronts, or finding non-dominated solutions. The algorithm presented incorporates Pareto optimality into every step of A* search, thus it is named A*-PO. Results of simulations show A*-PO outperformed several variations of the standard A* algorithm for MOO path planning. A planetary exploration rover case study was added to demonstrate the viability of A*-PO in a real-world application.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06366",
        "title": "Open Ended Intelligence: The individuation of Intelligent Agents",
        "authors": [
            "David Weinbaum",
            "Viktoras Veitas"
        ],
        "abstract": "Artificial General Intelligence is a field of research aiming to distill the principles of intelligence that operate independently of a specific problem domain or a predefined context and utilize these principles in order to synthesize systems capable of performing any intellectual task a human being is capable of and eventually go beyond that. While \"narrow\" artificial intelligence which focuses on solving specific problems such as speech recognition, text comprehension, visual pattern recognition, robotic motion, etc. has shown quite a few impressive breakthroughs lately, understanding general intelligence remains elusive. In the paper we offer a novel theoretical approach to understanding general intelligence. We start with a brief introduction of the current conceptual approach. Our critique exposes a number of serious limitations that are traced back to the ontological roots of the concept of intelligence. We then propose a paradigm shift from intelligence perceived as a competence of individual agents defined in relation to an a priori given problem domain or a goal, to intelligence perceived as a formative process of self-organization by which intelligent agents are individuated. We call this process open-ended intelligence. Open-ended intelligence is developed as an abstraction of the process of cognitive development so its application can be extended to general agents and systems. We introduce and discuss three facets of the idea: the philosophical concept of individuation, sense-making and the individuation of general cognitive agents. We further show how open-ended intelligence can be framed in terms of a distributed, self-organizing network of interacting elements and how such process is scalable. The framework highlights an important relation between coordination and intelligence and a new understanding of values. We conclude with a number of questions for future research.\n    ",
        "submission_date": "2015-05-23T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06537",
        "title": "A survey of SMS based Information Systems",
        "authors": [
            "Manish R. Joshi",
            "Varsha M. Pathak"
        ],
        "abstract": "Short Message Service (SMS) based Information Systems (SMSbIS) provide an excellent alternative to a traditional approach of obtaining specific information by direct (through phone) or indirect (IVRS, Web, Email) probing. Information and communication technology and far reaching mobile penetration has opened this new research trend Number of key players in Search industry including Microsoft and Google are attracted by the expected increase in volume of use of such applications. The wide range of applications and their public acceptance has motivated researchers to work in this research domain. Several applications such as SMS based information access using database management services, SMS based information retrieval through internet (search engine), SMS based information extraction, question answering, image retrieval etc. have been emerged. With the aim to understand the functionality involved in these systems, an extensive review of a few of these SMSbISs has been planned and executed by us. These systems are classified into four categories based on the objectives and domains of the applications. As a result of this study a well structured functional model is presented here. The model is evaluated in different dimensions, which is presented in this paper. In addition to this a chronological progress with respect to research and development in this upcoming field is compiled in this paper. Such an extensive review presented in this paper would definitely help the researchers and developers to understand the technical aspects of this field. The functional framework presented here would be useful to the system designers to design and develop an SMS based Information System of any specific domain.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06573",
        "title": "New results on inconsistency indices and their relationship with the quality of priority vector estimation",
        "authors": [
            "Andrzej Z. Grzybowski"
        ],
        "abstract": "The article is devoted to the problem of inconsistency in the pairwise comparisons based prioritization methodology. The issue of \"inconsistency\" in this context has gained much attention in recent years. The literature provides us with a number of different \"inconsistency\" indices suggested for measuring the inconsistency of the pairwise comparison matrix (PCM). The latter is understood as a deviation of the PCM from the \"consistent case\" - a notion that is formally well-defined in this theory. However the usage of the indices is justified only by some heuristics. It is still unclear what they really \"measure\". What is even more important and still not known is the relationship between their values and the \"consistency\" of the decision maker's judgments on one hand, and the prioritization results upon the other. We provide examples showing that it is necessary to distinguish between these three following tasks: the \"measuring\" of the \"PCM inconsistency\" and the PCM-based \"measuring\" of the consistency of decision maker's judgments and, finally, the \"measuring\" of the usefulness of the PCM as a source of information for estimation of the priority vector (PV). Next we focus on the third task, which seems to be the most important one in Multi-Criteria Decision Making. With the help of Monte Carlo experiments, we study the performance of various inconsistency indices as indicators of the final PV estimation quality. The presented results allow a deeper understanding of the information contained in these indices and help in choosing a proper one in a given situation. They also enable us to develop a new inconsistency characteristic and, based on it, to propose the PCM acceptance approach that is supported by the classical statistical methodology.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06651",
        "title": "A Logic of Knowing How",
        "authors": [
            "Yanjing Wang"
        ],
        "abstract": "In this paper, we propose a single-agent modal logic framework for reasoning about goal-direct \"knowing how\" based on ideas from linguistics, philosophy, modal logic and automated planning. We first define a modal language to express \"I know how to guarantee phi given psi\" with a semantics not based on standard epistemic models but labelled transition systems that represent the agent's knowledge of his own abilities. A sound and complete proof system is given to capture the valid reasoning patterns about \"knowing how\" where the most important axiom suggests its compositional nature.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06850",
        "title": "Implementing feedback in creative systems: A workshop approach",
        "authors": [
            "Joseph Corneli",
            "Anna Jordanous"
        ],
        "abstract": "One particular challenge in AI is the computational modelling and simulation of creativity. Feedback and learning from experience are key aspects of the creative process. Here we investigate how we could implement feedback in creative systems using a social model. From the field of creative writing we borrow the concept of a Writers Workshop as a model for learning through feedback. The Writers Workshop encourages examination, discussion and debates of a piece of creative work using a prescribed format of activities. We propose a computational model of the Writers Workshop as a roadmap for incorporation of feedback in artificial creativity systems. We argue that the Writers Workshop setting describes the anatomy of the creative process. We support our claim with a case study that describes how to implement the Writers Workshop model in a computational creativity system. We present this work using patterns other people can follow to implement similar designs in their own systems. We conclude by discussing the broader relevance of this model to other aspects of AI.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2015-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07263",
        "title": "Qsmodels: ASP Planning in Interactive Gaming Environment",
        "authors": [
            "Luca Padovani",
            "Alessandro Provetti"
        ],
        "abstract": "Qsmodels is a novel application of Answer Set Programming to interactive gaming environment. We describe a software architecture by which the behavior of a bot acting inside the Quake 3 Arena can be controlled by a planner. The planner is written as an Answer Set Program and is interpreted by the Smodels solver.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07434",
        "title": "Fair task allocation in transportation",
        "authors": [
            "Qing Chuan Ye",
            "Yingqian Zhang",
            "Rommert Dekker"
        ],
        "abstract": "Task allocation problems have traditionally focused on cost optimization. However, more and more attention is being given to cases in which cost should not always be the sole or major consideration. In this paper we study a fair task allocation problem in transportation where an optimal allocation not only has low cost but more importantly, it distributes tasks as even as possible among heterogeneous participants who have different capacities and costs to execute tasks. To tackle this fair minimum cost allocation problem we analyze and solve it in two parts using two novel polynomial-time algorithms. We show that despite the new fairness criterion, the proposed algorithms can solve the fair minimum cost allocation problem optimally in polynomial time. In addition, we conduct an extensive set of experiments to investigate the trade-off between cost minimization and fairness. Our experimental results demonstrate the benefit of factoring fairness into task allocation. Among the majority of test instances, fairness comes with a very small price in terms of cost.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07751",
        "title": "Pignistic Probability Transforms for Mixes of Low- and High-Probability Events",
        "authors": [
            "John J. Sudano"
        ],
        "abstract": "In some real world information fusion situations, time critical decisions must be made with an incomplete information set. Belief function theories (e.g., Dempster-Shafer theory of evidence, Transferable Belief Model) have been shown to provide a reasonable methodology for processing or fusing the quantitative clues or information measurements that form the incomplete information set. For decision making, the pignistic (from the Latin pignus, a bet) probability transform has been shown to be a good method of using Beliefs or basic belief assignments (BBAs) to make decisions. For many systems, one need only address the most-probable elements in the set. For some critical systems, one must evaluate the risk of wrong decisions and establish safe probability thresholds for decision making. This adds a greater complexity to decision making, since one must address all elements in the set that are above the risk decision threshold. The problem is greatly simplified if most of the probabilities fall below this threshold. Finding a probability transform that properly represents mixes of low- and high-probability events is essential. This article introduces four new pignistic probability transforms with an implementation that uses the latest values of Beliefs, Plausibilities, or BBAs to improve the pignistic probability estimates. Some of them assign smaller values of probabilities for smaller values of Beliefs or BBAs than the Smets pignistic transform. They also assign higher probability values for larger values of Beliefs or BBAs than the Smets pignistic transform. These probability transforms will assign a value of probability that converges faster to the values below the risk threshold. A probability information content (PIC) variable is also introduced that assigns an information content value to any set of probability. Four operators are defined to help simplify the derivations.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07872",
        "title": "Towards combinatorial clustering: preliminary research survey",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper describes clustering problems from the combinatorial viewpoint. A brief systemic survey is presented including the following: (i) basic clustering problems (e.g., classification, clustering, sorting, clustering with an order over cluster), (ii) basic approaches to assessment of objects and object proximities (i.e., scales, comparison, aggregation issues), (iii) basic approaches to evaluation of local quality characteristics for clusters and total quality characteristics for clustering solutions, (iv) clustering as multicriteria optimization problem, (v) generalized modular clustering framework, (vi) basic clustering models/methods (e.g., hierarchical clustering, k-means clustering, minimum spanning tree based clustering, clustering as assignment, detection of clisue/quasi-clique based clustering, correlation clustering, network communities based clustering), Special attention is targeted to formulation of clustering as multicriteria optimization models. Combinatorial optimization models are used as auxiliary problems (e.g., assignment, partitioning, knapsack problem, multiple choice problem, morphological clique problem, searching for consensus/median for structures). Numerical examples illustrate problem formulations, solving methods, and applications. The material can be used as follows: (a) a research survey, (b) a fundamental for designing the structure/architecture of composite modular clustering software, (c) a bibliography reference collection, and (d) a tutorial.\n    ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00091",
        "title": "Sistem penunjang keputusan kelayakan pemberian pinjaman dengna metode fuzzy tsukamoto",
        "authors": [
            "Tri Murti",
            "Leon Andretti Abdillah",
            "Muhammad Sobri"
        ],
        "abstract": "Decision support systems (DSS) can be used to help settlement issues or decisions that are semi-structured or structured. The method used is Fuzzy Tsukamoto. PT Triprima Finance is a company engaged in the service sector lending with collateral in the form of Motor Vehicle Owner Book or car (reg). PT. Triprima Finance should consider borrowing from its customers with the consent of the head manager. Such approval requires a long time because they have to pass through many stages of the reporting procedure. Decision-making activities at PT Triprima Finance carried out by the analysis process manually. To help overcome these problems, the need for completion method in accuracy and speed of decision making feasibility of lending. To overcome this need to develop a new system that is a decision support system Tsukamoto fuzzy method. is expected to facilitate kaposko to determine the decisions to be taken.\n    ",
        "submission_date": "2015-05-30T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00099",
        "title": "A Novel Energy Aware Node Clustering Algorithm for Wireless Sensor Networks Using a Modified Artificial Fish Swarm Algorithm",
        "authors": [
            "Reza Azizi",
            "Hasan Sedghi",
            "Hamid Shoja",
            "Alireza Sepas-Moghaddam"
        ],
        "abstract": "Clustering problems are considered amongst the prominent challenges in statistics and computational science. Clustering of nodes in wireless sensor networks which is used to prolong the life-time of networks is one of the difficult tasks of clustering procedure. In order to perform nodes clustering, a number of nodes are determined as cluster heads and other ones are joined to one of these heads, based on different criteria e.g. Euclidean distance. So far, different approaches have been proposed for this process, where swarm and evolutionary algorithms contribute in this regard. In this study, a novel algorithm is proposed based on Artificial Fish Swarm Algorithm (AFSA) for clustering procedure. In the proposed method, the performance of the standard AFSA is improved by increasing balance between local and global searches. Furthermore, a new mechanism has been added to the base algorithm for improving convergence speed in clustering problems. Performance of the proposed technique is compared to a number of state-of-the-art techniques in this field and the outcomes indicate the supremacy of the proposed technique.\n    ",
        "submission_date": "2015-05-30T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00301",
        "title": "Interactive Knowledge Base Population",
        "authors": [
            "Travis Wolfe",
            "Mark Dredze",
            "James Mayfield",
            "Paul McNamee",
            "Craig Harman",
            "Tim Finin",
            "Benjamin Van Durme"
        ],
        "abstract": "Most work on building knowledge bases has focused on collecting entities and facts from as large a collection of documents as possible. We argue for and describe a new paradigm where the focus is on a high-recall extraction over a small collection of documents under the supervision of a human expert, that we call Interactive Knowledge Base Population (IKBP).\n    ",
        "submission_date": "2015-05-31T00:00:00",
        "last_modified_date": "2015-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00337",
        "title": "On Distributive Subalgebras of Qualitative Spatial and Temporal Calculi",
        "authors": [
            "Zhiguo Long",
            "Sanjiang Li"
        ],
        "abstract": "Qualitative calculi play a central role in representing and reasoning about qualitative spatial and temporal knowledge. This paper studies distributive subalgebras of qualitative calculi, which are subalgebras in which (weak) composition distributives over nonempty intersections. It has been proven for RCC5 and RCC8 that path consistent constraint network over a distributive subalgebra is always minimal and globally consistent (in the sense of strong $n$-consistency) in a qualitative sense. The well-known subclass of convex interval relations provides one such an example of distributive subalgebras. This paper first gives a characterisation of distributive subalgebras, which states that the intersection of a set of $n\\geq 3$ relations in the subalgebra is nonempty if and only if the intersection of every two of these relations is nonempty. We further compute and generate all maximal distributive subalgebras for Point Algebra, Interval Algebra, RCC5 and RCC8, Cardinal Relation Algebra, and Rectangle Algebra. Lastly, we establish two nice properties which will play an important role in efficient reasoning with constraint networks involving a large number of variables.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00366",
        "title": "Formal Concept Analysis for Knowledge Discovery from Biological Data",
        "authors": [
            "Khalid Raza"
        ],
        "abstract": "Due to rapid advancement in high-throughput techniques, such as microarrays and next generation sequencing technologies, biological data are increasing exponentially. The current challenge in computational biology and bioinformatics research is how to analyze these huge raw biological data to extract biologically meaningful knowledge. This review paper presents the applications of formal concept analysis for the analysis and knowledge discovery from biological data, including gene expression discretization, gene co-expression mining, gene expression clustering, finding genes in gene regulatory networks, enzyme/protein classifications, binding site classifications, and so on. It also presents a list of FCA-based software tools applied in biological domain and covers the challenges faced so far.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00529",
        "title": "Desirability and the birth of incomplete preferences",
        "authors": [
            "Marco Zaffalon",
            "Enrique Miranda"
        ],
        "abstract": "We establish an equivalence between two seemingly different theories: one is the traditional axiomatisation of incomplete preferences on horse lotteries based on the mixture independence axiom; the other is the theory of desirable gambles developed in the context of imprecise probability. The equivalence allows us to revisit incomplete preferences from the viewpoint of desirability and through the derived notion of coherent lower previsions. On this basis, we obtain new results and insights: in particular, we show that the theory of incomplete preferences can be developed assuming only the existence of a worst act---no best act is needed---, and that a weakened Archimedean axiom suffices too; this axiom allows us also to address some controversy about the regularity assumption (that probabilities should be positive---they need not), which enables us also to deal with uncountable possibility spaces; we show that it is always possible to extend in a minimal way a preference relation to one with a worst act, and yet the resulting relation is never Archimedean, except in a trivial case; we show that the traditional notion of state independence coincides with the notion called strong independence in imprecise probability---this leads us to give much a weaker definition of state independence than the traditional one; we rework and uniform the notions of complete preferences, beliefs, values; we argue that Archimedeanity does not capture all the problems that can be modelled with sets of expected utilities and we provide a new notion that does precisely that. Perhaps most importantly, we argue throughout that desirability is a powerful and natural setting to model, and work with, incomplete preferences, even in case of non-Archimedean problems. This leads us to suggest that desirability, rather than preference, should be the primitive notion at the basis of decision-theoretic axiomatisations.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00711",
        "title": "Quantifying Creativity in Art Networks",
        "authors": [
            "Ahmed Elgammal",
            "Babak Saleh"
        ],
        "abstract": "Can we develop a computer algorithm that assesses the creativity of a painting given its context within art history? This paper proposes a novel computational framework for assessing the creativity of creative products, such as paintings, sculptures, poetry, etc. We use the most common definition of creativity, which emphasizes the originality of the product and its influential value. The proposed computational framework is based on constructing a network between creative products and using this network to infer about the originality and influence of its nodes. Through a series of transformations, we construct a Creativity Implication Network. We show that inference about creativity in this network reduces to a variant of network centrality problems which can be solved efficiently. We apply the proposed framework to the task of quantifying creativity of paintings (and sculptures). We experimented on two datasets with over 62K paintings to illustrate the behavior of the proposed framework. We also propose a methodology for quantitatively validating the results of the proposed algorithm, which we call the \"time machine experiment\".\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00858",
        "title": "Stochastic And-Or Grammars: A Unified Framework and Logic Perspective",
        "authors": [
            "Kewei Tu"
        ],
        "abstract": "Stochastic And-Or grammars (AOG) extend traditional stochastic grammars of language to model other types of data such as images and events. In this paper we propose a representation framework of stochastic AOGs that is agnostic to the type of the data being modeled and thus unifies various domain-specific AOGs. Many existing grammar formalisms and probabilistic models in natural language processing, computer vision, and machine learning can be seen as special cases of this framework. We also propose a domain-independent inference algorithm of stochastic context-free AOGs and show its tractability under a reasonable assumption. Furthermore, we provide two interpretations of stochastic context-free AOGs as a subset of probabilistic logic, which connects stochastic AOGs to the field of statistical relational learning and clarifies their relation with a few existing statistical relational models.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00893",
        "title": "SkILL - a Stochastic Inductive Logic Learner",
        "authors": [
            "Joana C\u00f4rte-Real",
            "Theofrastos Mantadelis",
            "In\u00eas Dutra",
            "Ricardo Rocha"
        ],
        "abstract": "Probabilistic Inductive Logic Programming (PILP) is a rel- atively unexplored area of Statistical Relational Learning which extends classic Inductive Logic Programming (ILP). This work introduces SkILL, a Stochastic Inductive Logic Learner, which takes probabilistic annotated data and produces First Order Logic theories. Data in several domains such as medicine and bioinformatics have an inherent degree of uncer- tainty, that can be used to produce models closer to reality. SkILL can not only use this type of probabilistic data to extract non-trivial knowl- edge from databases, but it also addresses efficiency issues by introducing a novel, efficient and effective search strategy to guide the search in PILP environments. The capabilities of SkILL are demonstrated in three dif- ferent datasets: (i) a synthetic toy example used to validate the system, (ii) a probabilistic adaptation of a well-known biological metabolism ap- plication, and (iii) a real world medical dataset in the breast cancer domain. Results show that SkILL can perform as well as a deterministic ILP learner, while also being able to incorporate probabilistic knowledge that would otherwise not be considered.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00999",
        "title": "Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases",
        "authors": [
            "Alberto Garcia-Duran",
            "Antoine Bordes",
            "Nicolas Usunier",
            "Yves Grandvalet"
        ],
        "abstract": "This paper tackles the problem of endogenous link prediction for Knowledge Base completion. Knowledge Bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose Tatec a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01056",
        "title": "Performing Bayesian Risk Aggregation using Discrete Approximation Algorithms with Graph Factorization",
        "authors": [
            "Peng Lin"
        ],
        "abstract": "Risk aggregation is a popular method used to estimate the sum of a collection of financial assets or events, where each asset or event is modelled as a random variable. Applications, in the financial services industry, include insurance, operational risk, stress testing, and sensitivity analysis, but the problem is widely encountered in many other application domains. This thesis has contributed two algorithms to perform Bayesian risk aggregation when model exhibit hybrid dependency and high dimensional inter-dependency. The first algorithm operates on a subset of the general problem, with an emphasis on convolution problems, in the presence of continuous and discrete variables (so called hybrid models) and the second algorithm offer a universal method for general purpose inference over much wider classes of Bayesian Network models.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01062",
        "title": "Quizz: Targeted crowdsourcing with a billion (potential) users",
        "authors": [
            "Panagiotis G. Ipeirotis",
            "Evgeniy Gabrilovich"
        ],
        "abstract": "We describe Quizz, a gamified crowdsourcing system that simultaneously assesses the knowledge of users and acquires new knowledge from them. Quizz operates by asking users to complete short quizzes on specific topics; as a user answers the quiz questions, Quizz estimates the user's competence. To acquire new knowledge, Quizz also incorporates questions for which we do not have a known answer; the answers given by competent users provide useful signals for selecting the correct answers for these questions. Quizz actively tries to identify knowledgeable users on the Internet by running advertising campaigns, effectively leveraging the targeting capabilities of existing, publicly available, ad placement services. Quizz quantifies the contributions of the users using information theory and sends feedback to the advertisingsystem about each user. The feedback allows the ad targeting mechanism to further optimize ad placement.\n",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01071",
        "title": "Fast Generation of Best Interval Patterns for Nonmonotonic Constraints",
        "authors": [
            "Aleksey Buzmakov",
            "Sergei O. Kuznetsov",
            "Amedeo Napoli"
        ],
        "abstract": "In pattern mining, the main challenge is the exponential explosion of the set of patterns. Typically, to solve this problem, a constraint for pattern selection is introduced. One of the first constraints proposed in pattern mining is support (frequency) of a pattern in a dataset. Frequency is an anti-monotonic function, i.e., given an infrequent pattern, all its superpatterns are not frequent. However, many other constraints for pattern selection are not (anti-)monotonic, which makes it difficult to generate patterns satisfying these constraints. In this paper we introduce the notion of projection-antimonotonicity and $\\theta$-$\\Sigma\u00f8\\phi\\iota\\alpha$ algorithm that allows efficient generation of the best patterns for some nonmonotonic constraints. In this paper we consider stability and $\\Delta$-measure, which are nonmonotonic constraints, and apply them to interval tuple datasets. In the experiments, we compute best interval tuple patterns w.r.t. these measures and show the advantage of our approach over postfiltering approaches.\n",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01245",
        "title": "A density compensation-based path computing model for measuring semantic similarity",
        "authors": [
            "Xinhua Zhu",
            "Fei Li",
            "Hongchao Chen",
            "Qi Peng"
        ],
        "abstract": "The shortest path between two concepts in a taxonomic ontology is commonly used to represent the semantic distance between concepts in the edge-based semantic similarity measures. In the past, the edge counting is considered to be the default method for the path computation, which is simple, intuitive and has low computational complexity. However, a large lexical taxonomy of such as WordNet has the irregular densities of links between concepts due to its broad domain but. The edge counting-based path computation is powerless for this non-uniformity problem. In this paper, we advocate that the path computation is able to be separated from the edge-based similarity measures and form various general computing models. Therefore, in order to solve the problem of non-uniformity of concept density in a large taxonomic ontology, we propose a new path computing model based on the compensation of local area density of concepts, which is equal to the number of direct hyponyms of the subsumers of concepts in their shortest path. This path model considers the local area density of concepts as an extension of the edge-based path and converts the local area density divided by their depth into the compensation for edge-based path with an adjustable parameter, which idea has been proven to be consistent with the information theory. This model is a general path computing model and can be applied in various edge-based similarity algorithms. The experiment results show that the proposed path model improves the average correlation between edge-based measures with human judgments on Miller and Charles benchmark from less than 0.8 to more than 0.85, and has a big advantage in efficiency than information content (IC) computation in a dynamic ontology, thereby successfully solving the non-uniformity problem of taxonomic ontology.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01432",
        "title": "Encoding Markov Logic Networks in Possibilistic Logic",
        "authors": [
            "Ondrej Kuzelka",
            "Jesse Davis",
            "Steven Schockaert"
        ],
        "abstract": "Markov logic uses weighted formulas to compactly encode a probability distribution over possible worlds. Despite the use of logical formulas, Markov logic networks (MLNs) can be difficult to interpret, due to the often counter-intuitive meaning of their weights. To address this issue, we propose a method to construct a possibilistic logic theory that exactly captures what can be derived from a given MLN using maximum a posteriori (MAP) inference. Unfortunately, the size of this theory is exponential in general. We therefore also propose two methods which can derive compact theories that still capture MAP inference, but only for specific types of evidence. These theories can be used, among others, to make explicit the hidden assumptions underlying an MLN or to explain the predictions it makes.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01864",
        "title": "Grid-based angle-constrained path planning",
        "authors": [
            "Konstantin Yakovlev",
            "Egor Baskin",
            "Ivan Hramoin"
        ],
        "abstract": "Square grids are commonly used in robotics and game development as spatial models and well known in AI community heuristic search algorithms (such as A*, JPS, Theta* etc.) are widely used for path planning on grids. A lot of research is concentrated on finding the shortest (in geometrical sense) paths while in many applications finding smooth paths (rather than the shortest ones but containing sharp turns) is preferable. In this paper we study the problem of generating smooth paths and concentrate on angle constrained path planning. We put angle-constrained path planning problem formally and present a new algorithm tailored to solve it - LIAN. We examine LIAN both theoretically and empirically. We show that it is sound and complete (under some restrictions). We also show that LIAN outperforms the analogues when solving numerous path planning tasks within urban outdoor navigation scenarios.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02060",
        "title": "Similarity, Cardinality and Entropy for Bipolar Fuzzy Set in the Framework of Penta-valued Representation",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "In this paper one presents new similarity, cardinality and entropy measures for bipolar fuzzy set and for its particular forms like intuitionistic, paraconsistent and fuzzy set. All these are constructed in the framework of multi-valued representations and are based on a penta-valued logic that uses the following logical values: true, false, unknown, contradictory and ambiguous. Also a new distance for bounded real interval was defined.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02061",
        "title": "Entropy and Syntropy in the Context of Five-Valued Logics",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "This paper presents a five-valued representation of bifuzzy sets. This representation is related to a five-valued logic that uses the following values: true, false, inconsistent, incomplete and ambiguous. In the framework of five-valued representation, formulae for similarity, entropy and syntropy of bifuzzy sets are constructed.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02082",
        "title": "A Real-time Cargo Damage Management System via a Sorting Array Triangulation Technique",
        "authors": [
            "Philip B. Alipour",
            "Matteus Magnusson",
            "Martin W. Olsson",
            "Nooshin H. Ghasemi",
            "Lawrence Henesey"
        ],
        "abstract": "This report covers an intelligent decision support system (IDSS), which handles an efficient and effective way to rapidly inspect containerized cargos for defection. Defection is either cargo exposure to radiation, physical damages such as holes, punctured surfaces, iron surface oxidation, etc. The system uses a sorting array triangulation technique (SAT) and surface damage detection (SDD) to conduct the inspection. This new technique saves time and money on finding damaged goods during transportation such that, instead of running $n$ inspections on $n$ containers, only 3 inspections per triangulation or a ratio of $3:n$ is required, assuming $n > 3$ containers. The damaged stack in the array is virtually detected contiguous to an actually-damaged cargo by calculating nearby distances of such cargos, delivering reliable estimates for the whole local stack population. The estimated values on damaged, somewhat damaged and undamaged cargo stacks, are listed and profiled after being sorted by the program, thereby submitted to the manager for a final decision. The report describes the problem domain and the implementation of the simulator prototype, showing how the system operates via software, hardware with/without human agents, conducting real-time inspections and management per se.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02188",
        "title": "Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach",
        "authors": [
            "Yinlam Chow",
            "Aviv Tamar",
            "Shie Mannor",
            "Marco Pavone"
        ],
        "abstract": "In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.\n    ",
        "submission_date": "2015-06-06T00:00:00",
        "last_modified_date": "2015-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02312",
        "title": "A Framework for Constrained and Adaptive Behavior-Based Agents",
        "authors": [
            "Renato de Pontes Pereira",
            "Paulo Martins Engel"
        ],
        "abstract": "Behavior Trees are commonly used to model agents for robotics and games, where constrained behaviors must be designed by human experts in order to guarantee that these agents will execute a specific chain of actions given a specific set of perceptions. In such application areas, learning is a desirable feature to provide agents with the ability to adapt and improve interactions with humans and environment, but often discarded due to its unreliability. In this paper, we propose a framework that uses Reinforcement Learning nodes as part of Behavior Trees to address the problem of adding learning capabilities in constrained agents. We show how this framework relates to Options in Hierarchical Reinforcement Learning, ensuring convergence of nested learning nodes, and we empirically show that the learning nodes do not affect the execution of other nodes in the tree.\n    ",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2015-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02465",
        "title": "ASlib: A Benchmark Library for Algorithm Selection",
        "authors": [
            "Bernd Bischl",
            "Pascal Kerschke",
            "Lars Kotthoff",
            "Marius Lindauer",
            "Yuri Malitsky",
            "Alexandre Frechette",
            "Holger Hoos",
            "Frank Hutter",
            "Kevin Leyton-Brown",
            "Kevin Tierney",
            "Joaquin Vanschoren"
        ],
        "abstract": "The task of algorithm selection involves choosing an algorithm from a set of algorithms on a per-instance basis in order to exploit the varying performance of algorithms over a set of instances. The algorithm selection problem is attracting increasing attention from researchers and practitioners in AI. Years of fruitful applications in a number of domains have resulted in a large amount of data, but the community lacks a standard format or repository for this data. This situation makes it difficult to share and compare different approaches effectively, as is done in other, more established fields. It also unnecessarily hinders new researchers who want to work in this area. To address this problem, we introduce a standardized format for representing algorithm selection scenarios and a repository that contains a growing number of data sets from the literature. Our format has been designed to be able to express a wide variety of different scenarios. Demonstrating the breadth and power of our platform, we describe a set of example experiments that build and evaluate algorithm selection models through a common interface. The results display the potential of algorithm selection to achieve significant performance improvements across a broad range of problems and algorithms.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02561",
        "title": "On SAT Models Enumeration in Itemset Mining",
        "authors": [
            "Said Jabbour",
            "Lakhdar Sais",
            "Yakoub Salhi"
        ],
        "abstract": "Frequent itemset mining is an essential part of data analysis and data mining. Recent works propose interesting SAT-based encodings for the problem of discovering frequent itemsets. Our aim in this work is to define strategies for adapting SAT solvers to such encodings in order to improve models enumeration. In this context, we deeply study the effects of restart, branching heuristics and clauses learning. We then conduct an experimental evaluation on SAT-Based itemset mining instances to show how SAT solvers can be adapted to obtain an efficient SAT model enumerator.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02639",
        "title": "New Limits for Knowledge Compilation and Applications to Exact Model Counting",
        "authors": [
            "Paul Beame",
            "Vincent Liew"
        ],
        "abstract": "We show new limits on the efficiency of using current techniques to make exact probabilistic inference for large classes of natural problems. In particular we show new lower bounds on knowledge compilation to SDD and DNNF forms. We give strong lower bounds on the complexity of SDD representations by relating SDD size to best-partition communication complexity. We use this relationship to prove exponential lower bounds on the SDD size for representing a large class of problems that occur naturally as queries over probabilistic databases. A consequence is that for representing unions of conjunctive queries, SDDs are not qualitatively more concise than OBDDs. We also derive simple examples for which SDDs must be exponentially less concise than FBDDs. Finally, we derive exponential lower bounds on the sizes of DNNF representations using a new quasipolynomial simulation of DNNFs by nondeterministic FBDDs.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02850",
        "title": "Adversarial patrolling with spatially uncertain alarm signals",
        "authors": [
            "Nicola Basilico",
            "Giuseppe De Nittis",
            "Nicola Gatti"
        ],
        "abstract": "When securing complex infrastructures or large environments, constant surveillance of every area is not affordable. To cope with this issue, a common countermeasure is the usage of cheap but wide-ranged sensors, able to detect suspicious events that occur in large areas, supporting patrollers to improve the effectiveness of their strategies. However, such sensors are commonly affected by uncertainty. In the present paper, we focus on spatially uncertain alarm signals. That is, the alarm system is able to detect an attack but it is uncertain on the exact position where the attack is taking place. This is common when the area to be secured is wide such as in border patrolling and fair site surveillance. We propose, to the best of our knowledge, the first Patrolling Security Game model where a Defender is supported by a spatially uncertain alarm system which non-deterministically generates signals once a target is under attack. We show that finding the optimal strategy in arbitrary graphs is APX-hard even in zero-sum games and we provide two (exponential time) exact algorithms and two (polynomial time) approximation algorithms. Furthermore, we analyse what happens in environments with special topologies, showing that in linear and cycle graphs the optimal patrolling strategy can be found in polynomial time, de facto allowing our algorithms to be used in real-life scenarios, while in trees the problem is NP-hard. Finally, we show that without false positives and missed detections, the best patrolling strategy reduces to stay in a place, wait for a signal, and respond to it at best. This strategy is optimal even with non-negligible missed detection rates, which, unfortunately, affect every commercial alarm system. We evaluate our methods in simulation, assessing both quantitative and qualitative aspects.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02930",
        "title": "Arguments for the Effectiveness of Human Problem Solving",
        "authors": [
            "Frantisek Duris"
        ],
        "abstract": "The question of how humans solve problem has been addressed extensively. However, the direct study of the effectiveness of this process seems to be overlooked. In this paper, we address the issue of the effectiveness of human problem solving: we analyze where this effectiveness comes from and what cognitive mechanisms or heuristics are involved. Our results are based on the optimal probabilistic problem solving strategy that appeared in Solomonoff paper on general problem solving system. We provide arguments that a certain set of cognitive mechanisms or heuristics drive human problem solving in the similar manner as the optimal Solomonoff strategy. The results presented in this paper can serve both cognitive psychology in better understanding of human problem solving processes as well as artificial intelligence in designing more human-like agents.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03041",
        "title": "The Wreath Process: A totally generative model of geometric shape based on nested symmetries",
        "authors": [
            "Diana Borsa",
            "Thore Graepel",
            "Andrew Gordon"
        ],
        "abstract": "We consider the problem of modelling noisy but highly symmetric shapes that can be viewed as hierarchies of whole-part relationships in which higher level objects are composed of transformed collections of lower level objects. To this end, we propose the stochastic wreath process, a fully generative probabilistic model of drawings. Following Leyton's \"Generative Theory of Shape\", we represent shapes as sequences of transformation groups composed through a wreath product.\n",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03140",
        "title": "On-the-Job Learning with Bayesian Decision Theory",
        "authors": [
            "Keenon Werling",
            "Arun Chaganty",
            "Percy Liang",
            "Chris Manning"
        ],
        "abstract": "Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an \"on-the-job\" setting, where as inputs arrive, we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets---named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F1 improvement over having a single human label the whole set, and a 28% F1 improvement over online learning.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03425",
        "title": "Fast Online Clustering with Randomized Skeleton Sets",
        "authors": [
            "Krzysztof Choromanski",
            "Sanjiv Kumar",
            "Xiaofeng Liu"
        ],
        "abstract": "We present a new fast online clustering algorithm that reliably recovers arbitrary-shaped data clusters in high throughout data streams. Unlike the existing state-of-the-art online clustering methods based on k-means or k-medoid, it does not make any restrictive generative assumptions. In addition, in contrast to existing nonparametric clustering techniques such as DBScan or DenStream, it gives provable theoretical guarantees. To achieve fast clustering, we propose to represent each cluster by a skeleton set which is updated continuously as new data is seen. A skeleton set consists of weighted samples from the data where weights encode local densities. The size of each skeleton set is adapted according to the cluster geometry. The proposed technique automatically detects the number of clusters and is robust to outliers. The algorithm works for the infinite data stream where more than one pass over the data is not feasible. We provide theoretical guarantees on the quality of the clustering and also demonstrate its advantage over the existing state-of-the-art on several datasets.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03624",
        "title": "Bootstrapping Skills",
        "authors": [
            "Daniel J. Mankowitz",
            "Timothy A. Mann",
            "Shie Mannor"
        ],
        "abstract": "The monolithic approach to policy representation in Markov Decision Processes (MDPs) looks for a single policy that can be represented as a function from states to actions. For the monolithic approach to succeed (and this is not always possible), a complex feature representation is often necessary since the policy is a complex object that has to prescribe what actions to take all over the state space. This is especially true in large domains with complicated dynamics. It is also computationally inefficient to both learn and plan in MDPs using a complex monolithic approach. We present a different approach where we restrict the policy space to policies that can be represented as combinations of simpler, parameterized skills---a type of temporally extended action, with a simple policy representation. We introduce Learning Skills via Bootstrapping (LSB) that can use a broad family of Reinforcement Learning (RL) algorithms as a \"black box\" to iteratively learn parametrized skills. Initially, the learned skills are short-sighted but each iteration of the algorithm allows the skills to bootstrap off one another, improving each skill in the process. We prove that this bootstrapping process returns a near-optimal policy. Furthermore, our experiments demonstrate that LSB can solve MDPs that, given the same representational power, could not be solved by a monolithic approach. Thus, planning with learned skills results in better policies without requiring complex policy representations.\n    ",
        "submission_date": "2015-06-11T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03879",
        "title": "Leading Tree in DPCLUS and Its Impact on Building Hierarchies",
        "authors": [
            "Ji Xu",
            "Guoyin Wang"
        ],
        "abstract": "This paper reveals the tree structure as an intermediate result of clustering by fast search and find of density peaks (DPCLUS), and explores the power of using this tree to perform hierarchical clustering. The array used to hold the index of the nearest higher-densitied object for each object can be transformed into a Leading Tree (LT), in which each parent node P leads its child nodes to join the same cluster as P itself, and the child nodes are sorted by their gamma values in descendant order to accelerate the disconnecting of root in each subtree. There are two major advantages with the LT: One is dramatically reducing the running time of assigning noncenter data points to their cluster ID, because the assigning process is turned into just disconnecting the links from each center to its parent. The other is that the tree model for representing clusters is more informative. Because we can check which objects are more likely to be selected as centers in finer grained clustering, or which objects reach to its center via less jumps. Experiment results and analysis show the effectiveness and efficiency of the assigning process with an LT.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03949",
        "title": "New Results for Domineering from Combinatorial Game Theory Endgame Databases",
        "authors": [
            "Jos Uiterwijk",
            "Michael Barton"
        ],
        "abstract": "We have constructed endgame databases for all single-component positions up to 15 squares for Domineering, filled with exact Combinatorial Game Theory (CGT) values in canonical form. The most important findings are as follows.\n",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04272",
        "title": "Attacker and Defender Counting Approach for Abstract Argumentation",
        "authors": [
            "Fuan Pu",
            "Jian Luo",
            "Yulai Zhang",
            "Guiming Luo"
        ],
        "abstract": "In Dung's abstract argumentation, arguments are either acceptable or unacceptable, given a chosen notion of acceptability. This gives a coarse way to compare arguments. In this paper, we propose a counting approach for a more fine-gained assessment to arguments by counting the number of their respective attackers and defenders based on argument graph and argument game. An argument is more acceptable if the proponent puts forward more number of defenders for it and the opponent puts forward less number of attackers against it. We show that our counting model has two well-behaved properties: normalization and convergence. Then, we define a counting semantics based on this model, and investigate some general properties of the semantics.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2015-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04366",
        "title": "Artificial general intelligence through recursive data compression and grounded reasoning: a position paper",
        "authors": [
            "Arthur Franz"
        ],
        "abstract": "This paper presents a tentative outline for the construction of an artificial, generally intelligent system (AGI). It is argued that building a general data compression algorithm solving all problems up to a complexity threshold should be the main thrust of research. A measure for partial progress in AGI is suggested. Although the details are far from being clear, some general properties for a general compression algorithm are fleshed out. Its inductive bias should be flexible and adapt to the input data while constantly searching for a simple, orthogonal and complete set of hypotheses explaining the data. It should recursively reduce the size of its representations thereby compressing the data increasingly at every iteration.\n",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04929",
        "title": "ASPMT(QS): Non-Monotonic Spatial Reasoning with Answer Set Programming Modulo Theories",
        "authors": [
            "Przemys\u0142aw Andrzej Wa\u0142\u0119ga",
            "Mehul Bhatt",
            "Carl Schultz"
        ],
        "abstract": "The systematic modelling of \\emph{dynamic spatial systems} [9] is a key requirement in a wide range of application areas such as comonsense cognitive robotics, computer-aided architecture design, dynamic geographic information systems. We present ASPMT(QS), a novel approach and fully-implemented prototype for non-monotonic spatial reasoning ---a crucial requirement within dynamic spatial systems-- based on Answer Set Programming Modulo Theories (ASPMT). ASPMT(QS) consists of a (qualitative) spatial representation module (QS) and a method for turning tight ASPMT instances into Sat Modulo Theories (SMT) instances in order to compute stable models by means of SMT solvers. We formalise and implement concepts of default spatial reasoning and spatial frame axioms using choice formulas. Spatial reasoning is performed by encoding spatial relations as systems of polynomial constraints, and solving via SMT with the theory of real nonlinear arithmetic. We empirically evaluate ASPMT(QS) in comparison with other prominent contemporary spatial reasoning systems. Our results show that ASPMT(QS) is the only existing system that is capable of reasoning about indirect spatial effects (i.e. addressing the ramification problem), and integrating geometric and qualitative spatial information within a non-monotonic spatial reasoning context.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04945",
        "title": "Spatial Symmetry Driven Pruning Strategies for Efficient Declarative Spatial Reasoning",
        "authors": [
            "Carl Schultz",
            "Mehul Bhatt"
        ],
        "abstract": "Declarative spatial reasoning denotes the ability to (declaratively) specify and solve real-world problems related to geometric and qualitative spatial representation and reasoning within standard knowledge representation and reasoning (KR) based methods (e.g., logic programming and derivatives). One approach for encoding the semantics of spatial relations within a declarative programming framework is by systems of polynomial constraints. However, solving such constraints is computationally intractable in general (i.e. the theory of real-closed fields).\n",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04956",
        "title": "The Scope and Limits of Simulation in Cognitive Models",
        "authors": [
            "Ernest Davis",
            "Gary Marcus"
        ],
        "abstract": "It has been proposed that human physical reasoning consists largely of running \"physics engines in the head\" in which the future trajectory of the physical system under consideration is computed precisely using accurate scientific theories. In such models, uncertainty and incomplete knowledge is dealt with by sampling probabilistically over the space of possible trajectories (\"Monte Carlo simulation\"). We argue that such simulation-based models are too weak, in that there are many important aspects of human physical reasoning that cannot be carried out this way, or can only be carried out very inefficiently; and too strong, in that humans make large systematic errors that the models cannot account for. We conclude that simulation-based reasoning makes up at most a small part of a larger system that encompasses a wide range of additional cognitive processes.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05282",
        "title": "Why Bother With Syntax?",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "This short note discusses the role of syntax vs. semantics and the interplay between logic, philosophy, and language in computer science and game theory.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05382",
        "title": "Early Predictions of Movie Success: the Who, What, and When of Profitability",
        "authors": [
            "Michael T. Lash",
            "Kang Zhao"
        ],
        "abstract": "This paper proposes a decision support system to aid movie investment decisions at the early stage of movie productions. The system predicts the success of a movie based on its profitability by leveraging historical data from various sources. Using social network analysis and text mining techniques, the system automatically extracts several groups of features, including \"who\" are on the cast, \"what\" a movie is about, \"when\" a movie will be released, as well as \"hybrid\" features that match \"who\" with \"what\", and \"when\" with \"what\". Experiment results with movies during an 11-year period showed that the system outperforms benchmark methods by a large margin in predicting movie profitability. Novel features we proposed also made great contributions to the prediction. In addition to designing a decision support system with practical utilities, our analysis of key factors for movie profitability may also have implications for theoretical research on team performance and the success of creative work.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2016-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05846",
        "title": "Automated Assignment of Backbone NMR Data using Artificial Intelligence",
        "authors": [
            "John Emmons",
            "Steven Johnson",
            "Timothy Urness",
            "Adina Kilpatrick"
        ],
        "abstract": "Nuclear magnetic resonance (NMR) spectroscopy is a powerful method for the investigation of three-dimensional structures of biological molecules such as proteins. Determining a protein structure is essential for understanding its function and alterations in function which lead to disease. One of the major challenges of the post-genomic era is to obtain structural and functional information on the many unknown proteins encoded by thousands of newly identified genes. The goal of this research is to design an algorithm capable of automating the analysis of backbone protein NMR data by implementing AI strategies such as greedy and A* search.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05851",
        "title": "Smart Pacing for Effective Online Ad Campaign Optimization",
        "authors": [
            "Jian Xu",
            "Kuang-chih Lee",
            "Wentong Li",
            "Hang Qi",
            "Quan Lu"
        ],
        "abstract": "In targeted online advertising, advertisers look for maximizing campaign performance under delivery constraint within budget schedule. Most of the advertisers typically prefer to impose the delivery constraint to spend budget smoothly over the time in order to reach a wider range of audiences and have a sustainable impact. Since lots of impressions are traded through public auctions for online advertising today, the liquidity makes price elasticity and bid landscape between demand and supply change quite dynamically. Therefore, it is challenging to perform smooth pacing control and maximize campaign performance simultaneously. In this paper, we propose a smart pacing approach in which the delivery pace of each campaign is learned from both offline and online data to achieve smooth delivery and optimal performance goals. The implementation of the proposed approach in a real DSP system is also presented. Experimental evaluations on both real online ad campaigns and offline simulations show that our approach can effectively improve campaign performance and achieve delivery goals.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05908",
        "title": "Deep Knowledge Tracing",
        "authors": [
            "Chris Piech",
            "Jonathan Spencer",
            "Jonathan Huang",
            "Surya Ganguli",
            "Mehran Sahami",
            "Leonidas Guibas",
            "Jascha Sohl-Dickstein"
        ],
        "abstract": "Knowledge tracing---where a machine models the knowledge of a student as they interact with coursework---is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05969",
        "title": "HuTO: an Human Time Ontology for Semantic Web Applications",
        "authors": [
            "Papa Fary Diallo",
            "Olivier Corby",
            "Isabelle Mirbel",
            "Moussa Lo",
            "Seydina M. Ndiaye"
        ],
        "abstract": "The temporal phenomena have many facets that are studied by different communities. In Semantic Web, large heterogeneous data are handled and produced. These data often have informal, semi-formal or formal temporal information which must be interpreted by software agents. In this paper we present Human Time Ontology (HuTO) an RDFS ontology to annotate and represent temporal data. A major contribution of HuTO is the modeling of non-convex intervals giving the ability to write queries for this kind of interval. HuTO also incorporates normalization and reasoning rules to explicit certain information. HuTO also proposes an approach which associates a temporal dimension to the knowledge base content. This facilitates information retrieval by considering or not the temporal aspect.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06646",
        "title": "Nonparametric Bayesian Double Articulation Analyzer for Direct Language Acquisition from Continuous Speech Signals",
        "authors": [
            "Tadahiro Taniguchi",
            "Ryo Nakashima",
            "Shogo Nagasaka"
        ],
        "abstract": "Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the \"hierarchical Dirichlet process hidden language model\" (HDP-HLM). The HDP-HLM is obtained by extending the hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. An inference procedure for the HDP-HLM is derived using the blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. Based on the HDP-HLM and its inference procedure, we developed a novel double articulation analyzer. By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, the method can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner. The novel unsupervised double articulation analyzer is called NPB-DAA.\n",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07116",
        "title": "Scientific Discovery by Machine Intelligence: A New Avenue for Drug Research",
        "authors": [
            "Carlo A. Trugenberger"
        ],
        "abstract": "The majority of big data is unstructured and of this majority the largest chunk is text. While data mining techniques are well developed and standardized for structured, numerical data, the realm of unstructured data is still largely unexplored. The general focus lies on information extraction, which attempts to retrieve known information from text. The Holy Grail, however is knowledge discovery, where machines are expected to unearth entirely new facts and relations that were not previously known by any human expert. Indeed, understanding the meaning of text is often considered as one of the main characteristics of human intelligence. The ultimate goal of semantic artificial intelligence is to devise software that can understand the meaning of free text, at least in the practical sense of providing new, actionable information condensed out of a body of documents. As a stepping stone on the road to this vision I will introduce a totally new approach to drug research, namely that of identifying relevant information by employing a self-organizing semantic engine to text mine large repositories of biomedical research papers, a technique pioneered by Merck with the InfoCodex software. I will describe the methodology and a first successful experiment for the discovery of new biomarkers and phenotypes for diabetes and obesity on the basis of PubMed abstracts, public clinical trials and Merck internal documents. The reported approach shows much promise and has potential to impact fundamentally pharmaceutical research as a way to shorten time-to-market of novel drugs, and for early recognition of dead ends.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07359",
        "title": "Sequential Extensions of Causal and Evidential Decision Theory",
        "authors": [
            "Tom Everitt",
            "Jan Leike",
            "Marcus Hutter"
        ],
        "abstract": "Moving beyond the dualistic view in AI where agent and environment are separated incurs new challenges for decision making, as calculation of expected utility is no longer straightforward. The non-dualistic decision theory literature is split between causal decision theory and evidential decision theory. We extend these decision algorithms to the sequential setting where the agent alternates between taking actions and observing their consequences. We find that evidential decision theory has two natural extensions while causal decision theory only has one.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07990",
        "title": "Bisimulation and expressivity for conditional belief, degrees of belief, and safe belief",
        "authors": [
            "Mikkel Birkegaard Andersen",
            "Thomas Bolander",
            "Hans van Ditmarsch",
            "Martin Holm Jensen"
        ],
        "abstract": "Plausibility models are Kripke models that agents use to reason about knowledge and belief, both of themselves and of each other. Such models are used to interpret the notions of conditional belief, degrees of belief, and safe belief. The logic of conditional belief contains that modality and also the knowledge modality, and similarly for the logic of degrees of belief and the logic of safe belief. With respect to these logics, plausibility models may contain too much information. A proper notion of bisimulation is required that characterises them. We define that notion of bisimulation and prove the required characterisations: on the class of image-finite and preimage-finite models (with respect to the plausibility relation), two pointed Kripke models are modally equivalent in either of the three logics, if and only if they are bisimilar. As a result, the information content of such a model can be similarly expressed in the logic of conditional belief, or the logic of degrees of belief, or that of safe belief. This, we found a surprising result. Still, that does not mean that the logics are equally expressive: the logics of conditional and degrees of belief are incomparable, the logics of degrees of belief and safe belief are incomparable, while the logic of safe belief is more expressive than the logic of conditional belief. In view of the result on bisimulation characterisation, this is an equally surprising result. We hope our insights may contribute to the growing community of formal epistemology and on the relation between qualitative and quantitative modelling.\n    ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08009",
        "title": "Skopus: Mining top-k sequential patterns under leverage",
        "authors": [
            "Francois Petitjean",
            "Tao Li",
            "Nikolaj Tatti",
            "Geoffrey I. Webb"
        ],
        "abstract": "This paper presents a framework for exact discovery of the top-k sequential patterns under Leverage. It combines (1) a novel definition of the expected support for a sequential pattern - a concept on which most interestingness measures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest. Our interestingness measure employs the partition approach. A pattern is interesting to the extent that it is more frequent than can be explained by assuming independence between any of the pairs of patterns from which it can be composed. The larger the support compared to the expectation under independence, the more interesting is the pattern. We build on these two elements to exactly extract the k sequential patterns with highest leverage, consistent with our definition of expected support. We conduct experiments on both synthetic data with known patterns and real-world datasets; both experiments confirm the consistency and relevance of our approach with regard to the state of the art. This article was published in Data Mining and Knowledge Discovery and is accessible at ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2018-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08030",
        "title": "Dynamic Bayesian Ontology Languages",
        "authors": [
            "\u0130smail \u0130lkan Ceylan",
            "Rafael Pe\u00f1aloza"
        ],
        "abstract": "Many formalisms combining ontology languages with uncertainty, usually in the form of probabilities, have been studied over the years. Most of these formalisms, however, assume that the probabilistic structure of the knowledge remains static over time. We present a general approach for extending ontology languages to handle time-evolving uncertainty represented by a dynamic Bayesian network. We show how reasoning in the original language and dynamic Bayesian inferences can be exploited for effective reasoning in our framework.\n    ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2015-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08813",
        "title": "Argumentation Semantics for Prioritised Default Logic",
        "authors": [
            "Anthony P. Young",
            "Sanjay Modgil",
            "Odinaldo Rodrigues"
        ],
        "abstract": "We endow prioritised default logic (PDL) with argumentation semantics using the ASPIC+ framework for structured argumentation, and prove that the conclusions of the justified arguments are exactly the prioritised default extensions. Argumentation semantics for PDL will allow for the application of argument game proof theories to the process of inference in PDL, making the reasons for accepting a conclusion transparent and the inference process more intuitive. This also opens up the possibility for argumentation-based distributed reasoning and communication amongst agents with PDL representations of mental attitudes.\n    ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08919",
        "title": "Characterization of Logic Program Revision as an Extension of Propositional Revision",
        "authors": [
            "Nicolas Schwind",
            "Katsumi Inoue"
        ],
        "abstract": "We address the problem of belief revision of logic programs, i.e., how to incorporate to a logic program P a new logic program Q. Based on the structure of SE interpretations, Delgrande et al. adapted the well-known AGM framework to logic program (LP) revision. They identified the rational behavior of LP revision and introduced some specific operators. In this paper, a constructive characterization of all rational LP revision operators is given in terms of orderings over propositional interpretations with some further conditions specific to SE interpretations. It provides an intuitive, complete procedure for the construction of all rational LP revision operators and makes easier the comprehension of their semantic and computational properties. We give a particular consideration to logic programs of very general form, i.e., the generalized logic programs (GLPs). We show that every rational GLP revision operator is derived from a propositional revision operator satisfying the original AGM postulates. Interestingly, the further conditions specific to GLP revision are independent from the propositional revision operator on which a GLP revision operator is based. Taking advantage of our characterization result, we embed the GLP revision operators into structures of Boolean lattices, that allow us to bring to light some potential weaknesses in the adapted AGM postulates. To illustrate our claim, we introduce and characterize axiomatically two specific classes of (rational) GLP revision operators which arguably have a drastic behavior. We additionally consider two more restricted forms of logic programs, i.e., the disjunctive logic programs (DLPs) and the normal logic programs (NLPs) and adapt our characterization result to DLP and NLP revision operators.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00142",
        "title": "A Tool for Computing and Estimating the Volume of the Solution Space of SMT(LA)",
        "authors": [
            "Cunjing Ge",
            "Feifei Ma",
            "Jian Zhang"
        ],
        "abstract": "There are already quite a few tools for solving the Satisfiability Modulo Theories (SMT) problems. In this paper, we present \\texttt{VolCE}, a tool for counting the solutions of SMT constraints, or in other words, for computing the volume of the solution space. Its input is essentially a set of Boolean combinations of linear constraints, where the numeric variables are either all integers or all reals, and each variable is bounded. The tool extends SMT solving with integer solution counting and volume computation/estimation for convex polytopes. Effective heuristics are adopted, which enable the tool to deal with high-dimensional problem instances efficiently and accurately.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00353",
        "title": "An Empirical Evaluation of True Online TD(\u03bb)",
        "authors": [
            "Harm van Seijen",
            "A. Rupam Mahmood",
            "Patrick M. Pilarski",
            "Richard S. Sutton"
        ],
        "abstract": "The true online TD({\\lambda}) algorithm has recently been proposed (van Seijen and Sutton, 2014) as a universal replacement for the popular TD({\\lambda}) algorithm, in temporal-difference learning and reinforcement learning. True online TD({\\lambda}) has better theoretical properties than conventional TD({\\lambda}), and the expectation is that it also results in faster learning. In this paper, we put this hypothesis to the test. Specifically, we compare the performance of true online TD({\\lambda}) with that of TD({\\lambda}) on challenging examples, random Markov reward processes, and a real-world myoelectric prosthetic arm. We use linear function approximation with tabular, binary, and non-binary features. We assess the algorithms along three dimensions: computational cost, learning speed, and ease of use. Our results confirm the strength of true online TD({\\lambda}): 1) for sparse feature vectors, the computational overhead with respect to TD({\\lambda}) is minimal; for non-sparse features the computation time is at most twice that of TD({\\lambda}), 2) across all domains/representations the learning speed of true online TD({\\lambda}) is often better, but never worse than that of TD({\\lambda}), and 3) true online TD({\\lambda}) is easier to use, because it does not require choosing between trace types, and it is generally more stable with respect to the step-size. Overall, our results suggest that true online TD({\\lambda}) should be the first choice when looking for an efficient, general-purpose TD method.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00436",
        "title": "Online Transfer Learning in Reinforcement Learning Domains",
        "authors": [
            "Yusen Zhan",
            "Matthew E. Taylor"
        ],
        "abstract": "This paper proposes an online transfer framework to capture the interaction among agents and shows that current transfer learning in reinforcement learning is a special case of online transfer. Furthermore, this paper re-characterizes existing agents-teaching-agents methods as online transfer and analyze one such teaching method in three ways. First, the convergence of Q-learning and Sarsa with tabular representation with a finite budget is proven. Second, the convergence of Q-learning and Sarsa with linear function approximation is established. Third, the we show the asymptotic performance cannot be hurt through teaching. Additionally, all theoretical results are empirically validated.\n    ",
        "submission_date": "2015-07-02T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00814",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
        "authors": [
            "Bradly C. Stadie",
            "Sergey Levine",
            "Pieter Abbeel"
        ],
        "abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.\n    ",
        "submission_date": "2015-07-03T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00862",
        "title": "Using Monte Carlo method for searching partitionings of hard variants of Boolean satisfiability problem",
        "authors": [
            "Alexander Semenov",
            "Oleg Zaikin"
        ],
        "abstract": "In this paper we propose the approach for constructing partitionings of hard variants of the Boolean satisfiability problem (SAT). Such partitionings can be used for solving corresponding SAT instances in parallel. For the same SAT instance one can construct different partitionings, each of them is a set of simplified versions of the original SAT instance. The effectiveness of an arbitrary partitioning is determined by the total time of solving of all SAT instances from it. We suggest the approach, based on the Monte Carlo method, for estimating time of processing of an arbitrary partitioning. With each partitioning we associate a point in the special finite search space. The estimation of effectiveness of the particular partitioning is the value of predictive function in the corresponding point of this space. The problem of search for an effective partitioning can be formulated as a problem of optimization of the predictive function. We use metaheuristic algorithms (simulated annealing and tabu search) to move from point to point in the search space. In our computational experiments we found partitionings for SAT instances encoding problems of inversion of some cryptographic functions. Several of these SAT instances with realistic predicted solving time were successfully solved on a computing cluster and in the volunteer computing project SAT@home. The solving time agrees well with estimations obtained by the proposed method.\n    ",
        "submission_date": "2015-07-03T00:00:00",
        "last_modified_date": "2015-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01122",
        "title": "Modeling the Mind: A brief review",
        "authors": [
            "Gabriel Makdah"
        ],
        "abstract": "The brain is a powerful tool used to achieve amazing feats. There have been several significant advances in neuroscience and artificial brain research in the past two decades. This article is a review of such advances, ranging from the concepts of connectionism, to neural network architectures and high-dimensional representations. There have also been advances in biologically inspired cognitive architectures of which we will cite a few. We will be positioning relatively specific models in a much broader perspective, while comparing and contrasting their advantages and weaknesses. The projects presented are targeted to model the brain at different levels, utilizing different methodologies.\n    ",
        "submission_date": "2015-07-04T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01384",
        "title": "The method of artificial systems",
        "authors": [
            "Christopher A. Tucker"
        ],
        "abstract": "This document is written with the intention to describe in detail a method and means by which a computer program can reason about the world and in so doing, increase its analogue to a living system. As the literature is rife and it is apparent we, as scientists and engineers, have not found the solution, this document will attempt the solution by grounding its intellectual arguments within tenets of human cognition in Western philosophy. The result will be a characteristic description of a method to describe an artificial system analogous to that performed for a human. The approach was the substance of my Master's thesis, explored more deeply during the course of my postdoc research. It focuses primarily on context awareness and choice set within a boundary of available epistemology, which serves to describe it. Expanded upon, such a description strives to discover agreement with Kant's critique of reason to understand how it could be applied to define the architecture of its design. The intention has never been to mimic human or biological systems, rather, to understand the profoundly fundamental rules, when leveraged correctly, results in an artificial consciousness as noumenon while in keeping with the perception of it as phenomenon.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2017-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01425",
        "title": "Latent Belief Theory and Belief Dependencies: A Solution to the Recovery Problem in the Belief Set Theories",
        "authors": [
            "Ryuta Arisaka"
        ],
        "abstract": "The AGM recovery postulate says: assume a set of propositions X; assume that it is consistent and that it is closed under logical consequences; remove a belief P from the set minimally, but make sure that the resultant set is again some set of propositions X' which is closed under the logical consequences; now add P again and close the set under the logical consequences; and we should get a set of propositions that contains all the propositions that were in X. This postulate has since met objections; many have observed that it could bear counter-intuitive results. Nevertheless, the attempts that have been made so far to amend it either recovered the postulate in full, had to relinquish the assumption of the logical closure altogether, or else had to introduce fresh controversies of their own. We provide a solution to the recovery paradox in this work. Our theoretical basis is the recently proposed belief theory with latent beliefs (simply the latent belief theory for short). Firstly, through examples, we will illustrate that the vanilla latent belief theory can be made more expressive. We will identify that a latent belief, when it becomes visible, may remain visible only while the beliefs that triggered it into the agent's consciousness are in the agent's belief set. In order that such situations can be also handled, we will enrich the latent belief theory with belief dependencies among attributive beliefs, recording the information as to which belief is supported of its existence by which beliefs. We will show that the enriched latent belief theory does not possess the recovery property. The closure by logical consequences is maintained in the theory, however. Hence it serves as a solution to the open problem in the belief set theories.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01451",
        "title": "A model building framework for Answer Set Programming with external computations",
        "authors": [
            "Thomas Eiter",
            "Michael Fink",
            "Giovambattista Ianni",
            "Thomas Krennwallner",
            "Christoph Redl",
            "Peter Sch\u00fcller"
        ],
        "abstract": "As software systems are getting increasingly connected, there is a need for equipping nonmonotonic logic programs with access to external sources that are possibly remote and may contain information in heterogeneous formats. To cater for this need, HEX programs were designed as a generalization of answer set programs with an API style interface that allows to access arbitrary external sources, providing great flexibility. Efficient evaluation of such programs however is challenging, and it requires to interleave external computation and model building; to decide when to switch between these tasks is difficult, and existing approaches have limited scalability in many real-world application scenarios. We present a new approach for the evaluation of logic programs with external source access, which is based on a configurable framework for dividing the non-ground program into possibly overlapping smaller parts called evaluation units. The latter will be processed by interleaving external evaluation and model building using an evaluation graph and a model graph, respectively, and by combining intermediate results. Experiments with our prototype implementation show a significant improvement compared to previous approaches. While designed for HEX-programs, the new evaluation approach may be deployed to related rule-based formalisms as well.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2015-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01986",
        "title": "Toward Idealized Decision Theory",
        "authors": [
            "Nate Soares",
            "Benja Fallenstein"
        ],
        "abstract": "This paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests. We discuss the shortcomings of two standard formulations of decision theory, and demonstrate that they cannot be used to describe an idealized decision procedure suitable for approximation by artificial systems. We then explore the notions of policy selection and logical counterfactuals, two recent insights into decision theory that point the way toward promising paths for future research.\n    ",
        "submission_date": "2015-07-07T00:00:00",
        "last_modified_date": "2015-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02347",
        "title": "Achieving Synergy in Cognitive Behavior of Humanoids via Deep Learning of Dynamic Visuo-Motor-Attentional Coordination",
        "authors": [
            "Jungsik Hwang",
            "Minju Jung",
            "Naveen Madapana",
            "Jinhyung Kim",
            "Minkyu Choi",
            "Jun Tani"
        ],
        "abstract": "The current study examines how adequate coordination among different cognitive processes including visual recognition, attention switching, action preparation and generation can be developed via learning of robots by introducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN). The proposed model is built on coupling of a dynamic vision network, a motor generation network, and a higher level network allocated on top of these two. The simulation experiments using the iCub simulator were conducted for cognitive tasks including visual object manipulation responding to human gestures. The results showed that synergetic coordination can be developed via iterative learning through the whole network when spatio-temporal hierarchy and temporal one can be self-organized in the visual pathway and in the motor pathway, respectively, such that the higher level can manipulate them with abstraction.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02439",
        "title": "Automated Matchmaking to Improve Accuracy of Applicant Selection for University Education System",
        "authors": [
            "Oludayo O. Olugbara",
            "Manish Joshi",
            "Michael M. Modiba",
            "Virendrakumar C. Bhavsar"
        ],
        "abstract": "The accurate applicant selection for university education is imperative to ensure fairness and optimal use of institutional resources. Although various approaches are operational in tertiary educational institutions for selecting applicants, a novel method of automated matchmaking is explored in the current study. The method functions by matching a prospective students skills profile to a programmes requisites profile.\n",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02456",
        "title": "Towards Log-Linear Logics with Concrete Domains",
        "authors": [
            "Melisachew Wudage Chekol",
            "Jakob Huber",
            "Heiner Stuckenschmidt"
        ],
        "abstract": "We present $\\mathcal{MEL}^{++}$ (M denotes Markov logic networks) an extension of the log-linear description logics $\\mathcal{EL}^{++}$-LL with concrete domains, nominals, and instances. We use Markov logic networks (MLNs) in order to find the most probable, classified and coherent $\\mathcal{EL}^{++}$ ontology from an $\\mathcal{MEL}^{++}$ knowledge base. In particular, we develop a novel way to deal with concrete domains (also known as datatypes) by extending MLN's cutting plane inference (CPI) algorithm.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02563",
        "title": "Managing Autonomous Mobility on Demand Systems for Better Passenger Experience",
        "authors": [
            "Wen Shen",
            "Cristina Lopes"
        ],
        "abstract": "Autonomous mobility on demand systems, though still in their infancy, have very promising prospects in providing urban population with sustainable and safe personal mobility in the near future. While much research has been conducted on both autonomous vehicles and mobility on demand systems, to the best of our knowledge, this is the first work that shows how to manage autonomous mobility on demand systems for better passenger experience. We introduce the Expand and Target algorithm which can be easily integrated with three different scheduling strategies for dispatching autonomous vehicles. We implement an agent-based simulation platform and empirically evaluate the proposed approaches with the New York City taxi data. Experimental results demonstrate that the algorithm significantly improve passengers' experience by reducing the average passenger waiting time by up to 29.82% and increasing the trip success rate by up to 7.65%.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02870",
        "title": "Analysis of Microarray Data using Artificial Intelligence Based Techniques",
        "authors": [
            "Khalid Raza"
        ],
        "abstract": "Microarray is one of the essential technologies used by the biologist to measure genome-wide expression levels of genes in a particular organism under some particular conditions or stimuli. As microarrays technologies have become more prevalent, the challenges of analyzing these data for getting better insight about biological processes have essentially increased. Due to availability of artificial intelligence based sophisticated computational techniques, such as artificial neural networks, fuzzy logic, genetic algorithms, and many other nature-inspired algorithms, it is possible to analyse microarray gene expression data in more better way. Here, we reviewed artificial intelligence based techniques for the analysis of microarray gene expression data. Further, challenges in the field and future work direction have also been suggested.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02873",
        "title": "Lazy Explanation-Based Approximation for Probabilistic Logic Programming",
        "authors": [
            "Joris Renkens",
            "Angelika Kimmig",
            "Luc De Raedt"
        ],
        "abstract": "We introduce a lazy approach to the explanation-based approximation of probabilistic logic programs. It uses only the most significant part of the program when searching for explanations. The result is a fast and anytime approximate inference algorithm which returns hard lower and upper bounds on the exact probability. We experimentally show that this method outperforms state-of-the-art approximate inference.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02912",
        "title": "First-order integer programming for MAP problems",
        "authors": [
            "James Cussens"
        ],
        "abstract": "Finding the most probable (MAP) model in SRL frameworks such as Markov logic and Problog can, in principle, be solved by encoding the problem as a `grounded-out' mixed integer program (MIP). However, useful first-order structure disappears in this process motivating the development of first-order MIP approaches. Here we present mfoilp, one such approach. Since the syntax and semantics of mfoilp is essentially the same as existing approaches we focus here mainly on implementation and algorithmic issues. We start with the (conceptually) simple problem of using a logic program to generate a MIP instance before considering more ambitious exploitation of first-order representations.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03045",
        "title": "Markov Logic Networks for Natural Language Question Answering",
        "authors": [
            "Tushar Khot",
            "Niranjan Balasubramanian",
            "Eric Gribkoff",
            "Ashish Sabharwal",
            "Peter Clark",
            "Oren Etzioni"
        ],
        "abstract": "Our goal is to answer elementary-level science questions using knowledge extracted automatically from science textbooks, expressed in a subset of first-order logic. Given the incomplete and noisy nature of these automatically extracted rules, Markov Logic Networks (MLNs) seem a natural model to use, but the exact way of leveraging MLNs is by no means obvious. We investigate three ways of applying MLNs to our task. In the first, we simply use the extracted science rules directly as MLN clauses. Unlike typical MLN applications, our domain has long and complex rules, leading to an unmanageable number of groundings. We exploit the structure present in hard constraints to improve tractability, but the formulation remains ineffective. In the second approach, we instead interpret science rules as describing prototypical entities, thus mapping rules directly to grounded MLN assertions, whose constants are then clustered using existing entity resolution methods. This drastically simplifies the network, but still suffers from brittleness. Finally, our third approach, called Praline, uses MLNs to align the lexical elements as well as define and control how inference should be performed in this task. Our experiments, demonstrating a 15\\% accuracy boost and a 10x reduction in runtime, suggest that the flexibility and different inference semantics of Praline are a better fit for the natural language question answering task.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03097",
        "title": "Ontology Matching with Knowledge Rules",
        "authors": [
            "Shangpu Jiang",
            "Daniel Lowd",
            "Dejing Dou"
        ],
        "abstract": "Ontology matching is the process of automatically determining the semantic equivalences between the concepts of two ontologies. Most ontology matching algorithms are based on two types of strategies: terminology-based strategies, which align concepts based on their names or descriptions, and structure-based strategies, which exploit concept hierarchies to find the alignment. In many domains, there is additional information about the relationships of concepts represented in various ways, such as Bayesian networks, decision trees, and association rules. We propose to use the similarities between these relationships to find more accurate alignments. We accomplish this by defining soft constraints that prefer alignments where corresponding concepts have the same local relationships encoded as knowledge rules. We use a probabilistic framework to integrate this new knowledge-based strategy with standard terminology-based and structure-based strategies. Furthermore, our method is particularly effective in identifying correspondences between complex concepts. Our method achieves substantially better F-score than the previous state-of-the-art on three ontology matching domains.\n    ",
        "submission_date": "2015-07-11T00:00:00",
        "last_modified_date": "2015-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03168",
        "title": "Using Bayesian Network Representations for Effective Sampling from Generative Network Models",
        "authors": [
            "Pablo Robles-Granda",
            "Sebastian Moreno",
            "Jennifer Neville"
        ],
        "abstract": "Bayesian networks (BNs) are used for inference and sampling by exploiting conditional independence among random variables. Context specific independence (CSI) is a property of graphical models where additional independence relations arise in the context of particular values of random variables (RVs). Identifying and exploiting CSI properties can simplify inference. Some generative network models (models that generate social/information network samples from a network distribution P(G)), with complex interactions among a set of RVs, can be represented with probabilistic graphical models, in particular with BNs. In the present work we show one such a case. We discuss how a mixed Kronecker Product Graph Model can be represented as a BN, and study its BN properties that can be used for efficient sampling. Specifically, we show that instead of exhibiting CSI properties, the model has deterministic context-specific dependence (DCSD). Exploiting this property focuses the sampling method on a subset of the sampling space that improves efficiency.\n    ",
        "submission_date": "2015-07-11T00:00:00",
        "last_modified_date": "2015-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03181",
        "title": "A Probabilistic Approach to Knowledge Translation",
        "authors": [
            "Shangpu Jiang",
            "Daniel Lowd",
            "Dejing Dou"
        ],
        "abstract": "In this paper, we focus on a novel knowledge reuse scenario where the knowledge in the source schema needs to be translated to a semantically heterogeneous target schema. We refer to this task as \"knowledge translation\" (KT). Unlike data translation and transfer learning, KT does not require any data from the source or target schema. We adopt a probabilistic approach to KT by representing the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema all as probability distributions, specially using Markov random fields and Markov logic networks. Given the source knowledge and mappings, we use standard learning and inference algorithms for probabilistic graphical models to find an explicit probability distribution in the target schema that minimizes the Kullback-Leibler divergence from the implicit distribution. This gives us a compact probabilistic model that represents knowledge from the source schema as well as possible, respecting the uncertainty in both the source knowledge and the mapping. In experiments on both propositional and relational domains, we find that the knowledge obtained by KT is comparable to other approaches that require data, demonstrating that knowledge can be reused without data.\n    ",
        "submission_date": "2015-07-12T00:00:00",
        "last_modified_date": "2015-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03257",
        "title": "Use of the Triangular Fuzzy Numbers for Student Assessment",
        "authors": [
            "Michael Voskoglou"
        ],
        "abstract": "In an earlier work we have used the Triangular Fuzzy Numbers (TFNs)as an assessment tool of student ",
        "submission_date": "2015-07-12T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03638",
        "title": "Experimental analysis of data-driven control for a building heating system",
        "authors": [
            "Giuseppe Tommaso Costanzo",
            "Sandro Iacovella",
            "Frederik Ruelens",
            "T. Leurs",
            "Bert Claessens"
        ],
        "abstract": "Driven by the opportunity to harvest the flexibility related to building climate control for demand response applications, this work presents a data-driven control approach building upon recent advancements in reinforcement learning. More specifically, model assisted batch reinforcement learning is applied to the setting of building climate control subjected to a dynamic pricing. The underlying sequential decision making problem is cast on a markov decision problem, after which the control algorithm is detailed. In this work, fitted Q-iteration is used to construct a policy from a batch of experimental tuples. In those regions of the state space where the experimental sample density is low, virtual support samples are added using an artificial neural network. Finally, the resulting policy is shaped using domain knowledge. The control approach has been evaluated quantitatively using a simulation and qualitatively in a living lab. From the quantitative analysis it has been found that the control approach converges in approximately 20 days to obtain a control policy with a performance within 90% of the mathematical optimum. The experimental analysis confirms that within 10 to 20 days sensible policies are obtained that can be used for different outside temperature regimes.\n    ",
        "submission_date": "2015-07-13T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03920",
        "title": "Fuzzy Answer Set Computation via Satisfiability Modulo Theories",
        "authors": [
            "Mario Alviano",
            "Rafael Penaloza"
        ],
        "abstract": "Fuzzy answer set programming (FASP) combines two declarative frameworks, answer set programming and fuzzy logic, in order to model reasoning by default over imprecise information. Several connectives are available to combine different expressions; in particular the \\Godel and \\Luka fuzzy connectives are usually considered, due to their properties. Although the \\Godel conjunction can be easily eliminated from rule heads, we show through complexity arguments that such a simplification is infeasible in general for all other connectives. %, even if bodies are restricted to \\Luka or \\Godel conjunctions. The paper analyzes a translation of FASP programs into satisfiability modulo theories~(SMT), which in general produces quantified formulas because of the minimality of the semantics. Structural properties of many FASP programs allow to eliminate the quantification, or to sensibly reduce the number of quantified variables. Indeed, integrality constraints can replace recursive rules commonly used to force Boolean interpretations, and completion subformulas can guarantee minimality for acyclic programs with atomic heads. Moreover, head cycle free rules can be replaced by shifted subprograms, whose structure depends on the eliminated head connective, so that ordered completion may replace the minimality check if also \\Luka disjunction in rule bodies is acyclic. The paper also presents and evaluates a prototype system implementing these translations.\n",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03922",
        "title": "Complexity and Compilation of GZ-Aggregates in Answer Set Programming",
        "authors": [
            "Mario Alviano",
            "Nicola Leone"
        ],
        "abstract": "Gelfond and Zhang recently proposed a new stable model semantics based on Vicious Circle Principle in order to improve the interpretation of logic programs with aggregates. The paper focuses on this proposal, and analyzes the complexity of both coherence testing and cautious reasoning under the new semantics. Some surprising results highlight similarities and differences versus mainstream stable model semantics for aggregates. Moreover, the paper reports on the design of compilation techniques for implementing the new semantics on top of existing ASP solvers, which eventually lead to realize a prototype system that allows for experimenting with Gelfond-Zhang's aggregates.\n",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03923",
        "title": "Rewriting recursive aggregates in answer set programming: back to monotonicity",
        "authors": [
            "Mario Alviano",
            "Wolfgang Faber",
            "Martin Gebser"
        ],
        "abstract": "Aggregation functions are widely used in answer set programming for representing and reasoning on knowledge involving sets of objects collectively. Current implementations simplify the structure of programs in order to optimize the overall performance. In particular, aggregates are rewritten into simpler forms known as monotone aggregates. Since the evaluation of normal programs with monotone aggregates is in general on a lower complexity level than the evaluation of normal programs with arbitrary aggregates, any faithful translation function must introduce disjunction in rule heads in some cases. However, no function of this kind is known. The paper closes this gap by introducing a polynomial, faithful, and modular translation for rewriting common aggregation functions into the simpler form accepted by current solvers. A prototype system allows for experimenting with arbitrary recursive aggregates, which are also supported in the recent version 4.5 of the grounder \\textsc{gringo}, using the methods presented in this paper.\n",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03979",
        "title": "Planning as Tabled Logic Programming",
        "authors": [
            "Neng-Fa Zhou",
            "Roman Bartak",
            "Agostino Dovier"
        ],
        "abstract": "This paper describes Picat's planner, its implementation, and planning models for several domains used in International Planning Competition (IPC) 2014. Picat's planner is implemented by use of tabling. During search, every state encountered is tabled, and tabled states are used to effectively perform resource-bounded search. In Picat, structured data can be used to avoid enumerating all possible permutations of objects, and term sharing is used to avoid duplication of common state data. This paper presents several modeling techniques through the example models, ranging from designing state representations to facilitate data sharing and symmetry breaking, encoding actions with operations for efficient precondition checking and state updating, to incorporating domain knowledge and heuristics. Broadly, this paper demonstrates the effectiveness of tabled logic programming for planning, and argues the importance of modeling despite recent significant progress in domain-independent PDDL planners.\n    ",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04091",
        "title": "Evidential relational clustering using medoids",
        "authors": [
            "Kuang Zhou",
            "Arnaud Martin",
            "Quan Pan",
            "Zhun-Ga Liu"
        ],
        "abstract": "In real clustering applications, proximity data, in which only pairwise similarities or dissimilarities are known, is more general than object data, in which each pattern is described explicitly by a list of attributes. Medoid-based clustering algorithms, which assume the prototypes of classes are objects, are of great value for partitioning relational data sets. In this paper a new prototype-based clustering method, named Evidential C-Medoids (ECMdd), which is an extension of Fuzzy C-Medoids (FCMdd) on the theoretical framework of belief functions is proposed. In ECMdd, medoids are utilized as the prototypes to represent the detected classes, including specific classes and imprecise classes. Specific classes are for the data which are distinctly far from the prototypes of other classes, while imprecise classes accept the objects that may be close to the prototypes of more than one class. This soft decision mechanism could make the clustering results more cautious and reduce the misclassification rates. Experiments in synthetic and real data sets are used to illustrate the performance of ECMdd. The results show that ECMdd could capture well the uncertainty in the internal data structure. Moreover, it is more robust to the initializations compared with FCMdd.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04124",
        "title": "On the Computability of Solomonoff Induction and Knowledge-Seeking",
        "authors": [
            "Jan Leike",
            "Marcus Hutter"
        ],
        "abstract": "Solomonoff induction is held as a gold standard for learning, but it is known to be incomputable. We quantify its incomputability by placing various flavors of Solomonoff's prior M in the arithmetical hierarchy. We also derive computability bounds for knowledge-seeking agents, and give a limit-computable weakly asymptotically optimal reinforcement learning agent.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04630",
        "title": "Optimizing the computation of overriding",
        "authors": [
            "Piero Andrea Bonatti",
            "Iliana Mineva Petrova",
            "Luigi Sauro"
        ],
        "abstract": "We introduce optimization techniques for reasoning in DLN---a recently introduced family of nonmonotonic description logics whose characterizing features appear well-suited to model the applicative examples naturally arising in biomedical domains and semantic web access control policies. Such optimizations are validated experimentally on large KBs with more than 30K axioms. Speedups exceed 1 order of magnitude. For the first time, response times compatible with real-time reasoning are obtained with nonmonotonic KBs of this size.\n    ",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04928",
        "title": "A Brain-like Cognitive Process with Shared Methods",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper describes a new entropy-style of equation that may be useful in a general sense, but can be applied to a cognitive model with related processes. The model is based on the human brain, with automatic and distributed pattern activity. Methods for carrying out the different processes are suggested. The main purpose of this paper is to reaffirm earlier research on different knowledge-based and experience-based clustering techniques. The overall architecture has stayed essentially the same and so it is the localised processes or smaller details that have been updated. For example, a counting mechanism is used slightly differently, to measure a level of 'cohesion' instead of a 'correct' classification, over pattern instances. The introduction of features has further enhanced the architecture and the new entropy-style equation is proposed. While an earlier paper defined three levels of functional requirement, this paper re-defines the levels in a more human vernacular, with higher-level goals described in terms of action-result pairs.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2017-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05019",
        "title": "Support Vector Machine in Prediction of Building Energy Demand Using Pseudo Dynamic Approach",
        "authors": [
            "Subodh Paudel",
            "Phuong H. Nguyen",
            "Wil L. Kling",
            "Mohamed Elmitri",
            "Bruno Lacarri\u00e8re",
            "Olivier Le Corre"
        ],
        "abstract": "Building's energy consumption prediction is a major concern in the recent years and many efforts have been achieved in order to improve the energy management of buildings. In particular, the prediction of energy consumption in building is essential for the energy operator to build an optimal operating strategy, which could be integrated to building's energy management system (BEMS). This paper proposes a prediction model for building energy consumption using support vector machine (SVM). Data-driven model, for instance, SVM is very sensitive to the selection of training data. Thus the relevant days data selection method based on Dynamic Time Warping is used to train SVM model. In addition, to encompass thermal inertia of building, pseudo dynamic model is applied since it takes into account information of transition of energy consumption effects and occupancy profile. Relevant days data selection and whole training data model is applied to the case studies of Ecole des Mines de Nantes, France Office building. The results showed that support vector machine based on relevant data selection method is able to predict the energy consumption of building with a high accuracy in compare to whole data training. In addition, relevant data selection method is computationally cheaper (around 8 minute training time) in contrast to whole data training (around 31 hour for weekend and 116 hour for working days) and reveals realistic control implementation for online system as well.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05122",
        "title": "Human Gender Classification: A Review",
        "authors": [
            "Yingxiao Wu",
            "Yan Zhuang",
            "Xi Long",
            "Feng Lin",
            "Wenyao Xu"
        ],
        "abstract": "Gender contains a wide range of information regarding to the characteristics difference between male and female. Successful gender recognition is essential and critical for many applications in the commercial domains such as applications of human-computer interaction and computer-aided physiological or psychological analysis. Some have proposed various approaches for automatic gender classification using the features derived from human bodies and/or behaviors. First, this paper introduces the challenge and application for gender classification research. Then, the development and framework of gender classification are described. Besides, we compare these state-of-the-art approaches, including vision-based methods, biological information-based method, and social network information-based method, to provide a comprehensive review in the area of gender classification. In mean time, we highlight the strength and discuss the limitation of each method. Finally, this review also discusses several promising applications for the future work.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05268",
        "title": "Reinforcement Learning for the Unit Commitment Problem",
        "authors": [
            "Gal Dalal",
            "Shie Mannor"
        ],
        "abstract": "In this work we solve the day-ahead unit commitment (UC) problem, by formulating it as a Markov decision process (MDP) and finding a low-cost policy for generation scheduling. We present two reinforcement learning algorithms, and devise a third one. We compare our results to previous work that uses simulated annealing (SA), and show a 27% improvement in operation costs, with running time of 2.5 minutes (compared to 2.5 hours of existing state-of-the-art).\n    ",
        "submission_date": "2015-07-19T00:00:00",
        "last_modified_date": "2015-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05275",
        "title": "An Efficient Genetic Algorithm for Discovering Diverse-Frequent Patterns",
        "authors": [
            "Shanjida Khatun",
            "Hasib Ul Alam",
            "Swakkhar Shatabda"
        ],
        "abstract": "Working with exhaustive search on large dataset is infeasible for several reasons. Recently, developed techniques that made pattern set mining feasible by a general solver with long execution time that supports heuristic search and are limited to small datasets only. In this paper, we investigate an approach which aims to find diverse set of patterns using genetic algorithm to mine diverse frequent patterns. We propose a fast heuristic search algorithm that outperforms state-of-the-art methods on a standard set of benchmarks and capable to produce satisfactory results within a short period of time. Our proposed algorithm uses a relative encoding scheme for the patterns and an effective twin removal technique to ensure diversity throughout the search.\n    ",
        "submission_date": "2015-07-19T00:00:00",
        "last_modified_date": "2015-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05875",
        "title": "Efficient Dodgson-Score Calculation Using Heuristics and Parallel Computing",
        "authors": [
            "Arne Recknagel",
            "Tarek R. Besold"
        ],
        "abstract": "Conflict of interest is the permanent companion of any population of agents (computational or biological). For that reason, the ability to compromise is of paramount importance, making voting a key element of societal mechanisms. One of the voting procedures most often discussed in the literature and, due to its intuitiveness, also conceptually quite appealing is Charles Dodgson's scoring rule, basically using the respective closeness to being a Condorcet winner for evaluating competing alternatives. In this paper, we offer insights on the practical limits of algorithms computing the exact Dodgson scores from a number of votes. While the problem itself is theoretically intractable, this work proposes and analyses five different solutions which try distinct approaches to practically solve the issue in an effective manner. Additionally, three of the discussed procedures can be run in parallel which has the potential of drastically reducing the problem size.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05895",
        "title": "Decision Maker based on Atomic Switches",
        "authors": [
            "Song-Ju Kim",
            "Tohru Tsuruoka",
            "Tsuyoshi Hasegawa",
            "Masakazu Aono"
        ],
        "abstract": "We propose a simple model for an atomic switch-based decision maker (ASDM), and show that, as long as its total volume of precipitated Ag atoms is conserved when coupled with suitable operations, an atomic switch system provides a sophisticated \"decision-making\" capability that is known to be one of the most important intellectual abilities in human beings. We considered the multi-armed bandit problem (MAB); the problem of finding, as accurately and quickly as possible, the most profitable option from a set of options that gives stochastic rewards. These decisions are made as dictated by each volume of precipitated Ag atoms, which is moved in a manner similar to the fluctuations of a rigid body in a tug-of-war game. The \"tug-of-war (TOW) dynamics\" of the ASDM exhibits higher efficiency than conventional MAB solvers. We show analytical calculations that validate the statistical reasons for the ASDM dynamics to produce such high performance, despite its simplicity. These results imply that various physical systems, in which some conservation law holds, can be used to implement efficient \"decision-making objects.\" Efficient MAB solvers are useful for many practical applications, because MAB abstracts a variety of decision-making problems in real- world situations where an efficient trial-and-error is required. The proposed scheme will introduce a new physics-based analog computing paradigm, which will include such things as \"intelligent nano devices\" and \"intelligent information networks\" based on self-detection and self-judgment.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05956",
        "title": "Towards a Better Understanding of CAR, CDR, CADR and the Others",
        "authors": [
            "Thomas W. Lynch"
        ],
        "abstract": "This paper describes the IBM 704 architecture and the genesis of the names for CAR, and CDR, which, as it turns out, probably don't quite make sense. The paper suggests that this may not be all bad, as the names lend themselves to compounding. Indeed that the compound function names , such as CADR, or even CADADR, etc. may be read as little access programs.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06045",
        "title": "Adapting Stochastic Search For Real-time Dynamic Weighted Constraint Satisfaction",
        "authors": [
            "Gregory Hasseler"
        ],
        "abstract": "This work presents two new algorithms for performing constraint satisfaction. The first algorithm presented, DMaxWalkSat, is a constraint solver specialized for solving dynamic, weighted constraint satisfaction problems. The second algorithm, RDMaxWalkSat, is a derivative of DMaxWalkSat that has been modified into an anytime algorithm, and hence support realtime constraint satisfaction. DMaxWalkSat is shown to offer performance advantages in terms of solution quality and runtime over its parent constraint solver, MaxWalkSat. RDMaxWalkSat is shown to support anytime operation. The introduction of these algorithms brings another tool to the areas of computer science that naturally represent problems as constraint satisfaction problems, an example of which is the robust coherence algorithm.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06103",
        "title": "Taming Primary Key Violations to Query Large Inconsistent Data",
        "authors": [
            "Marco Manna",
            "Francesco Ricca",
            "Giorgio Terracina"
        ],
        "abstract": "Consistent query answering over a database that violates primary key constraints is a classical hard problem in database research that has been traditionally dealt with logic programming. However, the applicability of existing logic-based solutions is restricted to data sets of moderate size. This paper presents a novel decomposition and pruning strategy that reduces, in polynomial time, the problem of computing the consistent answer to a conjunctive query over a database subject to primary key constraints to a collection of smaller problems of the same sort that can be solved independently. The new strategy is naturally modeled and implemented using Answer Set Programming (ASP). An experiment run on benchmarks from the database world prove the effectiveness and efficiency of our ASP-based approach also on large data sets. To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of ICLP 2015.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06500",
        "title": "Mapping Big Data into Knowledge Space with Cognitive Cyber-Infrastructure",
        "authors": [
            "Hai Zhuge"
        ],
        "abstract": "Big data research has attracted great attention in science, technology, industry and society. It is developing with the evolving scientific paradigm, the fourth industrial revolution, and the transformational innovation of technologies. However, its nature and fundamental challenge have not been recognized, and its own methodology has not been formed. This paper explores and answers the following questions: What is big data? What are the basic methods for representing, managing and analyzing big data? What is the relationship between big data and knowledge? Can we find a mapping from big data into knowledge space? What kind of infrastructure is required to support not only big data management and analysis but also knowledge discovery, sharing and management? What is the relationship between big data and science paradigm? What is the nature and fundamental challenge of big data computing? A multi-dimensional perspective is presented toward a methodology of big data computing.\n    ",
        "submission_date": "2015-07-18T00:00:00",
        "last_modified_date": "2015-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06566",
        "title": "Learning Weak Constraints in Answer Set Programming",
        "authors": [
            "Mark Law",
            "Alessandra Russo",
            "Krysia Broda"
        ],
        "abstract": "This paper contributes to the area of inductive logic programming by presenting a new learning framework that allows the learning of weak constraints in Answer Set Programming (ASP). The framework, called Learning from Ordered Answer Sets, generalises our previous work on learning ASP programs without weak constraints, by considering a new notion of examples as ordered pairs of partial answer sets that exemplify which answer sets of a learned hypothesis (together with a given background knowledge) are preferred to others. In this new learning task inductive solutions are searched within a hypothesis space of normal rules, choice rules, and hard and weak constraints. We propose a new algorithm, ILASP2, which is sound and complete with respect to our new learning framework. We investigate its applicability to learning preferences in an interview scheduling problem and also demonstrate that when restricted to the task of learning ASP programs without weak constraints, ILASP2 can be much more efficient than our previously proposed system.\n    ",
        "submission_date": "2015-07-23T00:00:00",
        "last_modified_date": "2015-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06689",
        "title": "Improved Answer-Set Programming Encodings for Abstract Argumentation",
        "authors": [
            "Sarah A. Gaggl",
            "Norbert Manthey",
            "Alessandro Ronca",
            "Johannes P. Wallner",
            "Stefan Woltran"
        ],
        "abstract": "The design of efficient solutions for abstract argumentation problems is a crucial step towards advanced argumentation systems. One of the most prominent approaches in the literature is to use Answer-Set Programming (ASP) for this endeavor. In this paper, we present new encodings for three prominent argumentation semantics using the concept of conditional literals in disjunctions as provided by the ASP-system clingo. Our new encodings are not only more succinct than previous versions, but also outperform them on standard benchmarks.\n    ",
        "submission_date": "2015-07-23T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07058",
        "title": "The Digital Synaptic Neural Substrate: A New Approach to Computational Creativity",
        "authors": [
            "Azlan Iqbal",
            "Matej Guid",
            "Simon Colton",
            "Jana Krivec",
            "Shazril Azman",
            "Boshra Haghighi"
        ],
        "abstract": "We introduce a new artificial intelligence (AI) approach called, the 'Digital Synaptic Neural Substrate' (DSNS). It uses selected attributes from objects in various domains (e.g. chess problems, classical music, renowned artworks) and recombines them in such a way as to generate new attributes that can then, in principle, be used to create novel objects of creative value to humans relating to any one of the source domains. This allows some of the burden of creative content generation to be passed from humans to machines. The approach was tested in the domain of chess problem composition. We used it to automatically compose numerous sets of chess problems based on attributes extracted and recombined from chess problems and tournament games by humans, renowned paintings, computer-evolved abstract art, photographs of people, and classical music tracks. The quality of these generated chess problems was then assessed automatically using an existing and experimentally-validated computational chess aesthetics model. They were also assessed by human experts in the domain. The results suggest that attributes collected and recombined from chess and other domains using the DSNS approach can indeed be used to automatically generate chess problems of reasonably high aesthetic quality. In particular, a low quality chess source (i.e. tournament game sequences between weak players) used in combination with actual photographs of people was able to produce three-move chess problems of comparable quality or better to those generated using a high quality chess source (i.e. published compositions by human experts), and more efficiently as well. Why information from a foreign domain can be integrated and functional in this way remains an open question for now. The DSNS approach is, in principle, scalable and applicable to any domain in which objects have attributes that can be represented using real numbers.\n    ",
        "submission_date": "2015-07-25T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07295",
        "title": "Learning (Predictive) Risk Scores in the Presence of Censoring due to Interventions",
        "authors": [
            "Kirill Dyagilev",
            "Suchi Saria"
        ],
        "abstract": "A large and diverse set of measurements are regularly collected during a patient's hospital stay to monitor their health status. Tools for integrating these measurements into severity scores, that accurately track changes in illness severity, can improve clinicians ability to provide timely interventions. Existing approaches for creating such scores either 1) rely on experts to fully specify the severity score, or 2) train a predictive score, using supervised learning, by regressing against a surrogate marker of severity such as the presence of downstream adverse events. The first approach does not extend to diseases where an accurate score cannot be elicited from experts. The second approach often produces scores that suffer from bias due to treatment-related censoring (Paxton, 2013). We propose a novel ranking based framework for disease severity score learning (DSSL). DSSL exploits the following key observation: while it is challenging for experts to quantify the disease severity at any given time, it is often easy to compare the disease severity at two different times. Extending existing ranking algorithms, DSSL learns a function that maps a vector of patient's measurements to a scalar severity score such that the resulting score is temporally smooth and consistent with the expert's ranking of pairs of disease states. We apply DSSL to the problem of learning a sepsis severity score using a large, real-world dataset. The learned scores significantly outperform state-of-the-art clinical scores in ranking patient states by severity and in early detection of future adverse events. We also show that the learned disease severity trajectories are consistent with clinical expectations of disease evolution. Further, using simulated datasets, we show that DSSL exhibits better generalization performance to changes in treatment patterns compared to the above approaches.\n    ",
        "submission_date": "2015-07-27T00:00:00",
        "last_modified_date": "2015-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07462",
        "title": "Unification of Fusion Theories, Rules, Filters, Image Fusion and Target Tracking Methods (UFT)",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "The author has pledged in various papers, conference or seminar presentations, and scientific grant applications (between 2004-2015) for the unification of fusion theories, combinations of fusion rules, image fusion procedures, filter algorithms, and target tracking methods for more accurate applications to our real world problems - since neither fusion theory nor fusion rule fully satisfy all needed applications. For each particular application, one selects the most appropriate fusion space and fusion model, then the fusion rules, and the algorithms of implementation. He has worked in the Unification of the Fusion Theories (UFT), which looks like a cooking recipe, better one could say like a logical chart for a computer programmer, but one does not see another method to comprise/unify all things. The unification scenario presented herein, which is now in an incipient form, should periodically be updated incorporating new discoveries from the fusion and engineering research.\n    ",
        "submission_date": "2015-07-27T00:00:00",
        "last_modified_date": "2015-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07648",
        "title": "Projected Model Counting",
        "authors": [
            "Rehan Abdul Aziz",
            "Geoffrey Chu",
            "Christian Muise",
            "Peter Stuckey"
        ],
        "abstract": "Model counting is the task of computing the number of assignments to variables V that satisfy a given propositional theory F. Model counting is an essential tool in probabilistic reasoning. In this paper, we introduce the problem of model counting projected on a subset P of original variables that we call 'priority' variables. The task is to compute the number of assignments to P such that there exists an extension to 'non-priority' variables V\u00b6that satisfies F. Projected model counting arises when some parts of the model are irrelevant to the counts, in particular when we require additional variables to model the problem we are counting in SAT. We discuss three different approaches to projected model counting (two of which are novel), and compare their performance on different benchmark problems.\n",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07688",
        "title": "Belief and Truth in Hypothesised Behaviours",
        "authors": [
            "Stefano V. Albrecht",
            "Jacob W. Crandall",
            "Subramanian Ramamoorthy"
        ],
        "abstract": "There is a long history in game theory on the topic of Bayesian or \"rational\" learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07749",
        "title": "Scaling up Greedy Causal Search for Continuous Variables",
        "authors": [
            "Joseph D. Ramsey"
        ],
        "abstract": "As standardly implemented in R or the Tetrad program, causal search algorithms used most widely or effectively by scientists have severe dimensionality constraints that make them inappropriate for big data problems without sacrificing accuracy. However, implementation improvements are possible. We explore optimizations for the Greedy Equivalence Search that allow search on 50,000-variable problems in 13 minutes for sparse models with 1000 samples on a four-processor, 16G laptop computer. We finish a problem with 1000 samples on 1,000,000 variables in 18 hours for sparse models on a supercomputer node at the Pittsburgh Supercomputing Center with 40 processors and 384 G RAM. The same algorithm can be applied to discrete data, with a slower discrete score, though the discrete implementation currently does not scale as well in our experiments; we have managed to scale up to about 10,000 variables in sparse models with 1000 samples.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08073",
        "title": "Communication: Words and Conceptual Systems",
        "authors": [
            "Jian Yu"
        ],
        "abstract": "Words (phrases or symbols) play a key role in human life. Word (phrase or symbol) representation is the fundamental problem for knowledge representation and understanding. A word (phrase or symbol) usually represents a name of a category. However, it is always a challenge that how to represent a category can make it easily understood. In this paper, a new representation for a category is discussed, which can be considered a generalization of classic set. In order to reduce representation complexity, the economy principle of category representation is proposed. The proposed category representation provides a powerful tool for analyzing conceptual systems, relations between words, communication, knowledge, situations. More specifically, the conceptual system, word relations and communication are mathematically defined and classified such as ideal conceptual system, perfect communication and so on; relation between words and sentences is also studied, which shows that knowledge are words. Furthermore, how conceptual systems and words depend on situations is presented, and how truth is defined is also discussed.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08271",
        "title": "A Gauss-Newton Method for Markov Decision Processes",
        "authors": [
            "Thomas Furmston",
            "Guy Lever"
        ],
        "abstract": "Approximate Newton methods are a standard optimization tool which aim to maintain the benefits of Newton's method, such as a fast rate of convergence, whilst alleviating its drawbacks, such as computationally expensive calculation or estimation of the inverse Hessian. In this work we investigate approximate Newton methods for policy optimization in Markov Decision Processes (MDPs). We first analyse the structure of the Hessian of the objective function for MDPs. We show that, like the gradient, the Hessian exhibits useful structure in the context of MDPs and we use this analysis to motivate two Gauss-Newton Methods for MDPs. Like the Gauss-Newton method for non-linear least squares, these methods involve approximating the Hessian by ignoring certain terms in the Hessian which are difficult to estimate. The approximate Hessians possess desirable properties, such as negative definiteness, and we demonstrate several important performance guarantees including guaranteed ascent directions, invariance to affine transformation of the parameter space, and convergence guarantees. We finally provide a unifying perspective of key policy search algorithms, demonstrating that our second Gauss-Newton algorithm is closely related to both the EM-algorithm and natural gradient ascent applied to MDPs, but performs significantly better in practice on a range of challenging domains.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08444",
        "title": "Optimal estimates for short horizon travel time prediction in urban areas",
        "authors": [
            "Indre Zliobaite",
            "Mikhail Khokhlov"
        ],
        "abstract": "Increasing popularity of mobile route planning applications based on GPS technology provides opportunities for collecting traffic data in urban environments. One of the main challenges for travel time estimation and prediction in such a setting is how to aggregate data from vehicles that have followed different routes, and predict travel time for other routes of interest. One approach is to predict travel times for route segments, and sum those estimates to obtain a prediction for the whole route. We study how to obtain optimal predictions in this scenario. It appears that the optimal estimate, minimizing the expected mean absolute error, is a combination of the mean and the median travel times on each segment, where the combination function depends on the number of segments in the route of interest. We present a methodology for obtaining such predictions, and demonstrate its effectiveness with a case study using travel time data from a district of St. Petersburg collected over one year. The proposed methodology can be applied for real-time prediction of expected travel times in an urban road network.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08559",
        "title": "CRISNER: A Practically Efficient Reasoner for Qualitative Preferences",
        "authors": [
            "Ganesh Ram Santhanam",
            "Samik Basu",
            "Vasant Honavar"
        ],
        "abstract": "We present CRISNER (Conditional & Relative Importance Statement Network PrEference Reasoner), a tool that provides practically efficient as well as exact reasoning about qualitative preferences in popular ceteris paribus preference languages such as CP-nets, TCP-nets, CP-theories, etc. The tool uses a model checking engine to translate preference specifications and queries into appropriate Kripke models and verifiable properties over them respectively. The distinguishing features of the tool are: (1) exact and provably correct query answering for testing dominance, consistency with respect to a preference specification, and testing equivalence and subsumption of two sets of preferences; (2) automatic generation of proofs evidencing the correctness of answer produced by CRISNER to any of the above queries; (3) XML inputs and outputs that make it portable and pluggable into other applications. We also describe the extensible architecture of CRISNER, which can be extended to new reference formalisms based on ceteris paribus semantics that may be developed in the future.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08826",
        "title": "Studying a set of properties of inconsistency indices for pairwise comparisons",
        "authors": [
            "Matteo Brunelli"
        ],
        "abstract": "Pairwise comparisons between alternatives are a well-established tool to decompose decision problems into smaller and more easily tractable sub-problems. However, due to our limited rationality, the subjective preferences expressed by decision makers over pairs of alternatives can hardly ever be consistent. Therefore, several inconsistency indices have been proposed in the literature to quantify the extent of the deviation from complete consistency. Only recently, a set of properties has been proposed to define a family of functions representing inconsistency indices. The scope of this paper is twofold. Firstly, it expands the set of properties by adding and justifying a new one. Secondly, it continues the study of inconsistency indices to check whether or not they satisfy the above mentioned properties. Out of the four indices considered in this paper, in its present form, two fail to satisfy some properties. An adjusted version of one index is proposed so that it fulfills them.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00019",
        "title": "A Minimal Architecture for General Cognition",
        "authors": [
            "Michael S. Gashler",
            "Zachariah Kindle",
            "Michael R. Smith"
        ],
        "abstract": "A minimalistic cognitive architecture called MANIC is presented. The MANIC architecture requires only three function approximating models, and one state machine. Even with so few major components, it is theoretically sufficient to achieve functional equivalence with all other cognitive architectures, and can be practically trained. Instead of seeking to transfer architectural inspiration from biology into artificial intelligence, MANIC seeks to minimize novelty and follow the most well-established constructs that have evolved within various sub-fields of data science. From this perspective, MANIC offers an alternate approach to a long-standing objective of artificial intelligence. This paper provides a theoretical analysis of the MANIC architecture.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00212",
        "title": "Procedural Content Generation for GDL Descriptions of Simplified Boardgames",
        "authors": [
            "Jakub Kowalski",
            "Marek Szyku\u0142a"
        ],
        "abstract": "We present initial research towards procedural generation of Simplified Boardgames and translating them into an efficient GDL code. This is a step towards establishing Simplified Boardgames as a comparison class for General Game Playing agents. To generate playable, human readable, and balanced chess-like games we use an adaptive evolutionary algorithm with the fitness function based on simulated playouts. In future, we plan to use the proposed method to diversify and extend the set of GGP tournament games by those with fully automatically generated rules.\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2015-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00280",
        "title": "Learning from Pairwise Marginal Independencies",
        "authors": [
            "Johannes Textor",
            "Alexander Idelberger",
            "Maciej Li\u015bkiewicz"
        ],
        "abstract": "We consider graphs that represent pairwise marginal independencies amongst a set of variables (for instance, the zero entries of a covariance matrix for normal data). We characterize the directed acyclic graphs (DAGs) that faithfully explain a given set of independencies, and derive algorithms to efficiently enumerate such structures. Our results map out the space of faithful causal models for a given set of pairwise marginal independence relations. This allows us to show the extent to which causal inference is possible without using conditional independence tests.\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2015-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00377",
        "title": "Using Behavior Objects to Manage Complexity in Virtual Worlds",
        "authors": [
            "Martin \u010cern\u00fd",
            "Tom\u00e1\u0161 Plch",
            "Mat\u011bj Marko",
            "Jakub Gemrot",
            "Petr Ondr\u00e1\u010dek",
            "Cyril Brom"
        ],
        "abstract": "The quality of high-level AI of non-player characters (NPCs) in commercial open-world games (OWGs) has been increasing during the past years. However, due to constraints specific to the game industry, this increase has been slow and it has been driven by larger budgets rather than adoption of new complex AI techniques. Most of the contemporary AI is still expressed as hard-coded scripts. The complexity and manageability of the script codebase is one of the key limiting factors for further AI improvements. In this paper we address this issue. We present behavior objects - a general approach to development of NPC behaviors for large OWGs. Behavior objects are inspired by object-oriented programming and extend the concept of smart objects. Our approach promotes encapsulation of data and code for multiple related behaviors in one place, hiding internal details and embedding intelligence in the environment. Behavior objects are a natural abstraction of five different techniques that we have implemented to manage AI complexity in an upcoming AAA OWG. We report the details of the implementations in the context of behavior trees and the lessons learned during development. Our work should serve as inspiration for AI architecture designers from both the academia and the industry.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00509",
        "title": "Maintaining prediction quality under the condition of a growing knowledge space",
        "authors": [
            "Christoph Jahnz"
        ],
        "abstract": "Intelligence can be understood as an agent's ability to predict its environment's dynamic by a level of precision which allows it to effectively foresee opportunities and threats. Under the assumption that such intelligence relies on a knowledge space any effective reasoning would benefit from a maximum portion of useful and a minimum portion of misleading knowledge fragments. It begs the question of how the quality of such knowledge space can be kept high as the amount of knowledge keeps growing. This article proposes a mathematical model to describe general principles of how quality of a growing knowledge space evolves depending on error rate, error propagation and countermeasures. There is also shown to which extend the quality of a knowledge space collapses as removal of low quality knowledge fragments occurs too slowly for a given knowledge space's growth rate.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00749",
        "title": "Predicting respiratory motion for real-time tumour tracking in radiotherapy",
        "authors": [
            "Tomas Krilavicius",
            "Indre Zliobaite",
            "Henrikas Simonavicius",
            "Laimonas Jarusevicius"
        ],
        "abstract": "Purpose. Radiation therapy is a local treatment aimed at cells in and around a tumor. The goal of this study is to develop an algorithmic solution for predicting the position of a target in 3D in real time, aiming for the short fixed calibration time for each patient at the beginning of the procedure. Accurate predictions of lung tumor motion are expected to improve the precision of radiation treatment by controlling the position of a couch or a beam in order to compensate for respiratory motion during radiation treatment.\n",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00801",
        "title": "Identifying Avatar Aliases in Starcraft 2",
        "authors": [
            "Olivier Cavadenti",
            "Victor Codocedo",
            "Jean-Fran\u00e7ois Boulicaut",
            "Mehdi Kaytoue"
        ],
        "abstract": "In electronic sports, cyberathletes conceal their online training using different avatars (virtual identities), allowing them not being recognized by the opponents they may face in future competitions. In this article, we propose a method to tackle this avatar aliases identification problem. Our method trains a classifier on behavioural data and processes the confusion matrix to output label pairs which concentrate confusion. We experimented with Starcraft 2 and report our first results.\n    ",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00879",
        "title": "Qualitative Decision Methods for Multi-Attribute Decision Making",
        "authors": [
            "Ankit Agrawal"
        ],
        "abstract": "The fundamental problem underlying all multi-criteria decision analysis (MCDA) problems is that of dominance between any two alternatives: \"Given two alternatives A and B, each described by a set criteria, is A preferred to B with respect to a set of decision maker (DM) preferences over the criteria?\". Depending on the application in which MCDA is performed, the alternatives may represent strategies and policies for business, potential locations for setting up new facilities, designs of buildings, etc. The general objective of MCDA is to enable the DM to order all alternatives in order of the stated preferences, and choose the ones that are best, i.e., optimal with respect to the preferences over the criteria. This article presents and summarizes a recently developed MCDA framework that orders the set of alternatives when the relative importance preferences are incomplete, imprecise, or qualitative in nature.\n    ",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00986",
        "title": "On the Linear Belief Compression of POMDPs: A re-examination of current methods",
        "authors": [
            "Zhuoran Wang",
            "Paul A. Crook",
            "Wenshuo Tang",
            "Oliver Lemon"
        ],
        "abstract": "Belief compression improves the tractability of large-scale partially observable Markov decision processes (POMDPs) by finding projections from high-dimensional belief space onto low-dimensional approximations, where solving to obtain action selection policies requires fewer computations. This paper develops a unified theoretical framework to analyse three existing linear belief compression approaches, including value-directed compression and two non-negative matrix factorisation (NMF) based algorithms. The results indicate that all the three known belief compression methods have their own critical deficiencies. Therefore, projective NMF belief compression is proposed (P-NMF), aiming to overcome the drawbacks of the existing techniques. The performance of the proposed algorithm is examined on four POMDP problems of reasonably large scale, in comparison with existing techniques. Additionally, the competitiveness of belief compression is compared empirically to a state-of-the-art heuristic search based POMDP solver and their relative merits in solving large-scale POMDPs are investigated.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01191",
        "title": "A different perspective on a scale for pairwise comparisons",
        "authors": [
            "J. Fueloep",
            "W.W. Koczkodaj",
            "S.J. Szarek"
        ],
        "abstract": "One of the major challenges for collective intelligence is inconsistency, which is unavoidable whenever subjective assessments are involved. Pairwise comparisons allow one to represent such subjective assessments and to process them by analyzing, quantifying and identifying the inconsistencies.\n",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01834",
        "title": "Decomposition and Identification of Linear Structural Equation Models",
        "authors": [
            "Bryant Chen"
        ],
        "abstract": "In this paper, we address the problem of identifying linear structural equation models. We first extend the edge set half-trek criterion to cover a broader class of models. We then show that any semi-Markovian linear model can be recursively decomposed into simpler sub-models, resulting in improved identification power. Finally, we show that, unlike the existing methods developed for linear models, the resulting method subsumes the identification algorithm of non-parametric models.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2015-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02035",
        "title": "Security Games with Ambiguous Beliefs of Agents",
        "authors": [
            "Hossein Khani",
            "Mohsen Afsharchi"
        ],
        "abstract": "Currently the Dempster-Shafer based algorithm and Uniform Random Probability based algorithm are the preferred method of resolving security games, in which defenders are able to identify attackers and only strategy remained ambiguous. However this model is inefficient in situations where resources are limited and both the identity of the attackers and their strategies are ambiguous. The intent of this study is to find a more effective algorithm to guide the defenders in choosing which outside agents with which to cooperate given both ambiguities. We designed an experiment where defenders were compelled to engage with outside agents in order to maximize protection of their targets. We introduced two important notions: the behavior of each agent in target protection and the tolerance threshold in the target protection process. From these, we proposed an algorithm that was applied by each defender to determine the best potential assistant(s) with which to cooperate. Our results showed that our proposed algorithm is safer than the Dempster-Shafer based algorithm.\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2015-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02050",
        "title": "Crime Prediction Based On Crime Types And Using Spatial And Temporal Criminal Hotspots",
        "authors": [
            "Tahani Almanie",
            "Rsha Mirza",
            "Elizabeth Lor"
        ],
        "abstract": "This paper focuses on finding spatial and temporal criminal hotspots. It analyses two different real-world crimes datasets for Denver, CO and Los Angeles, CA and provides a comparison between the two datasets through a statistical analysis supported by several graphs. Then, it clarifies how we conducted Apriori algorithm to produce interesting frequent patterns for criminal hotspots. In addition, the paper shows how we used Decision Tree classifier and Naive Bayesian classifier in order to predict potential crime types. To further analyse crimes datasets, the paper introduces an analysis study by combining our findings of Denver crimes dataset with its demographics information in order to capture the factors that might affect the safety of neighborhoods. The results of this solution could be used to raise awareness regarding the dangerous locations and to help agencies to predict future crimes in a specific location within a particular time.\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2015-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02103",
        "title": "Lifted Representation of Relational Causal Models Revisited: Implications for Reasoning and Structure Learning",
        "authors": [
            "Sanghack Lee",
            "Vasant Honavar"
        ],
        "abstract": "Maier et al. (2010) introduced the relational causal model (RCM) for representing and inferring causal relationships in relational data. A lifted representation, called abstract ground graph (AGG), plays a central role in reasoning with and learning of RCM. The correctness of the algorithm proposed by Maier et al. (2013a) for learning RCM from data relies on the soundness and completeness of AGG for relational d-separation to reduce the learning of an RCM to learning of an AGG. We revisit the definition of AGG and show that AGG, as defined in Maier et al. (2013b), does not correctly abstract all ground graphs. We revise the definition of AGG to ensure that it correctly abstracts all ground graphs. We further show that AGG representation is not complete for relational d-separation, that is, there can exist conditional independence relations in an RCM that are not entailed by AGG. A careful examination of the relationship between the lack of completeness of AGG for relational d-separation and faithfulness conditions suggests that weaker notions of completeness, namely adjacency faithfulness and orientation faithfulness between an RCM and its AGG, can be used to learn an RCM from data.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02593",
        "title": "Type-Constrained Representation Learning in Knowledge Graphs",
        "authors": [
            "Denis Krompa\u00df",
            "Stephan Baier",
            "Volker Tresp"
        ],
        "abstract": "Large knowledge graphs increasingly add value to various applications that require machines to recognize and understand queries and their semantics, as in search or question answering systems. Latent variable models have increasingly gained attention for the statistical modeling of knowledge graphs, showing promising results in tasks related to knowledge graph completion and cleaning. Besides storing facts about the world, schema-based knowledge graphs are backed by rich semantic descriptions of entities and relation-types that allow machines to understand the notion of things and their semantic relationships. In this work, we study how type-constraints can generally support the statistical modeling with latent variable models. More precisely, we integrated prior knowledge in form of type-constraints in various state of the art latent variable approaches. Our experimental results show that prior knowledge on relation-types significantly improves these models up to 77% in link-prediction tasks. The achieved improvements are especially prominent when a low model complexity is enforced, a crucial requirement when these models are applied to very large datasets. Unfortunately, type-constraints are neither always available nor always complete e.g., they can become fuzzy when entities lack proper typing. We show that in these cases, it can be beneficial to apply a local closed-world assumption that approximates the semantics of relation-types based on observations made in the data.\n    ",
        "submission_date": "2015-08-11T00:00:00",
        "last_modified_date": "2015-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02681",
        "title": "Artificial Prediction Markets for Online Prediction of Continuous Variables-A Preliminary Report",
        "authors": [
            "Fatemeh Jahedpari",
            "Marina De Vos",
            "Sattar Hashemi",
            "Benjamin Hirsch",
            "Julian Padget"
        ],
        "abstract": "We propose the Artificial Continuous Prediction Market (ACPM) as a means to predict a continuous real value, by integrating a range of data sources and aggregating the results of different machine learning (ML) algorithms. ACPM adapts the concept of the (physical) prediction market to address the prediction of real values instead of discrete events. Each ACPM participant has a data source, a ML algorithm and a local decision-making procedure that determines what to bid on what value. The contributions of ACPM are: (i) adaptation to changes in data quality by the use of learning in: (a) the market, which weights each market participant to adjust the influence of each on the market prediction and (b) the participants, which use a Q-learning based trading strategy to incorporate the market prediction into their subsequent predictions, (ii) resilience to a changing population of low- and high-performing participants. We demonstrate the effectiveness of ACPM by application to an influenza-like illnesses data set, showing ACPM out-performs a range of well-known regression models and is resilient to variation in data source quality.\n    ",
        "submission_date": "2015-08-11T00:00:00",
        "last_modified_date": "2015-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03032",
        "title": "OOASP: Connecting Object-oriented and Logic Programming",
        "authors": [
            "Andreas Falkner",
            "Anna Ryabokon",
            "Gottfried Schenner",
            "Kostyantyn Shchekotykhin"
        ],
        "abstract": "Most of contemporary software systems are implemented using an object-oriented approach. Modeling phases -- during which software engineers analyze requirements to the future system using some modeling language -- are an important part of the development process, since modeling errors are often hard to recognize and correct.\n",
        "submission_date": "2015-08-12T00:00:00",
        "last_modified_date": "2015-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03170",
        "title": "Generation of Multimedia Artifacts: An Extractive Summarization-based Approach",
        "authors": [
            "Paulo Figueiredo",
            "Marta Apar\u00edcio",
            "David Martins de Matos",
            "Ricardo Ribeiro"
        ],
        "abstract": "We explore methods for content selection and address the issue of coherence in the context of the generation of multimedia artifacts. We use audio and video to present two case studies: generation of film tributes, and lecture-driven science talks. For content selection, we use centrality-based and diversity-based summarization, along with topic analysis. To establish coherence, we use the emotional content of music, for film tributes, and ensure topic similarity between lectures and documentaries, for science talks. Composition techniques for the production of multimedia artifacts are addressed as a means of organizing content, in order to improve coherence. We discuss our results considering the above aspects.\n    ",
        "submission_date": "2015-08-13T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03276",
        "title": "Talking about the Moving Image: A Declarative Model for Image Schema Based Embodied Perception Grounding and Language Generation",
        "authors": [
            "Jakob Suchan",
            "Mehul Bhatt",
            "Harshita Jhavar"
        ],
        "abstract": "We present a general theory and corresponding declarative model for the embodied grounding and natural language based analytical summarisation of dynamic visuo-spatial imagery. The declarative model ---ecompassing spatio-linguistic abstractions, image schemas, and a spatio-temporal feature based language generator--- is modularly implemented within Constraint Logic Programming (CLP). The implemented model is such that primitives of the theory, e.g., pertaining to space and motion, image schemata, are available as first-class objects with `deep semantics' suited for inference and query. We demonstrate the model with select examples broadly motivated by areas such as film, design, geography, smart environments where analytical natural language based externalisations of the moving image are central from the viewpoint of human interaction, evidence-based qualitative analysis, and sensemaking.\n",
        "submission_date": "2015-08-13T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03523",
        "title": "Sufficient and necessary conditions for Dynamic Programming in Valuation-Based Systems",
        "authors": [
            "Jordi Roca-Lacostena",
            "Jesus Cerquides",
            "Marc Pouly"
        ],
        "abstract": "Valuation algebras abstract a large number of formalisms for automated reasoning and enable the definition of generic inference procedures. Many of these formalisms provide some notion of solution. Typical examples are satisfying assignments in constraint systems, models in logics or solutions to linear equation systems.\n",
        "submission_date": "2015-08-14T00:00:00",
        "last_modified_date": "2015-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03671",
        "title": "Fuzzy Longest Common Subsequence Matching With FCM Using R",
        "authors": [
            "Ibrahim Ozkan",
            "I. Burhan Turksen"
        ],
        "abstract": "Capturing the interdependencies between real valued time series can be achieved by finding common similar patterns. The abstraction of time series makes the process of finding similarities closer to the way as humans do. Therefore, the abstraction by means of a symbolic levels and finding the common patterns attracts researchers. One particular algorithm, Longest Common Subsequence, has been used successfully as a similarity measure between two sequences including real valued time series. In this paper, we propose Fuzzy Longest Common Subsequence matching for time series.\n    ",
        "submission_date": "2015-08-14T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03686",
        "title": "Beyond-Quantum Modeling of Question Order Effects and Response Replicability in Psychological Measurements",
        "authors": [
            "Diederik Aerts",
            "Massimiliano Sassoli de Bianchi"
        ],
        "abstract": "A general tension-reduction (GTR) model was recently considered to derive quantum probabilities as (universal) averages over all possible forms of non-uniform fluctuations, and explain their considerable success in describing experimental situations also outside of the domain of physics, for instance in the ambit of quantum models of cognition and decision. Yet, this result also highlighted the possibility of observing violations of the predictions of the Born rule, in those situations where the averaging would not be large enough, or would be altered because of the combination of multiple measurements. In this article we show that this is indeed the case in typical psychological measurements exhibiting question order effects, by showing that their statistics of outcomes are inherently non-Hilbertian, and require the larger framework of the GTR-model to receive an exact mathematical description. We also consider another unsolved problem of quantum cognition: response replicability. It is has been observed that when question order effects and response replicability occur together, the situation cannot be handled anymore by quantum theory. However, we show that it can be easily and naturally described in the GTR-model. Based on these findings, we motivate the adoption in cognitive science of a hidden-measurements interpretation of the quantum formalism, and of its GTR-model generalization, as the natural interpretational framework explaining the data of psychological measurements on conceptual entities.\n    ",
        "submission_date": "2015-08-15T00:00:00",
        "last_modified_date": "2015-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03812",
        "title": "Causal Decision Trees",
        "authors": [
            "Jiuyong Li",
            "Saisai Ma",
            "Thuc Duy Le",
            "Lin Liu",
            "Jixue Liu"
        ],
        "abstract": "Uncovering causal relationships in data is a major objective of data analytics. Causal relationships are normally discovered with designed experiments, e.g. randomised controlled trials, which, however are expensive or infeasible to be conducted in many cases. Causal relationships can also be found using some well designed observational studies, but they require domain experts' knowledge and the process is normally time consuming. Hence there is a need for scalable and automated methods for causal relationship exploration in data. Classification methods are fast and they could be practical substitutes for finding causal signals in data. However, classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones. In this paper, we develop a causal decision tree where nodes have causal interpretations. Our method follows a well established causal inference framework and makes use of a classic statistical test. The method is practical for finding causal signals in large data sets.\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2015-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03819",
        "title": "From Observational Studies to Causal Rule Mining",
        "authors": [
            "Jiuyong Li",
            "Thuc Duy Le",
            "Lin Liu",
            "Jixue Liu",
            "Zhou Jin",
            "Bingyu Sun",
            "Saisai Ma"
        ],
        "abstract": "Randomised controlled trials (RCTs) are the most effective approach to causal discovery, but in many circumstances it is impossible to conduct RCTs. Therefore observational studies based on passively observed data are widely accepted as an alternative to RCTs. However, in observational studies, prior knowledge is required to generate the hypotheses about the cause-effect relationships to be tested, hence they can only be applied to problems with available domain knowledge and a handful of variables. In practice, many data sets are of high dimensionality, which leaves observational studies out of the opportunities for causal discovery from such a wealth of data sources. In another direction, many efficient data mining methods have been developed to identify associations among variables in large data sets. The problem is, causal relationships imply associations, but the reverse is not always true. However we can see the synergy between the two paradigms here. Specifically, association rule mining can be used to deal with the high-dimensionality problem while observational studies can be utilised to eliminate non-causal associations. In this paper we propose the concept of causal rules (CRs) and develop an algorithm for mining CRs in large data sets. We use the idea of retrospective cohort studies to detect CRs based on the results of association rule mining. Experiments with both synthetic and real world data sets have demonstrated the effectiveness and efficiency of CR mining. In comparison with the commonly used causal discovery methods, the proposed approach in general is faster and has better or competitive performance in finding correct or sensible causes. It is also capable of finding a cause consisting of multiple variables, a feature that other causal discovery methods do not possess.\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2015-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03863",
        "title": "Discrete Route/Trajectory Decision Making Problems",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper focuses on composite multistage decision making problems which are targeted to design a route/trajectory from an initial decision situation (origin) to goal (destination) decision situation(s). Automobile routing problem is considered as a basic physical metaphor. The problems are based on a discrete (combinatorial) operations/states design/solving space (e.g., digraph). The described types of discrete decision making problems can be considered as intelligent design of a route (trajectory, strategy) and can be used in many domains: (a) education (planning of student educational trajectory), (b) medicine (medical treatment), (c) economics (trajectory of start-up development). Several types of the route decision making problems are described: (i) basic route decision making, (ii) multi-goal route decision making, (iii) multi-route decision making, (iv) multi-route decision making with route/trajectory change(s), (v) composite multi-route decision making (solution is a composition of several routes/trajectories at several corresponding domains), and (vi) composite multi-route decision making with coordinated routes/trajectories. In addition, problems of modeling and building the design spaces are considered. Numerical examples illustrate the suggested approach. Three applications are considered: educational trajectory (orienteering problem), plan of start-up company (modular three-stage design), and plan of medical treatment (planning over digraph with two-component vertices).\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2015-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04032",
        "title": "Variable Elimination in the Fourier Domain",
        "authors": [
            "Yexiang Xue",
            "Stefano Ermon",
            "Ronan Le Bras",
            "Carla P. Gomes",
            "Bart Selman"
        ],
        "abstract": "The ability to represent complex high dimensional probability distributions in a compact form is one of the key insights in the field of graphical models. Factored representations are ubiquitous in machine learning and lead to major computational advantages. We explore a different type of compact representation based on discrete Fourier representations, complementing the classical approach based on conditional independencies. We show that a large class of probabilistic graphical models have a compact Fourier representation. This theoretical result opens up an entirely new way of approximating a probability distribution. We demonstrate the significance of this approach by applying it to the variable elimination algorithm. Compared with the traditional bucket representation and other approximate inference algorithms, we obtain significant improvements.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04087",
        "title": "The SP theory of intelligence: distinctive features and advantages",
        "authors": [
            "J. G. Wolff"
        ],
        "abstract": "This paper highlights distinctive features of the \"SP theory of intelligence\" and its apparent advantages compared with some AI-related alternatives. Distinctive features and advantages are: simplification and integration of observations and concepts; simplification and integration of structures and processes in computing systems; the theory is itself a theory of computing; it can be the basis for new architectures for computers; information compression via the matching and unification of patterns and, more specifically, via multiple alignment, is fundamental; transparency in the representation and processing of knowledge; the discovery of 'natural' structures via information compression (DONSVIC); interpretations of mathematics; interpretations in human perception and cognition; and realisation of abstract concepts in terms of neurons and their inter-connections (\"SP-neural\"). These things relate to AI-related alternatives: minimum length encoding and related concepts; deep learning in neural networks; unified theories of cognition and related research; universal search; Bayesian networks and more; pattern recognition and vision; the analysis, production, and translation of natural language; Unsupervised learning of natural language; exact and inexact forms of reasoning; representation and processing of diverse forms of knowledge; IBM's Watson; software engineering; solving problems associated with big data, and in the development of intelligence in autonomous robots. In conclusion, the SP system can provide a firm foundation for the long-term development of AI, with many potential benefits and applications. It may also deliver useful results on relatively short timescales. A high-parallel, open-source version of the SP machine, derived from the SP computer model, would be a means for researchers everywhere to explore what can be done with the system, and to create new versions of it.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2016-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04145",
        "title": "Reflective Oracles: A Foundation for Classical Game Theory",
        "authors": [
            "Benja Fallenstein",
            "Jessica Taylor",
            "Paul F. Christiano"
        ],
        "abstract": "Classical game theory treats players as special---a description of a game contains a full, explicit enumeration of all players---even though in the real world, \"players\" are no more fundamentally special than rocks or clouds. It isn't trivial to find a decision-theoretic foundation for game theory in which an agent's coplayers are a non-distinguished part of the agent's environment. Attempts to model both players and the environment as Turing machines, for example, fail for standard diagonalization reasons.\n",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04261",
        "title": "Learning Modulo Theories for preference elicitation in hybrid domains",
        "authors": [
            "Paolo Campigotto",
            "Roberto Battiti",
            "Andrea Passerini"
        ],
        "abstract": "This paper introduces CLEO, a novel preference elicitation algorithm capable of recommending complex objects in hybrid domains, characterized by both discrete and continuous attributes and constraints defined over them. The algorithm assumes minimal initial information, i.e., a set of catalog attributes, and defines decisional features as logic formulae combining Boolean and algebraic constraints over the attributes. The (unknown) utility of the decision maker (DM) is modelled as a weighted combination of features. CLEO iteratively alternates a preference elicitation step, where pairs of candidate solutions are selected based on the current utility model, and a refinement step where the utility is refined by incorporating the feedback received. The elicitation step leverages a Max-SMT solver to return optimal hybrid solutions according to the current utility model. The refinement step is implemented as learning to rank, and a sparsifying norm is used to favour the selection of few informative features in the combinatorial space of candidate decisional features.\n",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04570",
        "title": "Proposal for the creation of a research facility for the development of the SP machine",
        "authors": [
            "J. Gerard Wolff",
            "Vasile Palade"
        ],
        "abstract": "This is a proposal to create a research facility for the development of a high-parallel version of the \"SP machine\", based on the \"SP theory of intelligence\". We envisage that the new version of the SP machine will be an open-source software virtual machine, derived from the existing \"SP computer model\", and hosted on an existing high-performance computer. It will be a means for researchers everywhere to explore what can be done with the system and to create new versions of it. The SP system is a unique attempt to simplify and integrate observations and concepts across artificial intelligence, mainstream computing, mathematics, and human perception and cognition, with information compression as a unifying theme. Potential benefits and applications include helping to solve problems associated with big data; facilitating the development of autonomous robots; unsupervised learning, natural language processing, several kinds of reasoning, fuzzy pattern recognition at multiple levels of abstraction, computer vision, best-match and semantic forms of information retrieval, software engineering, medical diagnosis, simplification of computing systems, and the seamless integration of diverse kinds of knowledge and diverse aspects of intelligence. Additional motivations include the potential of the SP system to help solve problems in defence, security, and the detection and prevention of crime; potential in terms of economic, social, environmental, and academic criteria, and in terms of publicity; and the potential for international influence in research. The main elements of the proposed facility are described, including support for the development of \"SP-neural\", a neural version of the SP machine. The facility should be permanent in the sense that it should be available for the foreseeable future, and it should be designed to facilitate its use by researchers anywhere in the world.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04633",
        "title": "Drawing and Analyzing Causal DAGs with DAGitty",
        "authors": [
            "Johannes Textor"
        ],
        "abstract": "DAGitty is a software for drawing and analyzing causal diagrams, also known as directed acyclic graphs (DAGs). Functions include identification of minimal sufficient adjustment sets for estimating causal effects, diagnosis of insufficient or invalid adjustment via the identification of biasing paths, identification of instrumental variables, and derivation of testable implications. DAGitty is provided in the hope that it is useful for researchers and students in Epidemiology, Sociology, Psychology, and other empirical disciplines. The software should run in any web browser that supports modern JavaScript, HTML, and SVG. This is the user manual for DAGitty version 2.3. The manual is updated with every release of a new stable version. DAGitty is available at ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04872",
        "title": "Warehouse Layout Method Based on Ant Colony and Backtracking Algorithm",
        "authors": [
            "Ardy Wibowo Haryanto",
            "Adhi Kusnadi",
            "Yustinus Eko Soelistio"
        ],
        "abstract": "Warehouse is one of the important aspects of a company. Therefore, it is necessary to improve Warehouse Management System (WMS) to have a simple function that can determine the layout of the storage goods. In this paper we propose an improved warehouse layout method based on ant colony algorithm and backtracking algorithm. The method works on two steps. First, it generates a solutions parameter tree from backtracking algorithm. Then second, it deducts the solutions parameter by using a combination of ant colony algorithm and backtracking algorithm. This method was tested by measuring the time needed to build the tree and to fill up the space using two scenarios. The method needs 0.294 to 33.15 seconds to construct the tree and 3.23 seconds (best case) to 61.41 minutes (worst case) to fill up the warehouse. This method is proved to be an attractive alternative solution for warehouse layout system.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04885",
        "title": "Efficient Computation of Exact IRV Margins",
        "authors": [
            "Michelle Blom",
            "Peter J. Stuckey",
            "Vanessa J. Teague",
            "Ron Tidhar"
        ],
        "abstract": "The margin of victory is easy to compute for many election schemes but difficult for Instant Runoff Voting (IRV). This is important because arguments about the correctness of an election outcome usually rely on the size of the electoral margin. For example, risk-limiting audits require a knowledge of the margin of victory in order to determine how much auditing is necessary. This paper presents a practical branch-and-bound algorithm for exact IRV margin computation that substantially improves on the current best-known approach. Although exponential in the worst case, our algorithm runs efficiently in practice on all the real examples we could find. We can efficiently discover exact margins on election instances that cannot be solved by the current state-of-the-art.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04928",
        "title": "Duration and Interval Hidden Markov Model for Sequential Data Analysis",
        "authors": [
            "Hiromi Narimatsu",
            "Hiroyuki Kasai"
        ],
        "abstract": "Analysis of sequential event data has been recognized as one of the essential tools in data modeling and analysis field. In this paper, after the examination of its technical requirements and issues to model complex but practical situation, we propose a new sequential data model, dubbed Duration and Interval Hidden Markov Model (DI-HMM), that efficiently represents \"state duration\" and \"state interval\" of data events. This has significant implications to play an important role in representing practical time-series sequential data. This eventually provides an efficient and flexible sequential data retrieval. Numerical experiments on synthetic and real data demonstrate the efficiency and accuracy of the proposed DI-HMM.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05013",
        "title": "Message Passing and Combinatorial Optimization",
        "authors": [
            "Siamak Ravanbakhsh"
        ],
        "abstract": "Graphical models use the intuitive and well-studied methods of graph theory to implicitly represent dependencies between variables in large systems. They can model the global behaviour of a complex system by specifying only local factors. This thesis studies inference in discrete graphical models from an algebraic perspective and the ways inference can be used to express and approximate NP-hard combinatorial problems.\n",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05128",
        "title": "Lifted Relational Neural Networks",
        "authors": [
            "Gustav Sourek",
            "Vojtech Aschenbrenner",
            "Filip Zelezny",
            "Ondrej Kuzelka"
        ],
        "abstract": "We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05508",
        "title": "Towards Neural Network-based Reasoning",
        "authors": [
            "Baolin Peng",
            "Zhengdong Lu",
            "Hang Li",
            "Kam-Fai Wong"
        ],
        "abstract": "We propose Neural Reasoner, a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular structure exists in the question and facts, Neural Reasoner is able to accommodate different types of reasoning and different forms of language expressions. Despite the model complexity, Neural Reasoner can still be trained effectively in an end-to-end manner. Our empirical studies show that Neural Reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks (Positional Reasoning and Path Finding) proposed in [8]. For example, it improves the accuracy on Path Finding(10K) from 33.4% [6] to over 98%.\n    ",
        "submission_date": "2015-08-22T00:00:00",
        "last_modified_date": "2015-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05804",
        "title": "A note on the complexity of the causal ordering problem",
        "authors": [
            "Bernardo Gon\u00e7alves",
            "Fabio Porto"
        ],
        "abstract": "In this note we provide a concise report on the complexity of the causal ordering problem, originally introduced by Simon to reason about causal dependencies implicit in systems of mathematical equations. We show that Simon's classical algorithm to infer causal ordering is NP-Hard---an intractability previously guessed but never proven. We present then a detailed account based on Nayak's suggested algorithmic solution (the best available), which is dominated by computing transitive closure---bounded in time by $O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$ is the input system structure composed of a set $\\mathcal E$ of equations over a set $\\mathcal V$ of variables with number of variable appearances (density) $|\\mathcal S|$. We also comment on the potential of causal ordering for emerging applications in large-scale hypothesis management and analytics.\n    ",
        "submission_date": "2015-08-24T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06924",
        "title": "Using Thought-Provoking Children's Questions to Drive Artificial Intelligence Research",
        "authors": [
            "Erik T. Mueller",
            "Henry Minsky"
        ],
        "abstract": "We propose to use thought-provoking children's questions (TPCQs), namely Highlights BrainPlay questions, as a new method to drive artificial intelligence research and to evaluate the capabilities of general-purpose AI systems. These questions are designed to stimulate thought and learning in children, and they can be used to do the same thing in AI systems, while demonstrating the system's reasoning capabilities to the evaluator. We introduce the TPCQ task, which which takes a TPCQ question as input and produces as output (1) answers to the question and (2) learned generalizations. We discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay questions, and we report statistics on question type, question class, answer cardinality, answer class, types of knowledge needed, and types of reasoning needed. We find that BrainPlay questions span many aspects of intelligence. Because the answers to BrainPlay questions and the generalizations learned from them are often highly open-ended, we suggest using human judges for evaluation.\n    ",
        "submission_date": "2015-08-27T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06973",
        "title": "The Relation Between Acausality and Interference in Quantum-Like Bayesian Networks",
        "authors": [
            "Catarina Moreira",
            "Andreas Wichert"
        ],
        "abstract": "We analyse a quantum-like Bayesian Network that puts together cause/effect relationships and semantic similarities between events. These semantic similarities constitute acausal connections according to the Synchronicity principle and provide new relationships to quantum like probabilistic graphical models. As a consequence, beliefs (or any other event) can be represented in vector spaces, in which quantum parameters are determined by the similarities that these vectors share between them. Events attached by a semantic meaning do not need to have an explanation in terms of cause and effect.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2015-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07092",
        "title": "Mining Combined Causes in Large Data Sets",
        "authors": [
            "Saisai Ma",
            "Jiuyong Li",
            "Lin Liu",
            "Thuc Duy Le"
        ],
        "abstract": "In recent years, many methods have been developed for detecting causal relationships in observational data. Some of them have the potential to tackle large data sets. However, these methods fail to discover a combined cause, i.e. a multi-factor cause consisting of two or more component variables which individually are not causes. A straightforward approach to uncovering a combined cause is to include both individual and combined variables in the causal discovery using existing methods, but this scheme is computationally infeasible due to the huge number of combined variables. In this paper, we propose a novel approach to address this practical causal discovery problem, i.e. mining combined causes in large data sets. The experiments with both synthetic and real world data sets show that the proposed method can obtain high-quality causal discoveries with a high computational efficiency.\n    ",
        "submission_date": "2015-08-28T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07678",
        "title": "Online Model Evaluation in a Large-Scale Computational Advertising Platform",
        "authors": [
            "Shahriar Shariat",
            "Burkay Orten",
            "Ali Dasdan"
        ],
        "abstract": "Online media provides opportunities for marketers through which they can deliver effective brand messages to a wide range of audiences. Advertising technology platforms enable advertisers to reach their target audience by delivering ad impressions to online users in real time. In order to identify the best marketing message for a user and to purchase impressions at the right price, we rely heavily on bid prediction and optimization models. Even though the bid prediction models are well studied in the literature, the equally important subject of model evaluation is usually overlooked. Effective and reliable evaluation of an online bidding model is crucial for making faster model improvements as well as for utilizing the marketing budgets more efficiently. In this paper, we present an experimentation framework for bid prediction models where our focus is on the practical aspects of model evaluation. Specifically, we outline the unique challenges we encounter in our platform due to a variety of factors such as heterogeneous goal definitions, varying budget requirements across different campaigns, high seasonality and the auction-based environment for inventory purchasing. Then, we introduce return on investment (ROI) as a unified model performance (i.e., success) metric and explain its merits over more traditional metrics such as click-through rate (CTR) or conversion rate (CVR). Most importantly, we discuss commonly used evaluation and metric summarization approaches in detail and propose a more accurate method for online evaluation of new experimental models against the baseline. Our meta-analysis-based approach addresses various shortcomings of other methods and yields statistically robust conclusions that allow us to conclude experiments more quickly in a reliable manner. We demonstrate the effectiveness of our evaluation strategy on real campaign data through some experiments.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00584",
        "title": "Turing's Imitation Game has been Improved",
        "authors": [
            "Norbert B\u00e1tfai"
        ],
        "abstract": "Using the recently introduced universal computing model, called orchestrated machine, that represents computations in a dissipative environment, we consider a new kind of interpretation of Turing's Imitation Game. In addition we raise the question whether the intelligence may show fractal properties. Then we sketch a vision of what robotic cars are going to do in the future. Finally we give the specification of an artificial life game based on the concept of orchestrated machines. The purpose of this paper is to start the search for possible relationships between these different topics.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01023",
        "title": "Generating Weather Forecast Texts with Case Based Reasoning",
        "authors": [
            "Ibrahim Adeyanju"
        ],
        "abstract": "Several techniques have been used to generate weather forecast texts. In this paper, case based reasoning (CBR) is proposed for weather forecast text generation because similar weather conditions occur over time and should have similar forecast texts. CBR-METEO, a system for generating weather forecast texts was developed using a generic framework (jCOLIBRI) which provides modules for the standard components of the CBR architecture. The advantage in a CBR approach is that systems can be built in minimal time with far less human effort after initial consultation with experts. The approach depends heavily on the goodness of the retrieval and revision components of the CBR process. We evaluated CBRMETEO with NIST, an automated metric which has been shown to correlate well with human judgements for this domain. The system shows comparable performance with other NLG systems that perform the same task.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01040",
        "title": "Building a Truly Distributed Constraint Solver with JADE",
        "authors": [
            "Ibrahim Adeyanju"
        ],
        "abstract": "Real life problems such as scheduling meeting between people at different locations can be modelled as distributed Constraint Satisfaction Problems (CSPs). Suitable and satisfactory solutions can then be found using constraint satisfaction algorithms which can be exhaustive (backtracking) or otherwise (local search). However, most research in this area tested their algorithms by simulation on a single PC with a single program entry point. The main contribution of our work is the design and implementation of a truly distributed constraint solver based on a local search algorithm using Java Agent DEvelopment framework (JADE) to enable communication between agents on different machines. Particularly, we discuss design and implementation issues related to truly distributed constraint solver which might not be critical when simulated on a single machine. Evaluation results indicate that our truly distributed constraint solver works well within the observed limitations when tested with various distributed CSPs. Our application can also incorporate any constraint solving algorithm with little modifications.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01379",
        "title": "Ontology Based SMS Controller for Smart Phones",
        "authors": [
            "Mohammed A. Balubaid",
            "Umar Manzoor"
        ],
        "abstract": "Text analysis includes lexical analysis of the text and has been widely studied and used in diverse applications. In the last decade, researchers have proposed many efficient solutions to analyze / classify large text dataset, however, analysis / classification of short text is still a challenge because 1) the data is very sparse 2) It contains noise words and 3) It is difficult to understand the syntactical structure of the text. Short Messaging Service (SMS) is a text messaging service for mobile/smart phone and this service is frequently used by all mobile users. Because of the popularity of SMS service, marketing companies nowadays are also using this service for direct marketing also known as SMS ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01469",
        "title": "Quantization based Fast Inner Product Search",
        "authors": [
            "Ruiqi Guo",
            "Sanjiv Kumar",
            "Krzysztof Choromanski",
            "David Simcha"
        ],
        "abstract": "We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01549",
        "title": "Giraffe: Using Deep Reinforcement Learning to Play Chess",
        "authors": [
            "Matthew Lai"
        ],
        "abstract": "This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01644",
        "title": "Reinforcement Learning with Parameterized Actions",
        "authors": [
            "Warwick Masson",
            "Pravesh Ranchod",
            "George Konidaris"
        ],
        "abstract": "We introduce a model-free algorithm for learning in Markov decision processes with parameterized actions-discrete actions with continuous parameters. At each step the agent must select both which action to use and which parameters to use with that action. We introduce the Q-PAMDP algorithm for learning in these domains, show that it converges to a local optimum, and compare it to direct policy search in the goal-scoring and Platform domains.\n    ",
        "submission_date": "2015-09-05T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02012",
        "title": "Bounded Situation Calculus Action Theories",
        "authors": [
            "Giuseppe De Giacomo",
            "Yves Lesp\u00e9rance",
            "Fabio Patrizi"
        ],
        "abstract": "In this paper, we investigate bounded action theories in the situation calculus. A bounded action theory is one which entails that, in every situation, the number of object tuples in the extension of fluents is bounded by a given constant, although such extensions are in general different across the infinitely many situations. We argue that such theories are common in applications, either because facts do not persist indefinitely or because the agent eventually forgets some facts, as new ones are learnt. We discuss various classes of bounded action theories. Then we show that verification of a powerful first-order variant of the mu-calculus is decidable for such theories. Notably, this variant supports a controlled form of quantification across situations. We also show that through verification, we can actually check whether an arbitrary action theory maintains boundedness.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02151",
        "title": "C3: Lightweight Incrementalized MCMC for Probabilistic Programs using Continuations and Callsite Caching",
        "authors": [
            "Daniel Ritchie",
            "Andreas Stuhlm\u00fcller",
            "Noah D. Goodman"
        ],
        "abstract": "Lightweight, source-to-source transformation approaches to implementing MCMC for probabilistic programming languages are popular for their simplicity, support of existing deterministic code, and ability to execute on existing fast runtimes. However, they are also slow, requiring a complete re-execution of the program on every Metropolis Hastings proposal. We present a new extension to the lightweight approach, C3, which enables efficient, incrementalized re-execution of MH proposals. C3 is based on two core ideas: transforming probabilistic programs into continuation passing style (CPS), and caching the results of function calls. We show that on several common models, C3 reduces proposal runtime by 20-100x, in some cases reducing runtime complexity from linear in model size to constant. We also demonstrate nearly an order of magnitude speedup on a complex inverse procedural modeling application.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02384",
        "title": "A unified heuristic and an annotated bibliography for a large class of earliness-tardiness scheduling problems",
        "authors": [
            "Arthur Kramer",
            "Anand Subramanian"
        ],
        "abstract": "This work proposes a unified heuristic algorithm for a large class of earliness-tardiness (E-T) scheduling problems. We consider single/parallel machine E-T problems that may or may not consider some additional features such as idle time, setup times and release dates. In addition, we also consider those problems whose objective is to minimize either the total (average) weighted completion time or the total (average) weighted flow time, which arise as particular cases when the due dates of all jobs are either set to zero or to their associated release dates, respectively. The developed local search based metaheuristic framework is quite simple, but at the same time relies on sophisticated procedures for efficiently performing local search according to the characteristics of the problem. We present efficient move evaluation approaches for some parallel machine problems that generalize the existing ones for single machine problems. The algorithm was tested in hundreds of instances of several E-T problems and particular cases. The results obtained show that our unified heuristic is capable of producing high quality solutions when compared to the best ones available in the literature that were obtained by specific methods. Moreover, we provide an extensive annotated bibliography on the problems related to those considered in this work, where we not only indicate the approach(es) used in each publication, but we also point out the characteristics of the problem(s) considered. Beyond that, we classify the existing methods in different categories so as to have a better idea of the popularity of each type of solution procedure.\n    ",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02413",
        "title": "Learning Efficient Representations for Reinforcement Learning",
        "authors": [
            "Yanping Huang"
        ],
        "abstract": "Markov decision processes (MDPs) are a well studied framework for solving sequential decision making problems under uncertainty. Exact methods for solving MDPs based on dynamic programming such as policy iteration and value iteration are effective on small problems. In problems with a large discrete state space or with continuous state spaces, a compact representation is essential for providing an efficient approximation solutions to MDPs. Commonly used approximation algorithms involving constructing basis functions for projecting the value function onto a low dimensional subspace, and building a factored or hierarchical graphical model to decompose the transition and reward functions. However, hand-coding a good compact representation for a given reinforcement learning (RL) task can be quite difficult and time consuming. Recent approaches have attempted to automatically discover efficient representations for RL.\n",
        "submission_date": "2015-08-28T00:00:00",
        "last_modified_date": "2015-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02459",
        "title": "Evolving TSP heuristics using Multi Expression Programming",
        "authors": [
            "Mihai Oltean",
            "D. Dumitrescu"
        ],
        "abstract": "Multi Expression Programming (MEP) is an evolutionary technique that may be used for solving computationally difficult problems. MEP uses a linear solution representation. Each MEP individual is a string encoding complex expressions (computer programs). A MEP individual may encode multiple solutions of the current problem. In this paper MEP is used for evolving a Traveling Salesman Problem (TSP) heuristic for graphs satisfying triangle inequality. Evolved MEP heuristic is compared with Nearest Neighbor Heuristic (NN) and Minimum Spanning Tree Heuristic (MST) on some difficult problems in TSPLIB. For most of the considered problems the evolved MEP heuristic outperforms NN and MST. The obtained algorithm was tested against some problems in TSPLIB. The results emphasizes that evolved MEP heuristic is a powerful tool for solving difficult TSP instances.\n    ",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02709",
        "title": "A Topological Approach to Meta-heuristics: Analytical Results on the BFS vs. DFS Algorithm Selection Problem",
        "authors": [
            "Tom Everitt",
            "Marcus Hutter"
        ],
        "abstract": "Search is a central problem in artificial intelligence, and breadth-first search (BFS) and depth-first search (DFS) are the two most fundamental ways to search. In this paper we derive estimates for average BFS and DFS runtime. The average runtime estimates can be used to allocate resources or judge the hardness of a problem. They can also be used for selecting the best graph representation, and for selecting the faster algorithm out of BFS and DFS. They may also form the basis for an analysis of more advanced search methods. The paper treats both tree search and graph search. For tree search, we employ a probabilistic model of goal distribution; for graph search, the analysis depends on an additional statistic of path redundancy and average branching factor. As an application, we use the results to predict BFS and DFS runtime on two concrete grammar problems and on the N-puzzle. Experimental verification shows that our analytical approximations come close to empirical reality.\n    ",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2018-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02962",
        "title": "Coarse-to-Fine Sequential Monte Carlo for Probabilistic Programs",
        "authors": [
            "Andreas Stuhlm\u00fcller",
            "Robert X.D. Hawkins",
            "N. Siddharth",
            "Noah D. Goodman"
        ],
        "abstract": "Many practical techniques for probabilistic inference require a sequence of distributions that interpolate between a tractable distribution and an intractable distribution of interest. Usually, the sequences used are simple, e.g., based on geometric averages between distributions. When models are expressed as probabilistic programs, the models themselves are highly structured objects that can be used to derive annealing sequences that are more sensitive to domain structure. We propose an algorithm for transforming probabilistic programs to coarse-to-fine programs which have the same marginal distribution as the original programs, but generate the data at increasing levels of detail, from coarse to fine. We apply this algorithm to an Ising model, its depth-from-disparity variation, and a factorial hidden Markov model. We show preliminary evidence that the use of coarse-to-fine models can make existing generic inference algorithms more efficient.\n    ",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2015-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03057",
        "title": "The World of Combinatorial Fuzzy Problems and the Efficiency of Fuzzy Approximation Algorithms",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "abstract": "We re-examine a practical aspect of combinatorial fuzzy problems of various types, including search, counting, optimization, and decision problems. We are focused only on those fuzzy problems that take series of fuzzy input objects and produce fuzzy values. To solve such problems efficiently, we design fast fuzzy algorithms, which are modeled by polynomial-time deterministic fuzzy Turing machines equipped with read-only auxiliary tapes and write-only output tapes and also modeled by polynomial-size fuzzy circuits composed of fuzzy gates. We also introduce fuzzy proof verification systems to model the fuzzification of nondeterminism. Those models help us identify four complexity classes: Fuzzy-FPA of fuzzy functions, Fuzzy-PA and Fuzzy-NPA of fuzzy decision problems, and Fuzzy-NPAO of fuzzy optimization problems. Based on a relative approximation scheme targeting fuzzy membership degree, we formulate two notions of \"reducibility\" in order to compare the computational complexity of two fuzzy problems. These reducibility notions make it possible to locate the most difficult fuzzy problems in Fuzzy-NPA and in Fuzzy-NPAO.\n    ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03221",
        "title": "Recurrent Neural Network Based Modeling of Gene Regulatory Network Using Bat Algorithm",
        "authors": [
            "Sudip Mandal",
            "Goutam Saha",
            "Rajat K. Pal"
        ],
        "abstract": "Correct inference of genetic regulations inside a cell is one of the greatest challenges in post genomic era for the biologist and researchers. Several intelligent techniques and models were already proposed to identify the regulatory relations among genes from the biological database like time series microarray data. Recurrent Neural Network (RNN) is one of the most popular and simple approach to model the dynamics as well as to infer correct dependencies among genes. In this paper, Bat Algorithm (BA) is applied to optimize the model parameters of RNN model of Gene Regulatory Network (GRN). Initially the proposed method is tested against small artificial network without any noise and the efficiency is observed in term of number of iteration, number of population and BA optimization parameters. The model is also validated in presence of different level of random noise for the small artificial network and that proved its ability to infer the correct inferences in presence of noise like real world dataset. In the next phase of this research, BA based RNN is applied to real world benchmark time series microarray dataset of E. coli. The results prove that it can able to identify the maximum number of true positive regulation but also include some false positive regulations. Therefore, BA is very suitable for identifying biological plausible GRN with the help RNN model.\n    ",
        "submission_date": "2015-08-21T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03247",
        "title": "An Epsilon Hierarchical Fuzzy Twin Support Vector Regression",
        "authors": [
            "Arindam Chaudhuri"
        ],
        "abstract": "The research presents epsilon hierarchical fuzzy twin support vector regression based on epsilon fuzzy twin support vector regression and epsilon twin support vector regression. Epsilon FTSVR is achieved by incorporating trapezoidal fuzzy numbers to epsilon TSVR which takes care of uncertainty existing in forecasting problems. Epsilon FTSVR determines a pair of epsilon insensitive proximal functions by solving two related quadratic programming problems. The structural risk minimization principle is implemented by introducing regularization term in primal problems of epsilon FTSVR. This yields dual stable positive definite problems which improves regression performance. Epsilon FTSVR is then reformulated as epsilon HFTSVR consisting of a set of hierarchical layers each containing epsilon FTSVR. Experimental results on both synthetic and real datasets reveal that epsilon HFTSVR has remarkable generalization performance with minimum training time.\n    ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03389",
        "title": "Multi-Attribute Proportional Representation",
        "authors": [
            "Jerome Lang",
            "Piotr Skowron"
        ],
        "abstract": "We consider the following problem in which a given number of items has to be chosen from a predefined set. Each item is described by a vector of attributes and for each attribute there is a desired distribution that the selected set should have. We look for a set that fits as much as possible the desired distributions on all attributes. Examples of applications include choosing members of a representative committee, where candidates are described by attributes such as sex, age and profession, and where we look for a committee that for each attribute offers a certain representation, i.e., a single committee that contains a certain number of young and old people, certain number of men and women, certain number of people with different professions, etc. With a single attribute the problem collapses to the apportionment problem for party-list proportional representation systems (in such case the value of the single attribute would be a political affiliation of a candidate). We study the properties of the associated subset selection rules, as well as their computation complexity.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2021-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03390",
        "title": "Measuring an Artificial Intelligence System's Performance on a Verbal IQ Test For Young Children",
        "authors": [
            "Stellan Ohlsson",
            "Robert H. Sloan",
            "Gy\u00f6rgy Tur\u00e1n",
            "Aaron Urasky"
        ],
        "abstract": "We administered the Verbal IQ (VIQ) part of the Wechsler Preschool and Primary Scale of Intelligence (WPPSI-III) to the ConceptNet 4 AI system. The test questions (e.g., \"Why do we shake hands?\") were translated into ConceptNet 4 inputs using a combination of the simple natural language processing tools that come with ConceptNet together with short Python programs that we wrote. The question answering used a version of ConceptNet based on spectral methods. The ConceptNet system scored a WPPSI-III VIQ that is average for a four-year-old child, but below average for 5 to 7 year-olds. Large variations among subtests indicate potential areas of improvement. In particular, results were strongest for the Vocabulary and Similarities subtests, intermediate for the Information subtest, and lowest for the Comprehension and Word Reasoning subtests. Comprehension is the subtest most strongly associated with common sense. The large variations among subtests and ordinary common sense strongly suggest that the WPPSI-III VIQ results do not show that \"ConceptNet has the verbal abilities a four-year-old.\" Rather, children's IQ tests offer one objective metric for the evaluation and comparison of AI systems. Also, this work continues previous research on Psychometric AI.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03527",
        "title": "Sharing HOL4 and HOL Light proof knowledge",
        "authors": [
            "Thibault Gauthier",
            "Cezary Kaliszyk"
        ],
        "abstract": "New proof assistant developments often involve concepts similar to already formalized ones. When proving their properties, a human can often take inspiration from the existing formalized proofs available in other provers or libraries. In this paper we propose and evaluate a number of methods, which strengthen proof automation by learning from proof libraries of different provers. Certain conjectures can be proved directly from the dependencies induced by similar proofs in the other library. Even if exact correspondences are not found, learning-reasoning systems can make use of the association between proved theorems and their characteristics to predict the relevant premises. Such external help can be further combined with internal advice. We evaluate the proposed knowledge-sharing methods by reproving the HOL Light and HOL4 standard libraries. The learning-reasoning system HOL(y)Hammer, whose single best strategy could automatically find proofs for 30% of the HOL Light problems, can prove 40% with the knowledge from HOL4.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03534",
        "title": "Premise Selection and External Provers for HOL4",
        "authors": [
            "Thibault Gauthier",
            "Cezary Kaliszyk"
        ],
        "abstract": "Learning-assisted automated reasoning has recently gained popularity among the users of Isabelle/HOL, HOL Light, and Mizar. In this paper, we present an add-on to the HOL4 proof assistant and an adaptation of the HOLyHammer system that provides machine learning-based premise selection and automated reasoning also for HOL4. We efficiently record the HOL4 dependencies and extract features from the theorem statements, which form a basis for premise selection. HOLyHammer transforms the HOL4 statements in the various TPTP-ATP proof formats, which are then processed by the ATPs. We discuss the different evaluation settings: ATPs, accessible lemmas, and premise numbers. We measure the performance of HOLyHammer on the HOL4 standard library. The results are combined accordingly and compared with the HOL Light experiments, showing a comparably high quality of predictions. The system directly benefits HOL4 users by automatically finding proofs dependencies that can be reconstructed by Metis.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03564",
        "title": "Lazy Factored Inference for Functional Probabilistic Programming",
        "authors": [
            "Avi Pfeffer",
            "Brian Ruttenberg",
            "Amy Sliva",
            "Michael Howard",
            "Glenn Takata"
        ],
        "abstract": "Probabilistic programming provides the means to represent and reason about complex probabilistic models using programming language constructs. Even simple probabilistic programs can produce models with infinitely many variables. Factored inference algorithms are widely used for probabilistic graphical models, but cannot be applied to these programs because all the variables and factors have to be enumerated. In this paper, we present a new inference framework, lazy factored inference (LFI), that enables factored algorithms to be used for models with infinitely many variables. LFI expands the model to a bounded depth and uses the structure of the program to precisely quantify the effect of the unexpanded part of the model, producing lower and upper bounds to the probability of the query.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03585",
        "title": "Some Supplementaries to The Counting Semantics for Abstract Argumentation",
        "authors": [
            "Fuan Pu",
            "Jian Luo",
            "Guiming Luo"
        ],
        "abstract": "Dung's abstract argumentation framework consists of a set of interacting arguments and a series of semantics for evaluating them. Those semantics partition the powerset of the set of arguments into two classes: extensions and non-extensions. In order to reason with a specific semantics, one needs to take a credulous or skeptical approach, i.e. an argument is eventually accepted, if it is accepted in one or all extensions, respectively. In our previous work \\cite{ref-pu2015counting}, we have proposed a novel semantics, called \\emph{counting semantics}, which allows for a more fine-grained assessment to arguments by counting the number of their respective attackers and defenders based on argument graph and argument game. In this paper, we continue our previous work by presenting some supplementaries about how to choose the damaging factor for the counting semantics, and what relationships with some existing approaches, such as Dung's classical semantics, generic gradual valuations. Lastly, an axiomatic perspective on the ranking semantics induced by our counting semantics are presented.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03789",
        "title": "Bio-Inspired Human Action Recognition using Hybrid Max-Product Neuro-Fuzzy Classifier and Quantum-Behaved PSO",
        "authors": [
            "Bardia Yousefi",
            "Chu Kiong Loo"
        ],
        "abstract": "Studies on computational neuroscience through functional magnetic resonance imaging (fMRI) and following biological inspired system stated that human action recognition in the brain of mammalian leads two distinct pathways in the model, which are specialized for analysis of motion (optic flow) and form information. Principally, we have defined a novel and robust form features applying active basis model as form extractor in form pathway in the biological inspired model. An unbalanced synergetic neural net-work classifies shapes and structures of human objects along with tuning its attention parameter by quantum particle swarm optimization (QPSO) via initiation of Centroidal Voronoi Tessellations. These tools utilized and justified as strong tools for following biological system model in form pathway. But the final decision has done by combination of ultimate outcomes of both pathways via fuzzy inference which increases novality of proposed model. Combination of these two brain pathways is done by considering each feature sets in Gaussian membership functions with fuzzy product inference method. Two configurations have been proposed for form pathway: applying multi-prototype human action templates using two time synergetic neural network for obtaining uniform template regarding each actions, and second scenario that it uses abstracting human action in four key-frames. Experimental results showed promising accuracy performance on different datasets (KTH and Weizmann).\n    ",
        "submission_date": "2015-09-13T00:00:00",
        "last_modified_date": "2016-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03970",
        "title": "Natural scene statistics mediate the perception of image complexity",
        "authors": [
            "Nicolas Gauvrit",
            "Fernando Soler-Toscano",
            "Hector Zenil"
        ],
        "abstract": "Humans are sensitive to complexity and regularity in patterns. The subjective perception of pattern complexity is correlated to algorithmic (Kolmogorov-Chaitin) complexity as defined in computer science, but also to the frequency of naturally occurring patterns. However, the possible mediational role of natural frequencies in the perception of algorithmic complexity remains unclear. Here we reanalyze Hsu et al. (2010) through a mediational analysis, and complement their results in a new experiment. We conclude that human perception of complexity seems partly shaped by natural scenes statistics, thereby establishing a link between the perception of complexity and the effect of natural scene statistics.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04064",
        "title": "Benchmarking for Bayesian Reinforcement Learning",
        "authors": [
            "Michael Castronovo",
            "Damien Ernst",
            "Adrien Couetoux",
            "Raphael Fonteneau"
        ],
        "abstract": "In the Bayesian Reinforcement Learning (BRL) setting, agents try to maximise the collected rewards while interacting with their environment while using some prior knowledge that is accessed beforehand. Many BRL algorithms have already been proposed, but even though a few toy examples exist in the literature, there are still no extensive or rigorous benchmarks to compare them. The paper addresses this problem, and provides a new BRL comparison methodology along with the corresponding open source library. In this methodology, a comparison criterion that measures the performance of algorithms on large sets of Markov Decision Processes (MDPs) drawn from some probability distributions is defined. In order to enable the comparison of non-anytime algorithms, our methodology also includes a detailed analysis of the computation time requirement of each algorithm. Our library is released with all source code and documentation: it includes three test problems, each of which has two different prior distributions, and seven state-of-the-art RL algorithms. Finally, our library is illustrated by comparing all the available algorithms and the results are discussed.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04513",
        "title": "On Reasoning with RDF Statements about Statements using Singleton Property Triples",
        "authors": [
            "Vinh Nguyen",
            "Olivier Bodenreider",
            "Krishnaprasad Thirunarayan",
            "Gang Fu",
            "Evan Bolton",
            "N\u00faria Queralt Rosinach",
            "Laura I. Furlong",
            "Michel Dumontier",
            "Amit Sheth"
        ],
        "abstract": "The Singleton Property (SP) approach has been proposed for representing and querying metadata about RDF triples such as provenance, time, location, and evidence. In this approach, one singleton property is created to uniquely represent a relationship in a particular context, and in general, generates a large property hierarchy in the schema. It has become the subject of important questions from Semantic Web practitioners. Can an existing reasoner recognize the singleton property triples? And how? If the singleton property triples describe a data triple, then how can a reasoner infer this data triple from the singleton property triples? Or would the large property hierarchy affect the reasoners in some way? We address these questions in this paper and present our study about the reasoning aspects of the singleton properties. We propose a simple mechanism to enable existing reasoners to recognize the singleton property triples, as well as to infer the data triples described by the singleton property triples. We evaluate the effect of the singleton property triples in the reasoning processes by comparing the performance on RDF datasets with and without singleton properties. Our evaluation uses as benchmark the LUBM datasets and the LUBM-SP datasets derived from LUBM with temporal information added through singleton properties.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04771",
        "title": "Mapping Heritability of Large-Scale Brain Networks with a Billion Connections {\\em via} Persistent Homology",
        "authors": [
            "Moo K. Chung",
            "Victoria Vilalta-Gil",
            "Paul J. Rathouz",
            "Benjamin B. Lahey",
            "David H. Zald"
        ],
        "abstract": "In many human brain network studies, we do not have sufficient number (n) of images relative to the number (p) of voxels due to the prohibitively expensive cost of scanning enough subjects. Thus, brain network models usually suffer the small-n large-p problem. Such a problem is often remedied by sparse network models, which are usually solved numerically by optimizing L1-penalties. Unfortunately, due to the computational bottleneck associated with optimizing L1-penalties, it is not practical to apply such methods to construct large-scale brain networks at the voxel-level. In this paper, we propose a new scalable sparse network model using cross-correlations that bypass the computational bottleneck. Our model can build sparse brain networks at the voxel level with p > 25000. Instead of using a single sparse parameter that may not be optimal in other studies and datasets, the computational speed gain enables us to analyze the collection of networks at every possible sparse parameter in a coherent mathematical framework via persistent homology. The method is subsequently applied in determining the extent of heritability on a functional brain network at the voxel-level for the first time using twin fMRI.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04904",
        "title": "Causal Model Analysis using Collider v-structure with Negative Percentage Mapping",
        "authors": [
            "Pramod Kumar Parida",
            "Tshilidzi Marwala",
            "Snehashish Chakraverty"
        ],
        "abstract": "A major problem of causal inference is the arrangement of dependent nodes in a directed acyclic graph (DAG) with path coefficients and observed confounders. Path coefficients do not provide the units to measure the strength of information flowing from one node to the other. Here we proposed the method of causal structure learning using collider v-structures (CVS) with Negative Percentage Mapping (NPM) to get selective thresholds of information strength, to direct the edges and subjective confounders in a DAG. The NPM is used to scale the strength of information passed through nodes in units of percentage from interval from 0 to 1. The causal structures are constructed by bottom up approach using path coefficients, causal directions and confounders, derived implementing collider v-structure and NPM. The method is self-sufficient to observe all the latent confounders present in the causal model and capable of detecting every responsible causal direction. The results are tested for simulated datasets of non-Gaussian distributions and compared with DirectLiNGAM and ICA-LiNGAM to check efficiency of the proposed method.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05181",
        "title": "Efficient Task Collaboration with Execution Uncertainty",
        "authors": [
            "Dengji Zhao",
            "Sarvapali D. Ramchurn",
            "Nicholas R. Jennings"
        ],
        "abstract": "We study a general task allocation problem, involving multiple agents that collaboratively accomplish tasks and where agents may fail to successfully complete the tasks assigned to them (known as execution uncertainty). The goal is to choose an allocation that maximises social welfare while taking their execution uncertainty into account. We show that this can be achieved by using the post-execution verification (PEV)-based mechanism if and only if agents' valuations satisfy a multilinearity condition. We then consider a more complex setting where an agent's execution uncertainty is not completely predictable by the agent alone but aggregated from all agents' private opinions (known as trust). We show that PEV-based mechanism with trust is still truthfully implementable if and only if the trust aggregation is multilinear.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05434",
        "title": "A Study Investigating Typical Concepts and Guidelines for Ontology Building",
        "authors": [
            "Thabet Slimani"
        ],
        "abstract": "In semantic technologies, the shared common understanding of the structure of information among artifacts (people or software agents) can be realized by building an ontology. To do this, it is imperative for an ontology builder to answer several questions: a) what are the main components of an ontology? b) How an ontology look likes and how it works? c) Verify if it is required to consider reusing existing ontologies or not? c) What is the complexity of the ontology to be developed? d) What are the principles of ontology design and development? e) How to evaluate an ontology? This paper answers all the key questions above. The aim of this paper is to present a set of guiding principles to help ontology developers and also inexperienced users to answer such questions.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05725",
        "title": "Backdoors into Heterogeneous Classes of SAT and CSP",
        "authors": [
            "Serge Gaspers",
            "Neeldhara Misra",
            "Sebastian Ordyniak",
            "Stefan Szeider",
            "Stanislav \u017divn\u00fd"
        ],
        "abstract": "In this paper we extend the classical notion of strong and weak backdoor sets for SAT and CSP by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes; the union of the base classes forms a heterogeneous base class. Backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones, hence they are much more desirable but possibly harder to find. We draw a detailed complexity landscape for the problem of detecting strong and weak backdoor sets into heterogeneous base classes for SAT and CSP.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05742",
        "title": "Evaluation of Protein-protein Interaction Predictors with Noisy Partially Labeled Data Sets",
        "authors": [
            "Haohan Wang",
            "Madhavi K. Ganapathiraju"
        ],
        "abstract": "Protein-protein interaction (PPI) prediction is an important problem in machine learning and computational biology. However, there is no data set for training or evaluation purposes, where all the instances are accurately labeled. Instead, what is available are instances of positive class (with possibly noisy labels) and no instances of negative class. The non-availability of negative class data is typically handled with the observation that randomly chosen protein-pairs have a nearly 100% chance of being negative class, as only 1 in 1,500 protein pairs expected is expected to be an interacting pair. In this paper, we focused on the problem that non-availability of accurately labeled testing data sets in the domain of protein-protein interaction (PPI) prediction may lead to biased evaluation results. We first showed that not acknowledging the inherent skew in the interactome (i.e. rare occurrence of positive instances) leads to an over-estimated accuracy of the predictor. Then we show that, with the belief that positive interactions are a rare category, sampling random pairs of proteins excluding known interacting proteins set as the negative testing data set could lead to an under-estimated evaluation result. We formalized those two problems to validate the above claim, and based on the formalization, we proposed a balancing method to cancel out the over-estimation with under-estimation. Finally, our experiments validated the theoretical aspects and showed that this balancing evaluation could evaluate the exact performance without availability of golden standard data sets.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06254",
        "title": "Hybrid Optimization Algorithm for Large-Scale QoS-Aware Service Composition",
        "authors": [
            "Pablo Rodriguez-Mier",
            "Manuel Mucientes",
            "Manuel Lama"
        ],
        "abstract": "In this paper we present a hybrid approach for automatic composition of Web services that generates semantic input-output based compositions with optimal end-to-end QoS, minimizing the number of services of the resulting composition. The proposed approach has four main steps: 1) generation of the composition graph for a request; 2) computation of the optimal composition that minimizes a single objective QoS function; 3) multi-step optimizations to reduce the search space by identifying equivalent and dominated services; and 4) hybrid local-global search to extract the optimal QoS with the minimum number of services. An extensive validation with the datasets of the Web Service Challenge 2009-2010 and randomly generated datasets shows that: 1) the combination of local and global optimization is a general and powerful technique to extract optimal compositions in diverse scenarios; and 2) the hybrid strategy performs better than the state-of-the-art, obtaining solutions with less services and optimal QoS.\n    ",
        "submission_date": "2015-09-21T00:00:00",
        "last_modified_date": "2015-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06594",
        "title": "A Compositional Explanation of the Pet Fish Phenomenon",
        "authors": [
            "Bob Coecke",
            "Martha Lewis"
        ],
        "abstract": "The `pet fish' phenomenon is often cited as a paradigm example of the `non-compositionality' of human concept use. We show here how this phenomenon is naturally accommodated within a compositional distributional model of meaning. This model describes the meaning of a composite concept by accounting for interaction between its constituents via their grammatical roles. We give two illustrative examples to show how the qualitative phenomena are exhibited. We go on to apply the model to experimental data, and finally discuss extensions of the formalism.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06731",
        "title": "Poker-CNN: A Pattern Learning Strategy for Making Draws and Bets in Poker Games",
        "authors": [
            "Nikolai Yakovenko",
            "Liangliang Cao",
            "Colin Raffel",
            "James Fan"
        ],
        "abstract": "Poker is a family of card games that includes many variations. We hypothesize that most poker games can be solved as a pattern matching problem, and propose creating a strong poker playing system based on a unified poker representation. Our poker player learns through iterative self-play, and improves its understanding of the game by training on the results of its previous actions without sophisticated domain knowledge. We evaluate our system on three poker games: single player video poker, two-player Limit Texas Hold'em, and finally two-player 2-7 triple draw poker. We show that our model can quickly learn patterns in these very different poker games while it improves from zero knowledge to a competitive player against human experts.\n",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06842",
        "title": "A Feature-Based Comparison of Evolutionary Computing Techniques for Constrained Continuous Optimisation",
        "authors": [
            "Shayan Poursoltan",
            "Frank Neumann"
        ],
        "abstract": "Evolutionary algorithms have been frequently applied to constrained continuous optimisation problems. We carry out feature based comparisons of different types of evolutionary algorithms such as evolution strategies, differential evolution and particle swarm optimisation for constrained continuous optimisation. In our study, we examine how sets of constraints influence the difficulty of obtaining close to optimal solutions. Using a multi-objective approach, we evolve constrained continuous problems having a set of linear and/or quadratic constraints where the different evolutionary approaches show a significant difference in performance. Afterwards, we discuss the features of the constraints that exhibit a difference in performance of the different evolutionary approaches under consideration.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07266",
        "title": "CRDT: Correlation Ratio Based Decision Tree Model for Healthcare Data Mining",
        "authors": [
            "Smita Roy",
            "Samrat Mondal",
            "Asif Ekbal"
        ],
        "abstract": "The phenomenal growth in the healthcare data has inspired us in investigating robust and scalable models for data mining. For classification problems Information Gain(IG) based Decision Tree is one of the popular choices. However, depending upon the nature of the dataset, IG based Decision Tree may not always perform well as it prefers the attribute with more number of distinct values as the splitting attribute. Healthcare datasets generally have many attributes and each attribute generally has many distinct values. In this paper, we have tried to focus on this characteristics of the datasets while analysing the performance of our proposed approach which is a variant of Decision Tree model and uses the concept of Correlation Ratio(CR). Unlike IG based approach, this CR based approach has no biasness towards the attribute with more number of distinct values. We have applied our model on some benchmark healthcare datasets to show the effectiveness of the proposed technique.\n    ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07582",
        "title": "Constructing Abstraction Hierarchies Using a Skill-Symbol Loop",
        "authors": [
            "George Konidaris"
        ],
        "abstract": "We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08056",
        "title": "Discovery and Visualization of Nonstationary Causal Models",
        "authors": [
            "Kun Zhang",
            "Biwei Huang",
            "Jiji Zhang",
            "Bernhard Sch\u00f6lkopf",
            "Clark Glymour"
        ],
        "abstract": "It is commonplace to encounter nonstationary data, of which the underlying generating process may change over time or across domains. The nonstationarity presents both challenges and opportunities for causal discovery. In this paper we propose a principled framework to handle nonstationarity, and develop some methods to address three important questions. First, we propose an enhanced constraint-based method to detect variables whose local mechanisms are nonstationary and recover the skeleton of the causal structure over observed variables. Second, we present a way to determine some causal directions by taking advantage of information carried by changing distributions. Third, we develop a method for visualizing the nonstationarity of causal modules. Experimental results on various synthetic and real-world data sets are presented to demonstrate the efficacy of our methods.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2016-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08086",
        "title": "Optimal Release Time Decision from Fuzzy Mathematical Programming Perspective",
        "authors": [
            "Arvind Kumar",
            "Adarsh Anand",
            "Pankaj Kumar Garg",
            "Mohini Agarwal"
        ],
        "abstract": "Demand for high software reliability requires rigorous testing followed by requirement of robust modeling techniques for software quality prediction. On one side, firms have to steadily manage the reliability by testing it vigorously, the optimal release time determination is their biggest concern. In past many models have been developed and much research has been devoted towards assessment of release time of software. However, majority of the work deals in crisp study. This paper addresses the problem of release time prediction using fuzzy Logic. Here we have formulated a Fuzzy release time problem considering the cost of testing under the impact of warranty period. Results show that fuzzy model has good adaptability.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08329",
        "title": "Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA for the Design of Training Graphs",
        "authors": [
            "Alberto N. Escalante-B.",
            "Laurenz Wiskott"
        ],
        "abstract": "Slow feature analysis (SFA) is an unsupervised learning algorithm that extracts slowly varying features from a time series. Graph-based SFA (GSFA) is a supervised extension that can solve regression problems if followed by a post-processing regression algorithm. A training graph specifies arbitrary connections between the training samples. The connections in current graphs, however, only depend on the rank of the involved labels. Exploiting the exact label values makes further improvements in estimation accuracy possible.\n",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08434",
        "title": "Ensemble UCT Needs High Exploitation",
        "authors": [
            "S. Ali Mirsoleimani",
            "Aske Plaat",
            "Jaap van den Herik"
        ],
        "abstract": "Recent results have shown that the MCTS algorithm (a new, adaptive, randomized optimization algorithm) is effective in a remarkably diverse set of applications in Artificial Intelligence, Operations Research, and High Energy Physics. MCTS can find good solutions without domain dependent heuristics, using the UCT formula to balance exploitation and exploration. It has been suggested that the optimum in the exploitation- exploration balance differs for different search tree sizes: small search trees needs more exploitation; large search trees need more exploration. Small search trees occur in variations of MCTS, such as parallel and ensemble approaches. This paper investigates the possibility of improving the performance of Ensemble UCT by increasing the level of exploitation. As the search trees becomes smaller we achieve an improved performance. The results are important for improving the performance of large scale parallelism of MCTS.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08717",
        "title": "Towards Unveiling the Ontology Key Features Altering Reasoner Performances",
        "authors": [
            "Nourh\u00e8ne Alaya",
            "Sadok Ben Yahia",
            "Myriam Lamolle"
        ],
        "abstract": "Reasoning with ontologies is one of the core fields of research in Description Logics. A variety of efficient reasoner with highly optimized algorithms have been developed to allow inference tasks on expressive ontology languages such as OWL(DL). However, reasoner reported computing times have exceeded and sometimes fall behind the expected theoretical values. From an empirical perspective, it is not yet well understood, which particular aspects in the ontology are reasoner performance degrading factors. In this paper, we conducted an investigation about state of art works that attempted to portray potential correlation between reasoner empirical behaviour and particular ontological features. These works were analysed and then broken down into categories. Further, we proposed a set of ontology features covering a broad range of structural and syntactic ontology characteristics. We claim that these features are good indicators of the ontology hardness level against reasoning tasks.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08761",
        "title": "Reasoning in Infinitely Valued G-IALCQ",
        "authors": [
            "Stefan Borgwardt",
            "Rafael Pe\u00f1aloza"
        ],
        "abstract": "Fuzzy Description Logics (FDLs) are logic-based formalisms used to represent and reason with vague or imprecise knowledge. It has been recently shown that reasoning in most FDLs using truth values from the interval [0,1] becomes undecidable in the presence of a negation constructor and general concept inclusion axioms. One exception to this negative result are FDLs whose semantics is based on the infinitely valued G\u00f6del t-norm (G). In this paper, we extend previous decidability results for G-IALC to deal also with qualified number restrictions. Our novel approach is based on a combination of the known crispification technique for finitely valued FDLs and the automata-based procedure originally developed for reasoning in G-IALC. The proposed approach combines the advantages of these two methods, while removing their respective drawbacks.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08764",
        "title": "On the Min-cost Traveling Salesman Problem with Drone",
        "authors": [
            "Quang Minh Ha",
            "Yves Deville",
            "Quang Dung Pham",
            "Minh Ho\u00e0ng H\u00e0"
        ],
        "abstract": "Over the past few years, unmanned aerial vehicles (UAV), also known as drones, have been adopted as part of a new logistic method in the commercial sector called \"last-mile delivery\". In this novel approach, they are deployed alongside trucks to deliver goods to customers to improve the quality of service and reduce the transportation cost. This approach gives rise to a new variant of the traveling salesman problem (TSP), called TSP with drone (TSP-D). A variant of this problem that aims to minimize the time at which truck and drone finish the service (or, in other words, to maximize the quality of service) was studied in the work of Murray and Chu (2015). In contrast, this paper considers a new variant of TSP-D in which the objective is to minimize operational costs including total transportation cost and one created by waste time a vehicle has to wait for the other. The problem is first formulated mathematically. Then, two algorithms are proposed for the solution. The first algorithm (TSP-LS) was adapted from the approach proposed by Murray and Chu (2015), in which an optimal TSP solution is converted to a feasible TSP-D solution by local searches. The second algorithm, a Greedy Randomized Adaptive Search Procedure (GRASP), is based on a new split procedure that optimally splits any TSP tour into a TSP-D solution. After a TSP-D solution has been generated, it is then improved through local search operators. Numerical results obtained on various instances of both objective functions with different sizes and characteristics are presented. The results show that GRASP outperforms TSP-LS in terms of solution quality under an acceptable running time.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08792",
        "title": "An intelligent extension of Variable Neighbourhood Search for labelling graph problems",
        "authors": [
            "Sergio Consoli",
            "Jos\u00e8 Andr\u00e8s Moreno P\u00e8rez"
        ],
        "abstract": "In this paper we describe an extension of the Variable Neighbourhood Search (VNS) which integrates the basic VNS with other complementary approaches from machine learning, statistics and experimental algorithmic, in order to produce high-quality performance and to completely automate the resulting optimization strategy. The resulting intelligent VNS has been successfully applied to a couple of optimization problems where the solution space consists of the subsets of a finite reference set. These problems are the labelled spanning tree and forest problems that are formulated on an undirected labelled graph; a graph where each edge has a label in a finite set of labels L. The problems consist on selecting the subset of labels such that the subgraph generated by these labels has an optimal spanning tree or forest, respectively. These problems have several applications in the real-world, where one aims to ensure connectivity by means of homogeneous connections.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08891",
        "title": "The Computational Principles of Learning Ability",
        "authors": [
            "Hao Wu"
        ],
        "abstract": "It has been quite a long time since AI researchers in the field of computer science stop talking about simulating human intelligence or trying to explain how brain works. Recently, represented by deep learning techniques, the field of machine learning is experiencing unprecedented prosperity and some applications with near human-level performance bring researchers confidence to imply that their approaches are the promising candidate for understanding the mechanism of human brain. However apart from several ancient philological criteria and some imaginary black box tests (Turing test, Chinese room) there is no computational level explanation, definition or criteria about intelligence or any of its components. Base on the common sense that learning ability is one critical component of intelligence and inspect from the viewpoint of mapping relations, this paper presents two laws which explains what is the \"learning ability\" as we familiar with and under what conditions a mapping relation can be acknowledged as \"Learning Model\".\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08932",
        "title": "Two Phase $Q-$learning for Bidding-based Vehicle Sharing",
        "authors": [
            "Yinlam Chow",
            "Jia Yuan Yu",
            "Marco Pavone"
        ],
        "abstract": "We consider one-way vehicle sharing systems where customers can rent a car at one station and drop it off at another. The problem we address is to optimize the distribution of cars, and quality of service, by pricing rentals appropriately. We propose a bidding approach that is inspired from auctions and takes into account the significant uncertainty inherent in the problem data (e.g., pick-up and drop-off locations, time of requests, and duration of trips). Specifically, in contrast to current vehicle sharing systems, the operator does not set prices. Instead, customers submit bids and the operator decides whether to rent or not. The operator can even accept negative bids to motivate drivers to rebalance available cars to unpopular destinations within a city. We model the operator's sequential decision-making problem as a \\emph{constrained Markov decision problem} (CMDP) and propose and rigorously analyze a novel two phase $Q$-learning algorithm for its solution. Numerical experiments are presented and discussed.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08973",
        "title": "Symbol Emergence in Robotics: A Survey",
        "authors": [
            "Tadahiro Taniguchi",
            "Takayuki Nagai",
            "Tomoaki Nakamura",
            "Naoto Iwahashi",
            "Tetsuya Ogata",
            "Hideki Asoh"
        ],
        "abstract": "Humans can learn the use of language through physical interaction with their environment and semiotic communication with other people. It is very important to obtain a computational understanding of how humans can form a symbol system and obtain semiotic skills through their autonomous mental development. Recently, many studies have been conducted on the construction of robotic systems and machine-learning methods that can learn the use of language through embodied multimodal interaction with their environment and other systems. Understanding human social interactions and developing a robot that can smoothly communicate with human users in the long term, requires an understanding of the dynamics of symbol systems and is crucially important. The embodied cognition and social interaction of participants gradually change a symbol system in a constructive manner. In this paper, we introduce a field of research called symbol emergence in robotics (SER). SER is a constructive approach towards an emergent symbol system. The emergent symbol system is socially self-organized through both semiotic communications and physical interactions with autonomous cognitive developmental agents, i.e., humans and developmental robots. Specifically, we describe some state-of-art research topics concerning SER, e.g., multimodal categorization, word discovery, and a double articulation analysis, that enable a robot to obtain words and their embodied meanings from raw sensory--motor information, including visual information, haptic information, auditory information, and acoustic speech signals, in a totally unsupervised manner. Finally, we suggest future directions of research in SER.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09240",
        "title": "Solving a Mathematical Problem in Square War: a Go-like Board Game",
        "authors": [
            "Chu Luo"
        ],
        "abstract": "In this paper, we present a board game: Square War. The game definition of Square War is similar to the classic Chinese board game Go. Then we propose a mathematical problem of the game Square War. Finally, we show that the problem can be solved by using a method of mixed mathematics and computer science.\n    ",
        "submission_date": "2015-07-26T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00604",
        "title": "Online Vision- and Action-Based Object Classification Using Both Symbolic and Subsymbolic Knowledge Representations",
        "authors": [
            "Laura Steinert",
            "Jens Hoefinghoff",
            "Josef Pauli"
        ],
        "abstract": "If a robot is supposed to roam an environment and interact with objects, it is often necessary to know all possible objects in advance, so that a database with models of all objects can be generated for visual identification. However, this constraint cannot always be fulfilled. Due to that reason, a model based object recognition cannot be used to guide the robot's interactions. Therefore, this paper proposes a system that analyzes features of encountered objects and then uses these features to compare unknown objects to already known ones. From the resulting similarity appropriate actions can be derived. Moreover, the system enables the robot to learn object categories by grouping similar objects or by splitting existing categories. To represent the knowledge a hybrid form is used, consisting of both symbolic and subsymbolic representations.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00819",
        "title": "Intelligent Search Optimization using Artificial Fuzzy Logics",
        "authors": [
            "Jai Manral"
        ],
        "abstract": "Information on the web is prodigious; searching relevant information is difficult making web users to rely on search engines for finding relevant information on the web. Search engines index and categorize web pages according to their contents using crawlers and rank them accordingly. For given user query they retrieve millions of webpages and display them to users according to web-page rank. Every search engine has their own algorithms based on certain parameters for ranking web-pages. Search Engine Optimization (SEO) is that technique by which webmasters try to improve ranking of their websites by optimizing it according to search engines ranking parameters. It is the aim of this research to identify the most popular SEO techniques used by search engines for ranking web-pages and to establish their importance for indexing and categorizing web data. The research tries to establish that using more SEO parameters in ranking algorithms helps in retrieving better search results thus increasing user satisfaction.\n",
        "submission_date": "2015-10-03T00:00:00",
        "last_modified_date": "2015-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01291",
        "title": "A Common-Factor Approach for Multivariate Data Cleaning with an Application to Mars Phoenix Mission Data",
        "authors": [
            "Dongping Fang",
            "Elizabeth Oberlin",
            "Wei Ding",
            "Samuel P. Kounaves"
        ],
        "abstract": "Data quality is fundamentally important to ensure the reliability of data for stakeholders to make decisions. In real world applications, such as scientific exploration of extreme environments, it is unrealistic to require raw data collected to be perfect. As data miners, when it is infeasible to physically know the why and the how in order to clean up the data, we propose to seek the intrinsic structure of the signal to identify the common factors of multivariate data. Using our new data driven learning method, the common-factor data cleaning approach, we address an interdisciplinary challenge on multivariate data cleaning when complex external impacts appear to interfere with multiple data measurements. Existing data analyses typically process one signal measurement at a time without considering the associations among all signals. We analyze all signal measurements simultaneously to find the hidden common factors that drive all measurements to vary together, but not as a result of the true data measurements. We use common factors to reduce the variations in the data without changing the base mean level of the data to avoid altering the physical meaning.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01463",
        "title": "Local Rademacher Complexity Bounds based on Covering Numbers",
        "authors": [
            "Yunwen Lei",
            "Lixin Ding",
            "Yingzhou Bi"
        ],
        "abstract": "This paper provides a general result on controlling local Rademacher complexities, which captures in an elegant form to relate the complexities with constraint on the expected norm to the corresponding ones with constraint on the empirical norm. This result is convenient to apply in real applications and could yield refined local Rademacher complexity bounds for function classes satisfying general entropy conditions. We demonstrate the power of our complexity bounds by applying them to derive effective generalization error bounds.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01599",
        "title": "Disjunctive Answer Set Solvers via Templates",
        "authors": [
            "Remi Brochenin",
            "Yuliya Lierler",
            "Marco Maratea"
        ],
        "abstract": "Answer set programming is a declarative programming paradigm oriented towards difficult combinatorial search problems. A fundamental task in answer set programming is to compute stable models, i.e., solutions of logic programs. Answer set solvers are the programs that perform this task. The problem of deciding whether a disjunctive program has a stable model is $\\Sigma^P_2$-complete. The high complexity of reasoning within disjunctive logic programming is responsible for few solvers capable of dealing with such programs, namely DLV, GnT, Cmodels, CLASP and WASP. In this paper we show that transition systems introduced by Nieuwenhuis, Oliveras, and Tinelli to model and analyze satisfiability solvers can be adapted for disjunctive answer set solvers. Transition systems give a unifying perspective and bring clarity in the description and comparison of solvers. They can be effectively used for analyzing, comparing and proving correctness of search algorithms as well as inspiring new ideas in the design of disjunctive answer set solvers. In this light, we introduce a general template, which accounts for major techniques implemented in disjunctive solvers. We then illustrate how this general template captures solvers DLV, GnT and Cmodels. We also show how this framework provides a convenient tool for designing new solving algorithms by means of combinations of techniques employed in different solvers.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01659",
        "title": "DKP-AOM: results for OAEI 2015",
        "authors": [
            "Muhammad Fahad"
        ],
        "abstract": "In this paper, we present the results obtained by our DKP-AOM system within the OAEI 2015 campaign. DKP-AOM is an ontology merging tool designed to merge heterogeneous ontologies. In OAEI, we have participated with its ontology mapping component which serves as a basic module capable of matching large scale ontologies before their merging. This is our first successful participation in the Conference, OA4QA and Anatomy track of OAEI. DKP-AOM is participating with two versions (DKP-AOM and DKP-AOM_lite), DKP-AOM performs coherence analysis. In OA4QA track, DKPAOM out-performed in the evaluation and generated accurate alignments allowed to answer all the queries of the evaluation. We can also see its competitive results for the conference track in the evaluation initiative among other reputed systems. In the anatomy track, it has produced alignments within an allocated time and appeared in the list of systems which produce coherent results. Finally, we discuss some future work towards the development of DKP-AOM.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01970",
        "title": "Towards a general framework for an observation and knowledge based model of occupant behaviour in office buildings",
        "authors": [
            "Khadija Tijani",
            "Dung Ngo",
            "Stephane Ploix",
            "Benjamin Haas",
            "Julie Dugdale"
        ],
        "abstract": "This paper proposes a new general approach based on Bayesian networks to model the human behaviour. This approach represents human behaviour withprobabilistic cause-effect relations based not only on previous works, but also with conditional probabilities coming either from expert knowledge or deduced from observations. The approach has been used in the co-simulation of building physics and human behaviour in order to assess the CO 2 concentration in an office.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02173",
        "title": "Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models",
        "authors": [
            "John-Alexander M. Assael",
            "Niklas Wahlstr\u00f6m",
            "Thomas B. Sch\u00f6n",
            "Marc Peter Deisenroth"
        ],
        "abstract": "Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy (\"torques\") from pixel information only. We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information. The key ingredient is a deep dynamical model for learning a low-dimensional feature embedding of images jointly with a predictive model in this low-dimensional feature space. Joint learning is crucial for long-term predictions, which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed-loop control. Compared to state-of-the-art RL methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces, is lightweight and an important step toward fully autonomous end-to-end learning from pixels to torques.\n    ",
        "submission_date": "2015-10-08T00:00:00",
        "last_modified_date": "2015-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02828",
        "title": "Gelisp: A Library to Represent Musical CSPs and Search Strategies",
        "authors": [
            "Mauricio Toro",
            "Camilo Rueda",
            "Carlos Ag\u00f3n",
            "G\u00e9rard Assayag"
        ],
        "abstract": "In this paper we present Gelisp, a new library to represent musical Constraint Satisfaction Problems and search strategies intuitively. Gelisp has two interfaces, a command-line one for Common Lisp and a graphical one for OpenMusic. Using Gelisp, we solved a problem of automatic music generation proposed by composer Michael Jarrell and we found solutions for the All-interval series.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2015-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02867",
        "title": "Artificial Intelligence and Asymmetric Information Theory",
        "authors": [
            "Tshilidzi Marwala",
            "Evan Hurwitz"
        ],
        "abstract": "When human agents come together to make decisions, it is often the case that one human agent has more information than the other. This phenomenon is called information asymmetry and this distorts the market. Often if one human agent intends to manipulate a decision in its favor the human agent can signal wrong or right information. Alternatively, one human agent can screen for information to reduce the impact of asymmetric information on decisions. With the advent of artificial intelligence, signaling and screening have been made easier. This paper studies the impact of artificial intelligence on the theory of asymmetric information. It is surmised that artificial intelligent agents reduce the degree of information asymmetry and thus the market where these agents are deployed become more efficient. It is also postulated that the more artificial intelligent agents there are deployed in the market the less is the volume of trades in the market. This is because for many trades to happen the asymmetry of information on goods and services to be traded should exist, creating a sense of arbitrage.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02879",
        "title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain",
        "authors": [
            "Janarthanan Rajendran",
            "Aravind Srinivas",
            "Mitesh M. Khapra",
            "P Prasanna",
            "Balaraman Ravindran"
        ],
        "abstract": "Transferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications. The application of transfer poses two serious challenges which have not been adequately addressed. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of helping it. Second, the agent should be able to selectively transfer, which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose A2T (Attend, Adapt and Transfer), an attentive deep architecture which adapts and transfers from these source tasks. Our model is generic enough to effect transfer of either policies or value functions. Empirical evaluations on different learning algorithms show that A2T is an effective architecture for transfer by being able to avoid negative transfer while transferring selectively from multiple source tasks in the same domain.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2020-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03042",
        "title": "ParallelPC: an R package for efficient constraint based causal exploration",
        "authors": [
            "Thuc Duy Le",
            "Tao Hoang",
            "Jiuyong Li",
            "Lin Liu",
            "Shu Hu"
        ],
        "abstract": "Discovering causal relationships from data is the ultimate goal of many research areas. Constraint based causal exploration algorithms, such as PC, FCI, RFCI, PC-simple, IDA and Joint-IDA have achieved significant progress and have many applications. A common problem with these methods is the high computational complexity, which hinders their applications in real world high dimensional datasets, e.g gene expression datasets. In this paper, we present an R package, ParallelPC, that includes the parallelised versions of these causal exploration algorithms. The parallelised algorithms help speed up the procedure of experimenting big datasets and reduce the memory used when running the algorithms. The package is not only suitable for super-computers or clusters, but also convenient for researchers using personal computers with multi core CPUs. Our experiment results on real world datasets show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a single multicore computer. ParallelPC is available in CRAN repository at ",
        "submission_date": "2015-10-11T00:00:00",
        "last_modified_date": "2015-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03179",
        "title": "Data structuring for the ontological modelling of wind energy systems",
        "authors": [
            "Adrian Groza"
        ],
        "abstract": "Small wind projects encounter difficulties to be efficiently deployed, partly because wrong way data and information are managed. Ontologies can overcome the drawbacks of partially available, noisy, inconsistent, and heterogeneous data sources, by providing a semantic middleware between low level data and more general knowledge. In this paper, we engineer an ontology for the wind energy domain using description logic as technical instrumentation. We aim to integrate corpus of heterogeneous knowledge, both digital and human, in order to help the interested user to speed-up the initialization of a small-scale wind project. We exemplify one use case scenario of our ontology, that consists of automatically checking whether a planned wind project is compliant or not with the active regulations.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03317",
        "title": "The Inductive Constraint Programming Loop",
        "authors": [
            "Christian Bessiere",
            "Luc De Raedt",
            "Tias Guns",
            "Lars Kotthoff",
            "Mirco Nanni",
            "Siegfried Nijssen",
            "Barry O'Sullivan",
            "Anastasia Paparrizou",
            "Dino Pedreschi",
            "Helmut Simonis"
        ],
        "abstract": "Constraint programming is used for a variety of real-world optimisation problems, such as planning, scheduling and resource allocation problems. At the same time, one continuously gathers vast amounts of data about these problems. Current constraint programming software does not exploit such data to update schedules, resources and plans. We propose a new framework, that we call the Inductive Constraint Programming loop. In this approach data is gathered and analyzed systematically, in order to dynamically revise and adapt constraints and optimization criteria. Inductive Constraint Programming aims at bridging the gap between the areas of data mining and machine learning on the one hand, and constraint programming on the other hand.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03336",
        "title": "Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly Benchmark",
        "authors": [
            "Alexander Lavin",
            "Subutai Ahmad"
        ],
        "abstract": "Much of the world's data is streaming, time-series data, where anomalies give significant information in critical situations; examples abound in domains such as finance, IT, security, medical, and energy. Yet detecting anomalies in streaming data is a difficult task, requiring detectors to process data in real-time, not batches, and learn while simultaneously making predictions. There are no benchmarks to adequately test and score the efficacy of real-time anomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), which attempts to provide a controlled and repeatable environment of open-source tools to test and measure anomaly detection algorithms on streaming data. The perfect detector would detect all anomalies as soon as possible, trigger no false alarms, work with real-world time-series data across a variety of domains, and automatically adapt to changing statistics. Rewarding these characteristics is formalized in NAB, using a scoring algorithm designed for streaming data. NAB evaluates detectors on a benchmark dataset with labeled, real-world time-series data. We present these components, and give results and analyses for several open source, commercially-used algorithms. The goal for NAB is to provide a standard, open source framework with which the research community can compare and evaluate different algorithms for detecting anomalies in streaming data.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03592",
        "title": "UAVs using Bayesian Optimization to Locate WiFi Devices",
        "authors": [
            "Mattia Carpin",
            "Stefano Rosati",
            "Mohammad Emtiyaz Khan",
            "Bixio Rimoldi"
        ],
        "abstract": "We address the problem of localizing non-collaborative WiFi devices in a large region. Our main motive is to localize humans by localizing their WiFi devices, e.g. during search-and-rescue operations after a natural disaster. We use an active sensing approach that relies on Unmanned Aerial Vehicles (UAVs) to collect signal-strength measurements at informative locations. The problem is challenging since the measurement is received at arbitrary times and they are received only when the UAV is in close proximity to the device. For these reasons, it is extremely important to make prudent decision with very few measurements. We use the Bayesian optimization approach based on Gaussian process (GP) regression. This approach works well for our application since GPs give reliable predictions with very few measurements while Bayesian optimization makes a judicious trade-off between exploration and exploitation. In field experiments conducted over a region of 1000 $\\times$ 1000 $m^2$, we show that our approach reduces the search area to less than 100 meters around the WiFi device within 5 minutes only. Overall, our approach localizes the device in less than 15 minutes with an error of less than 20 meters.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03931",
        "title": "Structured Memory for Neural Turing Machines",
        "authors": [
            "Wei Zhang",
            "Yang Yu",
            "Bowen Zhou"
        ],
        "abstract": "Neural Turing Machines (NTM) contain memory component that simulates \"working memory\" in the brain to store and retrieve information to ease simple algorithms learning. So far, only linearly organized memory is proposed, and during experiments, we observed that the model does not always converge, and overfits easily when handling certain tasks. We think memory component is key to some faulty behaviors of NTM, and better organization of memory component could help fight those problems. In this paper, we propose several different structures of memory for NTM, and we proved in experiments that two of our proposed structured-memory NTMs could lead to better convergence, in term of speed and prediction accuracy on copy task and associative recall task as in (Graves et al. 2014).\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04183",
        "title": "Mathematical Foundations for Designing and Development of Intelligent Systems of Information Analysis",
        "authors": [
            "D.O. Terletskyi",
            "O.I. Provotar"
        ],
        "abstract": "This article is an attempt to combine different ways of working with sets of objects and their classes for designing and development of artificial intelligent systems (AIS) of analysis information, using object-oriented programming (OOP). This paper contains analysis of basic concepts of OOP and their relation with set theory and artificial intelligence (AI). Process of sets and multisets creation from different sides, in particular mathematical set theory, OOP and AI is considered. Definition of object and its properties, homogeneous and inhomogeneous classes of objects, set of objects, multiset of objects and constructive methods of their creation and classification are proposed. In addition, necessity of some extension of existing OOP tools for the purpose of practical implementation AIS of analysis information, using proposed approach, is shown.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2020-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04188",
        "title": "Universal and Determined Constructors of Multisets of Objects",
        "authors": [
            "Dmytro Terletskyi"
        ],
        "abstract": "This paper contains analysis of creation of sets and multisets as an approach for modeling of some aspects of human thinking. The creation of sets is considered within constructive object-oriented version of set theory (COOST), from different sides, in particular classical set theory, object-oriented programming (OOP) and development of intelligent information systems (IIS). The main feature of COOST in contrast to other versions of set theory is an opportunity to describe essences of objects more precisely, using their properties and methods, which can be applied to them. That is why this version of set theory is object-oriented and close to OOP. Within COOST, the author proposes universal constructor of multisets of objects that gives us a possibility to create arbitrary multisets of objects. In addition, a few determined constructors of multisets of objects, which allow creating multisets, using strictly defined schemas, also are proposed in the paper. Such constructors are very useful in cases of very big cardinalities of multisets, because they give us an opportunity to calculate a multiplicity of each object and cardinality of multiset before its creation. The proposed constructors of multisets of objects allow us to model in a sense corresponding processes of human thought, that in turn give us an opportunity to develop IIS, using these tools.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04194",
        "title": "Object-Oriented Dynamic Networks",
        "authors": [
            "Dmytro Terletskyi",
            "Alexandr Provotar"
        ],
        "abstract": "This paper contains description of such knowledge representation model as Object-Oriented Dynamic Network (OODN), which gives us an opportunity to represent knowledge, which can be modified in time, to build new relations between objects and classes of objects and to represent results of their modifications. The model is based on representation of objects via their properties and methods. It gives us a possibility to classify the objects and, in a sense, to build hierarchy of their types. Furthermore, it enables to represent relation of modification between concepts, to build new classes of objects based on existing classes and to create sets and multisets of concepts. OODN can be represented as a connected and directed graph, where nodes are concepts and edges are relations between them. Using such model of knowledge representation, we can consider modifications of knowledge and movement through the graph of network as a process of logical reasoning or finding the right solutions or creativity, etc. The proposed approach gives us an opportunity to model some aspects of human knowledge system and main mechanisms of human thought, in particular getting a new experience and knowledge.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04206",
        "title": "Exploiters-Based Knowledge Extraction in Object-Oriented Knowledge Representation",
        "authors": [
            "Dmytro Terletskyi"
        ],
        "abstract": "This paper contains the consideration of knowledge extraction mechanisms of such object-oriented knowledge representation models as frames, object-oriented programming and object-oriented dynamic networks. In addition, conception of universal exploiters within object-oriented dynamic networks is also discussed. The main result of the paper is introduction of new exploiters-based knowledge extraction approach, which provides generation of a finite set of new classes of objects, based on the basic set of classes. The methods for calculation of quantity of new classes, which can be obtained using proposed approach, and of quantity of types, which each of them describes, are proposed. Proof that basic set of classes, extended according to proposed approach, together with union exploiter create upper semilattice is given. The approach always allows generating of finitely defined set of new classes of objects for any object-oriented dynamic network. A quantity of these classes can be precisely calculated before the generation. It allows saving of only basic set of classes in the knowledge base.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04212",
        "title": "Inheritance in Object-Oriented Knowledge Representation",
        "authors": [
            "Dmytro Terletskyi"
        ],
        "abstract": "This paper contains the consideration of inheritance mechanism in such knowledge representation models as object-oriented programming, frames and object-oriented dynamic networks. In addition, inheritance within representation of vague and imprecise knowledge are also discussed. New types of inheritance, general classification of all known inheritance types and approach, which allows avoiding in many cases problems with exceptions, redundancy and ambiguity within object-oriented dynamic networks and their fuzzy extension, are introduced in the paper. The proposed approach bases on conception of homogeneous and inhomogeneous or heterogeneous class of objects, which allow building of inheritance hierarchy more flexibly and efficiently.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04420",
        "title": "Narrative Science Systems: A Review",
        "authors": [
            "Paramjot Kaur Sarao",
            "Puneet Mittal",
            "Rupinder Kaur"
        ],
        "abstract": "Automatic narration of events and entities is the need of the hour, especially when live reporting is critical and volume of information to be narrated is huge. This paper discusses the challenges in this context, along with the algorithms used to build such systems. From a systematic study, we can infer that most of the work done in this area is related to statistical data. It was also found that subjective evaluation or contribution of experts is also limited for narration context.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04817",
        "title": "Improving the Competency of First-Order Ontologies",
        "authors": [
            "Javier \u00c1lvez",
            "Paqui Lucio",
            "German Rigau"
        ],
        "abstract": "We introduce a new framework to evaluate and improve first-order (FO) ontologies using automated theorem provers (ATPs) on the basis of competency questions (CQs). Our framework includes both the adaptation of a methodology for evaluating ontologies to the framework of first-order logic and a new set of non-trivial CQs designed to evaluate FO versions of SUMO, which significantly extends the very small set of CQs proposed in the literature. Most of these new CQs have been automatically generated from a small set of patterns and the mapping of WordNet to SUMO. Applying our framework, we demonstrate that Adimen-SUMO v2.2 outperforms TPTP-SUMO. In addition, using the feedback provided by ATPs we have set an improved version of Adimen-SUMO (v2.4). This new version outperforms the previous ones in terms of competency. For instance, \"Humans can reason\" is automatically inferred from Adimen-SUMO v2.4, while it is neither deducible from TPTP-SUMO nor Adimen-SUMO v2.2.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04826",
        "title": "Evaluating the Competency of a First-Order Ontology",
        "authors": [
            "Javier \u00c1lvez",
            "Paqui Lucio",
            "German Rigau"
        ],
        "abstract": "We report on the results of evaluating the competency of a first-order ontology for its use with automated theorem provers (ATPs). The evaluation follows the adaptation of the methodology based on competency questions (CQs) [Gr\u00fcninger&Fox,1995] to the framework of first-order logic, which is presented in [\u00c1lvez&Lucio&Rigau,2015], and is applied to Adimen-SUMO [\u00c1lvez&Lucio&Rigau,2015]. The set of CQs used for this evaluation has been automatically generated from a small set of semantic patterns and the mapping of WordNet to SUMO. Analysing the results, we can conclude that it is feasible to use ATPs for working with Adimen-SUMO v2.4, enabling the resolution of goals by means of performing non-trivial inferences.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04914",
        "title": "Hybridization of Interval CP and Evolutionary Algorithms for Optimizing Difficult Problems",
        "authors": [
            "Charlie Vanaret",
            "Jean-Baptiste Gotteland",
            "Nicolas Durand",
            "Jean-Marc Alliot"
        ],
        "abstract": "The only rigorous approaches for achieving a numerical proof of optimality in global optimization are interval-based methods that interleave branching of the search-space and pruning of the subdomains that cannot contain an optimal solution. State-of-the-art solvers generally integrate local optimization algorithms to compute a good upper bound of the global minimum over each subspace. In this document, we propose a cooperative framework in which interval methods cooperate with evolutionary algorithms. The latter are stochastic algorithms in which a population of candidate solutions iteratively evolves in the search-space to reach satisfactory solutions.\n",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04931",
        "title": "Bad Universal Priors and Notions of Optimality",
        "authors": [
            "Jan Leike",
            "Marcus Hutter"
        ],
        "abstract": "A big open question of algorithmic information theory is the choice of the universal Turing machine (UTM). For Kolmogorov complexity and Solomonoff induction we have invariance theorems: the choice of the UTM changes bounds only by a constant. For the universally intelligent agent AIXI (Hutter, 2005) no invariance theorem is known. Our results are entirely negative: we discuss cases in which unlucky or adversarial choices of the UTM cause AIXI to misbehave drastically. We show that Legg-Hutter intelligence and thus balanced Pareto optimality is entirely subjective, and that every policy is Pareto optimal in the class of all computable environments. This undermines all existing optimality properties for AIXI. While it may still serve as a gold standard for AI, our results imply that AIXI is a relative theory, dependent on the choice of the UTM.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04935",
        "title": "Holographic Embeddings of Knowledge Graphs",
        "authors": [
            "Maximilian Nickel",
            "Lorenzo Rosasco",
            "Tomaso Poggio"
        ],
        "abstract": "Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HolE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HolE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-of-the-art methods for link prediction in knowledge graphs and relational learning benchmark datasets.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05189",
        "title": "Causal Falling Rule Lists",
        "authors": [
            "Fulton Wang",
            "Cynthia Rudin"
        ],
        "abstract": "A causal falling rule list (CFRL) is a sequence of if-then rules that specifies heterogeneous treatment effects, where (i) the order of rules determines the treatment effect subgroup a subject belongs to, and (ii) the treatment effect decreases monotonically down the list. A given CFRL parameterizes a hierarchical bayesian regression model in which the treatment effects are incorporated as parameters, and assumed constant within model-specific subgroups. We formulate the search for the CFRL best supported by the data as a Bayesian model selection problem, where we perform a search over the space of CFRL models, and approximate the evidence for a given CFRL model using standard variational techniques. We apply CFRL to a census wage dataset to identify subgroups of differing wage inequalities between men and women.\n    ",
        "submission_date": "2015-10-18T00:00:00",
        "last_modified_date": "2017-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05373",
        "title": "System Descriptions of the First International Competition on Computational Models of Argumentation (ICCMA'15)",
        "authors": [
            "Matthias Thimm",
            "Serena Villata"
        ],
        "abstract": "This volume contains the system description of the 18 solvers submitted to the First International Competition on Computational Models of Argumentation (ICCMA'15) and therefore gives an overview on state-of-the-art of computational approaches to abstract argumentation problems. Further information on the results of the competition and the performance of the individual solvers can be found on at ",
        "submission_date": "2015-10-19T00:00:00",
        "last_modified_date": "2015-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05572",
        "title": "On the Computability of AIXI",
        "authors": [
            "Jan Leike",
            "Marcus Hutter"
        ],
        "abstract": "How could we solve the machine learning and the artificial intelligence problem if we had infinite computation? Solomonoff induction and the reinforcement learning agent AIXI are proposed answers to this question. Both are known to be incomputable. In this paper, we quantify this using the arithmetical hierarchy, and prove upper and corresponding lower bounds for incomputability. We show that AIXI is not limit computable, thus it cannot be approximated using finite computation. Our main result is a limit-computable {\\epsilon}-optimal version of AIXI with infinite horizon that maximizes expected rewards.\n    ",
        "submission_date": "2015-10-19T00:00:00",
        "last_modified_date": "2015-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05963",
        "title": "Semantic, Cognitive, and Perceptual Computing: Advances toward Computing for Human Experience",
        "authors": [
            "Amit Sheth",
            "Pramod Anantharam",
            "Cory Henson"
        ],
        "abstract": "The World Wide Web continues to evolve and serve as the infrastructure for carrying massive amounts of multimodal and multisensory observations. These observations capture various situations pertinent to people's needs and interests along with all their idiosyncrasies. To support human-centered computing that empower people in making better and timely decisions, we look towards computation that is inspired by human perception and cognition. Toward this goal, we discuss computing paradigms of semantic computing, cognitive computing, and an emerging aspect of computing, which we call perceptual computing. In our view, these offer a continuum to make the most out of vast, growing, and diverse data pertinent to human needs and interests. We propose details of perceptual computing characterized by interpretation and exploration operations comparable to the interleaving of bottom and top brain processing.\n",
        "submission_date": "2015-10-20T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06153",
        "title": "Creating Scalable and Interactive Web Applications Using High Performance Latent Variable Models",
        "authors": [
            "Aaron Q Li",
            "Yuntian Deng",
            "Kublai Jing",
            "Joseph W Robinson"
        ],
        "abstract": "In this project we outline a modularized, scalable system for comparing Amazon products in an interactive and informative way using efficient latent variable models and dynamic visualization. We demonstrate how our system can build on the structure and rich review information of Amazon products in order to provide a fast, multifaceted, and intuitive comparison. By providing a condensed per-topic comparison visualization to the user, we are able to display aggregate information from the entire set of reviews while providing an interface that is at least as compact as the \"most helpful reviews\" currently displayed by Amazon, yet far more informative.\n    ",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2015-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06335",
        "title": "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems",
        "authors": [
            "Matteo Venanzi",
            "John Guiver",
            "Pushmeet Kohli",
            "Nick Jennings"
        ],
        "abstract": "Crowdsourcing systems commonly face the problem of aggregating multiple judgments provided by potentially unreliable workers. In addition, several aspects of the design of efficient crowdsourcing processes, such as defining worker's bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. Bringing this together, in this work we introduce a new time--sensitive Bayesian aggregation method that simultaneously estimates a task's duration and obtains reliable aggregations of crowdsourced judgments. Our method, called BCCTime, builds on the key insight that the time taken by a worker to perform a task is an important indicator of the likely quality of the produced judgment. To capture this, BCCTime uses latent variables to represent the uncertainty about the workers' completion time, the tasks' duration and the workers' accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. In contrast, workers with a lower propensity to valid labeling, such as spammers, bots or lazy labelers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message-passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labeling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real-world public datasets for entity linking tasks, we show that BCCTime produces up to 11% more accurate classifications and up to 100% more informative estimates of a task's duration compared to state-of-the-art methods.\n    ",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07217",
        "title": "An Efficient Implementation for WalkSAT",
        "authors": [
            "Sixue Liu"
        ],
        "abstract": "Stochastic local search (SLS) algorithms have exhibited great effectiveness in finding models of random instances of the Boolean satisfiability problem (SAT). As one of the most widely known and used SLS algorithm, WalkSAT plays a key role in the evolutions of SLS for SAT, and also hold state-of-the-art performance on random instances. This work proposes a novel implementation for WalkSAT which decreases the redundant calculations leading to a dramatically speeding up, thus dominates the latest version of WalkSAT including its advanced variants.\n    ",
        "submission_date": "2015-10-25T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07889",
        "title": "Learning Constructive Primitives for Online Level Generation and Real-time Content Adaptation in Super Mario Bros",
        "authors": [
            "Peizhi Shi",
            "Ke Chen"
        ],
        "abstract": "Procedural content generation (PCG) is of great interest to game design and development as it generates game content automatically. Motivated by the recent learning-based PCG framework and other existing PCG works, we propose an alternative approach to online content generation and adaptation in Super Mario Bros (SMB). Unlike most of existing works in SMB, our approach exploits the synergy between rule-based and learning-based methods to produce constructive primitives, quality yet controllable game segments in SMB. As a result, a complete quality game level can be generated online by integrating relevant constructive primitives via controllable parameters regarding geometrical features and procedure-level properties. Also the adaptive content can be generated in real time by dynamically selecting proper constructive primitives via an adaptation criterion, e.g., dynamic difficulty adjustment (DDA). Our approach is of several favorable properties in terms of content quality assurance, generation efficiency and controllability. Extensive simulation results demonstrate that the proposed approach can generate controllable yet quality game levels online and adaptable content for DDA in real time.\n    ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08266",
        "title": "Computing the Ramsey Number R(4,3,3) using Abstraction and Symmetry breaking",
        "authors": [
            "Michael Codish",
            "Michael Frank",
            "Avraham Itzhakov",
            "Alice Miller"
        ],
        "abstract": "The number $R(4,3,3)$ is often presented as the unknown Ramsey number with the best chances of being found \"soon\". Yet, its precise value has remained unknown for almost 50 years. This paper presents a methodology based on \\emph{abstraction} and \\emph{symmetry breaking} that applies to solve hard graph edge-coloring problems. The utility of this methodology is demonstrated by using it to compute the value $R(4,3,3)=30$. Along the way it is required to first compute the previously unknown set ${\\cal R}(3,3,3;13)$ consisting of 78{,}892 Ramsey colorings.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2015-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08525",
        "title": "Automatic Synthesis of Geometry Problems for an Intelligent Tutoring System",
        "authors": [
            "Chris Alvin",
            "Sumit Gulwani",
            "Rupak Majumdar",
            "Supratik Mukhopadhyay"
        ],
        "abstract": "This paper presents an intelligent tutoring system, GeoTutor, for Euclidean Geometry that is automatically able to synthesize proof problems and their respective solutions given a geometric figure together with a set of properties true of it. GeoTutor can provide personalized practice problems that address student deficiencies in the subject matter.\n    ",
        "submission_date": "2015-10-29T00:00:00",
        "last_modified_date": "2015-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.09033",
        "title": "Turing's Red Flag",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "Sometime in the future we will have to deal with the impact of AI's being mistaken for humans. For this reason, I propose that any autonomous system should be designed so that it is unlikely to be mistaken for anything besides an autonomous sysem, and should identify itself at the start of any interaction with another agent.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00041",
        "title": "Learning Causal Graphs with Small Interventions",
        "authors": [
            "Karthikeyan Shanmugam",
            "Murat Kocaoglu",
            "Alexandros G. Dimakis",
            "Sriram Vishwanath"
        ],
        "abstract": "We consider the problem of learning causal networks with interventions, when each intervention is limited in size under Pearl's Structural Equation Model with independent errors (SEM-IE). The objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph. Previous work has focused on the use of separating systems for complete graphs for this task. We prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case. In addition, we present a novel separating system construction, whose size is close to optimal and is arguably simpler than previous work in combinatorics. We also develop a novel information theoretic lower bound on the number of interventions that applies in full generality, including for randomized adaptive learning algorithms.\n",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00043",
        "title": "Learning Adversary Behavior in Security Games: A PAC Model Perspective",
        "authors": [
            "Arunesh Sinha",
            "Debarun Kar",
            "Milind Tambe"
        ],
        "abstract": "Recent applications of Stackelberg Security Games (SSG), from wildlife crime to urban crime, have employed machine learning tools to learn and predict adversary behavior using available data about defender-adversary interactions. Given these recent developments, this paper commits to an approach of directly learning the response function of the adversary. Using the PAC model, this paper lays a firm theoretical foundation for learning in SSGs (e.g., theoretically answer questions about the numbers of samples required to learn adversary behavior) and provides utility guarantees when the learned adversary model is used to plan the defender's strategy. The paper also aims to answer practical questions such as how much more data is needed to improve an adversary model's accuracy. Additionally, we explain a recently observed phenomenon that prediction accuracy of learned adversary behavior is not enough to discover the utility maximizing defender strategy. We provide four main contributions: (1) a PAC model of learning adversary response functions in SSGs; (2) PAC-model analysis of the learning of key, existing bounded rationality models in SSGs; (3) an entirely new approach to adversary modeling based on a non-parametric class of response functions with PAC-model analysis and (4) identification of conditions under which computing the best defender strategy against the learned adversary behavior is indeed the optimal strategy. Finally, we conduct experiments with real-world data from a national park in Uganda, showing the benefit of our new adversary modeling approach and verification of our PAC model predictions.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00787",
        "title": "A Pareto Optimal D* Search Algorithm for Multiobjective Path Planning",
        "authors": [
            "Alexander Lavin"
        ],
        "abstract": "Path planning is one of the most vital elements of mobile robotics, providing the agent with a collision-free route through the workspace. The global path plan can be calculated with a variety of informed search algorithms, most notably the A* search method, guaranteed to deliver a complete and optimal solution that minimizes the path cost. D* is widely used for its dynamic replanning capabilities. Path planning optimization typically looks to minimize the distance traversed from start to goal, but many mobile robot applications call for additional path planning objectives, presenting a multiobjective optimization (MOO) problem. Common search algorithms, e.g. A* and D*, are not well suited for MOO problems, yielding suboptimal results. The search algorithm presented in this paper is designed for optimal MOO path planning. The algorithm incorporates Pareto optimality into D*, and is thus named D*-PO. Non-dominated solution paths are guaranteed by calculating the Pareto front at each search step. Simulations were run to model a planetary exploration rover in a Mars environment, with five path costs. The results show the new, Pareto optimal D*-PO outperforms the traditional A* and D* algorithms for MOO path planning.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00840",
        "title": "Finetuning Randomized Heuristic Search For 2D Path Planning: Finding The Best Input Parameters For R* Algorithm Through Series Of Experiments",
        "authors": [
            "Konstantin Yakovlev",
            "Egor Baskin",
            "Ivan Hramoin"
        ],
        "abstract": "Path planning is typically considered in Artificial Intelligence as a graph searching problem and R* is state-of-the-art algorithm tailored to solve it. The algorithm decomposes given path finding task into the series of subtasks each of which can be easily (in computational sense) solved by well-known methods (such as A*). Parameterized random choice is used to perform the decomposition and as a result R* performance largely depends on the choice of its input parameters. In our work we formulate a range of assumptions concerning possible upper and lower bounds of R* parameters, their interdependency and their influence on R* performance. Then we evaluate these assumptions by running a large number of experiments. As a result we formulate a set of heuristic rules which can be used to initialize the values of R* parameters in a way that leads to algorithm's best performance.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01640",
        "title": "Computing sets of graded attribute implications with witnessed non-redundancy",
        "authors": [
            "Vilem Vychodil"
        ],
        "abstract": "In this paper we extend our previous results on sets of graded attribute implications with witnessed non-redundancy. We assume finite residuated lattices as structures of truth degrees and use arbitrary idempotent truth-stressing linguistic hedges as parameters which influence the semantics of graded attribute implications. In this setting, we introduce algorithm which transforms any set of graded attribute implications into an equivalent non-redundant set of graded attribute implications with saturated consequents whose non-redundancy is witnessed by antecedents of the formulas. As a consequence, we solve the open problem regarding the existence of general systems of pseudo-intents which appear in formal concept analysis of object-attribute data with graded attributes and linguistic hedges. Furthermore, we show a polynomial-time procedure for determining bases given by general systems of pseudo-intents from sets of graded attribute implications which are complete in data.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01710",
        "title": "Adaptive information-theoretic bounded rational decision-making with parametric priors",
        "authors": [
            "Jordi Grau-Moya",
            "Daniel A. Braun"
        ],
        "abstract": "Deviations from rational decision-making due to limited computational resources have been studied in the field of bounded rationality, originally proposed by Herbert Simon. There have been a number of different approaches to model bounded rationality ranging from optimality principles to heuristics. Here we take an information-theoretic approach to bounded rationality, where information-processing costs are measured by the relative entropy between a posterior decision strategy and a given fixed prior strategy. In the case of multiple environments, it can be shown that there is an optimal prior rendering the bounded rationality problem equivalent to the rate distortion problem for lossy compression in information theory. Accordingly, the optimal prior and posterior strategies can be computed by the well-known Blahut-Arimoto algorithm which requires the computation of partition sums over all possible outcomes and cannot be applied straightforwardly to continuous problems. Here we derive a sampling-based alternative update rule for the adaptation of prior behaviors of decision-makers and we show convergence to the optimal prior predicted by rate distortion theory. Importantly, the update rule avoids typical infeasible operations such as the computation of partition sums. We show in simulations a proof of concept for discrete action and environment domains. This approach is not only interesting as a generic computational method, but might also provide a more realistic model of human decision-making processes occurring on a fast and a slow time scale.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01960",
        "title": "An Action Language for Multi-Agent Domains: Foundations",
        "authors": [
            "Chitta Baral",
            "Gregory Gelfond",
            "Enrico Pontelli",
            "Tran Cao Son"
        ],
        "abstract": "In multi-agent domains (MADs), an agent's action may not just change the world and the agent's knowledge and beliefs about the world, but also may change other agents' knowledge and beliefs about the world and their knowledge and beliefs about other agents' knowledge and beliefs about the world. The goals of an agent in a multi-agent world may involve manipulating the knowledge and beliefs of other agents' and again, not just their knowledge/belief about the world, but also their knowledge about other agents' knowledge about the world. Our goal is to present an action language (mA+) that has the necessary features to address the above aspects in representing and RAC in MADs. mA+ allows the representation of and reasoning about different types of actions that an agent can perform in a domain where many other agents might be present -- such as world-altering actions, sensing actions, and announcement/communication actions. It also allows the specification of agents' dynamic awareness of action occurrences which has future implications on what agents' know about the world and other agents' knowledge about the world. mA+ considers three different types of awareness: full-, partial- awareness, and complete oblivion of an action occurrence and its effects. This keeps the language simple, yet powerful enough to address a large variety of knowledge manipulation scenarios in MADs. The semantics of mA+ relies on the notion of state, which is described by a pointed Kripke model and is used to encode the agent's knowledge and the real state of the world. It is defined by a transition function that maps pairs of actions and states into sets of states. We illustrate properties of the action theories, including properties that guarantee finiteness of the set of initial states and their practical implementability. Finally, we relate mA+ to other related formalisms that contribute to RAC in MADs.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2020-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02210",
        "title": "Learning Optimized Or's of And's",
        "authors": [
            "Tong Wang",
            "Cynthia Rudin"
        ],
        "abstract": "Or's of And's (OA) models are comprised of a small number of disjunctions of conjunctions, also called disjunctive normal form. An example of an OA model is as follows: If ($x_1 = $ `blue' AND $x_2=$ `middle') OR ($x_1 = $ `yellow'), then predict $Y=1$, else predict $Y=0$. Or's of And's models have the advantage of being interpretable to human experts, since they are a set of conditions that concisely capture the characteristics of a specific subset of data. We present two optimization-based machine learning frameworks for constructing OA models, Optimized OA (OOA) and its faster version, Optimized OA with Approximations (OOAx). We prove theoretical bounds on the properties of patterns in an OA model. We build OA models as a diagnostic screening tool for obstructive sleep apnea, that achieves high accuracy with a substantial gain in interpretability over other methods.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02420",
        "title": "Design of an Alarm System for Isfahan Ozone Level based on Artificial Intelligence Predictor Models",
        "authors": [
            "Ehsan Lotfi"
        ],
        "abstract": "The ozone level prediction is an important task of air quality agencies of modern cities. In this paper, we design an ozone level alarm system (OLP) for Isfahan city and test it through the real word data from 1-1-2000 to 7-6-2011. We propose a computer based system with three inputs and single output. The inputs include three sensors of solar ultraviolet (UV), total solar radiation (TSR) and total ozone (O3). And the output of the system is the predicted O3 of the next day and the alarm massages. A developed artificial intelligence (AI) algorithm is applied to determine the output, based on the inputs variables. For this issue, AI models, including supervised brain emotional learning (BEL), adaptive neuro-fuzzy inference system (ANFIS) and artificial neural networks (ANNs), are compared in order to find the best model. The simulation of the proposed system shows that it can be used successfully in prediction of major cities ozone level.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02426",
        "title": "A Winner-Take-All Approach to Emotional Neural Networks with Universal Approximation Property",
        "authors": [
            "E. Lotfi"
        ],
        "abstract": "Here, we propose a brain-inspired winner-take-all emotional neural network (WTAENN) and prove the universal approximation property for the novel architecture. WTAENN is a single layered feedforward neural network that benefits from the excitatory, inhibitory, and expandatory neural connections as well as the winner-take-all (WTA) competitions in the human brain s nervous system. The WTA competition increases the information capacity of the model without adding hidden neurons. The universal approximation capability of the proposed architecture is illustrated on two example functions, trained by a genetic algorithm, and then applied to several competing recent and benchmark problems such as in curve fitting, pattern recognition, classification and prediction. In particular, it is tested on twelve UCI classification datasets, a facial recognition problem, three real world prediction problems (2 chaotic time series of geomagnetic activity indices and wind farm power generation data), two synthetic case studies with constant and nonconstant noise variance as well as k-selector and linear programming problems. Results indicate the general applicability and often superiority of the approach in terms of higher accuracy and lower model complexity, especially where low computational complexity is imperative.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02432",
        "title": "A Study of an Modeling Method of T-S fuzzy System Based on Moving Fuzzy Reasoning and Its Application",
        "authors": [
            "Son-Il Kwak",
            "Gang Choe",
            "In-Song Kim",
            "Gyong-Ho Jo",
            "Chol-Jun Hwang"
        ],
        "abstract": "To improve the effectiveness of the fuzzy identification, a structure identification method based on moving rate is proposed for T-S fuzzy model. The proposed method is called \"T-S modeling (or T-S fuzzy identification method) based on moving rate\". First, to improve the shortcomings of existing fuzzy reasoning methods based on matching degree, the moving rates for s-type, z-type and trapezoidal membership functions of T-S fuzzy model were defined. Then, the differences between proposed moving rate and existing matching degree were explained. Next, the identification method based on moving rate is proposed for T-S model. Finally, the proposed identification method is applied to the fuzzy modeling for the precipitation forecast and security situation prediction. Test results show that the proposed method significantly improves the effectiveness of fuzzy identification.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02455",
        "title": "(Yet) Another Theoretical Model of Thinking",
        "authors": [
            "Patrick Virie"
        ],
        "abstract": "This paper presents a theoretical, idealized model of the thinking process with the following characteristics: 1) the model can produce complex thought sequences and can be generalized to new inputs, 2) it can receive and maintain input information indefinitely for the generation of thoughts and later use, and 3) it supports learning while executing. The crux of the model lies within the concept of internal consistency, or the generated thoughts should always be consistent with the inputs from which they are created. Its merit, apart from the capability to generate new creative thoughts from an internal mechanism, depends on the potential to help training to generalize better. This is consequently enabled by separating input information into several parts to be handled by different processing components with a focus mechanism to fetch information for each. This modularized view with the focus binds the model with the computationally capable Turing machines. And as a final remark, this paper constructively shows that the computational complexity of the model is at least, if not surpass, that of a universal Turing machine.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02669",
        "title": "Enacting textual entailment and ontologies for automated essay grading in chemical domain",
        "authors": [
            "Adrian Groza",
            "Roxana Szabo"
        ],
        "abstract": "We propose a system for automated essay grading using ontologies and textual entailment. The process of textual entailment is guided by hypotheses, which are extracted from a domain ontology. Textual entailment checks if the truth of the hypothesis follows from a given text. We enact textual entailment to compare students answer to a model answer obtained from ontology. We validated the solution against various essays written by students in the chemistry domain.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02889",
        "title": "A disembodied developmental robotic agent called Samu B\u00e1tfai",
        "authors": [
            "Norbert B\u00e1tfai"
        ],
        "abstract": "The agent program, called Samu, is an experiment to build a disembodied DevRob (Developmental Robotics) chatter bot that can talk in a natural language like humans do. One of the main design feature is that Samu can be interacted with using only a character terminal. This is important not only for practical aspects of Turing test or Loebner prize, but also for the study of basic principles of Developmental Robotics. Our purpose is to create a rapid prototype of Q-learning with neural network approximators for Samu. We sketch out the early stages of the development process of this prototype, where Samu's task is to predict the next sentence of tales or conversations. The basic objective of this paper is to reach the same results using reinforcement learning with general function approximators that can be achieved by using the classical Q lookup table on small input samples. The paper is closed by an experiment that shows a significant improvement in Samu's learning when using LZW tree to narrow the number of possible Q-actions.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03246",
        "title": "Taxonomy of Pathways to Dangerous AI",
        "authors": [
            "Roman V. Yampolskiy"
        ],
        "abstract": "In order to properly handle a dangerous Artificially Intelligent (AI) system it is important to understand how the system came to be in such a state. In popular culture (science fiction movies/books) AIs/Robots became self-aware and as a result rebel against humanity and decide to destroy it. While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI. In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI. To the best of our knowledge, this is the first attempt to systematically classify types of pathways leading to malevolent AI. Previous relevant work either surveyed specific goals/meta-rules which might lead to malevolent behavior in AIs (\u00d6zkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit at different stages of its development (Alexey Turchin, July 10 2015, July 10, 2015).\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03532",
        "title": "IBMMS Decision Support Tool For Management of Bank Telemarketing Campaigns",
        "authors": [
            "Ali Keles",
            "Ayturk Keles"
        ],
        "abstract": "Although direct marketing is a good method for banks to utilize in the face of global competition and the financial crisis, it has been shown to exhibit poor performance. However, there are some drawbacks to direct campaigns, such as those related to improving the negative attributes that customers ascribe to banks. To overcome these problems, attractive long-term deposit campaigns should be organized and managed more effectively. The aim of this study is to develop an Intelligent Bank Market Management System (IBMMS) for bank managers who want to manage efficient marketing campaigns. IBMMS is the first system developed by combining the power of data mining with the capabilities of expert systems in this area. Moreover, IBMMS includes important features that enable it to be intelligent: a knowledge base, an inference engine and an advisor. Using this system, a manager can successfully direct marketing campaigns and follow the decision schemas of customers both as individuals and as a group; moreover, a manager can make decisions that lead to the desired response by customers.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03897",
        "title": "IfcWoD, Semantically Adapting IFC Model Relations into OWL Properties",
        "authors": [
            "Tarcisio Mendes de Farias",
            "Ana Roxin",
            "Christophe Nicolle"
        ],
        "abstract": "In the context of Building Information Modelling, ontologies have been identified as interesting in achieving information interoperability. Regarding the construction and facility management domains, several IFC (Industry Foundation Classes) based ontologies have been developed, such as IfcOWL. In the context of ontology modelling, the constraint of optimizing the size of IFC STEP-based files can be leveraged. In this paper, we propose an adaptation of the IFC model into OWL which leverages from all modelling constraints required by the object-oriented structure of IFC schema. Therefore, we do not only present a syntactic but also a semantic adaptation of the IFC model. Our model takes into consideration the meaning of entities, relationships, properties and attributes defined by the IFC standard. Our approach presents several advantages compared to other initiatives such as the optimization of query execution time. Every advantage is defended by means of practical examples and benchmarks.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03958",
        "title": "Software Agents with Concerns of their Own",
        "authors": [
            "Luis Botelho",
            "Luis Nunes",
            "Ricardo Ribeiro",
            "Rui J. Lopes"
        ],
        "abstract": "We claim that it is possible to have artificial software agents for which their actions and the world they inhabit have first-person or intrinsic meanings. The first-person or intrinsic meaning of an entity to a system is defined as its relation with the system's goals and capabilities, given the properties of the environment in which it operates. Therefore, for a system to develop first-person meanings, it must see itself as a goal-directed actor, facing limitations and opportunities dictated by its own capabilities, and by the properties of the environment. The first part of the paper discusses this claim in the context of arguments against and proposals addressing the development of computer programs with first-person meanings. A set of definitions is also presented, most importantly the concepts of cold and phenomenal first-person meanings. The second part of the paper presents preliminary proposals and achievements, resulting of actual software implementations, within a research approach that aims to develop software agents that intrinsically understand their actions and what happens to them. As a result, an agent with no a priori notion of its goals and capabilities, and of the properties of its environment acquires all these notions by observing itself in action. The cold first-person meanings of the agent's actions and of what happens to it are defined using these acquired notions. Although not solving the full problem of first-person meanings, the proposed approach and preliminary results allow us some confidence to address the problems yet to be considered, in particular the phenomenal aspect of first-person meanings.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2019-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04143",
        "title": "Deep Reinforcement Learning in Parameterized Action Space",
        "authors": [
            "Matthew Hausknecht",
            "Peter Stone"
        ],
        "abstract": "Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2024-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04326",
        "title": "ICON Challenge on Algorithm Selection",
        "authors": [
            "Lars Kotthoff"
        ],
        "abstract": "We present the results of the ICON Challenge on Algorithm Selection.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04352",
        "title": "Introduzione all'Intelligenza Artificiale",
        "authors": [
            "Fabrizio Riguzzi"
        ],
        "abstract": "The paper presents an introduction to Artificial Intelligence (AI) in an accessible and informal but precise form. The paper focuses on the algorithmic aspects of the discipline, presenting the main techniques used in AI systems groped in symbolic and subsymbolic. The last part of the paper is devoted to the discussion ongoing among experts in the field and the public at large about on the advantages and disadvantages of AI and in particular on the possible dangers. The personal opinion of the author on this subject concludes the paper. -- --\n",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2021-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04636",
        "title": "Deep Reinforcement Learning with a Natural Language Action Space",
        "authors": [
            "Ji He",
            "Jianshu Chen",
            "Xiaodong He",
            "Jianfeng Gao",
            "Lihong Li",
            "Li Deng",
            "Mari Ostendorf"
        ],
        "abstract": "This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05506",
        "title": "Neurocontrol methods review",
        "authors": [
            "Artem Chernodub",
            "Dmitry Dziuba"
        ],
        "abstract": "Methods of applying neural networks to control plants are considered. Methods and schemes are described, their advantages and disadvantages are discussed.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05662",
        "title": "Discovering Underlying Plans Based on Distributed Representations of Actions",
        "authors": [
            "Xin Tian",
            "Hankz Hankui Zhuo",
            "Subbarao Kambhampati"
        ],
        "abstract": "Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either discover plans by maximally \"matching\" observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming complete domain models are available. In real world applications, however, target plans are often not from plan libraries and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Our approach is capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05719",
        "title": "Using Abduction in Markov Logic Networks for Root Cause Analysis",
        "authors": [
            "Joerg Schoenfisch",
            "Janno von St\u007fulpnagel",
            "Jens Ortmann",
            "Christian Meilicke",
            "Heiner Stuckenschmidt"
        ],
        "abstract": "IT infrastructure is a crucial part in most of today's business operations. High availability and reliability, and short response times to outages are essential. Thus a high amount of tool support and automation in risk management is desirable to decrease outages. We propose a new approach for calculating the root cause for an observed failure in an IT infrastructure. Our approach is based on Abduction in Markov Logic Networks. Abduction aims to find an explanation for a given observation in the light of some background knowledge. In failure diagnosis, the explanation corresponds to the root cause, the observation to the failure of a component, and the background knowledge to the dependency graph extended by potential risks. We apply a method to extend a Markov Logic Network in order to conduct abductive reasoning, which is not naturally supported in this formalism. Our approach exhibits a high amount of reusability and enables users without specific knowledge of a concrete infrastructure to gain viable insights in the case of an incident. We implemented the method in a tool and illustrate its suitability for root cause analysis by applying it to a sample scenario.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05749",
        "title": "Solution Repair/Recovery in Uncertain Optimization Environment",
        "authors": [
            "Oumaima Khaled"
        ],
        "abstract": "Operation management problems (such as Production Planning and Scheduling) are represented and formulated as optimization models. The resolution of such optimization models leads to solutions which have to be operated in an organization. However, the conditions under which the optimal solution is obtained rarely correspond exactly to the conditions under which the solution will be operated in the ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06030",
        "title": "BIRDNEST: Bayesian Inference for Ratings-Fraud Detection",
        "authors": [
            "Bryan Hooi",
            "Neil Shah",
            "Alex Beutel",
            "Stephan Gunnemann",
            "Leman Akoglu",
            "Mohit Kumar",
            "Disha Makhija",
            "Christos Faloutsos"
        ],
        "abstract": "Review fraud is a pervasive problem in online commerce, in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services. Fake reviews are often detected based on several signs, including 1) they occur in short bursts of time; 2) fraudulent user accounts have skewed rating distributions. However, these may both be true in any given dataset. Hence, in this paper, we propose an approach for detecting fraudulent reviews which combines these 2 approaches in a principled manner, allowing successful detection even when one of these signs is not present. To combine these 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD) model, a flexible Bayesian model of user rating behavior. Based on our model we formulate a likelihood-based suspiciousness metric, Normalized Expected Surprise Total (NEST). We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric. Experiments on real data show that BIRDNEST successfully spots review fraud in large, real-world graphs: the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06191",
        "title": "Abstract Attribute Exploration with Partial Object Descriptions",
        "authors": [
            "Daniel Borchmann",
            "Bernhard Ganter"
        ],
        "abstract": "Attribute exploration has been investigated in several studies, with particular emphasis on the algorithmic aspects of this knowledge acquisition method. In its basic version the method itself is rather simple and transparent. But when background knowledge and partially described counter-examples are admitted, it gets more difficult. Here we discuss this case in an abstract, somewhat \"axiomatic\" setting, providing a terminology that clarifies the abstract strategy of the method rather than its algorithmic implementation.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07211",
        "title": "Noisy Submodular Maximization via Adaptive Sampling with Applications to Crowdsourced Image Collection Summarization",
        "authors": [
            "Adish Singla",
            "Sebastian Tschiatschek",
            "Andreas Krause"
        ],
        "abstract": "We address the problem of maximizing an unknown submodular function that can only be accessed via noisy evaluations. Our work is motivated by the task of summarizing content, e.g., image collections, by leveraging users' feedback in form of clicks or ratings. For summarization tasks with the goal of maximizing coverage and diversity, submodular set functions are a natural choice. When the underlying submodular function is unknown, users' feedback can provide noisy evaluations of the function that we seek to maximize. We provide a generic algorithm -- \\submM{} -- for maximizing an unknown submodular function under cardinality constraints. This algorithm makes use of a novel exploration module -- \\blbox{} -- that proposes good elements based on adaptively sampling noisy function evaluations. \\blbox{} is able to accommodate different kinds of observation models such as value queries and pairwise comparisons. We provide PAC-style guarantees on the quality and sampling cost of the solution obtained by \\submM{}. We demonstrate the effectiveness of our approach in an interactive, crowdsourced image collection summarization application.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07275",
        "title": "Learning Simple Algorithms from Examples",
        "authors": [
            "Wojciech Zaremba",
            "Tomas Mikolov",
            "Armand Joulin",
            "Rob Fergus"
        ],
        "abstract": "We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using $Q$-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by $Q$-learning.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07373",
        "title": "What is the plausibility of probability?(revised 2003, 2015)",
        "authors": [
            "Stefan Arnborg",
            "Gunnar Sj\u00f6din"
        ],
        "abstract": "We present and examine a result related to uncertainty reasoning, namely that a certain plausibility space of Cox's type can be uniquely embedded in a minimal ordered field. This, although a purely mathematical result, can be claimed to imply that every rational method to reason with uncertainty must be based on sets of extended probability distributions, where extended probability is standard probability extended with infinitesimals.\n",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07663",
        "title": "Approximate Probabilistic Inference via Word-Level Counting",
        "authors": [
            "Supratik Chakraborty",
            "Kuldeep S. Meel",
            "Rakesh Mistry",
            "Moshe Y. Vardi"
        ],
        "abstract": "Hashing-based model counting has emerged as a promising approach for large-scale probabilistic inference on graphical models. A key component of these techniques is the use of xor-based 2-universal hash functions that operate over Boolean domains. Many counting problems arising in probabilistic inference are, however, naturally encoded over finite discrete domains. Techniques based on bit-level (or Boolean) hash functions require these problems to be propositionalized, making it impossible to leverage the remarkable progress made in SMT (Satisfiability Modulo Theory) solvers that can reason directly over words (or bit-vectors). In this work, we present the first approximate model counter that uses word-level hashing functions, and can directly leverage the power of sophisticated SMT solvers. Empirical evaluation over an extensive suite of benchmarks demonstrates the promise of the approach.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2016-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07972",
        "title": "Learning with Memory Embeddings",
        "authors": [
            "Volker Tresp",
            "Crist\u00f3bal Esteban",
            "Yinchong Yang",
            "Stephan Baier",
            "Denis Krompa\u00df"
        ],
        "abstract": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider time evolutions, time patterns and subsymbolic representations. In this paper we map embedding models, which were developed purely as solutions to technical problems for modelling temporal knowledge graphs, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory, sensory memory, short-term memory, and working memory. We discuss learning, query answering, the path from sensory input to semantic decoding, and the relationship between episodic memory and semantic memory. We introduce a number of hypotheses on human memory that can be derived from the developed mathematical models.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08099",
        "title": "Strategic Dialogue Management via Deep Reinforcement Learning",
        "authors": [
            "Heriberto Cuay\u00e1huitl",
            "Simon Keizer",
            "Oliver Lemon"
        ],
        "abstract": "Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning (DRL) for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan---where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53% win rate versus 3 automated players (`bots'), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08130",
        "title": "A Roadmap towards Machine Intelligence",
        "authors": [
            "Tomas Mikolov",
            "Armand Joulin",
            "Marco Baroni"
        ],
        "abstract": "The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08158",
        "title": "Plan Explicability and Predictability for Robot Task Planning",
        "authors": [
            "Yu Zhang",
            "Sarath Sreedharan",
            "Anagha Kulkarni",
            "Tathagata Chakraborti",
            "Hankz Hankui Zhuo",
            "Subbarao Kambhampati"
        ],
        "abstract": "Intelligent robots and machines are becoming pervasive in human populated environments. A desirable capability of these agents is to respond to goal-oriented commands by autonomously constructing task plans. However, such autonomy can add significant cognitive load and potentially introduce safety risks to humans when agents behave unexpectedly. Hence, for such agents to be helpful, one important requirement is for them to synthesize plans that can be easily understood by humans. While there exists previous work that studied socially acceptable robots that interact with humans in \"natural ways\", and work that investigated legible motion planning, there lacks a general solution for high level task planning. To address this issue, we introduce the notions of plan {\\it explicability} and {\\it predictability}. To compute these measures, first, we postulate that humans understand agent plans by associating abstract tasks with agent actions, which can be considered as a labeling process. We learn the labeling scheme of humans for agent plans from training examples using conditional random fields (CRFs). Then, we use the learned model to label a new plan to compute its explicability and predictability. These measures can be used by agents to proactively choose or directly synthesize plans that are more explicable and predictable to humans. We provide evaluations on a synthetic domain and with human subjects using physical robots to show the effectiveness of our approach\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08205",
        "title": "Breaking Symmetries in Graph Search with Canonizing Sets",
        "authors": [
            "Avraham Itzhakov",
            "Michael Codish"
        ],
        "abstract": "There are many complex combinatorial problems which involve searching for an undirected graph satisfying given constraints. Such problems are often highly challenging because of the large number of isomorphic representations of their solutions. This paper introduces effective and compact, complete symmetry breaking constraints for small graph search. Enumerating with these symmetry breaks generates all and only non-isomorphic solutions. For small search problems, with up to $10$ vertices, we compute instance independent symmetry breaking constraints. For small search problems with a larger number of vertices we demonstrate the computation of instance dependent constraints which are complete. We illustrate the application of complete symmetry breaking constraints to extend two known sequences from the OEIS related to graph enumeration. We also demonstrate the application of a generalization of our approach to fully-interchangeable matrix search problems.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08277",
        "title": "A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations",
        "authors": [
            "Shengxian Wan",
            "Yanyan Lan",
            "Jiafeng Guo",
            "Jun Xu",
            "Liang Pang",
            "Xueqi Cheng"
        ],
        "abstract": "Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through $k$-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08280",
        "title": "Welfare of Sequential Allocation Mechanisms for Indivisible Goods",
        "authors": [
            "Haris Aziz",
            "Thomas Kalinowski",
            "Toby Walsh",
            "Lirong Xia"
        ],
        "abstract": "Sequential allocation is a simple and attractive mechanism for the allocation of indivisible goods. Agents take turns, according to a policy, to pick items. Sequential allocation is guaranteed to return an allocation which is efficient but may not have an optimal social welfare. We consider therefore the relation between welfare and efficiency. We study the (computational) questions of what welfare is possible or necessary depending on the choice of policy. We also consider a novel control problem in which the chair chooses a policy to improve social welfare.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08350",
        "title": "A global Constraint for mining Sequential Patterns with GAP constraint",
        "authors": [
            "Amina Kemmar",
            "Samir Loudni",
            "Yahia Lebbah",
            "Patrice Boizumault",
            "Thierry Charnois"
        ],
        "abstract": "Sequential pattern mining (SPM) under gap constraint is a challenging task. Many efficient specialized methods have been developed but they are all suffering from a lack of genericity. The Constraint Programming (CP) approaches are not so effective because of the size of their encodings. In[7], we have proposed the global constraint Prefix-Projection for SPM which remedies to this drawback. However, this global constraint cannot be directly extended to support gap constraint. In this paper, we propose the global constraint GAP-SEQ enabling to handle SPM with or without gap constraint. GAP-SEQ relies on the principle of right pattern extensions. Experiments show that our approach clearly outperforms both CP approaches and the state-of-the-art cSpade method on large datasets.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08412",
        "title": "Beyond OWL 2 QL in OBDA: Rewritings and Approximations (Extended Version)",
        "authors": [
            "Elena Botoeva",
            "Diego Calvanese",
            "Valerio Santarelli",
            "Domenico Fabio Savo",
            "Alessandro Solimando",
            "Guohui Xiao"
        ],
        "abstract": "Ontology-based data access (OBDA) is a novel paradigm facilitating access to relational data, realized by linking data sources to an ontology by means of declarative mappings. DL-Lite_R, which is the logic underpinning the W3C ontology language OWL 2 QL and the current language of choice for OBDA, has been designed with the goal of delegating query answering to the underlying database engine, and thus is restricted in expressive power. E.g., it does not allow one to express disjunctive information, and any form of recursion on the data. The aim of this paper is to overcome these limitations of DL-Lite_R, and extend OBDA to more expressive ontology languages, while still leveraging the underlying relational technology for query answering. We achieve this by relying on two well-known mechanisms, namely conservative rewriting and approximation, but significantly extend their practical impact by bringing into the picture the mapping, an essential component of OBDA. Specifically, we develop techniques to rewrite OBDA specifications with an expressive ontology to \"equivalent\" ones with a DL-Lite_R ontology, if possible, and to approximate them otherwise. We do so by exploiting the high expressive power of the mapping layer to capture part of the domain semantics of rich ontology languages. We have implemented our techniques in the prototype system OntoProx, making use of the state-of-the-art OBDA system Ontop and the query answering system Clipper, and we have shown their feasibility and effectiveness with experiments on synthetic and real-world data.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08456",
        "title": "A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small Strategies in POMDPs",
        "authors": [
            "Krishnendu Chatterjee",
            "Martin Chmelik",
            "Jessica Davies"
        ],
        "abstract": "POMDPs are standard models for probabilistic planning problems, where an agent interacts with an uncertain environment. We study the problem of almost-sure reachability, where given a set of target states, the question is to decide whether there is a policy to ensure that the target set is reached with probability 1 (almost-surely). While in general the problem is EXPTIME-complete, in many practical cases policies with a small amount of memory suffice. Moreover, the existing solution to the problem is explicit, which first requires to construct explicitly an exponential reduction to a belief-support MDP. In this work, we first study the existence of observation-stationary strategies, which is NP-complete, and then small-memory strategies. We present a symbolic algorithm by an efficient encoding to SAT and using a SAT solver for the problem. We report experimental results demonstrating the scalability of our symbolic (SAT-based) approach.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08488",
        "title": "Bayesian Network Models for Adaptive Testing",
        "authors": [
            "Martin Plajner",
            "Ji\u0159\u00ed Vomlel"
        ],
        "abstract": "Computerized adaptive testing (CAT) is an interesting and promising approach to testing human abilities. In our research we use Bayesian networks to create a model of tested humans. We collected data from paper tests performed with grammar school students. In this article we first provide the summary of data used for our experiments. We propose several different Bayesian networks, which we tested and compared by cross-validation. Interesting results were obtained and are discussed in the paper. The analysis has brought a clearer view on the model selection problem. Future research is outlined in the concluding part of the paper.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08512",
        "title": "Some Epistemological Problems with the Knowledge Level in Cognitive Architectures",
        "authors": [
            "Antonio Lieto"
        ],
        "abstract": "This article addresses an open problem in the area of cognitive systems and architectures: namely the problem of handling (in terms of processing and reasoning capabilities) complex knowledge structures that can be at least plausibly comparable, both in terms of size and of typology of the encoded information, to the knowledge that humans process daily for executing everyday activities. Handling a huge amount of knowledge, and selectively retrieve it ac- cording to the needs emerging in different situational scenarios, is an important aspect of human intelligence. For this task, in fact, humans adopt a wide range of heuristics (Gigerenzer and Todd) due to their bounded rationality (Simon, 1957). In this perspective, one of the re- quirements that should be considered for the design, the realization and the evaluation of intelligent cognitively inspired systems should be represented by their ability of heuristically identify and retrieve, from the general knowledge stored in their artificial Long Term Memory (LTM), that one which is synthetically and contextually relevant. This require- ment, however, is often neglected. Currently, artificial cognitive systems and architectures are not able, de facto, to deal with complex knowledge structures that can be even slightly comparable to the knowledge heuris- tically managed by humans. In this paper I will argue that this is not only a technological problem but also an epistemological one and I will briefly sketch a proposal for a possible solution.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08574",
        "title": "A Stochastic Process Model of Classical Search",
        "authors": [
            "Dimitri Klimenko",
            "Hanna Kurniawati",
            "Marcus Gallagher"
        ],
        "abstract": "Among classical search algorithms with the same heuristic information, with sufficient memory A* is essentially as fast as possible in finding a proven optimal solution. However, in many situations optimal solutions are simply infeasible, and thus search algorithms that trade solution quality for speed are desirable. In this paper, we formalize the process of classical search as a metalevel decision problem, the Abstract Search MDP. For any given optimization criterion, this establishes a well-defined notion of the best possible behaviour for a search algorithm and offers a theoretical approach to the design of algorithms for that criterion. We proceed to approximately solve a version of the Abstract Search MDP for anytime algorithms and thus derive a novel search algorithm, Search by Maximizing the Incremental Rate of Improvement (SMIRI). SMIRI is shown to outperform current state-of-the-art anytime search algorithms on a parametrized stochastic tree model for most of the tested parameter values.\n    ",
        "submission_date": "2015-11-27T00:00:00",
        "last_modified_date": "2015-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08589",
        "title": "Shaping Proto-Value Functions via Rewards",
        "authors": [
            "Chandrashekar Lakshmi Narayanan",
            "Raj Kumar Maity",
            "Shalabh Bhatnagar"
        ],
        "abstract": "In this paper, we combine task-dependent reward shaping and task-independent proto-value functions to obtain reward dependent proto-value functions (RPVFs). In constructing the RPVFs we are making use of the immediate rewards which are available during the sampling phase but are not used in the PVF construction. We show via experiments that learning with an RPVF based representation is better than learning with just reward shaping or PVFs. In particular, when the state space is symmetrical and the rewards are asymmetrical, the RPVF capture the asymmetry better than the PVFs.\n    ",
        "submission_date": "2015-11-27T00:00:00",
        "last_modified_date": "2015-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08779",
        "title": "Multiagent Cooperation and Competition with Deep Reinforcement Learning",
        "authors": [
            "Ardi Tampuu",
            "Tambet Matiisen",
            "Dorian Kodelja",
            "Ilya Kuzovkin",
            "Kristjan Korjus",
            "Juhan Aru",
            "Jaan Aru",
            "Raul Vicente"
        ],
        "abstract": "Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efficiently. Agents trained under collaborative rewarding schemes find an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.\n    ",
        "submission_date": "2015-11-27T00:00:00",
        "last_modified_date": "2015-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08855",
        "title": "Semantic Folding Theory And its Application in Semantic Fingerprinting",
        "authors": [
            "Francisco De Sousa Webber"
        ],
        "abstract": "Human language is recognized as a very complex domain since decades. No computer system has been able to reach human levels of performance so far. The only known computational system capable of proper language processing is the human brain. While we gather more and more data about the brain, its fundamental computational processes still remain obscure. The lack of a sound computational brain theory also prevents the fundamental understanding of Natural Language Processing. As always when science lacks a theoretical foundation, statistical modeling is applied to accommodate as many sampled real-world data as possible. An unsolved fundamental issue is the actual representation of language (data) within the brain, denoted as the Representational Problem. Starting with Jeff Hawkins' Hierarchical Temporal Memory (HTM) theory, a consistent computational theory of the human cortex, we have developed a corresponding theory of language data representation: The Semantic Folding Theory. The process of encoding words, by using a topographic semantic space as distributional reference frame into a sparse binary representational vector is called Semantic Folding and is the central topic of this document. Semantic Folding describes a method of converting language from its symbolic representation (text) into an explicit, semantically grounded representation that can be generically processed by Hawkins' HTM networks. As it turned out, this change in representation, by itself, can solve many complex NLP problems by applying Boolean operators and a generic similarity function like the Euclidian Distance. Many practical problems of statistical NLP systems, like the high cost of computation, the fundamental incongruity of precision and recall , the complex tuning procedures etc., can be elegantly overcome by applying Semantic Folding.\n    ",
        "submission_date": "2015-11-28T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08967",
        "title": "Robotic Search & Rescue via Online Multi-task Reinforcement Learning",
        "authors": [
            "Lisa Lee"
        ],
        "abstract": "Reinforcement learning (RL) is a general and well-known method that a robot can use to learn an optimal control policy to solve a particular task. We would like to build a versatile robot that can learn multiple tasks, but using RL for each of them would be prohibitively expensive in terms of both time and wear-and-tear on the robot. To remedy this problem, we use the Policy Gradient Efficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL algorithm that enables the robot to efficiently learn multiple consecutive tasks by sharing knowledge between these tasks to accelerate learning and improve performance. We implemented and evaluated three RL methods--Q-learning, policy gradient RL, and PG-ELLA--on a ground robot whose task is to find a target object in an environment under different surface conditions. In this paper, we discuss our implementations as well as present an empirical analysis of their learning performance.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09047",
        "title": "Solving Transition-Independent Multi-agent MDPs with Sparse Interactions (Extended version)",
        "authors": [
            "Joris Scharpff",
            "Diederik M. Roijers",
            "Frans A. Oliehoek",
            "Matthijs T.J. Spaan",
            "Mathijs M. de Weerdt"
        ],
        "abstract": "In cooperative multi-agent sequential decision making under uncertainty, agents must coordinate to find an optimal joint policy that maximises joint value. Typical algorithms exploit additive structure in the value function, but in the fully-observable multi-agent MDP setting (MMDP) such structure is not present. We propose a new optimal solver for transition-independent MMDPs, in which agents can only affect their own state but their reward depends on joint transitions. We represent these dependencies compactly in conditional return graphs (CRGs). Using CRGs the value of a joint policy and the bounds on partially specified joint policies can be efficiently computed. We propose CoRe, a novel branch-and-bound policy search algorithm building on CRGs. CoRe typically requires less runtime than the available alternatives and finds solutions to problems previously unsolvable.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09080",
        "title": "Exploiting Anonymity in Approximate Linear Programming: Scaling to Large Multiagent MDPs (Extended Version)",
        "authors": [
            "Philipp Robbel",
            "Frans A. Oliehoek",
            "Mykel J. Kochenderfer"
        ],
        "abstract": "Many exact and approximate solution methods for Markov Decision Processes (MDPs) attempt to exploit structure in the problem and are based on factorization of the value function. Especially multiagent settings, however, are known to suffer from an exponential increase in value component sizes as interactions become denser, meaning that approximation architectures are restricted in the problem sizes and types they can handle. We present an approach to mitigate this limitation for certain types of multiagent systems, exploiting a property that can be thought of as \"anonymous influence\" in the factored MDP. Anonymous influence summarizes joint variable effects efficiently whenever the explicit representation of variable identity in the problem can be avoided. We show how representational benefits from anonymity translate into computational efficiencies, both for general variable elimination in a factor graph but in particular also for the approximate linear programming solution to factored MDPs. The latter allows to scale linear programming to factored MDPs that were previously unsolvable. Our results are shown for the control of a stochastic disease process over a densely connected graph with 50 nodes and 25 agents.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2016-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09147",
        "title": "Scaling POMDPs For Selecting Sellers in E-markets-Extended Version",
        "authors": [
            "Athirai A. Irissappane",
            "Frans A. Oliehoek",
            "Jie Zhang"
        ],
        "abstract": "In multiagent e-marketplaces, buying agents need to select good sellers by querying other buyers (called advisors). Partially Observable Markov Decision Processes (POMDPs) have shown to be an effective framework for optimally selecting sellers by selectively querying advisors. However, current solution methods do not scale to hundreds or even tens of agents operating in the e-market. In this paper, we propose the Mixture of POMDP Experts (MOPE) technique, which exploits the inherent structure of trust-based domains, such as the seller selection problem in e-markets, by aggregating the solutions of smaller sub-POMDPs. We propose a number of variants of the MOPE approach that we analyze theoretically and empirically. Experiments show that MOPE can scale up to a hundred agents thereby leveraging the presence of more advisors to significantly improve buyer satisfaction.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09249",
        "title": "On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "abstract": "This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially \"learning to think.\" The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as \"mirror neurons.\" Experimental results will be described in separate papers.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09300",
        "title": "Influence diagrams for the optimization of a vehicle speed profile",
        "authors": [
            "V\u00e1clav Kratochv\u00edl",
            "Ji\u0159\u00ed Vomlel"
        ],
        "abstract": "Influence diagrams are decision theoretic extensions of Bayesian networks. They are applied to diverse decision problems. In this paper we apply influence diagrams to the optimization of a vehicle speed profile. We present results of computational experiments in which an influence diagram was used to optimize the speed profile of a Formula 1 race car at the Silverstone F1 circuit. The computed lap time and speed profiles correspond well to those achieved by test pilots. An extended version of our model that considers a more complex optimization function and diverse traffic constraints is currently being tested onboard a testing car by a major car manufacturer. This paper opens doors for new applications of influence diagrams.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09460",
        "title": "Ask, and shall you receive?: Understanding Desire Fulfillment in Natural Language Text",
        "authors": [
            "Snigdha Chaturvedi",
            "Dan Goldwasser",
            "Hal Daume III"
        ],
        "abstract": "The ability to comprehend wishes or desires and their fulfillment is important to Natural Language Understanding. This paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled. We propose various unstructured and structured models that capture fulfillment cues such as the subject's emotional state and actions. Our experiments with two different datasets demonstrate the importance of understanding the narrative and discourse structure to address this task.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00047",
        "title": "Symbolic Neutrosophic Theory",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "Symbolic (or Literal) Neutrosophic Theory is referring to the use of abstract symbols (i.e. the letters T, I, F, or their refined indexed letters Tj, Ik, Fl) in neutrosophics. We extend the dialectical triad thesis-antithesis-synthesis to the neutrosophic tetrad thesis-antithesis-neutrothesis-neutrosynthesis. The we introduce the neutrosophic system that is a quasi or (t,i,f) classical system, in the sense that the neutrosophic system deals with quasi-terms (concepts, attributes, etc.). Then the notions of Neutrosophic Axiom, Neutrosophic Deducibility, Degree of Contradiction (Dissimilarity) of Two Neutrosophic Axioms, etc. Afterwards a new type of structures, called (t, i, f) Neutrosophic Structures, and we show particular cases of such structures in geometry and in algebra. Also, a short history of the neutrosophic set, neutrosophic numerical components and neutrosophic literal components, neutrosophic numbers, etc. We construct examples of splitting the literal indeterminacy (I) into literal subindeterminacies (I1, I2, and so on, Ir), and to define a multiplication law of these literal subindeterminacies in order to be able to build refined I neutrosophic algebraic structures. We define three neutrosophic actions and their properties. We then introduce the prevalence order on T,I,F with respect to a given neutrosophic operator. And the refinement of neutrosophic entities A, neutA, and antiA. Then we extend the classical logical operators to neutrosophic literal (symbolic) logical operators and to refined literal (symbolic) logical operators, and we define the refinement neutrosophic literal (symbolic) space. We introduce the neutrosophic quadruple numbers (a+bT+cI+dF) and the refined neutrosophic quadruple numbers. Then we define an absorbance law, based on a prevalence order, in order to multiply the neutrosophic quadruple numbers.\n    ",
        "submission_date": "2015-10-18T00:00:00",
        "last_modified_date": "2015-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00250",
        "title": "Evaluating Morphological Computation in Muscle and DC-motor Driven Models of Human Hopping",
        "authors": [
            "Keyan Ghazi-Zahedi",
            "Daniel F.B. Haeufle",
            "Guido Montufar",
            "Syn Schmitt",
            "Nihat Ay"
        ],
        "abstract": "In the context of embodied artificial intelligence, morphological computation refers to processes which are conducted by the body (and environment) that otherwise would have to be performed by the brain. Exploiting environmental and morphological properties is an important feature of embodied systems. The main reason is that it allows to significantly reduce the controller complexity. An important aspect of morphological computation is that it cannot be assigned to an embodied system per se, but that it is, as we show, behavior- and state-dependent. In this work, we evaluate two different measures of morphological computation that can be applied in robotic systems and in computer simulations of biological movement. As an example, these measures were evaluated on muscle and DC-motor driven hopping models. We show that a state-dependent analysis of the hopping behaviors provides additional insights that cannot be gained from the averaged measures alone. This work includes algorithms and computer code for the measures.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00355",
        "title": "Taxonomy grounded aggregation of classifiers with different label sets",
        "authors": [
            "Amrita Saha",
            "Sathish Indurthi",
            "Shantanu Godbole",
            "Subendhu Rongali",
            "Vikas C. Raykar"
        ],
        "abstract": "We describe the problem of aggregating the label predictions of diverse classifiers using a class taxonomy. Such a taxonomy may not have been available or referenced when the individual classifiers were designed and trained, yet mapping the output labels into the taxonomy is desirable to integrate the effort spent in training the constituent classifiers. A hierarchical taxonomy representing some domain knowledge may be different from, but partially mappable to, the label sets of the individual classifiers. We present a heuristic approach and a principled graphical model to aggregate the label predictions by grounding them into the available taxonomy. Our model aggregates the labels using the taxonomy structure as constraints to find the most likely hierarchically consistent class. We experimentally validate our proposed method on image and text classification tasks.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00573",
        "title": "Object-based World Modeling in Semi-Static Environments with Dependent Dirichlet-Process Mixtures",
        "authors": [
            "Lawson L.S. Wong",
            "Thanard Kurutach",
            "Leslie Pack Kaelbling",
            "Tom\u00e1s Lozano-P\u00e9rez"
        ],
        "abstract": "To accomplish tasks in human-centric indoor environments, robots need to represent and understand the world in terms of objects and their attributes. We refer to this attribute-based representation as a world model, and consider how to acquire it via noisy perception and maintain it over time, as objects are added, changed, and removed in the world. Previous work has framed this as multiple-target tracking problem, where objects are potentially in motion at all times. Although this approach is general, it is computationally expensive. We argue that such generality is not needed in typical world modeling tasks, where objects only change state occasionally. More efficient approaches are enabled by restricting ourselves to such semi-static environments.\n",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00964",
        "title": "Modeling Human Understanding of Complex Intentional Action with a Bayesian Nonparametric Subgoal Model",
        "authors": [
            "Ryo Nakahashi",
            "Chris L. Baker",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "Most human behaviors consist of multiple parts, steps, or subtasks. These structures guide our action planning and execution, but when we observe others, the latent structure of their actions is typically unobservable, and must be inferred in order to learn new skills by demonstration, or to assist others in completing their tasks. For example, an assistant who has learned the subgoal structure of a colleague's task can more rapidly recognize and support their actions as they unfold. Here we model how humans infer subgoals from observations of complex action sequences using a nonparametric Bayesian model, which assumes that observed actions are generated by approximately rational planning over unknown subgoal sequences. We test this model with a behavioral experiment in which humans observed different series of goal-directed actions, and inferred both the number and composition of the subgoal sequences associated with each goal. The Bayesian model predicts human subgoal inferences with high accuracy, and significantly better than several alternative models and straightforward heuristics. Motivated by this result, we simulate how learning and inference of subgoals can improve performance in an artificial user assistance task. The Bayesian model learns the correct subgoals from fewer observations, and better assists users by more rapidly and accurately inferring the goal of their actions than alternative approaches.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00965",
        "title": "Neural Enquirer: Learning to Query Tables with Natural Language",
        "authors": [
            "Pengcheng Yin",
            "Zhengdong Lu",
            "Hang Li",
            "Ben Kao"
        ],
        "abstract": "We proposed Neural Enquirer as a neural network architecture to execute a natural language (NL) query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parsers, Neural Enquirer is fully \"neuralized\": it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated NL queries on tables with rich structures.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2016-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00977",
        "title": "A Study on Artificial Intelligence IQ and Standard Intelligent Model",
        "authors": [
            "Feng Liu",
            "Yong Shi"
        ],
        "abstract": "Currently, potential threats of artificial intelligence (AI) to human have triggered a large controversy in society, behind which, the nature of the issue is whether the artificial intelligence (AI) system can be evaluated quantitatively. This article analyzes and evaluates the challenges that the AI development level is facing, and proposes that the evaluation methods for the human intelligence test and the AI system are not uniform; and the key reason for which is that none of the models can uniformly describe the AI system and the beings like human. Aiming at this problem, a standard intelligent system model is established in this study to describe the AI system and the beings like human uniformly. Based on the model, the article makes an abstract mathematical description, and builds the standard intelligent machine mathematical model; expands the Von Neumann architecture and proposes the Liufeng - Shiyong architecture; gives the definition of the artificial intelligence IQ, and establishes the artificial intelligence scale and the evaluation method; conduct the test on 50 search engines and three human subjects at different ages across the world, and finally obtains the ranking of the absolute IQ and deviation IQ ranking for artificial intelligence IQ 2014.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01124",
        "title": "Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions",
        "authors": [
            "Peter Sunehag",
            "Richard Evans",
            "Gabriel Dulac-Arnold",
            "Yori Zwols",
            "Daniel Visentin",
            "Ben Coppin"
        ],
        "abstract": "Many real-world problems come with action spaces represented as feature vectors. Although high-dimensional control is a largely unsolved problem, there has recently been progress for modest dimensionalities. Here we report on a successful attempt at addressing problems of dimensionality as high as $2000$, of a particular form. Motivated by important applications such as recommendation systems that do not fit the standard reinforcement learning frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A Slate-MDP is an MDP with a combinatorial action space consisting of slates (tuples) of primitive actions of which one is executed in an underlying MDP. The agent does not control the choice of this executed action and the action might not even be from the slate, e.g., for recommendation systems for which all recommendations can be ignored. We use deep Q-learning based on feature representations of both the state and action to learn the value of whole slates. Unlike existing methods, we optimize for both the combinatorial and sequential aspects of our tasks. The new agent's superiority over agents that either ignore the combinatorial or sequential long-term value aspect is demonstrated on a range of environments with dynamics from a real-world recommendation system. Further, we use deep deterministic policy gradients to learn a policy that for each position of the slate, guides attention towards the part of the action space in which the value is the highest and we only evaluate actions in this area. The attention is used within a sequentially greedy procedure leveraging submodularity. Finally, we show how introducing risk-seeking can dramatically improve the agents performance and ability to discover more far reaching strategies.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01272",
        "title": "CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data",
        "authors": [
            "Vikash Mansinghka",
            "Patrick Shafto",
            "Eric Jonas",
            "Cap Petschulat",
            "Max Gasner",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "There is a widespread need for statistical methods that can analyze high-dimensional datasets with- out imposing restrictive or opaque modeling assumptions. This paper describes a domain-general data analysis method called CrossCat. CrossCat infers multiple non-overlapping views of the data, each consisting of a subset of the variables, and uses a separate nonparametric mixture to model each view. CrossCat is based on approximately Bayesian inference in a hierarchical, nonparamet- ric model for data tables. This model consists of a Dirichlet process mixture over the columns of a data table in which each mixture component is itself an independent Dirichlet process mixture over the rows; the inner mixture components are simple parametric models whose form depends on the types of data in the table. CrossCat combines strengths of mixture modeling and Bayesian net- work structure learning. Like mixture modeling, CrossCat can model a broad class of distributions by positing latent variables, and produces representations that can be efficiently conditioned and sampled from for prediction. Like Bayesian networks, CrossCat represents the dependencies and independencies between variables, and thus remains accurate when there are multiple statistical signals. Inference is done via a scalable Gibbs sampling scheme; this paper shows that it works well in practice. This paper also includes empirical results on heterogeneous tabular data of up to 10 million cells, such as hospital cost and quality measures, voting records, unemployment rates, gene expression measurements, and images of handwritten digits. CrossCat infers structure that is consistent with accepted findings and common-sense knowledge in multiple domains and yields predictive accuracy competitive with generative, discriminative, and model-free alternatives.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01370",
        "title": "Locally Adaptive Translation for Knowledge Graph Embedding",
        "authors": [
            "Yantao Jia",
            "Yuanzhuo Wang",
            "Hailun Lin",
            "Xiaolong Jin",
            "Xueqi Cheng"
        ],
        "abstract": "Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01503",
        "title": "On the Min-cost Traveling Salesman Problem with Drone",
        "authors": [
            "Quang Minh Ha",
            "Yves Deville",
            "Quang Dung Pham",
            "Minh Ho\u00e0ng H\u00e0"
        ],
        "abstract": "Once known to be used exclusively in military domain, unmanned aerial vehicles (drones) have stepped up to become a part of new logistic method in commercial sector called \"last-mile delivery\". In this novel approach, small unmanned aerial vehicles (UAV), also known as drones, are deployed alongside with trucks to deliver goods to customers in order to improve the service quality or reduce the transportation cost. It gives rise to a new variant of the traveling salesman problem (TSP), of which we call TSP with drone (TSP-D). In this article, we consider a variant of TSP-D where the main objective is to minimize the total transportation cost. We also propose two heuristics: \"Drone First, Truck Second\" (DFTS) and \"Truck First, Drone Second\" (TFDS), to effectively solve the problem. The former constructs route for drone first while the latter constructs route for truck first. We solve a TSP to generate route for truck and propose a mixed integer programming (MIP) formulation with different profit functions to build route for drone. Numerical results obtained on many instances with different sizes and characteristics are presented. Recommendations on promising algorithm choices are also provided.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01613",
        "title": "A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony Algorithm",
        "authors": [
            "Wei-Hao Mao",
            "Fei Gao",
            "Yi-Jin Dong",
            "Wen-Ming Li"
        ],
        "abstract": "The Ramsey number is of vital importance in Ramsey's theorem. This paper proposed a novel methodology for constructing Ramsey graphs about R(3,10), which uses Artificial Bee Colony optimization(ABC) to raise the lower bound of Ramsey number R(3,10). The r(3,10)-graph contains two limitations, that is, neither complete graphs of order 3 nor independent sets of order 10. To resolve these limitations, a special mathematical model is put in the paradigm to convert the problems into discrete optimization whose smaller minimizers are correspondent to bigger lower bound as approximation of inf R(3,10). To demonstrate the potential of the proposed method, simulations are done to to minimize the amount of these two types of graphs. For the first time, four r(3,9,39) graphs with best approximation for inf R(3,10) are reported in simulations to support the current lower bound for R(3,10). The experiments' results show that the proposed paradigm for Ramsey number's calculation driven by ABC is a successful method with the advantages of high precision and robustness.\n    ",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2015-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01629",
        "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
        "authors": [
            "Yinlam Chow",
            "Mohammad Ghavamzadeh",
            "Lucas Janson",
            "Marco Pavone"
        ],
        "abstract": "In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account \\emph{risk}, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs.\n",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2017-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01885",
        "title": "Probabilistic Structural Controllability in Causal Bayesian Networks",
        "authors": [
            "Ardavan Salehi Nobandegani",
            "Ioannis N. Psaromiligkos"
        ],
        "abstract": "Humans routinely confront the following key question which could be viewed as a probabilistic variant of the controllability problem: While faced with an uncertain environment governed by causal structures, how should they practice their autonomy by intervening on driver variables, in order to increase (or decrease) the probability of attaining their desired (or undesired) state for some target variable? In this paper, for the first time, the problem of probabilistic controllability in Causal Bayesian Networks (CBNs) is studied. More specifically, the aim of this paper is two-fold: (i) to introduce and formalize the problem of probabilistic structural controllability in CBNs, and (ii) to identify a sufficient set of driver variables for the purpose of probabilistic structural controllability of a generic CBN. We also elaborate on the nature of minimality the identified set of driver variables satisfies. In this context, the term \"structural\" signifies the condition wherein solely the structure of the CBN is known.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01915",
        "title": "Knowledge Sharing in Coalitions",
        "authors": [
            "Guifei Jiang",
            "Dongmo Zhang",
            "Laurent Perrussel"
        ],
        "abstract": "The aim of this paper is to investigate the interplay between knowledge shared by a group of agents and its coalition ability. We investigate this relation in the standard context of imperfect information concurrent game. We assume that whenever a set of agents form a coalition to achieve a goal, they share their knowledge before acting. Based on this assumption, we propose a new semantics for alternating-time temporal logic with imperfect information and perfect recall. It turns out that this semantics is sufficient to preserve all the desirable properties of coalition ability in traditional coalitional logics. Meanwhile, we investigate how knowledge sharing within a group of agents contributes to its coalitional ability through the interplay of epistemic and coalition modalities. This work provides a partial answer to the question: which kind of group knowledge is required for a group to achieve their goals in the context of imperfect information.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02078",
        "title": "From rules to runs: A dynamic epistemic take on imperfect information games",
        "authors": [
            "Kai Li",
            "Yanjing Wang"
        ],
        "abstract": "In the literature of game theory, the information sets of extensive form games have different interpretations, which may lead to confusions and paradoxical cases. We argue that the problem lies in the mix-up of two interpretations of the extensive form game structures: game rules or game runs which do not always coincide. In this paper, we try to separate and connect these two views by proposing a dynamic epistemic framework in which we can compute the runs step by step from the game rules plus the given assumptions of the players. We propose a modal logic to describe players' knowledge and its change during the plays, and provide a complete axiomatization. We also show that, under certain conditions, the mix-up of the rules and the runs is not harmful due to the structural similarity of the two.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02140",
        "title": "Contamination-Free Measures and Algebraic Operations",
        "authors": [
            "A Mani"
        ],
        "abstract": "An open concept of rough evolution and an axiomatic approach to granules was also developed recently by the present author. Subsequently the concepts were used in the formal framework of rough Y-systems (RYS) for developing on granular correspondences by her. These have since been used for a new approach towards comparison of rough algebraic semantics across different semantic domains by way of correspondences that preserve rough evolution and try to avoid contamination. In this research paper, new methods are proposed and a semantics for handling possibly contaminated operations and structured bigness is developed. These would also be of natural interest for relative consistency of one collection of knowledge relative other.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02266",
        "title": "Sensitivity analysis, multilinearity and beyond",
        "authors": [
            "Manuele Leonelli",
            "Christiane G\u00f6rgen",
            "Jim Q. Smith"
        ],
        "abstract": "Sensitivity methods for the analysis of the outputs of discrete Bayesian networks have been extensively studied and implemented in different software packages. These methods usually focus on the study of sensitivity functions and on the impact of a parameter change to the Chan-Darwiche distance. Although not fully recognized, the majority of these results heavily rely on the multilinear structure of atomic probabilities in terms of the conditional probability parameters associated with this type of network. By defining a statistical model through the polynomial expression of its associated defining conditional probabilities, we develop a unifying approach to sensitivity methods applicable to a large suite of models including extensions of Bayesian networks, for instance context-specific and dynamic ones, and chain event graphs. By then focusing on models whose defining polynomial is multilinear, our algebraic approach enables us to prove that the Chan-Darwiche distance is minimized for a certain class of multi-parameter contemporaneous variations when parameters are proportionally covaried.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02406",
        "title": "Learning Discrete Bayesian Networks from Continuous Data",
        "authors": [
            "Yi-Chun Chen",
            "Tim Allan Wheeler",
            "Mykel John Kochenderfer"
        ],
        "abstract": "Learning Bayesian networks from raw data can help provide insights into the relationships between variables. While real data often contains a mixture of discrete and continuous-valued variables, many Bayesian network structure learning algorithms assume all random variables are discrete. Thus, continuous variables are often discretized when learning a Bayesian network. However, the choice of discretization policy has significant impact on the accuracy, speed, and interpretability of the resulting models. This paper introduces a principled Bayesian discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the established minimum description length algorithm. In addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn Bayesian network structures.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2018-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02752",
        "title": "A Novel Regularized Principal Graph Learning Framework on Explicit Graph Representation",
        "authors": [
            "Qi Mao",
            "Li Wang",
            "Ivor W. Tsang",
            "Yijun Sun"
        ],
        "abstract": "Many scientific datasets are of high dimension, and the analysis usually requires visual manipulation by retaining the most important structures of data. Principal curve is a widely used approach for this purpose. However, many existing methods work only for data with structures that are not self-intersected, which is quite restrictive for real applications. A few methods can overcome the above problem, but they either require complicated human-made rules for a specific task with lack of convergence guarantee and adaption flexibility to different tasks, or cannot obtain explicit structures of data. To address these issues, we develop a new regularized principal graph learning framework that captures the local information of the underlying graph structure based on reversed graph embedding. As showcases, models that can learn a spanning tree or a weighted undirected $\\ell_1$ graph are proposed, and a new learning algorithm is developed that learns a set of principal points and a graph structure from data, simultaneously. The new algorithm is simple with guaranteed convergence. We then extend the proposed framework to deal with large-scale data. Experimental results on various synthetic and six real world datasets show that the proposed method compares favorably with baselines and can uncover the underlying structure correctly.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2016-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03020",
        "title": "Learning measures of semi-additive behaviour",
        "authors": [
            "Hamidreza Chinaei",
            "Mohsen Rais-Ghasem",
            "Frank Rudzicz"
        ],
        "abstract": "In business analytics, measure values, such as sales numbers or volumes of cargo transported, are often summed along values of one or more corresponding categories, such as time or shipping container. However, not every measure should be added by default (e.g., one might more typically want a mean over the heights of a set of people); similarly, some measures should only be summed within certain constraints (e.g., population measures need not be summed over years). In systems such as Watson Analytics, the exact additive behaviour of a measure is often determined by a human expert. In this work, we propose a small set of features for this issue. We use these features in a case-based reasoning approach, where the system suggests an aggregation behaviour, with 86% accuracy in our collected dataset.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03516",
        "title": "Subsumptive reflection in SNOMED CT: a large description logic-based terminology for diagnosis",
        "authors": [
            "A.M. Mohan Rao"
        ],
        "abstract": "Description logic (DL) based biomedical terminology (SNOMED CT) is used routinely in medical practice. However, diagnostic inference using such terminology is precluded by its complexity. Here we propose a model that simplifies these inferential components. We propose three concepts that classify clinical features and examined their effect on inference using SNOMED CT. We used PAIRS (Physician Assistant Artificial Intelligence Reference System) database (1964 findings for 485 disorders, 18 397 disease feature links) for our analysis. We also use a 50-million medical word corpus for estimating the vectors of disease-feature links. Our major results are 10% of finding-disorder links are concomitant in both assertion and negation where as 90% are either concomitant in assertion or negation. Logical implications of PAIRS data on SNOMED CT include 70% of the links do not share any common system while 18% share organ and 12% share both system and organ. Applications of these principles for inference are discussed and suggestions are made for deriving a diagnostic process using SNOMED CT. Limitations of these processes and suggestions for improvements are also discussed.\n    ",
        "submission_date": "2015-12-11T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04087",
        "title": "True Online Temporal-Difference Learning",
        "authors": [
            "Harm van Seijen",
            "A. Rupam Mahmood",
            "Patrick M. Pilarski",
            "Marlos C. Machado",
            "Richard S. Sutton"
        ],
        "abstract": "The temporal-difference methods TD($\\lambda$) and Sarsa($\\lambda$) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD($\\lambda$) and true online Sarsa($\\lambda$), respectively (van Seijen & Sutton, 2014). These new versions maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD($\\lambda$)/Sarsa($\\lambda$) with regular TD($\\lambda$)/Sarsa($\\lambda$) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. Besides the empirical results, we provide an in-depth analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporal-difference methods can be derived by making changes to the online forward view and then rewriting the update equations.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2016-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04097",
        "title": "Using Linear Constraints for Logic Program Termination Analysis",
        "authors": [
            "Marco Calautti",
            "Sergio Greco",
            "Cristian Molinaro",
            "Irina Trubitsyna"
        ],
        "abstract": "It is widely acknowledged that function symbols are an important feature in answer set programming, as they make modeling easier, increase the expressive power, and allow us to deal with infinite domains. The main issue with their introduction is that the evaluation of a program might not terminate and checking whether it terminates or not is undecidable. To cope with this problem, several classes of logic programs have been proposed where the use of function symbols is restricted but the program evaluation termination is guaranteed. Despite the significant body of work in this area, current approaches do not include many simple practical programs whose evaluation terminates. In this paper, we present the novel classes of rule-bounded and cycle-bounded programs, which overcome different limitations of current approaches by performing a more global analysis of how terms are propagated from the body to the head of rules. Results on the correctness, the complexity, and the expressivity of the proposed approach are provided.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04105",
        "title": "Policy Gradient Methods for Off-policy Control",
        "authors": [
            "Lucas Lehnert",
            "Doina Precup"
        ],
        "abstract": "Off-policy learning refers to the problem of learning the value function of a way of behaving, or policy, while following a different policy. Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ, converge even when using function approximation and incremental updates. However, they have been developed for the case of a fixed behavior policy. In control problems, one would like to adapt the behavior policy over time to become more greedy with respect to the existing value function. In this paper, we present the first gradient-based learning algorithms for this problem, which rely on the framework of policy gradient in order to modify the behavior policy. We present derivations of the algorithms, a convergence theorem, and empirical evidence showing that they compare favorably to existing approaches.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04358",
        "title": "An Event Calculus Production Rule System for Reasoning in Dynamic and Uncertain Domains",
        "authors": [
            "Theodore Patkos",
            "Dimitris Plexousakis",
            "Abdelghani Chibani",
            "Yacine Amirat"
        ],
        "abstract": "Action languages have emerged as an important field of Knowledge Representation for reasoning about change and causality in dynamic domains. This article presents Cerbere, a production system designed to perform online causal, temporal and epistemic reasoning based on the Event Calculus. The framework implements the declarative semantics of the underlying logic theories in a forward-chaining rule-based reasoning system, coupling the high expressiveness of its formalisms with the efficiency of rule-based systems. To illustrate its applicability, we present both the modeling of benchmark problems in the field, as well as its utilization in the challenging domain of smart spaces. A hybrid framework that combines logic-based with probabilistic reasoning has been developed, that aims to accommodate activity recognition and monitoring tasks in smart spaces. Under consideration in Theory and Practice of Logic Programming (TPLP)\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04387",
        "title": "Data-driven Sequential Monte Carlo in Probabilistic Programming",
        "authors": [
            "Yura N Perov",
            "Tuan Anh Le",
            "Frank Wood"
        ],
        "abstract": "Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) algorithms in existing probabilistic programming systems suboptimally use only model priors as proposal distributions. In this work, we describe an approach for training a discriminative model, namely a neural network, in order to approximate the optimal proposal by using posterior estimates from previous runs of inference. We show an example that incorporates a data-driven proposal for use in a non-parametric model in the Anglican probabilistic programming system. Our results show that data-driven proposals can significantly improve inference performance so that considerably fewer particles are necessary to perform a good posterior estimation.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04467",
        "title": "A Model for Safety Case Confidence Assessment",
        "authors": [
            "J\u00e9r\u00e9mie Guiochet",
            "Quynh Anh Do Hoang",
            "Mohamed Kaaniche"
        ],
        "abstract": "Building a safety case is a common approach to make expert judgement explicit about safety of a system. The issue of confidence in such argumentation is still an open research field. Providing quantitative estimation of confidence is an interesting approach to manage complexity of arguments. This paper explores the main current approaches, and proposes a new model for quantitative confidence estimation based on Belief Theory for its definition, and on Bayesian Belief Networks for its propagation in safety case networks.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04652",
        "title": "Hyper-Heuristic Algorithm for Finding Efficient Features in Diagnose of Lung Cancer Disease",
        "authors": [
            "Mitra Montazeri",
            "Mahdieh Soleymani Baghshah",
            "Ahmad Enhesari"
        ],
        "abstract": "Background: Lung cancer was known as primary cancers and the survival rate of cancer is about 15%. Early detection of lung cancer is the leading factor in survival rate. All symptoms (features) of lung cancer do not appear until the cancer spreads to other areas. It needs an accurate early detection of lung cancer, for increasing the survival rate. For accurate detection, it need characterizes efficient features and delete redundancy features among all features. Feature selection is the problem of selecting informative features among all features. Materials and Methods: Lung cancer database consist of 32 patient records with 57 features. This database collected by Hong and Youngand indexed in the University of California Irvine repository. Experimental contents include the extracted from the clinical data and X-ray data, etc. The data described 3 types of pathological lung cancers and all features are taking an integer value 0-3. In our study, new method is proposed for identify efficient features of lung cancer. It is based on Hyper-Heuristic. Results: We obtained an accuracy of 80.63% using reduced 11 feature set. The proposed method compare to the accuracy of 5 machine learning feature selections. The accuracy of these 5 methods are 60.94, 57.81, 68.75, 60.94 and 68.75. Conclusions: The proposed method has better performance with the highest level of accuracy. Therefore, the proposed model is recommended for identifying an efficient symptom of Disease. These finding are very important in health research, particularly in allocation of medical resources for patients who predicted as high-risks\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2016-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04792",
        "title": "From One Point to A Manifold: Knowledge Graph Embedding For Precise Link Prediction",
        "authors": [
            "Han Xiao",
            "Minlie Huang",
            "Xiaoyan Zhu"
        ],
        "abstract": "Knowledge graph embedding aims at offering a numerical knowledge representation paradigm by transforming the entities and relations into continuous vector space. However, existing methods could not characterize the knowledge graph in a fine degree to make a precise prediction. There are two reasons: being an ill-posed algebraic system and applying an overstrict geometric form. As precise prediction is critical, we propose an manifold-based embedding principle (\\textbf{ManifoldE}) which could be treated as a well-posed algebraic system that expands the position of golden triples from one point in current models to a manifold in ours. Extensive experiments show that the proposed models achieve substantial improvements against the state-of-the-art baselines especially for the precise prediction task, and yet maintain high efficiency.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2017-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04860",
        "title": "Increasing the Action Gap: New Operators for Reinforcement Learning",
        "authors": [
            "Marc G. Bellemare",
            "Georg Ostrovski",
            "Arthur Guez",
            "Philip S. Thomas",
            "R\u00e9mi Munos"
        ],
        "abstract": "This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04976",
        "title": "Conditions for Normative Decision Making at the Fire Ground",
        "authors": [
            "Adam Krasuski"
        ],
        "abstract": "We discuss the changes in an attitude to decision making at the fire ground. The changes are driven by the recent technological shift. The emerging new approaches in sensing and data processing (under common umbrella of Cyber-Physical Systems) allow for leveling off the gap, between humans and machines, in perception of the fire ground. Furthermore, results from descriptive decision theory question the rationality of human choices. This creates the need for searching and testing new approaches for decision making during emergency. We propose the framework that addresses this need. The primary feature of the framework are possibilities for incorporation of normative and prescriptive approaches to decision making. The framework also allows for comparison of the performance of decisions, between human and machine.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05006",
        "title": "BayesDB: A probabilistic programming system for querying the probable implications of data",
        "authors": [
            "Vikash Mansinghka",
            "Richard Tibbetts",
            "Jay Baxter",
            "Pat Shafto",
            "Baxter Eaves"
        ],
        "abstract": "Is it possible to make statistical inference broadly accessible to non-statisticians without sacrificing mathematical rigor or inference quality? This paper describes BayesDB, a probabilistic programming platform that aims to enable users to query the probable implications of their data as directly as SQL databases enable them to query the data itself. This paper focuses on four aspects of BayesDB: (i) BQL, an SQL-like query language for Bayesian data analysis, that answers queries by averaging over an implicit space of probabilistic models; (ii) techniques for implementing BQL using a broad class of multivariate probabilistic models; (iii) a semi-parametric Bayesian model-builder that auomatically builds ensembles of factorial mixture models to serve as baselines; and (iv) MML, a \"meta-modeling\" language for imposing qualitative constraints on the model-builder and combining baseline models with custom algorithmic and statistical models that can be implemented in external software. BayesDB is illustrated using three applications: cleaning and exploring a public database of Earth satellites; assessing the evidence for temporal dependence between macroeconomic indicators; and analyzing a salary survey.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05247",
        "title": "Solving stable matching problems using answer set programming",
        "authors": [
            "Sofie De Clercq",
            "Steven Schockaert",
            "Martine De Cock",
            "Ann Now\u00e9"
        ],
        "abstract": "Since the introduction of the stable marriage problem (SMP) by Gale and Shapley (1962), several variants and extensions have been investigated. While this variety is useful to widen the application potential, each variant requires a new algorithm for finding the stable matchings. To address this issue, we propose an encoding of the SMP using answer set programming (ASP), which can straightforwardly be adapted and extended to suit the needs of specific applications. The use of ASP also means that we can take advantage of highly efficient off-the-shelf solvers. To illustrate the flexibility of our approach, we show how our ASP encoding naturally allows us to select optimal stable matchings, i.e. matchings that are optimal according to some user-specified criterion. To the best of our knowledge, our encoding offers the first exact implementation to find sex-equal, minimum regret, egalitarian or maximum cardinality stable matchings for SMP instances in which individuals may designate unacceptable partners and ties between preferences are allowed.\n",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05294",
        "title": "Feature Representation for ICU Mortality",
        "authors": [
            "Harini Suresh"
        ],
        "abstract": "Good predictors of ICU Mortality have the potential to identify high-risk patients earlier, improve ICU resource allocation, or create more accurate population-level risk models. Machine learning practitioners typically make choices about how to represent features in a particular model, but these choices are seldom evaluated quantitatively. This study compares the performance of different representations of clinical event data from MIMIC II in a logistic regression model to predict 36-hour ICU mortality. The most common representations are linear (normalized counts) and binary (yes/no). These, along with a new representation termed \"hill\", are compared using both L1 and L2 regularization. Results indicate that the introduced \"hill\" representation outperforms both the binary and linear representations, the hill representation thus has the potential to improve existing models of ICU mortality.\n    ",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2016-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05406",
        "title": "Signal Representations on Graphs: Tools and Applications",
        "authors": [
            "Siheng Chen",
            "Rohan Varma",
            "Aarti Singh",
            "Jelena Kova\u010devi\u0107"
        ],
        "abstract": "We present a framework for representing and modeling data on graphs. Based on this framework, we study three typical classes of graph signals: smooth graph signals, piecewise-constant graph signals, and piecewise-smooth graph signals. For each class, we provide an explicit definition of the graph signals and construct a corresponding graph dictionary with desirable properties. We then study how such graph dictionary works in two standard tasks: approximation and sampling followed with recovery, both from theoretical as well as algorithmic perspectives. Finally, for each class, we present a case study of a real-world problem by using the proposed methodology.\n    ",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05467",
        "title": "Unsupervised Feature Construction for Improving Data Representation and Semantics",
        "authors": [
            "Marian-Andrei Rizoiu",
            "Julien Velcin",
            "St\u00e9phane Lallich"
        ],
        "abstract": "Feature-based format is the main data representation format used by machine learning algorithms. When the features do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly-constructed features are rarely comprehensible. We seek to construct, in an unsupervised way, new features that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new features as conjunctions of the initial primitive features or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like $sky \\wedge \\neg building \\wedge panorama$ would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is used in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05484",
        "title": "Deep Active Object Recognition by Joint Label and Action Prediction",
        "authors": [
            "Mohsen Malmir",
            "Karan Sikka",
            "Deborah Forster",
            "Ian Fasel",
            "Javier R. Movellan",
            "Garrison W. Cottrell"
        ],
        "abstract": "An active object recognition system has the advantage of being able to act in the environment to capture images that are more suited for training and that lead to better performance at test time. In this paper, we propose a deep convolutional neural network for active object recognition that simultaneously predicts the object label, and selects the next action to perform on the object with the aim of improving recognition performance. We treat active object recognition as a reinforcement learning problem and derive the cost function to train the network for joint prediction of the object label and the action. A generative model of object similarities based on the Dirichlet distribution is proposed and embedded in the network for encoding the state of the system. The training is carried out by simultaneously minimizing the label and action prediction errors using gradient descent. We empirically show that the proposed network is able to predict both the object label and the actions on GERMS, a dataset for active object recognition. We compare the test label prediction accuracy of the proposed model with Dirichlet and Naive Bayes state encoding. The results of experiments suggest that the proposed model equipped with Dirichlet state encoding is superior in performance, and selects images that lead to better training and higher accuracy of label prediction at test time.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05569",
        "title": "A thermodynamical approach towards multi-criteria decision making (MCDM)",
        "authors": [
            "Mohit Verma",
            "J. Rajasankar"
        ],
        "abstract": "In multi-criteria decision making (MCDM) problems, ratings are assigned to the alternatives on different criteria by the expert group. In this paper, we propose a thermodynamically consistent model for MCDM using the analogies for thermodynamical indicators - energy, exergy and entropy. The most commonly used method for analysing MCDM problem is Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS). The conventional TOPSIS method uses a measure similar to that of energy for the ranking of alternatives. We demonstrate that the ranking of the alternatives is more meaningful if we use exergy in place of energy. The use of exergy is superior due to the inclusion of a factor accounting for the quality of the ratings by the expert group. The unevenness in the ratings by the experts is measured by entropy. The procedure for the calculation of the thermodynamical indicators is explained in both crisp and fuzzy environment. Finally, two case studies are carried out to demonstrate effectiveness of the proposed model.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05832",
        "title": "Learning the Preferences of Ignorant, Inconsistent Agents",
        "authors": [
            "Owain Evans",
            "Andreas Stuhlmueller",
            "Noah D. Goodman"
        ],
        "abstract": "An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05849",
        "title": "Modeling Progress in AI",
        "authors": [
            "Miles Brundage"
        ],
        "abstract": "Participants in recent discussions of AI-related issues ranging from intelligence explosion to technological unemployment have made diverse claims about the nature, pace, and drivers of progress in AI. However, these theories are rarely specified in enough detail to enable systematic evaluation of their assumptions or to extrapolate progress quantitatively, as is often done with some success in other technological domains. After reviewing relevant literatures and justifying the need for more rigorous modeling of AI progress, this paper contributes to that research program by suggesting ways to account for the relationship between hardware speed increases and algorithmic improvements in AI, the role of human inputs in enabling AI capabilities, and the relationships between different sub-fields of AI. It then outlines ways of tailoring AI progress models to generate insights on the specific issue of technological unemployment, and outlines future directions for research on AI progress.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06034",
        "title": "Ontology-driven Information Extraction",
        "authors": [
            "Weronika T. Adrian",
            "Nicola Leone",
            "Marco Manna"
        ],
        "abstract": "Homogeneous unstructured data (HUD) are collections of unstructured documents that share common properties, such as similar layout, common file format, or common domain of values. Building on such properties, it would be desirable to automatically process HUD to access the main information through a semantic layer -- typically an ontology -- called semantic view. Hence, we propose an ontology-based approach for extracting semantically rich information from HUD, by integrating and extending recent technologies and results from the fields of classical information extraction, table recognition, ontologies, text annotation, and logic programming. Moreover, we design and implement a system, named KnowRex, that has been successfully applied to curriculum vitae in the Europass style to offer a semantic view of them, and be able, for example, to select those which exhibit required skills.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06211",
        "title": "Test-Driven Development of ontologies (extended version)",
        "authors": [
            "C. Maria Keet",
            "Agnieszka Lawrynowicz"
        ],
        "abstract": "Emerging ontology authoring methods to add knowledge to an ontology focus on ameliorating the validation bottleneck. The verification of the newly added axiom is still one of trying and seeing what the reasoner says, because a systematic testbed for ontology authoring is missing. We sought to address this by introducing the approach of test-driven development for ontology authoring. We specify 36 generic tests, as TBox queries and TBox axioms tested through individuals, and structure their inner workings in an `open box'-way, which cover the OWL 2 DL language features. This is implemented as a Protege plugin so that one can perform a TDD test as a black box test. We evaluated the two test approaches on their performance. The TBox queries were faster, and that effect is more pronounced the larger the ontology is. We provide a general sequence of a TDD process for ontology engineering as a foundation for a TDD methodology.\n    ",
        "submission_date": "2015-12-19T00:00:00",
        "last_modified_date": "2015-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06427",
        "title": "Towards Integrated Glance To Restructuring in Combinatorial Optimization",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The paper focuses on a new class of combinatorial problems which consists in restructuring of solutions (as sets/structures) in combinatorial optimization. Two main features of the restructuring process are examined: (i) a cost of the restructuring, (ii) a closeness to a goal solution. Three types of the restructuring problems are under study: (a) one-stage structuring, (b) multi-stage structuring, and (c) structuring over changed element set. One-criterion and multicriteria problem formulations can be considered. The restructuring problems correspond to redesign (improvement, upgrade) of modular systems or solutions. The restructuring approach is described and illustrated (problem statements, solving schemes, examples) for the following combinatorial optimization problems: knapsack problem, multiple choice problem, assignment problem, spanning tree problems, clustering problem, multicriteria ranking (sorting) problem, morphological clique problem. Numerical examples illustrate the restructuring problems and solving schemes.\n    ",
        "submission_date": "2015-12-20T00:00:00",
        "last_modified_date": "2015-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06633",
        "title": "Constrained Sampling and Counting: Universal Hashing Meets SAT Solving",
        "authors": [
            "Kuldeep S. Meel",
            "Moshe Vardi",
            "Supratik Chakraborty",
            "Daniel J. Fremont",
            "Sanjit A. Seshia",
            "Dror Fried",
            "Alexander Ivrii",
            "Sharad Malik"
        ],
        "abstract": "Constrained sampling and counting are two fundamental problems in artificial intelligence with a diverse range of applications, spanning probabilistic reasoning and planning to constrained-random verification. While the theory of these problems was thoroughly investigated in the 1980s, prior work either did not scale to industrial size instances or gave up correctness guarantees to achieve scalability. Recently, we proposed a novel approach that combines universal hashing and SAT solving and scales to formulas with hundreds of thousands of variables without giving up correctness guarantees. This paper provides an overview of the key ingredients of the approach and discusses challenges that need to be overcome to handle larger real-world instances.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06747",
        "title": "Multivariate Time Series Classification Using Dynamic Time Warping Template Selection for Human Activity Recognition",
        "authors": [
            "Skyler Seto",
            "Wenyu Zhang",
            "Yichen Zhou"
        ],
        "abstract": "Accurate and computationally efficient means for classifying human activities have been the subject of extensive research efforts. Most current research focuses on extracting complex features to achieve high classification accuracy. We propose a template selection approach based on Dynamic Time Warping, such that complex feature extraction and domain knowledge is avoided. We demonstrate the predictive capability of the algorithm on both simulated and real smartphone data.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06992",
        "title": "On the Differential Privacy of Bayesian Inference",
        "authors": [
            "Zuhe Zhang",
            "Benjamin Rubinstein",
            "Christos Dimitrakakis"
        ],
        "abstract": "We study how to communicate findings of Bayesian inference to third parties, while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on proba-bilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian na{\u00ef}ve Bayes and Bayesian linear regression illustrate the application of our mechanisms.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07048",
        "title": "Beauty and Brains: Detecting Anomalous Pattern Co-Occurrences",
        "authors": [
            "Roel Bertens",
            "Jilles Vreeken",
            "Arno Siebes"
        ],
        "abstract": "Our world is filled with both beautiful and brainy people, but how often does a Nobel Prize winner also wins a beauty pageant? Let us assume that someone who is both very beautiful and very smart is more rare than what we would expect from the combination of the number of beautiful and brainy people. Of course there will still always be some individuals that defy this stereotype; these beautiful brainy people are exactly the class of anomaly we focus on in this paper. They do not posses intrinsically rare qualities, it is the unexpected combination of factors that makes them stand out.\n",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07056",
        "title": "Keeping it Short and Simple: Summarising Complex Event Sequences with Multivariate Patterns",
        "authors": [
            "Roel Bertens",
            "Jilles Vreeken",
            "Arno Siebes"
        ],
        "abstract": "We study how to obtain concise descriptions of discrete multivariate sequential data. In particular, how to do so in terms of rich multivariate sequential patterns that can capture potentially highly interesting (cor)relations between sequences. To this end we allow our pattern language to span over the domains (alphabets) of all sequences, allow patterns to overlap temporally, as well as allow for gaps in their occurrences.\n",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07143",
        "title": "SR-Clustering: Semantic Regularized Clustering for Egocentric Photo Streams Segmentation",
        "authors": [
            "Mariella Dimiccoli",
            "Marc Bola\u00f1os",
            "Estefania Talavera",
            "Maedeh Aghaei",
            "Stavri G. Nikolov",
            "Petia Radeva"
        ],
        "abstract": "While wearable cameras are becoming increasingly popular, locating relevant information in large unstructured collections of egocentric images is still a tedious and time consuming processes. This paper addresses the problem of organizing egocentric photo streams acquired by a wearable camera into semantically meaningful segments. First, contextual and semantic information is extracted for each image by employing a Convolutional Neural Networks approach. Later, by integrating language processing, a vocabulary of concepts is defined in a semantic space. Finally, by exploiting the temporal coherence in photo streams, images which share contextual and semantic attributes are grouped together. The resulting temporal segmentation is particularly suited for further analysis, ranging from activity and event recognition to semantic indexing and summarization. Experiments over egocentric sets of nearly 17,000 images, show that the proposed approach outperforms state-of-the-art methods.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07162",
        "title": "Heuristic algorithms for finding distribution reducts in probabilistic rough set model",
        "authors": [
            "Xi'ao Ma",
            "Guoyin Wang",
            "Hong Yu"
        ],
        "abstract": "Attribute reduction is one of the most important topics in rough set theory. Heuristic attribute reduction algorithms have been presented to solve the attribute reduction problem. It is generally known that fitness functions play a key role in developing heuristic attribute reduction algorithms. The monotonicity of fitness functions can guarantee the validity of heuristic attribute reduction algorithms. In probabilistic rough set model, distribution reducts can ensure the decision rules derived from the reducts are compatible with those derived from the original decision table. However, there are few studies on developing heuristic attribute reduction algorithms for finding distribution reducts. This is partly due to the fact that there are no monotonic fitness functions that are used to design heuristic attribute reduction algorithms in probabilistic rough set model. The main objective of this paper is to develop heuristic attribute reduction algorithms for finding distribution reducts in probabilistic rough set model. For one thing, two monotonic fitness functions are constructed, from which equivalence definitions of distribution reducts can be obtained. For another, two modified monotonic fitness functions are proposed to evaluate the significance of attributes more effectively. On this basis, two heuristic attribute reduction algorithms for finding distribution reducts are developed based on addition-deletion method and deletion method. In particular, the monotonicity of fitness functions guarantees the rationality of the proposed heuristic attribute reduction algorithms. Results of experimental analysis are included to quantify the effectiveness of the proposed fitness functions and distribution reducts.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07590",
        "title": "Randomized Social Choice Functions Under Metric Preferences",
        "authors": [
            "Elliot Anshelevich",
            "John Postl"
        ],
        "abstract": "We determine the quality of randomized social choice mechanisms in a setting in which the agents have metric preferences: every agent has a cost for each alternative, and these costs form a metric. We assume that these costs are unknown to the mechanisms (and possibly even to the agents themselves), which means we cannot simply select the optimal alternative, i.e. the alternative that minimizes the total agent cost (or median agent cost). However, we do assume that the agents know their ordinal preferences that are induced by the metric space. We examine randomized social choice functions that require only this ordinal information and select an alternative that is good in expectation with respect to the costs from the metric. To quantify how good a randomized social choice function is, we bound the distortion, which is the worst-case ratio between expected cost of the alternative selected and the cost of the optimal alternative. We provide new distortion bounds for a variety of randomized mechanisms, for both general metrics and for important special cases. Our results show a sizable improvement in distortion over deterministic mechanisms.\n    ",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07679",
        "title": "Deep Reinforcement Learning in Large Discrete Action Spaces",
        "authors": [
            "Gabriel Dulac-Arnold",
            "Richard Evans",
            "Hado van Hasselt",
            "Peter Sunehag",
            "Timothy Lillicrap",
            "Jonathan Hunt",
            "Timothy Mann",
            "Theophane Weber",
            "Thomas Degris",
            "Ben Coppin"
        ],
        "abstract": "Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07721",
        "title": "Measuring pattern retention in anonymized data -- where one measure is not enough",
        "authors": [
            "Sam Fletcher",
            "Md Zahidul Islam"
        ],
        "abstract": "In this paper, we explore how modifying data to preserve privacy affects the quality of the patterns discoverable in the data. For any analysis of modified data to be worth doing, the data must be as close to the original as possible. Therein lies a problem -- how does one make sure that modified data still contains the information it had before modification? This question is not the same as asking if an accurate classifier can be built from the modified data. Often in the literature, the prediction accuracy of a classifier made from modified (anonymized) data is used as evidence that the data is similar to the original. We demonstrate that this is not the case, and we propose a new methodology for measuring the retention of the patterns that existed in the original data. We then use our methodology to design three measures that can be easily implemented, each measuring aspects of the data that no pre-existing techniques can measure. These measures do not negate the usefulness of prediction accuracy or other measures -- they are complementary to them, and support our argument that one measure is almost never enough.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07734",
        "title": "RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent Predicate Cycles",
        "authors": [
            "Zhichun Wang",
            "Juanzi Li"
        ],
        "abstract": "Recently, several large-scale RDF knowledge bases have been built and applied in many knowledge-based applications. To further increase the number of facts in RDF knowledge bases, logic rules can be used to predict new facts based on the existing ones. Therefore, how to automatically learn reliable rules from large-scale knowledge bases becomes increasingly important. In this paper, we propose a novel rule learning approach named RDF2Rules for RDF knowledge bases. RDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting frequent patterns in knowledge bases, and then generates rules from the mined FPCs. Because each FPC can produce multiple rules, and effective pruning strategy is used in the process of mining FPCs, RDF2Rules works very efficiently. Another advantage of RDF2Rules is that it uses the entity type information when generates and evaluates rules, which makes the learned rules more accurate. Experiments show that our approach outperforms the compared approach in terms of both efficiency and accuracy.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07931",
        "title": "Probabilistic Model-Based Approach for Heart Beat Detection",
        "authors": [
            "Hugh Chen",
            "Yusuf Erol",
            "Eric Shen",
            "Stuart Russell"
        ],
        "abstract": "Nowadays, hospitals are ubiquitous and integral to modern society. Patients flow in and out of a veritable whirlwind of paperwork, consultations, and potential inpatient admissions, through an abstracted system that is not without flaws. One of the biggest flaws in the medical system is perhaps an unexpected one: the patient alarm system. One longitudinal study reported an 88.8% rate of false alarms, with other studies reporting numbers of similar magnitudes. These false alarm rates lead to a number of deleterious effects that manifest in a significantly lower standard of care across clinics.\n",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07943",
        "title": "Toward a Research Agenda in Adversarial Reasoning: Computational Approaches to Anticipating the Opponent's Intent and Actions",
        "authors": [
            "Alexander Kott",
            "Michael Ownby"
        ],
        "abstract": "This paper defines adversarial reasoning as computational approaches to inferring and anticipating an enemy's perceptions, intents and actions. It argues that adversarial reasoning transcends the boundaries of game theory and must also leverage such disciplines as cognitive modeling, control theory, AI planning and others. To illustrate the challenges of applying adversarial reasoning to real-world problems, the paper explores the lessons learned in the CADET - a battle planning system that focuses on brigade-level ground operations and involves adversarial reasoning. From this example of current capabilities, the paper proceeds to describe RAID - a DARPA program that aims to build capabilities in adversarial reasoning, and how such capabilities would address practical requirements in Defense and other application areas.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08048",
        "title": "Using Data Analytics to Detect Anomalous States in Vehicles",
        "authors": [
            "Sandeep Nair Narayanan",
            "Sudip Mittal",
            "Anupam Joshi"
        ],
        "abstract": "Vehicles are becoming more and more connected, this opens up a larger attack surface which not only affects the passengers inside vehicles, but also people around them. These vulnerabilities exist because modern systems are built on the comparatively less secure and old CAN bus framework which lacks even basic authentication. Since a new protocol can only help future vehicles and not older vehicles, our approach tries to solve the issue as a data analytics problem and use machine learning techniques to secure cars. We develop a Hidden Markov Model to detect anomalous states from real data collected from vehicles. Using this model, while a vehicle is in operation, we are able to detect and issue alerts. Our model could be integrated as a plug-n-play device in all new and old cars.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08451",
        "title": "GELATO and SAGE: An Integrated Framework for MS Annotation",
        "authors": [
            "Khalifeh AlJadda",
            "Rene Ranzinger",
            "Melody Porterfield",
            "Brent Weatherly",
            "Mohammed Korayem",
            "John A. Miller",
            "Khaled Rasheed",
            "Krys J. Kochut",
            "William S. York"
        ],
        "abstract": "Several algorithms and tools have been developed to (semi) automate the process of glycan identification by interpreting Mass Spectrometric data. However, each has limitations when annotating MSn data with thousands of MS spectra using uncurated public databases. Moreover, the existing tools are not designed to manage MSn data where n > 2. We propose a novel software package to automate the annotation of tandem MS data. This software consists of two major components. The first, is a free, semi-automated MSn data interpreter called the Glycomic Elucidation and Annotation Tool (GELATO). This tool extends and automates the functionality of existing open source projects, namely, GlycoWorkbench (GWB) and GlycomeDB. The second is a machine learning model called Smart Anotation Enhancement Graph (SAGE), which learns the behavior of glycoanalysts to select annotations generated by GELATO that emulate human interpretation of the spectra.\n    ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08525",
        "title": "Mining Massive Hierarchical Data Using a Scalable Probabilistic Graphical Model",
        "authors": [
            "Khalifeh AlJadda",
            "Mohammed Korayem",
            "Camilo Ortiz",
            "Trey Grainger",
            "John A. Miller",
            "Khaled Rasheed",
            "Krys J. Kochut",
            "William S. York",
            "Rene Ranzinger",
            "Melody Porterfield"
        ],
        "abstract": "Probabilistic Graphical Models (PGM) are very useful in the fields of machine learning and data mining. The crucial limitation of those models,however, is the scalability. The Bayesian Network, which is one of the most common PGMs used in machine learning and data mining, demonstrates this limitation when the training data consists of random variables, each of them has a large set of possible values. In the big data era, one would expect new extensions to the existing PGMs to handle the massive amount of data produced these days by computers, sensors and other electronic devices. With hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, Bayesian Networks become infeasible for representing the probability distributions. In this paper we introduce an extension to Bayesian Networks to handle massive sets of hierarchical data in a reasonable amount of time and space. The proposed model achieves perfect precision of 1.0 and high recall of 0.93 when it is used as multi-label classifier for the annotation of mass spectrometry data. On another data set of 1.5 billion search logs provided by ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2015-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08553",
        "title": "Conditional probability generation methods for high reliability effects-based decision making",
        "authors": [
            "Wolfgang Garn",
            "Panos Louvieris"
        ],
        "abstract": "Decision making is often based on Bayesian networks. The building blocks for Bayesian networks are its conditional probability tables (CPTs). These tables are obtained by parameter estimation methods, or they are elicited from subject matter experts (SME). Some of these knowledge representations are insufficient approximations. Using knowledge fusion of cause and effect observations lead to better predictive decisions. We propose three new methods to generate CPTs, which even work when only soft evidence is provided. The first two are novel ways of mapping conditional expectations to the probability space. The third is a column extraction method, which obtains CPTs from nonlinear functions such as the multinomial logistic regression. Case studies on military effects and burnt forest desertification have demonstrated that so derived CPTs have highly reliable predictive power, including superiority over the CPTs obtained from SMEs. In this context, new quality measures for determining the goodness of a CPT and for comparing CPTs with each other have been introduced. The predictive power and enhanced reliability of decision making based on the novel CPT generation methods presented in this paper have been confirmed and validated within the context of the case studies.\n    ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2015-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08710",
        "title": "On the Foundations of the Brussels Operational-Realistic Approach to Cognition",
        "authors": [
            "Diederik Aerts",
            "Massimiliano Sassoli de Bianchi",
            "Sandro Sozzo"
        ],
        "abstract": "The scientific community is becoming more and more interested in the research that applies the mathematical formalism of quantum theory to model human decision-making. In this paper, we provide the theoretical foundations of the quantum approach to cognition that we developed in Brussels. These foundations rest on the results of two decade studies on the axiomatic and operational-realistic approaches to the foundations of quantum physics. The deep analogies between the foundations of physics and cognition lead us to investigate the validity of quantum theory as a general and unitary framework for cognitive processes, and the empirical success of the Hilbert space models derived by such investigation provides a strong theoretical confirmation of this validity. However, two situations in the cognitive realm, 'question order effects' and 'response replicability', indicate that even the Hilbert space framework could be insufficient to reproduce the collected data. This does not mean that the mentioned operational-realistic approach would be incorrect, but simply that a larger class of measurements would be in force in human cognition, so that an extended quantum formalism may be needed to deal with all of them. As we will explain, the recently derived 'extended Bloch representation' of quantum theory (and the associated 'general tension-reduction' model) precisely provides such extended formalism, while remaining within the same unitary interpretative framework.\n    ",
        "submission_date": "2015-12-29T00:00:00",
        "last_modified_date": "2015-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08811",
        "title": "Combining Fuzzy Cognitive Maps and Discrete Random Variables",
        "authors": [
            "Piotr Szwed"
        ],
        "abstract": "In this paper we propose an extension to the Fuzzy Cognitive Maps (FCMs) that aims at aggregating a number of reasoning tasks into a one parallel run. The described approach consists in replacing real-valued activation levels of concepts (and further influence weights) by random variables. Such extension, followed by the implemented software tool, allows for determining ranges reached by concept activation levels, sensitivity analysis as well as statistical analysis of multiple reasoning results. We replace multiplication and addition operators appearing in the FCM state equation by appropriate convolutions applicable for discrete random variables. To make the model computationally feasible, it is further augmented with aggregation operations for discrete random variables. We discuss four implemented aggregators, as well as we report results of preliminary tests.\n    ",
        "submission_date": "2015-12-29T00:00:00",
        "last_modified_date": "2015-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08899",
        "title": "Modeling Variations of First-Order Horn Abduction in Answer Set Programming",
        "authors": [
            "Peter Sch\u00fcller"
        ],
        "abstract": "We study abduction in First Order Horn logic theories where all atoms can be abduced and we are looking for preferred solutions with respect to three objective functions: cardinality minimality, coherence, and weighted abduction. We represent this reasoning problem in Answer Set Programming (ASP), in order to obtain a flexible framework for experimenting with global constraints and objective functions, and to test the boundaries of what is possible with ASP. Realizing this problem in ASP is challenging as it requires value invention and equivalence between certain constants, because the Unique Names Assumption does not hold in general. To permit reasoning in cyclic theories, we formally describe fine-grained variations of limiting Skolemization. We identify term equivalence as a main instantiation bottleneck, and improve the efficiency of our approach with on-demand constraints that were used to eliminate the same bottleneck in state-of-the-art solvers. We evaluate our approach experimentally on the ACCEL benchmark for plan recognition in Natural Language Understanding. Our encodings are publicly available, modular, and our approach is more efficient than state-of-the-art solvers on the ACCEL benchmark.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2018-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08969",
        "title": "Evaluating Go Game Records for Prediction of Player Attributes",
        "authors": [
            "Josef Moud\u0159\u00edk",
            "Petr Baudi\u0161",
            "Roman Neruda"
        ],
        "abstract": "We propose a way of extracting and aggregating per-move evaluations from sets of Go game records. The evaluations capture different aspects of the games such as played patterns or statistic of sente/gote sequences. Using machine learning algorithms, the evaluations can be utilized to predict different relevant target variables. We apply this methodology to predict the strength and playing style of the player (e.g. territoriality or aggressivity) with good accuracy. We propose a number of possible applications including aiding in Go study, seeding real-work ranks of internet players or tuning of Go-playing programs.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2015-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09075",
        "title": "A Notation for Markov Decision Processes",
        "authors": [
            "Philip S. Thomas",
            "Billy Okal"
        ],
        "abstract": "This paper specifies a notation for Markov decision processes.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2016-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09254",
        "title": "Evolving Non-linear Stacking Ensembles for Prediction of Go Player Attributes",
        "authors": [
            "Josef Moud\u0159\u00edk",
            "Roman Neruda"
        ],
        "abstract": "The paper presents an application of non-linear stacking ensembles for prediction of Go player attributes. An evolutionary algorithm is used to form a diverse ensemble of base learners, which are then aggregated by a stacking ensemble. This methodology allows for an efficient prediction of different attributes of Go players from sets of their games. These attributes can be fairly general, in this work, we used the strength and style of the players.\n    ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2015-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01086",
        "title": "A Novel Design of a Parallel Machine Learnt Generational Garbage Collector",
        "authors": [
            "Vasanthakumar Soundararajan"
        ],
        "abstract": "The Generational Garbage collection involves organizing the heap into different divisions of memory space in-order to filter long-lived objects from short-lived objects through moving the surviving object of each generation Garbage Collection cycle to another memory space updating its age and reclaiming space from the dead ones. The problem in this method is that the longer an object is alive during its initial generations the longer the garbage collector will have to deal with it by checking for its reachability from the root and promoting it to other space divisions where as the ultimate goal of the Garbage Collector is to reclaim memory from unreachable objects at a minimal time possible. This paper is a proposal of a method where the lifetime of every object getting into the heap will be predicted and will be placed in heap accordingly for the garbage collector to deal more with reclaiming space from dead objects and less in promoting the live ones to the higher level.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01576",
        "title": "Improving image watermarking based on Tabu search by Chaos",
        "authors": [
            "Mohammad Tafaghodi",
            "Meysam Ghaffari",
            "Alimohammad Latif",
            "Seyed Rasoul Mousavi"
        ],
        "abstract": "With the fast development of communication and multimedia technology, the rights of the owners of multimedia products is vulnerable to the unauthorized copies and watermarking is one of the best known methods for proving the ownership of a product. In this paper we prosper the previous watermarking method which was based on Tabu search by Chaos. The modification applied in the permutation step of watermarking and the initial population generation of the Tabu search. We analyze our method on some well known images and experimental results shows the improvement in the quality and speed of the proposed watermarking method.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01678",
        "title": "LeoTask: a fast, flexible and reliable framework for computational research",
        "authors": [
            "Changwang Zhang",
            "Shi Zhou",
            "Benjamin M. Chain"
        ],
        "abstract": "LeoTask is a Java library for computation-intensive and time-consuming research tasks. It automatically executes tasks in parallel on multiple CPU cores on a computing facility. It uses a configuration file to enable automatic exploration of parameter space and flexible aggregation of results, and therefore allows researchers to focus on programming the key logic of a computing task. It also supports reliable recovery from interruptions, dynamic and cloneable networks, and integration with the plotting software Gnuplot.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02036",
        "title": "Improving the Deductive System DES with Persistence by Using SQL DBMS's",
        "authors": [
            "Fernando S\u00e1enz-P\u00e9rez"
        ],
        "abstract": "This work presents how persistent predicates have been included in the in-memory deductive system DES by relying on external SQL database management systems. We introduce how persistence is supported from a user-point of view and the possible applications the system opens up, as the deductive expressive power is projected to relational databases. Also, we describe how it is possible to intermix computations of the deductive engine and the external database, explaining its implementation and some optimizations. Finally, a performance analysis is undertaken, comparing the system with current relational database systems.\n    ",
        "submission_date": "2015-01-09T00:00:00",
        "last_modified_date": "2015-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02192",
        "title": "Investigation of a chaotic spiking neuron model",
        "authors": [
            "M. Alhawarat",
            "T. Olde Scheper",
            "N.T. Crook"
        ],
        "abstract": "Chaos provides many interesting properties that can be used to achieve computational tasks. Such properties are sensitivity to initial conditions, space filling, control and synchronization. Chaotic neural models have been devised to exploit such properties. In this paper, a chaotic spiking neuron model is investigated experimentally. This investigation is performed to understand the dynamic behaviours of the model.\n",
        "submission_date": "2015-01-09T00:00:00",
        "last_modified_date": "2015-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02315",
        "title": "Long-term causal effects via behavioral game theory",
        "authors": [
            "Panagiotis",
            "Toulis",
            "David C. Parkes"
        ],
        "abstract": "Planned experiments are the gold standard in reliably comparing the causal effect of switching from a baseline policy to a new policy. One critical shortcoming of classical experimental methods, however, is that they typically do not take into account the dynamic nature of response to policy changes. For instance, in an experiment where we seek to understand the effects of a new ad pricing policy on auction revenue, agents may adapt their bidding in response to the experimental pricing changes. Thus, causal effects of the new pricing policy after such adaptation period, the {\\em long-term causal effects}, are not captured by the classical methodology even though they clearly are more indicative of the value of the new policy. Here, we formalize a framework to define and estimate long-term causal effects of policy changes in multiagent economies. Central to our approach is behavioral game theory, which we leverage to formulate the ignorability assumptions that are necessary for causal inference. Under such assumptions we estimate long-term causal effects through a latent space approach, where a behavioral model of how agents act conditional on their latent behaviors is combined with a temporal model of how behaviors evolve over time.\n    ",
        "submission_date": "2015-01-10T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02527",
        "title": "Autodetection and Classification of Hidden Cultural City Districts from Yelp Reviews",
        "authors": [
            "Harini Suresh",
            "Nicholas Locascio"
        ],
        "abstract": "Topic models are a way to discover underlying themes in an otherwise unstructured collection of documents. In this study, we specifically used the Latent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to classify restaurants based off of their reviews. Furthermore, we hypothesize that within a city, restaurants can be grouped into similar \"clusters\" based on both location and similarity. We used several different clustering methods, including K-means Clustering and a Probabilistic Mixture Model, in order to uncover and classify districts, both well-known and hidden (i.e. cultural areas like Chinatown or hearsay like \"the best street for Italian restaurants\") within a city. We use these models to display and label different clusters on a map. We also introduce a topic similarity heatmap that displays the similarity distribution in a city to a new restaurant.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02629",
        "title": "Scaling-up Empirical Risk Minimization: Optimization of Incomplete U-statistics",
        "authors": [
            "St\u00e9phan Cl\u00e9men\u00e7on",
            "Aur\u00e9lien Bellet",
            "Igor Colin"
        ],
        "abstract": "In a wide range of statistical learning problems such as ranking, clustering or metric learning among others, the risk is accurately estimated by $U$-statistics of degree $d\\geq 1$, i.e. functionals of the training data with low variance that take the form of averages over $k$-tuples. From a computational perspective, the calculation of such statistics is highly expensive even for a moderate sample size $n$, as it requires averaging $O(n^d)$ terms. This makes learning procedures relying on the optimization of such data functionals hardly feasible in practice. It is the major goal of this paper to show that, strikingly, such empirical risks can be replaced by drastically computationally simpler Monte-Carlo estimates based on $O(n)$ terms only, usually referred to as incomplete $U$-statistics, without damaging the $O_{\\mathbb{P}}(1/\\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM) procedures. For this purpose, we establish uniform deviation results describing the error made when approximating a $U$-process by its incomplete version under appropriate complexity assumptions. Extensions to model selection, fast rate situations and various sampling techniques are also considered, as well as an application to stochastic gradient descent for ERM. Finally, numerical examples are displayed in order to provide strong empirical evidence that the approach we promote largely surpasses more naive subsampling techniques.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02662",
        "title": "Social Participation Ontology: community documentation, enhancements and use examples",
        "authors": [
            "Renato Fabbri",
            "Henrique Parra Parra Filho",
            "Rodrigo Bandeira de Luna",
            "Ricardo Augusto Poppi Martins",
            "Flor Karina Mamani Amanqui",
            "Dilvan de Abreu Moreira",
            "Osvaldo Novais de Oliveira Junior"
        ],
        "abstract": "Participatory democracy advances in virtually all governments and especially in South America which exhibits a mixed culture and social predisposition. This article presents the \"Social Participation Ontology\" (OPS from the Brazilian name \\emph{Ontologia de Participa\u00e7\u00e3o Social}) implemented in compliance with the Web Ontology Language standard (OWL) for fostering social participation, specially in virtual platforms. The entities and links of OPS were defined based on an extensive collaboration of specialists. It is shown that OPS is instrumental for information retrieval from the contents of the portal, both in terms of the actors (at various levels) as well as mechanisms and activities. Significantly, OPS is linked to other OWL ontologies as an upper ontology and via FOAF and BFO as higher upper ontologies, which yields sound organization and access of knowledge and data. In order to illustrate the usefulness of OPS, we present results on ontological expansion and integration with other ontologies and data. Ongoing work involves further adoption of OPS by the official Brazilian federal portal for social participation and NGO s, and further linkage to other ontologies for social participation.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04346",
        "title": "Mathematical Language Processing: Automatic Grading and Feedback for Open Response Mathematical Questions",
        "authors": [
            "Andrew S. Lan",
            "Divyanshu Vats",
            "Andrew E. Waters",
            "Richard G. Baraniuk"
        ],
        "abstract": "While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04358",
        "title": "Does Learning Imply a Decrease in the Entropy of Behavior?",
        "authors": [
            "Paul E. Smaldino"
        ],
        "abstract": "Shannon's information entropy measures of the uncertainty of an event's outcome. If learning about a system reflects a decrease in uncertainty, then a plausible intuition is that learning should be accompanied by a decrease in the entropy of the organism's actions and/or perceptual states. To address whether this intuition is valid, I examined an artificial organism -- a simple robot -- that learned to navigate in an arena and analyzed the entropy of the outcome variables action, state, and reward. Entropy did indeed decrease in the initial stages of learning, but two factors complicated the scenario: (1) the introduction of new options discovered during the learning process and (2) the shifting patterns of perceptual and environmental states resulting from changes to the robot's learned movement strategies. These factors lead to a subsequent increase in entropy as the agent learned. I end with a discussion of the utility of information-based characterizations of learning.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04413",
        "title": "Statistical-mechanical analysis of pre-training and fine tuning in deep learning",
        "authors": [
            "Masayuki Ohzeki"
        ],
        "abstract": "In this paper, we present a statistical-mechanical analysis of deep learning. We elucidate some of the essential components of deep learning---pre-training by unsupervised learning and fine tuning by supervised learning. We formulate the extraction of features from the training data as a margin criterion in a high-dimensional feature-vector space. The self-organized classifier is then supplied with small amounts of labelled data, as in deep learning. Although we employ a simple single-layer perceptron model, rather than directly analyzing a multi-layer neural network, we find a nontrivial phase transition that is dependent on the number of unlabelled data in the generalization error of the resultant classifier. In this sense, we evaluate the efficacy of the unsupervised learning component of deep learning. The analysis is performed by the replica method, which is a sophisticated tool in statistical mechanics. We validate our result in the manner of deep learning, using a simple iterative algorithm to learn the weight vector on the basis of belief propagation.\n    ",
        "submission_date": "2015-01-19T00:00:00",
        "last_modified_date": "2015-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04792",
        "title": "Designing a Belief Function-Based Accessibility Indicator to Improve Web Browsing for Disabled People",
        "authors": [
            "Jean-Christophe Dubois",
            "Yolande Le Gall",
            "Arnaud Martin"
        ],
        "abstract": "The purpose of this study is to provide an accessibility measure of web-pages, in order to draw disabled users to the pages that have been designed to be ac-cessible to them. Our approach is based on the theory of belief functions, using data which are supplied by reports produced by automatic web content assessors that test the validity of criteria defined by the WCAG 2.0 guidelines proposed by the World Wide Web Consortium (W3C) organization. These tools detect errors with gradual degrees of certainty and their results do not always converge. For these reasons, to fuse information coming from the reports, we choose to use an information fusion framework which can take into account the uncertainty and imprecision of infor-mation as well as divergences between sources. Our accessibility indicator covers four categories of deficiencies. To validate the theoretical approach in this context, we propose an evaluation completed on a corpus of 100 most visited French news websites, and 2 evaluation tools. The results obtained illustrate the interest of our accessibility indicator.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04832",
        "title": "Big Data: How Geo-information Helped Shape the Future of Data Engineering",
        "authors": [
            "Robert Jeansoulin"
        ],
        "abstract": "Very large data sets are the common rule in automated mapping, GIS, remote sensing, and what we can name geo-information. Indeed, in 1983 Landsat was already delivering gigabytes of data, and other sensors were in orbit or ready for launch, and a tantamount of cartographic data was being digitized. The retrospective paper revisits several issues that geo-information sciences had to face from the early stages on, including: structure ( to bring some structure to the data registered from a sampled signal, metadata); processing (huge amounts of data for big computers and fast algorithms); uncertainty (the kinds of errors, their quantification); consistency (when merging different sources of data is logically allowed, and meaningful); ontologies (clear and agreed shared definitions, if any kind of decision should be based upon them). All these issues are the background of Internet queries, and the underlying technology has been shaped during those years when geo-information engineering emerged.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05215",
        "title": "A Separation Theorem for Chain Event Graphs",
        "authors": [
            "Peter A. Thwaites",
            "Jim Q. Smith"
        ],
        "abstract": "Bayesian Networks (BNs) are popular graphical models for the representation of statistical problems embodying dependence relationships between a number of variables. Much of this popularity is due to the d-separation theorem of Pearl and Lauritzen, which allows an analyst to identify the conditional independence statements that a model of the problem embodies using only the topology of the graph. However for many problems the complete model dependence structure cannot be depicted by a BN. The Chain Event Graph (CEG) was introduced for these types of problem. In this paper we introduce a separation theorem for CEGs, analogous to the d-separation theorem for BNs, which likewise allows an analyst to identify the conditional independence structure of their model from the topology of the graph.\n    ",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2015-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05290",
        "title": "Managing large-scale scientific hypotheses as uncertain and probabilistic data",
        "authors": [
            "Bernardo Gon\u00e7alves"
        ],
        "abstract": "In view of the paradigm shift that makes science ever more data-driven, in this thesis we propose a synthesis method for encoding and managing large-scale deterministic scientific hypotheses as uncertain and probabilistic data.\n",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05426",
        "title": "Classification of Message Spreading in a Heterogeneous Social Network",
        "authors": [
            "Siwar Jendoubi",
            "Arnaud Martin",
            "Ludovic Li\u00e9tard",
            "Boutheina Ben Yaghlane"
        ],
        "abstract": "Nowadays, social networks such as Twitter, Facebook and LinkedIn become increasingly popular. In fact, they introduced new habits, new ways of communication and they collect every day several information that have different sources. Most existing research works fo-cus on the analysis of homogeneous social networks, i.e. we have a single type of node and link in the network. However, in the real world, social networks offer several types of nodes and links. Hence, with a view to preserve as much information as possible, it is important to consider so-cial networks as heterogeneous and uncertain. The goal of our paper is to classify the social message based on its spreading in the network and the theory of belief functions. The proposed classifier interprets the spread of messages on the network, crossed paths and types of links. We tested our classifier on a real word network that we collected from Twitter, and our experiments show the performance of our belief classifier.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05973",
        "title": "Inferring and Learning from Neuronal Correspondences",
        "authors": [
            "Ashish Kapoor",
            "E. Paxon Frady",
            "Stefanie Jegelka",
            "William B. Kristan",
            "Eric Horvitz"
        ],
        "abstract": "We introduce and study methods for inferring and learning from correspondences among neurons. The approach enables alignment of data from distinct multiunit studies of nervous systems. We show that the methods for inferring correspondences combine data effectively from cross-animal studies to make joint inferences about behavioral decision making that are not possible with the data from a single animal. We focus on data collection, machine learning, and prediction in the representative and long-studied invertebrate nervous system of the European medicinal leech. Acknowledging the computational intractability of the general problem of identifying correspondences among neurons, we introduce efficient computational procedures for matching neurons across animals. The methods include techniques that adjust for missing cells or additional cells in the different data sets that may reflect biological or experimental variation. The methods highlight the value harnessing inference and learning in new kinds of computational microscopes for multiunit neurobiological studies.\n    ",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06206",
        "title": "Dynamics of Belief: Abduction, Horn Knowledge Base And Database Updates",
        "authors": [
            "Radhakrishnan Delhibabu"
        ],
        "abstract": "The dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information. In order to apply the rationality result of belief dynamics theory to various practical problems, it should be generalized in two respects: first it should allow a certain part of belief to be declared as immutable; and second, the belief state need not be deductively closed. Such a generalization of belief dynamics, referred to as base dynamics, is presented in this paper, along with the concept of a generalized revision algorithm for knowledge bases (Horn or Horn logic with stratified negation). We show that knowledge base dynamics has an interesting connection with kernel change via hitting set and abduction. In this paper, we show how techniques from disjunctive logic programming can be used for efficient (deductive) database updates. The key idea is to transform the given database together with the update request into a disjunctive (datalog) logic program and apply disjunctive techniques (such as minimal model reasoning) to solve the original update problem. The approach extends and integrates standard techniques for efficient query answering and integrity checking. The generation of a hitting set is carried out through a hyper tableaux calculus and magic set that is focused on the goal of minimality. The present paper provides a comparative study of view update algorithms in rational approach. For, understand the basic concepts with abduction, we provide an abductive framework for knowledge base dynamics. Finally, we demonstrate how belief base dynamics can provide an axiomatic characterization for insertion a view atom to the database. We give a quick overview of the main operators for belief change, in particular, belief update versus database update.\n    ",
        "submission_date": "2015-01-25T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06727",
        "title": "Factorization, Inference and Parameter Learning in Discrete AMP Chain Graphs",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "We address some computational issues that may hinder the use of AMP chain graphs in practice. Specifically, we show how a discrete probability distribution that satisfies all the independencies represented by an AMP chain graph factorizes according to it. We show how this factorization makes it possible to perform inference and parameter learning efficiently, by adapting existing algorithms for Markov and Bayesian networks. Finally, we turn our attention to another issue that may hinder the use of AMP CGs, namely the lack of an intuitive interpretation of their edges. We provide one such interpretation.\n    ",
        "submission_date": "2015-01-27T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06769",
        "title": "Particle Gibbs with Ancestor Sampling for Probabilistic Programs",
        "authors": [
            "Jan-Willem van de Meent",
            "Hongseok Yang",
            "Vikash Mansinghka",
            "Frank Wood"
        ],
        "abstract": "Particle Markov chain Monte Carlo techniques rank among current state-of-the-art methods for probabilistic program inference. A drawback of these techniques is that they rely on importance resampling, which results in degenerate particle trajectories and a low effective sample size for variables sampled early in a program. We here develop a formalism to adapt ancestor resampling, a technique that mitigates particle degeneracy, to the probabilistic programming setting. We present empirical results that demonstrate nontrivial performance gains.\n    ",
        "submission_date": "2015-01-27T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00062",
        "title": "A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue Fever",
        "authors": [
            "Vadrevu Sree Hari Rao",
            "Mallenahalli Naresh Kumar"
        ],
        "abstract": "Identification of the influential clinical symptoms and laboratory features that help in the diagnosis of dengue fever in early phase of the illness would aid in designing effective public health management and virological surveillance strategies. Keeping this as our main objective we develop in this paper, a new computational intelligence based methodology that predicts the diagnosis in real time, minimizing the number of false positives and false negatives. Our methodology consists of three major components (i) a novel missing value imputation procedure that can be applied on any data set consisting of categorical (nominal) and/or numeric (real or integer) (ii) a wrapper based features selection method with genetic search for extracting a subset of most influential symptoms that can diagnose the illness and (iii) an alternating decision tree method that employs boosting for generating highly accurate decision rules. The predictive models developed using our methodology are found to be more accurate than the state-of-the-art methodologies used in the diagnosis of the dengue fever.\n    ",
        "submission_date": "2015-01-31T00:00:00",
        "last_modified_date": "2015-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00130",
        "title": "The Search for Computational Intelligence",
        "authors": [
            "Joseph Corneli",
            "Ewen Maclean"
        ],
        "abstract": "We define and explore in simulation several rules for the local evolution of generative rules for 1D and 2D cellular automata. Our implementation uses strategies from conceptual blending. We discuss potential applications to modelling social dynamics.\n    ",
        "submission_date": "2015-01-31T00:00:00",
        "last_modified_date": "2015-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00245",
        "title": "Injury risk prediction for traffic accidents in Porto Alegre/RS, Brazil",
        "authors": [
            "Christian S. Perone"
        ],
        "abstract": "This study describes the experimental application of Machine Learning techniques to build prediction models that can assess the injury risk associated with traffic accidents. This work uses an freely available data set of traffic accident records that took place in the city of Porto Alegre/RS (Brazil) during the year of 2013. This study also provides an analysis of the most important attributes of a traffic accident that could produce an outcome of injury to the people involved in the accident.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00725",
        "title": "Cheaper and Better: Selecting Good Workers for Crowdsourcing",
        "authors": [
            "Hongwei Li",
            "Qiang Liu"
        ],
        "abstract": "Crowdsourcing provides a popular paradigm for data collection at scale. We study the problem of selecting subsets of workers from a given worker pool to maximize the accuracy under a budget constraint. One natural question is whether we should hire as many workers as the budget allows, or restrict on a small number of top-quality workers. By theoretically analyzing the error rate of a typical setting in crowdsourcing, we frame the worker selection problem into a combinatorial optimization problem and propose an algorithm to solve it efficiently. Empirical results on both simulated and real-world datasets show that our algorithm is able to select a small number of high-quality workers, and performs as good as, sometimes even better than, the much larger crowds as the budget allows.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01321",
        "title": "Numerical Solution of Fuzzy Stochastic Differential Equation",
        "authors": [
            "Sukanta Nayak",
            "Snehashish Chakraverty"
        ],
        "abstract": "In this paper an alternative approach to solve uncertain Stochastic Differential Equation (SDE) is proposed. This uncertainty occurs due to the involved parameters in system and these are considered as Triangular Fuzzy Numbers (TFN). Here the proposed fuzzy arithmetic in [2] is used as a tool to handle Fuzzy Stochastic Differential Equation (FSDE). In particular, a system of Ito stochastic differential equations is analysed with fuzzy parameters. Further exact and Euler Maruyama approximation methods with fuzzy values are demonstrated and solved some standard SDE.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01852",
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02493",
        "title": "Implicit Contextual Integrity in Online Social Networks",
        "authors": [
            "Natalia Criado",
            "Jose M. Such"
        ],
        "abstract": "Many real incidents demonstrate that users of Online Social Networks need mechanisms that help them manage their interactions by increasing the awareness of the different contexts that coexist in Online Social Networks and preventing them from exchanging inappropriate information in those contexts or disseminating sensitive information from some contexts to others. Contextual integrity is a privacy theory that conceptualises the appropriateness of information sharing based on the contexts in which this information is to be shared. Computational models of Contextual Integrity assume the existence of well-defined contexts, in which individuals enact pre-defined roles and information sharing is governed by an explicit set of norms. However, contexts in Online Social Networks are known to be implicit, unknown a priori and ever changing; users relationships are constantly evolving; and the information sharing norms are implicit. This makes current Contextual Integrity models not suitable for Online Social Networks.\n",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02606",
        "title": "The Power of Randomization: Distributed Submodular Maximization on Massive Datasets",
        "authors": [
            "Rafael da Ponte Barbosa",
            "Alina Ene",
            "Huy L. Nguyen",
            "Justin Ward"
        ],
        "abstract": "A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We develop a simple distributed algorithm that is embarrassingly parallel and it achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02643",
        "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions",
        "authors": [
            "Alina Ene",
            "Huy L. Nguyen"
        ],
        "abstract": "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of ``simple\" functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02761",
        "title": "Generative Moment Matching Networks",
        "authors": [
            "Yujia Li",
            "Kevin Swersky",
            "Richard Zemel"
        ],
        "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03322",
        "title": "Boost Phrase-level Polarity Labelling with Review-level Sentiment Classification",
        "authors": [
            "Yongfeng Zhang",
            "Min Zhang",
            "Yiqun Liu",
            "Shaoping Ma"
        ],
        "abstract": "Sentiment analysis on user reviews helps to keep track of user reactions towards products, and make advices to users about what to buy. State-of-the-art review-level sentiment classification techniques could give pretty good precisions of above 90%. However, current phrase-level sentiment analysis approaches might only give sentiment polarity labelling precisions of around 70%~80%, which is far from satisfaction and restricts its application in many practical tasks. In this paper, we focus on the problem of phrase-level sentiment polarity labelling and attempt to bridge the gap between phrase-level and review-level sentiment analysis. We investigate the inconsistency between the numerical star ratings and the sentiment orientation of textual user reviews. Although they have long been treated as identical, which serves as a basic assumption in previous work, we find that this assumption is not necessarily true. We further propose to leverage the results of review-level sentiment classification to boost the performance of phrase-level polarity labelling using a novel constrained convex optimization framework. Besides, the framework is capable of integrating various kinds of information sources and heuristics, while giving the global optimal solution due to its convexity. Experimental results on both English and Chinese reviews show that our framework achieves high labelling precisions of up to 89%, which is a significant improvement from current approaches.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2015-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03473",
        "title": "Collaborative Filtering Bandits",
        "authors": [
            "Shuai Li",
            "Alexandros Karatzoglou",
            "Claudio Gentile"
        ],
        "abstract": "Classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, we investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Our algorithm takes into account the collaborative effects that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. We provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. We also provide a regret analysis within a standard linear stochastic noise setting.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03536",
        "title": "Speeding up Permutation Testing in Neuroimaging",
        "authors": [
            "Chris Hinrichs",
            "Vamsi K Ithapu",
            "Qinyuan Sun",
            "Sterling C Johnson",
            "Vikas Singh"
        ],
        "abstract": "Multiple hypothesis testing is a significant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non-parametric method of estimating the FWER for a given $\\alpha$-threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we show that permutation testing in fact amounts to populating the columns of a very large matrix ${\\bf P}$. By analyzing the spectrum of this matrix, under certain conditions, we see that ${\\bf P}$ has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub--sampled --- on the order of $0.5\\%$ --- matrix completion methods. Based on this observation, we propose a novel permutation testing methodology which offers a large speedup, without sacrificing the fidelity of the estimated FWER. Our evaluations on four different neuroimaging datasets show that a computational speedup factor of roughly $50\\times$ can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated $\\alpha$-threshold is also recovered faithfully, and is stable.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03796",
        "title": "Variable and value elimination in binary constraint satisfaction via forbidden patterns",
        "authors": [
            "David A. Cohen",
            "Martin C. Cooper",
            "Guillaume Escamocher",
            "Stanislav Zivny"
        ],
        "abstract": "Variable or value elimination in a constraint satisfaction problem (CSP) can be used in preprocessing or during search to reduce search space size. A variable elimination rule (value elimination rule) allows the polynomial-time identification of certain variables (domain elements) whose elimination, without the introduction of extra compensatory constraints, does not affect the satisfiability of an instance. We show that there are essentially just four variable elimination rules and three value elimination rules defined by forbidding generic sub-instances, known as irreducible existential patterns, in arc-consistent CSP instances. One of the variable elimination rules is the already-known Broken Triangle Property, whereas the other three are novel. The three value elimination rules can all be seen as strict generalisations of neighbourhood substitution.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03945",
        "title": "Efficiency and complexity of price competition among single-product vendors",
        "authors": [
            "Ioannis Caragiannis",
            "Xenophon Chatzigeorgiou",
            "Panagiotis Kanellopoulos",
            "George A. Krimpas",
            "Nikos Protopapas",
            "Alexandros A. Voudouris"
        ],
        "abstract": "Motivated by recent progress on pricing in the AI literature, we study marketplaces that contain multiple vendors offering identical or similar products and unit-demand buyers with different valuations on these vendors. The objective of each vendor is to set the price of its product to a fixed value so that its profit is maximized. The profit depends on the vendor's price itself and the total volume of buyers that find the particular price more attractive than the price of the vendor's competitors. We model the behaviour of buyers and vendors as a two-stage full-information game and study a series of questions related to the existence, efficiency (price of anarchy) and computational complexity of equilibria in this game. To overcome situations where equilibria do not exist or exist but are highly inefficient, we consider the scenario where some of the vendors are subsidized in order to keep prices low and buyers highly satisfied.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04049",
        "title": "How essential are unstructured clinical narratives and information fusion to clinical trial recruitment?",
        "authors": [
            "Preethi Raghavan",
            "James L. Chen",
            "Eric Fosler-Lussier",
            "Albert M. Lai"
        ],
        "abstract": "Electronic health records capture patient information using structured controlled vocabularies and unstructured narrative text. While structured data typically encodes lab values, encounters and medication lists, unstructured data captures the physician's interpretation of the patient's condition, prognosis, and response to therapeutic intervention. In this paper, we demonstrate that information extraction from unstructured clinical narratives is essential to most clinical applications. We perform an empirical study to validate the argument and show that structured data alone is insufficient in resolving eligibility criteria for recruiting patients onto clinical trials for chronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is essential to solving 59% of the CLL trial criteria and 77% of the prostate cancer trial criteria. More specifically, for resolving eligibility criteria with temporal constraints, we show the need for temporal reasoning and information integration with medical events within and across unstructured clinical narratives and structured data.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04149",
        "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation",
        "authors": [
            "Po-Sen Huang",
            "Minje Kim",
            "Mark Hasegawa-Johnson",
            "Paris Smaragdis"
        ],
        "abstract": "Monaural source separation is important for many real world applications. It is challenging because, with only a single channel of information available, without any constraints, an infinite number of solutions are possible. In this paper, we explore joint optimization of masking functions and deep recurrent neural networks for monaural source separation tasks, including monaural speech separation, monaural singing voice separation, and speech denoising. The joint optimization of the deep recurrent neural networks with an extra masking layer enforces a reconstruction constraint. Moreover, we explore a discriminative criterion for training neural networks to further enhance the separation performance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT datasets for speech separation, singing voice separation, and speech denoising tasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to NMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and 4.32--5.42 dB GSIR gain compared to existing models in the singing voice separation task, and outperform NMF and DNN baselines in the speech denoising task.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04266",
        "title": "Constrained Nonlinear Model Predictive Control of an MMA Polymerization Process via Evolutionary Optimization",
        "authors": [
            "Masoud Abbaszadeh",
            "Reza Solgi"
        ],
        "abstract": "In this work, a nonlinear model predictive controller is developed for a batch polymerization process. The physical model of the process is parameterized along a desired trajectory resulting in a trajectory linearized piecewise model (a multiple linear model bank) and the parameters are identified for an experimental polymerization reactor. Then, a multiple model adaptive predictive controller is designed for thermal trajectory tracking of the MMA polymerization. The input control signal to the process is constrained by the maximum thermal power provided by the heaters. The constrained optimization in the model predictive controller is solved via genetic algorithms to minimize a DMC cost function in each sampling interval.\n    ",
        "submission_date": "2015-02-15T00:00:00",
        "last_modified_date": "2015-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05696",
        "title": "Approval Voting and Incentives in Crowdsourcing",
        "authors": [
            "Nihar B. Shah",
            "Dengyong Zhou",
            "Yuval Peres"
        ],
        "abstract": "The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (\"strictly proper\") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05774",
        "title": "Low-Cost Learning via Active Data Procurement",
        "authors": [
            "Jacob Abernethy",
            "Yiling Chen",
            "Chien-Ju Ho",
            "Bo Waggoner"
        ],
        "abstract": "We design mechanisms for online procurement of data held by strategic agents for machine learning tasks. The challenge is to use past data to actively price future data and give learning guarantees even when an agent's cost for revealing her data may depend arbitrarily on the data itself. We achieve this goal by showing how to convert a large class of no-regret algorithms into online posted-price and learning mechanisms. Our results in a sense parallel classic sample complexity guarantees, but with the key resource being money rather than quantity of data: With a budget constraint $B$, we give robust risk (predictive error) bounds on the order of $1/\\sqrt{B}$. Because we use an active approach, we can often guarantee to do significantly better by leveraging correlations between costs and data.\n",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05844",
        "title": "An Approach For Transforming of Relational Databases to OWL Ontology",
        "authors": [
            "Mona Dadjoo",
            "Esmaeil Kheirkhah"
        ],
        "abstract": "Rapid growth of documents, web pages, and other types of text content is a huge challenge for the modern content management systems. One of the problems in the areas of information storage and retrieval is the lacking of semantic data. Ontologies can present knowledge in sharable and repeatedly usable manner and provide an effective way to reduce the data volume overhead by encoding the structure of a particular domain. Metadata in relational databases can be used to extract ontology from database in a special domain. According to solve the problem of sharing and reusing of data, approaches based on transforming relational database to ontology are proposed. In this paper we propose a method for automatic ontology construction based on relational database. Mining and obtaining further components from relational database leads to obtain knowledge with high semantic power and more expressiveness. Triggers are one of the database components which could be transformed to the ontology model and increase the amount of power and expressiveness of knowledge by presenting part of the knowledge dynamically\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05974",
        "title": "Development of a VO Registry Subject Ontology using Automated Methods",
        "authors": [
            "Brian Thomas"
        ],
        "abstract": "We report on our initial work to automate the generation of a domain ontology using subject fields of resources held in the Virtual Observatory registry. Preliminary results are comparable to more generalized ontology learning software currently in use. We expect to be able to refine our solution to improve both the depth and breadth of the generated ontology.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05988",
        "title": "Deep Learning for Multi-label Classification",
        "authors": [
            "Jesse Read",
            "Fernando Perez-Cruz"
        ],
        "abstract": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06025",
        "title": "OntoLoki: an automatic, instance-based method for the evaluation of biological ontologies on the Semantic Web",
        "authors": [
            "Benjamin M. Good",
            "Gavin Ha",
            "Chi K. Ho",
            "Mark D. Wilkinson"
        ],
        "abstract": "The delineation of logical definitions for each class in an ontology and the consistent application of these definitions to the assignment of instances to classes are important criteria for ontology evaluation. If ontologies are specified with property-based restrictions on class membership, then such consistency can be checked automatically. If no such logical restrictions are applied, as is the case with many biological ontologies, there are currently no automated methods for measuring the semantic consistency of instance assignment on an ontology-wide scale, nor for inferring the patterns of properties that might define a particular class. We constructed a program that takes as its input an OWL/RDF knowledge base containing an ontology, instances associated with each of the classes in the ontology, and properties of those instances. For each class, it outputs: 1) a rule for determining class membership based on the properties of the instances and 2) a quantitative score for the class that reflects the ability of the identified rule to correctly predict class membership for the instances in the knowledge base. We evaluated this program using both artificial knowledge bases of known quality and real, widely used ontologies. The results indicate that the suggested method can be used to conduct objective, automatic, data-driven evaluations of biological ontologies without formal class definitions in regards to the property-based consistency of instance-assignment. This inductive method complements existing, purely deductive approaches to automatic consistency checking, offering not just the potential to help in the ontology engineering process but also in the knowledge discovery process.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06030",
        "title": "Decentralized Control of Partially Observable Markov Decision Processes using Belief Space Macro-actions",
        "authors": [
            "Shayegan Omidshafiei",
            "Ali-akbar Agha-mohammadi",
            "Christopher Amato",
            "Jonathan P. How"
        ],
        "abstract": "The focus of this paper is on solving multi-robot planning problems in continuous spaces with partial observability. Decentralized partially observable Markov decision processes (Dec-POMDPs) are general models for multi-robot coordination problems, but representing and solving Dec-POMDPs is often intractable for large problems. To allow for a high-level representation that is natural for multi-robot problems and scalable to large discrete and continuous problems, this paper extends the Dec-POMDP model to the decentralized partially observable semi-Markov decision process (Dec-POSMDP). The Dec-POSMDP formulation allows asynchronous decision-making by the robots, which is crucial in multi-robot domains. We also present an algorithm for solving this Dec-POSMDP which is much more scalable than previous methods since it can incorporate closed-loop belief space macro-actions in planning. These macro-actions are automatically constructed to produce robust solutions. The proposed method's performance is evaluated on a complex multi-robot package delivery problem under uncertainty, showing that our approach can naturally represent multi-robot problems and provide high-quality solutions for large-scale problems.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06626",
        "title": "Optimal Sparse Linear Auto-Encoders and Sparse PCA",
        "authors": [
            "Malik Magdon-Ismail",
            "Christos Boutsidis"
        ],
        "abstract": "Principal components analysis (PCA) is the optimal linear auto-encoder of data, and it is often used to construct features. Enforcing sparsity on the principal components can promote better generalization, while improving the interpretability of the features. We study the problem of constructing optimal sparse linear auto-encoders. Two natural questions in such a setting are: i) Given a level of sparsity, what is the best approximation to PCA that can be achieved? ii) Are there low-order polynomial-time algorithms which can asymptotically achieve this optimal tradeoff between the sparsity and the approximation quality?\n",
        "submission_date": "2015-02-23T00:00:00",
        "last_modified_date": "2015-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07019",
        "title": "Building with Drones: Accurate 3D Facade Reconstruction using MAVs",
        "authors": [
            "Shreyansh Daftry",
            "Christof Hoppe",
            "Horst Bischof"
        ],
        "abstract": "Automatic reconstruction of 3D models from images using multi-view Structure-from-Motion methods has been one of the most fruitful outcomes of computer vision. These advances combined with the growing popularity of Micro Aerial Vehicles as an autonomous imaging platform, have made 3D vision tools ubiquitous for large number of Architecture, Engineering and Construction applications among audiences, mostly unskilled in computer vision. However, to obtain high-resolution and accurate reconstructions from a large-scale object using SfM, there are many critical constraints on the quality of image data, which often become sources of inaccuracy as the current 3D reconstruction pipelines do not facilitate the users to determine the fidelity of input data during the image acquisition. In this paper, we present and advocate a closed-loop interactive approach that performs incremental reconstruction in real-time and gives users an online feedback about the quality parameters like Ground Sampling Distance (GSD), image redundancy, etc on a surface mesh. We also propose a novel multi-scale camera network design to prevent scene drift caused by incremental map building, and release the first multi-scale image sequence dataset as a benchmark. Further, we evaluate our system on real outdoor scenes, and show that our interactive pipeline combined with a multi-scale camera network approach provides compelling accuracy in multi-view reconstruction tasks when compared against the state-of-the-art methods.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07571",
        "title": "Online Fair Division: analysing a Food Bank problem",
        "authors": [
            "Martin Aleksandrov",
            "Haris Aziz",
            "Serge Gaspers",
            "Toby Walsh"
        ],
        "abstract": "We study an online model of fair division designed to capture features of a real world charity problem. We consider two simple mechanisms for this model in which agents simply declare what items they like. We analyse several axiomatic properties of these mechanisms like strategy-proofness and envy-freeness. Finally, we perform a competitive analysis and compute the price of anarchy.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08029",
        "title": "Describing Videos by Exploiting Temporal Structure",
        "authors": [
            "Li Yao",
            "Atousa Torabi",
            "Kyunghyun Cho",
            "Nicolas Ballas",
            "Christopher Pal",
            "Hugo Larochelle",
            "Aaron Courville"
        ],
        "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08039",
        "title": "Probabilistic Zero-shot Classification with Semantic Rankings",
        "authors": [
            "Jihun Hamm",
            "Mikhail Belkin"
        ],
        "abstract": "In this paper we propose a non-metric ranking-based representation of semantic similarity that allows natural aggregation of semantic information from multiple heterogeneous sources. We apply the ranking-based representation to zero-shot learning problems, and present deterministic and probabilistic zero-shot classifiers which can be built from pre-trained classifiers without retraining. We demonstrate their the advantages on two large real-world image datasets. In particular, we show that aggregating different sources of semantic information, including crowd-sourcing, leads to more accurate classification.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00022",
        "title": "Plagiarism Detection in Polyphonic Music using Monaural Signal Separation",
        "authors": [
            "Soham De",
            "Indradyumna Roy",
            "Tarunima Prabhakar",
            "Kriti Suneja",
            "Sourish Chaudhuri",
            "Rita Singh",
            "Bhiksha Raj"
        ],
        "abstract": "Given the large number of new musical tracks released each year, automated approaches to plagiarism detection are essential to help us track potential violations of copyright. Most current approaches to plagiarism detection are based on musical similarity measures, which typically ignore the issue of polyphony in music. We present a novel feature space for audio derived from compositional modelling techniques, commonly used in signal separation, that provides a mechanism to account for polyphony without incurring an inordinate amount of computational overhead. We employ this feature representation in conjunction with traditional audio feature representations in a classification framework which uses an ensemble of distance features to characterize pairs of songs as being plagiarized or not. Our experiments on a database of about 3000 musical track pairs show that the new feature space characterization produces significant improvements over standard baselines.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00036",
        "title": "Norm-Based Capacity Control in Neural Networks",
        "authors": [
            "Behnam Neyshabur",
            "Ryota Tomioka",
            "Nathan Srebro"
        ],
        "abstract": "We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00075",
        "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
        "authors": [
            "Kai Sheng Tai",
            "Richard Socher",
            "Christopher D. Manning"
        ],
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00082",
        "title": "Group Event Detection with a Varying Number of Group Members for Video Surveillance",
        "authors": [
            "Weiyao Lin",
            "Ming-Ting Sun",
            "Radha Poovendran",
            "Zhengyou Zhang"
        ],
        "abstract": "This paper presents a novel approach for automatic recognition of group activities for video surveillance applications. We propose to use a group representative to handle the recognition with a varying number of group members, and use an Asynchronous Hidden Markov Model (AHMM) to model the relationship between people. Furthermore, we propose a group activity detection algorithm which can handle both symmetric and asymmetric group activities, and demonstrate that this approach enables the detection of hierarchical interactions between people. Experimental results show the effectiveness of our approach.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00237",
        "title": "Task Allocation in Robotic Swarms: Explicit Communication Based Approaches",
        "authors": [
            "Aryo Jamshidpey",
            "Mohsen Afsharchi"
        ],
        "abstract": "In this paper we study multi robot cooperative task allocation issue in a situation where a swarm of robots is deployed in a confined unknown environment where the number of colored spots which represent tasks and the ratios of them are unknown. The robots should cover this spots as far as possible to do cleaning and sampling actions desirably. It means that they should discover the spots cooperatively and spread proportional to the spots area and avoid from remaining idle. We proposed 4 self-organized distributed methods which are called hybrid methods for coping with this scenario. In two different experiments the performance of the methods is analyzed. We compared them with each other and investigated their scalability and robustness in term of single point of failure.\n    ",
        "submission_date": "2015-03-01T00:00:00",
        "last_modified_date": "2015-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00244",
        "title": "23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and Management",
        "authors": [
            "Nima Bari",
            "Roman Vichr",
            "Kamran Kowsari",
            "Simon Y. Berkovich"
        ],
        "abstract": "The global influence of Big Data is not only growing but seemingly endless. The trend is leaning towards knowledge that is attained easily and quickly from massive pools of Big Data. Today we are living in the technological world that Dr. Usama Fayyad and his distinguished research fellows discussed in the introductory explanations of Knowledge Discovery in Databases (KDD) predicted nearly two decades ago. Indeed, they were precise in their outlook on Big Data analytics. In fact, the continued improvement of the interoperability of machine learning, statistics, database building and querying fused to create this increasingly popular science- Data Mining and Knowledge Discovery. The next generation computational theories are geared towards helping to extract insightful knowledge from even larger volumes of data at higher rates of speed. As the trend increases in popularity, the need for a highly adaptive solution for knowledge discovery will be necessary. In this research paper, we are introducing the investigation and development of 23 bit-questions for a Metaknowledge template for Big Data Processing and clustering purposes. This research aims to demonstrate the construction of this methodology and proves the validity and the beneficial utilization that brings Knowledge Discovery from Big Data.\n    ",
        "submission_date": "2015-03-01T00:00:00",
        "last_modified_date": "2015-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00245",
        "title": "Novel Metaknowledge-based Processing Technique for Multimedia Big Data clustering challenges",
        "authors": [
            "Nima Bari",
            "Roman Vichr",
            "Kamran Kowsari",
            "Simon Y. Berkovich"
        ],
        "abstract": "Past research has challenged us with the task of showing relational patterns between text-based data and then clustering for predictive analysis using Golay Code technique. We focus on a novel approach to extract metaknowledge in multimedia datasets. Our collaboration has been an on-going task of studying the relational patterns between datapoints based on metafeatures extracted from metaknowledge in multimedia datasets. Those selected are significant to suit the mining technique we applied, Golay Code algorithm. In this research paper we summarize findings in optimization of metaknowledge representation for 23-bit representation of structured and unstructured multimedia data in order to\n    ",
        "submission_date": "2015-03-01T00:00:00",
        "last_modified_date": "2015-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00841",
        "title": "Robustly Leveraging Prior Knowledge in Text Classification",
        "authors": [
            "Biao Liu",
            "Minlie Huang"
        ],
        "abstract": "Prior knowledge has been shown very useful to address many natural language processing tasks. Many approaches have been proposed to formalise a variety of knowledge, however, whether the proposed approach is robust or sensitive to the knowledge supplied to the model has rarely been discussed. In this paper, we propose three regularization terms on top of generalized expectation criteria, and conduct extensive experiments to justify the robustness of the proposed methods. Experimental results demonstrate that our proposed methods obtain remarkable improvements and are much more robust than baselines.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01070",
        "title": "Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research",
        "authors": [
            "Atousa Torabi",
            "Christopher Pal",
            "Hugo Larochelle",
            "Aaron Courville"
        ],
        "abstract": "In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video/sentences from 92 DVDs and is growing.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01334",
        "title": "Faster quantum mixing for slowly evolving sequences of Markov chains",
        "authors": [
            "Davide Orsucci",
            "Hans J. Briegel",
            "Vedran Dunjko"
        ],
        "abstract": "Markov chain methods are remarkably successful in computational physics, machine learning, and combinatorial optimization. The cost of such methods often reduces to the mixing time, i.e., the time required to reach the steady state of the Markov chain, which scales as $\\delta^{-1}$, the inverse of the spectral gap. It has long been conjectured that quantum computers offer nearly generic quadratic improvements for mixing problems. However, except in special cases, quantum algorithms achieve a run-time of $\\mathcal{O}(\\sqrt{\\delta^{-1}} \\sqrt{N})$, which introduces a costly dependence on the Markov chain size $N,$ not present in the classical case. Here, we re-address the problem of mixing of Markov chains when these form a slowly evolving sequence. This setting is akin to the simulated annealing setting and is commonly encountered in physics, material sciences and machine learning. We provide a quantum memory-efficient algorithm with a run-time of $\\mathcal{O}(\\sqrt{\\delta^{-1}} \\sqrt[4]{N})$, neglecting logarithmic terms, which is an important improvement for large state spaces. Moreover, our algorithms output quantum encodings of distributions, which has advantages over classical outputs. Finally, we discuss the run-time bounds of mixing algorithms and show that, under certain assumptions, our algorithms are optimal.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2018-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01444",
        "title": "Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and Applications",
        "authors": [
            "Tae-Hyun Oh",
            "Yu-Wing Tai",
            "Jean-Charles Bazin",
            "Hyeongwoo Kim",
            "In So Kweon"
        ],
        "abstract": "Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values, which implicitly encourages the target rank constraint. Our experimental analyses show that, when the number of samples is deficient, our approach leads to a higher success rate than conventional rank minimization, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, motion edge detection, photometric stereo, image alignment and recovery, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01488",
        "title": "Random Serial Dictatorship versus Probabilistic Serial Rule: A Tale of Two Random Mechanisms",
        "authors": [
            "Hadi Hosseini",
            "Kate Larson",
            "Robin Cohen"
        ],
        "abstract": "For assignment problems where agents, specifying ordinal preferences, are allocated indivisible objects, two widely studied randomized mechanisms are the Random Serial Dictatorship (RSD) and Probabilistic Serial Rule (PS). These two mechanisms both have desirable economic and computational properties, but the outcomes they induce can be incomparable in many instances, thus creating challenges in deciding which mechanism to adopt in practice. In this paper we first look at the space of lexicographic preferences and show that, as opposed to the general preference domain, RSD satisfies envyfreeness. Moreover, we show that although under lexicographic preferences PS is strategyproof when the number of objects is less than or equal agents, it is strictly manipulable when there are more objects than agents. In the space of general preferences, we provide empirical results on the (in)comparability of RSD and PS, analyze economic properties, and provide further insights on the applicability of each mechanism in different application domains.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01521",
        "title": "Jointly Learning Multiple Measures of Similarities from Triplet Comparisons",
        "authors": [
            "Liwen Zhang",
            "Subhransu Maji",
            "Ryota Tomioka"
        ],
        "abstract": "Similarity between objects is multi-faceted and it can be easier for human annotators to measure it when the focus is on a specific aspect. We consider the problem of mapping objects into view-specific embeddings where the distance between them is consistent with the similarity comparisons of the form \"from the t-th view, object A is more similar to B than to C\". Our framework jointly learns view-specific embeddings exploiting correlations between views. Experiments on a number of datasets, including one of multi-view crowdsourced comparison on bird images, show the proposed method achieves lower triplet generalization error when compared to both learning embeddings independently for each view and all views pooled into one view. Our method can also be used to learn multiple measures of similarity over input features taking class labels into account and compares favorably to existing approaches for multi-task metric learning on the ISOLET dataset.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01603",
        "title": "External Validity: From Do-Calculus to Transportability Across Populations",
        "authors": [
            "Judea Pearl",
            "Elias Bareinboim"
        ],
        "abstract": "The generalizability of empirical findings to new environments, settings or populations, often called \"external validity,\" is essential in most scientific explorations. This paper treats a particular problem of generalizability, called \"transportability,\" defined as a license to transfer causal effects learned in experimental studies to a new population, in which only observational studies can be conducted. We introduce a formal representation called \"selection diagrams\" for expressing knowledge about differences and commonalities between populations of interest and, using this representation, we reduce questions of transportability to symbolic derivations in the do-calculus. This reduction yields graph-based procedures for deciding, prior to observing any data, whether causal effects in the target population can be inferred from experimental findings in the study population. When the answer is affirmative, the procedures identify what experimental and observational findings need be obtained from the two populations, and how they can be combined to ensure bias-free transport.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01707",
        "title": "Mapping-equivalence and oid-equivalence of single-function object-creating conjunctive queries",
        "authors": [
            "Angela Bonifati",
            "Werner Nutt",
            "Riccardo Torlone",
            "Jan Van den Bussche"
        ],
        "abstract": "Conjunctive database queries have been extended with a mechanism for object creation to capture important applications such as data exchange, data integration, and ontology-based data access. Object creation generates new object identifiers in the result, that do not belong to the set of constants in the source database. The new object identifiers can be also seen as Skolem terms. Hence, object-creating conjunctive queries can also be regarded as restricted second-order tuple-generating dependencies (SO tgds), considered in the data exchange literature.\n",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2016-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01820",
        "title": "Latent Hierarchical Model for Activity Recognition",
        "authors": [
            "Ninghang Hu",
            "Gwenn Englebienne",
            "Zhongyu Lou",
            "Ben Kr\u00f6se"
        ],
        "abstract": "We present a novel hierarchical model for human activity recognition. In contrast to approaches that successively recognize actions and activities, our approach jointly models actions and activities in a unified framework, and their labels are simultaneously predicted. The model is embedded with a latent layer that is able to capture a richer class of contextual information in both state-state and observation-state pairs. Although loops are present in the model, the model has an overall linear-chain structure, where the exact inference is tractable. Therefore, the model is very efficient in both inference and learning. The parameters of the graphical model are learned with a Structured Support Vector Machine (Structured-SVM). A data-driven approach is used to initialize the latent variables; therefore, no manual labeling for the latent states is required. The experimental results from using two benchmark datasets show that our model outperforms the state-of-the-art approach, and our model is computationally more efficient.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01910",
        "title": "Sequential Relevance Maximization with Binary Feedback",
        "authors": [
            "Vijay Kamble",
            "Nadia Fawaz",
            "Fernando Silveira"
        ],
        "abstract": "Motivated by online settings where users can provide explicit feedback about the relevance of products that are sequentially presented to them, we look at the recommendation process as a problem of dynamically optimizing this relevance feedback. Such an algorithm optimizes the fine tradeoff between presenting the products that are most likely to be relevant, and learning the preferences of the user so that more relevant recommendations can be made in the future.\n",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01967",
        "title": "Information entropy as an anthropomorphic concept",
        "authors": [
            "Panteleimon Rodis"
        ],
        "abstract": "According to E.T. Jaynes and E.P. Wigner, entropy is an anthropomorphic concept in the sense that in a physical system correspond many thermodynamic systems. The physical system can be examined from many points of view each time examining different variables and calculating entropy differently. In this paper we discuss how this concept may be applied in information entropy; how Shannon's definition of entropy can fit in Jayne's and Wigner's statement. This is achieved by generalizing Shannon's notion of information entropy and this is the main contribution of the paper. Then we discuss how entropy under these considerations may be used for the comparison of password complexity and as a measure of diversity useful in the analysis of the behavior of genetic algorithms.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02009",
        "title": "Towards an intelligent VNS heuristic for the k-labelled spanning forest problem",
        "authors": [
            "Sergio Consoli",
            "Jos\u00e8 Andr\u00e8s Moreno P\u00e8rez",
            "Nenad Mladenovic"
        ],
        "abstract": "In a currently ongoing project, we investigate a new possibility for solving the k-labelled spanning forest (kLSF) problem by an intelligent Variable Neighbourhood Search (Int-VNS) metaheuristic. In the kLSF problem we are given an undirected input graph G and an integer positive value k, and the aim is to find a spanning forest of G having the minimum number of connected components and the upper bound k on the number of labels to use. The problem is related to the minimum labelling spanning tree (MLST) problem, whose goal is to get the spanning tree of the input graph with the minimum number of labels, and has several applications in the real world, where one aims to ensure connectivity by means of homogeneous connections. The Int-VNS metaheuristic that we propose for the kLSF problem is derived from the promising intelligent VNS strategy recently proposed for the MLST problem, and integrates the basic VNS for the kLSF problem with other complementary approaches from machine learning, statistics and experimental algorithmics, in order to produce high-quality performance and to completely automate the resulting strategy.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02128",
        "title": "Exact Hybrid Covariance Thresholding for Joint Graphical Lasso",
        "authors": [
            "Qingming Tang",
            "Chao Yang",
            "Jian Peng",
            "Jinbo Xu"
        ],
        "abstract": "This paper considers the problem of estimating multiple related Gaussian graphical models from a $p$-dimensional dataset consisting of different classes. Our work is based upon the formulation of this problem as group graphical lasso. This paper proposes a novel hybrid covariance thresholding algorithm that can effectively identify zero entries in the precision matrices and split a large joint graphical lasso problem into small subproblems. Our hybrid covariance thresholding method is superior to existing uniform thresholding methods in that our method can split the precision matrix of each individual class using different partition schemes and thus split group graphical lasso into much smaller subproblems, each of which can be solved very fast. In addition, this paper establishes necessary and sufficient conditions for our hybrid covariance thresholding algorithm. The superior performance of our thresholding method is thoroughly analyzed and illustrated by a few experiments on simulated data and real gene expression data.\n    ",
        "submission_date": "2015-03-07T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02129",
        "title": "Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior",
        "authors": [
            "Qingming Tang",
            "Siqi Sun",
            "Jinbo Xu"
        ],
        "abstract": "Learning the network structure underlying data is an important problem in machine learning. This paper introduces a novel prior to study the inference of scale-free networks, which are widely used to model social and biological networks. The prior not only favors a desirable global node degree distribution, but also takes into consideration the relative strength of all the possible edges adjacent to the same node and the estimated degree of each individual node.\n",
        "submission_date": "2015-03-07T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02364",
        "title": "Neural Responding Machine for Short-Text Conversation",
        "authors": [
            "Lifeng Shang",
            "Zhengdong Lu",
            "Hang Li"
        ],
        "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02510",
        "title": "Compositional Distributional Semantics with Long Short Term Memory",
        "authors": [
            "Phong Le",
            "Willem Zuidema"
        ],
        "abstract": "We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the `memory cell') and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02578",
        "title": "Modeling State-Conditional Observation Distribution using Weighted Stereo Samples for Factorial Speech Processing Models",
        "authors": [
            "Mahdi Khademian",
            "Mohammad Mehdi Homayounpour"
        ],
        "abstract": "This paper investigates the effectiveness of factorial speech processing models in noise-robust automatic speech recognition tasks. For this purpose, the paper proposes an idealistic approach for modeling state-conditional observation distribution of factorial models based on weighted stereo samples. This approach is an extension to previous single pass retraining for ideal model compensation which is extended here to support multiple audio sources. Non-stationary noises can be considered as one of these audio sources with multiple states. Experiments of this paper over the set A of the Aurora 2 dataset show that recognition performance can be improved by this consideration. The improvement is significant in low signal to noise energy conditions, up to 4% absolute word recognition accuracy. In addition to the power of the proposed method in accurate representation of state-conditional observation distribution, it has an important advantage over previous methods by providing the opportunity to independently select feature spaces for both source and corrupted features. This opens a new window for seeking better feature spaces appropriate for noisy speech, independent from clean speech features.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02834",
        "title": "Doubly Robust Policy Evaluation and Optimization",
        "authors": [
            "Miroslav Dud\u00edk",
            "Dumitru Erhan",
            "John Langford",
            "Lihong Li"
        ],
        "abstract": "We study sequential decision making in environments where rewards are only partially observed, but can be modeled as a function of observed contexts and the chosen action by the decision maker. This setting, known as contextual bandits, encompasses a wide variety of applications such as health care, content recommendation and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. In this work, we leverage the strengths and overcome the weaknesses of the two approaches by applying the doubly robust estimation technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust estimation uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice in policy evaluation and optimization.\n    ",
        "submission_date": "2015-03-10T00:00:00",
        "last_modified_date": "2015-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03211",
        "title": "A Multi-Gene Genetic Programming Application for Predicting Students Failure at School",
        "authors": [
            "J.O. Orove",
            "N.E. Osegi",
            "B.O. Eke"
        ],
        "abstract": "Several efforts to predict student failure rate (SFR) at school accurately still remains a core problem area faced by many in the educational sector. The procedure for forecasting SFR are rigid and most often times require data scaling or conversion into binary form such as is the case of the logistic model which may lead to lose of information and effect size attenuation. Also, the high number of factors, incomplete and unbalanced dataset, and black boxing issues as in Artificial Neural Networks and Fuzzy logic systems exposes the need for more efficient tools. Currently the application of Genetic Programming (GP) holds great promises and has produced tremendous positive results in different sectors. In this regard, this study developed GPSFARPS, a software application to provide a robust solution to the prediction of SFR using an evolutionary algorithm known as multi-gene genetic programming. The approach is validated by feeding a testing data set to the evolved GP models. Result obtained from GPSFARPS simulations show its unique ability to evolve a suitable failure rate expression with a fast convergence at 30 generations from a maximum specified generation of 500. The multi-gene system was also able to minimize the evolved model expression and accurately predict student failure rate using a subset of the original expression\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03467",
        "title": "Multigrid with rough coefficients and Multiresolution operator decomposition from Hierarchical Information Games",
        "authors": [
            "Houman Owhadi"
        ],
        "abstract": "We introduce a near-linear complexity (geometric and meshless/algebraic) multigrid/multiresolution method for PDEs with rough ($L^\\infty$) coefficients with rigorous a-priori accuracy and performance estimates. The method is discovered through a decision/game theory formulation of the problems of (1) identifying restriction and interpolation operators (2) recovering a signal from incomplete measurements based on norm constraints on its image under a linear operator (3) gambling on the value of the solution of the PDE based on a hierarchy of nested measurements of its solution or source term. The resulting elementary gambles form a hierarchy of (deterministic) basis functions of $H^1_0(\\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands with respect to the scalar product induced by the energy norm of the PDE (2) enable sparse compression of the solution space in $H^1_0(\\Omega)$ (3) induce an orthogonal multiresolution operator decomposition. The operating diagram of the multigrid method is that of an inverted pyramid in which gamblets are computed locally (by virtue of their exponential decay), hierarchically (from fine to coarse scales) and the PDE is decomposed into a hierarchy of independent linear systems with uniformly bounded condition numbers. The resulting algorithm is parallelizable both in space (via localization) and in bandwith/subscale (subscales can be computed independently from each other). Although the method is deterministic it has a natural Bayesian interpretation under the measure of probability emerging (as a mixed strategy) from the information game formulation and multiresolution approximations form a martingale with respect to the filtration induced by the hierarchy of nested measurements.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2017-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03506",
        "title": "Diverse Landmark Sampling from Determinantal Point Processes for Scalable Manifold Learning",
        "authors": [
            "Christian Wachinger",
            "Polina Golland"
        ],
        "abstract": "High computational costs of manifold learning prohibit its application for large point sets. A common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the Nystr\u00f6m method. The two main challenges that arise are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error, (ii) the graph constructed from sparsely sampled landmarks must approximate the manifold well. We propose the sampling of landmarks from determinantal distributions on non-Euclidean spaces. Since current determinantal sampling algorithms have the same complexity as those for manifold learning, we present an efficient approximation running in linear time. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Bhattacharyya distance improves the embedding of sparsely sampled manifolds. Our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03974",
        "title": "Hyper Temporal Networks",
        "authors": [
            "Carlo Comin",
            "Roberto Posenato",
            "Romeo Rizzi"
        ],
        "abstract": "Simple Temporal Networks (STNs) provide a powerful and general tool for representing conjunctions of maximum delay constraints over ordered pairs of temporal variables. In this paper we introduce Hyper Temporal Networks (HyTNs), a strict generalization of STNs, to overcome the limitation of considering only conjunctions of constraints but maintaining a practical efficiency in the consistency check of the instances. In a Hyper Temporal Network a single temporal hyperarc constraint may be defined as a set of two or more maximum delay constraints which is satisfied when at least one of these delay constraints is satisfied. HyTNs are meant as a light generalization of STNs offering an interesting compromise. On one side, there exist practical pseudo-polynomial time algorithms for checking consistency and computing feasible schedules for HyTNs. On the other side, HyTNs offer a more powerful model accommodating natural constraints that cannot be expressed by STNs like Trigger off exactly delta min before (after) the occurrence of the first (last) event in a set., which are used to represent synchronization events in some process aware information systems/workflow models proposed in the literature.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2017-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04135",
        "title": "Transitive reasoning with imprecise probabilities",
        "authors": [
            "Angelo Gilio",
            "Niki Pfeifer",
            "Giuseppe Sanfilippo"
        ],
        "abstract": "We study probabilistically informative (weak) versions of transitivity, by using suitable definitions of defaults and negated defaults, in the setting of coherence and imprecise probabilities. We represent p-consistent sequences of defaults and/or negated defaults by g-coherent imprecise probability assessments on the respective sequences of conditional events. Finally, we prove the coherent probability propagation rules for Weak Transitivity and the validity of selected inference patterns by proving the p-entailment for the associated knowledge bases.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04193",
        "title": "Non-normal modalities in variants of Linear Logic",
        "authors": [
            "Daniele Porello",
            "Nicolas Troquard"
        ],
        "abstract": "This article presents modal versions of resource-conscious logics. We concentrate on extensions of variants of Linear Logic with one minimal non-normal modality. In earlier work, where we investigated agency in multi-agent systems, we have shown that the results scale up to logics with multiple non-minimal modalities. Here, we start with the language of propositional intuitionistic Linear Logic without the additive disjunction, to which we add a modality. We provide an interpretation of this language on a class of Kripke resource models extended with a neighbourhood function: modal Kripke resource models. We propose a Hilbert-style axiomatization and a Gentzen-style sequent calculus. We show that the proof theories are sound and complete with respect to the class of modal Kripke resource models. We show that the sequent calculus admits cut elimination and that proof-search is in PSPACE. We then show how to extend the results when non-commutative connectives are added to the language. Finally, we put the logical framework to use by instantiating it as logics of agency. In particular, we propose a logic to reason about the resource-sensitive use of artefacts and illustrate it with a variety of examples.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04864",
        "title": "GeomRDF: A Geodata Converter with a Fine-Grained Structured Representation of Geometry in the Web",
        "authors": [
            "Fay\u00e7al Hamdi",
            "Nathalie Abadie",
            "B\u00e9n\u00e9dicte Bucher",
            "Abdelfettah Feliachi"
        ],
        "abstract": "In recent years, with the advent of the web of data, a growing number of national mapping agencies tend to publish their geospatial data as Linked Data. However, differences between traditional GIS data models and Linked Data model can make the publication process more complicated. Besides, it may require, to be done, the setting of several parameters and some expertise in the semantic web technologies. In addition, the use of standards like GeoSPARQL (or ad hoc predicates) is mandatory to perform spatial queries on published geospatial data. In this paper, we present GeomRDF, a tool that helps users to convert spatial data from traditional GIS formats to RDF model easily. It generates geometries represented as GeoSPARQL WKT literal but also as structured geometries that can be exploited by using only the RDF query language, SPARQL. GeomRDF was implemented as a module in the RDF publication platform Datalift. A validation of GeomRDF has been realized against the French administrative units dataset (provided by IGN France).\n    ",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05140",
        "title": "ProtVec: A Continuous Distributed Representation of Biological Sequences",
        "authors": [
            "Ehsaneddin Asgari",
            "Mohammad R.K. Mofrad"
        ],
        "abstract": "We introduce a new representation and feature extraction method for biological sequences. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. In the present paper, we focus on protein-vectors that can be utilized in a wide array of bioinformatics investigations such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. In this method, we adopt artificial neural network approaches and represent a protein sequence with a single dense n-dimensional vector. To evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families, where an average family classification accuracy of 93%+-0.06% is obtained, outperforming existing family classification methods. In addition, we use ProtVec representation to predict disordered proteins from structured proteins. Two databases of disordered sequences are used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0% accuracy. These results indicate that by only providing sequence data for various proteins into this model, accurate information about protein structure can be determined.\n    ",
        "submission_date": "2015-03-17T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05296",
        "title": "Efficient Machine Learning for Big Data: A Review",
        "authors": [
            "O. Y. Al-Jarrah",
            "P. D. Yoo",
            "S Muhaidat",
            "G. K. Karagiannidis",
            "K. Taha"
        ],
        "abstract": "With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years, in fact, as much as 90% of current data were created in the last couple of years,a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven, the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.\n    ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05479",
        "title": "Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm",
        "authors": [
            "Qinqing Zheng",
            "Ryota Tomioka"
        ],
        "abstract": "We consider the problem of recovering a low-rank tensor from its noisy observation. Previous work has shown a recovery guarantee with signal to noise ratio $O(n^{\\lceil K/2 \\rceil /2})$ for recovering a $K$th order rank one tensor of size $n\\times \\cdots \\times n$ by recursive unfolding. In this paper, we first improve this bound to $O(n^{K/4})$ by a much simpler approach, but with a more careful analysis. Then we propose a new norm called the subspace norm, which is based on the Kronecker products of factors obtained by the proposed simple estimator. The imposed Kronecker structure allows us to show a nearly ideal $O(\\sqrt{n}+\\sqrt{H^{K-1}})$ bound, in which the parameter $H$ controls the blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with $H=O(1)$.\n    ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05947",
        "title": "Reduced Basis Decomposition: a Certified and Fast Lossy Data Compression Algorithm",
        "authors": [
            "Yanlai Chen"
        ],
        "abstract": "Dimension reduction is often needed in the area of data mining. The goal of these methods is to map the given high-dimensional data into a low-dimensional space preserving certain properties of the initial data. There are two kinds of techniques for this purpose. The first, projective methods, builds an explicit linear projection from the high-dimensional space to the low-dimensional one. On the other hand, the nonlinear methods utilizes nonlinear and implicit mapping between the two spaces. In both cases, the methods considered in literature have usually relied on computationally very intensive matrix factorizations, frequently the Singular Value Decomposition (SVD). The computational burden of SVD quickly renders these dimension reduction methods infeasible thanks to the ever-increasing sizes of the practical datasets.\n",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06239",
        "title": "Block-Wise MAP Inference for Determinantal Point Processes with Application to Change-Point Detection",
        "authors": [
            "Jinye Zhang",
            "Zhijian Ou"
        ],
        "abstract": "Existing MAP inference algorithms for determinantal point processes (DPPs) need to calculate determinants or conduct eigenvalue decomposition generally at the scale of the full kernel, which presents a great challenge for real-world applications. In this paper, we introduce a class of DPPs, called BwDPPs, that are characterized by an almost block diagonal kernel matrix and thus can allow efficient block-wise MAP inference. Furthermore, BwDPPs are successfully applied to address the difficulty of selecting change-points in the problem of change-point detection (CPD), which results in a new BwDPP-based CPD method, named BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is first created based on existing well-studied metrics. Then, these change-point candidates are treated as DPP items, and DPP-based subset selection is conducted to give the final estimate of the change-points that favours both quality and diversity. The effectiveness of BwDppCpd is demonstrated through extensive experiments on five real-world datasets.\n    ",
        "submission_date": "2015-03-20T00:00:00",
        "last_modified_date": "2015-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06316",
        "title": "Identifying Similar Patients Using Self-Organising Maps: A Case Study on Type-1 Diabetes Self-care Survey Responses",
        "authors": [
            "Santosh Tirunagari",
            "Norman Poh",
            "Guosheng Hu",
            "David Windridge"
        ],
        "abstract": "Diabetes is considered a lifestyle disease and a well managed self-care plays an important role in the treatment. Clinicians often conduct surveys to understand the self-care behaviors in their patients. In this context, we propose to use Self-Organising Maps (SOM) to explore the survey data for assessing the self-care behaviors in Type-1 diabetic patients. Specifically, SOM is used to visualize high dimensional similar patient profiles, which is rarely discussed. Experiments demonstrate that our findings through SOM analysis corresponds well to the expectations of the clinicians. In addition, our findings inspire the experts to improve their understanding of the self-care behaviors for their patients. The principle findings in our study show: 1) patients who take correct dose of insulin, inject insulin at the right time, 2) patients who take correct food portions undertake regular physical activity and 3) patients who eat on time take correct food portions.\n    ",
        "submission_date": "2015-03-21T00:00:00",
        "last_modified_date": "2015-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06350",
        "title": "Boosting Convolutional Features for Robust Object Proposals",
        "authors": [
            "Nikolaos Karianakis",
            "Thomas J. Fuchs",
            "Stefano Soatto"
        ],
        "abstract": "Deep Convolutional Neural Networks (CNNs) have demonstrated excellent performance in image classification, but still show room for improvement in object-detection tasks with many categories, in particular for cluttered scenes and occlusion. Modern detection algorithms like Regions with CNNs (Girshick et al., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions which with high probability represent objects, where in turn CNNs are deployed for classification. Selective Search represents a family of sophisticated algorithms that are engineered with multiple segmentation, appearance and saliency cues, typically coming with a significant run-time overhead. Furthermore, (Hosang et al., 2014) have shown that most methods suffer from low reproducibility due to unstable superpixels, even for slight image perturbations. Although CNNs are subsequently used for classification in top-performing object-detection pipelines, current proposal methods are agnostic to how these models parse objects and their rich learned representations. As a result they may propose regions which may not resemble high-level objects or totally miss some of them. To overcome these drawbacks we propose a boosting approach which directly takes advantage of hierarchical CNN features for detecting regions of interest fast. We demonstrate its performance on ImageNet 2013 detection benchmark and compare it with state-of-the-art methods.\n    ",
        "submission_date": "2015-03-21T00:00:00",
        "last_modified_date": "2015-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06483",
        "title": "Construction of FuzzyFind Dictionary using Golay Coding Transformation for Searching Applications",
        "authors": [
            "Kamran Kowsari",
            "Maryam Yammahi",
            "Nima Bari",
            "Roman Vichr",
            "Faisal Alsaby",
            "Simon Y. Berkovich"
        ],
        "abstract": "Searching through a large volume of data is very critical for companies, scientists, and searching engines applications due to time complexity and memory complexity. In this paper, a new technique of generating FuzzyFind Dictionary for text mining was introduced. We simply mapped the 23 bits of the English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more FuzzyFind Dictionary, and reflecting the presence or absence of particular letters. This representation preserves closeness of word distortions in terms of closeness of the created binary vectors within Hamming distance of 2 deviations. This paper talks about the Golay Coding Transformation Hash Table and how it can be used on a FuzzyFind Dictionary as a new technology for using in searching through big data. This method is introduced by linear time complexity for generating the dictionary and constant time complexity to access the data and update by new data sets, also updating for new data sets is linear time depends on new data points. This technique is based on searching only for letters of English that each segment has 23 bits, and also we have more than 23-bit and also it could work with more segments as reference table.\n    ",
        "submission_date": "2015-03-22T00:00:00",
        "last_modified_date": "2015-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06485",
        "title": "The concept of free will as an infinite metatheoretic recursion",
        "authors": [
            "Hanaan Hashim",
            "R. Srikanth"
        ],
        "abstract": "It is argued that the concept of free will, like the concept of truth in formal languages, requires a separation between an object level and a meta-level for being consistently defined. The Jamesian two-stage model, which deconstructs free will into the causally open \"free\" stage with its closure in the \"will\" stage, is implicitly a move in this direction. However, to avoid the dilemma of determinism, free will additionally requires an infinite regress of causal meta-stages, making free choice a hypertask. We use this model to define free will of the rationalist-compatibilist type. This is shown to provide a natural three-way distinction between quantum indeterminism, freedom and free will, applicable respectively to artificial intelligence (AI), animal agents and human agents. We propose that the causal hierarchy in our model corresponds to a hierarchy of Turing uncomputability. Possible neurobiological and behavioral tests to demonstrate free will experimentally are suggested. Ramifications of the model for physics, evolutionary biology, neuroscience, neuropathological medicine and moral philosophy are briefly outlined.\n    ",
        "submission_date": "2015-03-22T00:00:00",
        "last_modified_date": "2015-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06572",
        "title": "A Machine Learning Approach to Predicting the Smoothed Complexity of Sorting Algorithms",
        "authors": [
            "Bichen Shi",
            "Michel Schellekens",
            "Georgiana Ifrim"
        ],
        "abstract": "Smoothed analysis is a framework for analyzing the complexity of an algorithm, acting as a bridge between average and worst-case behaviour. For example, Quicksort and the Simplex algorithm are widely used in practical applications, despite their heavy worst-case complexity. Smoothed complexity aims to better characterize such algorithms. Existing theoretical bounds for the smoothed complexity of sorting algorithms are still quite weak. Furthermore, empirically computing the smoothed complexity via its original definition is computationally infeasible, even for modest input sizes. In this paper, we focus on accurately predicting the smoothed complexity of sorting algorithms, using machine learning techniques. We propose two regression models that take into account various properties of sorting algorithms and some of the known theoretical results in smoothed analysis to improve prediction quality. We show experimental results for predicting the smoothed complexity of Quicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore filling the gap between known theoretical and empirical results.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06902",
        "title": "A Note on Information-Directed Sampling and Thompson Sampling",
        "authors": [
            "Li Zhou"
        ],
        "abstract": "This note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers.\n    ",
        "submission_date": "2015-03-24T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07159",
        "title": "Modeling context and situations in pervasive computing environments",
        "authors": [
            "Preeti Bhargava",
            "Shivsubramani Krishnamoorthy",
            "Ashok Agrawala"
        ],
        "abstract": "In pervasive computing environments, various entities often have to cooperate and integrate seamlessly in a \\emph{situation} which can, thus, be considered as an amalgamation of the context of several entities interacting and coordinating with each other, and often performing one or more activities. However, none of the existing context models and ontologies address situation modeling. In this paper, we describe the design, structure and implementation of a generic, flexible and extensible context ontology called Rover Context Model Ontology (RoCoMO) for context and situation modeling in pervasive computing systems and environments. We highlight several limitations of the existing context models and ontologies, such as lack of provision for provenance, traceability, quality of context, multiple representation of contextual information, as well as support for security, privacy and interoperability, and explain how we are addressing these limitations in our approach. We also illustrate the applicability and utility of RoCoMO using a practical and extensive case study.\n    ",
        "submission_date": "2015-03-24T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07206",
        "title": "Geometry and Determinism of Optimal Stationary Control in Partially Observable Markov Decision Processes",
        "authors": [
            "Guido Montufar",
            "Keyan Ghazi-Zahedi",
            "Nihat Ay"
        ],
        "abstract": "It is well known that for any finite state Markov decision process (MDP) there is a memoryless deterministic policy that maximizes the expected reward. For partially observable Markov decision processes (POMDPs), optimal memoryless policies are generally stochastic. We study the expected reward optimization problem over the set of memoryless stochastic policies. We formulate this as a constrained linear optimization problem and develop a corresponding geometric framework. We show that any POMDP has an optimal memoryless policy of limited stochasticity, which allows us to reduce the dimensionality of the search space. Experiments demonstrate that this approach enables better and faster convergence of the policy gradient on the evaluated systems.\n    ",
        "submission_date": "2015-03-24T00:00:00",
        "last_modified_date": "2016-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07220",
        "title": "Individual Planning in Agent Populations: Exploiting Anonymity and Frame-Action Hypergraphs",
        "authors": [
            "Ekhlas Sonu",
            "Yingke Chen",
            "Prashant Doshi"
        ],
        "abstract": "Interactive partially observable Markov decision processes (I-POMDP) provide a formal framework for planning for a self-interested agent in multiagent settings. An agent operating in a multiagent environment must deliberate about the actions that other agents may take and the effect these actions have on the environment and the rewards it receives. Traditional I-POMDPs model this dependence on the actions of other agents using joint action and model spaces. Therefore, the solution complexity grows exponentially with the number of agents thereby complicating scalability. In this paper, we model and extend anonymity and context-specific independence -- problem structures often present in agent populations -- for computational gain. We empirically demonstrate the efficiency from exploiting these problem structures by solving a new multiagent problem involving more than 1,000 agents.\n    ",
        "submission_date": "2015-03-24T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07284",
        "title": "A Rule-Based Short Query Intent Identification System",
        "authors": [
            "Arijit De",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "Using SMS (Short Message System), cell phones can be used to query for information about various topics. In an SMS based search system, one of the key problems is to identify a domain (broad topic) associated with the user query; so that a more comprehensive search can be carried out by the domain specific search engine. In this paper we use a rule based approach, to identify the domain, called Short Query Intent Identification System (SQIIS). We construct two different rule-bases using different strategies to suit query intent identification. We evaluate the two rule-bases experimentally.\n    ",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07294",
        "title": "Using Latent Semantic Analysis to Identify Quality in Use (QU) Indicators from User Reviews",
        "authors": [
            "Wendy Tan Wei Syn",
            "Bong Chih How",
            "Issa Atoum"
        ],
        "abstract": "The paper describes a novel approach to categorize users' reviews according to the three Quality in Use (QU) indicators defined in ISO: effectiveness, efficiency and freedom from risk. With the tremendous amount of reviews published each day, there is a need to automatically summarize user reviews to inform us if any of the software able to meet requirement of a company according to the quality requirements. We implemented the method of Latent Semantic Analysis (LSA) and its subspace to predict QU indicators. We build a reduced dimensionality universal semantic space from Information System journals and Amazon reviews. Next, we projected set of indicators' measurement scales into the universal semantic space and represent them as subspace. In the subspace, we can map similar measurement scales to the unseen reviews and predict the QU indicators. Our preliminary study able to obtain the average of F-measure, 0.3627.\n    ",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07469",
        "title": "Properties of Sparse Distributed Representations and their Application to Hierarchical Temporal Memory",
        "authors": [
            "Subutai Ahmad",
            "Jeff Hawkins"
        ],
        "abstract": "Empirical evidence demonstrates that every region of the neocortex represents information using sparse activity patterns. This paper examines Sparse Distributed Representations (SDRs), the primary information representation strategy in Hierarchical Temporal Memory (HTM) systems and the neocortex. We derive a number of properties that are core to scaling, robustness, and generalization. We use the theory to provide practical guidelines and illustrate the power of SDRs as the basis of HTM. Our goal is to help create a unified mathematical and practical framework for SDRs as it relates to cortical function.\n    ",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07717",
        "title": "ASPeRiX, a First Order Forward Chaining Approach for Answer Set Computing",
        "authors": [
            "Claire Lef\u00e8vre",
            "Christopher B\u00e9atrix",
            "Igor St\u00e9phan",
            "Laurent Garcia"
        ],
        "abstract": "The natural way to use Answer Set Programming (ASP) to represent knowledge in Artificial Intelligence or to solve a combinatorial problem is to elaborate a first order logic program with default negation. In a preliminary step this program with variables is translated in an equivalent propositional one by a first tool: the grounder. Then, the propositional program is given to a second tool: the solver. This last one computes (if they exist) one or many answer sets (stable models) of the program, each answer set encoding one solution of the initial problem. Until today, almost all ASP systems apply this two steps computation. In this article, the project ASPeRiX is presented as a first order forward chaining approach for Answer Set Computing. This project was amongst the first to introduce an approach of answer set computing that escapes the preliminary phase of rule instantiation by integrating it in the search process. The methodology applies a forward chaining of first order rules that are grounded on the fly by means of previously produced atoms. Theoretical foundations of the approach are presented, the main algorithms of the ASP solver ASPeRiX are detailed and some experiments and comparisons with existing systems are provided.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08141",
        "title": "Revisable Justified Belief: Preliminary Report",
        "authors": [
            "Alexandru Baltag",
            "Bryan Renne",
            "Sonja Smets"
        ],
        "abstract": "The theory $\\mathsf{CDL}$ of Conditional Doxastic Logic is the single-agent version of Board's multi-agent theory $\\mathsf{BRSIC}$ of conditional belief. $\\mathsf{CDL}$ may be viewed as a version of AGM belief revision theory in which Boolean combinations of revisions are expressible in the language. We introduce a theory $\\mathsf{JCDL}$ of Justified Conditional Doxastic Logic that replaces conditional belief formulas $B^\\psi\\varphi$ by expressions $t{\\,:^{\\psi}}\\varphi$ made up of a term $t$ whose syntactic structure suggests a derivation of the belief $\\varphi$ after revision by $\\psi$. This allows us to think of terms $t$ as reasons justifying a belief in various formulas after a revision takes place. We show that $\\mathsf{JCDL}$-theorems are the exact analogs of $\\mathsf{CDL}$-theorems, and that this result holds the other way around as well. This allows us to think of $\\mathsf{JCDL}$ as a theory of revisable justified belief.\n    ",
        "submission_date": "2015-03-27T00:00:00",
        "last_modified_date": "2015-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08363",
        "title": "Active Model Aggregation via Stochastic Mirror Descent",
        "authors": [
            "Ravi Ganti"
        ],
        "abstract": "We consider the problem of learning convex aggregation of models, that is as good as the best convex aggregation, for the binary classification problem. Working in the stream based active learning setting, where the active learner has to make a decision on-the-fly, if it wants to query for the label of the point currently seen in the stream, we propose a stochastic-mirror descent algorithm, called SMD-AMA, with entropy regularization. We establish an excess risk bounds for the loss of the convex aggregate returned by SMD-AMA to be of the order of $O\\left(\\sqrt{\\frac{\\log(M)}{T^{1-\\mu}}}\\right)$, where $\\mu\\in [0,1)$ is an algorithm dependent parameter, that trades-off the number of labels queried, and excess risk.\n    ",
        "submission_date": "2015-03-28T00:00:00",
        "last_modified_date": "2015-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08381",
        "title": "Towards Easier and Faster Sequence Labeling for Natural Language Processing: A Search-based Probabilistic Online Learning Framework (SAPO)",
        "authors": [
            "Xu Sun",
            "Shuming Ma",
            "Yi Zhang",
            "Xuancheng Ren"
        ],
        "abstract": "There are two major approaches for sequence labeling. One is the probabilistic gradient-based methods such as conditional random fields (CRF) and neural networks (e.g., RNN), which have high accuracy but drawbacks: slow training, and no support of search-based optimization (which is important in many cases). The other is the search-based learning methods such as structured perceptron and margin infused relaxed algorithm (MIRA), which have fast training but also drawbacks: low accuracy, no probabilistic information, and non-convergence in real-world tasks. We propose a novel and \"easy\" solution, a search-based probabilistic online learning method, to address most of those issues. The method is \"easy\", because the optimization algorithm at the training stage is as simple as the decoding algorithm at the test stage. This method searches the output candidates, derives probabilities, and conducts efficient online learning. We show that this method with fast training and theoretical guarantee of convergence, which is easy to implement, can support search-based optimization and obtain top accuracy. Experiments on well-known tasks show that our method has better accuracy than CRF and BiLSTM\\footnote{The SAPO code is released at \\url{",
        "submission_date": "2015-03-29T00:00:00",
        "last_modified_date": "2018-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08992",
        "title": "Hybrid spreading mechanisms and T cell activation shape the dynamics of HIV-1 infection",
        "authors": [
            "Changwang Zhang",
            "Shi Zhou",
            "Elisabetta Groppelli",
            "Pierre Pellegrino",
            "Ian Williams",
            "Persephone Borrow",
            "Benjamin M. Chain",
            "Clare Jolly"
        ],
        "abstract": "HIV-1 can disseminate between susceptible cells by two mechanisms: cell-free infection following fluid-phase diffusion of virions and by highly-efficient direct cell-to-cell transmission at immune cell contacts. The contribution of this hybrid spreading mechanism, which is also a characteristic of some important computer worm outbreaks, to HIV-1 progression in vivo remains unknown. Here we present a new mathematical model that explicitly incorporates the ability of HIV-1 to use hybrid spreading mechanisms and evaluate the consequences for HIV-1 pathogenenesis. The model captures the major phases of the HIV-1 infection course of a cohort of treatment naive patients and also accurately predicts the results of the Short Pulse Anti-Retroviral Therapy at Seroconversion (SPARTAC) trial. Using this model we find that hybrid spreading is critical to seed and establish infection, and that cell-to-cell spread and increased CD4+ T cell activation are important for HIV-1 progression. Notably, the model predicts that cell-to-cell spread becomes increasingly effective as infection progresses and thus may present a considerable treatment barrier. Deriving predictions of various treatments' influence on HIV-1 progression highlights the importance of earlier intervention and suggests that treatments effectively targeting cell-to-cell HIV-1 spread can delay progression to AIDS. This study suggests that hybrid spreading is a fundamental feature of HIV infection, and provides the mathematical framework incorporating this feature with which to evaluate future therapeutic strategies.\n    ",
        "submission_date": "2015-03-31T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.09105",
        "title": "Two Timescale Stochastic Approximation with Controlled Markov noise and Off-policy temporal difference learning",
        "authors": [
            "Prasenjit Karmakar",
            "Shalabh Bhatnagar"
        ],
        "abstract": "We present for the first time an asymptotic convergence analysis of two time-scale stochastic approximation driven by `controlled' Markov noise. In particular, both the faster and slower recursions have non-additive controlled Markov noise components in addition to martingale difference noise. We analyze the asymptotic behavior of our framework by relating it to limiting differential inclusions in both time-scales that are defined in terms of the ergodic occupation measures associated with the controlled Markov processes. Finally, we present a solution to the off-policy convergence problem for temporal difference learning with linear function approximation, using our results.\n    ",
        "submission_date": "2015-03-31T00:00:00",
        "last_modified_date": "2017-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00110",
        "title": "The Libra Toolkit for Probabilistic Models",
        "authors": [
            "Daniel Lowd",
            "Amirmohammad Rooshenas"
        ],
        "abstract": "The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks. Compared to other toolkits, Libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient. It also includes a variety of algorithms for learning graphical models in which inference is potentially intractable, and for performing exact and approximate inference. Libra is released under a 2-clause BSD license to encourage broad use in academia and industry.\n    ",
        "submission_date": "2015-04-01T00:00:00",
        "last_modified_date": "2015-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00522",
        "title": "Monte Carlo Localization in Hand-Drawn Maps",
        "authors": [
            "Bahram Behzadian",
            "Pratik Agarwal",
            "Wolfram Burgard",
            "Gian Diego Tipaldi"
        ],
        "abstract": "Robot localization is a one of the most important problems in robotics. Most of the existing approaches assume that the map of the environment is available beforehand and focus on accurate metrical localization. In this paper, we address the localization problem when the map of the environment is not present beforehand, and the robot relies on a hand-drawn map from a non-expert user. We addressed this problem by expressing the robot pose in the pixel coordinate and simultaneously estimate a local deformation of the hand-drawn map. Experiments show that we are able to localize the robot in the correct room with a robustness up to 80%\n    ",
        "submission_date": "2015-04-02T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01639",
        "title": "Ego-Object Discovery",
        "authors": [
            "Marc Bola\u00f1os",
            "Petia Radeva"
        ],
        "abstract": "Lifelogging devices are spreading faster everyday. This growth can represent great benefits to develop methods for extraction of meaningful information about the user wearing the device and his/her environment. In this paper, we propose a semi-supervised strategy for easily discovering objects relevant to the person wearing a first-person camera. Given an egocentric video/images sequence acquired by the camera, our algorithm uses both the appearance extracted by means of a convolutional neural network and an object refill methodology that allows to discover objects even in case of small amount of object appearance in the collection of images. An SVM filtering strategy is applied to deal with the great part of the False Positive object candidates found by most of the state of the art object detectors. We validate our method on a new egocentric dataset of 4912 daily images acquired by 4 persons as well as on both PASCAL 2012 and MSRC datasets. We obtain for all of them results that largely outperform the state of the art approach. We make public both the EDUB dataset and the algorithm code.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01783",
        "title": "Proximal operators for multi-agent path planning",
        "authors": [
            "Jos\u00e9 Bento",
            "Nate Derbinsky",
            "Charles Mathy",
            "Jonathan S. Yedidia"
        ],
        "abstract": "We address the problem of planning collision-free paths for multiple agents using optimization methods known as proximal algorithms. Recently this approach was explored in Bento et al. 2013, which demonstrated its ease of parallelization and decentralization, the speed with which the algorithms generate good quality solutions, and its ability to incorporate different proximal operators, each ensuring that paths satisfy a desired property. Unfortunately, the operators derived only apply to paths in 2D and require that any intermediate waypoints we might want agents to follow be preassigned to specific agents, limiting their range of applicability. In this paper we resolve these limitations. We introduce new operators to deal with agents moving in arbitrary dimensions that are faster to compute than their 2D predecessors and we introduce landmarks, space-time positions that are automatically assigned to the set of agents under different optimality criteria. Finally, we report the performance of the new operators in several numerical experiments.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02141",
        "title": "Detecting Falls with X-Factor Hidden Markov Models",
        "authors": [
            "Shehroz S. Khan",
            "Michelle E. Karg",
            "Dana Kulic",
            "Jesse Hoey"
        ],
        "abstract": "Identification of falls while performing normal activities of daily living (ADL) is important to ensure personal safety and well-being. However, falling is a short term activity that occurs infrequently. This poses a challenge to traditional classification algorithms, because there may be very little training data for falls (or none at all). This paper proposes an approach for the identification of falls using a wearable device in the absence of training data for falls but with plentiful data for normal ADL. We propose three `X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen falls using \"inflated\" output covariances (observation models). To estimate the inflated covariances, we propose a novel cross validation method to remove \"outliers\" from the normal ADL that serve as proxies for the unseen falls and allow learning the XHMMs using only normal activities. We tested the proposed XHMM approaches on two activity recognition datasets and show high detection rates for falls in the absence of fall-specific training data. We show that the traditional method of choosing a threshold based on maximum of negative of log-likelihood to identify unseen falls is ill-posed for this problem. We also show that supervised classification methods perform poorly when very limited fall data are available during the training phase.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2017-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02150",
        "title": "Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual Entailment in NTCIR RITE Evaluation Tasks",
        "authors": [
            "Wei-Jie Huang",
            "Chao-Lin Liu"
        ],
        "abstract": "We computed linguistic information at the lexical, syntactic, and semantic levels for Recognizing Inference in Text (RITE) tasks for both traditional and simplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing, named-entity recognition, and near synonym recognition were employed, and features like counts of common words, statement lengths, negation words, and antonyms were considered to judge the entailment relationships of two statements, while we explored both heuristics-based functions and machine-learning approaches. The reported systems showed robustness by simultaneously achieving second positions in the binary-classification subtasks for both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted more experiments with the test data of NTCIR-9 RITE, with good results. We also extended our work to search for better configurations of our classifiers and investigated contributions of individual features. This extended work showed interesting results and should encourage further discussion.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02930",
        "title": "Knowledge reduction of dynamic covering decision information systems with varying attribute values",
        "authors": [
            "Mingjie Cai"
        ],
        "abstract": "Knowledge reduction of dynamic covering information systems involves with the time in practical situations. In this paper, we provide incremental approaches to computing the type-1 and type-2 characteristic matrices of dynamic coverings because of varying attribute values. Then we present incremental algorithms of constructing the second and sixth approximations of sets by using characteristic matrices. We employ experimental results to illustrate that the incremental approaches are effective to calculate approximations of sets in dynamic covering information systems. Finally, we perform knowledge reduction of dynamic covering information systems with the incremental approaches.\n    ",
        "submission_date": "2015-04-12T00:00:00",
        "last_modified_date": "2015-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03071",
        "title": "Robobarista: Object Part based Transfer of Manipulation Trajectories from Crowd-sourcing in 3D Pointclouds",
        "authors": [
            "Jaeyong Sung",
            "Seok Hyun Jin",
            "Ashutosh Saxena"
        ],
        "abstract": "There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and design a deep learning model that can handle large noise in the manipulation demonstrations and learns features from three different modalities: point-clouds, language and trajectory. In order to collect a large number of manipulation demonstrations for different objects, we developed a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot can even manipulate objects it has never seen before.\n    ",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03386",
        "title": "Tractable Query Answering and Optimization for Extensions of Weakly-Sticky Datalog+-",
        "authors": [
            "Mostafa Milani",
            "Leopoldo Bertossi"
        ],
        "abstract": "We consider a semantic class, weakly-chase-sticky (WChS), and a syntactic subclass, jointly-weakly-sticky (JWS), of Datalog+- programs. Both extend that of weakly-sticky (WS) programs, which appear in our applications to data quality. For WChS programs we propose a practical, polynomial-time query answering algorithm (QAA). We establish that the two classes are closed under magic-sets rewritings. As a consequence, QAA can be applied to the optimized programs. QAA takes as inputs the program (including the query) and semantic information about the \"finiteness\" of predicate positions. For the syntactic subclasses JWS and WS of WChS, this additional information is computable.\n    ",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03425",
        "title": "Automated Analysis and Prediction of Job Interview Performance",
        "authors": [
            "Iftekhar Naim",
            "M. Iftekhar Tanveer",
            "Daniel Gildea",
            "Mohammed",
            "Hoque"
        ],
        "abstract": "We present a computational framework for automatically quantifying verbal and nonverbal behaviors in the context of job interviews. The proposed framework is trained by analyzing the videos of 138 interview sessions with 69 internship-seeking undergraduates at the Massachusetts Institute of Technology (MIT). Our automated analysis includes facial expressions (e.g., smiles, head gestures, facial tracking points), language (e.g., word counts, topic modeling), and prosodic information (e.g., pitch, intonation, and pauses) of the interviewees. The ground truth labels are derived by taking a weighted average over the ratings of 9 independent judges. Our framework can automatically predict the ratings for interview traits such as excitement, friendliness, and engagement with correlation coefficients of 0.75 or higher, and can quantify the relative importance of prosody, language, and facial expressions. By analyzing the relative feature weights learned by the regression models, our framework recommends to speak more fluently, use less filler words, speak as \"we\" (vs. \"I\"), use more unique words, and smile more. We also find that the students who were rated highly while answering the first interview question were also rated highly overall (i.e., first impression matters). Finally, our MIT Interview dataset will be made available to other researchers to further validate and expand our findings.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03659",
        "title": "Temporal ordering of clinical events",
        "authors": [
            "Azad Dehghan"
        ],
        "abstract": "This report describes a minimalistic set of methods engineered to anchor clinical events onto a temporal space. Specifically, we describe methods to extract clinical events (e.g., Problems, Treatments and Tests), temporal expressions (i.e., time, date, duration, and frequency), and temporal links (e.g., Before, After, Overlap) between events and temporal entities. These methods are developed and validated using high quality datasets.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04716",
        "title": "Gap Analysis of Natural Language Processing Systems with respect to Linguistic Modality",
        "authors": [
            "Vishal Shukla"
        ],
        "abstract": "Modality is one of the important components of grammar in linguistics. It lets speaker to express attitude towards, or give assessment or potentiality of state of affairs. It implies different senses and thus has different perceptions as per the context. This paper presents an account showing the gap in the functionality of the current state of art Natural Language Processing (NLP) systems. The contextual nature of linguistic modality is studied. In this paper, the works and logical approaches employed by Natural Language Processing systems dealing with modality are reviewed. It sees human cognition and intelligence as multi-layered approach that can be implemented by intelligent systems for learning. Lastly, current flow of research going on within this field is talked providing futurology.\n    ",
        "submission_date": "2015-04-18T00:00:00",
        "last_modified_date": "2015-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04850",
        "title": "Exploring Bayesian Models for Multi-level Clustering of Hierarchically Grouped Sequential Data",
        "authors": [
            "Adway Mitra"
        ],
        "abstract": "A wide range of Bayesian models have been proposed for data that is divided hierarchically into groups. These models aim to cluster the data at different levels of grouping, by assigning a mixture component to each datapoint, and a mixture distribution to each group. Multi-level clustering is facilitated by the sharing of these components and distributions by the groups. In this paper, we introduce the concept of Degree of Sharing (DoS) for the mixture components and distributions, with an aim to analyze and classify various existing models. Next we introduce a generalized hierarchical Bayesian model, of which the existing models can be shown to be special cases. Unlike most of these models, our model takes into account the sequential nature of the data, and various other temporal structures at different levels while assigning mixture components and distributions. We show one specialization of this model aimed at hierarchical segmentation of news transcripts, and present a Gibbs Sampling based inference algorithm for it. We also show experimentally that the proposed model outperforms existing models for the same task.\n    ",
        "submission_date": "2015-04-19T00:00:00",
        "last_modified_date": "2015-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04914",
        "title": "Negatively Correlated Search",
        "authors": [
            "Ke Tang",
            "Peng Yang",
            "Xin Yao"
        ],
        "abstract": "Evolutionary Algorithms (EAs) have been shown to be powerful tools for complex optimization problems, which are ubiquitous in both communication and big data analytics. This paper presents a new EA, namely Negatively Correlated Search (NCS), which maintains multiple individual search processes in parallel and models the search behaviors of individual search processes as probability distributions. NCS explicitly promotes negatively correlated search behaviors by encouraging differences among the probability distributions (search behaviors). By this means, individual search processes share information and cooperate with each other to search diverse regions of a search space, which makes NCS a promising method for non-convex optimization. The cooperation scheme of NCS could also be regarded as a novel diversity preservation scheme that, different from other existing schemes, directly promotes diversity at the level of search behaviors rather than merely trying to maintain diversity among candidate solutions. Empirical studies showed that NCS is competitive to well-established search methods in the sense that NCS achieved the best overall performance on 20 multimodal (non-convex) continuous optimization problems. The advantages of NCS over state-of-the-art approaches are also demonstrated with a case study on the synthesis of unequally spaced linear antenna arrays.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05122",
        "title": "Optimal Nudging: Solving Average-Reward Semi-Markov Decision Processes as a Minimal Sequence of Cumulative Tasks",
        "authors": [
            "Reinaldo Uribe Muriel",
            "Fernando Lozando",
            "Charles Anderson"
        ],
        "abstract": "This paper describes a novel method to solve average-reward semi-Markov decision processes, by reducing them to a minimal sequence of cumulative reward problems. The usual solution methods for this type of problems update the gain (optimal average reward) immediately after observing the result of taking an action. The alternative introduced, optimal nudging, relies instead on setting the gain to some fixed value, which transitorily makes the problem a cumulative-reward task, solving it by any standard reinforcement learning method, and only then updating the gain in a way that minimizes uncertainty in a minmax sense. The rule for optimal gain update is derived by exploiting the geometric features of the w-l space, a simple mapping of the space of policies. The total number of cumulative reward tasks that need to be solved is shown to be small. Some experiments are presented to explore the features of the algorithm and to compare its performance with other approaches.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05457",
        "title": "Graphlet-based lazy associative graph classification",
        "authors": [
            "Yury Kashnitsky",
            "Sergei O. Kuznetsov"
        ],
        "abstract": "The paper addresses the graph classification problem and introduces a modification of the lazy associative classification method to efficiently handle intersections of graphs. Graph intersections are approximated with all common subgraphs up to a fixed size similarly to what is done with graphlet kernels. We explain the idea of the algorithm with a toy example and describe our experiments with a predictive toxicology dataset.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05469",
        "title": "Visual analytics in FCA-based clustering",
        "authors": [
            "Yury Kashnitsky"
        ],
        "abstract": "Visual analytics is a subdomain of data analysis which combines both human and machine analytical abilities and is applied mostly in decision-making and data mining tasks. Triclustering, based on Formal Concept Analysis (FCA), was developed to detect groups of objects with similar properties under similar conditions. It is used in Social Network Analysis (SNA) and is a basis for certain types of recommender systems. The problem of triclustering algorithms is that they do not always produce meaningful clusters. This article describes a specific triclustering algorithm and a prototype of a visual analytics platform for working with obtained clusters. This tool is designed as a testing frameworkis and is intended to help an analyst to grasp the results of triclustering and recommender algorithms, and to make decisions on meaningfulness of certain triclusters and recommendations.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05603",
        "title": "Formalizing Preference Utilitarianism in Physical World Models",
        "authors": [
            "Caspar Oesterheld"
        ],
        "abstract": "Most ethical work is done at a low level of formality. This makes practical moral questions inaccessible to formal and natural sciences and can lead to misunderstandings in ethical discussion. In this paper, we use Bayesian inference to introduce a formalization of preference utilitarianism in physical world models, specifically cellular automata. Even though our formalization is not immediately applicable, it is a first step in providing ethics and ultimately the question of how to \"make the world better\" with a formal basis.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05811",
        "title": "Learning of Behavior Trees for Autonomous Agents",
        "authors": [
            "Michele Colledanchise",
            "Ramviyas Parasuraman",
            "Petter \u00d6gren"
        ],
        "abstract": "Definition of an accurate system model for Automated Planner (AP) is often impractical, especially for real-world problems. Conversely, off-the-shelf planners fail to scale up and are domain dependent. These drawbacks are inherited from conventional transition systems such as Finite State Machines (FSMs) that describes the action-plan execution generated by the AP. On the other hand, Behavior Trees (BTs) represent a valid alternative to FSMs presenting many advantages in terms of modularity, reactiveness, scalability and domain-independence. In this paper, we propose a model-free AP framework using Genetic Programming (GP) to derive an optimal BT for an autonomous agent to achieve a given goal in unknown (but fully observable) environments. We illustrate the proposed framework using experiments conducted with an open source benchmark Mario AI for automated generation of BTs that can play the game character Mario to complete a certain level at various levels of difficulty to include enemies and obstacles.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05932",
        "title": "Allocating Indivisible Items in Categorized Domains",
        "authors": [
            "Erika Mackin",
            "Lirong Xia"
        ],
        "abstract": "We formulate a general class of allocation problems called categorized domain allocation problems (CDAPs), where indivisible items from multiple categories are allocated to agents without monetary transfer and each agent gets at least one item per category.\n",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05996",
        "title": "Non-Adaptive Policies for 20 Questions Target Localization",
        "authors": [
            "Ehsan Variani",
            "Kamel Lahouel",
            "Avner Bar-Hen",
            "Bruno Jedynak"
        ],
        "abstract": "The problem of target localization with noise is addressed. The target is a sample from a continuous random variable with known distribution and the goal is to locate it with minimum mean squared error distortion. The localization scheme or policy proceeds by queries, or questions, weather or not the target belongs to some subset as it is addressed in the 20-question framework. These subsets are not constrained to be intervals and the answers to the queries are noisy. While this situation is well studied for adaptive querying, this paper is focused on the non adaptive querying policies based on dyadic questions. The asymptotic minimum achievable distortion under such policies is derived. Furthermore, a policy named the Aurelian1 is exhibited which achieves asymptotically this distortion.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06058",
        "title": "Security Games with Information Leakage: Modeling and Computation",
        "authors": [
            "Haifeng Xu",
            "Albert X. Jiang",
            "Arunesh Sinha",
            "Zinovi Rabinovich",
            "Shaddin Dughmi",
            "Milind Tambe"
        ],
        "abstract": "Most models of Stackelberg security games assume that the attacker only knows the defender's mixed strategy, but is not able to observe (even partially) the instantiated pure strategy. Such partial observation of the deployed pure strategy -- an issue we refer to as information leakage -- is a significant concern in practical applications. While previous research on patrolling games has considered the attacker's real-time surveillance, our settings, therefore models and techniques, are fundamentally different. More specifically, after describing the information leakage model, we start with an LP formulation to compute the defender's optimal strategy in the presence of leakage. Perhaps surprisingly, we show that a key subproblem to solve this LP (more precisely, the defender oracle) is NP-hard even for the simplest of security game models. We then approach the problem from three possible directions: efficient algorithms for restricted cases, approximation algorithms, and heuristic algorithms for sampling that improves upon the status quo. Our experiments confirm the necessity of handling information leakage and the advantage of our algorithms.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06078",
        "title": "x.ent: R Package for Entities and Relations Extraction based on Unsupervised Learning and Document Structure",
        "authors": [
            "Nicolas Turenne",
            "Tien Phan"
        ],
        "abstract": "Relation extraction with accurate precision is still a challenge when processing full text databases. We propose an approach based on cooccurrence analysis in each document for which we used document organization to improve accuracy of relation extraction. This approach is implemented in a R package called \\emph{",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06158",
        "title": "From End-User's Requirements to Web Services Retrieval: A Semantic and Intention-Driven Approach",
        "authors": [
            "Isabelle Mirbel",
            "Pierre Crescenzo"
        ],
        "abstract": "In this paper, we present SATIS, a framework to derive Web Service specifications from end-user's requirements in order to opera-tionalise business processes in the context of a specific application domain. The aim of SATIS is to provide to neuroscientists, which are not familiar with computer science, a complete solution to easily find a set of Web Services to implement an image processing pipeline. More precisely, our framework offers the capability to capture high-level end-user's requirements in an iterative and incremental way and to turn them into queries to retrieve Web Services description. The whole framework relies on reusable and combinable elements which can be shared out by a community of users sharing some interest or problems for a given topic. In our approach, we adopt Web semantic languages and models as a unified framework to deal with end-user's requirements and Web Service descriptions in order to take advantage of their reasoning and traceability capabilities.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06341",
        "title": "Strategic Teaching and Learning in Games",
        "authors": [
            "Burkhard C. Schipper"
        ],
        "abstract": "It is known that there are uncoupled learning heuristics leading to Nash equilibrium in all finite games. Why should players use such learning heuristics and where could they come from? We show that there is no uncoupled learning heuristic leading to Nash equilibrium in all finite games that a player has an incentive to adopt, that would be evolutionary stable or that could \"learn itself\". Rather, a player has an incentive to strategically teach such a learning opponent in order secure at least the Stackelberg leader payoff. The impossibility result remains intact when restricted to the classes of generic games, two-player games, potential games, games with strategic complements or 2x2 games, in which learning is known to be \"nice\". More generally, it also applies to uncoupled learning heuristics leading to correlated equilibria, rationalizable outcomes, iterated admissible outcomes, or minimal curb sets. A possibility result restricted to \"strategically trivial\" games fails if some generic games outside this class are considered as well.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06665",
        "title": "Using Syntax-Based Machine Translation to Parse English into Abstract Meaning Representation",
        "authors": [
            "Michael Pust",
            "Ulf Hermjakob",
            "Kevin Knight",
            "Daniel Marcu",
            "Jonathan May"
        ],
        "abstract": "We present a parser for Abstract Meaning Representation (AMR). We treat English-to-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser improves upon state-of-the-art results by 7 Smatch points.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06825",
        "title": "Comparison of Training Methods for Deep Neural Networks",
        "authors": [
            "Patrick O. Glauner"
        ],
        "abstract": "This report describes the difficulties of training neural networks and in particular deep neural networks. It then provides a literature review of training methods for deep neural networks, with a focus on pre-training. It focuses on Deep Belief Networks composed of Restricted Boltzmann Machines and Stacked Autoencoders and provides an outreach on further and alternative approaches. It also includes related practical recommendations from the literature on training them. In the second part, initial experiments using some of the covered methods are performed on two databases. In particular, experiments are performed on the MNIST hand-written digit dataset and on facial emotion data from a Kaggle competition. The results are discussed in the context of results reported in other research papers. An error rate lower than the best contribution to the Kaggle competition is achieved using an optimized Stacked Autoencoder.\n    ",
        "submission_date": "2015-04-26T00:00:00",
        "last_modified_date": "2015-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07107",
        "title": "Fast Sampling for Bayesian Max-Margin Models",
        "authors": [
            "Wenbo Hu",
            "Jun Zhu",
            "Bo Zhang"
        ],
        "abstract": "Bayesian max-margin models have shown superiority in various practical applications, such as text categorization, collaborative prediction, social network link prediction and crowdsourcing, and they conjoin the flexibility of Bayesian modeling and predictive strengths of max-margin learning. However, Monte Carlo sampling for these models still remains challenging, especially for applications that involve large-scale datasets. In this paper, we present the stochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to implement and computationally efficient. We show the approximate detailed balance property of subgradient HMC which reveals a natural and validated generalization of the ordinary HMC. Furthermore, we investigate the variants that use stochastic subsampling and thermostats for better scalability and mixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we efficiently solve the posterior inference task of various Bayesian max-margin models and extensive experimental results demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07313",
        "title": "Private Disclosure of Information in Health Tele-monitoring",
        "authors": [
            "Daniel Aranki",
            "Ruzena Bajcsy"
        ],
        "abstract": "We present a novel framework, called Private Disclosure of Information (PDI), which is aimed to prevent an adversary from inferring certain sensitive information about subjects using the data that they disclosed during communication with an intended recipient. We show cases where it is possible to achieve perfect privacy regardless of the adversary's auxiliary knowledge while preserving full utility of the information to the intended recipient and provide sufficient conditions for such cases. We also demonstrate the applicability of PDI on a real-world data set that simulates a health tele-monitoring scenario.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07324",
        "title": "Reader-Aware Multi-Document Summarization via Sparse Coding",
        "authors": [
            "Piji Li",
            "Lidong Bing",
            "Wai Lam",
            "Hang Li",
            "Yi Liao"
        ],
        "abstract": "We propose a new MDS paradigm called reader-aware multi-document summarization (RA-MDS). Specifically, a set of reader comments associated with the news reports are also collected. The generated summaries from the reports for the event should be salient according to not only the reports but also the reader comments. To tackle this RA-MDS problem, we propose a sparse-coding-based method that is able to calculate the salience of the text units by jointly considering news reports and reader comments. Another reader-aware characteristic of our framework is to improve linguistic quality via entity rewriting. The rewriting consideration is jointly assessed together with other summarization requirements under a unified optimization model. To support the generation of compressive summaries via optimization, we explore a finer syntactic unit, namely, noun/verb phrase. In this work, we also generate a data set for conducting RA-MDS. Extensive experiments on this data set and some classical data sets demonstrate the effectiveness of our proposed approach.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08050",
        "title": "Detecting Concept-level Emotion Cause in Microblogging",
        "authors": [
            "Shuangyong Song",
            "Yao Meng"
        ],
        "abstract": "In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead of the mere word-level models, to discover causes of microblogging users' diversified emotions on specific hot event. A modified topic-supervised biterm topic model is utilized in CECM to detect emotion topics' in event-related tweets, and then context-sensitive topical PageRank is utilized to detect meaningful multiword expressions as emotion causes. Experimental results on a dataset from Sina Weibo, one of the largest microblogging websites in China, show CECM can better detect emotion causes than baseline methods.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08319",
        "title": "A weighted U statistic for association analysis considering genetic heterogeneity",
        "authors": [
            "Changshuai Wei",
            "Robert C. Elston",
            "Qing Lu"
        ],
        "abstract": "Converging evidence suggests that common complex diseases with the same or similar clinical manifestations could have different underlying genetic etiologies. While current research interests have shifted toward uncovering rare variants and structural variations predisposing to human diseases, the impact of heterogeneity in genetic studies of complex diseases has been largely overlooked. Most of the existing statistical methods assume the disease under investigation has a homogeneous genetic effect and could, therefore, have low power if the disease undergoes heterogeneous pathophysiological and etiological processes. In this paper, we propose a heterogeneity weighted U (HWU) method for association analyses considering genetic heterogeneity. HWU can be applied to various types of phenotypes (e.g., binary and continuous) and is computationally effcient for high- dimensional genetic data. Through simulations, we showed the advantage of HWU when the underlying genetic etiology of a disease was heterogeneous, as well as the robustness of HWU against different model assumptions (e.g., phenotype distributions). Using HWU, we conducted a genome-wide analysis of nicotine dependence from the Study of Addiction: Genetics and Environments (SAGE) dataset. The genome-wide analysis of nearly one million genetic markers took 7 hours, identifying heterogeneous effects of two new genes (i.e., CYP3A5 and IKBKB) on nicotine dependence.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00138",
        "title": "Compositional Distributional Semantics with Compact Closed Categories and Frobenius Algebras",
        "authors": [
            "Dimitri Kartsaklis"
        ],
        "abstract": "This thesis contributes to ongoing research related to the categorical compositional model for natural language of Coecke, Sadrzadeh and Clark in three ways: Firstly, I propose a concrete instantiation of the abstract framework based on Frobenius algebras (joint work with Sadrzadeh). The theory improves shortcomings of previous proposals, extends the coverage of the language, and is supported by experimental work that improves existing results. The proposed framework describes a new class of compositional models that find intuitive interpretations for a number of linguistic phenomena. Secondly, I propose and evaluate in practice a new compositional methodology which explicitly deals with the different levels of lexical ambiguity (joint work with Pulman). A concrete algorithm is presented, based on the separation of vector disambiguation from composition in an explicit prior step. Extensive experimental work shows that the proposed methodology indeed results in more accurate composite representations for the framework of Coecke et al. in particular and every other class of compositional models in general. As a last contribution, I formalize the explicit treatment of lexical ambiguity in the context of the categorical framework by resorting to categorical quantum mechanics (joint work with Coecke). In the proposed extension, the concept of a distributional vector is replaced with that of a density matrix, which compactly represents a probability distribution over the potential different meanings of the specific word. Composition takes the form of quantum measurements, leading to interesting analogies between quantum physics and linguistics.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00277",
        "title": "Grounded Discovery of Coordinate Term Relationships between Software Entities",
        "authors": [
            "Dana Movshovitz-Attias",
            "William W. Cohen"
        ],
        "abstract": "We present an approach for the detection of coordinate-term relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the \"grounded\" entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00401",
        "title": "Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to LIFT, ROC & BIRD",
        "authors": [
            "David M. W. Powers"
        ],
        "abstract": "Evaluation often aims to reduce the correctness or error characteristics of a system down to a single number, but that always involves trade-offs. Another way of dealing with this is to quote two numbers, such as Recall and Precision, or Sensitivity and Specificity. But it can also be useful to see more than this, and a graphical approach can explore sensitivity to cost, prevalence, bias, noise, parameters and hyper-parameters.\n",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2020-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00755",
        "title": "Towards the Ontology Web Search Engine",
        "authors": [
            "Olegs Verhodubs"
        ],
        "abstract": "The project of the Ontology Web Search Engine is presented in this paper. The main purpose of this paper is to develop such a project that can be easily implemented. Ontology Web Search Engine is software to look for and index ontologies in the Web. OWL (Web Ontology Languages) ontologies are meant, and they are necessary for the functioning of the SWES (Semantic Web Expert System). SWES is an expert system that will use found ontologies from the Web, generating rules from them, and will supplement its knowledge base with these generated rules. It is expected that the SWES will serve as a universal expert system for the average user.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00828",
        "title": "Dynamic Consistency of Conditional Simple Temporal Networks via Mean Payoff Games: a Singly-Exponential Time DC-Checking",
        "authors": [
            "Carlo Comin",
            "Romeo Rizzi"
        ],
        "abstract": "Conditional Simple Temporal Network (CSTN) is a constraint-based graph-formalism for conditional temporal planning. It offers a more flexible formalism than the equivalent CSTP model of Tsamardinos, Vidal and Pollack, from which it was derived mainly as a sound formalization. Three notions of consistency arise for CSTNs and CSTPs: weak, strong, and dynamic. Dynamic consistency is the most interesting notion, but it is also the most challenging and it was conjectured to be hard to assess. Tsamardinos, Vidal and Pollack gave a doubly-exponential time algorithm for deciding whether a CSTN is dynamically-consistent and to produce, in the positive case, a dynamic execution strategy of exponential size. In the present work we offer a proof that deciding whether a CSTN is dynamically-consistent is coNP-hard and provide the first singly-exponential time algorithm for this problem, also producing a dynamic execution strategy whenever the input CSTN is dynamically-consistent. The algorithm is based on a novel connection with Mean Payoff Games, a family of two-player combinatorial games on graphs well known for having applications in model-checking and formal verification. The presentation of such connection is mediated by the Hyper Temporal Network model, a tractable generalization of Simple Temporal Networks whose consistency checking is equivalent to determining Mean Payoff Games. In order to analyze the algorithm we introduce a refined notion of dynamic-consistency, named \\epsilon-dynamic-consistency, and present a sharp lower bounding analysis on the critical value of the reaction time \\hat{\\varepsilon} where the CSTN transits from being, to not being, dynamically-consistent. The proof technique introduced in this analysis of \\hat{\\varepsilon} is applicable more in general when dealing with linear difference constraints which include strict inequalities.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00863",
        "title": "A Feature-based Classification Technique for Answering Multi-choice World History Questions",
        "authors": [
            "Shuangyong Song",
            "Yao Meng",
            "Zhongguang Zheng",
            "Jun Sun"
        ],
        "abstract": "Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01071",
        "title": "A Spatiotemporal Context Definition for Service Adaptation Prediction in a Pervasive Computing Environment",
        "authors": [
            "Darine Ameyed",
            "Moeiz Miraoui",
            "Chakib Tadj"
        ],
        "abstract": "Pervasive systems refers to context-aware systems that can sense their context, and adapt their behavior accordingly to provide adaptable services. Proactive adaptation of such systems allows changing the service and the context based on prediction. However, the definition of the context is still vague and not suitable to prediction. In this paper we discuss and classify previous definitions of context. Then, we present a new definition which allows pervasive systems to understand and predict their contexts. We analyze the essential lines that fall within the context definition, and propose some scenarios to make it clear our approach.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01121",
        "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
        "authors": [
            "Mateusz Malinowski",
            "Marcus Rohrbach",
            "Mario Fritz"
        ],
        "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01179",
        "title": "A Generalized Similarity U Test for Multivariate Analysis of Sequencing Data",
        "authors": [
            "Changshuai Wei",
            "Qing Lu"
        ],
        "abstract": "Sequencing-based studies are emerging as a major tool for genetic association studies of complex diseases. These studies pose great challenges to the traditional statistical methods (e.g., single-locus analyses based on regression methods) because of the high-dimensionality of data and the low frequency of genetic variants. In addition, there is a great interest in biology and epidemiology to identify genetic risk factors contributed to multiple disease phenotypes. The multiple phenotypes can often follow different distributions, which violates the assumptions of most current methods. In this paper, we propose a generalized similarity U test, referred to as GSU. GSU is a similarity-based test and can handle high-dimensional genotypes and phenotypes. We studied the theoretical properties of GSU, and provided the efficient p-value calculation for association test as well as the sample size and power calculation for the study design. Through simulation, we found that GSU had advantages over existing methods in terms of power and robustness to phenotype distributions. Finally, we used GSU to perform a multivariate analysis of sequencing data in the Dallas Heart Study and identified a joint association of 4 genes with 5 metabolic related phenotypes.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01204",
        "title": "A Weighted U Statistic for Genetic Association Analyses of Sequencing Data",
        "authors": [
            "Changshuai Wei",
            "Ming Li",
            "Zihuai He",
            "Olga Vsevolozhskaya",
            "Daniel J. Schaid",
            "Qing Lu"
        ],
        "abstract": "With advancements in next generation sequencing technology, a massive amount of sequencing data are generated, offering a great opportunity to comprehensively investigate the role of rare variants in the genetic etiology of complex diseases. Nevertheless, this poses a great challenge for the statistical analysis of high-dimensional sequencing data. The association analyses based on traditional statistical methods suffer substantial power loss because of the low frequency of genetic variants and the extremely high dimensionality of the data. We developed a weighted U statistic, referred to as WU-seq, for the high-dimensional association analysis of sequencing data. Based on a non-parametric U statistic, WU-SEQ makes no assumption of the underlying disease model and phenotype distribution, and can be applied to a variety of phenotypes. Through simulation studies and an empirical study, we showed that WU-SEQ outperformed a commonly used SKAT method when the underlying assumptions were violated (e.g., the phenotype followed a heavy-tailed distribution). Even when the assumptions were satisfied, WU-SEQ still attained comparable performance to SKAT. Finally, we applied WU-seq to sequencing data from the Dallas Heart Study (DHS), and detected an association between ANGPTL 4 and very low density lipoprotein cholesterol.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01206",
        "title": "Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint Association among Low Marginal Effect loci",
        "authors": [
            "Changshuai Wei",
            "Daniel J. Schaid",
            "Qing Lu"
        ],
        "abstract": "Common complex diseases are likely influenced by the interplay of hundreds, or even thousands, of genetic variants. Converging evidence shows that genetic variants with low marginal effects (LME) play an important role in disease development. Despite their potential significance, discovering LME genetic variants and assessing their joint association on high dimensional data (e.g., genome wide association studies) remain a great challenge. To facilitate joint association analysis among a large ensemble of LME genetic variants, we proposed a computationally efficient and powerful approach, which we call Trees Assembling Mann whitney (TAMW). Through simulation studies and an empirical data application, we found that TAMW outperformed multifactor dimensionality reduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW) when the underlying complex disease involves multiple LME loci and their interactions. For instance, in a simulation with 20 interacting LME loci, TAMW attained a higher power (power=0.931) than both MDR (power=0.599) and LRMW (power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci, TAMW also identified a stronger joint association with CD than those detected by MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conduct a genome wide analysis. The analysis of 459K single nucleotide polymorphisms was completed in 40 hours using parallel computing, and revealed a joint association predisposing to CD (p-value=2.763e-19). Further analysis of the newly discovered association suggested that 13 genes, such as ATG16L1 and LACC1, may play an important role in CD pathophysiological and etiological processes.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01419",
        "title": "Fast Differentially Private Matrix Factorization",
        "authors": [
            "Ziqi Liu",
            "Yu-Xiang Wang",
            "Alexander J. Smola"
        ],
        "abstract": "Differentially private collaborative filtering is a challenging task, both in terms of accuracy and speed. We present a simple algorithm that is provably differentially private, while offering good performance, using a novel connection of differential privacy to Bayesian posterior sampling via Stochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm lends itself to efficient implementation. By careful systems design and by exploiting the power law behavior of the data to maximize CPU cache bandwidth we are able to generate 1024 dimensional models at a rate of 8.5 million recommendations per second on a single PC.\n    ",
        "submission_date": "2015-05-06T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01539",
        "title": "Graphical Potential Games",
        "authors": [
            "Luis E. Ortiz"
        ],
        "abstract": "Potential games, originally introduced in the early 1990's by Lloyd Shapley, the 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are a very important class of models in game theory. They have special properties such as the existence of Nash equilibria in pure strategies. This note introduces graphical versions of potential games. Special cases of graphical potential games have already found applicability in many areas of science and engineering beyond economics, including artificial intelligence, computer vision, and machine learning. They have been effectively applied to the study and solution of important real-world problems such as routing and congestion in networks, distributed resource allocation (e.g., public goods), and relaxation-labeling for image segmentation. Implicit use of graphical potential games goes back at least 40 years. Several classes of games considered standard in the literature, including coordination games, local interaction games, lattice games, congestion games, and party-affiliation games, are instances of graphical potential games. This note provides several characterizations of graphical potential games by leveraging well-known results from the literature on probabilistic graphical models. A major contribution of the work presented here that particularly distinguishes it from previous work is establishing that the convergence of certain type of game-playing rules implies that the agents/players must be embedded in some graphical potential game.\n    ",
        "submission_date": "2015-05-06T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01620",
        "title": "Structure Formation in Large Theories",
        "authors": [
            "Serge Autexier",
            "Dieter Hutter"
        ],
        "abstract": "Structuring theories is one of the main approaches to reduce the combinatorial explosion associated with reasoning and exploring large theories. In the past we developed the notion of development graphs as a means to represent and maintain structured theories. In this paper we present a methodology and a resulting implementation to reveal the hidden structure of flat theories by transforming them into detailed development graphs. We review our approach using plain TSTP-representations of MIZAR articles obtaining more structured and also more concise theories.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01629",
        "title": "LeoPARD --- A Generic Platform for the Implementation of Higher-Order Reasoners",
        "authors": [
            "Max Wisniewski",
            "Alexander Steen",
            "Christoph Benzm\u00fcller"
        ],
        "abstract": "LeoPARD supports the implementation of knowledge representation and reasoning tools for higher-order logic(s). It combines a sophisticated data structure layer (polymorphically typed {\\lambda}-calculus with nameless spine notation, explicit substitutions, and perfect term sharing) with an ambitious multi-agent blackboard architecture (supporting prover parallelism at the term, clause, and search level). Further features of LeoPARD include a parser for all TPTP dialects, a command line interpreter, and generic means for the integration of external reasoners.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01757",
        "title": "Contextual Analysis for Middle Eastern Languages with Hidden Markov Models",
        "authors": [
            "Kazem Taghva"
        ],
        "abstract": "Displaying a document in Middle Eastern languages requires contextual analysis due to different presentational forms for each character of the alphabet. The words of the document will be formed by the joining of the correct positional glyphs representing corresponding presentational forms of the characters. A set of rules defines the joining of the glyphs. As usual, these rules vary from language to language and are subject to interpretation by the software developers.\n",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01809",
        "title": "Language Models for Image Captioning: The Quirks and What Works",
        "authors": [
            "Jacob Devlin",
            "Hao Cheng",
            "Hao Fang",
            "Saurabh Gupta",
            "Li Deng",
            "Xiaodong He",
            "Geoffrey Zweig",
            "Margaret Mitchell"
        ],
        "abstract": "Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02000",
        "title": "Deep Learning for Medical Image Segmentation",
        "authors": [
            "Matthew Lai"
        ],
        "abstract": "This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer's Disease. We found that a slightly unconventional \"stacked 2D\" approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular \"tri-planar\" approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02074",
        "title": "Exploring Models and Data for Image Question Answering",
        "authors": [
            "Mengye Ren",
            "Ryan Kiros",
            "Richard Zemel"
        ],
        "abstract": "This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02206",
        "title": "Learning image representations tied to ego-motion",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ],
        "abstract": "Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond predictably to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02408",
        "title": "DistMS: A Non-Portfolio Distributed Solver for Maximum Satisfiability",
        "authors": [
            "Miguel Neves",
            "In\u00eas Lynce",
            "Vasco Manquinho"
        ],
        "abstract": "The most successful parallel SAT and MaxSAT solvers follow a portfolio approach, where each thread applies a different algorithm (or the same algorithm configured differently) to solve a given problem instance. The main goal of building a portfolio is to diversify the search process being carried out by each thread. As soon as one thread finishes, the instance can be deemed solved. In this paper we present a new open source distributed solver for MaxSAT solving that addresses two issues commonly found in multicore parallel solvers, namely memory contention and scalability. Preliminary results show that our non-portfolio distributed MaxSAT solver outperforms its sequential version and is able to solve more instances as the number of processes increases.\n    ",
        "submission_date": "2015-05-10T00:00:00",
        "last_modified_date": "2015-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02419",
        "title": "Improved Relation Extraction with Feature-Rich Compositional Embedding Models",
        "authors": [
            "Matthew R. Gormley",
            "Mo Yu",
            "Mark Dredze"
        ],
        "abstract": "Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings. We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) hand-crafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results.\n    ",
        "submission_date": "2015-05-10T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02648",
        "title": "Towards Formal Fault Tree Analysis using Theorem Proving",
        "authors": [
            "Waqar Ahmed",
            "Osman Hasan"
        ],
        "abstract": "Fault Tree Analysis (FTA) is a dependability analysis technique that has been widely used to predict reliability, availability and safety of many complex engineering systems. Traditionally, these FTA-based analyses are done using paper-and-pencil proof methods or computer simulations, which cannot ascertain absolute correctness due to their inherent limitations. As a complementary approach, we propose to use the higher-order-logic theorem prover HOL4 to conduct the FTA-based analysis of safety-critical systems where accuracy of failure analysis is a dire need. In particular, the paper presents a higher-order-logic formalization of generic Fault Tree gates, i.e., AND, OR, NAND, NOR, XOR and NOT and the formal verification of their failure probability expressions. Moreover, we have formally verified the generic probabilistic inclusion-exclusion principle, which is one of the foremost requirements for conducting the FTA-based failure analysis of any given system. For illustration purposes, we conduct the FTA-based failure analysis of a solar array that is used as the main source of power for the Dong Fang Hong-3 (DFH-3) satellite.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02729",
        "title": "Sample complexity of learning Mahalanobis distance metrics",
        "authors": [
            "Nakul Verma",
            "Kristin Branson"
        ],
        "abstract": "Metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower- and upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. However, by leveraging the structure of the data distribution, we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset. Our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset's intrinsic complexity, yielding better generalization. Experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise.\n    ",
        "submission_date": "2015-05-11T00:00:00",
        "last_modified_date": "2015-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03463",
        "title": "Exploring Strategy-Proofness, Uniqueness, and Pareto Optimality for the Stable Matching Problem with Couples",
        "authors": [
            "Andrew Perrault",
            "Joanna Drummond",
            "Fahiem Bacchus"
        ],
        "abstract": "The Stable Matching Problem with Couples (SMP-C) is a ubiquitous real-world extension of the stable matching problem (SMP) involving complementarities. Although SMP can be solved in polynomial time, SMP-C is NP-Complete. Hence, it is not clear which, if any, of the theoretical results surrounding the canonical SMP problem apply in this setting. In this paper, we use a recently-developed SAT encoding to solve SMP-C exactly. This allows us to enumerate all stable matchings for any given instance of SMP-C. With this tool, we empirically evaluate some of the properties that have been hypothesized to hold for SMP-C.\n",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03540",
        "title": "Brain Tumor Segmentation with Deep Neural Networks",
        "authors": [
            "Mohammad Havaei",
            "Axel Davy",
            "David Warde-Farley",
            "Antoine Biard",
            "Aaron Courville",
            "Yoshua Bengio",
            "Chris Pal",
            "Pierre-Marc Jodoin",
            "Hugo Larochelle"
        ],
        "abstract": "In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data.\n",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03996",
        "title": "Norm Monitoring under Partial Action Observability",
        "authors": [
            "Natalia Criado",
            "Jose M. Such"
        ],
        "abstract": "In the context of using norms for controlling multi-agent systems, a vitally important question that has not yet been addressed in the literature is the development of mechanisms for monitoring norm compliance under partial action observability. This paper proposes the reconstruction of unobserved actions to tackle this problem. In particular, we formalise the problem of reconstructing unobserved actions, and propose an information model and algorithms for monitoring norms under partial action observability using two different processes for reconstructing unobserved actions. Our evaluation shows that reconstructing unobserved actions increases significantly the number of norm violations and fulfilments detected.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04123",
        "title": "Margins, Kernels and Non-linear Smoothed Perceptrons",
        "authors": [
            "Aaditya Ramdas",
            "Javier Pe\u00f1a"
        ],
        "abstract": "We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes - the Perceptron (primal) and Von-Neumann (dual) algorithms.\n",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04214",
        "title": "Algorithmic Connections Between Active Learning and Stochastic Convex Optimization",
        "authors": [
            "Aaditya Ramdas",
            "Aarti Singh"
        ],
        "abstract": "Interesting theoretical associations have been established by recent papers between the fields of active learning and stochastic convex optimization due to the common role of feedback in sequential querying mechanisms. In this paper, we continue this thread in two parts by exploiting these relations for the first time to yield novel algorithms in both fields, further motivating the study of their intersection. First, inspired by a recent optimization algorithm that was adaptive to unknown uniform convexity parameters, we present a new active learning algorithm for one-dimensional thresholds that can yield minimax rates by adapting to unknown noise parameters. Next, we show that one can perform $d$-dimensional stochastic minimization of smooth uniformly convex functions when only granted oracle access to noisy gradient signs along any coordinate instead of real-valued gradients, by using a simple randomized coordinate descent procedure where each line search can be solved by $1$-dimensional active learning, provably achieving the same error convergence rate as having the entire real-valued gradient. Combining these two parts yields an algorithm that solves stochastic convex optimization of uniformly convex and smooth functions using only noisy gradient signs by repeatedly performing active learning, achieves optimal rates and is adaptive to all unknown convexity and smoothness parameters.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04215",
        "title": "An Analysis of Active Learning With Uniform Feature Noise",
        "authors": [
            "Aaditya Ramdas",
            "Barnabas Poczos",
            "Aarti Singh",
            "Larry Wasserman"
        ],
        "abstract": "In active learning, the user sequentially chooses values for feature $X$ and an oracle returns the corresponding label $Y$. In this paper, we consider the effect of feature noise in active learning, which could arise either because $X$ itself is being measured, or it is corrupted in transmission to the oracle, or the oracle returns the label of a noisy version of the query point. In statistics, feature noise is known as \"errors in variables\" and has been studied extensively in non-active settings. However, the effect of feature noise in active learning has not been studied before. We consider the well-known Berkson errors-in-variables model with additive uniform noise of width $\\sigma$.\n",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04406",
        "title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic",
        "authors": [
            "Stephen H. Bach",
            "Matthias Broecheler",
            "Bert Huang",
            "Lise Getoor"
        ],
        "abstract": "A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2017-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04518",
        "title": "Emergence-focused design in complex system simulation",
        "authors": [
            "Chris Marriott",
            "Jobran Chebib"
        ],
        "abstract": "Emergence is a phenomenon taken for granted in science but also still not well understood. We have developed a model of artificial genetic evolution intended to allow for emergence on genetic, population and social levels. We present the details of the current state of our environment, agent, and reproductive models. In developing our models we have relied on a principle of using non-linear systems to model as many systems as possible including mutation and recombination, gene-environment interaction, agent metabolism, agent survival, resource gathering and sexual reproduction. In this paper we review the genetic dynamics that have emerged in our system including genotype-phenotype divergence, genetic drift, pseudogenes, and gene duplication. We conclude that emergence-focused design in complex system simulation is necessary to reproduce the multilevel emergence seen in the natural world.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04542",
        "title": "Scalable Parallel Numerical Constraint Solver Using Global Load Balancing",
        "authors": [
            "Daisuke Ishii",
            "Kazuki Yoshizoe",
            "Toyotaro Suzumura"
        ],
        "abstract": "We present a scalable parallel solver for numerical constraint satisfaction problems (NCSPs). Our parallelization scheme consists of homogeneous worker solvers, each of which runs on an available core and communicates with others via the global load balancing (GLB) method. The parallel solver is implemented with X10 that provides an implementation of GLB as a library. In experiments, several NCSPs from the literature were solved and attained up to 516-fold speedup using 600 cores of the TSUBAME2.5 supercomputer.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04636",
        "title": "Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning",
        "authors": [
            "Mu Li",
            "Dave G. Andersen",
            "Alexander J. Smola"
        ],
        "abstract": "Distributed computing excels at processing large scale data, but the communication cost for synchronizing the shared parameters may slow down the overall performance. Fortunately, the interactions between parameter and data in many problems are sparse, which admits efficient partition in order to reduce the communication overhead.\n",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04746",
        "title": "Spatial database implementation of fuzzy region connection calculus for analysing the relationship of diseases",
        "authors": [
            "Somaye Davari",
            "Nasser Ghadiri"
        ],
        "abstract": "Analyzing huge amounts of spatial data plays an important role in many emerging analysis and decision-making domains such as healthcare, urban planning, agriculture and so on. For extracting meaningful knowledge from geographical data, the relationships between spatial data objects need to be analyzed. An important class of such relationships are topological relations like the connectedness or overlap between regions. While real-world geographical regions such as lakes or forests do not have exact boundaries and are fuzzy, most of the existing analysis methods neglect this inherent feature of topological relations. In this paper, we propose a method for handling the topological relations in spatial databases based on fuzzy region connection calculus (RCC). The proposed method is implemented in PostGIS spatial database and evaluated in analyzing the relationship of diseases as an important application domain. We also used our fuzzy RCC implementation for fuzzification of the skyline operator in spatial databases. The results of the evaluation show that our method provides a more realistic view of spatial relationships and gives more flexibility to the data analyst to extract meaningful and accurate results in comparison with the existing methods.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2016-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04771",
        "title": "DopeLearning: A Computational Approach to Rap Lyrics Generation",
        "authors": [
            "Eric Malmi",
            "Pyry Takala",
            "Hannu Toivonen",
            "Tapani Raiko",
            "Aristides Gionis"
        ],
        "abstract": "Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04935",
        "title": "Towards Data-Driven Autonomics in Data Centers",
        "authors": [
            "Alina S\u00eerbu",
            "Ozalp Babaoglu"
        ],
        "abstract": "Continued reliance on human operators for managing data centers is a major impediment for them from ever reaching extreme dimensions. Large computer systems in general, and data centers in particular, will ultimately be managed using predictive computational and executable models obtained through data-science tools, and at that point, the intervention of humans will be limited to setting high-level goals and policies rather than performing low-level operations. Data-driven autonomics, where management and control are based on holistic predictive models that are built and updated using generated data, opens one possible path towards limiting the role of operators in data centers. In this paper, we present a data-science study of a public Google dataset collected in a 12K-node cluster with the goal of building and evaluating a predictive model for node failures. We use BigQuery, the big data SQL platform from the Google Cloud suite, to process massive amounts of data and generate a rich feature set characterizing machine state over time. We describe how an ensemble classifier can be built out of many Random Forest classifiers each trained on these features, to predict if machines will fail in a future 24-hour window. Our evaluation reveals that if we limit false positive rates to 5%, we can achieve true positive rates between 27% and 88% with precision varying between 50% and 72%. We discuss the practicality of including our predictive model as the central component of a data-driven autonomic manager and operating it on-line with live data streams (rather than off-line on data logs). All of the scripts used for BigQuery and classification analyses are publicly available from the authors' website.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04972",
        "title": "Recursion in RDF Data Shape Languages",
        "authors": [
            "Arthur Ryman"
        ],
        "abstract": "An RDF data shape is a description of the expected contents of an RDF document (aka graph) or dataset. A major part of this description is the set of constraints that the document or dataset is required to satisfy. W3C recently (2014) chartered the RDF Data Shapes Working Group to define SHACL, a standard RDF data shape language. We refer to the ability to name and reference shape language elements as recursion. This article provides a precise definition of the meaning of recursion as used in Resource Shape 2.0. The definition of recursion presented in this article is largely independent of language-specific details. We speculate that it also applies to ShEx and to all three of the current proposals for SHACL. In particular, recursion is not permitted in the SHACL-SPARQL proposal, but we conjecture that recursion could be added by using the definition proposed here as a top-level control structure.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05004",
        "title": "An Experimental Comparison of Hybrid Algorithms for Bayesian Network Structure Learning",
        "authors": [
            "Maxime Gasse",
            "Alex Aussem",
            "Haytham Elghazel"
        ],
        "abstract": "We present a novel hybrid algorithm for Bayesian network structure learning, called Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. It is based on a subroutine called HPC, that combines ideas from incremental and divide-and-conquer constraint-based methods to learn the parents and children of a target variable. We conduct an experimental comparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning, on several benchmarks with various data sizes. Our extensive experiments show that H2PC outperforms MMHC both in terms of goodness of fit to new data and in terms of the quality of the network structure itself, which is closer to the true dependence structure of the data. The source code (in R) of H2PC as well as all data sets used for the empirical tests are publicly available.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05022",
        "title": "Modular Action Language ALM",
        "authors": [
            "Daniela Inclezan",
            "Michael Gelfond"
        ],
        "abstract": "The paper introduces a new modular action language, ALM, and illustrates the methodology of its use. It is based on the approach of Gelfond and Lifschitz (1993; 1998) in which a high-level action language is used as a front end for a logic programming system description. The resulting logic programming representation is used to perform various computational tasks. The methodology based on existing action languages works well for small and even medium size systems, but is not meant to deal with larger systems that require structuring of knowledge. ALM is meant to remedy this problem. Structuring of knowledge in ALM is supported by the concepts of module (a formal description of a specific piece of knowledge packaged as a unit), module hierarchy, and library, and by the division of a system description of ALM into two parts: theory and structure. A theory consists of one or more modules with a common theme, possibly organized into a module hierarchy based on a dependency relation. It contains declarations of sorts, attributes, and properties of the domain together with axioms describing them. Structures are used to describe the domain's objects. These features, together with the means for defining classes of a domain as special cases of previously defined ones, facilitate the stepwise development, testing, and readability of a knowledge base, as well as the creation of knowledge representation libraries. To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05190",
        "title": "Image Reconstruction from Bag-of-Visual-Words",
        "authors": [
            "Hiroharu Kato",
            "Tatsuya Harada"
        ],
        "abstract": "The objective of this work is to reconstruct an original image from Bag-of-Visual-Words (BoVW). Image reconstruction from features can be a means of identifying the characteristics of features. Additionally, it enables us to generate novel images via features. Although BoVW is the de facto standard feature for image recognition and retrieval, successful image reconstruction from BoVW has not been reported yet. What complicates this task is that BoVW lacks the spatial information for including visual words. As described in this paper, to estimate an original arrangement, we propose an evaluation function that incorporates the naturalness of local adjacency and the global position, with a method to obtain related parameters using an external image database. To evaluate the performance of our method, we reconstruct images of objects of 101 kinds. Additionally, we apply our method to analyze object classifiers and to generate novel images via BoVW.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05613",
        "title": "Parallel Streaming Signature EM-tree: A Clustering Algorithm for Web Scale Applications",
        "authors": [
            "Christopher M. de Vries",
            "Lance De Vine",
            "Shlomo Geva",
            "Richi Nayak"
        ],
        "abstract": "The proliferation of the web presents an unsolved problem of automatically analyzing billions of pages of natural language. We introduce a scalable algorithm that clusters hundreds of millions of web pages into hundreds of thousands of clusters. It does this on a single mid-range machine using efficient algorithms and compressed document representations. It is applied to two web-scale crawls covering tens of terabytes. ClueWeb09 and ClueWeb12 contain 500 and 733 million web pages and were clustered into 500,000 to 700,000 clusters. To the best of our knowledge, such fine grained clustering has not been previously demonstrated. Previous approaches clustered a sample that limits the maximum number of discoverable clusters. The proposed EM-tree algorithm uses the entire collection in clustering and produces several orders of magnitude more clusters than the existing algorithms. Fine grained clustering is necessary for meaningful clustering in massive collections where the number of distinct topics grows linearly with collection size. These fine-grained clusters show an improved cluster quality when assessed with two novel evaluations using ad hoc search relevance judgments and spam classifications for external validation. These evaluations solve the problem of assessing the quality of clusters where categorical labeling is unavailable and unfeasible.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05723",
        "title": "On the relation between accuracy and fairness in binary classification",
        "authors": [
            "Indre Zliobaite"
        ],
        "abstract": "Our study revisits the problem of accuracy-fairness tradeoff in binary classification. We argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions, otherwise conclusions about performance may be misleading, because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions. We provide methodological recommendations for sound comparison of non-discriminatory classifiers, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05770",
        "title": "Variational Inference with Normalizing Flows",
        "authors": [
            "Danilo Jimenez Rezende",
            "Shakir Mohamed"
        ],
        "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06072",
        "title": "Diffusion Methods for Classification with Pairwise Relationships",
        "authors": [
            "Pedro F. Felzenszwalb",
            "Benar F. Svaiter"
        ],
        "abstract": "We define two algorithms for propagating information in classification problems with pairwise relationships. The algorithms are based on contraction maps and are related to non-linear diffusion and random walks on graphs. The approach is also related to message passing algorithms, including belief propagation and mean field methods. The algorithms we describe are guaranteed to converge on graphs with arbitrary topology. Moreover they always converge to a unique fixed point, independent of initialization. We prove that the fixed points of the algorithms under consideration define lower-bounds on the energy function and the max-marginals of a Markov random field. The theoretical results also illustrate a relationship between message passing algorithms and value iteration for an infinite horizon Markov decision process. We illustrate the practical application of the algorithms under study with numerical experiments in image restoration, stereo depth estimation and binary classification on a grid.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2019-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06294",
        "title": "A Frobenius Model of Information Structure in Categorical Compositional Distributional Semantics",
        "authors": [
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "The categorical compositional distributional model of Coecke, Sadrzadeh and Clark provides a linguistically motivated procedure for computing the meaning of a sentence as a function of the distributional meaning of the words therein. The theoretical framework allows for reasoning about compositional aspects of language and offers structural ways of studying the underlying relationships. While the model so far has been applied on the level of syntactic structures, a sentence can bring extra information conveyed in utterances via intonational means. In the current paper we extend the framework in order to accommodate this additional information, using Frobenius algebraic structures canonically induced over the basis of finite-dimensional vector spaces. We detail the theory, provide truth-theoretic and distributional semantics for meanings of intonationally-marked utterances, and present justifications and extensive examples.\n    ",
        "submission_date": "2015-05-23T00:00:00",
        "last_modified_date": "2015-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07096",
        "title": "A U.S. Research Roadmap for Human Computation",
        "authors": [
            "Pietro Michelucci",
            "Lea Shanley",
            "Janis Dickinson",
            "Haym Hirsh"
        ],
        "abstract": "The Web has made it possible to harness human cognition en masse to achieve new capabilities. Some of these successes are well known; for example Wikipedia has become the go-to place for basic information on all things; Duolingo engages millions of people in real-life translation of text, while simultaneously teaching them to speak foreign languages; and ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2015-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07548",
        "title": "Multidefender Security Games",
        "authors": [
            "Jian Lou",
            "Andrew M. Smith",
            "Yevgeniy Vorobeychik"
        ],
        "abstract": "Stackelberg security game models and associated computational tools have seen deployment in a number of high-consequence security settings, such as LAX canine patrols and Federal Air Marshal Service. These models focus on isolated systems with only one defender, despite being part of a more complex system with multiple players. Furthermore, many real systems such as transportation networks and the power grid exhibit interdependencies between targets and, consequently, between decision makers jointly charged with protecting them. To understand such multidefender strategic interactions present in security, we investigate game theoretic models of security games with multiple defenders. Unlike most prior analysis, we focus on the situations in which each defender must protect multiple targets, so that even a single defender's best response decision is, in general, highly non-trivial. We start with an analytical investigation of multidefender security games with independent targets, offering an equilibrium and price-of-anarchy analysis of three models with increasing generality. In all models, we find that defenders have the incentive to over-protect targets, at times significantly. Additionally, in the simpler models, we find that the price of anarchy is unbounded, linearly increasing both in the number of defenders and the number of targets per defender. Considering interdependencies among targets, we develop a novel mixed-integer linear programming formulation to compute a defender's best response, and make use of this formulation in approximating Nash equilibria of the game. We apply this approach towards computational strategic analysis of several models of networks representing interdependencies, including real-world power networks. Our analysis shows how network structure and the probability of failure spread determine the propensity of defenders to over- or under-invest in security.\n    ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08153",
        "title": "Feature Representation for Online Signature Verification",
        "authors": [
            "Mohsen Fayyaz",
            "Mohammad Hajizadeh_Saffar",
            "Mohammad Sabokrou",
            "Mahmood Fathy"
        ],
        "abstract": "Biometrics systems have been used in a wide range of applications and have improved people authentication. Signature verification is one of the most common biometric methods with techniques that employ various specifications of a signature. Recently, deep learning has achieved great success in many fields, such as image, sounds and text processing. In this paper, deep learning method has been used for feature extraction and feature selection.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00195",
        "title": "Recurrent Neural Networks with External Memory for Language Understanding",
        "authors": [
            "Baolin Peng",
            "Kaisheng Yao"
        ],
        "abstract": "Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.\n    ",
        "submission_date": "2015-05-31T00:00:00",
        "last_modified_date": "2015-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00482",
        "title": "Learning Regular Languages over Large Ordered Alphabets",
        "authors": [
            "Irini-Eleftheria Mens",
            "Oded Maler"
        ],
        "abstract": " This work is concerned with regular languages defined over large alphabets, either infinite or just too large to be expressed enumeratively. We define a generic model where transitions are labeled by elements of a finite partition of the alphabet. We then extend Angluin's L* algorithm for learning regular languages from examples for such automata. We have implemented this algorithm and we demonstrate its behavior where the alphabet is a subset of the natural or real numbers. We sketch the extension of the algorithm to a class of languages over partially ordered alphabets. \n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00935",
        "title": "Discovering Valuable Items from Massive Data",
        "authors": [
            "Hastagiri P. Vanchinathan",
            "Andreas Marfurt",
            "Charles-Antoine Robelin",
            "Donald Kossmann",
            "Andreas Krause"
        ],
        "abstract": "Suppose there is a large collection of items, each with an associated cost and an inherent utility that is revealed only once we commit to selecting it. Given a budget on the cumulative cost of the selected items, how can we pick a subset of maximal value? This task generalizes several important problems such as multi-arm bandits, active search and the knapsack problem. We present an algorithm, GP-Select, which utilizes prior knowledge about similarity be- tween items, expressed as a kernel function. GP-Select uses Gaussian process prediction to balance exploration (estimating the unknown value of items) and exploitation (selecting items of high value). We extend GP-Select to be able to discover sets that simultaneously have high utility and are diverse. Our preference for diversity can be specified as an arbitrary monotone submodular function that quantifies the diminishing returns obtained when selecting similar items. Furthermore, we exploit the structure of the model updates to achieve an order of magnitude (up to 40X) speedup in our experiments without resorting to approximations. We provide strong guarantees on the performance of GP-Select and apply it to three real-world case studies of industrial relevance: (1) Refreshing a repository of prices in a Global Distribution System for the travel industry, (2) Identifying diverse, binding-affine peptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale recommender system by recommending items to users.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01072",
        "title": "Homogeneous Spiking Neuromorphic System for Real-World Pattern Recognition",
        "authors": [
            "Xinyu Wu",
            "Vishal Saxena",
            "Kehan Zhu"
        ],
        "abstract": "A neuromorphic chip that combines CMOS analog spiking neurons and memristive synapses offers a promising solution to brain-inspired computing, as it can provide massive neural network parallelism and density. Previous hybrid analog CMOS-memristor approaches required extensive CMOS circuitry for training, and thus eliminated most of the density advantages gained by the adoption of memristor synapses. Further, they used different waveforms for pre and post-synaptic spikes that added undesirable circuit overhead. Here we describe a hardware architecture that can feature a large number of memristor synapses to learn real-world patterns. We present a versatile CMOS neuron that combines integrate-and-fire behavior, drives passive memristors and implements competitive learning in a compact circuit module, and enables in-situ plasticity in the memristor synapses. We demonstrate handwritten-digits recognition using the proposed architecture using transistor-level circuit simulations. As the described neuromorphic architecture is homogeneous, it realizes a fundamental building block for large-scale energy-efficient brain-inspired silicon chips that could lead to next-generation cognitive computing.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01094",
        "title": "Traversing Knowledge Graphs in Vector Space",
        "authors": [
            "Kelvin Guu",
            "John Miller",
            "Percy Liang"
        ],
        "abstract": "Path queries on a knowledge graph can be used to answer compositional questions such as \"What languages are spoken by people living in Lisbon?\". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \"compositional\" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01170",
        "title": "A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc Coordination in Multiagent Systems",
        "authors": [
            "Stefano V. Albrecht",
            "Subramanian Ramamoorthy"
        ],
        "abstract": "The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01273",
        "title": "Summarization of Films and Documentaries Based on Subtitles and Scripts",
        "authors": [
            "Marta Apar\u00edcio",
            "Paulo Figueiredo",
            "Francisco Raposo",
            "David Martins de Matos",
            "Ricardo Ribeiro",
            "Lu\u00eds Marujo"
        ],
        "abstract": "We assess the performance of generic text summarization algorithms applied to films and documentaries, using the well-known behavior of summarization of news articles as reference. We use three datasets: (i) news articles, (ii) film scripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics are used for comparing generated summaries against news abstracts, plot summaries, and synopses. We show that the best performing algorithms are LSA, for news articles and documentaries, and LexRank and Support Sets, for films. Despite the different nature of films and documentaries, their relative behavior is in accordance with that obtained for news articles.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01326",
        "title": "Probabilistic Numerics and Uncertainty in Computations",
        "authors": [
            "Philipp Hennig",
            "Michael A Osborne",
            "Mark Girolami"
        ],
        "abstract": "We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data has led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimisers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01597",
        "title": "Abstractive Multi-Document Summarization via Phrase Selection and Merging",
        "authors": [
            "Lidong Bing",
            "Piji Li",
            "Yi Liao",
            "Wai Lam",
            "Weiwei Guo",
            "Rebecca J. Passonneau"
        ],
        "abstract": "We propose an abstraction-based multi-document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of-the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01911",
        "title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video",
        "authors": [
            "Lionel Pigou",
            "A\u00e4ron van den Oord",
            "Sander Dieleman",
            "Mieke Van Herreweghe",
            "Joni Dambre"
        ],
        "abstract": "Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02113",
        "title": "Selective Greedy Equivalence Search: Finding Optimal Bayesian Networks Using a Polynomial Number of Score Evaluations",
        "authors": [
            "David Maxwell Chickering",
            "Christopher Meek"
        ],
        "abstract": "We introduce Selective Greedy Equivalence Search (SGES), a restricted version of Greedy Equivalence Search (GES). SGES retains the asymptotic correctness of GES but, unlike GES, has polynomial performance guarantees. In particular, we show that when data are sampled independently from a distribution that is perfect with respect to a DAG ${\\cal G}$ defined over the observable variables then, in the limit of large data, SGES will identify ${\\cal G}$'s equivalence class after a number of score evaluations that is (1) polynomial in the number of nodes and (2) exponential in various complexity measures including maximum-number-of-parents, maximum-clique-size, and a new measure called {\\em v-width} that is at least as small as---and potentially much smaller than---the other two. More generally, we show that for any hereditary and equivalence-invariant property $\\Pi$ known to hold in ${\\cal G}$, we retain the large-sample optimality guarantees of GES even if we ignore any GES deletion operator during the backward phase that results in a state for which $\\Pi$ does not hold in the common-descendants subgraph.\n    ",
        "submission_date": "2015-06-06T00:00:00",
        "last_modified_date": "2015-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02264",
        "title": "Visual Learning of Arithmetic Operations",
        "authors": [
            "Yedid Hoshen",
            "Shmuel Peleg"
        ],
        "abstract": "A simple Neural Network model is presented for end-to-end visual learning of arithmetic operations from pictures of numbers. The input consists of two pictures, each showing a 7-digit number. The output, also a picture, displays the number showing the result of an arithmetic operation (e.g., addition or subtraction) on the two input numbers. The concepts of a number, or of an operator, are not explicitly introduced. This indicates that addition is a simple cognitive task, which can be learned visually using a very small number of neurons.\n",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2015-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02442",
        "title": "NP-hardness of sortedness constraints",
        "authors": [
            "Irena Rusu"
        ],
        "abstract": "In Constraint Programming, global constraints allow to model and solve many combinatorial problems. Among these constraints, several sortedness constraints have been defined, for which propagation algorithms are available, but for which the tractability is not settled. We show that the sort(U,V) constraint (Older et. al, 1995) is intractable for integer variables whose domains are not limited to intervals. As a consequence, the similar result holds for the sort(U,V, P) constraint (Zhou, 1996). Moreover, the intractability holds even under the stability condition present in the recently introduced keysorting(U,V,Keys,P) constraint (Carlsson et al., 2014), and requiring that the order of the variables with the same value in the list U be preserved in the list V. Therefore, keysorting(U,V,Keys,P) is intractable as well.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02750",
        "title": "Self Organizing Maps Whose Topologies Can Be Learned With Adaptive Binary Search Trees Using Conditional Rotations",
        "authors": [
            "C\u00e9sar A. Astudillo",
            "B. John Oommen"
        ],
        "abstract": "Numerous variants of Self-Organizing Maps (SOMs) have been proposed in the literature, including those which also possess an underlying structure, and in some cases, this structure itself can be defined by the user Although the concepts of growing the SOM and updating it have been studied, the whole issue of using a self-organizing Adaptive Data Structure (ADS) to further enhance the properties of the underlying SOM, has been unexplored. In an earlier work, we impose an arbitrary, user-defined, tree-like topology onto the codebooks, which consequently enforced a neighborhood phenomenon and the so-called tree-based Bubble of Activity. In this paper, we consider how the underlying tree itself can be rendered dynamic and adaptively transformed. To do this, we present methods by which a SOM with an underlying Binary Search Tree (BST) structure can be adaptively re-structured using Conditional Rotations (CONROT). These rotations on the nodes of the tree are local, can be done in constant time, and performed so as to decrease the Weighted Path Length (WPL) of the entire tree. In doing this, we introduce the pioneering concept referred to as Neural Promotion, where neurons gain prominence in the Neural Network (NN) as their significance increases. We are not aware of any research which deals with the issue of Neural Promotion. The advantages of such a scheme is that the user need not be aware of any of the topological peculiarities of the stochastic data distribution. Rather, the algorithm, referred to as the TTOSOM with Conditional Rotations (TTOCONROT), converges in such a manner that the neurons are ultimately placed in the input space so as to represent its stochastic distribution, and additionally, the neighborhood properties of the neurons suit the best BST that represents the data. These properties have been confirmed by our experimental results on a variety of data sets.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02922",
        "title": "An Ensemble method for Content Selection for Data-to-text Systems",
        "authors": [
            "Dimitra Gkatzia",
            "Helen Hastie"
        ],
        "abstract": "We present a novel approach for automatic report generation from time-series data, in the context of student feedback generation. Our proposed methodology treats content selection as a multi-label classification (MLC) problem, which takes as input time-series data (students' learning data) and outputs a summary of these data (feedback). Unlike previous work, this method considers all data simultaneously using ensembles of classifiers, and therefore, it achieves higher accuracy and F- score compared to meaningful baselines.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03340",
        "title": "Teaching Machines to Read and Comprehend",
        "authors": [
            "Karl Moritz Hermann",
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Edward Grefenstette",
            "Lasse Espeholt",
            "Will Kay",
            "Mustafa Suleyman",
            "Phil Blunsom"
        ],
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03374",
        "title": "An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives",
        "authors": [
            "Shipra Agrawal",
            "Nikhil R. Devanur",
            "Lihong Li"
        ],
        "abstract": "We consider a contextual version of multi-armed bandit problem with global knapsack constraints. In each round, the outcome of pulling an arm is a scalar reward and a resource consumption vector, both dependent on the context, and the global knapsack constraints require the total consumption for each resource to be below some pre-fixed budget. The learning agent competes with an arbitrary set of context-dependent policies. This problem was introduced by Badanidiyuru et al. (2014), who gave a computationally inefficient algorithm with near-optimal regret bounds for it. We give a computationally efficient algorithm for this problem with slightly better regret bounds, by generalizing the approach of Agarwal et al. (2014) for the non-constrained version of the problem. The computational time of our algorithm scales logarithmically in the size of the policy space. This answers the main open question of Badanidiyuru et al. (2014). We also extend our results to a variant where there are no knapsack constraints but the objective is an arbitrary Lipschitz concave function of the sum of outcome vectors.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2016-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03378",
        "title": "On the Prior Sensitivity of Thompson Sampling",
        "authors": [
            "Che-Yu Liu",
            "Lihong Li"
        ],
        "abstract": "The empirically successful Thompson Sampling algorithm for stochastic bandits has drawn much interest in understanding its theoretical properties. One important benefit of the algorithm is that it allows domain knowledge to be conveniently encoded as a prior distribution to balance exploration and exploitation more effectively. While it is generally believed that the algorithm's regret is low (high) when the prior is good (bad), little is known about the exact dependence. In this paper, we fully characterize the algorithm's worst-case dependence of regret on the choice of prior, focusing on a special yet representative case. These results also provide insights into the general sensitivity of the algorithm to the choice of priors. In particular, with $p$ being the prior probability mass of the true reward-generating model, we prove $O(\\sqrt{T/p})$ and $O(\\sqrt{(1-p)T})$ regret upper bounds for the bad- and good-prior cases, respectively, as well as \\emph{matching} lower bounds. Our proofs rely on the discovery of a fundamental property of Thompson Sampling and make heavy use of martingale theory, both of which appear novel in the literature, to the best of our knowledge.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03379",
        "title": "The Online Coupon-Collector Problem and Its Application to Lifelong Reinforcement Learning",
        "authors": [
            "Emma Brunskill",
            "Lihong Li"
        ],
        "abstract": "Transferring knowledge across a sequence of related tasks is an important challenge in reinforcement learning (RL). Despite much encouraging empirical evidence, there has been little theoretical analysis. In this paper, we study a class of lifelong RL problems: the agent solves a sequence of tasks modeled as finite Markov decision processes (MDPs), each of which is from a finite set of MDPs with the same state/action sets and different transition/reward functions. Motivated by the need for cross-task exploration in lifelong learning, we formulate a novel online coupon-collector problem and give an optimal algorithm. This allows us to develop a new lifelong RL algorithm, whose overall sample complexity in a sequence of tasks is much smaller than single-task learning, even if the sequence of tasks is generated by an adversary. Benefits of the algorithm are demonstrated in simulated problems, including a recently introduced human-robot interaction problem.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03493",
        "title": "Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts",
        "authors": [
            "Aaron Schein",
            "John Paisley",
            "David M. Blei",
            "Hanna Wallach"
        ],
        "abstract": "We present a Bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. For decades, political scientists have collected and analyzed records of the form \"country $i$ took action $a$ toward country $j$ at time $t$\"---known as dyadic events---in order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a low-dimensional, interpretable representation of their salient patterns. We demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counterparts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political scientists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international affairs.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04089",
        "title": "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences",
        "authors": [
            "Hongyuan Mei",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence \"regions\" salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or task-specific annotations (e.g., seed lexicons). It is therefore generalizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04299",
        "title": "Query-Answer Causality in Databases: Abductive Diagnosis and View-Updates",
        "authors": [
            "Babak Salimi",
            "Leopoldo Bertossi"
        ],
        "abstract": "Causality has been recently introduced in databases, to model, characterize and possibly compute causes for query results (answers). Connections between query causality and consistency-based diagnosis and database repairs (wrt. integrity constrain violations) have been established in the literature. In this work we establish connections between query causality and abductive diagnosis and the view-update problem. The unveiled relationships allow us to obtain new complexity results for query causality -the main focus of our work- and also for the two other areas.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04319",
        "title": "Generating and Exploring S-Box Multivariate Quadratic Equation Systems with SageMath",
        "authors": [
            "A.-M. Leventi-Peetz",
            "J.-V. Peetz"
        ],
        "abstract": "A new method to derive Multivariate Quadratic equation systems (MQ) for the input and output bit variables of a cryptographic S-box from its algebraic expressions with the aid of the computer mathematics software system SageMath is presented. We consolidate the deficiency of previously presented MQ metrics, supposed to quantify the resistance of S-boxes against algebraic attacks.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2021-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04349",
        "title": "Rare Speed-up in Automatic Theorem Proving Reveals Tradeoff Between Computational Time and Information Value",
        "authors": [
            "Santiago Hern\u00e1ndez-Orozco",
            "Francisco Hern\u00e1ndez-Quiroz",
            "Hector Zenil",
            "Wilfried Sieg"
        ],
        "abstract": "We show that strategies implemented in automatic theorem proving involve an interesting tradeoff between execution speed, proving speedup/computational time and usefulness of information. We advance formal definitions for these concepts by way of a notion of normality related to an expected (optimal) theoretical speedup when adding useful information (other theorems as axioms), as compared with actual strategies that can be effectively and efficiently implemented. We propose the existence of an ineluctable tradeoff between this normality and computational time complexity. The argument quantifies the usefulness of information in terms of (positive) speed-up. The results disclose a kind of no-free-lunch scenario and a tradeoff of a fundamental nature. The main theorem in this paper together with the numerical experiment---undertaken using two different automatic theorem provers AProS and Prover9 on random theorems of propositional logic---provide strong theoretical and empirical arguments for the fact that finding new useful information for solving a specific problem (theorem) is, in general, as hard as the problem (theorem) itself.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04365",
        "title": "Leveraging Word Embeddings for Spoken Document Summarization",
        "authors": [
            "Kuan-Yu Chen",
            "Shih-Hung Liu",
            "Hsin-Min Wang",
            "Berlin Chen",
            "Hsin-Hsi Chen"
        ],
        "abstract": "Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04744",
        "title": "Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game",
        "authors": [
            "Vlad Niculae",
            "Srijan Kumar",
            "Jordan Boyd-Graber",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Interpersonal relations are fickle, with close friendships often dissolving into enmity. In this work, we explore linguistic cues that presage such transitions by studying dyadic interactions in an online strategy game where players form alliances and break those alliances through betrayal. We characterize friendships that are unlikely to last and examine temporal patterns that foretell betrayal.\n",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05001",
        "title": "Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection",
        "authors": [
            "Liliana Lo Presti",
            "Marco La Cascia"
        ],
        "abstract": "This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix. The paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-the-shelf classification tools are adopted.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05012",
        "title": "Emotion Analysis of Songs Based on Lyrical and Audio Features",
        "authors": [
            "Adit Jamdar",
            "Jessica Abraham",
            "Karishma Khanna",
            "Rahul Dubey"
        ],
        "abstract": "In this paper, a method is proposed to detect the emotion of a song based on its lyrical and audio features. Lyrical features are generated by segmentation of lyrics during the process of data extraction. ANEW and WordNet knowledge is then incorporated to compute Valence and Arousal values. In addition to this, linguistic association rules are applied to ensure that the issue of ambiguity is properly addressed. Audio features are used to supplement the lyrical ones and include attributes like energy, tempo, and danceability. These features are extracted from The Echo Nest, a widely used music intelligence platform. Construction of training and test sets is done on the basis of social tags extracted from the ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05154",
        "title": "SNA-based reasoning for multiagent team composition",
        "authors": [
            "Andre Filipe de Moraes Batista",
            "Maria das Gra\u00e7as Bruno Marietto"
        ],
        "abstract": "The social network analysis (SNA), branch of complex systems can be used in the construction of multiagent systems. This paper proposes a study of how social network analysis can assist in modeling multiagent systems, while addressing similarities and differences between the two theories. We built a prototype of multi-agent systems for resolution of tasks through the formation of teams of agents that are formed on the basis of the social network established between agents. Agents make use of performance indicators to assess when should change their social network to maximize the participation in teams\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05198",
        "title": "SAT-based Analysis of Large Real-world Feature Models is Easy",
        "authors": [
            "Jia Hui Liang",
            "Vijay Ganesh",
            "Venkatesh Raman",
            "Krzysztof Czarnecki"
        ],
        "abstract": "Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-to-analyze is that the vast majority of the variables in these models are unrestricted, i.e., the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size, explaining why solvers scale so well. Further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability. Additionally, we experimented with a series of well-known non-backtracking simplifications that are particularly effective in solving FMs. The remaining variables/clauses after simplifications, called the core, are so few that they are easily solved even with backtracking, further strengthening our conclusions.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05424",
        "title": "Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume Maximization",
        "authors": [
            "Conrado Silva Miranda",
            "Fernando Jos\u00e9 Von Zuben"
        ],
        "abstract": "This paper introduces a high-performance hybrid algorithm, called Hybrid Hypervolume Maximization Algorithm (H2MA), for multi-objective optimization that alternates between exploring the decision space and exploiting the already obtained non-dominated solutions. The proposal is centered on maximizing the hypervolume indicator, thus converting the multi-objective problem into a single-objective one. The exploitation employs gradient-based methods, but considering a single candidate efficient solution at a time, to overcome limitations associated with population-based approaches and also to allow an easy control of the number of solutions provided. There is an interchange between two steps. The first step is a deterministic local exploration, endowed with an automatic procedure to detect stagnation. When stagnation is detected, the search is switched to a second step characterized by a stochastic global exploration using an evolutionary algorithm. Using five ZDT benchmarks with 30 variables, the performance of the new algorithm is compared to state-of-the-art algorithms for multi-objective optimization, more specifically NSGA-II, SPEA2, and SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume and smaller distance to the true Pareto frontier with significantly less function evaluations, even when the gradient is estimated numerically. Furthermore, although only continuous decision spaces have been considered here, discrete decision spaces could also have been treated, replacing gradient-based search by hill-climbing. Finally, a thorough explanation is provided to support the expressive gain in performance that was achieved.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05573",
        "title": "Emergence of synchrony in an Adaptive Interaction Model",
        "authors": [
            "Kevin Sanlaville",
            "G\u00e9rard Assayag",
            "Fr\u00e9d\u00e9ric Bevilacqua",
            "Catherine Pelachaud"
        ],
        "abstract": "In a Human-Computer Interaction context, we aim to elaborate an adaptive and generic interaction model in two different use cases: Embodied Conversational Agents and Creative Musical Agents for musical improvisation. To reach this goal, we'll try to use the concepts of adaptation and synchronization to enhance the interactive abilities of our agents and guide the development of our interaction model, and will try to make synchrony emerge from non-verbal dimensions of interaction.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05692",
        "title": "A hybrid algorithm for Bayesian network structure learning with application to multi-label learning",
        "authors": [
            "Maxime Gasse",
            "Alex Aussem",
            "Haytham Elghazel"
        ],
        "abstract": "We present a novel hybrid algorithm for Bayesian network structure learning, called H2PC. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. The algorithm is based on divide-and-conquer constraint-based subroutines to learn the local structure around a target variable. We conduct two series of experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning. First, we use eight well-known Bayesian network benchmarks with various data sizes to assess the quality of the learned structure returned by the algorithms. Our extensive experiments show that H2PC outperforms MMHC in terms of goodness of fit to new data and quality of the network structure with respect to the true dependence structure of the data. Second, we investigate H2PC's ability to solve the multi-label learning problem. We provide theoretical results to characterize and identify graphically the so-called minimal label powersets that appear as irreducible factors in the joint distribution under the faithfulness condition. The multi-label learning problem is then decomposed into a series of multi-class classification problems, where each multi-class variable encodes a label powerset. H2PC is shown to compare favorably to MMHC in terms of global classification accuracy over ten multi-label data sets covering different application domains. Overall, our experiments support the conclusions that local structural learning with H2PC in the form of local neighborhood induction is a theoretically well-motivated and empirically effective learning framework that is well suited to multi-label learning. The source code (in R) of H2PC as well as all data sets used for the empirical tests are publicly available.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05903",
        "title": "Detecting Real-World Influence Through Twitter",
        "authors": [
            "Jean-Val{\u00e8}re Cossu",
            "Nicolas Dugu{\u00e9}",
            "Vincent Labatut"
        ],
        "abstract": "In this paper, we investigate the issue of detecting the real-life influence of people based on their Twitter account. We propose an overview of common Twitter features used to characterize such accounts and their activity, and show that these are inefficient in this context. In particular, retweets and followers numbers, and Klout score are not relevant to our analysis. We thus propose several Machine Learning approaches based on Natural Language Processing and Social Network Analysis to label Twitter users as Influencers or not. We also rank them according to a predicted influence level. Our proposals are evaluated over the CLEF RepLab 2014 dataset, and outmatch state-of-the-art ranking methods.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2021-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05934",
        "title": "Expectation Particle Belief Propagation",
        "authors": [
            "Thibaut Lienart",
            "Yee Whye Teh",
            "Arnaud Doucet"
        ],
        "abstract": "We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of Ihler and McAllester (2009) at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06366",
        "title": "A Novel Method for Stock Forecasting based on Fuzzy Time Series Combined with the Longest Common/Repeated Sub-sequence",
        "authors": [
            "He-Wen Chen",
            "Zih-Ci Wang",
            "Shu-Yu Kuo",
            "Yao-Hsin Chou"
        ],
        "abstract": "Stock price forecasting is an important issue for investors since extreme accuracy in forecasting can bring about high profits. Fuzzy Time Series (FTS) and Longest Common/Repeated Sub-sequence (LCS/LRS) are two important issues for forecasting prices. However, to the best of our knowledge, there are no significant studies using LCS/LRS to predict stock prices. It is impossible that prices stay exactly the same as historic prices. Therefore, this paper proposes a state-of-the-art method which combines FTS and LCS/LRS to predict stock prices. This method is based on the principle that history will repeat itself. It uses different interval lengths in FTS to fuzzify the prices, and LCS/LRS to look for the same pattern in the historical prices to predict future stock prices. In the experiment, we examine various intervals of fuzzy time sets in order to achieve high prediction accuracy. The proposed method outperforms traditional methods in terms of prediction accuracy and, furthermore, it is easy to implement.\n    ",
        "submission_date": "2015-06-21T00:00:00",
        "last_modified_date": "2015-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06418",
        "title": "Extreme Extraction: Only One Hour per Relation",
        "authors": [
            "Raphael Hoffmann",
            "Luke Zettlemoyer",
            "Daniel S. Weld"
        ],
        "abstract": "Information Extraction (IE) aims to automatically generate a large knowledge base from natural language text, but progress remains slow. Supervised learning requires copious human annotation, while unsupervised and weakly supervised approaches do not deliver competitive accuracy. As a result, most fielded applications of IE, as well as the leading TAC-KBP systems, rely on significant amounts of manual engineering. Even \"Extreme\" methods, such as those reported in Freedman et al. 2011, require about 10 hours of expert labor per relation.\n",
        "submission_date": "2015-06-21T00:00:00",
        "last_modified_date": "2015-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06714",
        "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses",
        "authors": [
            "Alessandro Sordoni",
            "Michel Galley",
            "Michael Auli",
            "Chris Brockett",
            "Yangfeng Ji",
            "Margaret Mitchell",
            "Jian-Yun Nie",
            "Jianfeng Gao",
            "Bill Dolan"
        ],
        "abstract": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06833",
        "title": "A Survey of Current Datasets for Vision and Language Research",
        "authors": [
            "Francis Ferraro",
            "Nasrin Mostafazadeh",
            "Ting-Hao",
            "Huang",
            "Lucy Vanderwende",
            "Jacob Devlin",
            "Michel Galley",
            "Margaret Mitchell"
        ],
        "abstract": "Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07220",
        "title": "Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks",
        "authors": [
            "Yangtuo Peng",
            "Hui Jiang"
        ],
        "abstract": "Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage financial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can significantly improve the stock prediction accuracy on a standard financial database over the baseline system using only the historical price information.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07504",
        "title": "Objective Variables for Probabilistic Revenue Maximization in Second-Price Auctions with Reserve",
        "authors": [
            "Maja R. Rudolph",
            "Joseph G. Ellis",
            "David M. Blei"
        ],
        "abstract": "Many online companies sell advertisement space in second-price auctions with reserve. In this paper, we develop a probabilistic method to learn a profitable strategy to set the reserve price. We use historical auction data with features to fit a predictor of the best reserve price. This problem is delicate - the structure of the auction is such that a reserve price set too high is much worse than a reserve price set too low. To address this we develop objective variables, a new framework for combining probabilistic modeling with optimal decision-making. Objective variables are \"hallucinated observations\" that transform the revenue maximization task into a regularized maximum likelihood estimation problem, which we solve with an EM algorithm. This framework enables a variety of prediction mechanisms to set the reserve price. As examples, we study objective variable methods with regression, kernelized regression, and neural networks on simulated and real data. Our methods outperform previous approaches both in terms of scalability and profit.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08126",
        "title": "Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest",
        "authors": [
            "Dragomir Radev",
            "Amanda Stent",
            "Joel Tetreault",
            "Aasish Pappu",
            "Aikaterini Iliakopoulou",
            "Agustin Chanfreau",
            "Paloma de Juan",
            "Jordi Vallmitjana",
            "Alejandro Jaimes",
            "Rahul Jha",
            "Bob Mankoff"
        ],
        "abstract": "The New Yorker publishes a weekly captionless cartoon. More than 5,000 readers submit captions for it. The editors select three of them and ask the readers to pick the funniest one. We describe an experiment that compares a dozen automatic methods for selecting the funniest caption. We show that negative sentiment, human-centeredness, and lexical centrality most strongly match the funniest captions, followed by positive sentiment. These results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal (vision+text) systems. As part of this work, a large set of cartoons and captions is being made available to the community.\n    ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2015-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08425",
        "title": "Deep-Plant: Plant Identification with convolutional neural networks",
        "authors": [
            "Sue Han Lee",
            "Chee Seng Chan",
            "Paul Wilkin",
            "Paolo Remagnino"
        ],
        "abstract": "This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.\n    ",
        "submission_date": "2015-06-28T00:00:00",
        "last_modified_date": "2015-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08544",
        "title": "Exact and approximate inference in graphical models: variable elimination and beyond",
        "authors": [
            "Nathalie Peyrard",
            "Marie-Jos\u00e9e Cros",
            "Simon de Givry",
            "Alain Franc",
            "St\u00e9phane Robin",
            "R\u00e9gis Sabbadin",
            "Thomas Schiex",
            "Matthieu Vignes"
        ],
        "abstract": "Probabilistic graphical models offer a powerful framework to account for the dependence structure between variables, which is represented as a graph. However, the dependence between variables may render inference tasks intractable. In this paper we review techniques exploiting the graph structure for exact inference, borrowed from optimisation and computer science. They are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated. The so-called treewidth of the graph characterises this algorithmic complexity: low-treewidth graphs can be processed efficiently. The first message that we illustrate is therefore the idea that for inference in graphical model, the number of variables is not the limiting factor, and it is worth checking for the treewidth before turning to approximate methods. We show how algorithms providing an upper bound of the treewidth can be exploited to derive a 'good' elimination order enabling to perform exact inference. The second message is that when the treewidth is too large, algorithms for approximate inference linked to the principle of variable elimination, such as loopy belief propagation and variational approaches, can lead to accurate results while being much less time consuming than Monte-Carlo approaches. We illustrate the techniques reviewed in this article on benchmarks of inference problems in genetic linkage analysis and computer vision, as well as on hidden variables restoration in coupled Hidden Markov Models.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2018-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08781",
        "title": "On Design Mining: Coevolution and Surrogate Models",
        "authors": [
            "Richard J. Preen",
            "Larry Bull"
        ],
        "abstract": "Design mining is the use of computational intelligence techniques to iteratively search and model the attribute space of physical objects evaluated directly through rapid prototyping to meet given objectives. It enables the exploitation of novel materials and processes without formal models or complex simulation. In this paper, we focus upon the coevolutionary nature of the design process when it is decomposed into concurrent sub-design threads due to the overall complexity of the task. Using an abstract, tuneable model of coevolution we consider strategies to sample sub-thread designs for whole system testing and how best to construct and use surrogate models within the coevolutionary scenario. Drawing on our findings, the paper then describes the effective design of an array of six heterogeneous vertical-axis wind turbines.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08909",
        "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
        "authors": [
            "Ryan Lowe",
            "Nissan Pow",
            "Iulian Serban",
            "Joelle Pineau"
        ],
        "abstract": "This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08941",
        "title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
        "authors": [
            "Karthik Narasimhan",
            "Tejas Kulkarni",
            "Regina Barzilay"
        ],
        "abstract": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08959",
        "title": "A Large-Scale Car Dataset for Fine-Grained Categorization and Verification",
        "authors": [
            "Linjie Yang",
            "Ping Luo",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Updated on 24/09/2015: This update provides preliminary experiment results for fine-grained classification on the surveillance data of CompCars. The train/test splits are provided in the updated dataset. See details in Section 6.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00043",
        "title": "Top-N recommendations in the presence of sparsity: An NCD-based approach",
        "authors": [
            "Athanasios N. Nikolakopoulos",
            "John D. Garofalakis"
        ],
        "abstract": "Making recommendations in the presence of sparsity is known to present one of the most challenging problems faced by collaborative filtering methods. In this work we tackle this problem by exploiting the innately hierarchical structure of the item space following an approach inspired by the theory of Decomposability. We view the itemspace as a Nearly Decomposable system and we define blocks of closely related elements and corresponding indirect proximity components. We study the theoretical properties of the decomposition and we derive sufficient conditions that guarantee full item space coverage even in cold-start recommendation scenarios. A comprehensive set of experiments on the MovieLens and the Yahoo!R2Music datasets, using several widely applied performance metrics, support our model's theoretically predicted properties and verify that NCDREC outperforms several state-of-the-art algorithms, in terms of recommendation accuracy, diversity and sparseness insensitivity.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00066",
        "title": "Fast Cross-Validation for Incremental Learning",
        "authors": [
            "Pooria Joulani",
            "Andr\u00e1s Gy\u00f6rgy",
            "Csaba Szepesv\u00e1ri"
        ],
        "abstract": "Cross-validation (CV) is one of the main tools for performance estimation and parameter tuning in machine learning.   The general recipe for computing CV estimate   is to run a learning algorithm separately   for each CV fold, a computationally expensive process.   In this paper, we propose a new approach to reduce   the computational burden of CV-based performance estimation.   As opposed to all previous attempts, which are specific to a particular   learning model or problem domain, we propose a general method applicable   to a large class of incremental learning algorithms,   which are uniquely fitted to big data problems.   In particular, our method applies to a wide   range of supervised and unsupervised learning tasks with different   performance criteria, as long as the base learning algorithm is   incremental. We show that the running time of the algorithm scales   logarithmically, rather than linearly, in the number of CV   folds. Furthermore, the algorithm has favorable properties for   parallel and distributed implementation. Experiments with   state-of-the-art incremental learning algorithms confirm the   practicality of the proposed method.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00257",
        "title": "From Causes for Database Queries to Repairs and Model-Based Diagnosis and Back",
        "authors": [
            "Leopoldo Bertossi",
            "Babak Salimi"
        ],
        "abstract": "In this work we establish and investigate connections between causes for query answers in databases, database repairs wrt. denial constraints, and consistency-based diagnosis. The first two are relatively new research areas in databases, and the third one is an established subject in knowledge representation. We show how to obtain database repairs from causes, and the other way around. Causality problems are formulated as diagnosis problems, and the diagnoses provide causes and their responsibilities. The vast body of research on database repairs can be applied to the newer problems of computing actual causes for query answers and their responsibilities. These connections, which are interesting per se, allow us, after a transition -inspired by consistency-based diagnosis- to computational problems on hitting sets and vertex covers in hypergraphs, to obtain several new algorithmic and complexity results for database causality.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2016-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00407",
        "title": "Fast Convergence of Regularized Learning in Games",
        "authors": [
            "Vasilis Syrgkanis",
            "Alekh Agarwal",
            "Haipeng Luo",
            "Robert E. Schapire"
        ],
        "abstract": "We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at $O(T^{-3/4})$, while the sum of utilities converges to an approximate optimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates. We show a black-box reduction for any algorithm in the class to achieve $\\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of [Rakhlin and Shridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-player zero-sum games for specific algorithms.\n    ",
        "submission_date": "2015-07-02T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00567",
        "title": "Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge Evolution",
        "authors": [
            "Pooyan Jamshidi",
            "Amir Sharifloo",
            "Claus Pahl",
            "Andreas Metzger",
            "Giovani Estrada"
        ],
        "abstract": "Cloud controllers aim at responding to application demands by automatically scaling the compute resources at runtime to meet performance guarantees and minimize resource costs. Existing cloud controllers often resort to scaling strategies that are codified as a set of adaptation rules. However, for a cloud provider, applications running on top of the cloud infrastructure are more or less black-boxes, making it difficult at design time to define optimal or pre-emptive adaptation rules. Thus, the burden of taking adaptation decisions often is delegated to the cloud application. Yet, in most cases, application developers in turn have limited knowledge of the cloud infrastructure. In this paper, we propose learning adaptation rules during runtime. To this end, we introduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KE learns and modifies fuzzy rules at runtime. The benefit is that for designing cloud controllers, we do not have to rely solely on precise design-time knowledge, which may be difficult to acquire. FQL4KE empowers users to specify cloud controllers by simply adjusting weights representing priorities in system goals instead of specifying complex adaptation rules. The applicability of FQL4KE has been experimentally assessed as part of the cloud application framework ElasticBench. The experimental results indicate that FQL4KE outperforms our previously developed fuzzy controller without learning mechanisms and the native Azure auto-scaling.\n    ",
        "submission_date": "2015-07-02T00:00:00",
        "last_modified_date": "2015-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00996",
        "title": "A New Approach to Probabilistic Programming Inference",
        "authors": [
            "Frank Wood",
            "Jan Willem van de Meent",
            "Vikash Mansinghka"
        ],
        "abstract": "We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is simple to implement and easy to parallelize. It applies to Turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochastic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings methods.\n    ",
        "submission_date": "2015-07-03T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01193",
        "title": "Dependency Recurrent Neural Language Models for Sentence Completion",
        "authors": [
            "Piotr Mirowski",
            "Andreas Vlachos"
        ],
        "abstract": "Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the state-of-the-art models on this task.\n    ",
        "submission_date": "2015-07-05T00:00:00",
        "last_modified_date": "2015-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01269",
        "title": "Semi-supervised Multi-sensor Classification via Consensus-based Multi-View Maximum Entropy Discrimination",
        "authors": [
            "Tianpei Xie",
            "Nasser M. Nasrabadi",
            "Alfred O. Hero III"
        ],
        "abstract": "In this paper, we consider multi-sensor classification when there is a large number of unlabeled samples. The problem is formulated under the multi-view learning framework and a Consensus-based Multi-View Maximum Entropy Discrimination (CMV-MED) algorithm is proposed. By iteratively maximizing the stochastic agreement between multiple classifiers on the unlabeled dataset, the algorithm simultaneously learns multiple high accuracy classifiers. We demonstrate that our proposed method can yield improved performance over previous multi-view learning approaches by comparing performance on three real multi-sensor data sets.\n    ",
        "submission_date": "2015-07-05T00:00:00",
        "last_modified_date": "2015-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01569",
        "title": "Emphatic Temporal-Difference Learning",
        "authors": [
            "A. Rupam Mahmood",
            "Huizhen Yu",
            "Martha White",
            "Richard S. Sutton"
        ],
        "abstract": "Emphatic algorithms are temporal-difference learning algorithms that change their effective state distribution by selectively emphasizing and de-emphasizing their updates on different time steps. Recent works by Sutton, Mahmood and White (2015), and Yu (2015) show that by varying the emphasis in a particular way, these algorithms become stable and convergent under off-policy training with linear function approximation. This paper serves as a unified summary of the available results from both works. In addition, we demonstrate the empirical benefits from the flexibility of emphatic algorithms, including state-dependent discounting, state-dependent bootstrapping, and the user-specified allocation of function approximation resources.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01731",
        "title": "Prediction of Radiation Fog by DNA Computing",
        "authors": [
            "Kumar Sankar Ray",
            "Mandrita Mondal"
        ],
        "abstract": "In this paper we propose a wet lab algorithm for prediction of radiation fog by DNA computing. The concept of DNA computing is essentially exploited for generating the classifier algorithm in the wet lab. The classifier is based on a new concept of similarity based fuzzy reasoning suitable for wet lab implementation. This new concept of similarity based fuzzy reasoning is different from conventional approach to fuzzy reasoning based on similarity measure and also replaces the logical aspect of classical fuzzy reasoning by DNA chemistry. Thus, we add a new dimension to existing forms of fuzzy reasoning by bringing it down to nanoscale. We exploit the concept of massive parallelism of DNA computing by designing this new classifier in the wet lab. This newly designed classifier is very much generalized in nature and apart from prediction of radiation fog this methodology can be applied to other types of data also. To achieve our goal we first fuzzify the given observed parameters in a form of synthetic DNA sequence which is called fuzzy DNA and which handles the vague concept of human reasoning.\n    ",
        "submission_date": "2015-07-07T00:00:00",
        "last_modified_date": "2015-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01839",
        "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding",
        "authors": [
            "Mingbo Ma",
            "Liang Huang",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC.\n    ",
        "submission_date": "2015-07-07T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02020",
        "title": "Generating Navigable Semantic Maps from Social Sciences Corpora",
        "authors": [
            "Thierry Poibeau",
            "Pablo Ruiz"
        ],
        "abstract": "It is now commonplace to observe that we are facing a deluge of online information. Researchers have of course long acknowledged the potential value of this information since digital traces make it possible to directly observe, describe and analyze social facts, and above all the co-evolution of ideas and communities over time. However, most online information is expressed through text, which means it is not directly usable by machines, since computers require structured, organized and typed information in order to be able to manipulate it. Our goal is thus twofold: 1. Provide new natural language processing techniques aiming at automatically extracting relevant information from texts, especially in the context of social sciences, and connect these pieces of information so as to obtain relevant socio-semantic networks; 2. Provide new ways of exploring these socio-semantic networks, thanks to tools allowing one to dynamically navigate these networks, de-construct and re-construct them interactively, from different points of view following the needs expressed by domain experts.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02021",
        "title": "Archaeology in the Digital Age: From Paper to Databases",
        "authors": [
            "Fr\u00e9d\u00e9rique M\u00e9lanie-Becquet",
            "Johan Ferguth",
            "Katherine Gruel",
            "Thierry Poibeau"
        ],
        "abstract": "Research units in archaeology often manage large and precious archives containing various documents, including reports on fieldwork, scholarly studies and reference books. These archives are of course invaluable, recording decades of work, but are generally hard to consult and access. In this context, digitizing full text documents is not enough: information must be formalized, structured and easy to access thanks to friendly user interfaces.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02084",
        "title": "Shedding Light on the Asymmetric Learning Capability of AdaBoost",
        "authors": [
            "Iago Landesa-V\u00e1zquez",
            "Jos\u00e9 Luis Alba-Castro"
        ],
        "abstract": "In this paper, we propose a different insight to analyze AdaBoost. This analysis reveals that, beyond some preconceptions, AdaBoost can be directly used as an asymmetric learning algorithm, preserving all its theoretical properties. A novel class-conditional description of AdaBoost, which models the actual asymmetric behavior of the algorithm, is presented.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02086",
        "title": "The Role of Pragmatics in Legal Norm Representation",
        "authors": [
            "Shashishekar Ramakrishna",
            "Lukasz Gorski",
            "Adrian Paschke"
        ],
        "abstract": "Despite the 'apparent clarity' of a given legal provision, its application may result in an outcome that does not exactly conform to the semantic level of a statute. The vagueness within a legal text is induced intentionally to accommodate all possible scenarios under which such norms should be applied, thus making the role of pragmatics an important aspect also in the representation of a legal norm and reasoning on top of it. The notion of pragmatics considered in this paper does not focus on the aspects associated with judicial decision making. The paper aims to shed light on the aspects of pragmatics in legal linguistics, mainly focusing on the domain of patent law, only from a knowledge representation perspective. The philosophical discussions presented in this paper are grounded based on the legal theories from Grice and Marmor.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02139",
        "title": "Model of human collective decision-making in complex environments",
        "authors": [
            "Giuseppe Carbone",
            "Ilaria Giannoccaro"
        ],
        "abstract": "A continuous-time Markov process is proposed to analyze how a group of humans solves a complex task, consisting in the search of the optimal set of decisions on a fitness landscape. Individuals change their opinions driven by two different forces: (i) the self-interest, which pushes them to increase their own fitness values, and (ii) the social interactions, which push individuals to reduce the diversity of their opinions in order to reach consensus. Results show that the performance of the group is strongly affected by the strength of social interactions and by the level of knowledge of the individuals. Increasing the strength of social interactions improves the performance of the team. However, too strong social interactions slow down the search of the optimal solution and worsen the performance of the group. In particular, we find that the threshold value of the social interaction strength, which leads to the emergence of a superior intelligence of the group, is just the critical threshold at which the consensus among the members sets in. We also prove that a moderate level of knowledge is already enough to guarantee high performance of the group in making decisions.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02154",
        "title": "Double-Base Asymmetric AdaBoost",
        "authors": [
            "Iago Landesa-V\u00e1zquez",
            "Jos\u00e9 Luis Alba-Castro"
        ],
        "abstract": "Based on the use of different exponential bases to define class-dependent error bounds, a new and highly efficient asymmetric boosting scheme, coined as AdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical derivation procedure, unlike most of the other approaches in the literature, our algorithm preserves all the formal guarantees and properties of original (cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive AdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel derivation scheme enables an extremely efficient conditional search procedure, dramatically improving and simplifying the training phase of the algorithm. Experiments, both over synthetic and real datasets, reveal that AdaBoostDB is able to save over 99% training time with regard to Cost-Sensitive AdaBoost, providing the same cost-sensitive results. This computational advantage of AdaBoostDB can make a difference in problems managing huge pools of weak classifiers in which boosting techniques are commonly used.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03663",
        "title": "Twist your logic with TouIST",
        "authors": [
            "Khaled Skander Ben Slimane",
            "Alexis Comte",
            "Olivier Gasquet",
            "Abdelwahab Heba",
            "Olivier Lezaud",
            "Frederic Maris",
            "Mael Valais"
        ],
        "abstract": "SAT provers are powerful tools for solving real-sized logic problems, but using them requires solid programming knowledge and may be seen w.r.t.\\ logic like assembly language w.r.t.\\ programming. Something like a high level language was missing to ease various users to take benefit of these tools. {\\sc \\texttt {TouIST}}\\ aims at filling this gap. It is devoted to propositional logic and its main features are 1) to offer a high-level logic langage for expressing succintly complex formulas (e.g.\\ formulas describing Sudoku rules, planification problems,\\ldots) and 2) to find models to these formulas by using the adequate powerful prover, which the user has no need to know about. It consists in a friendly interface that offers several syntactic facilities and which is connected with some sufficiently powerful provers allowing to automatically solve big instances of difficult problems (such as time-tables or Sudokus). It can interact with various provers: pure SAT solver but also SMT provers (SAT modulo theories - like linear theory of reals, etc) and thus may also be used by beginners for experiencing with pure propositional problems up to graduate students or even researchers for solving planification problems involving big sets of fluents and numerical constraints on them.\n    ",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03719",
        "title": "A New Framework for Distributed Submodular Maximization",
        "authors": [
            "Rafael da Ponte Barbosa",
            "Alina Ene",
            "Huy L. Nguyen",
            "Justin Ward"
        ],
        "abstract": "A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. A lot of recent effort has been devoted to developing distributed algorithms for these problems. However, these results suffer from high number of rounds, suboptimal approximation ratios, or both. We develop a framework for bringing existing algorithms in the sequential setting to the distributed setting, achieving near optimal approximation ratios for many settings in only a constant number of MapReduce rounds. Our techniques also give a fast sequential algorithm for non-monotone maximization subject to a matroid constraint.\n    ",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2016-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04121",
        "title": "Solomonoff Induction Violates Nicod's Criterion",
        "authors": [
            "Jan Leike",
            "Marcus Hutter"
        ],
        "abstract": "Nicod's criterion states that observing a black raven is evidence for the hypothesis H that all ravens are black. We show that Solomonoff induction does not satisfy Nicod's criterion: there are time steps in which observing black ravens decreases the belief in H. Moreover, while observing any computable infinite string compatible with H, the belief in H decreases infinitely often when using the unnormalized Solomonoff prior, but only finitely often when using the normalized Solomonoff prior. We argue that the fault is not with Solomonoff induction; instead we should reject Nicod's criterion.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04125",
        "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part I: Theoretical Perspective",
        "authors": [
            "Iago Landesa-V\u00e1zquez",
            "Jos\u00e9 Luis Alba-Castro"
        ],
        "abstract": "Boosting algorithms have been widely used to tackle a plethora of problems. In the last few years, a lot of approaches have been proposed to provide standard AdaBoost with cost-sensitive capabilities, each with a different focus. However, for the researcher, these algorithms shape a tangled set with diffuse differences and properties, lacking a unifying analysis to jointly compare, classify, evaluate and discuss those approaches on a common basis. In this series of two papers we aim to revisit the various proposals, both from theoretical (Part I) and practical (Part II) perspectives, in order to analyze their specific properties and behavior, with the final goal of identifying the algorithm providing the best and soundest results.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04126",
        "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part II: Empirical Analysis",
        "authors": [
            "Iago Landesa-V\u00e1zquez",
            "Jos\u00e9 Luis Alba-Castro"
        ],
        "abstract": "A lot of approaches, each following a different strategy, have been proposed in the literature to provide AdaBoost with cost-sensitive properties. In the first part of this series of two papers, we have presented these algorithms in a homogeneous notational framework, proposed a clustering scheme for them and performed a thorough theoretical analysis of those approaches with a fully theoretical foundation. The present paper, in order to complete our analysis, is focused on the empirical study of all the algorithms previously presented over a wide range of heterogeneous classification problems. The results of our experiments, confirming the theoretical conclusions, seem to reveal that the simplest approach, just based on cost-sensitive weight initialization, is the one showing the best and soundest results, despite having been recurrently overlooked in the literature.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04285",
        "title": "Learning Action Models: Qualitative Approach",
        "authors": [
            "Thomas Bolander",
            "Nina Gierasimczuk"
        ],
        "abstract": "In dynamic epistemic logic, actions are described using action models. In this paper we introduce a framework for studying learnability of action models from observations. We present first results concerning propositional action models. First we check two basic learnability criteria: finite identifiability (conclusively inferring the appropriate action model in finite time) and identifiability in the limit (inconclusive convergence to the right action model). We show that deterministic actions are finitely identifiable, while non-deterministic actions require more learning power-they are identifiable in the limit. We then move on to a particular learning method, which proceeds via restriction of a space of events within a learning-specific action model. This way of learning closely resembles the well-known update method from dynamic epistemic logic. We introduce several different learning methods suited for finite identifiability of particular types of deterministic actions.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04296",
        "title": "Massively Parallel Methods for Deep Reinforcement Learning",
        "authors": [
            "Arun Nair",
            "Praveen Srinivasan",
            "Sam Blackwell",
            "Cagdas Alcicek",
            "Rory Fearon",
            "Alessandro De Maria",
            "Vedavyas Panneershelvam",
            "Mustafa Suleyman",
            "Charles Beattie",
            "Stig Petersen",
            "Shane Legg",
            "Volodymyr Mnih",
            "Koray Kavukcuoglu",
            "David Silver"
        ],
        "abstract": "We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04635",
        "title": "Black-Box Policy Search with Probabilistic Programs",
        "authors": [
            "Jan-Willem van de Meent",
            "Brooks Paige",
            "David Tolpin",
            "Frank Wood"
        ],
        "abstract": "In this work, we explore how probabilistic programs can be used to represent policies in sequential decision problems. In this formulation, a probabilistic program is a black-box stochastic simulator for both the problem domain and the agent. We relate classic policy gradient techniques to recently introduced black-box variational methods which generalize to probabilistic program inference. We present case studies in the Canadian traveler problem, Rock Sample, and a benchmark for optimal diagnosis inspired by Guess Who. Each study illustrates how programs can efficiently represent policies using moderate numbers of parameters.\n    ",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04808",
        "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models",
        "authors": [
            "Iulian V. Serban",
            "Alessandro Sordoni",
            "Yoshua Bengio",
            "Aaron Courville",
            "Joelle Pineau"
        ],
        "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04811",
        "title": "Lift-Based Bidding in Ad Selection",
        "authors": [
            "Jian Xu",
            "Xuhui Shao",
            "Jianjie Ma",
            "Kuang-chih Lee",
            "Hang Qi",
            "Quan Lu"
        ],
        "abstract": "Real-time bidding (RTB) has become one of the largest online advertising markets in the world. Today the bid price per ad impression is typically decided by the expected value of how it can lead to a desired action event (e.g., registering an account or placing a purchase order) to the advertiser. However, this industry standard approach to decide the bid price does not consider the actual effect of the ad shown to the user, which should be measured based on the performance lift among users who have been or have not been exposed to a certain treatment of ads. In this paper, we propose a new bidding strategy and prove that if the bid price is decided based on the performance lift rather than absolute performance value, advertisers can actually gain more action events. We describe the modeling methodology to predict the performance lift and demonstrate the actual performance gain through blind A/B test with real ad campaigns in an industry-leading Demand-Side Platform (DSP). We also discuss the relationship between attribution models and bidding strategies. We prove that, to move the DSPs to bid based on performance lift, they should be rewarded according to the relative performance lift they contribute.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2016-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04913",
        "title": "Tree-based Visualization and Optimization for Image Collection",
        "authors": [
            "Xintong Han",
            "Chongyang Zhang",
            "Weiyao Lin",
            "Mingliang Xu",
            "Bin Sheng",
            "Tao Mei"
        ],
        "abstract": "The visualization of an image collection is the process of displaying a collection of images on a screen under some specific layout requirements. This paper focuses on an important problem that is not well addressed by the previous methods: visualizing image collections into arbitrary layout shapes while arranging images according to user-defined semantic or visual correlations (e.g., color or object category). To this end, we first propose a property-based tree construction scheme to organize images of a collection into a tree structure according to user-defined properties. In this way, images can be adaptively placed with the desired semantic or visual correlations in the final visualization layout. Then, we design a two-step visualization optimization scheme to further optimize image layouts. As a result, multiple layout effects including layout shape and image overlap ratio can be effectively controlled to guarantee a satisfactory visualization. Finally, we also propose a tree-transfer scheme such that visualization layouts can be adaptively changed when users select different \"images of interest\". We demonstrate the effectiveness of our proposed approach through the comparisons with state-of-the-art visualization techniques.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04997",
        "title": "FRULER: Fuzzy Rule Learning through Evolution for Regression",
        "authors": [
            "I. Rodr\u00edguez-Fdez",
            "M. Mucientes",
            "A. Bugar\u00edn"
        ],
        "abstract": "In regression problems, the use of TSK fuzzy systems is widely extended due to the precision of the obtained models. Moreover, the use of simple linear TSK models is a good choice in many real problems due to the easy understanding of the relationship between the output and input variables. In this paper we present FRULER, a new genetic fuzzy system for automatically learning accurate and simple linguistic TSK fuzzy rule bases for regression problems. In order to reduce the complexity of the learned models while keeping a high accuracy, the algorithm consists of three stages: instance selection, multi-granularity fuzzy discretization of the input variables, and the evolutionary learning of the rule base that uses the Elastic Net regularization to obtain the consequents of the rules. Each stage was validated using 28 real-world datasets and FRULER was compared with three state of the art enetic fuzzy systems. Experimental results show that FRULER achieves the most accurate and simple models compared even with approximative approaches.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05272",
        "title": "Optimizing Phylogenetic Supertrees Using Answer Set Programming",
        "authors": [
            "Laura Koponen",
            "Emilia Oikarinen",
            "Tomi Janhunen",
            "Laura S\u00e4il\u00e4"
        ],
        "abstract": "The supertree construction problem is about combining several phylogenetic trees with possibly conflicting information into a single tree that has all the leaves of the source trees as its leaves and the relationships between the leaves are as consistent with the source trees as possible. This leads to an optimization problem that is computationally challenging and typically heuristic methods, such as matrix representation with parsimony (MRP), are used. In this paper we consider the use of answer set programming to solve the supertree construction problem in terms of two alternative encodings. The first is based on an existing encoding of trees using substructures known as quartets, while the other novel encoding captures the relationships present in trees through direct projections. We use these encodings to compute a genus-level supertree for the family of cats (Felidae). Furthermore, we compare our results to recent supertrees obtained by the MRP method.\n    ",
        "submission_date": "2015-07-19T00:00:00",
        "last_modified_date": "2015-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05388",
        "title": "Dual-normal Logic Programs - the Forgotten Class",
        "authors": [
            "Johannes K. Fichte",
            "Miroslaw Truszczynski",
            "Stefan Woltran"
        ],
        "abstract": "Disjunctive Answer Set Programming is a powerful declarative programming paradigm with complexity beyond NP. Identifying classes of programs for which the consistency problem is in NP is of interest from the theoretical standpoint and can potentially lead to improvements in the design of answer set programming solvers. One of such classes consists of dual-normal programs, where the number of positive body atoms in proper rules is at most one. Unlike other classes of programs, dual-normal programs have received little attention so far. In this paper we study this class. We relate dual-normal programs to propositional theories and to normal programs by presenting several inter-translations. With the translation from dual-normal to normal programs at hand, we introduce the novel class of body-cycle free programs, which are in many respects dual to head-cycle free programs. We establish the expressive power of dual-normal programs in terms of SE- and UE-models, and compare them to normal programs. We also discuss the complexity of deciding whether dual-normal programs are strongly and uniformly equivalent.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05497",
        "title": "RAPS: A Recommender Algorithm Based on Pattern Structures",
        "authors": [
            "Dmitry I. Ignatov",
            "Denis Kornilov"
        ],
        "abstract": "We propose a new algorithm for recommender systems with numeric ratings which is based on Pattern Structures (RAPS). As the input the algorithm takes rating matrix, e.g., such that it contains movies rated by users. For a target user, the algorithm returns a rated list of items (movies) based on its previous ratings and ratings of other users. We compare the results of the proposed algorithm in terms of precision and recall measures with Slope One, one of the state-of-the-art item-based algorithms, on Movie Lens dataset and RAPS demonstrates the best or comparable quality.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05722",
        "title": "A Review of Network Traffic Analysis and Prediction Techniques",
        "authors": [
            "Manish Joshi",
            "Theyazn Hassn Hadi"
        ],
        "abstract": "Analysis and prediction of network traffic has applications in wide comprehensive set of areas and has newly attracted significant number of studies. Different kinds of experiments are conducted and summarized to identify various problems in existing computer network applications. Network traffic analysis and prediction is a proactive approach to ensure secure, reliable and qualitative network communication. Various techniques are proposed and experimented for analyzing network traffic including neural network based techniques to data mining techniques. Similarly, various Linear and non-linear models are proposed for network traffic prediction. Several interesting combinations of network analysis and prediction techniques are implemented to attain efficient and effective results.\n",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2015-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05920",
        "title": "Generalized Totalizer Encoding for Pseudo-Boolean Constraints",
        "authors": [
            "Saurabh Joshi",
            "Ruben Martins",
            "Vasco Manquinho"
        ],
        "abstract": "Pseudo-Boolean constraints, also known as 0-1 Integer Linear Constraints, are used to model many real-world problems. A common approach to solve these constraints is to encode them into a SAT formula. The runtime of the SAT solver on such formula is sensitive to the manner in which the given pseudo-Boolean constraints are encoded. In this paper, we propose generalized Totalizer encoding (GTE), which is an arc-consistency preserving extension of the Totalizer encoding to pseudo-Boolean constraints. Unlike some other encodings, the number of auxiliary variables required for GTE does not depend on the magnitudes of the coefficients. Instead, it depends on the number of distinct combinations of these coefficients. We show the superiority of GTE with respect to other encodings when large pseudo-Boolean constraints have low number of distinct coefficients. Our experimental results also show that GTE remains competitive even when the pseudo-Boolean constraints do not have this characteristic.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06837",
        "title": "YARBUS : Yet Another Rule Based belief Update System",
        "authors": [
            "Jeremy Fix",
            "Herve Frezza-buet"
        ],
        "abstract": "We introduce a new rule based system for belief tracking in dialog systems. Despite the simplicity of the rules being considered, the proposed belief tracker ranks favourably compared to the previous submissions on the second and third Dialog State Tracking challenges. The results of this simple tracker allows to reconsider the performances of previous submissions using more elaborate techniques.\n    ",
        "submission_date": "2015-07-24T00:00:00",
        "last_modified_date": "2015-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07045",
        "title": "The Square Root Agreement Rule for Incentivizing Truthful Feedback on Online Platforms",
        "authors": [
            "Vijay Kamble",
            "Nihar Shah",
            "David Marn",
            "Abhay Parekh",
            "Kannan Ramachandran"
        ],
        "abstract": "A major challenge in obtaining evaluations of products or services on e-commerce platforms is eliciting informative responses in the absence of verifiability. This paper proposes the Square Root Agreement Rule (SRA): a simple reward mechanism that incentivizes truthful responses to objective evaluations on such platforms. In this mechanism, an agent gets a reward for an evaluation only if her answer matches that of her peer, where this reward is inversely proportional to a popularity index of the answer. This index is defined to be the square root of the empirical frequency at which any two agents performing the same evaluation agree on the particular answer across evaluations of similar entities operating on the platform. Rarely agreed-upon answers thus earn a higher reward than answers for which agreements are relatively more common.\n",
        "submission_date": "2015-07-25T00:00:00",
        "last_modified_date": "2022-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07374",
        "title": "A genetic algorithm for autonomous navigation in partially observable domain",
        "authors": [
            "Maxim Borisyak",
            "Andrey Ustyuzhanin"
        ],
        "abstract": "The problem of autonomous navigation is one of the basic problems for robotics. Although, in general, it may be challenging when an autonomous vehicle is placed into partially observable domain. In this paper we consider simplistic environment model and introduce a navigation algorithm based on Learning Classifier System.\n    ",
        "submission_date": "2015-07-27T00:00:00",
        "last_modified_date": "2015-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07677",
        "title": "Computation of Stackelberg Equilibria of Finite Sequential Games",
        "authors": [
            "Branislav Bosansky",
            "Simina Branzei",
            "Kristoffer Arnsfelt Hansen",
            "Peter Bro Miltersen",
            "Troels Bjerre Sorensen"
        ],
        "abstract": "The Stackelberg equilibrium solution concept describes optimal strategies to commit to: Player 1 (termed the leader) publicly commits to a strategy and Player 2 (termed the follower) plays a best response to this strategy (ties are broken in favor of the leader). We study Stackelberg equilibria in finite sequential games (or extensive-form games) and provide new exact algorithms, approximate algorithms, and hardness results for several classes of these sequential games.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07870",
        "title": "Detect & Describe: Deep learning of bank stress in the news",
        "authors": [
            "Samuel R\u00f6nnqvist",
            "Peter Sarlin"
        ],
        "abstract": "News is a pertinent source of information on financial risks and stress factors, which nevertheless is challenging to harness due to the sparse and unstructured nature of natural text. We propose an approach based on distributional semantics and deep learning with neural networks to model and link text to a scarce set of bank distress events. Through unsupervised training, we learn semantic vector representations of news articles as predictors of distress events. The predictive model that we learn can signal coinciding stress with an aggregated index at bank or European level, while crucially allowing for automatic extraction of text descriptions of the events, based on passages with high stress levels. The method offers insight that models based on other types of data cannot provide, while offering a general means for interpreting this type of semantic-predictive model. We model bank distress with data on 243 events and 6.6M news articles for 101 large European banks.\n    ",
        "submission_date": "2015-07-25T00:00:00",
        "last_modified_date": "2015-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07998",
        "title": "Document Embedding with Paragraph Vectors",
        "authors": [
            "Andrew M. Dai",
            "Christopher Olah",
            "Quoc V. Le"
        ],
        "abstract": "Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08467",
        "title": "A Model for Foraging Ants, Controlled by Spiking Neural Networks and Double Pheromones",
        "authors": [
            "Cristian Jimenez-Romero",
            "David Sousa-Rodrigues",
            "Jeffrey H. Johnson",
            "Vitorino Ramos"
        ],
        "abstract": "A model of an Ant System where ants are controlled by a spiking neural circuit and a second order pheromone mechanism in a foraging task is presented. A neural circuit is trained for individual ants and subsequently the ants are exposed to a virtual environment where a swarm of ants performed a resource foraging task. The model comprises an associative and unsupervised learning strategy for the neural circuit of the ant. The neural circuit adapts to the environment by means of classical conditioning. The initially unknown environment includes different types of stimuli representing food and obstacles which, when they come in direct contact with the ant, elicit a reflex response in the motor neural system of the ant: moving towards or away from the source of the stimulus. The ants are released on a landscape with multiple food sources where one ant alone would have difficulty harvesting the landscape to maximum efficiency. The introduction of a double pheromone mechanism yields better results than traditional ant colony optimization strategies. Traditional ant systems include mainly a positive reinforcement pheromone. This approach uses a second pheromone that acts as a marker for forbidden paths (negative feedback). This blockade is not permanent and is controlled by the evaporation rate of the pheromones. The combined action of both pheromones acts as a collective stigmergic memory of the swarm, which reduces the search space of the problem. This paper explores how the adaptation and learning abilities observed in biologically inspired cognitive architectures is synergistically enhanced by swarm optimization strategies. The model portraits two forms of artificial intelligent behaviour: at the individual level the spiking neural network is the main controller and at the collective level the pheromone distribution is a map towards the solution emerged by the colony.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08482",
        "title": "Framework for learning agents in quantum environments",
        "authors": [
            "Vedran Dunjko",
            "Jacob M. Taylor",
            "Hans J. Briegel"
        ],
        "abstract": "In this paper we provide a broad framework for describing learning agents in general quantum environments. We analyze the types of classically specified environments which allow for quantum enhancements in learning, by contrasting environments to quantum oracles. We show that whether or not quantum improvements are at all possible depends on the internal structure of the quantum environment. If the environments are constructed and the internal structure is appropriately chosen, or if the agent has limited capacities to influence the internal states of the environment, we show that improvements in learning times are possible in a broad range of scenarios. Such scenarios we call luck-favoring settings. The case of constructed environments is particularly relevant for the class of model-based learning agents, where our results imply a near-generic improvement.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08717",
        "title": "Systematic Verification of the Modal Logic Cube in Isabelle/HOL",
        "authors": [
            "Christoph Benzm\u00fcller",
            "Maximilian Claus",
            "Nik Sultana"
        ],
        "abstract": "We present an automated verification of the well-known modal logic cube in Isabelle/HOL, in which we prove the inclusion relations between the cube's logics using automated reasoning tools. Prior work addresses this problem but without restriction to the modal logic cube, and using encodings in first-order logic in combination with first-order automated theorem provers. In contrast, our solution is more elegant, transparent and effective. It employs an embedding of quantified modal logic in classical higher-order logic. Automated reasoning tools, such as Sledgehammer with LEO-II, Satallax and CVC4, Metis and Nitpick, are employed to achieve full automation. Though successful, the experiments also motivate some technical improvements in the Isabelle/HOL tool.\n\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08726",
        "title": "Robustness in sparse linear models: relative efficiency based on robust approximate message passing",
        "authors": [
            "Jelena Bradic"
        ],
        "abstract": "Understanding efficiency in high dimensional linear models is a longstanding problem of interest. Classical work with smaller dimensional problems dating back to Huber and Bickel has illustrated the benefits of efficient loss functions. When the number of parameters $p$ is of the same order as the sample size $n$, $p \\approx n$, an efficiency pattern different from the one of Huber was recently established. In this work, we consider the effects of model selection on the estimation efficiency of penalized methods. In particular, we explore whether sparsity, results in new efficiency patterns when $p > n$. In the interest of deriving the asymptotic mean squared error for regularized M-estimators, we use the powerful framework of approximate message passing. We propose a novel, robust and sparse approximate message passing algorithm (RAMP), that is adaptive to the error distribution. Our algorithm includes many non-quadratic and non-differentiable loss functions. We derive its asymptotic mean squared error and show its convergence, while allowing $p, n, s \\to \\infty$, with $n/p \\in (0,1)$ and $n/s \\in (1,\\infty)$. We identify new patterns of relative efficiency regarding a number of penalized $M$ estimators, when $p$ is much larger than $n$. We show that the classical information bound is no longer reachable, even for light--tailed error distributions. We show that the penalized least absolute deviation estimator dominates the penalized least square estimator, in cases of heavy--tailed distributions. We observe this pattern for all choices of the number of non-zero parameters $s$, both $s \\leq n$ and $s \\approx n$. In non-penalized problems where $s =p \\approx n$, the opposite regime holds. Therefore, we discover that the presence of model selection significantly changes the efficiency patterns.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2016-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08750",
        "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
        "authors": [
            "Junhyuk Oh",
            "Xiaoxiao Guo",
            "Honglak Lee",
            "Richard Lewis",
            "Satinder Singh"
        ],
        "abstract": "Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00037",
        "title": "Neuro-Fuzzy Algorithmic (NFA) Models and Tools for Estimation",
        "authors": [
            "Danny Ho",
            "Luiz Fernando Capretz",
            "Xishi Huang",
            "Jing Ren"
        ],
        "abstract": "Accurate estimation such as cost estimation, quality estimation and risk analysis is a major issue in management. We propose a patent pending soft computing framework to tackle this challenging problem. Our generic framework is independent of the nature and type of estimation. It consists of neural network, fuzzy logic, and an algorithmic estimation model. We made use of the Constructive Cost Model (COCOMO), Analysis of Variance (ANOVA), and Function Point Analysis as the algorithmic models and validated the accuracy of the Neuro-Fuzzy Algorithmic (NFA) Model in software cost estimation using industrial project data. Our model produces more accurate estimation than using an algorithmic model alone. We also discuss the prototypes of our tools that implement the NFA Model. We conclude with our roadmap and direction to enrich the model in tackling different estimation challenges.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00059",
        "title": "Mixed Logical and Probabilistic Reasoning for Planning and Explanation Generation in Robotics",
        "authors": [
            "Zenon Colaco",
            "Mohan Sridharan"
        ],
        "abstract": "Robots assisting humans in complex domains have to represent knowledge and reason at both the sensorimotor level and the social level. The architecture described in this paper couples the non-monotonic logical reasoning capabilities of a declarative language with probabilistic belief revision, enabling robots to represent and reason with qualitative and quantitative descriptions of knowledge and degrees of belief. Specifically, incomplete domain knowledge, including information that holds in all but a few exceptional situations, is represented as a Answer Set Prolog (ASP) program. The answer set obtained by solving this program is used for inference, planning, and for jointly explaining (a) unexpected action outcomes due to exogenous actions and (b) partial scene descriptions extracted from sensor input. For any given task, each action in the plan contained in the answer set is executed probabilistically. The subset of the domain relevant to the action is identified automatically, and observations extracted from sensor inputs perform incremental Bayesian updates to a belief distribution defined over this domain subset, with highly probable beliefs being committed to the ASP program. The architecture's capabilities are illustrated in simulation and on a mobile robot in the context of a robot waiter operating in the dining room of a restaurant.\n    ",
        "submission_date": "2015-08-01T00:00:00",
        "last_modified_date": "2015-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00116",
        "title": "Extending SROIQ with Constraint Networks and Grounded Circumscription",
        "authors": [
            "Arjun Bhardwaj"
        ],
        "abstract": "Developments in semantic web technologies have promoted ontological encoding of knowledge from diverse domains. However, modelling many practical domains requires more expressiveness than what the standard description logics (most prominently SROIQ) support. In this paper, we extend the expressive DL SROIQ with constraint networks (resulting in the logic SROIQc) and grounded circumscription (resulting in the logic GC-SROIQ). Applications of constraint modelling include embedding ontologies with temporal or spatial information, while those of grounded circumscription include defeasible inference and closed world reasoning.\n",
        "submission_date": "2015-08-01T00:00:00",
        "last_modified_date": "2015-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00457",
        "title": "Evolutionary Multimodal Optimization: A Short Survey",
        "authors": [
            "Ka-Chun Wong"
        ],
        "abstract": "Real world problems always have different multiple solutions. For instance, optical engineers need to tune the recording parameters to get as many optimal solutions as possible for multiple trials in the varied-line-spacing holographic grating design problem. Unfortunately, most traditional optimization techniques focus on solving for a single optimal solution. They need to be applied several times; yet all solutions are not guaranteed to be found. Thus the multimodal optimization problem was proposed. In that problem, we are interested in not only a single optimal point, but also the others. With strong parallel search capability, evolutionary algorithms are shown to be particularly effective in solving this type of problem. In particular, the evolutionary algorithms for multimodal optimization usually not only locate multiple optima in a single run, but also preserve their population diversity throughout a run, resulting in their global optimization ability on multimodal functions. In addition, the techniques for multimodal optimization are borrowed as diversity maintenance techniques to other problems. In this chapter, we describe and review the state-of-the-arts evolutionary algorithms for multimodal optimization in terms of methodology, benchmarking, and application.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00507",
        "title": "A Weakly Supervised Learning Approach based on Spectral Graph-Theoretic Grouping",
        "authors": [
            "Tameem Adel",
            "Alexander Wong",
            "Daniel Stashuk"
        ],
        "abstract": "In this study, a spectral graph-theoretic grouping strategy for weakly supervised classification is introduced, where a limited number of labelled samples and a larger set of unlabelled samples are used to construct a larger annotated training set composed of strongly labelled and weakly labelled samples. The inherent relationship between the set of strongly labelled samples and the set of unlabelled samples is established via spectral grouping, with the unlabelled samples subsequently weakly annotated based on the strongly labelled samples within the associated spectral groups. A number of similarity graph models for spectral grouping, including two new similarity graph models introduced in this study, are explored to investigate their performance in the context of weakly supervised classification in handling different types of data. Experimental results using benchmark datasets as well as real EMG datasets demonstrate that the proposed approach to weakly supervised classification can provide noticeable improvements in classification performance, and that the proposed similarity graph models can lead to ultimate learning results that are either better than or on a par with existing similarity graph models in the context of spectral grouping for weakly supervised classification.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00655",
        "title": "Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance based High Dimensional Two Sample Testing",
        "authors": [
            "Aaditya Ramdas",
            "Sashank J. Reddi",
            "Barnabas Poczos",
            "Aarti Singh",
            "Larry Wasserman"
        ],
        "abstract": "Nonparametric two sample testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. We refer to the most common settings as mean difference alternatives (MDA), for testing differences only in first moments, and general difference alternatives (GDA), which is about testing for any difference in distributions. A large number of test statistics have been proposed for both these settings. This paper connects three classes of statistics - high dimensional variants of Hotelling's t-test, statistics based on Reproducing Kernel Hilbert Spaces, and energy statistics based on pairwise distances. We ask the question: how much statistical power do popular kernel and distance based tests for GDA have when the unknown distributions differ in their means, compared to specialized tests for MDA?\n",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00689",
        "title": "Factor Graphs for Quantum Probabilities",
        "authors": [
            "Hans-Andrea Loeliger",
            "Pascal O. Vontobel"
        ],
        "abstract": "A factor-graph representation of quantum-mechanical probabilities (involving any number of measurements) is proposed. Unlike standard statistical models, the proposed representation uses auxiliary variables (state variables) that are not random variables. All joint probability distributions are marginals of some complex-valued function $q$, and it is demonstrated how the basic concepts of quantum mechanics relate to factorizations and marginals of $q$.\n    ",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01083",
        "title": "Ontology Bulding vs Data Harvesting and Cleaning for Smart-city Services",
        "authors": [
            "Pierfrancesco Bellini",
            "Paolo Nesi",
            "Nadia Rauch"
        ],
        "abstract": "Presently, a very large number of public and private data sets are available around the local governments. In most cases, they are not semantically interoperable and a huge human effort is needed to create integrated ontologies and knowledge base for smart city. Smart City ontology is not yet standardized, and a lot of research work is needed to identify models that can easily support the data reconciliation, the management of the complexity and reasoning. In this paper, a system for data ingestion and reconciliation smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing a big volume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to smart-city ontology and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users. The paper presents the process adopted to produce the ontology and the knowledge base and the mechanisms adopted for the verification, reconciliation and validation. Some examples about the possible usage of the coherent knowledge base produced are also offered and are accessible from the RDF-Store.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01086",
        "title": "Km4City Ontology Building vs Data Harvesting and Cleaning for Smart-city Services",
        "authors": [
            "Pierfrancesco Bellini",
            "Monica Benigni",
            "Riccardo Billero",
            "Paolo Nesi",
            "Nadia Rauch"
        ],
        "abstract": "Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies and knowledge base for smart city. Smart City ontology is not yet standardized, and a lot of research work is needed to identify models that can easily support the data reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system for data ingestion and reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing a big data volume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and the big data architecture for the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-Store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01192",
        "title": "Mining for Causal Relationships: A Data-Driven Study of the Islamic State",
        "authors": [
            "Andrew Stanton",
            "Amanda Thart",
            "Ashish Jain",
            "Priyank Vyas",
            "Arpan Chatterjee",
            "Paulo Shakarian"
        ],
        "abstract": "The Islamic State of Iraq and al-Sham (ISIS) is a dominant insurgent group operating in Iraq and Syria that rose to prominence when it took over Mosul in June, 2014. In this paper, we present a data-driven approach to analyzing this group using a dataset consisting of 2200 incidents of military activity surrounding ISIS and the forces that oppose it (including Iraqi, Syrian, and the American-led coalition). We combine ideas from logic programming and causal reasoning to mine for association rules for which we present evidence of causality. We present relationships that link ISIS vehicle-bourne improvised explosive device (VBIED) activity in Syria with military operations in Iraq, coalition air strikes, and ISIS IED activity, as well as rules that may serve as indicators of spikes in indirect fire, suicide attacks, and arrests.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01306",
        "title": "Replication and Generalization of PRECISE",
        "authors": [
            "Michael Minock",
            "Nils Everling"
        ],
        "abstract": "This report describes an initial replication study of the PRECISE system and develops a clearer, more formal description of the approach. Based on our evaluation, we conclude that the PRECISE results do not fully replicate. However the formalization developed here suggests a road map to further enhance and extend the approach pioneered by PRECISE.\n",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01345",
        "title": "Fuzzy Logic Based Direct Torque Control Of Induction Motor With Space Vector Modulation",
        "authors": [
            "Fatih Korkmaz",
            "\u0130smail Topalo\u011flu",
            "Hayati Mamur"
        ],
        "abstract": "The induction motors have wide range of applications for due to its well-known advantages like brushless structures, low costs and robust performances. Over the past years, many kind of control methods are proposed for the induction motors and direct torque control has gained huge importance inside of them due to fast dynamic torque responses and simple control structures. However, the direct torque control method has still some handicaps against the other control methods and most of the important of these handicaps is high torque ripple. This paper suggests a new approach, Fuzzy logic based space vector modulation, on the direct torque controlled induction motors and aim of the approach is to overcome high torque ripple disadvantages of conventional direct torque control. In order to test and compare the proposed direct torque control method with conventional direct torque control method simulations, in Matlab/Simulink,have been carried out in different working conditions. The simulation results showed that a significant improvement in the dynamic torque and speed responses when compared to the conventional direct torque control method.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01447",
        "title": "Using Linguistic Analysis to Translate Arabic Natural Language Queries to SPARQL",
        "authors": [
            "Iyad AlAgha"
        ],
        "abstract": "The logic-based machine-understandable framework of the Semantic Web often challenges naive users when they try to query ontology-based knowledge bases. Existing research efforts have approached this problem by introducing Natural Language (NL) interfaces to ontologies. These NL interfaces have the ability to construct SPARQL queries based on NL user queries. However, most efforts were restricted to queries expressed in English, and they often benefited from the advancement of English NLP tools. However, little research has been done to support querying the Arabic content on the Semantic Web by using NL queries. This paper presents a domain-independent approach to translate Arabic NL queries to SPARQL by leveraging linguistic analysis. Based on a special consideration on Noun Phrases (NPs), our approach uses a language parser to extract NPs and the relations from Arabic parse trees and match them to the underlying ontology. It then utilizes knowledge in the ontology to group NPs into triple-based representations. A SPARQL query is finally generated by extracting targets and modifiers, and interpreting them into SPARQL. The interpretation of advanced semantic features including negation, conjunctive and disjunctive modifiers is also supported. The approach was evaluated by using two datasets consisting of OWL test data and queries, and the obtained results have confirmed its feasibility to translate Arabic NL queries to SPARQL.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02354",
        "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning",
        "authors": [
            "Jianpeng Cheng",
            "Dimitri Kartsaklis"
        ],
        "abstract": "Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02626",
        "title": "Answering Fuzzy Conjunctive Queries over Finitely Valued Fuzzy Ontologies",
        "authors": [
            "Stefan Borgwardt",
            "Theofilos Mailis",
            "Rafael Pe\u00f1aloza",
            "Anni-Yasmin Turhan"
        ],
        "abstract": "Fuzzy Description Logics (DLs) provide a means for representing vague knowledge about an application domain. In this paper, we study fuzzy extensions of conjunctive queries (CQs) over the DL $\\mathcal{SROIQ}$ based on finite chains of degrees of truth. To answer such queries, we extend a well-known technique that reduces the fuzzy ontology to a classical one, and use classical DL reasoners as a black box. We improve the complexity of previous reduction techniques for finitely valued fuzzy DLs, which allows us to prove tight complexity results for answering certain kinds of fuzzy CQs. We conclude with an experimental evaluation of a prototype implementation, showing the feasibility of our approach.\n    ",
        "submission_date": "2015-08-11T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03064",
        "title": "Multiple-Path Selection for new Highway Alignments using Discrete Algorithms",
        "authors": [
            "Yasha Pushak",
            "Warren Hare",
            "Yves Lucet"
        ],
        "abstract": "This paper addresses the problem of finding multiple near-optimal, spatially-dissimilar paths that can be considered as alternatives in the decision making process, for finding optimal corridors in which to construct a new road. We further consider combinations of techniques for reducing the costs associated with the computation and increasing the accuracy of the cost formulation. Numerical results for five algorithms to solve the dissimilar multipath problem show that a \"bidirectional approach\" yields the fastest running times and the most robust algorithm. Further modifications of the algorithms to reduce the running time were tested and it is shown that running time can be reduced by an average of 56 percent without compromising the quality of the results.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03846",
        "title": "Schema Independent Relational Learning",
        "authors": [
            "Jose Picado",
            "Arash Termehchy",
            "Alan Fern",
            "Parisa Ataei"
        ],
        "abstract": "Learning novel concepts and relations from relational databases is an important problem with many applications in database systems and machine learning. Relational learning algorithms learn the definition of a new relation in terms of existing relations in the database. Nevertheless, the same data set may be represented under different schemas for various reasons, such as efficiency, data quality, and usability. Unfortunately, the output of current relational learning algorithms tends to vary quite substantially over the choice of schema, both in terms of learning accuracy and efficiency. This variation complicates their off-the-shelf application. In this paper, we introduce and formalize the property of schema independence of relational learning algorithms, and study both the theoretical and empirical dependence of existing algorithms on the common class of (de) composition schema transformations. We study both sample-based learning algorithms, which learn from sets of labeled examples, and query-based algorithms, which learn by asking queries to an oracle. We prove that current relational learning algorithms are generally not schema independent. For query-based learning algorithms we show that the (de) composition transformations influence their query complexity. We propose Castor, a sample-based relational learning algorithm that achieves schema independence by leveraging data dependencies. We support the theoretical results with an empirical study that demonstrates the schema dependence/independence of several algorithms on existing benchmark and real-world datasets under (de) compositions.\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03891",
        "title": "REBA: A Refinement-Based Architecture for Knowledge Representation and Reasoning in Robotics",
        "authors": [
            "Mohan Sridharan",
            "Michael Gelfond",
            "Shiqi Zhang",
            "Jeremy Wyatt"
        ],
        "abstract": "This paper describes an architecture for robots that combines the complementary strengths of probabilistic graphical models and declarative programming to represent and reason with logic-based and probabilistic descriptions of uncertainty and domain knowledge. An action language is extended to support non-boolean fluents and non-deterministic causal laws. This action language is used to describe tightly-coupled transition diagrams at two levels of granularity, with a fine-resolution transition diagram defined as a refinement of a coarse-resolution transition diagram of the domain. The coarse-resolution system description, and a history that includes (prioritized) defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action, the robot automatically zooms to the part of the fine-resolution transition diagram relevant to this action. A probabilistic representation of the uncertainty in sensing and actuation is then included in this zoomed fine-resolution system description, and used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract action as a sequence of concrete actions, with the corresponding observations being recorded in the coarse-resolution history and used for subsequent reasoning. The architecture is evaluated in simulation and on a mobile robot moving objects in an indoor domain, to show that it supports reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2018-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04112",
        "title": "Molding CNNs for text: non-linear, non-consecutive convolutions",
        "authors": [
            "Tao Lei",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ],
        "abstract": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04159",
        "title": "Reasoning in complex environments with the SelectScript declarative language",
        "authors": [
            "Andr\u00e9 Dietrich",
            "Sebastian Zug",
            "Luigi Nardi",
            "J\u00f6rg Kaiser"
        ],
        "abstract": "SelectScript is an extendable, adaptable, and declarative domain-specific language aimed at information retrieval from simulation environments and robotic world models in an SQL-like manner. In this work we have extended the language in two directions. First, we have implemented hierarchical queries; second, we improve efficiency enabling manual design space exploration on different \"search\" strategies. We demonstrate the applicability of such extensions in two application problems; the basic language concepts are explained by solving the classical problem of the Towers of Hanoi and then a common path planning problem in a complex 3D environment is implemented.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04186",
        "title": "Distributed Deep Q-Learning",
        "authors": [
            "Hao Yi Ong",
            "Kevin Chavez",
            "Augustus Hong"
        ],
        "abstract": "We propose a distributed deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is based on the deep Q-network, a convolutional neural network trained with a variant of Q-learning. Its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state. To distribute the deep Q-network training, we adapt the DistBelief software framework to the context of efficiently training reinforcement learning agents. As a result, the method is completely asynchronous and scales well with the number of machines. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to achieve reasonable success on a simple game with minimal parameter tuning.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04395",
        "title": "End-to-End Attention-based Large Vocabulary Speech Recognition",
        "authors": [
            "Dzmitry Bahdanau",
            "Jan Chorowski",
            "Dmitriy Serdyuk",
            "Philemon Brakel",
            "Yoshua Bengio"
        ],
        "abstract": "Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04522",
        "title": "Fishing out Winners from Vote Streams",
        "authors": [
            "Arnab Bhattacharyya",
            "Palash Dey"
        ],
        "abstract": "We investigate the problem of winner determination from computational social choice theory in the data stream model. Specifically, we consider the task of summarizing an arbitrarily ordered stream of $n$ votes on $m$ candidates into a small space data structure so as to be able to obtain the winner determined by popular voting rules. As we show, finding the exact winner requires storing essentially all the votes. So, we focus on the problem of finding an {\\em $\\eps$-winner}, a candidate who could win by a change of at most $\\eps$ fraction of the votes. We show non-trivial upper and lower bounds on the space complexity of $\\eps$-winner determination for several voting rules, including $k$-approval, $k$-veto, scoring rules, approval, maximin, Bucklin, Copeland, and plurality with run off.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04561",
        "title": "Memetics and Neural Models of Conspiracy Theories",
        "authors": [
            "W\u0142odzis\u0142aw Duch"
        ],
        "abstract": "Conspiracy theories, or in general seriously distorted beliefs, are widespread. How and why are they formed in the brain is still more a matter of speculation rather than science. In this paper one plausible mechanisms is investigated: rapid freezing of high neuroplasticity (RFHN). Emotional arousal increases neuroplasticity and leads to creation of new pathways spreading neural activation. Using the language of neurodynamics a meme is defined as quasi-stable associative memory attractor state. Depending on the temporal characteristics of the incoming information and the plasticity of the network, memory may self-organize creating memes with large attractor basins, linking many unrelated input patterns. Memes with fake rich associations distort relations between memory states. Simulations of various neural network models trained with competitive Hebbian learning (CHL) on stationary and non-stationary data lead to the same conclusion: short learning with high plasticity followed by rapid decrease of plasticity leads to memes with large attraction basins, distorting input pattern representations in associative memory. Such system-level models may be used to understand creation of distorted beliefs and formation of conspiracy memes, understood as strong attractor states of the neurodynamics.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2021-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05117",
        "title": "The backtracking survey propagation algorithm for solving random K-SAT problems",
        "authors": [
            "Raffaele Marino",
            "Giorgio Parisi",
            "Federico Ricci-Tersenghi"
        ],
        "abstract": "Discrete combinatorial optimization has a central role in many scientific disciplines, however, for hard problems we lack linear time algorithms that would allow us to solve very large instances. Moreover, it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve. Here we study random K-satisfiability problems with $K=3,4$, which are known to be very hard close to the SAT-UNSAT threshold, where problems stop having solutions. We show that the backtracking survey propagation algorithm, in a time practically linear in the problem size, is able to find solutions very close to the threshold, in a region unreachable by any other algorithm. All solutions found have no frozen variables, thus supporting the conjecture that only unfrozen solutions can be found in linear time, and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05328",
        "title": "Multi-agent Reinforcement Learning with Sparse Interactions by Negotiation and Knowledge Transfer",
        "authors": [
            "Luowei Zhou",
            "Pei Yang",
            "Chunlin Chen",
            "Yang Gao"
        ],
        "abstract": "Reinforcement learning has significant applications for multi-agent systems, especially in unknown dynamic environments. However, most multi-agent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multi-agent problems. In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSI) is presented. In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the non-strict Equilibrium Dominating Strategy Profile (non-strict EDSP) or Meta equilibrium for their joint actions. The presented NegoSI algorithm consists of four parts: the equilibrium-based framework for sparse interactions, the negotiation for the equilibrium set, the minimum variance method for selecting one joint action and the knowledge transfer of local Q-values. In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively. To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: steps of each episode (SEE), rewards of each episode (REE) and average runtime (AR). The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm. Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms.\n    ",
        "submission_date": "2015-08-21T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05608",
        "title": "The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms",
        "authors": [
            "Yahel David",
            "Nahum Shimkin"
        ],
        "abstract": "We consider the Max $K$-Armed Bandit problem, where a learning agent is faced with several sources (arms) of items (rewards), and interested in finding the best item overall. At each time step the agent chooses an arm, and obtains a random real valued reward. The rewards of each arm are assumed to be i.i.d., with an unknown probability distribution that generally differs among the arms. Under the PAC framework, we provide lower bounds on the sample complexity of any $(\\epsilon,\\delta)$-correct algorithm, and propose algorithms that attain this bound up to logarithmic factors. We compare the performance of this multi-arm algorithms to the variant in which the arms are not distinguishable by the agent and are chosen randomly at each stage. Interestingly, when the maximal rewards of the arms happen to be similar, the latter approach may provide better performance.\n    ",
        "submission_date": "2015-08-23T00:00:00",
        "last_modified_date": "2015-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06013",
        "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution",
        "authors": [
            "Zeinab Bahmani",
            "Leopoldo Bertossi",
            "Nikolaos Vasiloglou"
        ],
        "abstract": "Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called matching dependencies (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating three components of ER: (a) Classifiers for duplicate/non-duplicate record pairs built using machine learning (ML) techniques, (b) MDs for supporting both the blocking phase of ML and the merge itself; and (c) The use of the declarative language LogiQL -an extended form of Datalog supported by the LogicBlox platform- for data processing, and the specification and enforcement of MDs.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06096",
        "title": "Unsatisfiable Cores and Lower Bounding for Constraint Programming",
        "authors": [
            "Nicholas Downing",
            "Thibaut Feydy",
            "Peter J. Stuckey"
        ],
        "abstract": "Constraint Programming (CP) solvers typically tackle optimization problems by repeatedly finding solutions to a problem while placing tighter and tighter bounds on the solution cost. This approach is somewhat naive, especially for soft-constraint optimization problems in which the soft constraints are mostly satisfied. Unsatisfiable-core approaches to solving soft constraint problems in SAT (e.g. MAXSAT) force all soft constraints to be hard initially. When solving fails they return an unsatisfiable core, as a set of soft constraints that cannot hold simultaneously. These are reverted to soft and solving continues. Since lazy clause generation solvers can also return unsatisfiable cores we can adapt this approach to constraint programming. We adapt the original MAXSAT unsatisfiable core solving approach to be usable for constraint programming and define a number of extensions. Experimental results show that our methods are beneficial on a broad class of CP-optimization benchmarks involving soft constraints, cardinality or preferences.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06161",
        "title": "Robot Language Learning, Generation, and Comprehension",
        "authors": [
            "Daniel Paul Barrett",
            "Scott Alan Bronikowski",
            "Haonan Yu",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We present a unified framework which supports grounding natural-language semantics in robotic driving. This framework supports acquisition (learning grounded meanings of nouns and prepositions from human annotation of robotic driving paths), generation (using such acquired meanings to generate sentential description of new robotic driving paths), and comprehension (using such acquired meanings to support automated driving to accomplish navigational goals specified in natural language). We evaluate the performance of these three tasks by having independent human judges rate the semantic fidelity of the sentences associated with paths, achieving overall average correctness of 94.6% and overall average completeness of 85.6%.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06191",
        "title": "A Neuro-Fuzzy Method to Improving Backfiring Conversion Ratios",
        "authors": [
            "Justin Wong",
            "Danny Ho",
            "Luiz Fernando Capretz"
        ],
        "abstract": "Software project estimation is crucial aspect in delivering software on time and on budget. Software size is an important metric in determining the effort, cost, and productivity. Today, source lines of code and function point are the most used sizing metrics. Backfiring is a well-known technique for converting between function points and source lines of code. However when backfiring is used, there is a high margin of error. This study introduces a method to improve the accuracy of backfiring. Intelligent systems have been used in software prediction models to improve performance over traditional techniques. For this reason, a hybrid Neuro-Fuzzy is used because it takes advantages of the neural networks learning and fuzzy logic human-like reasoning. This paper describes an improved backfiring technique which uses Neuro-Fuzzy and compares the new method against the default conversion ratios currently used by software practitioners.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06235",
        "title": "Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm",
        "authors": [
            "Daniel Khashabi",
            "John Wieting",
            "Jeffrey Yufei Liu",
            "Feng Liang"
        ],
        "abstract": "In this paper, we propose a model-based clustering method (TVClust) that robustly incorporates noisy side information as soft-constraints and aims to seek a consensus between side information and the observed data. Our method is based on a nonparametric Bayesian hierarchical model that combines the probabilistic model for the data instance and the one for the side-information. An efficient Gibbs sampling algorithm is proposed for posterior inference. Using the small-variance asymptotics of our probabilistic model, we then derive a new deterministic clustering algorithm (RDP-means). It can be viewed as an extension of K-means that allows for the inclusion of side information and has the additional property that the number of clusters does not need to be specified a priori. Empirical studies have been carried out to compare our work with many constrained clustering algorithms from the literature on both a variety of data sets and under a variety of conditions such as using noisy side information and erroneous k values. The results of our experiments show strong results for our probabilistic and deterministic approaches under these conditions when compared to other algorithms in the literature.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06538",
        "title": "Causality, Information and Biological Computation: An algorithmic software approach to life, disease and the immune system",
        "authors": [
            "Hector Zenil",
            "Angelika Schmidt",
            "Jesper Tegn\u00e9r"
        ],
        "abstract": "Biology has taken strong steps towards becoming a computer science aiming at reprogramming nature after the realisation that nature herself has reprogrammed organisms by harnessing the power of natural selection and the digital prescriptive nature of replicating DNA. Here we further unpack ideas related to computability, algorithmic information theory and software engineering, in the context of the extent to which biology can be (re)programmed, and with how we may go about doing so in a more systematic way with all the tools and concepts offered by theoretical computer science in a translation exercise from computing to molecular biology and back. These concepts provide a means to a hierarchical organization thereby blurring previously clear-cut lines between concepts like matter and life, or between tumour types that are otherwise taken as different and may not have however a different cause. This does not diminish the properties of life or make its components and functions less interesting. On the contrary, this approach makes for a more encompassing and integrated view of nature, one that subsumes observer and observed within the same system, and can generate new perspectives and tools with which to view complex diseases like cancer, approaching them afresh from a software-engineering viewpoint that casts evolution in the role of programmer, cells as computing machines, DNA and genes as instructions and computer programs, viruses as hacking devices, the immune system as a software debugging tool, and diseases as an information-theoretic battlefield where all these forces deploy. We show how information theory and algorithmic programming may explain fundamental mechanisms of life and death.\n    ",
        "submission_date": "2015-08-24T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06781",
        "title": "Computing Stable Coalitions: Approximation Algorithms for Reward Sharing",
        "authors": [
            "Elliot Anshelevich",
            "Shreyas Sekar"
        ],
        "abstract": "Consider a setting where selfish agents are to be assigned to coalitions or projects from a fixed set P. Each project k is characterized by a valuation function; v_k(S) is the value generated by a set S of agents working on project k. We study the following classic problem in this setting: \"how should the agents divide the value that they collectively create?\". One traditional approach in cooperative game theory is to study core stability with the implicit assumption that there are infinite copies of one project, and agents can partition themselves into any number of coalitions. In contrast, we consider a model with a finite number of non-identical projects; this makes computing both high-welfare solutions and core payments highly non-trivial.\n",
        "submission_date": "2015-08-27T00:00:00",
        "last_modified_date": "2015-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06976",
        "title": "Real-time Top-K Predictive Query Processing over Event Streams",
        "authors": [
            "Saurav Acharya",
            "Byung Suk Lee",
            "Paul Hines"
        ],
        "abstract": "This paper addresses the problem of predicting the k events that are most likely to occur next, over historical real-time event streams. Existing approaches to causal prediction queries have a number of limitations. First, they exhaustively search over an acyclic causal network to find the most likely k effect events; however, data from real event streams frequently reflect cyclic causality. Second, they contain conservative assumptions intended to exclude all possible non-causal links in the causal network; it leads to the omission of many less-frequent but important causal links. We overcome these limitations by proposing a novel event precedence model and a run-time causal inference mechanism. The event precedence model constructs a first order absorbing Markov chain incrementally over event streams, where an edge between two events signifies a temporal precedence relationship between them, which is a necessary condition for causality. Then, the run-time causal inference mechanism learns causal relationships dynamically during query processing. This is done by removing some of the temporal precedence relationships that do not exhibit causality in the presence of other events in the event precedence model. This paper presents two query processing algorithms -- one performs exhaustive search on the model and the other performs a more efficient reduced search with early termination. Experiments using two real datasets (cascading blackouts in power systems and web page views) verify the effectiveness of the probabilistic top-k prediction queries and the efficiency of the algorithms. Specifically, the reduced search algorithm reduced runtime, relative to exhaustive search, by 25-80% (depending on the application) with only a small reduction in accuracy.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2015-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07275",
        "title": "A Comparison Between Decision Trees and Decision Tree Forest Models for Software Development Effort Estimation",
        "authors": [
            "Ali Bou Nassif",
            "Mohammad Azzeh",
            "Luiz Fernando Capretz",
            "Danny Ho"
        ],
        "abstract": "Accurate software effort estimation has been a challenge for many software practitioners and project managers. Underestimation leads to disruption in the projects estimated cost and delivery. On the other hand, overestimation causes outbidding and financial losses in business. Many software estimation models exist; however, none have been proven to be the best in all situations. In this paper, a decision tree forest (DTF) model is compared to a traditional decision tree (DT) model, as well as a multiple linear regression model (MLR). The evaluation was conducted using ISBSG and Desharnais industrial datasets. Results show that the DTF model is competitive and can be used as an alternative in software effort prediction.\n    ",
        "submission_date": "2015-08-28T00:00:00",
        "last_modified_date": "2015-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07680",
        "title": "Domain Generalization for Object Recognition with Multi-task Autoencoders",
        "authors": [
            "Muhammad Ghifary",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang",
            "David Balduzzi"
        ],
        "abstract": "The problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition.\n",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07753",
        "title": "Learning Structures of Bayesian Networks for Variable Groups",
        "authors": [
            "Pekka Parviainen",
            "Samuel Kaski"
        ],
        "abstract": "Bayesian networks, and especially their structures, are powerful tools for representing conditional independencies and dependencies between random variables. In applications where related variables form a priori known groups, chosen to represent different \"views\" to or aspects of the same entities, one may be more interested in modeling dependencies between groups of variables rather than between individual variables. Motivated by this, we study prospects of representing relationships between variable groups using Bayesian network structures. We show that for dependency structures between groups to be expressible exactly, the data have to satisfy the so-called groupwise faithfulness assumption. We also show that one cannot learn causal relations between groups using only groupwise conditional independencies, but also variable-wise relations are needed. Additionally, we present algorithms for finding the groupwise dependency structures.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00061",
        "title": "Value function approximation via low-rank models",
        "authors": [
            "Hao Yi Ong"
        ],
        "abstract": "We propose a novel value function approximation technique for Markov decision processes. We consider the problem of compactly representing the state-action value function using a low-rank and sparse matrix model. The problem is to decompose a matrix that encodes the true value function into low-rank and sparse components, and we achieve this using Robust Principal Component Analysis (PCA). Under minimal assumptions, this Robust PCA problem can be solved exactly via the Principal Component Pursuit convex optimization problem. We experiment the procedure on several examples and demonstrate that our method yields approximations essentially identical to the true function.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00190",
        "title": "GR2RSS: Publishing Linked Open Commerce Data as RSS and Atom Feeds",
        "authors": [
            "Alex Stolz",
            "Martin Hepp"
        ],
        "abstract": "The integration of Linked Open Data (LOD) content in Web pages is a challenging and sometimes tedious task for Web developers. At the same moment, most software packages for blogs, content management systems (CMS), and shop applications support the consumption of feed formats, namely RSS and Atom. In this technical report, we demonstrate an on-line tool that fetches e-commerce data from a SPARQL endpoint and syndicates obtained results as RSS or Atom feeds. Our approach combines (1) the popularity and broad tooling support of existing feed formats, (2) the precision of queries against structured data built upon common Web vocabularies like ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2015-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00685",
        "title": "A Neural Attention Model for Abstractive Sentence Summarization",
        "authors": [
            "Alexander M. Rush",
            "Sumit Chopra",
            "Jason Weston"
        ],
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00690",
        "title": "A Fuzzy Approach for Feature Evaluation and Dimensionality Reduction to Improve the Quality of Web Usage Mining Results",
        "authors": [
            "Zahid Ansari",
            "M.F.Azeem",
            "A. Vinaya Babu",
            "Waseem Ahmed"
        ],
        "abstract": "Web Usage Mining is the application of data mining techniques to web usage log repositories in order to discover the usage patterns that can be used to analyze the users navigational behavior. During the preprocessing stage, raw web log data is transformed into a set of user profiles. Each user profile captures a set of URLs representing a user session. Clustering can be applied to this sessionized data in order to capture similar interests and trends among users navigational patterns. Since the sessionized data may contain thousands of user sessions and each user session may consist of hundreds of URL accesses, dimensionality reduction is achieved by eliminating the low support URLs. Very small sessions are also removed in order to filter out the noise from the data. But direct elimination of low support URLs and small sized sessions may results in loss of a significant amount of information especially when the count of low support URLs and small sessions is large. We propose a fuzzy solution to deal with this problem by assigning weights to URLs and user sessions based on a fuzzy membership function. After assigning the weights we apply a Fuzzy c-Mean Clustering algorithm to discover the clusters of user profiles. In this paper, we describe our fuzzy set theoretic approach to perform feature selection (or dimensionality reduction) and session weight assignment. Finally we compare our soft computing based approach of dimensionality reduction with the traditional approach of direct elimination of small sessions and low support count URLs. Our results show that fuzzy feature evaluation and dimensionality reduction results in better performance and validity indices for the discovered clusters.\n    ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2015-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00838",
        "title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment",
        "authors": [
            "Hongyuan Mei",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01168",
        "title": "Semi-described and semi-supervised learning with Gaussian processes",
        "authors": [
            "Andreas Damianou",
            "Neil D. Lawrence"
        ],
        "abstract": "Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as \"semi-described learning\". We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01599",
        "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing",
        "authors": [
            "Parminder Bhatia",
            "Yangfeng Ji",
            "Jacob Eisenstein"
        ],
        "abstract": "Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classification-based methods.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01815",
        "title": "Research: Analysis of Transport Model that Approximates Decision Taker's Preferences",
        "authors": [
            "Valery Vilisov"
        ],
        "abstract": "Paper provides a method for solving the reverse Monge-Kantorovich transport problem (TP). It allows to accumulate positive decision-taking experience made by decision-taker in situations that can be presented in the form of TP. The initial data for the solution of the inverse TP is the information on orders, inventories and effective decisions take by decision-taker. The result of solving the inverse TP contains evaluations of the TPs payoff matrix elements. It can be used in new situations to select the solution corresponding to the preferences of the decision-taker. The method allows to gain decision-taker experience, so it can be used by others. The method allows to build the model of decision-taker preferences in a specific application area. The model can be updated regularly to ensure its relevance and adequacy to the decision-taker system of preferences. This model is adaptive to the current preferences of the decision taker.\n    ",
        "submission_date": "2015-09-06T00:00:00",
        "last_modified_date": "2015-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01920",
        "title": "Risk-Averse Approximate Dynamic Programming with Quantile-Based Risk Measures",
        "authors": [
            "Daniel R. Jiang",
            "Warren B. Powell"
        ],
        "abstract": "In this paper, we consider a finite-horizon Markov decision process (MDP) for which the objective at each stage is to minimize a quantile-based risk measure (QBRM) of the sequence of future costs; we call the overall objective a dynamic quantile-based risk measure (DQBRM). In particular, we consider optimizing dynamic risk measures where the one-step risk measures are QBRMs, a class of risk measures that includes the popular value at risk (VaR) and the conditional value at risk (CVaR). Although there is considerable theoretical development of risk-averse MDPs in the literature, the computational challenges have not been explored as thoroughly. We propose data-driven and simulation-based approximate dynamic programming (ADP) algorithms to solve the risk-averse sequential decision problem. We address the issue of inefficient sampling for risk applications in simulated settings and present a procedure, based on importance sampling, to direct samples toward the \"risky region\" as the ADP algorithm progresses. Finally, we show numerical results of our algorithms in the context of an application involving risk-averse bidding for energy storage.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2017-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01978",
        "title": "An Approach to the Analysis of the South Slavic Medieval Labels Using Image Texture",
        "authors": [
            "Darko Brodic",
            "Alessia Amelio",
            "Zoran N. Milivojevic"
        ],
        "abstract": "The paper presents a new script classification method for the discrimination of the South Slavic medieval labels. It consists in the textural analysis of the script types. In the first step, each letter is coded by the equivalent script type, which is defined by its typographical features. Obtained coded text is subjected to the run-length statistical analysis and to the adjacent local binary pattern analysis in order to extract the features. The result shows a diversity between the extracted features of the scripts, which makes the feature classification more effective. It is the basis for the classification process of the script identification by using an extension of a state-of-the-art approach for document clustering. The proposed method is evaluated on an example of hand-engraved in stone and hand-printed in paper labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate very positive results, which prove the effectiveness of the proposed method.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02458",
        "title": "A Behavior Analysis-Based Game Bot Detection Approach Considering Various Play Styles",
        "authors": [
            "Yeounoh Chung",
            "Chang-yong Park",
            "Noo-ri Kim",
            "Hana Cho",
            "Taebok Yoon",
            "Hunjoo Lee",
            "Jee-Hyong Lee"
        ],
        "abstract": "An approach for game bot detection in MMORPGs is proposed based on the analysis of game playing behavior. Since MMORPGs are large scale games, users can play in various ways. This variety in playing behavior makes it hard to detect game bots based on play behaviors. In order to cope with this problem, the proposed approach observes game playing behaviors of users and groups them by their behavioral similarities. Then, it develops a local bot detection model for each player group. Since the locally optimized models can more accurately detect game bots within each player group, the combination of those models brings about overall improvement. For a practical purpose of reducing the workloads of the game servers in service, the game data is collected at a low resolution in time. Behavioral features are selected and developed to accurately detect game bots with the low resolution data, considering common aspects of MMORPG playing. Through the experiment with the real data from a game currently in service, it is shown that the proposed local model approach yields more accurate results.\n    ",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03005",
        "title": "Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies",
        "authors": [
            "David Balduzzi",
            "Muhammad Ghifary"
        ],
        "abstract": "This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.\n    ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03044",
        "title": "Recurrent Reinforcement Learning: A Hybrid Approach",
        "authors": [
            "Xiujun Li",
            "Lihong Li",
            "Jianfeng Gao",
            "Xiaodong He",
            "Jianshu Chen",
            "Li Deng",
            "Ji He"
        ],
        "abstract": "Successful applications of reinforcement learning in real-world problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent's entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we propose a new family of hybrid models that combines the strength of both supervised learning (SL) and reinforcement learning (RL), trained in a joint fashion: The SL component can be a recurrent neural networks (RNN) or its long short-term memory (LSTM) version, which is equipped with the desired property of being able to capture long-term dependency on history, thus providing an effective way of learning the representation of hidden states. The RL component is a deep Q-network (DQN) that learns to optimize the control for maximizing long-term rewards. Extensive experiments in a direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best among a set of previous state-of-the-art methods.\n    ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03371",
        "title": "Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems",
        "authors": [
            "Fabian Tschopp"
        ],
        "abstract": "This work presents and analyzes three convolutional neural network (CNN) models for efficient pixelwise classification of images. When using convolutional neural networks to classify single pixels in patches of a whole image, a lot of redundant computations are carried out when using sliding window networks. This set of new architectures solve this issue by either removing redundant computations or using fully convolutional architectures that inherently predict many pixels at once.\n",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03977",
        "title": "Optimization of anemia treatment in hemodialysis patients via reinforcement learning",
        "authors": [
            "Pablo Escandell-Montero",
            "Milena Chermisi",
            "Jos\u00e9 M. Mart\u00ednez-Mart\u00ednez",
            "Juan G\u00f3mez-Sanchis",
            "Carlo Barbieri",
            "Emilio Soria-Olivas",
            "Flavio Mari",
            "Joan Vila-Franc\u00e9s",
            "Andrea Stopper",
            "Emanuele Gatti",
            "Jos\u00e9 D. Mart\u00edn-Guerrero"
        ],
        "abstract": "Objective: Anemia is a frequent comorbidity in hemodialysis patients that can be successfully treated by administering erythropoiesis-stimulating agents (ESAs). ESAs dosing is currently based on clinical protocols that often do not account for the high inter- and intra-individual variability in the patient's response. As a result, the hemoglobin level of some patients oscillates around the target range, which is associated with multiple risks and side-effects. This work proposes a methodology based on reinforcement learning (RL) to optimize ESA therapy.\n",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04265",
        "title": "Double Relief with progressive weighting function",
        "authors": [
            "Gabriel Prat Masramon",
            "Llu\u00eds A. Belanche Mu\u00f1oz"
        ],
        "abstract": "Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem's domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task.\n",
        "submission_date": "2015-09-12T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04581",
        "title": "Kernelized Deep Convolutional Neural Network for Describing Complex Images",
        "authors": [
            "Zhen Liu"
        ],
        "abstract": "With the impressive capability to capture visual content, deep convolutional neural networks (CNN) have demon- strated promising performance in various vision-based ap- plications, such as classification, recognition, and objec- t detection. However, due to the intrinsic structure design of CNN, for images with complex content, it achieves lim- ited capability on invariance to translation, rotation, and re-sizing changes, which is strongly emphasized in the s- cenario of content-based image retrieval. In this paper, to address this problem, we proposed a new kernelized deep convolutional neural network. We first discuss our motiva- tion by an experimental study to demonstrate the sensitivi- ty of the global CNN feature to the basic geometric trans- formations. Then, we propose to represent visual content with approximate invariance to the above geometric trans- formations from a kernelized perspective. We extract CNN features on the detected object-like patches and aggregate these patch-level CNN features to form a vectorial repre- sentation with the Fisher vector model. The effectiveness of our proposed algorithm is demonstrated on image search application with three benchmark datasets.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05016",
        "title": "Recurrent Neural Networks for Driver Activity Anticipation via Sensory-Fusion Architecture",
        "authors": [
            "Ashesh Jain",
            "Avi Singh",
            "Hema S Koppula",
            "Shane Soh",
            "Ashutosh Saxena"
        ],
        "abstract": "Anticipating the future actions of a human is a widely studied problem in robotics that requires spatio-temporal reasoning. In this work we propose a deep learning approach for anticipation in sensory-rich robotics applications. We introduce a sensory-fusion architecture which jointly learns to anticipate and fuse information from multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We train our architecture in a sequence-to-sequence prediction manner, and it explicitly learns to predict the future given only a partial temporal context. We further introduce a novel loss layer for anticipation which prevents over-fitting and encourages early anticipation. We use our architecture to anticipate driving maneuvers several seconds before they happen on a natural driving data set of 1180 miles. The context for maneuver anticipation comes from multiple sensors installed on the vehicle. Our approach shows significant improvement over the state-of-the-art in maneuver anticipation by increasing the precision from 77.4% to 90.5% and recall from 71.2% to 87.4%.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05209",
        "title": "Extraction of evidence tables from abstracts of randomized clinical trials using a maximum entropy classifier and global constraints",
        "authors": [
            "Antonio Trenta",
            "Anthony Hunter",
            "Sebastian Riedel"
        ],
        "abstract": "Systematic use of the published results of randomized clinical trials is increasingly important in evidence-based medicine. In order to collate and analyze the results from potentially numerous trials, evidence tables are used to represent trials concerning a set of interventions of interest. An evidence table has columns for the patient group, for each of the interventions being compared, for the criterion for the comparison (e.g. proportion who survived after 5 years from treatment), and for each of the results. Currently, it is a labour-intensive activity to read each published paper and extract the information for each field in an evidence table. There have been some NLP studies investigating how some of the features from papers can be extracted, or at least the relevant sentences identified. However, there is a lack of an NLP system for the systematic extraction of each item of information required for an evidence table. We address this need by a combination of a maximum entropy classifier, and integer linear programming. We use the later to handle constraints on what is an acceptable classification of the features to be extracted. With experimental results, we demonstrate substantial advantages in using global constraints (such as the features describing the patient group, and the interventions, must occur before the features describing the results of the comparison).\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05257",
        "title": "(Blue) Taxi Destination and Trip Time Prediction from Partial Trajectories",
        "authors": [
            "Hoang Thanh Lam",
            "Ernesto Diaz-Aviles",
            "Alessandra Pascale",
            "Yiannis Gkoufas",
            "Bei Chen"
        ],
        "abstract": "Real-time estimation of destination and travel time for taxis is of great importance for existing electronic dispatch systems. We present an approach based on trip matching and ensemble learning, in which we leverage the patterns observed in a dataset of roughly 1.7 million taxi journeys to predict the corresponding final destination and travel time for ongoing taxi trips, as a solution for the ECML/PKDD Discovery Challenge 2015 competition. The results of our empirical evaluation show that our approach is effective and very robust, which led our team -- BlueTaxi -- to the 3rd and 7th position of the final rankings for the trip time and destination prediction tasks, respectively. Given the fact that the final rankings were computed using a very small test set (with only 320 trips) we believe that our approach is one of the most robust solutions for the challenge based on the consistency of our good results across the test sets.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05315",
        "title": "A Simulated Annealing Approach to Bayesian Inference",
        "authors": [
            "Carlo Albert"
        ],
        "abstract": "A generic algorithm for the extraction of probabilistic (Bayesian) information about model parameters from data is presented. The algorithm propagates an ensemble of particles in the product space of model parameters and outputs. Each particle update consists of a random jump in parameter space followed by a simulation of a model output and a Metropolis acceptance/rejection step based on a comparison of the simulated output to the data. The distance of a particle to the data is interpreted as an energy and the algorithm is reducing the associated temperature of the ensemble such that entropy production is minimized. If this simulated annealing is not too fast compared to the mixing speed in parameter space, the parameter marginal of the ensemble approaches the Bayesian posterior distribution. Annealing is adaptive and depends on certain extensive thermodynamic quantities that can easily be measured throughout run-time. In the general case, we propose annealing with a constant entropy production rate, which is optimal as long as annealing is not too fast. For the practically relevant special case of no prior knowledge, we derive an optimal fast annealing schedule with a non-constant entropy production rate. The algorithm does not require the calculation of the density of the model likelihood, which makes it interesting for Bayesian parameter inference with stochastic models, whose likelihood functions are typically very high dimensional integrals.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05437",
        "title": "Class Association Rules Mining based Rough Set Method",
        "authors": [
            "Thabet Slimani"
        ],
        "abstract": "This paper investigates the mining of class association rules with rough set approach. In data mining, an association occurs between two set of elements when one element set happen together with another. A class association rule set (CARs) is a subset of association rules with classes specified as their consequences. We present an efficient algorithm for mining the finest class rule set inspired form Apriori algorithm, where the support and confidence are computed based on the elementary set of lower approximation included in the property of rough set theory. Our proposed approach has been shown very effective, where the rough set approach for class association discovery is much simpler than the classic association method.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05463",
        "title": "Learning from Synthetic Data Using a Stacked Multichannel Autoencoder",
        "authors": [
            "Xi Zhang",
            "Yanwei Fu",
            "Shanshan Jiang",
            "Leonid Sigal",
            "Gady Agam"
        ],
        "abstract": "Learning from synthetic data has many important and practical applications. An example of application is photo-sketch recognition. Using synthetic data is challenging due to the differences in feature distributions between synthetic and real data, a phenomenon we term synthetic gap. In this paper, we investigate and formalize a general framework-Stacked Multichannel Autoencoder (SMCAE) that enables bridging the synthetic gap and learning from synthetic data more efficiently. In particular, we show that our SMCAE can not only transform and use synthetic data on the challenging face-sketch recognition task, but that it can also help simulate real images, which can be used for training classifiers for recognition. Preliminary experiments validate the effectiveness of the framework.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05526",
        "title": "Proceedings Thirteenth International Workshop on the ACL2 Theorem Prover and Its Applications",
        "authors": [
            "Matt Kaufmann",
            "David L. Rager"
        ],
        "abstract": "This volume contains the proceedings of the Thirteenth International Workshop on the ACL2 Theorem Prover and Its Applications, ACL2 2015, a two-day workshop held in Austin, Texas, USA, on October 1-2, 2015. ACL2 workshops occur at approximately 18-month intervals and provide a major technical forum for researchers to present and discuss improvements and extensions to the theorem prover, comparisons of ACL2 with other systems, and applications of ACL2 in formal verification.\n",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05636",
        "title": "Visual Generalized Coordinates",
        "authors": [
            "M. Seetha Ramaiah",
            "Amitabha Mukerjee",
            "Arindam Chakraborty",
            "Sadbodh Sharma"
        ],
        "abstract": "An open problem in robotics is that of using vision to identify a robot's own body and the world around it. Many models attempt to recover the traditional C-space parameters. Instead, we propose an alternative C-space by deriving generalized coordinates from $n$ images of the robot. We show that the space of such images is bijective to the motion space, so these images lie on a manifold $\\mathcal{V}$ homeomorphic to the canonical C-space. We now approximate this manifold as a set of $n$ neighbourhood tangent spaces that result in a graph, which we call the Visual Roadmap (VRM). Given a new robot image, we perform inverse kinematics visually by interpolating between nearby images in the image space. Obstacles are projected onto the VRM in $O(n)$ time by superimposition of images, leading to the identification of collision poses. The edges joining the free nodes can now be checked with a visual local planner, and free-space motions computed in $O(nlogn)$ time. This enables us to plan paths in the image space for a robot manipulator with unknown link geometries, DOF, kinematics, obstacles, and camera pose. We sketch the proofs for the main theoretical ideas, identify the assumptions, and demonstrate the approach for both articulated and mobile robots. We also investigate the feasibility of the process by investigating various metrics and image sampling densities, and demonstrate it on simulated and real robots.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05722",
        "title": "Energy saving in smart homes based on consumer behaviour: A case study",
        "authors": [
            "Michael Zehnder",
            "Holger Wache",
            "Hans-Friedrich Witschel",
            "Danilo Zanatta",
            "Miguel Rodriguez"
        ],
        "abstract": "This paper presents a case study of a recommender system that can be used to save energy in smart homes without lowering the comfort of the inhabitants. We present an algorithm that uses consumer behavior data only and uses machine learning to suggest actions for inhabitants to reduce the energy consumption of their homes. The system mines for frequent and periodic patterns in the event data provided by the Digitalstrom home automation system. These patterns are converted into association rules, prioritized and compared with the current behavior of the inhabitants. If the system detects an opportunities to save energy without decreasing the comfort level it sends a recommendation to the residents.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05870",
        "title": "Exploiting Reduction Rules and Data Structures: Local Search for Minimum Vertex Cover in Massive Graphs",
        "authors": [
            "Yi Fan",
            "Chengqian Li",
            "Zongjie Ma",
            "LjiLjana Brankovic",
            "Vladimir Estivill-Castro",
            "Abdul Sattar"
        ],
        "abstract": "The Minimum Vertex Cover (MinVC) problem is a well-known NP-hard problem. Recently there has been great interest in solving this problem on real-world massive graphs. For such graphs, local search is a promising approach to finding optimal or near-optimal solutions. In this paper we propose a local search algorithm that exploits reduction rules and data structures to solve the MinVC problem in such graphs. Experimental results on a wide range of real-word massive graphs show that our algorithm finds better covers than state-of-the-art local search algorithms for MinVC. Also we present interesting results about the complexities of some well-known heuristics.\n    ",
        "submission_date": "2015-09-19T00:00:00",
        "last_modified_date": "2015-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05962",
        "title": "Telugu OCR Framework using Deep Learning",
        "authors": [
            "Rakesh Achanta",
            "Trevor Hastie"
        ],
        "abstract": "In this paper, we address the task of Optical Character Recognition(OCR) for the Telugu script. We present an end-to-end framework that segments the text image, classifies the characters and extracts lines using a language model. The segmentation is based on mathematical morphology. The classification module, which is the most challenging task of the three, is a deep convolutional neural network. The language is modelled as a third degree markov chain at the glyph level. Telugu script is a complex alphasyllabary and the language is agglutinative, making the problem hard. In this paper we apply the latest advances in neural networks to achieve state-of-the-art error rates. We also review convolutional neural networks in great detail and expound the statistical justification behind the many tricks needed to make Deep Learning work.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06279",
        "title": "Sports highlights generation based on acoustic events detection: A rugby case study",
        "authors": [
            "Anant Baijal",
            "Jaeyoun Cho",
            "Woojung Lee",
            "Byeong-Seob Ko"
        ],
        "abstract": "We approach the challenging problem of generating highlights from sports broadcasts utilizing audio information only. A language-independent, multi-stage classification approach is employed for detection of key acoustic events which then act as a platform for summarization of highlight scenes. Objective results and human experience indicate that our system is highly efficient.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06589",
        "title": "Graph Kernels exploiting Weisfeiler-Lehman Graph Isomorphism Test Extensions",
        "authors": [
            "Giovanni Da San Martino",
            "Nicol\u00f2 Navarin",
            "Alessandro Sperduti"
        ],
        "abstract": "In this paper we present a novel graph kernel framework inspired the by the Weisfeiler-Lehman (WL) isomorphism tests. Any WL test comprises a relabelling phase of the nodes based on test-specific information extracted from the graph, for example the set of neighbours of a node. We defined a novel relabelling and derived two kernels of the framework from it. The novel kernels are very fast to compute and achieve state-of-the-art results on five real-world datasets.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06664",
        "title": "Reasoning about Entailment with Neural Attention",
        "authors": [
            "Tim Rockt\u00e4schel",
            "Edward Grefenstette",
            "Karl Moritz Hermann",
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Phil Blunsom"
        ],
        "abstract": "While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06849",
        "title": "Minimum Weight Perfect Matching via Blossom Belief Propagation",
        "authors": [
            "Sungsoo Ahn",
            "Sejun Park",
            "Michael Chertkov",
            "Jinwoo Shin"
        ],
        "abstract": "Max-product Belief Propagation (BP) is a popular message-passing algorithm for computing a Maximum-A-Posteriori (MAP) assignment over a distribution represented by a Graphical Model (GM). It has been shown that BP can solve a number of combinatorial optimization problems including minimum weight matching, shortest path, network flow and vertex cover under the following common assumption: the respective Linear Programming (LP) relaxation is tight, i.e., no integrality gap is present. However, when LP shows an integrality gap, no model has been known which can be solved systematically via sequential applications of BP. In this paper, we develop the first such algorithm, coined Blossom-BP, for solving the minimum weight matching problem over arbitrary graphs. Each step of the sequential algorithm requires applying BP over a modified graph constructed by contractions and expansions of blossoms, i.e., odd sets of vertices. Our scheme guarantees termination in O(n^2) of BP runs, where n is the number of vertices in the original graph. In essence, the Blossom-BP offers a distributed version of the celebrated Edmonds' Blossom algorithm by jumping at once over many sub-steps with a single BP. Moreover, our result provides an interpretation of the Edmonds' algorithm as a sequence of LPs.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07035",
        "title": "Designing Behaviour in Bio-inspired Robots Using Associative Topologies of Spiking-Neural-Networks",
        "authors": [
            "Cristian Jimenez-Romero",
            "David Sousa-Rodrigues",
            "Jeffrey H. Johnson"
        ],
        "abstract": "This study explores the design and control of the behaviour of agents and robots using simple circuits of spiking neurons and Spike Timing Dependent Plasticity (STDP) as a mechanism of associative and unsupervised learning. Based on a \"reward and punishment\" classical conditioning, it is demonstrated that these robots learnt to identify and avoid obstacles as well as to identify and look for rewarding stimuli. Using the simulation and programming environment NetLogo, a software engine for the Integrate and Fire model was developed, which allowed us to monitor in discrete time steps the dynamics of each single neuron, synapse and spike in the proposed neural networks. These spiking neural networks (SNN) served as simple brains for the experimental robots. The Lego Mindstorms robot kit was used for the embodiment of the simulated agents. In this paper the topological building blocks are presented as well as the neural parameters required to reproduce the experiments. This paper summarizes the resulting behaviour as well as the observed dynamics of the neural circuits. The Internet-link to the NetLogo code is included in the annex.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07062",
        "title": "Boolean Hedonic Games",
        "authors": [
            "Haris Aziz",
            "Paul Harrenstein",
            "J\u00e9r\u00f4me Lang",
            "Michael Wooldridge"
        ],
        "abstract": "We study hedonic games with dichotomous preferences. Hedonic games are cooperative games in which players desire to form coalitions, but only care about the makeup of the coalitions of which they are members; they are indifferent about the makeup of other coalitions. The assumption of dichotomous preferences means that, additionally, each player's preference relation partitions the set of coalitions of which that player is a member into just two equivalence classes: satisfactory and unsatisfactory. A player is indifferent between satisfactory coalitions, and is indifferent between unsatisfactory coalitions, but strictly prefers any satisfactory coalition over any unsatisfactory coalition. We develop a succinct representation for such games, in which each player's preference relation is represented by a propositional formula. We show how solution concepts for hedonic games with dichotomous preferences are characterised by propositional formulas.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07074",
        "title": "Quantification of sand fraction from seismic attributes using Neuro-Fuzzy approach",
        "authors": [
            "Akhilesh K Verma",
            "Soumi Chaki",
            "Aurobinda Routray",
            "William K Mohanty",
            "Mamata Jenamani"
        ],
        "abstract": "In this paper, we illustrate the modeling of a reservoir property (sand fraction) from seismic attributes namely seismic impedance, seismic amplitude, and instantaneous frequency using Neuro-Fuzzy (NF) approach. Input dataset includes 3D post-stacked seismic attributes and six well logs acquired from a hydrocarbon field located in the western coast of India. Presence of thin sand and shale layers in the basin area makes the modeling of reservoir characteristic a challenging task. Though seismic data is helpful in extrapolation of reservoir properties away from boreholes; yet, it could be challenging to delineate thin sand and shale reservoirs using seismic data due to its limited resolvability. Therefore, it is important to develop state-of-art intelligent methods for calibrating a nonlinear mapping between seismic data and target reservoir variables. Neural networks have shown its potential to model such nonlinear mappings; however, uncertainties associated with the model and datasets are still a concern. Hence, introduction of Fuzzy Logic (FL) is beneficial for handling these uncertainties. More specifically, hybrid variants of Artificial Neural Network (ANN) and fuzzy logic, i.e., NF methods, are capable for the modeling reservoir characteristics by integrating the explicit knowledge representation power of FL with the learning ability of neural networks. The documented results in this study demonstrate acceptable resemblance between target and predicted variables, and hence, encourage the application of integrated machine learning approaches such as Neuro-Fuzzy in reservoir characterization domain. Furthermore, visualization of the variation of sand probability in the study area would assist in identifying placement of potential wells for future drilling operations.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07175",
        "title": "Exploration and Exploitation of Victorian Science in Darwin's Reading Notebooks",
        "authors": [
            "Jaimie Murdock",
            "Colin Allen",
            "Simon DeDeo"
        ],
        "abstract": "Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. To study this decision-making process, we examine the reading choices made by one of the most celebrated scientists of the modern era: Charles Darwin. From the full-text of books listed in his chronologically-organized reading journals, we generate topic models to quantify his local (text-to-text) and global (text-to-past) reading decisions using Kullback-Liebler Divergence, a cognitively-validated, information-theoretic measure of relative surprise. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by qualitative scholarship and Darwin's own self-commentary. Our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. Our quantitative methods advance the study of cognitive search through a framework for testing interactions between individual and collective behavior and between short- and long-term consumption choices. This novel application of topic modeling to characterize individual reading complements widespread studies of collective scientific behavior.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07627",
        "title": "Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection",
        "authors": [
            "Hirokatsu Kataoka",
            "Kenji Iwata",
            "Yutaka Satoh"
        ],
        "abstract": "In this paper, we evaluate convolutional neural network (CNN) features using the AlexNet architecture and very deep convolutional network (VGGNet) architecture. To date, most CNN researchers have employed the last layers before output, which were extracted from the fully connected feature layers. However, since it is unlikely that feature representation effectiveness is dependent on the problem, this study evaluates additional convolutional layers that are adjacent to fully connected layers, in addition to executing simple tuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and transformation, using tools such as principal component analysis. In our experiments, we carried out detection and classification tasks using the Caltech 101 and Daimler Pedestrian Benchmark Datasets.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07831",
        "title": "Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds, Language and Trajectories",
        "authors": [
            "Jaeyong Sung",
            "Ian Lenz",
            "Ashutosh Saxena"
        ],
        "abstract": "A robot operating in a real-world environment needs to perform reasoning over a variety of sensor modalities such as vision, language and motion trajectories. However, it is extremely challenging to manually design features relating such disparate modalities. In this work, we introduce an algorithm that learns to embed point-cloud, natural language, and manipulation trajectory data into a shared embedding space with a deep neural network. To learn semantically meaningful spaces throughout our network, we use a loss-based margin to bring embeddings of relevant pairs closer together while driving less-relevant cases from different modalities further apart. We use this both to pre-train its lower layers and fine-tune our final embedding space, leading to a more robust representation. We test our algorithm on the task of manipulating novel objects and appliances based on prior experience with other objects. On a large dataset, we achieve significant improvements in both accuracy and inference time over the previous state of the art. We also perform end-to-end experiments on a PR2 robot utilizing our learned embedding space.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07838",
        "title": "Training Deep Networks with Structured Layers by Matrix Backpropagation",
        "authors": [
            "Catalin Ionescu",
            "Orestis Vantzos",
            "Cristian Sminchisescu"
        ],
        "abstract": "Deep neural network architectures have recently produced excellent results in a variety of areas in artificial intelligence and visual recognition, well surpassing traditional shallow architectures trained using hand-designed features. The power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields, and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation. An open problem is the inclusion of layers that perform global, structured matrix computations like segmentation (e.g. normalized cuts) or higher-order pooling (e.g. log-tangent space metrics defined over the manifold of symmetric positive definite matrices) while preserving the validity and efficiency of an end-to-end deep training framework. In this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures. At the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations. The proposed matrix backpropagation methodology applies broadly to a variety of problems in machine learning or computational perception. Here we illustrate it by performing visual segmentation experiments using the BSDS and MSCOCO benchmarks, where we show that deep networks relying on second-order pooling and normalized cuts layers, trained end-to-end using matrix backpropagation, outperform counterparts that do not take advantage of such global layers.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07897",
        "title": "Quantum Look at two Common Logics: the Logic of Primitive Thinking and the Logic of Everyday Human Reasoning",
        "authors": [
            "E. D. Vol"
        ],
        "abstract": "Based on ideas of quantum theory of open systems and psychological dual system theory we propose two novel versions of Non-Boolean logic. The first version can be interpreted in our opinion as simplified description of primitive (mythological) thinking and the second one as the toy model of everyday human reasoning in which aside from logical deduction, heuristic elements and beliefs also play the considerable role. Several arguments in favor of the interpretations proposed are adduced and discussed in the paper as well.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08088",
        "title": "Approximation and Heuristic Algorithms for Probabilistic Physical Search on General Graphs",
        "authors": [
            "Noam Hazon",
            "Mira Gonen",
            "Max Kleb"
        ],
        "abstract": "We consider an agent seeking to obtain an item, potentially available at different locations in a physical environment. The traveling costs between locations are known in advance, but there is only probabilistic knowledge regarding the possible prices of the item at any given location. Given such a setting, the problem is to find a plan that maximizes the probability of acquiring the good while minimizing both travel and purchase costs. Sample applications include agents in search-and-rescue or exploration missions, e.g., a rover on Mars seeking to mine a specific mineral. These probabilistic physical search problems have been previously studied, but we present the first approximation and heuristic algorithms for solving such problems on general graphs. We establish an interesting connection between these problems and classical graph-search problems, which led us to provide the approximation algorithms and hardness of approximation results for our settings. We further suggest several heuristics for practical use, and demonstrate their effectiveness with simulation on real graph structure and synthetic graphs.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08255",
        "title": "Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory",
        "authors": [
            "Fergal Byrne"
        ],
        "abstract": "In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM) as a model of neocortical computation, the theory and the algorithms have evolved dramatically. This paper presents a detailed description of HTM's Cortical Learning Algorithm (CLA), including for the first time a rigorous mathematical formulation of all aspects of the computations. Prediction Assisted CLA (paCLA), a refinement of the CLA is presented, which is both closer to the neuroscience and adds significantly to the computational power. Finally, we summarise the key functions of neocortex which are expressed in paCLA implementations.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08396",
        "title": "An Innovative Approach for online Meta Search Engine Optimization",
        "authors": [
            "Jai Manral",
            "Mohammed Alamgir Hossain"
        ],
        "abstract": "This paper presents an approach to identify efficient techniques used in Web Search Engine Optimization (SEO). Understanding SEO factors which can influence page ranking in search engine is significant for webmasters who wish to attract large number of users to their website. Different from previous relevant research, in this study we developed an intelligent Meta search engine which aggregates results from various search engines and ranks them based on several important SEO parameters. The research tries to establish that using more SEO parameters in ranking algorithms helps in retrieving better search results thus increasing user satisfaction. Initial results generated from Meta search engine outperformed existing search engines in terms of better retrieved search results with high precision.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08535",
        "title": "Boolean Matrix Factorization and Noisy Completion via Message Passing",
        "authors": [
            "Siamak Ravanbakhsh",
            "Barnabas Poczos",
            "Russell Greiner"
        ],
        "abstract": "Boolean matrix factorization and Boolean matrix completion from noisy observations are desirable unsupervised data-analysis methods due to their interpretability, but hard to perform due to their NP-hardness. We treat these problems as maximum a posteriori inference problems in a graphical model and present a message passing approach that scales linearly with the number of observations and factors. Our empirical study demonstrates that message passing is able to recover low-rank Boolean matrices, in the boundaries of theoretically possible recovery and compares favorably with state-of-the-art in real-world applications, such collaborative filtering with large-scale Boolean data.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08634",
        "title": "Learning dynamic Boltzmann machines with spike-timing dependent plasticity",
        "authors": [
            "Takayuki Osogami",
            "Makoto Otsuka"
        ],
        "abstract": "We propose a particularly structured Boltzmann machine, which we refer to as a dynamic Boltzmann machine (DyBM), as a stochastic model of a multi-dimensional time-series. The DyBM can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure. This proposed structure is motivated by postulates and observations, from biological neural networks, that the synaptic weight is strengthened or weakened, depending on the timing of spikes (i.e., spike-timing dependent plasticity or STDP). We show that the learning rule of updating the parameters of the DyBM in the direction of maximizing the likelihood of given time-series can be interpreted as STDP with long term potentiation and long term depression. The learning rule has a guarantee of convergence and can be performed in a distributed matter (i.e., local in space) with limited memory (i.e., local in time).\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08639",
        "title": "Tuned and GPU-accelerated parallel data mining from comparable corpora",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "The multilingual nature of the world makes translation a crucial requirement today. Parallel dictionaries constructed by humans are a widely-available resource, but they are limited and do not provide enough coverage for good quality translation purposes, due to out-of-vocabulary words and neologisms. This motivates the use of statistical translation systems, which are unfortunately dependent on the quantity and quality of training data. Such has a very limited availability especially for some languages and very narrow text domains. Is this research we present our improvements to Yalign mining methodology by reimplementing the comparison algorithm, introducing a tuning scripts and by improving performance using GPU computing acceleration. The experiments are conducted on various text domains and bi-data is extracted from the Wikipedia dumps.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08731",
        "title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning",
        "authors": [
            "Shakir Mohamed",
            "Danilo Jimenez Rezende"
        ],
        "abstract": "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09149",
        "title": "Knowledge-based system for collaborative process specification",
        "authors": [
            "Frederick Benaben",
            "Vatcharaphun Rajsiri",
            "Jean-Pierre Lorr\u00e9",
            "Herv\u00e9 Pingaud"
        ],
        "abstract": "This paper presents an ontology-based approach for the design of a collaborative business process model (CBP). This CBP is considered as a specification of needs in order to build a collaboration information system (CIS) for a network of organisations. The study is a part of a model driven engineering approach of the CIS in a specific enterprise interoperability framework that will be summarised.  An adaptation of the Business Process Modeling Notation (BPMN) is  used to represent the CBP model. We develop a knowledge-based system (KbS) which is composed of three main parts: knowledge gathering, knowledge representation and reasoning, and collaborative business process modelling. The first part starts from a high abstraction level where knowledge from business partners is captured. A collaboration ontology is defined in order to provide a structure to store and use the knowledge captured.  In parallel, we try to reuse generic existing knowledge about business processes from the MIT Process Handbook repository. This results in a collaboration process ontology that is also described.  A set of rules is defined in order to extract knowledge about fragments of the CBP model from the two previous ontologies. These fragments are finally assembled in the third part of the KbS. A prototype of the KbS has been developed in order to implement and support this approach. The prototype is a computer-aided design tool of the CBP. In this paper, we will present the theoretical aspects of each part of this KbS as well as the tools that we developed and used in order to support its functionalities.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09152",
        "title": "Supporting interoperability of collaborative networks through engineering of a service-based Mediation Information System (MISE 2.0)",
        "authors": [
            "Frederick Benaben",
            "Wenxin Mu",
            "Nicolas Boissel-Dallier",
            "Anne-Marie Barthe-Delano\u00eb",
            "Sarah Zribi",
            "Herve Pingaud"
        ],
        "abstract": "The Mediation Information System Engineering project is currently finishing its second iteration (MISE 2.0). The main objective of this scientific project is to provide any emerging collaborative situation with methods and tools to deploy a Mediation Information System (MIS). MISE 2.0 aims at defining and designing a service-based platform, dedicated to initiating and supporting the interoperability of collaborative situations among potential partners. This MISE 2.0 platform implements a model-driven engineering approach to the design of a service-oriented MIS dedicated to supporting the collaborative situation. This approach is structured in three layers, each providing their own key innovative points: (i) the gathering of individual and collaborative knowledge to provide appropriate collaborative business behaviour (key point: knowledge management, including semantics, exploitation and capitalization), (ii) deployment of a mediation information system able to computerize the previously deduced collaborative processes (key point: the automatic generation of collaborative workflows, including connection with existing devices or services) (iii) the management of the agility of the obtained collaborative network of organizations (key point: supervision of collaborative situations and relevant exploitation of the gathered data). MISE covers business issues (through BPM), technical issues (through an SOA) and agility issues of collaborative situations (through EDA).\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00087",
        "title": "Clamping Improves TRW and Mean Field Approximations",
        "authors": [
            "Adrian Weller",
            "Justin Domke"
        ],
        "abstract": "We examine the effect of clamping variables for approximate inference in undirected graphical models with pairwise relationships and discrete variables. For any number of variable labels, we demonstrate that clamping and summing approximate sub-partition functions can lead only to a decrease in the partition function estimate for TRW, and an increase for the naive mean field method, in each case guaranteeing an improvement in the approximation and bound. We next focus on binary variables, add the Bethe approximation to consideration and examine ways to choose good variables to clamp, introducing new methods. We show the importance of identifying highly frustrated cycles, and of checking the singleton entropy of a variable. We explore the value of our methods by empirical analysis and draw lessons to guide practitioners.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00331",
        "title": "Multimodal Hierarchical Dirichlet Process-based Active Perception",
        "authors": [
            "Tadahiro Taniguchi",
            "Toshiaki Takano",
            "Ryo Yoshino"
        ],
        "abstract": "In this paper, we propose an active perception method for recognizing object categories based on the multimodal hierarchical Dirichlet process (MHDP). The MHDP enables a robot to form object categories using multimodal information, e.g., visual, auditory, and haptic information, which can be observed by performing actions on an object. However, performing many actions on a target object requires a long time. In a real-time scenario, i.e., when the time is limited, the robot has to determine the set of actions that is most effective for recognizing a target object. We propose an MHDP-based active perception method that uses the information gain (IG) maximization criterion and lazy greedy algorithm. We show that the IG maximization criterion is optimal in the sense that the criterion is equivalent to a minimization of the expected Kullback--Leibler divergence between a final recognition state and the recognition state after the next set of actions. However, a straightforward calculation of IG is practically impossible. Therefore, we derive an efficient Monte Carlo approximation method for IG by making use of a property of the MHDP. We also show that the IG has submodular and non-decreasing properties as a set function because of the structure of the graphical model of the MHDP. Therefore, the IG maximization problem is reduced to a submodular maximization problem. This means that greedy and lazy greedy algorithms are effective and have a theoretical justification for their performance. We conducted an experiment using an upper-torso humanoid robot and a second one using synthetic data. The experimental results show that the method enables the robot to select a set of actions that allow it to recognize target objects quickly and accurately. The results support our theoretical outcomes.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2016-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00523",
        "title": "Implementing Efficient All Solutions SAT Solvers",
        "authors": [
            "Takahisa Toda",
            "Takehide Soh"
        ],
        "abstract": "All solutions SAT (AllSAT for short) is a variant of propositional satisfiability problem. Despite its significance, AllSAT has been relatively unexplored compared to other variants. We thus survey and discuss major techniques of AllSAT solvers. We faithfully implement them and conduct comprehensive experiments using a large number of instances and various types of solvers including one of the few public softwares. The experiments reveal solver's characteristics. Our implemented solvers are made publicly available so that other researchers can easily develop their solver by modifying our codes and compare it with existing methods.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00552",
        "title": "Exposing the Probabilistic Causal Structure of Discrimination",
        "authors": [
            "Francesco Bonchi",
            "Sara Hajian",
            "Bud Mishra",
            "Daniele Ramazzotti"
        ],
        "abstract": "Discrimination discovery from data is an important task aiming at identifying patterns of illegal and unethical discriminatory activities against protected-by-law groups, e.g., ethnic minorities. While any legally-valid proof of discrimination requires evidence of causality, the state-of-the-art methods are essentially correlation-based, albeit, as it is well known, correlation does not imply causation.\n",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00878",
        "title": "Client Profiling for an Anti-Money Laundering System",
        "authors": [
            "Claudio Alexandre",
            "Jo\u00e3o Balsa"
        ],
        "abstract": "We present a data mining approach for profiling bank clients in order to support the process of detection of anti-money laundering operations. We first present the overall system architecture, and then focus on the relevant component for this paper. We detail the experiments performed on real world data from a financial institution, which allowed us to group clients in clusters and then generate a set of classification rules. We discuss the relevance of the founded client profiles and of the generated classification rules. According to the defined overall agent-based architecture, these rules will be incorporated in the knowledge base of the intelligent agents responsible for the signaling of suspicious transactions.\n    ",
        "submission_date": "2015-10-03T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01064",
        "title": "Boosting in the presence of outliers: adaptive classification with non-convex loss functions",
        "authors": [
            "Alexander Hanbo Li",
            "Jelena Bradic"
        ],
        "abstract": "This paper examines the role and efficiency of the non-convex loss functions for binary classification problems. In particular, we investigate how to design a simple and effective boosting algorithm that is robust to the outliers in the data. The analysis of the role of a particular non-convex loss for prediction accuracy varies depending on the diminishing tail properties of the gradient of the loss -- the ability of the loss to efficiently adapt to the outlying data, the local convex properties of the loss and the proportion of the contaminated data. In order to use these properties efficiently, we propose a new family of non-convex losses named $\\gamma$-robust losses. Moreover, we present a new boosting framework, {\\it Arch Boost}, designed for augmenting the existing work such that its corresponding classification algorithm is significantly more adaptable to the unknown data contamination. Along with the Arch Boosting framework, the non-convex losses lead to the new class of boosting algorithms, named adaptive, robust, boosting (ARB). Furthermore, we present theoretical examples that demonstrate the robustness properties of the proposed algorithms. In particular, we develop a new breakdown point analysis and a new influence function analysis that demonstrate gains in robustness. Moreover, we present new theoretical results, based only on local curvatures, which may be used to establish statistical and optimization properties of the proposed Arch boosting algorithms with highly non-convex loss functions. Extensive numerical calculations are used to illustrate these theoretical properties and reveal advantages over the existing boosting methods when data exhibits a number of outliers.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01344",
        "title": "Within-Brain Classification for Brain Tumor Segmentation",
        "authors": [
            "Mohammad Havaei",
            "Hugo Larochelle",
            "Philippe Poulin",
            "Pierre-Marc Jodoin"
        ],
        "abstract": "Purpose: In this paper, we investigate a framework for interactive brain tumor segmentation which, at its core, treats the problem of interactive brain tumor segmentation as a machine learning problem.\n",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01784",
        "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback",
        "authors": [
            "Ruining He",
            "Julian McAuley"
        ],
        "abstract": "Modern recommender systems model people and items by discovering or `teasing apart' the underlying dimensions that encode the properties of items and users' preferences toward them. Critically, such dimensions are uncovered based on user feedback, often in implicit form (such as purchase histories, browsing logs, etc.); in addition, some recommender systems make use of side information, such as product attributes, temporal information, or review text. However one important feature that is typically ignored by existing personalized recommendation and ranking methods is the visual appearance of the items being considered. In this paper we propose a scalable factorization model to incorporate visual signals into predictors of people's opinions, which we apply to a selection of large, real-world datasets. We make use of visual features extracted from product images using (pre-trained) deep networks, on top of which we learn an additional layer that uncovers the visual dimensions that best explain the variation in people's feedback. This not only leads to significantly more accurate personalized ranking methods, but also helps to alleviate cold start issues, and qualitatively to analyze the visual dimensions that influence people's opinions.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02045",
        "title": "Budget Constraints in Prediction Markets",
        "authors": [
            "Nikhil Devanur",
            "Miroslav Dud\u00edk",
            "Zhiyi Huang",
            "David M. Pennock"
        ],
        "abstract": "We give a detailed characterization of optimal trades under budget constraints in a prediction market with a cost-function-based automated market maker. We study how the budget constraints of individual traders affect their ability to impact the market price. As a concrete application of our characterization, we give sufficient conditions for a property we call budget additivity: two traders with budgets B and B' and the same beliefs would have a combined impact equal to a single trader with budget B+B'. That way, even if a single trader cannot move the market much, a crowd of like-minded traders can have the same desired effect. When the set of payoff vectors associated with outcomes, with coordinates corresponding to securities, is affinely independent, we obtain that a generalization of the heavily-used logarithmic market scoring rule is budget additive, but the quadratic market scoring rule is not. Our results may be used both descriptively, to understand if a particular market maker is affected by budget constraints or not, and prescriptively, as a recipe to construct markets.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02951",
        "title": "On oblivious branching programs with bounded repetition that cannot efficiently compute CNFs of bounded treewidth",
        "authors": [
            "Igor Razgon"
        ],
        "abstract": "In this paper we study complexity of an extension of ordered binary decision diagrams (OBDDs) called $c$-OBDDs on CNFs of bounded (primal graph) treewidth. In particular, we show that for each $k$ there is a class of CNFs of treewidth $k \\geq 3$ for which the equivalent $c$-OBDDs are of size $\\Omega(n^{k/(8c-4)})$. Moreover, this lower bound holds if $c$-OBDD is non-deterministic and semantic. Our second result uses the above lower bound to separate the above model from sentential decision diagrams (SDDs). In order to obtain the lower bound, we use a structural graph parameter called matching width. Our third result shows that matching width and pathwidth are linearly related.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03164",
        "title": "Context-Aware Bandits",
        "authors": [
            "Shuai Li",
            "Purushottam Kar"
        ],
        "abstract": "We propose an efficient Context-Aware clustering of Bandits (CAB) algorithm, which can capture collaborative effects. CAB can be easily deployed in a real-world recommendation system, where multi-armed bandits have been shown to perform well in particular with respect to the cold-start problem. CAB utilizes a context-aware clustering augmented by exploration-exploitation strategies. CAB dynamically clusters the users based on the content universe under consideration. We give a theoretical analysis in the standard stochastic multi-armed bandits setting. We show the efficiency of our approach on production and real-world datasets, demonstrate the scalability, and, more importantly, the significant increased prediction performance against several state-of-the-art methods.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2017-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03370",
        "title": "Asymptotic Logical Uncertainty and The Benford Test",
        "authors": [
            "Scott Garrabrant",
            "Siddharth Bhaskar",
            "Abram Demski",
            "Joanna Garrabrant",
            "George Koleszarik",
            "Evan Lloyd"
        ],
        "abstract": "We give an algorithm A which assigns probabilities to logical sentences. For any simple infinite sequence of sentences whose truth-values appear indistinguishable from a biased coin that outputs \"true\" with probability p, we have that the sequence of probabilities that A assigns to these sentences converges to p.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03517",
        "title": "A Multilevel Coordinate Search Algorithm for Well Placement, Control and Joint Optimization",
        "authors": [
            "Xiang Wang",
            "Ronald D. Haynes",
            "Qihong Feng"
        ],
        "abstract": "Determining optimal well placements and controls are two important tasks in oil field development. These problems are computationally expensive, nonconvex, and contain multiple optima. The practical solution of these problems require efficient and robust algorithms. In this paper, the multilevel coordinate search (MCS) algorithm is applied for well placement and control optimization problems. MCS is a derivative-free algorithm that combines global and local search. Both synthetic and real oil fields are considered. The performance of MCS is compared to generalized pattern search (GPS), particle swarm optimization (PSO), and covariance matrix adaptive evolution strategy (CMA-ES) algorithms. Results show that the MCS algorithm is strongly competitive, and outperforms for the joint optimization problem and with a limited computational budget. The effect of parameter settings for MCS are compared for the test examples. For the joint optimization problem we compare the performance of the simultaneous and sequential procedures and show the utility of the latter.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04163",
        "title": "Embarrassingly Parallel Variational Inference in Nonconjugate Models",
        "authors": [
            "Willie Neiswanger",
            "Chong Wang",
            "Eric Xing"
        ],
        "abstract": "We develop a parallel variational inference (VI) procedure for use in data-distributed settings, where each machine only has access to a subset of data and runs VI independently, without communicating with other machines. This type of \"embarrassingly parallel\" procedure has recently been developed for MCMC inference algorithms; however, in many cases it is not possible to directly extend this procedure to VI methods without requiring certain restrictive exponential family conditions on the form of the model. Furthermore, most existing (nonparallel) VI methods are restricted to use on conditionally conjugate models, which limits their applicability. To combat these issues, we make use of the recently proposed nonparametric VI to facilitate an embarrassingly parallel VI procedure that can be applied to a wider scope of models, including to nonconjugate models. We derive our embarrassingly parallel VI algorithm, analyze our method theoretically, and demonstrate our method empirically on a few nonconjugate models.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04373",
        "title": "Scatter Component Analysis: A Unified Framework for Domain Adaptation and Domain Generalization",
        "authors": [
            "Muhammad Ghifary",
            "David Balduzzi",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang"
        ],
        "abstract": "This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04609",
        "title": "Layer-Specific Adaptive Learning Rates for Deep Networks",
        "authors": [
            "Bharat Singh",
            "Soham De",
            "Yangmuzi Zhang",
            "Thomas Goldstein",
            "Gavin Taylor"
        ],
        "abstract": "The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04972",
        "title": "Normalization of Relative and Incomplete Temporal Expressions in Clinical Narratives",
        "authors": [
            "Weiyi Sun",
            "Anna Rumshisky",
            "Ozlem Uzuner"
        ],
        "abstract": "We analyze the RI-TIMEXes in temporally annotated corpora and propose two hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative domain: the anchor point hypothesis and the anchor relation hypothesis. We annotate the RI-TIMEXes in three corpora to study the characteristics of RI-TMEXes in different domains. This informed the design of our RI-TIMEX normalization system for the clinical domain, which consists of an anchor point classifier, an anchor relation classifier and a rule-based RI-TIMEX text span parser. We experiment with different feature sets and perform error analysis for each system component. The annotation confirmed the hypotheses that we can simplify the RI-TIMEXes normalization task using two multi-label classifiers. Our system achieves anchor point classification, anchor relation classification and rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under relaxed matching criteria) respectively on the held-out test set of the 2012 i2b2 temporal relation challenge. Experiments with feature sets reveals some interesting findings such as the verbal tense feature does not inform the anchor relation classification in clinical narratives as much as the tokens near the RI-TIMEX. Error analysis shows that underrepresented anchor point and anchor relation classes are difficult to detect. We formulate the RI-TIMEX normalization problem as a pair of multi-label classification problems. Considering only the RI-TIMEX extraction and normalization, the system achieves statistically significant improvement over the RI-TIMEX results of the best systems in the 2012 i2b2 challenge.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05613",
        "title": "PERCH: Perception via Search for Multi-Object Recognition and Localization",
        "authors": [
            "Venkatraman Narayanan",
            "Maxim Likhachev"
        ],
        "abstract": "In many robotic domains such as flexible automated manufacturing or personal assistance, a fundamental perception task is that of identifying and localizing objects whose 3D models are known. Canonical approaches to this problem include discriminative methods that find correspondences between feature descriptors computed over the model and observed data. While these methods have been employed successfully, they can be unreliable when the feature descriptors fail to capture variations in observed data; a classic cause being occlusion. As a step towards deliberative reasoning, we present PERCH: PErception via SeaRCH, an algorithm that seeks to find the best explanation of the observed sensor data by hypothesizing possible scenes in a generative fashion. Our contributions are: i) formulating the multi-object recognition and localization task as an optimization problem over the space of hypothesized scenes, ii) exploiting structure in the optimization to cast it as a combinatorial search problem on what we call the Monotone Scene Generation Tree, and iii) leveraging parallelization and recent advances in multi-heuristic search in making combinatorial search tractable. We prove that our system can guaranteedly produce the best explanation of the scene under the chosen cost function, and validate our claims on real world RGB-D test data. Our experimental results show that we can identify and localize objects under heavy occlusion--cases where state-of-the-art methods struggle.\n    ",
        "submission_date": "2015-10-19T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05911",
        "title": "Discriminative Predicate Path Mining for Fact Checking in Knowledge Graphs",
        "authors": [
            "Baoxu Shi",
            "Tim Weninger"
        ],
        "abstract": "Traditional fact checking by experts and analysts cannot keep pace with the volume of newly created information. It is important and necessary, therefore, to enhance our ability to computationally determine whether some statement of fact is true or false. We view this problem as a link-prediction task in a knowledge graph, and present a discriminative path-based method for fact checking in knowledge graphs that incorporates connectivity, type information, and predicate interactions. Given a statement S of the form (subject, predicate, object), for example, (Chicago, capitalOf, Illinois), our approach mines discriminative paths that alternatively define the generalized statement (U.S. city, predicate, U.S. state) and uses the mined rules to evaluate the veracity of statement S. We evaluate our approach by examining thousands of claims related to history, geography, biology, and politics using a public, million node knowledge graph extracted from Wikipedia and PubMedDB. Not only does our approach significantly outperform related models, we also find that the discriminative predicate path model is easily interpretable and provides sensible reasons for the final determination.\n    ",
        "submission_date": "2015-10-20T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06143",
        "title": "High Performance Latent Variable Models",
        "authors": [
            "Aaron Q. Li",
            "Amr Ahmed",
            "Mu Li",
            "Vanja Josifovski"
        ],
        "abstract": "  Latent variable models have accumulated a considerable amount of interest from the industry and academia for their versatility in a wide range of applications. A large amount of effort has been made to develop systems that is able to extend the systems to a large scale, in the hope to make use of them on industry scale data. In this paper, we describe a system that operates at a scale orders of magnitude higher than previous works, and an order of magnitude faster than state-of-the-art system at the same scale, at the same time showing more robustness and more accurate results.\n",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07313",
        "title": "Safe Control under Uncertainty",
        "authors": [
            "Dorsa Sadigh",
            "Ashish Kapoor"
        ],
        "abstract": "Controller synthesis for hybrid systems that satisfy temporal specifications expressing various system properties is a challenging problem that has drawn the attention of many researchers. However, making the assumption that such temporal properties are deterministic is far from the reality. For example, many of the properties the controller has to satisfy are learned through machine learning techniques based on sensor input data. In this paper, we propose a new logic, Probabilistic Signal Temporal Logic (PrSTL), as an expressive language to define the stochastic properties, and enforce probabilistic guarantees on them. We further show how to synthesize safe controllers using this logic for cyber-physical systems under the assumption that the stochastic properties are based on a set of Gaussian random variables. One of the key distinguishing features of PrSTL is that the encoded logic is adaptive and changes as the system encounters additional data and updates its beliefs about the latent random variables that define the safety properties. We demonstrate our approach by synthesizing safe controllers under the PrSTL specifications for multiple case studies including control of quadrotors and autonomous vehicles in dynamic environments.\n    ",
        "submission_date": "2015-10-25T00:00:00",
        "last_modified_date": "2015-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07389",
        "title": "The Human Kernel",
        "authors": [
            "Andrew Gordon Wilson",
            "Christoph Dann",
            "Christopher G. Lucas",
            "Eric P. Xing"
        ],
        "abstract": "Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07526",
        "title": "Empirical Study on Deep Learning Models for Question Answering",
        "authors": [
            "Yang Yu",
            "Wei Zhang",
            "Chung-Wei Hang",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07787",
        "title": "Redesigning pattern mining algorithms for supercomputers",
        "authors": [
            "Kazuki Yoshizoe",
            "Aika Terada",
            "Koji Tsuda"
        ],
        "abstract": "Upcoming many core processors are expected to employ a distributed memory architecture similar to currently available supercomputers, but parallel pattern mining algorithms amenable to the architecture are not comprehensively studied. We present a novel closed pattern mining algorithm with a well-engineered communication protocol, and generalize it to find statistically significant patterns from personal genome data. For distributing communication evenly, it employs global load balancing with multiple stacks distributed on a set of cores organized as a hypercube with random edges. Our algorithm achieved up to 1175-fold speedup by using 1200 cores for solving a problem with 11,914 items and 697 transactions, while the naive approach of separating the search space failed completely.\n    ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2015-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08565",
        "title": "Attention with Intention for a Neural Network Conversation Model",
        "authors": [
            "Kaisheng Yao",
            "Geoffrey Zweig",
            "Baolin Peng"
        ],
        "abstract": "In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs.\n    ",
        "submission_date": "2015-10-29T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08568",
        "title": "Feature-Based Diversity Optimization for Problem Instance Classification",
        "authors": [
            "Wanru Gao",
            "Samadhi Nallaperuma",
            "Frank Neumann"
        ],
        "abstract": "Understanding the behaviour of heuristic search methods is a challenge. This even holds for simple local search methods such as 2-OPT for the Traveling Salesperson problem. In this paper, we present a general framework that is able to construct a diverse set of instances that are hard or easy for a given search heuristic. Such a diverse set is obtained by using an evolutionary algorithm for constructing hard or easy instances that are diverse with respect to different features of the underlying problem. Examining the constructed instance sets, we show that many combinations of two or three features give a good classification of the TSP instances in terms of whether they are hard to be solved by 2-OPT.\n    ",
        "submission_date": "2015-10-29T00:00:00",
        "last_modified_date": "2020-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08578",
        "title": "My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em Competition",
        "authors": [
            "Sam Ganzfried"
        ],
        "abstract": "The first ever human vs. computer no-limit Texas hold 'em competition took place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this article I present my thoughts on the competition design, agent architecture, and lessons learned.\n    ",
        "submission_date": "2015-10-29T00:00:00",
        "last_modified_date": "2016-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08906",
        "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning",
        "authors": [
            "Christoph Dann",
            "Emma Brunskill"
        ],
        "abstract": "Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound $\\tilde O(\\frac{|\\mathcal S|^2 |\\mathcal A| H^2}{\\epsilon^2} \\ln\\frac 1 \\delta)$ and a lower PAC bound $\\tilde \\Omega(\\frac{|\\mathcal S| |\\mathcal A| H^2}{\\epsilon^2} \\ln \\frac 1 {\\delta + c})$ that match up to log-terms and an additional linear dependency on the number of states $|\\mathcal S|$. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least $H^3$.\n    ",
        "submission_date": "2015-10-29T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08971",
        "title": "Robust Subspace Clustering via Tighter Rank Approximation",
        "authors": [
            "Zhao Kang",
            "Chong Peng",
            "Qiang Cheng"
        ],
        "abstract": "Matrix rank minimization problem is in general NP-hard. The nuclear norm is used to substitute the rank function in many recent studies. Nevertheless, the nuclear norm approximation adds all singular values together and the approximation error may depend heavily on the magnitudes of singular values. This might restrict its capability in dealing with many practical problems. In this paper, an arctangent function is used as a tighter approximation to the rank function. We use it on the challenging subspace clustering problem. For this nonconvex minimization problem, we develop an effective optimization procedure based on a type of augmented Lagrange multipliers (ALM) method. Extensive experiments on face clustering and motion segmentation show that the proposed method is effective for rank approximation.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08983",
        "title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition",
        "authors": [
            "Yu Zhang",
            "Guoguo Chen",
            "Dong Yu",
            "Kaisheng Yao",
            "Sanjeev Khudanpur",
            "James Glass"
        ],
        "abstract": "In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and $5.3\\%$ relative improvement respectively.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.09130",
        "title": "Latent Bayesian melding for integrating individual and population models",
        "authors": [
            "Mingjun Zhong",
            "Nigel Goddard",
            "Charles Sutton"
        ],
        "abstract": "In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour, whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matching expectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00083",
        "title": "Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex",
        "authors": [
            "Jeff Hawkins",
            "Subutai Ahmad"
        ],
        "abstract": "Neocortical neurons have thousands of excitatory synapses. It is a mystery how neurons integrate the input from so many synapses and what kind of large-scale network behavior this enables. It has been previously proposed that non-linear properties of dendrites enable neurons to recognize multiple patterns. In this paper we extend this idea by showing that a neuron with several thousand synapses arranged along active dendrites can learn to accurately and robustly recognize hundreds of unique patterns of cellular activity, even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where some of the patterns recognized by a neuron lead to action potentials and define the classic receptive field of the neuron, whereas the majority of the patterns recognized by a neuron act as predictions by slightly depolarizing the neuron without immediately generating an action potential. We then present a network model based on neurons with these properties and show that the network learns a robust model of time-based sequences. Given the similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory is a universal property of neocortical tissue. We further propose that cellular layers in the neocortex implement variations of the same sequence memory algorithm to achieve different aspects of inference and behavior. The neuron and network models we introduce are robust over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. The sequence capacity of the network scales linearly with the number of synapses on each neuron. Thus neurons need thousands of synapses to learn the many temporal patterns in sensory stimuli and motor sequences.\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00384",
        "title": "Z Specification for the W3C Editor's Draft Core SHACL Semantics",
        "authors": [
            "Arthur Ryman"
        ],
        "abstract": "This article provides a formalization of the W3C Draft Core SHACL Semantics specification using Z notation. This formalization exercise has identified a number of quality issues in the draft. It has also established that the recursive definitions in the draft are well-founded. Further formal validation of the draft will require the use of an executable specification technology.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00573",
        "title": "From random walks to distances on unweighted graphs",
        "authors": [
            "Tatsunori B. Hashimoto",
            "Yi Sun",
            "Tommi S. Jaakkola"
        ],
        "abstract": "Large unweighted directed graphs are commonly used to capture relations between entities. A fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices. Despite the significance of this problem, statistical characterization of the proposed metrics has been limited. We introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus. Using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the Laplace transformed hitting time (LTHT). The metric serves as a natural, provably well-behaved alternative to the expected hitting time. We establish a general correspondence between hitting times of the Brownian motion and analogous hitting times on the graph. We show that the LTHT is consistent with respect to the underlying metric of a geometric graph, preserves clustering tendency, and remains robust against random addition of non-geometric edges. Tests on simulated and real-world data show that the LTHT matches theoretical predictions and outperforms alternatives.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00725",
        "title": "Toward an Efficient Multi-class Classification in an Open Universe",
        "authors": [
            "Wajdi Dhifli",
            "Abdoulaye Banir\u00e9 Diallo"
        ],
        "abstract": "Classification is a fundamental task in machine learning and data mining. Existing classification methods are designed to classify unknown instances within a set of previously known training classes. Such a classification takes the form of a prediction within a closed-set of classes. However, a more realistic scenario that fits real-world applications is to consider the possibility of encountering instances that do not belong to any of the training classes, $i.e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce Galaxy-X, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances. In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unknown ones. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2018-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00813",
        "title": "SAT as a game",
        "authors": [
            "Olivier Bailleux"
        ],
        "abstract": "We propose a funny representation of SAT. While the primary interest is to present propositional satisfiability in a playful way for pedagogical purposes, it could also inspire new search heuristics.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00915",
        "title": "SWISH: SWI-Prolog for Sharing",
        "authors": [
            "Jan Wielemaker",
            "Torbj\u00f6rn Lager",
            "Fabrizio Riguzzi"
        ],
        "abstract": "Recently, we see a new type of interfaces for programmers based on web technology. For example, JSFiddle, IPython Notebook and R-studio. Web technology enables cloud-based solutions, embedding in tutorial web pages, atractive rendering of results, web-scale cooperative development, etc. This article describes SWISH, a web front-end for Prolog. A public website exposes SWI-Prolog using SWISH, which is used to run small Prolog programs for demonstration, experimentation and education. We connected SWISH to the ClioPatria semantic web toolkit, where it allows for collaborative development of programs and queries related to a dataset as well as performing maintenance tasks on the running server and we embedded SWISH in the Learn Prolog Now! online Prolog book.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00916",
        "title": "Lowering the learning curve for declarative programming: a Python API for the IDP system",
        "authors": [
            "Joost Vennekens"
        ],
        "abstract": "Programmers may be hesitant to use declarative systems, because of the associated learning curve. In this paper, we present an API that integrates the IDP Knowledge Base system into the Python programming language. IDP is a state-of-the-art logical system, which uses SAT, SMT, Logic Programming and Answer Set Programming technology. Python is currently one of the most widely used (teaching) languages for programming. The first goal of our API is to allow a Python programmer to use the declarative power of IDP, without needing to learn any new syntax or semantics. The second goal is allow IDP to be added to/removed from an existing code base with minimal changes.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00920",
        "title": "A web-based IDE for IDP",
        "authors": [
            "Ingmar Dasseville",
            "Gerda Janssens"
        ],
        "abstract": "IDP is a knowledge base system based on first order logic. It is finding its way to a larger public but is still facing practical challenges. Adoption of new languages requires a newcomer-friendly way for users to interact with it. Both an online presence to try to convince potential users to download the system and offline availability to develop larger applications are essential. We developed an IDE which can serve both purposes through the use of web technology. It enables us to provide the user with a modern IDE with relatively little effort.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00924",
        "title": "Bound Your Models! How to Make OWL an ASP Modeling Language",
        "authors": [
            "Sarah Alice Gaggl",
            "Sebastian Rudolph",
            "Lukas Schweizer"
        ],
        "abstract": "To exploit the Web Ontology Language OWL as an answer set programming (ASP) language, we introduce the notion of bounded model semantics, as an intuitive and computationally advantageous alternative to its classical semantics. We show that a translation into ASP allows for solving a wide range of bounded-model reasoning tasks, including satisfiability and axiom entailment but also novel ones such as model extraction and enumeration. Ultimately, our work facilitates harnessing advanced semantic web modeling environments for the logic programming community through an \"off-label use\" of OWL.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00928",
        "title": "Visualising interactive inferences with IDPD3",
        "authors": [
            "Ruben Lapauw",
            "Ingmar Dasseville",
            "Marc Denecker"
        ],
        "abstract": "A large part of the use of knowledge base systems is the interpretation of the output by the end-users and the interaction with these users. Even during the development process visualisations can be a great help to the developer. We created IDPD3 as a library to visualise models of logic theories. IDPD3 is a new version of $ID^{P}_{Draw}$ and adds support for visualised interactive simulations.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01029",
        "title": "Understanding symmetries in deep networks",
        "authors": [
            "Vijay Badrinarayanan",
            "Bamdev Mishra",
            "Roberto Cipolla"
        ],
        "abstract": "Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01411",
        "title": "Learning in Auctions: Regret is Hard, Envy is Easy",
        "authors": [
            "Constantinos Daskalakis",
            "Vasilis Syrgkanis"
        ],
        "abstract": "A line of recent work provides welfare guarantees of simple combinatorial auction formats, such as selling m items via simultaneous second price auctions (SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et al. 2013). These guarantees hold even when the auctions are repeatedly executed and players use no-regret learning algorithms. Unfortunately, off-the-shelf no-regret algorithms for these auctions are computationally inefficient as the number of actions is exponential. We show that this obstacle is insurmountable: there are no polynomial-time no-regret algorithms for SiSPAs, unless RP$\\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises the question of how good outcomes polynomially-bounded bidders may discover in such auctions.\n",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01419",
        "title": "Train and Test Tightness of LP Relaxations in Structured Prediction",
        "authors": [
            "Ofer Meshi",
            "Mehrdad Mahdavi",
            "Adrian Weller",
            "David Sontag"
        ],
        "abstract": "Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01754",
        "title": "Symmetry-invariant optimization in deep networks",
        "authors": [
            "Vijay Badrinarayanan",
            "Bamdev Mishra",
            "Roberto Cipolla"
        ],
        "abstract": "Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01870",
        "title": "Thoughts on Massively Scalable Gaussian Processes",
        "authors": [
            "Andrew Gordon Wilson",
            "Christoph Dann",
            "Hannes Nickisch"
        ],
        "abstract": "We introduce a framework and early results for massively scalable Gaussian processes (MSGP), significantly extending the KISS-GP approach of Wilson and Nickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs) on billions of datapoints, without requiring distributed inference, or severe assumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GP learning and inference to $O(n)$, and the standard $O(n^2)$ complexity per test point prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices as Kronecker products of Toeplitz matrices approximated by circulant matrices. This multi-level circulant approximation allows one to unify the orthogonal computational benefits of fast Kronecker and Toeplitz approaches, and is significantly faster than either approach in isolation; 2) local kernel interpolation and inducing points to allow for arbitrarily located data inputs, and $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block structure (BTTB), which enables fast inference and learning when multidimensional Kronecker structure is not present; and 4) projections of the input space to flexibly model correlated inputs and high dimensional data. The ability to handle many ($m \\approx n$) inducing points allows for near-exact accuracy and large scale kernel learning.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02163",
        "title": "Submodular Hamming Metrics",
        "authors": [
            "Jennifer Gillenwater",
            "Rishabh Iyer",
            "Bethany Lusch",
            "Rahul Kidambi",
            "Jeff Bilmes"
        ],
        "abstract": "We show that there is a largely unexplored class of functions (positive polymatroids) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over. By exploiting submodularity, we are able to give hardness results and approximation algorithms for optimizing over such metrics. Additionally, we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task (a form of clustering) and also a metric maximization task (generating diverse k-best lists).\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02222",
        "title": "Deep Kernel Learning",
        "authors": [
            "Andrew Gordon Wilson",
            "Zhiting Hu",
            "Ruslan Salakhutdinov",
            "Eric P. Xing"
        ],
        "abstract": "We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost $O(n)$ for $n$ training points, and predictions cost $O(1)$ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02290",
        "title": "Combining Privileged Information to Improve Context-Aware Recommender Systems",
        "authors": [
            "Camila V. Sundermann",
            "Marcos A. Domingues",
            "Ricardo M. Marcacini",
            "Solange O. Rezende"
        ],
        "abstract": "A recommender system is an information filtering technology which can be used to predict preference ratings of items (products, services, movies, etc) and/or to output a ranking of items that are likely to be of interest to the user. Context-aware recommender systems (CARS) learn and predict the tastes and preferences of users by incorporating available contextual information in the recommendation process. One of the major challenges in context-aware recommender systems research is the lack of automatic methods to obtain contextual information for these systems. Considering this scenario, in this paper, we propose to use contextual information from topic hierarchies of the items (web pages) to improve the performance of context-aware recommender systems. The topic hierarchies are constructed by an extension of the LUPI-based Incremental Hierarchical Clustering method that considers three types of information: traditional bag-of-words (technical information), and the combination of named entities (privileged information I) with domain terms (privileged information II). We evaluated the contextual information in four context-aware recommender systems. Different weights were assigned to each type of information. The empirical results demonstrated that topic hierarchies with the combination of the two kinds of privileged information can provide better recommendations.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2019-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02385",
        "title": "Review-Level Sentiment Classification with Sentence-Level Polarity Correction",
        "authors": [
            "Sylvester Olubolu Orimaye",
            "Saadat M. Alhashmi",
            "Eu-Gene Siew",
            "Sang Jung Kang"
        ],
        "abstract": "We propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction. Our polarity correction technique takes into account the consistency of the polarities (positive and negative) of sentences within each product review before performing the actual machine learning task. While sentences with inconsistent polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers. The technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique. Experimental results show an average of 82% F-measure on four different product review domains.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2015-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02436",
        "title": "Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment using Compound Skip-grams",
        "authors": [
            "Sylvester Olubolu Orimaye",
            "Kah Yee Tai",
            "Jojo Sze-Meng Wong",
            "Chee Piau Wong"
        ],
        "abstract": "Predicting Mild Cognitive Impairment (MCI) is currently a challenge as existing diagnostic criteria rely on neuropsychological examinations. Automated Machine Learning (ML) models that are trained on verbal utterances of MCI patients can aid diagnosis. Using a combination of skip-gram features, our model learned several linguistic biomarkers to distinguish between 19 patients with MCI and 19 healthy control individuals from the DementiaBank language transcript clinical dataset. Results show that a model with compound of skip-grams has better AUC and could help ML prediction on small MCI data sample.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02619",
        "title": "Decomposition Bounds for Marginal MAP",
        "authors": [
            "Wei Ping",
            "Qiang Liu",
            "Alexander Ihler"
        ],
        "abstract": "Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02872",
        "title": "Visual Language Modeling on CNN Image Representations",
        "authors": [
            "Hiroharu Kato",
            "Tatsuya Harada"
        ],
        "abstract": "Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02916",
        "title": "Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders",
        "authors": [
            "Zhouhan Lin",
            "Yushi Chen",
            "Xing Zhao",
            "Gang Wang"
        ],
        "abstract": "Hyperspectral image (HSI) classification is a hot topic in the remote sensing community. This paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02917",
        "title": "Detecting events and key actors in multi-person videos",
        "authors": [
            "Vignesh Ramanathan",
            "Jonathan Huang",
            "Sami Abu-El-Haija",
            "Alexander Gorban",
            "Kevin Murphy",
            "Li Fei-Fei"
        ],
        "abstract": "Multi-person event recognition is a challenging task, often with many people active in the scene but only a small subset contributing to an actual event. In this paper, we propose a model which learns to detect events in such videos while automatically \"attending\" to the people responsible for the event. Our model does not use explicit annotations regarding who or where those people are during training and testing. In particular, we track people in videos and use a recurrent neural network (RNN) to represent the track features. We learn time-varying attention weights to combine these features at each time-instant. The attended features are then processed using another RNN for event detection/classification. Since most video datasets with multiple people are restricted to a small number of videos, we also collected a new basketball dataset comprising 257 basketball games with 14K event annotations corresponding to 11 event classes. Our model outperforms state-of-the-art methods for both event classification and detection on this new dataset. Additionally, we show that the attention mechanism is able to consistently localize the relevant players.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02995",
        "title": "Incorporating Knowledge into Structural Equation Models using Auxiliary Variables",
        "authors": [
            "Bryant Chen",
            "Judea Pearl",
            "Elias Bareinboim"
        ],
        "abstract": "In this paper, we extend graph-based identification methods by allowing background knowledge in the form of non-zero parameter values. Such information could be obtained, for example, from a previously conducted randomized experiment, from substantive understanding of the domain, or even an identification technique. To incorporate such information systematically, we propose the addition of auxiliary variables to the model, which are constructed so that certain paths will be conveniently cancelled. This cancellation allows the auxiliary variables to help conventional methods of identification (e.g., single-door criterion, instrumental variables, half-trek criterion), as well as model testing (e.g., d-separation, over-identification). Moreover, by iteratively alternating steps of identification and adding auxiliary variables, we can improve the power of existing identification methods via a bootstrapping approach that does not require external knowledge. We operationalize this method for simple instrumental sets (a generalization of instrumental variables) and show that the resulting method is able to identify at least as many models as the most general identification method for linear systems known to date. We further discuss the application of auxiliary variables to the tasks of model testing and z-identification.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2016-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03012",
        "title": "Information retrieval in folktales using natural language processing",
        "authors": [
            "Adrian Groza",
            "Lidia Corde"
        ],
        "abstract": "Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based on Propp's model for folktales morphology.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03292",
        "title": "From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge",
        "authors": [
            "Somak Aditya",
            "Yezhou Yang",
            "Chitta Baral",
            "Cornelia Fermuller",
            "Yiannis Aloimonos"
        ],
        "abstract": "In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a \"commonsense\" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03361",
        "title": "Discovery Radiomics via StochasticNet Sequencers for Cancer Detection",
        "authors": [
            "Mohammad Javad Shafiee",
            "Audrey G. Chung",
            "Devinder Kumar",
            "Farzad Khalvati",
            "Masoom Haider",
            "Alexander Wong"
        ],
        "abstract": "Radiomics has proven to be a powerful prognostic tool for cancer detection, and has previously been applied in lung, breast, prostate, and head-and-neck cancer studies with great success. However, these radiomics-driven methods rely on pre-defined, hand-crafted radiomic feature sets that can limit their ability to characterize unique cancer traits. In this study, we introduce a novel discovery radiomics framework where we directly discover custom radiomic features from the wealth of available medical imaging data. In particular, we leverage novel StochasticNet radiomic sequencers for extracting custom radiomic features tailored for characterizing unique cancer tissue phenotype. Using StochasticNet radiomic sequencers discovered using a wealth of lung CT data, we perform binary classification on 42,340 lung lesions obtained from the CT scans of 93 patients in the LIDC-IDRI dataset. Preliminary results show significant improvement over previous state-of-the-art methods, indicating the potential of the proposed discovery radiomics framework for improving cancer screening and diagnosis.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03690",
        "title": "Deep Multimodal Semantic Embeddings for Speech and Images",
        "authors": [
            "David Harwath",
            "James Glass"
        ],
        "abstract": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03722",
        "title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning",
        "authors": [
            "Nan Jiang",
            "Lihong Li"
        ],
        "abstract": "We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL in real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator's accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the hardness of the problem, and show that our estimator can match the lower bound in certain scenarios.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03749",
        "title": "Complexity of the Description Logic ALCM",
        "authors": [
            "Monica Martinez",
            "Edelweis Rohrer",
            "Paula Severi"
        ],
        "abstract": "In this paper we show that the problem of checking consistency of a knowledge base in the Description Logic ALCM is ExpTime-complete. The M stands for meta-modelling as defined by Motz, Rohrer and Severi. To show our main result, we define an ExpTime Tableau algorithm as an extension of an algorithm for checking consistency of a knowledge base in ALC by Nguyen and Szalas.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03816",
        "title": "Characterizing Concept Drift",
        "authors": [
            "Geoffrey I. Webb",
            "Roy Hyde",
            "Hong Cao",
            "Hai Long Nguyen",
            "Francois Petitjean"
        ],
        "abstract": "Most machine learning models are static, but the world is dynamic, and increasing online deployment of learned models gives increasing urgency to the development of efficient and effective mechanisms to address learning in the context of non-stationary distributions, or as it is commonly called concept drift. However, the key issue of characterizing the different types of drift that can occur has not previously been subjected to rigorous definition and analysis. In particular, while some qualitative drift categorizations have been proposed, few have been formally defined, and the quantitative descriptions required for precise and objective understanding of learner performance have not existed. We present the first comprehensive framework for quantitative analysis of drift. This supports the development of the first comprehensive set of formal definitions of types of concept drift. The formal definitions clarify ambiguities and identify gaps in previous definitions, giving rise to a new comprehensive taxonomy of concept drift types and a solid foundation for research into mechanisms to detect and address concept drift.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04137",
        "title": "Seeing the Unseen Network: Inferring Hidden Social Ties from Respondent-Driven Sampling",
        "authors": [
            "Lin Chen",
            "Forrest W. Crawford",
            "Amin Karbasi"
        ],
        "abstract": "Learning about the social structure of hidden and hard-to-reach populations --- such as drug users and sex workers --- is a major goal of epidemiological and public health research on risk behaviors and disease prevention. Respondent-driven sampling (RDS) is a peer-referral process widely used by many health organizations, where research subjects recruit other subjects from their social network. In such surveys, researchers observe who recruited whom, along with the time of recruitment and the total number of acquaintances (network degree) of respondents. However, due to privacy concerns, the identities of acquaintances are not disclosed. In this work, we show how to reconstruct the underlying network structure through which the subjects are recruited. We formulate the dynamics of RDS as a continuous-time diffusion process over the underlying graph and derive the likelihood for the recruitment time series under an arbitrary recruitment time distribution. We develop an efficient stochastic optimization algorithm called RENDER (REspoNdent-Driven nEtwork Reconstruction) that finds the network that best explains the collected data. We support our analytical results through an exhaustive set of experiments on both synthetic and real data.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04190",
        "title": "On Choosing Committees Based on Approval Votes in the Presence of Outliers",
        "authors": [
            "Palash Dey",
            "Neeldhara Misra",
            "Y. Narahari"
        ],
        "abstract": "We study the computational complexity of committee selection problem for several approval-based voting rules in the presence of outliers. Our first result shows that outlier consideration makes committee selection problem intractable for approval, net approval, and minisum approval voting rules. We then study parameterized complexity of this problem with five natural parameters, namely the target score, the size of the committee (and its dual parameter, the number of candidates outside the committee), the number of outliers (and its dual parameter, the number of non-outliers). For net approval and minisum approval voting rules, we provide a dichotomous result, resolving the parameterized complexity of this problem for all subsets of five natural parameters considered (by showing either FPT or W[1]-hardness for all subsets of parameters). For the approval voting rule, we resolve the parameterized complexity of this problem for all subsets of parameters except one.\n",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04317",
        "title": "Novel Feature Extraction, Selection and Fusion for Effective Malware Family Classification",
        "authors": [
            "Mansour Ahmadi",
            "Dmitry Ulyanov",
            "Stanislav Semenov",
            "Mikhail Trofimov",
            "Giorgio Giacinto"
        ],
        "abstract": "Modern malware is designed with mutation characteristics, namely polymorphism and metamorphism, which causes an enormous growth in the number of variants of malware samples. Categorization of malware samples on the basis of their behaviors is essential for the computer security community, because they receive huge number of malware everyday, and the signature extraction process is usually based on malicious parts characterizing malware families. Microsoft released a malware classification challenge in 2015 with a huge dataset of near 0.5 terabytes of data, containing more than 20K malware samples. The analysis of this dataset inspired the development of a novel paradigm that is effective in categorizing malware variants into their actual family groups. This paradigm is presented and discussed in the present paper, where emphasis has been given to the phases related to the extraction, and selection of a set of novel features for the effective representation of malware samples. Features can be grouped according to different characteristics of malware behavior, and their fusion is performed according to a per-class weighting paradigm. The proposed method achieved a very high accuracy ($\\approx$ 0.998) on the Microsoft Malware Challenge dataset.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04387",
        "title": "Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode Resource-constrained Multi-project Scheduling Problem",
        "authors": [
            "Shahriar Asta",
            "Daniel Karapetyan",
            "Ahmed Kheiri",
            "Ender \u00d6zcan",
            "Andrew J. Parkes"
        ],
        "abstract": "Multi-mode resource and precedence-constrained project scheduling is a well-known challenging real-world optimisation problem. An important variant of the problem requires scheduling of activities for multiple projects considering availability of local and global resources while respecting a range of constraints. A critical aspect of the benchmarks addressed in this paper is that the primary objective is to minimise the sum of the project completion times, with the usual makespan minimisation as a secondary objective. We observe that this leads to an expected different overall structure of good solutions and discuss the effects this has on the algorithm design. This paper presents a carefully designed hybrid of Monte-Carlo tree search, novel neighbourhood moves, memetic algorithms, and hyper-heuristic methods. The implementation is also engineered to increase the speed with which iterations are performed, and to exploit the computing power of multicore machines. Empirical evaluation shows that the resulting information-sharing multi-component algorithm significantly outperforms other solvers on a set of \"hidden\" instances, i.e. instances not available at the algorithm design phase.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04412",
        "title": "Dynamic Sum Product Networks for Tractable Inference on Sequence Data (Extended Version)",
        "authors": [
            "Mazen Melibari",
            "Pascal Poupart",
            "Prashant Doshi",
            "George Trimponias"
        ],
        "abstract": "Sum-Product Networks (SPN) have recently emerged as a new class of tractable probabilistic graphical models. Unlike Bayesian networks and Markov networks where inference may be exponential in the size of the network, inference in SPNs is in time linear in the size of the network. Since SPNs represent distributions over a fixed set of variables only, we propose dynamic sum product networks (DSPNs) as a generalization of SPNs for sequence data of varying length. A DSPN consists of a template network that is repeated as many times as needed to model data sequences of any length. We present a local search technique to learn the structure of the template network. In contrast to dynamic Bayesian networks for which inference is generally exponential in the number of variables per time slice, DSPNs inherit the linear inference complexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and other models on several datasets of sequence data.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04583",
        "title": "Demand-Driven Incremental Object Queries",
        "authors": [
            "Yanhong A. Liu",
            "Jon Brandvein",
            "Scott D. Stoller",
            "Bo Lin"
        ],
        "abstract": "Object queries are essential in information seeking and decision making in vast areas of applications. However, a query may involve complex conditions on objects and sets, which can be arbitrarily nested and aliased. The objects and sets involved as well as the demand---the given parameter values of interest---can change arbitrarily. How to implement object queries efficiently under all possible updates, and furthermore to provide complexity guarantees?\n",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04646",
        "title": "Word Embedding based Correlation Model for Question/Answer Matching",
        "authors": [
            "Yikang Shen",
            "Wenge Rong",
            "Nan Jiang",
            "Baolin Peng",
            "Jie Tang",
            "Zhang Xiong"
        ],
        "abstract": "With the development of community based question answering (Q&A) services, a large scale of Q&A archives have been accumulated and are an important information and knowledge resource on the web. Question and answer matching has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, we try to improve the matching accuracy by overcoming the lexical gap between question and answer pairs. A Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding, given a random pair of words, WEC can score their co-occurrence probability in Q&A pairs and it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method's promising potential.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04798",
        "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization",
        "authors": [
            "Baohan Xu",
            "Yanwei Fu",
            "Yu-Gang Jiang",
            "Boyang Li",
            "Leonid Sigal"
        ],
        "abstract": "Emotion is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in understanding video emotion: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2018-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05101",
        "title": "How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?",
        "authors": [
            "Ferenc Husz\u00e1r"
        ],
        "abstract": "Modern applications and progress in deep learning research have created renewed interest for generative models of text and of images. However, even today it is unclear what objective functions one should use to train and evaluate these models. In this paper we present two contributions.\n",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05175",
        "title": "Convolutional Models for Joint Object Categorization and Pose Estimation",
        "authors": [
            "Mohamed Elhoseiny",
            "Tarek El-Gaaly",
            "Amr Bakry",
            "Ahmed Elgammal"
        ],
        "abstract": "In the task of Object Recognition, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose regression using these approaches has received relatively much less attention. In this paper we show how deep architectures, specifically Convolutional Neural Networks (CNN), can be adapted to the task of simultaneous categorization and pose estimation of objects. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations of CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets. Our models achieve better than state-of-the-art performance on both datasets.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05234",
        "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering",
        "authors": [
            "Huijuan Xu",
            "Kate Saenko"
        ],
        "abstract": "We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single \"hop\" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3].\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05385",
        "title": "Bayesian Optimization with Dimension Scheduling: Application to Biological Systems",
        "authors": [
            "Doniyor Ulmasov",
            "Caroline Baroukh",
            "Benoit Chachuat",
            "Marc Peter Deisenroth",
            "Ruth Misener"
        ],
        "abstract": "Bayesian Optimization (BO) is a data-efficient method for global black-box optimization of an expensive-to-evaluate fitness function. BO typically assumes that computation cost of BO is cheap, but experiments are time consuming or costly. In practice, this allows us to optimize ten or fewer critical parameters in up to 1,000 experiments. But experiments may be less expensive than BO methods assume: In some simulation models, we may be able to conduct multiple thousands of experiments in a few hours, and the computational burden of BO is no longer negligible compared to experimentation time. To address this challenge we introduce a new Dimension Scheduling Algorithm (DSA), which reduces the computational burden of BO for many experiments. The key idea is that DSA optimizes the fitness function only along a small set of dimensions at each iteration. This DSA strategy (1) reduces the necessary computation time, (2) finds good solutions faster than the traditional BO method, and (3) can be parallelized straightforwardly. We evaluate the DSA in the context of optimizing parameters of dynamic models of microalgae metabolism and show faster convergence than traditional BO.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05488",
        "title": "Active exploration of sensor networks from a robotics perspective",
        "authors": [
            "Christian Blum",
            "Verena V. Hafner"
        ],
        "abstract": "Traditional algorithms for robots who need to integrate into a wireless network often focus on one specific task. In this work we want to develop simple, adaptive and reusable algorithms for real world applications for this scenario. Starting with the most basic task for mobile wireless network nodes, finding the position of another node, we introduce an algorithm able to solve this task. We then show how this algorithm can readily be employed to solve a large number of other related tasks like finding the optimal position to bridge two static network nodes. For this we first introduce a meta-algorithm inspired by autonomous robot learning strategies and the concept of internal models which yields a class of source seeking algorithms for mobile nodes. The effectiveness of this algorithm is demonstrated in real world experiments using a physical mobile robot and standard 802.11 wireless LAN in an office environment. We also discuss the differences to conventional algorithms and give the robotics perspective on this class of algorithms. Then we proceed to show how more complex tasks, which might be encountered by mobile nodes, can be encoded in the same framework and how the introduced algorithm can solve them. These tasks can be direct (cross layer) optimization tasks or can also encode more complex tasks like bridging two network nodes. We choose the bridging scenario as an example, implemented on a real physical robot, and show how the robot can solve it in a real world experiment.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05493",
        "title": "Gated Graph Sequence Neural Networks",
        "authors": [
            "Yujia Li",
            "Daniel Tarlow",
            "Marc Brockschmidt",
            "Richard Zemel"
        ],
        "abstract": "Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05547",
        "title": "Return of Frustratingly Easy Domain Adaptation",
        "authors": [
            "Baochen Sun",
            "Jiashi Feng",
            "Kate Saenko"
        ],
        "abstract": "Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being \"frustratingly easy\" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05643",
        "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation",
        "authors": [
            "Md Kamrul Hasan",
            "Christopher J. Pal"
        ],
        "abstract": "We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with Gaussian- Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05835",
        "title": "Alternative Markov and Causal Properties for Acyclic Directed Mixed Graphs",
        "authors": [
            "Jose M. Pe\u00f1a"
        ],
        "abstract": "We extend Andersson-Madigan-Perlman chain graphs by (i) relaxing the semidirected acyclity constraint so that only directed cycles are forbidden, and (ii) allowing up to two edges between any pair of nodes. We introduce global, and ordered local and pairwise Markov properties for the new models. We show the equivalence of these properties for strictly positive probability distributions. We also show that when the random variables are continuous, the new models can be interpreted as systems of structural equations with correlated errors. This enables us to adapt Pearl's do-calculus to them. Finally, we describe an exact algorithm for learning the new models from observational and interventional data via answer set programming.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2016-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05897",
        "title": "Censoring Representations with an Adversary",
        "authors": [
            "Harrison Edwards",
            "Amos Storkey"
        ],
        "abstract": "In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alternatively it can be that that representation of data must not have identifying information. We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images. We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alternate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases. The flexibility of this method is shown via a novel problem: removing annotations from images, from unaligned training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05911",
        "title": "Behavior Query Discovery in System-Generated Temporal Graphs",
        "authors": [
            "Bo Zong",
            "Xusheng Xiao",
            "Zhichun Li",
            "Zhenyu Wu",
            "Zhiyun Qian",
            "Xifeng Yan",
            "Ambuj K. Singh",
            "Guofei Jiang"
        ],
        "abstract": "Computer system monitoring generates huge amounts of logs that record the interaction of system entities. How to query such data to better understand system behaviors and identify potential system risks and malicious behaviors becomes a challenging task for system administrators due to the dynamics and heterogeneity of the data. System monitoring data are essentially heterogeneous temporal graphs with nodes being system entities and edges being their interactions over time. Given the complexity of such graphs, it becomes time-consuming for system administrators to manually formulate useful queries in order to examine abnormal activities, attacks, and vulnerabilities in computer systems.\n",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05943",
        "title": "Unitary-Group Invariant Kernels and Features from Transformed Unlabeled Data",
        "authors": [
            "Dipan K. Pal",
            "Marios Savvides"
        ],
        "abstract": "The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to the unitary group while having theoretical guarantees in addressing practical issues such as (1) unavailability of transformed versions of labelled data and (2) not observing all transformations. We present a theoretically motivated alternate approach to the invariant kernel SVM. Unlike previous approaches to the invariant SVM, the proposed formulation solves both issues mentioned. We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability. We present experiments on the UCI ML datasets to illustrate and validate our methods.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06052",
        "title": "Overcoming Language Variation in Sentiment Analysis with Social Attention",
        "authors": [
            "Yi Yang",
            "Jacob Eisenstein"
        ],
        "abstract": "Variation in language is ubiquitous, particularly in newer forms of writing such as social media. Fortunately, variation is not random, it is often linked to social properties of the author. In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation. The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author's position in the social network. This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This model significantly improves the accuracies of sentiment analysis on Twitter and on review data.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2017-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06341",
        "title": "Communicating Semantics: Reference by Description",
        "authors": [
            "Ramanathan V Guha",
            "Vineet Gupta"
        ],
        "abstract": "Messages often refer to entities such as people, places and events. Correct identification of the intended reference is an essential part of communication. Lack of shared unique names often complicates entity reference. Shared knowledge can be used to construct uniquely identifying descriptive references for entities with ambiguous names. We introduce a mathematical model for `Reference by Description', derive results on the conditions under which, with high probability, programs can construct unambiguous references to most entities in the domain of discourse and provide empirical validation of these results.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06380",
        "title": "Unsupervised Learning of Visual Structure using Predictive Generative Networks",
        "authors": [
            "William Lotter",
            "Gabriel Kreiman",
            "David Cox"
        ],
        "abstract": "The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06410",
        "title": "Better Computer Go Player with Neural Network and Long-term Prediction",
        "authors": [
            "Yuandong Tian",
            "Yan Zhu"
        ],
        "abstract": "Competing with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go's high branching factor makes traditional search techniques ineffective, even on leading-edge hardware, and Go's evaluation function could change drastically with one stone change. Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited. We extend this idea in our bot named darkforest, which relies on a DCNN designed for long-term predictions. Darkforest substantially improves the win rate for pattern-matching approaches against MCTS-based approaches, even with looser search budgets. Against human players, the newest versions, darkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, a substantial improvement upon the estimated 4k-5k ranks for DCNN reported in Clark & Storkey (2015) based on games against other machine players. Adding MCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000 rollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone) except for AlphaGo [Silver et al. (2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06438",
        "title": "Joint Word Representation Learning using a Corpus and a Semantic Lexicon",
        "authors": [
            "Danushka Bollegala",
            "Alsuhaibani Mohammed",
            "Takanori Maehara",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "Methods for learning word representations using large text corpora have received much attention lately due to their impressive performance in numerous natural language processing (NLP) tasks such as, semantic similarity measurement, and word analogy detection. Despite their success, these data-driven word representation learning methods do not consider the rich semantic relational structure between words in a co-occurring context. On the other hand, already much manual effort has gone into the construction of semantic lexicons such as the WordNet that represent the meanings of words by defining the various relationships that exist among the words in a language. We consider the question, can we improve the word representations learnt using a corpora by integrating the knowledge from semantic lexicons?. For this purpose, we propose a joint word representation learning method that simultaneously predicts the co-occurrences of two words in a sentence subject to the relational constrains given by the semantic lexicon. We use relations that exist between words in the lexicon to regularize the word representations learnt from the corpus. Our proposed method statistically significantly outperforms previously proposed methods for incorporating semantic lexicons into word representations on several benchmark datasets for semantic similarity and word analogy.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06586",
        "title": "Crowd Behavior Analysis: A Review where Physics meets Biology",
        "authors": [
            "Ven Jyn Kok",
            "Mei Kuan Lim",
            "Chee Seng Chan"
        ],
        "abstract": "Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irre- vocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision stud- ies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06728",
        "title": "Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning",
        "authors": [
            "Natalia Neverova",
            "Christian Wolf",
            "Florian Nebout",
            "Graham Taylor"
        ],
        "abstract": "We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06890",
        "title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond",
        "authors": [
            "Chun Kai Ling",
            "Kian Hsiang Low",
            "Patrick Jaillet"
        ],
        "abstract": "This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of epsilon-GPP with performance guarantee. We empirically demonstrate the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06891",
        "title": "Near-Optimal Active Learning of Multi-Output Gaussian Processes",
        "authors": [
            "Yehong Zhang",
            "Trong Nghia Hoang",
            "Kian Hsiang Low",
            "Mohan Kankanhalli"
        ],
        "abstract": "This paper addresses the problem of active learning of a multi-output Gaussian process (MOGP) model representing multiple types of coexisting correlated environmental phenomena. In contrast to existing works, our active learning problem involves selecting not just the most informative sampling locations to be observed but also the types of measurements at each selected location for minimizing the predictive uncertainty (i.e., posterior joint entropy) of a target phenomenon of interest given a sampling budget. Unfortunately, such an entropy criterion scales poorly in the numbers of candidate sampling locations and selected observations when optimized. To resolve this issue, we first exploit a structure common to sparse MOGP models for deriving a novel active learning criterion. Then, we exploit a relaxed form of submodularity property of our new criterion for devising a polynomial-time approximation algorithm that guarantees a constant-factor approximation of that achieved by the optimal set of selected observations. Empirical evaluation on real-world datasets shows that our proposed approach outperforms existing algorithms for active learning of MOGP and single-output GP models.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06995",
        "title": "Non-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation",
        "authors": [
            "Paolo Dragone"
        ],
        "abstract": "Non-sentential utterances (NSUs) are utterances that lack a complete sentential form but whose meaning can be inferred from the dialogue context, such as \"OK\", \"where?\", \"probably at his apartment\". The interpretation of non-sentential utterances is an important problem in computational linguistics since they constitute a frequent phenomena in dialogue and they are intrinsically context-dependent. The interpretation of NSUs is the task of retrieving their full semantic content from their form and the dialogue context. The first half of this thesis is devoted to the NSU classification task. Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \"resolution rules\" built on top of the Type Theory with Records (TTR). Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism Lison (2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation. However, the probabilistic rules can also encode probabilistic knowledge, thereby providing a principled account of ambiguities in the NSU resolution process.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2015-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07209",
        "title": "Multi-Agent Continuous Transportation with Online Balanced Partitioning",
        "authors": [
            "Chao Wang",
            "Somchaya Liemhetcharat",
            "Kian Hsiang Low"
        ],
        "abstract": "We introduce the concept of continuous transportation task to the context of multi-agent systems. A continuous transportation task is one in which a multi-agent team visits a number of fixed locations, picks up objects, and delivers them to a final destination. The goal is to maximize the rate of transportation while the objects are replenished over time. Examples of problems that need continuous transportation are foraging, area sweeping, and first/last mile problem. Previous approaches typically neglect the interference and are highly dependent on communications among agents. Some also incorporate an additional reconnaissance agent to gather information. In this paper, we present a hybrid of centralized and distributed approaches that minimize the interference and communications in the multi-agent team without the need for a reconnaissance agent. We contribute two partitioning-transportation algorithms inspired by existing algorithms, and contribute one novel online partitioning-transportation algorithm with information gathering in the multi-agent team. Our algorithms have been implemented and tested extensively in the simulation. The results presented in this paper demonstrate the effectiveness of our algorithms that outperform the existing algorithms, even without any communications between the agents and without the presence of a reconnaissance agent.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07361",
        "title": "Interpretable Two-level Boolean Rule Learning for Classification",
        "authors": [
            "Guolong Su",
            "Dennis Wei",
            "Kush R. Varshney",
            "Dmitry M. Malioutov"
        ],
        "abstract": "This paper proposes algorithms for learning two-level Boolean rules in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF, i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule. Two formulations are proposed. The first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule. We generalize a previously proposed linear programming (LP) relaxation from one-level to two-level rules. The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample. Based on this second formulation, block coordinate descent and alternating minimization algorithms are developed. Experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity, and the two algorithms based on the Hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison. A proposed approach to binarize any fractional values in the optimal solutions of LP relaxations is also shown to be effective.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07401",
        "title": "MazeBase: A Sandbox for Learning from Games",
        "authors": [
            "Sainbayar Sukhbaatar",
            "Arthur Szlam",
            "Gabriel Synnaeve",
            "Soumith Chintala",
            "Rob Fergus"
        ],
        "abstract": "This paper introduces MazeBase: an environment for simple 2D games, designed as a sandbox for machine learning approaches to reasoning and planning. Within it, we create 10 simple games embodying a range of algorithmic tasks (e.g. if-then statements or set negation). A variety of neural models (fully connected, convolutional network, memory network) are deployed via reinforcement learning on these games, with and without a procedurally generated curriculum. Despite the tasks' simplicity, the performance of the models is far from optimal, suggesting directions for future development. We also demonstrate the versatility of MazeBase by using it to emulate small combat scenarios from StarCraft. Models trained on the MazeBase version can be directly applied to StarCraft, where they consistently beat the in-game AI.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07569",
        "title": "A Survey of Signed Network Mining in Social Media",
        "authors": [
            "Jiliang Tang",
            "Yi Chang",
            "Charu Aggarwal",
            "Huan Liu"
        ],
        "abstract": "Many real-world relations can be represented by signed networks with positive and negative links, as a result of which signed network analysis has attracted increasing attention from multiple disciplines. With the increasing prevalence of social media networks, signed network analysis has evolved from developing and measuring theories to mining tasks. In this article, we present a review of mining signed networks in the context of social media and discuss some promising research directions and new frontiers. We begin by giving basic concepts and unique properties and principles of signed networks. Then we classify and review tasks of signed network mining with representative algorithms. We also delineate some tasks that have not been extensively studied with formal definitions and also propose research directions to expand the field of signed network mining.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07710",
        "title": "Searching for Objects using Structure in Indoor Scenes",
        "authors": [
            "Varun K. Nagaraja",
            "Vlad I. Morariu",
            "Larry S. Davis"
        ],
        "abstract": "To identify the location of objects of a particular class, a passive computer vision system generally processes all the regions in an image to finally output few regions. However, we can use structure in the scene to search for objects without processing the entire image. We propose a search technique that sequentially processes image regions such that the regions that are more likely to correspond to the query class object are explored earlier. We frame the problem as a Markov decision process and use an imitation learning algorithm to learn a search strategy. Since structure in the scene is essential for search, we work with indoor scene images as they contain both unary scene context information and object-object context in the scene. We perform experiments on the NYU-depth v2 dataset and show that the unary scene context features alone can achieve a significantly high average precision while processing only 20-25\\% of the regions for classes like bed and sofa. By considering object-object context along with the scene context features, the performance is further improved for classes like counter, lamp, pillow and sofa.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08136",
        "title": "Unifying Decision Trees Split Criteria Using Tsallis Entropy",
        "authors": [
            "Yisen Wang",
            "Chaobing Song",
            "Shu-Tao Xia"
        ],
        "abstract": "The construction of efficient and effective decision trees remains a key topic in machine learning because of their simplicity and flexibility. A lot of heuristic algorithms have been proposed to construct near-optimal decision trees. ID3, C4.5 and CART are classical decision tree algorithms and the split criteria they used are Shannon entropy, Gain Ratio and Gini index respectively. All the split criteria seem to be independent, actually, they can be unified in a Tsallis entropy framework. Tsallis entropy is a generalization of Shannon entropy and provides a new approach to enhance decision trees' performance with an adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC) algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index, which generalizes the split criteria of decision trees. More importantly, we reveal the relations between Tsallis entropy with different $q$ and other split criteria. Experimental results on UCI data sets indicate that the TEC algorithm achieves statistically significant improvement over the classical algorithms.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08250",
        "title": "Recurrent Instance Segmentation",
        "authors": [
            "Bernardino Romera-Paredes",
            "Philip H. S. Torr"
        ],
        "abstract": "Instance segmentation is the problem of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation approaches consist of ensembles of modules that are trained independently of each other, thus missing opportunities for joint learning. Here we propose a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially. The model is based on a recurrent neural network that sequentially finds objects and their segmentations one at a time. This net is provided with a spatial memory that keeps track of what pixels have been explained and allows occlusion handling. In order to train the model we designed a principled loss function that accurately represents the properties of the instance segmentation problem. In the experiments carried out, we found that our method outperforms recent approaches on multiple person segmentation, and all state of the art approaches on the Plant Phenotyping dataset for leaf counting.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08495",
        "title": "Incremental Truncated LSTD",
        "authors": [
            "Clement Gehring",
            "Yangchen Pan",
            "Martha White"
        ],
        "abstract": "Balancing between computational efficiency and sample efficiency is an important goal in reinforcement learning. Temporal difference (TD) learning algorithms stochastically update the value function, with a linear time complexity in the number of features, whereas least-squares temporal difference (LSTD) algorithms are sample efficient but can be quadratic in the number of features. In this work, we develop an efficient incremental low-rank LSTD({\\lambda}) algorithm that progresses towards the goal of better balancing computation and sample efficiency. The algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample complexity of LSTD. We derive a simulation bound on the solution given by truncated low-rank approximation, illustrating a bias- variance trade-off dependent on the choice of rank. We demonstrate that the algorithm effectively balances computational complexity and sample efficiency for policy evaluation in a benchmark task and a high-dimensional energy allocation domain.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08724",
        "title": "On the convergence of cycle detection for navigational reinforcement learning",
        "authors": [
            "Tom J. Ameloot",
            "Jan Van den Bussche"
        ],
        "abstract": "We consider a reinforcement learning framework where agents have to navigate from start states to goal states. We prove convergence of a cycle-detection learning algorithm on a class of tasks that we call reducible. Reducible tasks have an acyclic solution. We also syntactically characterize the form of the final policy. This characterization can be used to precisely detect the convergence point in a simulation. Our result demonstrates that even simple algorithms can be successful in learning a large class of nontrivial tasks. In addition, our framework is elementary in the sense that we only use basic concepts to formally prove convergence.\n    ",
        "submission_date": "2015-11-27T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08915",
        "title": "Column-Oriented Datalog Materialization for Large Knowledge Graphs (Extended Technical Report)",
        "authors": [
            "Jacopo Urbani",
            "Ceriel Jacobs",
            "Markus Kr\u00f6tzsch"
        ],
        "abstract": "The evaluation of Datalog rules over large Knowledge Graphs (KGs) is essential for many applications. In this paper, we present a new method of materializing Datalog inferences, which combines a column-based memory layout with novel optimization methods that avoid redundant inferences at runtime. The pro-active caching of certain subqueries further increases efficiency. Our empirical evaluation shows that this approach can often match or even surpass the performance of state-of-the-art systems, especially under restricted resources.\n    ",
        "submission_date": "2015-11-28T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08952",
        "title": "Bootstrapping Ternary Relation Extractors",
        "authors": [
            "Ndapandula Nakashole"
        ],
        "abstract": "Binary relation extraction methods have been widely studied in recent years. However, few methods have been developed for higher n-ary relation extraction. One limiting factor is the effort required to generate training data. For binary relations, one only has to provide a few dozen pairs of entities per relation, as training data. For ternary relations (n=3), each training instance is a triplet of entities, placing a greater cognitive load on people. For example, many people know that Google acquired Youtube but not the dollar amount or the date of the acquisition and many people know that Hillary Clinton is married to Bill Clinton by not the location or date of their wedding. This makes higher n-nary training data generation a time consuming exercise in searching the Web. We present a resource for training ternary relation extractors. This was generated using a minimally supervised yet effective approach. We present statistics on the size and the quality of the dataset.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2019-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09107",
        "title": "Machine Learning Sentiment Prediction based on Hybrid Document Representation",
        "authors": [
            "Panagiotis Stalidis",
            "Maria Giatsoglou",
            "Konstantinos Diamantaras",
            "George Sarigiannidis",
            "Konstantinos Ch. Chatzisavvas"
        ],
        "abstract": "Automated sentiment analysis and opinion mining is a complex process concerning the extraction of useful subjective information from text. The explosion of user generated content on the Web, especially the fact that millions of users, on a daily basis, express their opinions on products and services to blogs, wikis, social networks, message boards, etc., render the reliable, automated export of sentiments and opinions from unstructured text crucial for several commercial applications. In this paper, we present a novel hybrid vectorization approach for textual resources that combines a weighted variant of the popular Word2Vec representation (based on Term Frequency-Inverse Document Frequency) representation and with a Bag- of-Words representation and a vector of lexicon-based sentiment values. The proposed text representation approach is assessed through the application of several machine learning classification algorithms on a dataset that is used extensively in literature for sentiment detection. The classification accuracy derived through the proposed hybrid vectorization approach is higher than when its individual components are used for text represenation, and comparable with state-of-the-art sentiment detection methodologies.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09376",
        "title": "Modeling Dynamic Relationships Between Characters in Literary Novels",
        "authors": [
            "Snigdha Chaturvedi",
            "Shashank Srivastava",
            "Hal Daume III",
            "Chris Dyer"
        ],
        "abstract": "Studying characters plays a vital role in computationally representing and interpreting narratives. Unlike previous work, which has focused on inferring character roles, we focus on the problem of modeling their relationships. Rather than assuming a fixed relationship for a character pair, we hypothesize that relationships are dynamic and temporally evolve with the progress of the narrative, and formulate the problem of relationship modeling as a structured prediction problem. We propose a semi-supervised framework to learn relationship sequences from fully as well as partially labeled data. We present a Markovian model capable of accumulating historical beliefs about the relationship and status changes. We use a set of rich linguistic and semantically motivated features that incorporate world knowledge to investigate the textual content of narrative. We empirically demonstrate that such a framework outperforms competitive baselines.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00112",
        "title": "Inferring Interpersonal Relations in Narrative Summaries",
        "authors": [
            "Shashank Srivastava",
            "Snigdha Chaturvedi",
            "Tom Mitchell"
        ],
        "abstract": "Characterizing relationships between people is fundamental for the understanding of narratives. In this work, we address the problem of inferring the polarity of relationships between people in narrative summaries. We formulate the problem as a joint structured prediction for each narrative, and present a model that combines evidence from linguistic and semantic features, as well as features based on the structure of the social community in the text. We also provide a clustering-based approach that can exploit regularities in narrative types. e.g., learn an affinity for love-triangles in romantic stories. On a dataset of movie summaries from Wikipedia, our structured models provide more than a 30% error-reduction over a competitive baseline that considers pairs of characters in isolation.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00165",
        "title": "Learning Using 1-Local Membership Queries",
        "authors": [
            "Galit Bary"
        ],
        "abstract": "Classic machine learning algorithms learn from labelled examples. For example, to design a machine translation system, a typical training set will consist of English sentences and their translation. There is a stronger model, in which the algorithm can also query for labels of new examples it creates. E.g, in the translation task, the algorithm can create a new English sentence, and request its translation from the user during training. This combination of examples and queries has been widely studied. Yet, despite many theoretical results, query algorithms are almost never used. One of the main causes for this is a report (Baum and Lang, 1992) on very disappointing empirical performance of a query algorithm. These poor results were mainly attributed to the fact that the algorithm queried for labels of examples that are artificial, and impossible to interpret by humans.\n",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00177",
        "title": "LSTM Neural Reordering Feature for Statistical Machine Translation",
        "authors": [
            "Yiming Cui",
            "Shijin Wang",
            "Jianfeng Li"
        ],
        "abstract": "Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00306",
        "title": "A Hybrid Intelligent Model for Software Cost Estimation",
        "authors": [
            "Wei Lin Du",
            "Luiz Fernando Capretz",
            "Ali Bou Nassif",
            "Danny Ho"
        ],
        "abstract": "Accurate software development effort estimation is critical to the success of software projects. Although many techniques and algorithmic models have been developed and implemented by practitioners, accurate software development effort prediction is still a challenging endeavor in the field of software engineering, especially in handling uncertain and imprecise inputs and collinear characteristics. In this paper, a hybrid in-telligent model combining a neural network model integrated with fuzzy model (neuro-fuzzy model) has been used to improve the accuracy of estimating software cost. The performance of the proposed model is assessed by designing and conducting evaluation with published project and industrial data. Results have shown that the proposed model demonstrates the ability of improving the estimation accuracy by 18% based on the Mean Magnitude of Relative Error (MMRE) criterion.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00397",
        "title": "A New Approach for Scalable Analysis of Microbial Communities",
        "authors": [
            "Ehsaneddin Asgari",
            "Kiavash Garakani",
            "Mohammad R.K Mofrad"
        ],
        "abstract": "Microbial communities play important roles in the function and maintenance of various biosystems, ranging from human body to the environment. Current methods for analysis of microbial communities are typically based on taxonomic phylogenetic alignment using 16S rRNA metagenomic or Whole Genome Sequencing data. In typical characterizations of microbial communities, studies deal with billions of micobial sequences, aligning them to a phylogenetic tree. We introduce a new approach for the efficient analysis of microbial communities. Our new reference-free analysis tech- nique is based on n-gram sequence analysis of 16S rRNA data and reduces the processing data size dramatically (by 105 fold), without requiring taxonomic alignment. The proposed approach is applied to characterize phenotypic microbial community differ- ences in different settings. Specifically, we applied this approach in classification of microbial com- munities across different body sites, characterization of oral microbiomes associated with healthy and diseased individuals, and classification of microbial communities longitudinally during the develop- ment of infants. Different dimensionality reduction methods are introduced that offer a more scalable analysis framework, while minimizing the loss in classification accuracies. Among dimensionality re- duction techniques, we propose a continuous vector representation for microbial communities, which can widely be used for deep learning applications in microbial informatics.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00442",
        "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing",
        "authors": [
            "Ke Li",
            "Jitendra Malik"
        ],
        "abstract": "Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2017-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00570",
        "title": "Attribute2Image: Conditional Image Generation from Visual Attributes",
        "authors": [
            "Xinchen Yan",
            "Jimei Yang",
            "Kihyuk Sohn",
            "Honglak Lee"
        ],
        "abstract": "This paper investigates a novel problem of generating images from visual attributes. We model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder. We experiment with natural images of faces and birds and demonstrate that the proposed models are capable of generating realistic and diverse samples with disentangled latent representations. We use a general energy minimization algorithm for posterior inference of latent variables given novel images. Therefore, the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00743",
        "title": "Recognizing Semantic Features in Faces using Deep Learning",
        "authors": [
            "Amogh Gudi"
        ],
        "abstract": "The human face constantly conveys information, both consciously and subconsciously. However, as basic as it is for humans to visually interpret this information, it is quite a big challenge for machines. Conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics, but they suffer from lack of robustness and high computation time. This thesis aims to explore ways for machines to learn to interpret semantic information available in faces in an automated manner without requiring manual design of feature detectors, using the approach of Deep Learning. This thesis provides a study of the effects of various factors and hyper-parameters of deep neural networks in the process of determining an optimal network configuration for the task of semantic facial feature recognition. This thesis explores the effectiveness of the system to recognize the various semantic features (like emotions, age, gender, ethnicity etc.) present in faces. Furthermore, the relation between the effect of high-level concepts on low level features is explored through an analysis of the similarities in low-level descriptors of different semantic features. This thesis also demonstrates a novel idea of using a deep network to generate 3-D Active Appearance Models of faces from real-world 2-D images.\n",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00880",
        "title": "The GTR-model: a universal framework for quantum-like measurements",
        "authors": [
            "Diederik Aerts",
            "Massimiliano Sassoli de Bianchi"
        ],
        "abstract": "We present a very general geometrico-dynamical description of physical or more abstract entities, called the 'general tension-reduction' (GTR) model, where not only states, but also measurement-interactions can be represented, and the associated outcome probabilities calculated. Underlying the model is the hypothesis that indeterminism manifests as a consequence of unavoidable fluctuations in the experimental context, in accordance with the 'hidden-measurements interpretation' of quantum mechanics. When the structure of the state space is Hilbertian, and measurements are of the 'universal' kind, i.e., are the result of an average over all possible ways of selecting an outcome, the GTR-model provides the same predictions of the Born rule, and therefore provides a natural completed version of quantum mechanics. However, when the structure of the state space is non-Hilbertian and/or not all possible ways of selecting an outcome are available to be actualized, the predictions of the model generally differ from the quantum ones, especially when sequential measurements are considered. Some paradigmatic examples will be discussed, taken from physics and human cognition. Particular attention will be given to some known psychological effects, like question order effects and response replicability, which we show are able to generate non-Hilbertian statistics. We also suggest a realistic interpretation of the GTR-model, when applied to human cognition and decision, which we think could become the generally adopted interpretative framework in quantum cognition research.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01027",
        "title": "Discrete Equilibrium Sampling with Arbitrary Nonequilibrium Processes",
        "authors": [
            "Firas Hamze",
            "Evgeny Andryash"
        ],
        "abstract": "We present a novel framework for performing statistical sampling, expectation estimation, and partition function approximation using \\emph{arbitrary} heuristic stochastic processes defined over discrete state spaces. Using a highly parallel construction we call the \\emph{sequential constraining process}, we are able to simultaneously generate states with the heuristic process and accurately estimate their probabilities, even when they are far too small to be realistically inferred by direct counting. After showing that both theoretically correct importance sampling and Markov chain Monte Carlo are possible using the sequential constraining process, we integrate it into a methodology called \\emph{state space sampling}, extending the ideas of state space search from computer science to the sampling context. The methodology comprises a dynamic data structure that constructs a robust Bayesian model of the statistics generated by the heuristic process subject to an accuracy constraint, the posterior Kullback-Leibler divergence. Sampling from the dynamic structure will generally yield partial states, which are completed by recursively calling the heuristic to refine the structure and resuming the sampling. Our experiments on various Ising models suggest that state space sampling enables heuristic state generation with accurate probability estimates, demonstrated by illustrating the convergence of a simulated annealing process to the Boltzmann distribution with increasing run length. Consequently, heretofore unprecedented direct importance sampling using the \\emph{final} (marginal) distribution of a generic stochastic process is allowed, potentially augmenting the range of algorithms at the Monte Carlo practitioner's disposal.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01041",
        "title": "Querying with \u0141ukasiewicz logic",
        "authors": [
            "Stefano Aguzzoli",
            "Pietro Codara",
            "Tommaso Flaminio",
            "Brunella Gerla",
            "Diego Valota"
        ],
        "abstract": "In this paper we present, by way of case studies, a proof of concept, based on a prototype working on a automotive data set, aimed at showing the potential usefulness of using formulas of \u0141ukasiewicz propositional logic to query databases in a fuzzy way. Our approach distinguishes itself for its stress on the purely linguistic, contraposed with numeric, formulations of queries. Our queries are expressed in the pure language of logic, and when we use (integer) numbers, these stand for shortenings of formulas on the syntactic level, and serve as linguistic hedges on the semantic one. Our case-study queries aim first at showing that each numeric-threshold fuzzy query is simulated by a \u0141ukasiewicz formula. Then they focus on the expressing power of \u0141ukasiewicz logic which easily allows for updating queries by clauses and for modifying them through a potentially infinite variety of linguistic hedges implemented with a uniform syntactic mechanism. Finally we shall hint how, already at propositional level, \u0141ukasiewicz natural semantics enjoys a degree of reflection, allowing to write syntactically simple queries that semantically work as meta-queries weighing the contribution of simpler ones.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01110",
        "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization",
        "authors": [
            "Yang Song",
            "Jun Zhu"
        ],
        "abstract": "Bayesian matrix completion has been studied based on a low-rank matrix factorization formulation with promising results. However, little work has been done on Bayesian matrix completion based on the more direct spectral regularization formulation. We fill this gap by presenting a novel Bayesian matrix completion method based on spectral regularization. In order to circumvent the difficulties of dealing with the orthonormality constraints of singular vectors, we derive a new equivalent form with relaxed constraints, which then leads us to design an adaptive version of spectral regularization feasible for Bayesian inference. Our Bayesian method requires no parameter tuning and can infer the number of latent factors automatically. Experiments on synthetic and real datasets demonstrate encouraging results on rank recovery and collaborative filtering, with notably good results for very sparse matrices.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01173",
        "title": "Building Memory with Concept Learning Capabilities from Large-scale Knowledge Base",
        "authors": [
            "Jiaxin Shi",
            "Jun Zhu"
        ],
        "abstract": "We present a new perspective on neural knowledge base (KB) embeddings, from which we build a framework that can model symbolic knowledge in the KB together with its learning process. We show that this framework well regularizes previous neural KB embedding model for superior performance in reasoning tasks, while having the capabilities of dealing with unseen entities, that is, to learn their embeddings from natural language descriptions, which is very like human's behavior of learning semantic concepts.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01249",
        "title": "Quantifying knowledge with a new calculus for belief functions - a generalization of probability theory",
        "authors": [
            "Timber Kerkvliet",
            "Ronald Meester"
        ],
        "abstract": "We first show that there are practical situations in for instance forensic and gambling settings, in which applying classical probability theory, that is, based on the axioms of Kolmogorov, is problematic. We then introduce and discuss Shafer belief functions. Technically, Shafer belief functions generalize probability distributions. Philosophically, they pertain to individual or shared knowledge of facts, rather than to facts themselves, and therefore can be interpreted as generalizing epistemic probability, that is, probability theory interpreted epistemologically. Belief functions are more flexible and better suited to deal with certain types of uncertainty than classical probability distributions. We develop a new calculus for belief functions which does not use the much criticized Dempster's rule of combination, by generalizing the classical notions of conditioning and independence in a natural and uncontroversial way. Using this calculus, we explain our rejection of Dempster's rule in detail. We apply the new theory to a number of examples, including a gambling example and an example in a forensic setting. We prove a law of large numbers for belief functions and offer a betting interpretation similar to the Dutch Book Theorem for probability distributions.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01250",
        "title": "Assessing forensic evidence by computing belief functions",
        "authors": [
            "Timber Kerkvliet",
            "Ronald Meester"
        ],
        "abstract": "We first discuss certain problems with the classical probabilistic approach for assessing forensic evidence, in particular its inability to distinguish between lack of belief and disbelief, and its inability to model complete ignorance within a given population. We then discuss Shafer belief functions, a generalization of probability distributions, which can deal with both these objections. We use a calculus of belief functions which does not use the much criticized Dempster rule of combination, but only the very natural Dempster-Shafer conditioning. We then apply this calculus to some classical forensic problems like the various island problems and the problem of parental identification. If we impose no prior knowledge apart from assuming that the culprit or parent belongs to a given population (something which is possible in our setting), then our answers differ from the classical ones when uniform or other priors are imposed. We can actually retrieve the classical answers by imposing the relevant priors, so our setup can and should be interpreted as a generalization of the classical methodology, allowing more flexibility. We show how our calculus can be used to develop an analogue of Bayes' rule, with belief functions instead of classical probabilities. We also discuss consequences of our theory for legal practice.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01325",
        "title": "Toward a Taxonomy and Computational Models of Abnormalities in Images",
        "authors": [
            "Babak Saleh",
            "Ahmed Elgammal",
            "Jacob Feldman",
            "Ali Farhadi"
        ],
        "abstract": "The human visual system can spot an abnormal image, and reason about what makes it strange. This task has not received enough attention in computer vision. In this paper we study various types of atypicalities in images in a more comprehensive way than has been done before. We propose a new dataset of abnormal images showing a wide range of atypicalities. We design human subject experiments to discover a coarse taxonomy of the reasons for abnormality. Our experiments reveal three major categories of abnormality: object-centric, scene-centric, and contextual. Based on this taxonomy, we propose a comprehensive computational model that can predict all different types of abnormality in images and outperform prior arts in abnormality recognition.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01537",
        "title": "Reuse of Neural Modules for General Video Game Playing",
        "authors": [
            "Alexander Braylan",
            "Mark Hollenbeck",
            "Elliot Meyerson",
            "Risto Miikkulainen"
        ],
        "abstract": "A general approach to knowledge transfer is introduced in which an agent controlled by a neural network adapts how it reuses existing networks as it learns in a new domain. Networks trained for a new domain can improve their performance by routing activation selectively through previously learned neural structure, regardless of how or for what it was learned. A neuroevolution implementation of this approach is presented with application to high-dimensional sequential decision-making domains. This approach is more general than previous approaches to neural transfer for reinforcement learning. It is domain-agnostic and requires no prior assumptions about the nature of task relatedness or mappings. The method is analyzed in a stochastic version of the Arcade Learning Environment, demonstrating that it improves performance in some of the more complex Atari 2600 games, and that the success of transfer can be predicted based on a high-level characterization of game dynamics.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01587",
        "title": "Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text",
        "authors": [
            "Sahil Garg",
            "Aram Galstyan",
            "Ulf Hermjakob",
            "Daniel Marcu"
        ],
        "abstract": "We advance the state of the art in biomolecular interaction extraction with three contributions: (i) We show that deep, Abstract Meaning Representations (AMR) significantly improve the accuracy of a biomolecular interaction extraction system when compared to a baseline that relies solely on surface- and syntax-based features; (ii) In contrast with previous approaches that infer relations on a sentence-by-sentence basis, we expand our framework to enable consistent predictions over sets of sentences (documents); (iii) We further modify and expand a graph kernel learning framework to enable concurrent exploitation of automatically induced AMR (semantic) and dependency structure (syntactic) representations. Our experiments show that our approach yields interaction extraction systems that are more robust in environments where there is a significant mismatch between training and test conditions.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01715",
        "title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding",
        "authors": [
            "Hang Qi",
            "Tianfu Wu",
            "Mun-Wai Lee",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form \"true/false\" (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or non-polar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines. We also provide a baseline implementation and result analyses.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01752",
        "title": "Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation",
        "authors": [
            "Sujith Ravi",
            "Qiming Diao"
        ],
        "abstract": "Traditional graph-based semi-supervised learning (SSL) approaches, even though widely applied, are not suited for massive data and large label scenarios since they scale linearly with the number of edges $|E|$ and distinct labels $m$. To deal with the large label size problem, recent works propose sketch-based methods to approximate the distribution on labels per node thereby achieving a space reduction from $O(m)$ to $O(\\log m)$, under certain conditions. In this paper, we present a novel streaming graph-based SSL approximation that captures the sparsity of the label distribution and ensures the algorithm propagates labels accurately, and further reduces the space complexity per node to $O(1)$. We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. We also study different graph construction mechanisms for natural language applications and propose a robust graph augmentation strategy trained using state-of-the-art unsupervised deep learning architectures that yields further significant quality gains.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01764",
        "title": "Fast Algorithms for Game-Theoretic Centrality Measures",
        "authors": [
            "Piotr Lech Szczepa\u0144ski"
        ],
        "abstract": "In this dissertation, we analyze the computational properties of game-theoretic centrality measures. The key idea behind game-theoretic approach to network analysis is to treat nodes as players in a cooperative game, where the value of each coalition of nodes is determined by certain graph properties. Next, the centrality of any individual node is determined by a chosen game-theoretic solution concept (notably, the Shapley value) in the same way as the payoff of a player in a cooperative game. On one hand, the advantage of game-theoretic centrality measures is that nodes are ranked not only according to their individual roles but also according to how they contribute to the role played by all possible subsets of nodes. On the other hand, the disadvantage is that the game-theoretic solution concepts are typically computationally challenging. The main contribution of this dissertation is that we show that a wide variety of game-theoretic solution concepts on networks can be computed in polynomial time. Our focus is on centralities based on the Shapley value and its various extensions, such as the Semivalues and Coalitional Semivalues. Furthermore, we prove #P-hardness of computing the Shapley value in connectivity games and propose an algorithm to compute it. Finally, we analyse computational properties of generalized version of cooperative games in which order of player matters. We propose a new representation for such games, called generalized marginal contribution networks, that allows for polynomial computation in the size of the representation of two dedicated extensions of the Shapley value to this class of games.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2015-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01872",
        "title": "Driverseat: Crowdstrapping Learning Tasks for Autonomous Driving",
        "authors": [
            "Pranav Rajpurkar",
            "Toki Migimatsu",
            "Jeff Kiske",
            "Royce Cheng-Yue",
            "Sameep Tandon",
            "Tao Wang",
            "Andrew Ng"
        ],
        "abstract": "While emerging deep-learning systems have outclassed knowledge-based approaches in many tasks, their application to detection tasks for autonomous technologies remains an open field for scientific exploration. Broadly, there are two major developmental bottlenecks: the unavailability of comprehensively labeled datasets and of expressive evaluation strategies. Approaches for labeling datasets have relied on intensive hand-engineering, and strategies for evaluating learning systems have been unable to identify failure-case scenarios. Human intelligence offers an untapped approach for breaking through these bottlenecks. This paper introduces Driverseat, a technology for embedding crowds around learning systems for autonomous driving. Driverseat utilizes crowd contributions for (a) collecting complex 3D labels and (b) tagging diverse scenarios for ready evaluation of learning systems. We demonstrate how Driverseat can crowdstrap a convolutional neural network on the lane-detection task. More generally, crowdstrapping introduces a valuable paradigm for any technology that can benefit from leveraging the powerful combination of human and computer intelligence.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01926",
        "title": "Thinking Required",
        "authors": [
            "Kamil Rocki"
        ],
        "abstract": "There exists a theory of a single general-purpose learning algorithm which could explain the principles its operation. It assumes the initial rough architecture, a small library of simple innate circuits which are prewired at birth. and proposes that all significant mental algorithms are learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from architectural and functional perspectives.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02011",
        "title": "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies",
        "authors": [
            "Vincent Fran\u00e7ois-Lavet",
            "Raphael Fonteneau",
            "Damien Ernst"
        ],
        "abstract": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03012",
        "title": "ShapeNet: An Information-Rich 3D Model Repository",
        "authors": [
            "Angel X. Chang",
            "Thomas Funkhouser",
            "Leonidas Guibas",
            "Pat Hanrahan",
            "Qixing Huang",
            "Zimo Li",
            "Silvio Savarese",
            "Manolis Savva",
            "Shuran Song",
            "Hao Su",
            "Jianxiong Xiao",
            "Li Yi",
            "Fisher Yu"
        ],
        "abstract": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03375",
        "title": "Convolutional Monte Carlo Rollouts in Go",
        "authors": [
            "Peter H. Jin",
            "Kurt Keutzer"
        ],
        "abstract": "In this work, we present a MCTS-based Go-playing program which uses convolutional networks in all parts. Our method performs MCTS in batches, explores the Monte Carlo search tree using Thompson sampling and a convolutional network, and evaluates convnet-based rollouts on the GPU. We achieve strong win rates against open source Go programs and attain competitive results against state of the art convolutional net-based Go-playing programs.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03899",
        "title": "Query Answering over Contextualized RDF/OWL Knowledge with Forall-Existential Bridge Rules: Decidable Finite Extension Classes (Post Print)",
        "authors": [
            "Mathew Joseph",
            "Gabriel Kuper",
            "Till Mossakowski",
            "Luciano Serafini"
        ],
        "abstract": "The proliferation of contextualized knowledge in the Semantic Web (SW) has led to the popularity of knowledge formats such as \\emph{quads} in the SW community. A quad is an extension of an RDF triple with contextual information of the triple. In this paper, we study the problem of query answering over quads augmented with forall-existential bridge rules that enable interoperability of reasoning between triples in various contexts. We call a set of quads together with such expressive bridge rules, a quad-system. Query answering over quad-systems is undecidable, in general. We derive decidable classes of quad-systems, for which query answering can be done using forward chaining. Sound, complete and terminating procedures, which are adaptations of the well known chase algorithm, are provided for these classes for deciding query entailment. Safe, msafe, and csafe class of quad-systems restrict the structure of blank nodes generated during the chase computation process to be directed acyclic graphs (DAGs) of bounded depth. RR and restricted RR classes do not allow the generation of blank nodes during the chase computation process. Both data and combined complexity of query entailment has been established for the classes derived. We further show that quad-systems are equivalent to forall-existential rules whose predicates are restricted to ternary arity, modulo polynomial time translations. We subsequently show that the technique of safety, strictly subsumes in expressivity, some of the well known and expressive techniques, such as joint acyclicity and model faithful acyclicity, used for decidability guarantees in the realm of forall-existential rules.\n    ",
        "submission_date": "2015-12-12T00:00:00",
        "last_modified_date": "2015-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04021",
        "title": "The Rationale behind the Concept of Goal",
        "authors": [
            "Guido Governatori",
            "Francesco Olivieri",
            "Simone Scannapieco",
            "Antonino Rotolo",
            "Matteo Cristani"
        ],
        "abstract": "The paper proposes a fresh look at the concept of goal and advances that motivational attitudes like desire, goal and intention are just facets of the broader notion of (acceptable) outcome. We propose to encode the preferences of an agent as sequences of \"alternative acceptable outcomes\". We then study how the agent's beliefs and norms can be used to filter the mental attitudes out of the sequences of alternative acceptable outcomes. Finally, we formalise such intuitions in a novel Modal Defeasible Logic and we prove that the resulting formalisation is computationally feasible.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04114",
        "title": "Building and Measuring Privacy-Preserving Predictive Blacklists",
        "authors": [
            "Luca Melis",
            "Apostolos Pyrgelis",
            "Emiliano De Cristofaro"
        ],
        "abstract": "(Withdrawn) Collaborative security initiatives are increasingly often advocated to improve timeliness and effectiveness of threat mitigation. Among these, collaborative predictive blacklisting (CPB) aims to forecast attack sources based on alerts contributed by multiple organizations that might be targeted in similar ways. Alas, CPB proposals thus far have only focused on improving hit counts, but overlooked the impact of collaboration on false positives and false negatives. Moreover, sharing threat intelligence often prompts important privacy, confidentiality, and liability issues. In this paper, we first provide a comprehensive measurement analysis of two state-of-the-art CPB systems: one that uses a trusted central party to collect alerts [Soldo et al., Infocom'10] and a peer-to-peer one relying on controlled data sharing [Freudiger et al., DIMVA'15], studying the impact of collaboration on both correct and incorrect predictions. Then, we present a novel privacy-friendly approach that significantly improves over previous work, achieving a better balance of true and false positive rates, while minimizing information disclosure. Finally, we present an extension that allows our system to scale to very large numbers of organizations.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2018-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04134",
        "title": "Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect",
        "authors": [
            "Qifei Wang",
            "Gregorij Kurillo",
            "Ferda Ofli",
            "Ruzena Bajcsy"
        ],
        "abstract": "Microsoft Kinect camera and its skeletal tracking capabilities have been embraced by many researchers and commercial developers in various applications of real-time human movement analysis. In this paper, we evaluate the accuracy of the human kinematic motion data in the first and second generation of the Kinect system, and compare the results with an optical motion capture system. We collected motion data in 12 exercises for 10 different subjects and from three different viewpoints. We report on the accuracy of the joint localization and bone length estimation of Kinect skeletons in comparison to the motion capture. We also analyze the distribution of the joint localization offsets by fitting a mixture of Gaussian and uniform distribution models to determine the outliers in the Kinect motion data. Our analysis shows that overall Kinect 2 has more robust and more accurate tracking of human pose as compared to Kinect 1.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04295",
        "title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator",
        "authors": [
            "Lukas Cavigelli",
            "Luca Benini"
        ],
        "abstract": "An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04419",
        "title": "Sentence Entailment in Compositional Distributional Semantics",
        "authors": [
            "Esma Balkir",
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "Distributional semantic models provide vector representations for words by gathering co-occurrence frequencies from corpora of text. Compositional distributional models extend these from words to phrases and sentences. In categorical compositional distributional semantics, phrase and sentence representations are functions of their grammatical structure and representations of the words therein. In this setting, grammatical structures are formalised by morphisms of a compact closed category and meanings of words are formalised by objects of the same category. These can be instantiated in the form of vectors or density matrices. This paper concerns the applications of this model to phrase and sentence level entailment. We argue that entropy-based distances of vectors and density matrices provide a good candidate to measure word-level entailment, show the advantage of density matrices over vectors for word level entailments, and prove that these distances extend compositionally from words to phrases and sentences. We exemplify our theoretical constructions on real data and a toy entailment dataset and provide preliminary experimental evidence.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2018-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05245",
        "title": "Symphony from Synapses: Neocortex as a Universal Dynamical Systems Modeller using Hierarchical Temporal Memory",
        "authors": [
            "Fergal Byrne"
        ],
        "abstract": "Reverse engineering the brain is proving difficult, perhaps impossible. While many believe that this is just a matter of time and effort, a different approach might help. Here, we describe a very simple idea which explains the power of the brain as well as its structure, exploiting complex dynamics rather than abstracting it away. Just as a Turing Machine is a Universal Digital Computer operating in a world of symbols, we propose that the brain is a Universal Dynamical Systems Modeller, evolved bottom-up (itself using nested networks of interconnected, self-organised dynamical systems) to prosper in a world of dynamical systems.\n",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05504",
        "title": "Blind, Greedy, and Random: Ordinal Approximation Algorithms for Matching and Clustering",
        "authors": [
            "Elliot Anshelevich",
            "Shreyas Sekar"
        ],
        "abstract": "We study Matching and other related problems in a partial information setting where the agents' utilities for being matched to other agents are hidden and the mechanism only has access to ordinal preference information. Our model is motivated by the fact that in many settings, agents cannot express the numerical values of their utility for different outcomes, but are still able to rank the outcomes in their order of preference. Specifically, we study problems where the ground truth exists in the form of a weighted graph, and look to design algorithms that approximate the true optimum matching using only the preference orderings for each agent (induced by the hidden weights) as input. If no restrictions are placed on the weights, then one cannot hope to do better than the simple greedy algorithm, which yields a half optimal matching. Perhaps surprisingly, we show that by imposing a little structure on the weights, we can improve upon the trivial algorithm significantly: we design a 1.6-approximation algorithm for instances where the hidden weights obey the metric inequality. Using our algorithms for matching as a black-box, we also design new approximation algorithms for other closely related problems: these include a a 3.2-approximation for the problem of clustering agents into equal sized partitions, a 4-approximation algorithm for Densest k-subgraph, and a 2.14-approximation algorithm for Max TSP. These results are the first non-trivial ordinal approximation algorithms for such problems, and indicate that we can design robust algorithms even when we are agnostic to the precise agent utilities.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05509",
        "title": "An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments",
        "authors": [
            "Denis Steckelmacher",
            "Peter Vrancx"
        ],
        "abstract": "This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long Short-Term Memory, Gated Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures. A variant of fitted Q iteration, based on Advantage values instead of Q values, is also explored. The results show that GRU performs significantly better than LSTM and MUT1 for most of the problems considered, requiring less training episodes and less CPU time before learning a very good policy. Advantage learning also tends to produce better results.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05665",
        "title": "Probabilistic Programming with Gaussian Process Memoization",
        "authors": [
            "Ulrich Schaechtle",
            "Ben Zinberg",
            "Alexey Radul",
            "Kostas Stathis",
            "Vikash K. Mansinghka"
        ],
        "abstract": "Gaussian Processes (GPs) are widely used tools in statistics, machine learning, robotics, computer vision, and scientific computation. However, despite their popularity, they can be difficult to apply; all but the simplest classification or regression applications require specification and inference over complex covariance functions that do not admit simple analytical posteriors. This paper shows how to embed Gaussian processes in any higher-order probabilistic programming language, using an idiom based on memoization, and demonstrates its utility by implementing and extending classic and state-of-the-art GP applications. The interface to Gaussian processes, called gpmem, takes an arbitrary real-valued computational process as input and returns a statistical emulator that automatically improve as the original process is invoked and its input-output behavior is recorded. The flexibility of gpmem is illustrated via three applications: (i) robust GP regression with hierarchical hyper-parameter learning, (ii) discovering symbolic expressions from time-series data by fully Bayesian structure learning over kernels generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian optimization with automatic inference and action selection. All applications share a single 50-line Python library and require fewer than 20 lines of probabilistic code each.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05742",
        "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems",
        "authors": [
            "Iulian Vlad Serban",
            "Ryan Lowe",
            "Peter Henderson",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "abstract": "During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research in this area, we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems. We discuss important characteristics of these datasets, how they can be used to learn diverse dialogue strategies, and their other potential uses. We also examine methods for transfer learning between datasets and the use of external knowledge. Finally, we discuss appropriate choice of evaluation metrics for the learning objective.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05868",
        "title": "On Voting and Facility Location",
        "authors": [
            "Michal Feldman",
            "Amos Fiat",
            "Iddan Golomb"
        ],
        "abstract": "We study mechanisms for candidate selection that seek to minimize the social cost, where voters and candidates are associated with points in some underlying metric space. The social cost of a candidate is the sum of its distances to each voter. Some of our work assumes that these points can be modeled on a real line, but other results of ours are more general.\n",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05875",
        "title": "Quadripolar Relational Model: a framework for the description of borderline and narcissistic personality disorders",
        "authors": [
            "Alessandro Fontana"
        ],
        "abstract": "Borderline personality disorder and narcissistic personality disorder are important nosographic entities and have been subject of intensive investigations. The currently prevailing psychodynamic theory for mental disorders is based on the repertoire of defense mechanisms employed. Another line of research is concerned with the study of psychological traumas and dissociation as a defensive response. Both theories can be used to shed light on some aspects of pathological mental functioning, and have many points of contact. This work merges these two psychological theories, and builds a model of mental function in a relational context called Quadripolar Relational Model. The model, which is enriched with ideas borrowed from the field of computer science, leads to a new therapeutic proposal for psychological traumas and personality disorders.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05986",
        "title": "Can Pretrained Neural Networks Detect Anatomy?",
        "authors": [
            "Vlado Menkovski",
            "Zharko Aleksovski",
            "Axel Saalbach",
            "Hannes Nickisch"
        ],
        "abstract": "Convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. In medical imaging, there is a huge variety of possible imaging modalities and contrasts, where annotated data is usually very scarce. We present two approaches to deal with this challenge. A network pretrained in a different domain with abundant data is used as a feature extractor, while a subsequent classifier is trained on a small target dataset; and a deep architecture trained with heavy augmentation and equipped with sophisticated regularization methods. We test the approaches on a corpus of X-ray images to design an anatomy detection system.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06293",
        "title": "A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction",
        "authors": [
            "Thomas Wiatowski",
            "Helmut B\u00f6lcskei"
        ],
        "abstract": "Deep convolutional neural networks have led to breakthrough results in numerous practical machine learning tasks such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a trainable classifier. The mathematical analysis of deep convolutional neural networks for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory that encompasses general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned filters), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating, e.g., sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor we prove a translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth, and we establish deformation sensitivity bounds that apply to signal classes such as, e.g., band-limited functions, cartoon functions, and Lipschitz functions.\n    ",
        "submission_date": "2015-12-19T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06492",
        "title": "Remote Health Coaching System and Human Motion Data Analysis for Physical Therapy with Microsoft Kinect",
        "authors": [
            "Qifei Wang",
            "Gregorij Kurillo",
            "Ferda Ofli",
            "Ruzena Bajcsy"
        ],
        "abstract": "This paper summarizes the recent progress we have made for the computer vision technologies in physical therapy with the accessible and affordable devices. We first introduce the remote health coaching system we build with Microsoft Kinect. Since the motion data captured by Kinect is noisy, we investigate the data accuracy of Kinect with respect to the high accuracy motion capture system. We also propose an outlier data removal algorithm based on the data distribution. In order to generate the kinematic parameter from the noisy data captured by Kinect, we propose a kinematic filtering algorithm based on Unscented Kalman Filter and the kinematic model of human skeleton. The proposed algorithm can obtain smooth kinematic parameter with reduced noise compared to the kinematic parameter generated from the raw motion data from Kinect.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06789",
        "title": "Information-Theoretic Bounded Rationality",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun",
            "Justin Dyer",
            "Kee-Eung Kim",
            "Naftali Tishby"
        ],
        "abstract": "Bounded rationality, that is, decision-making and planning under resource limitations, is widely regarded as an important open problem in artificial intelligence, reinforcement learning, computational neuroscience and economics. This paper offers a consolidated presentation of a theory of bounded rationality based on information-theoretic ideas. We provide a conceptual justification for using the free energy functional as the objective function for characterizing bounded-rational decisions. This functional possesses three crucial properties: it controls the size of the solution space; it has Monte Carlo planners that are exact, yet bypass the need for exhaustive search; and it captures model uncertainty arising from lack of evidence or from interacting with other agents having unknown intentions. We discuss the single-step decision-making case, and show how to extend it to sequential decisions using equivalence transformations. This extension yields a very general class of decision problems that encompass classical decision rules (e.g. EXPECTIMAX and MINIMAX) as limit cases, as well as trust- and risk-sensitive planning.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06863",
        "title": "Addressing Complex and Subjective Product-Related Queries with Customer Reviews",
        "authors": [
            "Julian McAuley",
            "Alex Yang"
        ],
        "abstract": "Online reviews are often our first port of call when considering products and purchases online. When evaluating a potential purchase, we may have a specific query in mind, e.g. `will this baby seat fit in the overhead compartment of a 747?' or `will I like this album if I liked Taylor Swift's 1989?'. To answer such questions we must either wade through huge volumes of consumer reviews hoping to find one that is relevant, or otherwise pose our question directly to the community via a Q/A system.\n",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06945",
        "title": "Restricted Predicates for Hypothetical Datalog",
        "authors": [
            "Fernando S\u00e1enz-P\u00e9rez"
        ],
        "abstract": "Hypothetical Datalog is based on an intuitionistic semantics rather than on a classical logic semantics, and embedded implications are allowed in rule bodies. While the usual implication (i.e., the neck of a Horn clause) stands for inferring facts, an embedded implication plays the role of assuming its premise for deriving its consequence. A former work introduced both a formal framework and a goal-oriented tabled implementation, allowing negation in rule bodies. While in that work positive assumptions for both facts and rules can occur in the premise, negative assumptions are not allowed. In this work, we cover this subject by introducing a new concept: a restricted predicate, which allows negative assumptions by pruning the usual semantics of a predicate. This new setting has been implemented in the deductive system DES.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07430",
        "title": "The ERA of FOLE: Foundation",
        "authors": [
            "Robert E. Kent"
        ],
        "abstract": "This paper discusses the representation of ontologies in the first-order logical environment {\\ttfamily FOLE}. An ontology defines the primitives with which to model the knowledge resources for a community of discourse. These primitives consist of classes, relationships and properties. An ontology uses formal axioms to constrain the interpretation of these primitives. In short, an ontology specifies a logical theory. This paper continues the discussion of the representation and interpretation of ontologies in the first-order logical environment {\\ttfamily FOLE}. The formalism and semantics of (many-sorted) first-order logic can be developed in both a \\emph{classification form} and an \\emph{interpretation form}. Two papers, the current paper, defining the concept of a structure, and ``The {\\ttfamily ERA} of {\\ttfamily FOLE}: Superstructure'', defining the concept of a sound logic, represent the \\emph{classification form}, corresponding to ideas discussed in the ``Information Flow Framework''. Two papers, ``The {\\ttfamily FOLE} Table'', defining the concept of a relational table, and ``The {\\ttfamily FOLE} Database'', defining the concept of a relational database, represent the \\emph{interpretation form}, expanding on material found in the paper ``Database Semantics''. Although the classification form follows the entity-relationship-attribute data model of Chen, the interpretation form incorporates the relational data model of Codd. A fifth paper ``{\\ttfamily FOLE} Equivalence'' proves that the classification form is equivalent to the interpretation form. In general, the {\\ttfamily FOLE} representation uses a conceptual structures approach, that is completely compatible with the theory of institutions, formal concept analysis and information flow.\n    ",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2023-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07487",
        "title": "Selecting the top-quality item through crowd scoring",
        "authors": [
            "Alessandro Nordio",
            "Alberto Tarable",
            "Emilio Leonardi",
            "Marco Ajmone Marsan"
        ],
        "abstract": "We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, for example in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy and biased evaluation. All received evaluations are then combined, to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then, we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. In the simulations we show that some of the algorithms achieve near optimal performance for a suitable setting of the system parameters.\n    ",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2017-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07636",
        "title": "Representation and Coding of Signal Geometry",
        "authors": [
            "Petros T Boufounos",
            "Shantanu Rane",
            "Hassan Mansour"
        ],
        "abstract": "Approaches to signal representation and coding theory have traditionally focused on how to best represent signals using parsimonious representations that incur the lowest possible distortion. Classical examples include linear and non-linear approximations, sparse representations, and rate-distortion theory. Very often, however, the goal of processing is to extract specific information from the signal, and the distortion should be measured on the extracted information. The corresponding representation should, therefore, represent that information as parsimoniously as possible, without necessarily accurately representing the signal itself.\n",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2015-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07650",
        "title": "The Max $K$-Armed Bandit: PAC Lower Bounds and Efficient Algorithms",
        "authors": [
            "Yahel David",
            "Nahum Shimkin"
        ],
        "abstract": "We consider the Max $K$-Armed Bandit problem, where a learning agent is faced with several stochastic arms, each a source of i.i.d. rewards of unknown distribution. At each time step the agent chooses an arm, and observes the reward of the obtained sample. Each sample is considered here as a separate item with the reward designating its value, and the goal is to find an item with the highest possible value. Our basic assumption is a known lower bound on the {\\em tail function} of the reward distributions. Under the PAC framework, we provide a lower bound on the sample complexity of any $(\\epsilon,\\delta)$-correct algorithm, and propose an algorithm that attains this bound up to logarithmic factors. We analyze the robustness of the proposed algorithm and in addition, we compare the performance of this algorithm to the variant in which the arms are not distinguishable by the agent and are chosen randomly at each stage. Interestingly, when the maximal rewards of the arms happen to be similar, the latter approach may provide better performance.\n    ",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2015-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07942",
        "title": "Multi-Level Cause-Effect Systems",
        "authors": [
            "Krzysztof Chalupka",
            "Pietro Perona",
            "Frederick Eberhardt"
        ],
        "abstract": "We present a domain-general account of causation that applies to settings in which macro-level causal relations between two systems are of interest, but the relevant causal features are poorly understood and have to be aggregated from vast arrays of micro-measurements. Our approach generalizes that of Chalupka et al. (2015) to the setting in which the macro-level effect is not specified. We formalize the connection between micro- and macro-variables in such situations and provide a coherent framework describing causal relations at multiple levels of analysis. We present an algorithm that discovers macro-variable causes and effects from micro-level measurements obtained from an experiment. We further show how to design experiments to discover macro-variables from observational micro-variable data. Finally, we show that under specific conditions, one can identify multiple levels of causal structure. Throughout the article, we use a simulated neuroscience multi-unit recording experiment to illustrate the ideas and the algorithms.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08030",
        "title": "Device and System Level Design Considerations for Analog-Non-Volatile-Memory Based Neuromorphic Architectures",
        "authors": [
            "Sukru Burc Eryilmaz",
            "Duygu Kuzum",
            "Shimeng Yu",
            "H.-S. Philip Wong"
        ],
        "abstract": "This paper gives an overview of recent progress in the brain inspired computing field with a focus on implementation using emerging memories as electronic synapses. Design considerations and challenges such as requirements and design targets on multilevel states, device variability, programming energy, array-level connectivity, fan-in/fanout, wire energy, and IR drop are presented. Wires are increasingly important in design decisions, especially for large systems, and cycle-to-cycle variations have large impact on learning performance.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08120",
        "title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning",
        "authors": [
            "Fanhua Shang",
            "James Cheng",
            "Hong Cheng"
        ],
        "abstract": "Multi-relational learning has received lots of attention from researchers in various research communities. Most existing methods either suffer from superlinear per-iteration cost, or are sensitive to the given ranks. To address both issues, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics, which can be generalized as a graph Laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration (SHOOI) version. We first induce the equivalence relation of the Schatten p-norm (0<p<\\infty) of a low multi-linear rank tensor and its core tensor. Then we achieve a much smaller matrix trace norm minimization problem. Finally, we develop two efficient augmented Lagrange multiplier algorithms to solve our problems with convergence guarantees. Extensive experiments using both real and synthetic datasets, even though with only a few observations, verified both the efficiency and effectiveness of our methods.\n    ",
        "submission_date": "2015-12-26T00:00:00",
        "last_modified_date": "2016-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08849",
        "title": "Learning Natural Language Inference with LSTM",
        "authors": [
            "Shuohang Wang",
            "Jing Jiang"
        ],
        "abstract": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI). In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a match-LSTM to perform word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. On the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the state of the art.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08949",
        "title": "Simple, Robust and Optimal Ranking from Pairwise Comparisons",
        "authors": [
            "Nihar B. Shah",
            "Martin J. Wainwright"
        ],
        "abstract": "We consider data in the form of pairwise comparisons of n items, with the goal of precisely identifying the top k items for some value of k < n, or alternatively, recovering a ranking of all the items. We analyze the Copeland counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) its computational efficiency leads to speed-ups of several orders of magnitude in computation time as compared to prior work; (b) it is robust in that theoretical guarantees impose no conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) it is an optimal method up to constant factors, meaning that it achieves the information-theoretic limits for recovering the top k-subset. We extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09204",
        "title": "Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index Policies",
        "authors": [
            "Weici Hu",
            "Peter I. Frazier"
        ],
        "abstract": "We consider effort allocation in crowdsourcing, where we wish to assign labeling tasks to imperfect homogeneous crowd workers to maximize overall accuracy in a continuous-time Bayesian setting, subject to budget and time constraints. The Bayes-optimal policy for this problem is the solution to a partially observable Markov decision process, but the curse of dimensionality renders the computation infeasible. Based on the Lagrangian Relaxation technique in Adelman & Mersereau (2008), we provide a computationally tractable instance-specific upper bound on the value of this Bayes-optimal policy, which can in turn be used to bound the optimality gap of any other sub-optimal policy. In an approach similar in spirit to the Whittle index for restless multiarmed bandits, we provide an index policy for effort allocation in crowdsourcing and demonstrate numerically that it outperforms other stateof- arts and performs close to optimal solution.\n    ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2015-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09354",
        "title": "An (MI)LP-based Primal Heuristic for 3-Architecture Connected Facility Location in Urban Access Network Design",
        "authors": [
            "Fabio D'Andreagiovanni",
            "Fabian Mett",
            "Jonad Pulaj"
        ],
        "abstract": "We investigate the 3-architecture Connected Facility Location Problem arising in the design of urban telecommunication access networks. We propose an original optimization model for the problem that includes additional variables and constraints to take into account wireless signal coverage. Since the problem can prove challenging even for modern state-of-the art optimization solvers, we propose to solve it by an original primal heuristic which combines a probabilistic fixing procedure, guided by peculiar Linear Programming relaxations, with an exact MIP heuristic, based on a very large neighborhood search. Computational experiments on a set of realistic instances show that our heuristic can find solutions associated with much lower optimality gaps than a state-of-the-art solver.\n    ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    }
]