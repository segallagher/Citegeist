[
    {
        "url": "https://arxiv.org/abs/1401.0104",
        "title": "PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction",
        "authors": [
            "Yukun Bao",
            "Tao Xiong",
            "Zhongyi Hu"
        ],
        "abstract": "Multi-step-ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction, and is continually under research. Recently, the multiple-input several multiple-outputs (MISMO) modeling strategy has been proposed as a promising alternative for multi-step-ahead time series prediction, exhibiting advantages compared with the two currently dominating strategies, the iterated and the direct strategies. Built on the established MISMO strategy, this study proposes a particle swarm optimization (PSO)-based MISMO modeling strategy, which is capable of determining the number of sub-models in a self-adaptive mode, with varying prediction horizons. Rather than deriving crisp divides with equal-size s prediction horizons from the established MISMO, the proposed PSO-MISMO strategy, implemented with neural networks, employs a heuristic to create flexible divides with varying sizes of prediction horizons and to generate corresponding sub-models, providing considerable flexibility in model construction, which has been validated with simulated and real datasets.\n    ",
        "submission_date": "2013-12-31T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0180",
        "title": "Decision Making under Uncertainty: A Quasimetric Approach",
        "authors": [
            "Steve N'Guyen",
            "Cl\u00e9ment Moulin-Frier",
            "Jacques Droulez"
        ],
        "abstract": "We propose a new approach for solving a class of discrete decision making problems under uncertainty with positive cost. This issue concerns multiple and diverse fields such as engineering, economics, artificial intelligence, cognitive science and many others. Basically, an agent has to choose a single or series of actions from a set of options, without knowing for sure their consequences. Schematically, two main approaches have been followed: either the agent learns which option is the correct one to choose in a given situation by trial and error, or the agent already has some knowledge on the possible consequences of his decisions; this knowledge being generally expressed as a conditional probability distribution. In the latter case, several optimal or suboptimal methods have been proposed to exploit this uncertain knowledge in various contexts. In this work, we propose following a different approach, based on the geometric intuition of distance. More precisely, we define a goal independent quasimetric structure on the state space, taking into account both cost function and transition probability. We then compare precision and computation time with classical approaches.\n    ",
        "submission_date": "2013-12-31T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0245",
        "title": "A Review: Expert System for Diagnosis of Myocardial Infarction",
        "authors": [
            "S.J Gath",
            "R.V Kulkarni"
        ],
        "abstract": "A computer Program Capable of performing at a human-expert level in a narrow problem domain area is called an expert system. Management of uncertainty is an intrinsically important issue in the design of expert systems because much of the information in the knowledge base of a typical expert system is imprecise, incomplete or not totally reliable. In this paper, the author present s the review of past work that has been carried out by various researchers based on development of expert systems for the diagnosis of cardiac disease\n    ",
        "submission_date": "2014-01-01T00:00:00",
        "last_modified_date": "2014-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0802",
        "title": "A stochastic model for Case-Based Reasoning",
        "authors": [
            "Michael Gr. Voskoglou"
        ],
        "abstract": "Case-Bsed Reasoning (CBR) is a recent theory for problem-solving and learning in computers and ",
        "submission_date": "2014-01-04T00:00:00",
        "last_modified_date": "2014-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1024",
        "title": "Solver Scheduling via Answer Set Programming",
        "authors": [
            "Holger Hoos",
            "Roland Kaminski",
            "Marius Lindauer",
            "Torsten Schaub"
        ],
        "abstract": "Although Boolean Constraint Technology has made tremendous progress over the last decade, the efficacy of state-of-the-art solvers is known to vary considerably across different types of problem instances and is known to depend strongly on algorithm parameters. This problem was addressed by means of a simple, yet effective approach using handmade, uniform and unordered schedules of multiple solvers in ppfolio, which showed very impressive performance in the 2011 SAT Competition. Inspired by this, we take advantage of the modeling and solving capacities of Answer Set Programming (ASP) to automatically determine more refined, that is, non-uniform and ordered solver schedules from existing benchmarking data. We begin by formulating the determination of such schedules as multi-criteria optimization problems and provide corresponding ASP encodings. The resulting encodings are easily customizable for different settings and the computation of optimum schedules can mostly be done in the blink of an eye, even when dealing with large runtime data sets stemming from many solvers on hundreds to thousands of instances. Also, the fact that our approach can be customized easily enabled us to swiftly adapt it to generate parallel schedules for multi-processor machines.\n    ",
        "submission_date": "2014-01-06T00:00:00",
        "last_modified_date": "2014-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1061",
        "title": "Learning optimization models in the presence of unknown relations",
        "authors": [
            "Sicco Verwer",
            "Yingqian Zhang",
            "Qing Chuan Ye"
        ],
        "abstract": "In a sequential auction with multiple bidding agents, it is highly challenging to determine the ordering of the items to sell in order to maximize the revenue due to the fact that the autonomy and private information of the agents heavily influence the outcome of the auction.\n",
        "submission_date": "2014-01-06T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1247",
        "title": "Tractability through Exchangeability: A New Perspective on Efficient Probabilistic Inference",
        "authors": [
            "Mathias Niepert",
            "Guy Van den Broeck"
        ],
        "abstract": "Exchangeability is a central notion in statistics and probability theory. The assumption that an infinite sequence of data points is exchangeable is at the core of Bayesian statistics. However, finite exchangeability as a statistical property that renders probabilistic inference tractable is less well-understood. We develop a theory of finite exchangeability and its relation to tractable probabilistic inference. The theory is complementary to that of independence and conditional independence. We show that tractable inference in probabilistic models with high treewidth and millions of variables can be understood using the notion of finite (partial) exchangeability. We also show that existing lifted inference algorithms implicitly utilize a combination of conditional independence and partial exchangeability.\n    ",
        "submission_date": "2014-01-07T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1465",
        "title": "Cortical prediction markets",
        "authors": [
            "David Balduzzi"
        ],
        "abstract": "We investigate cortical learning from the perspective of mechanism design. First, we show that discretizing standard models of neurons and synaptic plasticity leads to rational agents maximizing simple scoring rules. Second, our main result is that the scoring rules are proper, implying that neurons faithfully encode expected utilities in their synaptic weights and encode high-scoring outcomes in their spikes. Third, with this foundation in hand, we propose a biologically plausible mechanism whereby neurons backpropagate incentives which allows them to optimize their usefulness to the rest of cortex. Finally, experiments show that networks that backpropagate incentives can learn simple tasks.\n    ",
        "submission_date": "2014-01-07T00:00:00",
        "last_modified_date": "2014-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1533",
        "title": "Proposta di nuovi strumenti per comprendere come funziona la cognizione (Novel tools to understand how cognition works)",
        "authors": [
            "Devis Pantano"
        ],
        "abstract": "I think that the main reason why we do not understand the general principles of how knowledge works (and probably also the reason why we have not yet designed and built efficient machines capable of artificial intelligence), is not the excessive complexity of cognitive phenomena, but the lack of the conceptual and methodological tools to properly address the problem. It is like trying to build up Physics without the concept of number, or to understand the origin of species without including the mechanism of natural selection. In this paper I propose some new conceptual and methodological tools, which seem to offer a real opportunity to understand the logic of cognitive processes. I propose a new method to properly treat the concepts of structure and schema, and to perform on them operations of structural analysis. These operations allow to move straightforwardly from concrete to more abstract representations. With these tools I will suggest a definition for the concept of rule, of regularity and of emergent phenomena. From the analysis of some important aspects of the rules, I suggest to distinguish them in operational and associative rules. I propose that associative rules assume a dominant role in cognition. I also propose a definition for the concept of problem. At the end I will briefly illustrate a possible general model for cognitive systems.\n    ",
        "submission_date": "2014-01-07T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1669",
        "title": "Smart machines and the SP theory of intelligence",
        "authors": [
            "J. Gerard Wolff"
        ],
        "abstract": "These notes describe how the \"SP theory of intelligence\", and its embodiment in the \"SP machine\", may help to realise cognitive computing, as described in the book \"Smart Machines\". In the SP system, information compression and a concept of \"multiple alignment\" are centre stage. The system is designed to integrate such things as unsupervised learning, pattern recognition, probabilistic reasoning, and more. It may help to overcome the problem of variety in big data, it may serve in pattern recognition and in the unsupervised learning of structure in data, and it may facilitate the management and transmission of big data. There is potential, via information compression, for substantial gains in computational efficiency, especially in the use of energy. The SP system may help to realise data-centric computing, perhaps via a development of Hebb's concept of a \"cell assembly\", or via the use of light or DNA for the processing of information. It has potential in the management of errors and uncertainty in data, in medical diagnosis, in processing streams of data, and in promoting adaptability in robots.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2011",
        "title": "A logic for reasoning about ambiguity",
        "authors": [
            "Joseph Y. Halpern",
            "Willemien Kets"
        ],
        "abstract": "Standard models of multi-agent modal logic do not capture the fact that information is often \\emph{ambiguous}, and may be interpreted in different ways by different agents. We propose a framework that can model this, and consider different semantics that capture different assumptions about the agents' beliefs regarding whether or not there is ambiguity. We examine the expressive power of logics of ambiguity compared to logics that cannot model ambiguity, with respect to the different semantics that we propose.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2121",
        "title": "Emotional Responses in Artificial Agent-Based Systems: Reflexivity and Adaptation in Artificial Life",
        "authors": [
            "Carlos Pedro Gon\u00e7alves"
        ],
        "abstract": "The current work addresses a virtual environment with self-replicating agents whose decisions are based on a form of \"somatic computation\" (soma - body) in which basic emotional responses, taken in parallelism to actual living organisms, are introduced as a way to provide the agents with greater reflexive abilities. The work provides a contribution to the field of Artificial Intelligence (AI) and Artificial Life (ALife) in connection to a neurobiology-based cognitive framework for artificial systems and virtual environments' simulations. The performance of the agents capable of emotional responses is compared with that of self-replicating automata, and the implications of research on emotions and AI, in connection to both virtual agents as well as robots, is addressed regarding possible future directions and applications.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2153",
        "title": "Ontology - Based Dynamic Business Process Customization",
        "authors": [
            "V.Karthikeyan",
            "V.J.Vijayalakshmi",
            "P.Jeyakumar"
        ],
        "abstract": "The interaction between business models is used in consumer centric manner instead of using a producer centric approach for customizing the business process in cloud environment. The knowledge based human semantic web is used for customizing the business process It introduces the Human Semantic Web as a conceptual interface, providing human-understandable semantics on top of the ordinary Semantic Web, which provides machine-readable semantics based on RDF in this mismatching is a major problem. To overcome this following technique automatic customization detection is an automated process of detecting possible elements or variables of a business process that needto be especially treated in order to suit the requirement of the other process. To the business processto be customized as the primary business process and those that it collaborates with as secondary business process or SBP Automatic customization enactment is an automated process of taking actions to perform the customization on the PBP according to the detected customization spots and the automatic reasoning on the customization conceptualization knowledge framework. The process of customizing businessprocesses by composite the web pages by using web service.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2184",
        "title": "Variations on Memetic Algorithms for Graph Coloring Problems",
        "authors": [
            "Laurent Moalic",
            "Alexandre Gondran"
        ],
        "abstract": "Graph vertex coloring with a given number of colors is a well-known and much-studied NP-complete ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2474",
        "title": "Transformation-based Feature Computation for Algorithm Portfolios",
        "authors": [
            "Barry Hurley",
            "Serdar Kadioglu",
            "Yuri Malitsky",
            "Barry O'Sullivan"
        ],
        "abstract": "Instance-specific algorithm configuration and algorithm portfolios have been shown to offer significant improvements over single algorithm approaches in a variety of application domains. In the SAT and CSP domains algorithm portfolios have consistently dominated the main competitions in these fields for the past five years. For a portfolio approach to be effective there are two crucial conditions that must be met. First, there needs to be a collection of complementary solvers with which to make a portfolio. Second, there must be a collection of problem features that can accurately identify structural differences between instances. This paper focuses on the latter issue: feature representation, because, unlike SAT, not every problem has well-studied features. We employ the well-known SATzilla feature set, but compute alternative sets on different SAT encodings of CSPs. We show that regardless of what encoding is used to convert the instances, adequate structural information is maintained to differentiate between problem instances, and that this can be exploited to make an effective portfolio-based CSP solver.\n    ",
        "submission_date": "2014-01-10T00:00:00",
        "last_modified_date": "2014-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2483",
        "title": "Dempster-Shafer Theory for Move Prediction in Start Kicking of The Bicycle Kick of Sepak Takraw Game",
        "authors": [
            "Andino Maseleno",
            "Md. Mahmud Hasan"
        ],
        "abstract": "This paper presents Dempster-Shafer theory for move prediction in start kicking of the bicycle kick of sepak takraw game. Sepak takraw is a highly complex net-barrier kicking sport that involves dazzling displays of quick reflexes, acrobatic twists, turns and swerves of the agile human body movement. A Bicycle kick or Scissor kick is a physical move made by throwing the body up into the air, making a shearing movement with the legs to get one leg in front of the other without holding on to the ground. Specifically, this paper considers bicycle kick of sepak takraw game in start kicking of the ball with uncertainty where player has different awareness regarding the contingencies. We have chosen Dempster-Shafer theory because the advantages of the Dempster-Shafer theory which include the ability to model information in a flexible way without requiring a probability to be assigned to each element in a set, providing a convenient and simple mechanism for combining two or more pieces of evidence under certain conditions, it can model ignorance explicitly, rejection of the law of additivity for belief in disjoint propositions.\n    ",
        "submission_date": "2014-01-10T00:00:00",
        "last_modified_date": "2014-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2503",
        "title": "Does Restraining End Effect Matter in EMD-Based Modeling Framework for Time Series Prediction? Some Experimental Evidences",
        "authors": [
            "Tao Xiong",
            "Yukun Bao",
            "Zhongyi Hu"
        ],
        "abstract": "Following the \"decomposition-and-ensemble\" principle, the empirical mode decomposition (EMD)-based modeling framework has been widely used as a promising alternative for nonlinear and nonstationary time series modeling and prediction. The end effect, which occurs during the sifting process of EMD and is apt to distort the decomposed sub-series and hurt the modeling process followed, however, has been ignored in previous studies. Addressing the end effect issue, this study proposes to incorporate end condition methods into EMD-based decomposition and ensemble modeling framework for one- and multi-step ahead time series prediction. Four well-established end condition methods, Mirror method, Coughlin's method, Slope-based method, and Rato's method, are selected, and support vector regression (SVR) is employed as the modeling technique. For the purpose of justification and comparison, well-known NN3 competition data sets are used and four well-established prediction models are selected as benchmarks. The experimental results demonstrated that significant improvement can be achieved by the proposed EMD-based SVR models with end condition methods. The EMD-SBM-SVR model and EMD-Rato-SVR model, in particular, achieved the best prediction performances in terms of goodness of forecast measures and equality of accuracy of competing forecasts test.\n    ",
        "submission_date": "2014-01-11T00:00:00",
        "last_modified_date": "2014-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3428",
        "title": "A Heuristic Search Approach to Planning with Continuous Resources in Stochastic Domains",
        "authors": [
            "Nicolas Meuleau",
            "Emmanuel Benazera",
            "Ronen I. Brafman",
            "Eric A. Hansen",
            "Mausam"
        ],
        "abstract": "We consider the problem of optimal planning in stochastic domains with resource constraints, where the resources are continuous and the choice of action at each step depends on resource availability. We introduce the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables, where the continuous variables represent monotonic resources. Like other heuristic search algorithms, HAO* leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy. We show that this approach is especially effective when resource constraints limit how much of the state space is reachable. Experimental results demonstrate its effectiveness in the domain that motivates our research: automated planning for planetary exploration rovers.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3430",
        "title": "A Unifying Framework for Structural Properties of CSPs: Definitions, Complexity, Tractability",
        "authors": [
            "Lucas Bordeaux",
            "Marco Cadoli",
            "Toni Mancini"
        ],
        "abstract": "Literature on Constraint Satisfaction exhibits the definition of several structural properties that can be possessed by CSPs, like (in)consistency, substitutability or interchangeability. Current tools for constraint solving typically detect such properties efficiently by means of incomplete yet effective algorithms, and use them to reduce the search space and boost search.\n",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3431",
        "title": "Compositional Belief Update",
        "authors": [
            "James Delgrande",
            "Yi Jin",
            "Francis Jeffry Pelletier"
        ],
        "abstract": "In this paper we explore a class of belief update operators, in which the definition of the operator is compositional with respect to the sentence to be added. The goal is to provide an update operator that is intuitive, in that its definition is based on a recursive decomposition of the update sentences structure, and that may be reasonably implemented.  In addressing update, we first provide a definition phrased in terms of the models of a knowledge base.  While this operator satisfies a core group of the benchmark Katsuno-Mendelzon update postulates, not all of the postulates are satisfied.  Other Katsuno-Mendelzon postulates can be obtained by suitably restricting the syntactic form of the sentence for update, as we show.  In restricting the syntactic form of the sentence for update, we also obtain a hierarchy of update operators with Winsletts standard semantics as the most basic interesting approach captured.  We subsequently give an algorithm which captures this approach; in the general case the algorithm is exponential, but with some not-unreasonable assumptions we obtain an algorithm that is linear in the size of the knowledge base.  Hence the resulting approach has much better complexity characteristics than other operators in some situations.  We also explore other compositional belief change operators: erasure is developed as a dual operator to update; we show that a forget operator is definable in terms of update; and we give a definition of the compositional revision operator.  We obtain that compositional revision, under the most natural definition, yields the Satoh revision operator.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3432",
        "title": "A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments",
        "authors": [
            "Tinne De Laet",
            "Joris De Schutter",
            "Herman Bruyninckx"
        ],
        "abstract": "This paper proposes and experimentally validates a Bayesian network model of a range finder adapted to dynamic environments. All modeling assumptions are rigorously explained, and all model parameters have a physical interpretation. This approach results in a transparent and intuitive model. With respect to the state of the art beam model this paper: (i) proposes a different functional form for the probability of range measurements caused by unmodeled objects, (ii) intuitively explains the discontinuity encountered in te state of the art beam model, and (iii) reduces the number of model parameters, while maintaining the same representational power for experimental data. The proposed beam model is called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood and a variational Bayesian estimator (both based on expectation-maximization) are proposed to learn the model parameters.\n",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3436",
        "title": "Online Planning Algorithms for POMDPs",
        "authors": [
            "St\u00e9phane Ross",
            "Joelle Pineau",
            "S\u00e9bastien Paquet",
            "Brahim Chaib-draa"
        ],
        "abstract": "Partially Observable Markov Decision Processes (POMDPs) provide a rich framework for sequential decision-making under uncertainty in stochastic domains. However, solving a POMDP is often intractable except for small problems due to their complexity. Here, we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution. Online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment. Our objectives here are to survey the various existing online POMDP methods, analyze their properties and discuss their advantages and disadvantages; and to thoroughly evaluate these online approaches in different environments under various metrics (return, error bound reduction, lower bound improvement). Our experimental results indicate that state-of-the-art online heuristic search methods can handle large POMDP domains efficiently.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3437",
        "title": "Learning Partially Observable Deterministic Action Models",
        "authors": [
            "Eyal Amir",
            "Allen Chang"
        ],
        "abstract": "We present exact algorithms for identifying deterministic-actions effects and preconditions in dynamic partially observable domains.  They apply when one does not know the action model(the way actions affect the world) of a domain and must learn it from partial observations over time. Such scenarios are common in real world applications. They are challenging for AI tasks because traditional domain structures that underly tractability (e.g., conditional independence) fail there (e.g., world features become correlated). Our work departs from traditional assumptions about partial observations and action models. In particular, it focuses on problems in which actions are deterministic of simple logical structure and observation models have all features observed with some frequency. We yield tractable algorithms for the modified problem for such domains. \n",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3438",
        "title": "The Ultrametric Constraint and its Application to Phylogenetics",
        "authors": [
            "Neil C.A. Moore",
            "Patrick Prosser"
        ],
        "abstract": "A phylogenetic tree shows the evolutionary relationships among species. Internal nodes of the tree represent speciation events and leaf nodes correspond to species. A goal of phylogenetics is to combine such trees into larger trees, called supertrees, whilst respecting the relationships in the original trees.  A rooted tree exhibits an ultrametric property; that is, for any three leaves of the tree it must be that one pair has a deeper most recent common ancestor than the other pairs, or that all three have the same most recent common ancestor. This inspires a constraint programming encoding for rooted trees. We present an efficient constraint that enforces the ultrametric property over a symmetric array of constrained integer variables, with the inevitable property that the lower bounds of any three variables are mutually supportive. We show that this allows an efficient constraint-based solution to the supertree construction problem. We demonstrate that the versatility of constraint programming can be exploited to allow solutions to variants of the supertree construction problem.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3439",
        "title": "Interactive Policy Learning through Confidence-Based Autonomy",
        "authors": [
            "Sonia Chernova",
            "Manuela Veloso"
        ],
        "abstract": "We present Confidence-Based Autonomy (CBA), an interactive algorithm for policy learning from demonstration.  The CBA algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents.  The first component, Confident Execution, enables the agent to identify states in which demonstration is required, to request a demonstration from the human teacher and to learn a policy based on the acquired data.  The algorithm selects demonstrations based on a measure of action selection confidence, and our results show that using Confident Execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher.  The second algorithmic component, Corrective Demonstration, enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance.  CBA and its individual components are compared and evaluated in a complex simulated driving domain.  The complete CBA algorithm results in the best overall learning performance, successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3442",
        "title": "Asynchronous Forward Bounding for Distributed COPs",
        "authors": [
            "Amir Gershman",
            "Amnon Meisels",
            "Roie Zivan"
        ],
        "abstract": "A new search algorithm for solving distributed constraint optimization problems (DisCOPs) is presented. Agents assign variables sequentially and compute bounds on partial assignments asynchronously. The asynchronous bounds computation is based on the propagation of partial assignments. The asynchronous forward-bounding algorithm (AFB) is a distributed optimization search algorithm that keeps one consistent partial assignment at all times. The algorithm is described in detail and its correctness proven. Experimental evaluation shows that AFB outperforms synchronous branch and bound by many orders of magnitude, and produces a phase transition as the tightness of the problem increases. This is an analogous effect to the phase transition that has been observed when local consistency maintenance is applied to MaxCSPs. The AFB algorithm is further enhanced by the addition of a backjumping mechanism, resulting in the AFB-BJ algorithm.  Distributed backjumping is based on accumulated information on bounds of all values and on processing concurrently a queue of candidate goals for the next move back. The AFB-BJ algorithm is compared experimentally to other DisCOP algorithms (ADOPT, DPOP, OptAPO) and is shown to be a very efficient algorithm for DisCOPs.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3443",
        "title": "Computational Logic Foundations of KGP Agents",
        "authors": [
            "Antonis Kakas",
            "Paolo Mancarella",
            "Fariba Sadri",
            "Kostas Stathis",
            "Francesca Toni"
        ],
        "abstract": "This paper presents the computational logic foundations of a model of agency called the KGP (Knowledge, Goals and Plan model. This model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments.  KGP provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agents state in response to reasoning, sensing and acting. Transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences, as well as selection operators for providing inputs to transitions.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3444",
        "title": "On the Qualitative Comparison of Decisions Having Positive and Negative Features",
        "authors": [
            "Didier Dubois",
            "H\u00e9l\u00e8ne Fargier",
            "Jean-Fran\u00e7ois Bonnefon"
        ],
        "abstract": "Making a decision is often a matter of listing and comparing positive and negative arguments. In such cases, the evaluation scale for decisions should be considered bipolar, that is, negative and positive values should be explicitly distinguished. That is what is done, for example, in Cumulative Prospect Theory. However, contraryto the latter framework that presupposes genuine numerical assessments, human agents often decide on the basis of an ordinal ranking of the pros and the cons, and by focusing on the most salient arguments. In other terms, the decision process is qualitative as well as bipolar. In this article, based on a bipolar extension of possibility theory, we define and axiomatically characterize several decision rules tailored for the joint handling of positive and negative arguments in an ordinal setting. The simplest rules can be viewed as extensions of the maximin and maximax criteria to the bipolar case, and consequently suffer from poor decisive power. More decisive rules that refine the former are also proposed. These refinements agree both with principles of efficiency and with the spirit of order-of-magnitude reasoning, that prevails in qualitative decision theory. The most refined decision rule uses leximin rankings of the pros and the cons, and the ideas of counting arguments of equal strength and cancelling pros by cons. It is shown to come down to a special case of Cumulative Prospect Theory, and to subsume the Take the Best heuristic studied by cognitive psychologists.\n\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3448",
        "title": "AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Graphical Models",
        "authors": [
            "Robert Mateescu",
            "Rina Dechter",
            "Radu Marinescu"
        ],
        "abstract": "Inspired by the recently introduced framework of AND/OR search spaces for graphical models, we propose to augment Multi-Valued Decision Diagrams (MDD) with AND nodes, in order to capture function decomposition structure and to extend these compiled data structures to general weighted graphical models (e.g., probabilistic models). We present the AND/OR Multi-Valued Decision Diagram (AOMDD) which compiles a graphical model into a canonical form that supports polynomial (e.g., solution counting, belief updating) or constant time (e.g. equivalence of graphical models) queries. We provide two algorithms for compiling the AOMDD of a graphical model. The first is search-based, and works by applying reduction rules to the trace of the memory intensive AND/OR search algorithm. The second is inference-based and uses a Bucket Elimination schedule to combine the AOMDDs of the input functions via the the APPLY operator. For both algorithms, the compilation time and the size of the AOMDD are, in the worst case, exponential in the treewidth of the graphical model, rather than pathwidth as is known for ordered binary decision diagrams (OBDDs). We introduce the concept of semantic treewidth, which helps explain why the size of a decision diagram is often much smaller than the worst case bound. We provide an experimental evaluation that demonstrates the potential of AOMDDs.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3450",
        "title": "Completeness and Performance Of The APO Algorithm",
        "authors": [
            "Tal Grinshpoun",
            "Amnon Meisels"
        ],
        "abstract": "Asynchronous Partial Overlay (APO) is a search algorithm that uses cooperative mediation to solve Distributed Constraint Satisfaction Problems (DisCSPs). The algorithm partitions the search into different subproblems of the DisCSP. The original proof of completeness of the APO algorithm is based on the growth of the size of the subproblems. The present paper demonstrates that this expected growth of subproblems does not occur in some situations, leading to a termination problem of the algorithm. The problematic parts in the APO algorithm that interfere with its completeness are identified and necessary modifications to the algorithm that fix these problematic parts are given. The resulting version of the algorithm, Complete Asynchronous Partial Overlay (CompAPO), ensures its completeness. Formal proofs for the soundness and completeness of CompAPO are given. A detailed performance evaluation of CompAPO comparing it to other DisCSP algorithms is presented, along with an extensive experimental evaluation of the algorithm's unique behavior. Additionally, an optimization version of the algorithm, CompOptAPO, is presented, discussed, and evaluated.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3453",
        "title": "The Computational Complexity of Dominance and Consistency in CP-Nets",
        "authors": [
            "Judy Goldsmith",
            "Jerome Lang",
            "Miroslaw Truszczyski",
            "Nic Wilson"
        ],
        "abstract": "We investigate the computational complexity of testing dominance and consistency in CP-nets. Previously, the complexity of dominance has been determined for restricted classes in which the dependency graph of the CP-net is acyclic. However, there are preferences of interest that define cyclic dependency graphs; these are modeled with general CP-nets. In our main results, we show here that both dominance and consistency for general CP-nets are PSPACE-complete. We then consider the concept of strong dominance, dominance equivalence and dominance incomparability, and several notions of optimality, and identify the complexity of the corresponding decision problems. The reductions used in the proofs are from STRIPS planning, and thus reinforce the earlier established connections between both areas.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3455",
        "title": "Monte Carlo Sampling Methods for Approximating Interactive POMDPs",
        "authors": [
            "Prashant Doshi",
            "Piotr J. Gmytrasiewicz"
        ],
        "abstract": "Partially observable Markov decision processes (POMDPs) provide a principled framework for sequential planning in uncertain single agent settings. An extension of POMDPs to multiagent settings, called interactive POMDPs (I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief systems which represent an agent's belief about the physical world, about beliefs of other agents, and about their beliefs about others' beliefs. This modification makes the difficulties of obtaining solutions due to complexity of the belief and policy spaces even more acute. We describe a general method for obtaining approximate solutions of I-POMDPs based on particle filtering (PF). We introduce the interactive PF, which descends the levels of the interactive belief hierarchies and samples and propagates beliefs at each level. The interactive PF is able to mitigate the belief space complexity, but it does not address the policy space complexity. To mitigate the policy space complexity -- sometimes also called the curse of history -- we utilize a complementary method based on sampling likely observations while building the look ahead reachability tree. While this approach does not completely address the curse of history, it beats back the curse's impact substantially. We provide experimental results and chart future work.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3458",
        "title": "Solving #SAT and Bayesian Inference with Backtracking Search",
        "authors": [
            "Fahiem Bacchus",
            "Shannon Dalmao",
            "Toniann Pitassi"
        ],
        "abstract": "Inference in Bayes Nets (BAYES) is an important problem with numerous applications in probabilistic reasoning. Counting the number of satisfying assignments of a propositional formula (#SAT) is a closely related problem of fundamental theoretical importance. Both these problems, and others, are members of the class of sum-of-products (SUMPROD) problems. In this paper we show that standard backtracking search when augmented with a simple memoization scheme (caching) can solve any sum-of-products problem with time complexity that is at least as good any other state-of-the-art exact algorithm, and that it can also achieve the best known time-space tradeoff. Furthermore, backtracking's ability to utilize more flexible variable orderings allows us to prove that it can achieve an exponential speedup over other standard algorithms for SUMPROD on some instances.\n",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3459",
        "title": "Generic Preferences over Subsets of Structured Objects",
        "authors": [
            "Maxim Binshtok",
            "Ronen I. Brafman",
            "Carmel Domshlak",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Various tasks in decision making and decision support systems require selecting a preferred subset of a given set of items. Here we focus on problems where the individual items are described using a set of characterizing attributes, and a generic preference specification is required, that is, a specification that can work with an arbitrary set of items. For example, preferences over the content of an online newspaper should have this form: At each viewing, the newspaper contains a subset of the set of articles currently available. Our preference specification over this subset should be provided offline, but we should be able to use it to select a subset of any currently available set of articles, e.g., based on their tags. We present a general approach for lifting formalisms for specifying preferences over objects with multiple attributes into ones that specify preferences over subsets of such objects. We also show how we can compute an optimal subset given such a specification in a relatively efficient manner. We provide an empirical evaluation of the approach as well as some worst-case complexity results.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3460",
        "title": "Policy Iteration for Decentralized Control of Markov Decision Processes",
        "authors": [
            "Daniel S. Bernstein",
            "Christopher Amato",
            "Eric A. Hansen",
            "Shlomo Zilberstein"
        ],
        "abstract": "Coordination of distributed agents is required for problems arising in many areas, including multi-robot systems, networking and e-commerce.  As a formal framework for such problems, we use the decentralized partially observable Markov decision process (DEC-POMDP).  Though much work has been done on optimal dynamic programming algorithms for the single-agent version of the problem, optimal algorithms for the multiagent case have been elusive.  The main contribution of this paper is an optimal policy iteration algorithm for solving DEC-POMDPs.  The algorithm uses stochastic finite-state controllers to represent policies.  The solution can include a correlation device, which allows agents to correlate their actions without communicating.  This approach alternates between expanding the controller and performing value-preserving transformations, which modify the controller without sacrificing value.  We present two efficient value-preserving transformations: one can reduce the size of the controller and the other can improve its value while keeping the size fixed.  Empirical results demonstrate the usefulness of value-preserving transformations in increasing value while keeping controller size to a minimum. To broaden the applicability of the approach, we also present a heuristic version of the policy iteration algorithm, which sacrifices convergence to optimality.  This algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents actions are known. While this assumption may not hold in general, it helps produce higher quality solutions in our test problems.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3461",
        "title": "A Bilinear Programming Approach for Multiagent Planning",
        "authors": [
            "Marek Petrik",
            "Shlomo Zilberstein"
        ],
        "abstract": "Multiagent planning and coordination problems are common and known to be computationally hard.  We show that a wide range of two-agent problems can be formulated as bilinear programs.  We present a successive approximation algorithm that significantly outperforms the coverage set algorithm, which is the state-of-the-art method for this class of multiagent problems. Because the algorithm is formulated for bilinear programs, it is more general and simpler to implement. The new algorithm can be terminated at any time and-unlike the coverage set algorithm-it facilitates the derivation of a useful online performance bound. It is also much more efficient, on average reducing the computation time of the optimal solution by about four orders of magnitude.  Finally, we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm, extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3467",
        "title": "Planning over Chain Causal Graphs for Variables with Domains of Size 5 Is NP-Hard",
        "authors": [
            "Omer Gim\u00e9nez",
            "Anders Jonsson"
        ],
        "abstract": "Recently, considerable focus has been given to the problem of determining the boundary between tractable and intractable planning problems. In this paper, we study the complexity of planning in the class C_n of planning problems, characterized by unary operators and directed path causal graphs. Although this is one of the simplest forms of causal graphs a planning problem can have, we show that planning is intractable for C_n (unless P = NP), even if the domains of state variables have bounded size. In particular, we show that plan existence for C_n^k is NP-hard for k>=5 by reduction from CNFSAT. Here, k denotes the upper bound on the size of the state variable domains. Our result reduces the complexity gap for the class C_n^k to cases k=3 and k=4 only, since C_n^2 is known to be tractable.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3468",
        "title": "Compiling Uncertainty Away in Conformant Planning Problems with Bounded Width",
        "authors": [
            "Hector Palacios",
            "Hector Geffner"
        ],
        "abstract": "Conformant planning is the problem of finding a sequence of actions for achieving a goal in the presence of uncertainty in the initial state or action effects.  The problem has been approached as a path-finding problem in belief space where good belief representations and heuristics are critical for scaling up.  In this work, a different formulation is introduced for conformant problems with deterministic actions where they are automatically converted into classical ones and solved by an off-the-shelf classical planner.  The translation maps literals L and sets of assumptions t about the initial situation, into new literals KL/t that represent that L must be true if t is initially true.  We lay out a general translation scheme that is sound and establish the conditions under which the translation is also complete.  We show that the complexity of the complete translation is exponential in a parameter of the problem called the conformant width, which for most benchmarks is bounded. The planner based on this translation exhibits good performance in comparison with existing planners, and is the basis for T0, the best performing planner in the Conformant Track of the 2006 International Planning Competition.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3469",
        "title": "Exploiting Single-Cycle Symmetries in Continuous Constraint Problems",
        "authors": [
            "Vicente Ruiz de Angulo",
            "Carme Torras"
        ],
        "abstract": "Symmetries in discrete constraint satisfaction problems have been explored and exploited in the last years, but symmetries in continuous constraint problems have not received the same attention. Here we focus on permutations of the variables consisting of one single cycle. We propose a procedure that takes advantage of these symmetries by interacting with a continuous constraint solver without interfering with it. A key concept in this procedure are the classes of symmetric boxes formed by bisecting a n-dimensional cube at the same point in all dimensions at the same time. We analyze these classes and quantify them as a function of the cube dimensionality. Moreover, we propose a simple algorithm to generate the representatives of all these classes for any number of variables at very high rates. A problem example from the chemical and#64257;eld and the cyclic n-roots problem are used to show the performance of the approach in practice.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3470",
        "title": "Message-Based Web Service Composition, Integrity Constraints, and Planning under Uncertainty: A New Connection",
        "authors": [
            "J\u00f6rg Hoffmann",
            "Piergiorgio Bertoli",
            "Malte Helmert",
            "Marco Pistore"
        ],
        "abstract": "Thanks to recent advances, AI Planning has become the underlying technique for several applications. Figuring prominently among these is automated Web Service Composition (WSC) at the \"capability\" level, where services are described in terms of preconditions and effects over ontological concepts. A key issue in addressing WSC as planning is that ontologies are not only formal vocabularies; they also axiomatize the possible relationships between concepts. Such axioms correspond to what has been termed \"integrity constraints\" in the actions and change literature, and applying a web service is essentially a belief update operation. The reasoning required for belief update is known to be harder than reasoning in the ontology itself. The support for belief update is severely limited in current planning tools.\n",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3471",
        "title": "Conservative Inference Rule for Uncertain Reasoning under Incompleteness",
        "authors": [
            "Marco Zaffalon",
            "Enrique Miranda"
        ],
        "abstract": "In this paper we formulate the problem of inference under incomplete information in very general terms. This includes modelling the process responsible for the incompleteness, which we call the incompleteness process. We allow the process behaviour to be partly unknown. Then we use Walleys theory of coherent lower previsions, a generalisation of the Bayesian theory to imprecision, to derive the rule to update beliefs under incompleteness that logically follows from our assumptions, and that we call conservative inference rule. This rule has some remarkable properties: it is an abstract rule to update beliefs that can be applied in any situation or domain; it gives us the opportunity to be neither too optimistic nor too pessimistic about the incompleteness process, which is a necessary condition to draw reliable while strong enough conclusions; and it is a coherent rule, in the sense that it cannot lead to inconsistencies. We give examples to show how the new rule can be applied in expert systems, in parametric statistical inference, and in pattern classification, and discuss more generally the view of incompleteness processes defended here as well as some of its consequences.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3474",
        "title": "Optimal Value of Information in Graphical Models",
        "authors": [
            "Andreas Krause",
            "Carlos Guestrin"
        ],
        "abstract": "Many real-world decision making tasks require us to choose among several expensive observations. In a sensor network, for example, it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty. In medical decision making tasks, one needs to select which tests to administer before deciding on the most effective treatment. It has been general practice to use heuristic-guided procedures for selecting observations. In this paper, we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models. For example, our algorithms allow to optimally label hidden variables in Hidden Markov Models (HMMs). We provide results for both selecting the optimal subset of observations, and for obtaining an optimal conditional observation plan.\n",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3477",
        "title": "Solving Weighted Constraint Satisfaction Problems with Memetic/Exact Hybrid Algorithms",
        "authors": [
            "Jos\u00e9 Enrique Gallardo",
            "Carlos Cotta",
            "Antonio Jos\u00e9 Fern\u00e1ndez"
        ],
        "abstract": "A weighted constraint satisfaction problem (WCSP) is a constraint satisfaction problem in which preferences among solutions can be expressed. Bucket elimination is a complete technique commonly used to solve this kind of constraint satisfaction problem. When the memory required to apply bucket elimination is too high, a heuristic method based on it (denominated mini-buckets) can be used to calculate bounds for the optimal solution. Nevertheless, the curse of dimensionality makes these techniques impractical on large scale problems. In response to this situation, we present a memetic algorithm for WCSPs in which bucket elimination is used as a mechanism for recombining solutions, providing the best possible child from the parental set. Subsequently, a multi-level model in which this exact/metaheuristic hybrid is further hybridized with branch-and-bound techniques and mini-buckets is studied. As a case study, we have applied these algorithms to the resolution of the maximum density still life problem, a hard constraint optimization problem based on Conways game of life. The resulting algorithm consistently finds optimal patterns for up to date solved instances in less time than current approaches. Moreover, it is shown that this proposal provides new best known solutions for very large instances.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3481",
        "title": "Bounds Arc Consistency for Weighted CSPs",
        "authors": [
            "Matthias Zytnicki",
            "Christine Gaspin",
            "Simon de Givry",
            "Thomas Schiex"
        ],
        "abstract": "  The Weighted Constraint Satisfaction Problem (WCSP) framework allows representing and solving problems involving both hard constraints and cost functions. It has been applied to various problems, including resource allocation, bioinformatics, scheduling, etc. To solve such problems, solvers usually rely on branch-and-bound algorithms equipped with local consistency filtering, mostly soft arc consistency.  However, these techniques are not well suited to solve problems with very large domains. Motivated by the resolution of an RNA gene localization problem inside large genomic sequences, and in the spirit of bounds consistency for large domains in crisp CSPs, we introduce soft bounds arc consistency, a new weighted local consistency specifically designed for WCSP with very large domains. Compared to  soft arc consistency, BAC provides significantly improved time and space asymptotic complexity. In this paper, we show how the semantics of cost functions can be exploited to further improve the time complexity of BAC. We also compare both in theory and in practice the efficiency of BAC on a WCSP  with bounds consistency enforced on a crisp CSP using cost variables. On two different real problems modeled as WCSP, including our RNA gene localization problem, we observe that maintaining bounds arc consistency outperforms arc consistency and also improves over bounds consistency enforced on a constraint model with cost variables.\n\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3483",
        "title": "Relaxed Survey Propagation for The Weighted Maximum Satisfiability Problem",
        "authors": [
            "Hai Leong Chieu",
            "Wee Sun Sun Lee"
        ],
        "abstract": "The survey propagation (SP) algorithm has been shown to work well on large instances of the random 3-SAT problem near its phase transition. It was shown that SP estimates marginals over covers that represent clusters of solutions. The SP-y algorithm generalizes SP to work on the maximum satisfiability (Max-SAT) problem, but the cover interpretation of SP does not generalize to SP-y. In this paper,  we formulate the relaxed survey propagation (RSP) algorithm, which extends the SP algorithm to apply to the  weighted Max-SAT problem. We show that RSP has an interpretation of  estimating marginals over covers violating a set of clauses with  minimal weight. This naturally generalizes the cover  interpretation of SP. Empirically, we show that RSP outperforms  SP-y and other state-of-the-art Max-SAT solvers on random Max-SAT instances. RSP also outperforms state-of-the-art weighted Max-SAT solvers on random weighted Max-SAT instances.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3486",
        "title": "The Role of Macros in Tractable Planning",
        "authors": [
            "Anders Jonsson"
        ],
        "abstract": "This paper presents several new tractability results for planning based on macros. We describe an algorithm that optimally solves planning problems in a class that we call inverted tree reducible, and is provably tractable for several subclasses of this class. By using macros to store partial plans that recur frequently in the solution, the algorithm is polynomial in time and space even for exponentially long plans. We generalize the inverted tree reducible class in several ways and describe modifications of the algorithm to deal with these new classes. Theoretical results are validated in experiments.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3489",
        "title": "Join-Graph Propagation Algorithms",
        "authors": [
            "Robert Mateescu",
            "Kalev Kask",
            "Vibhav Gogate",
            "Rina Dechter"
        ],
        "abstract": "The paper investigates parameterized approximate message-passing schemes that are based on bounded inference and are inspired by Pearl's belief propagation algorithm (BP). We start with the bounded inference mini-clustering algorithm and then move to the iterative scheme called Iterative Join-Graph Propagation (IJGP), that combines both iteration and bounded inference. Algorithm IJGP belongs to the class of Generalized Belief Propagation algorithms, a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation, as well as a number of other state-of-the-art algorithms on several classes of networks. We also provide insight into the accuracy of iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation schemes.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3490",
        "title": "BnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm",
        "authors": [
            "William Yeoh",
            "Ariel Felner",
            "Sven Koenig"
        ],
        "abstract": "Distributed constraint optimization (DCOP) problems are a popular way of formulating and solving agent-coordination problems. A DCOP problem is a problem where several agents coordinate their values such that the sum of the resulting constraint costs is minimal. It is often desirable to solve DCOP problems with memory-bounded and asynchronous algorithms. We introduce Branch-and-Bound ADOPT (BnB-ADOPT), a memory-bounded asynchronous DCOP search algorithm that uses the message-passing and communication framework of ADOPT (Modi, Shen, Tambe, and Yokoo, 2005), a well known memory-bounded asynchronous DCOP search algorithm, but changes the search strategy of ADOPT from best-first search to depth-first branch-and-bound search. Our experimental results show that BnB-ADOPT finds cost-minimal solutions up to one order of magnitude faster than ADOPT for a variety of large DCOP problems and is as fast as NCBB, a memory-bounded synchronous DCOP search algorithm, for most of these DCOP problems. Additionally, it is often desirable to find bounded-error solutions for DCOP problems within a reasonable amount of time since finding cost-minimal solutions is NP-hard. The existing bounded-error approximation mechanism allows users only to specify an absolute error bound on the solution cost but a relative error bound is often more intuitive. Thus, we present two new bounded-error approximation mechanisms that allow for relative error bounds and implement them on top of BnB-ADOPT.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3491",
        "title": "Soft Goals Can Be Compiled Away",
        "authors": [
            "Emil Keyder",
            "Hector Geffner"
        ],
        "abstract": "Soft goals extend the classical model of planning with a simple model of preferences. The best plans are then not the ones with least cost but the ones with maximum utility, where the utility of a plan is the sum of the utilities of the soft goals achieved minus the plan cost. Finding plans with high utility appears to involve two linked problems: choosing a subset of soft goals to achieve and finding a low-cost plan to achieve them.  New search algorithms and heuristics have been developed for planning with soft goals, and a new track has been introduced in the International Planning Competition (IPC) to test their performance. In this note, we show however that these extensions are not needed: soft goals do not increase the expressive power of the basic model of planning with action costs, as they can easily be compiled away. We apply this compilation to the problems of the net-benefit track of the most recent IPC, and show that optimal and satisficing cost-based planners do better on the compiled problems than optimal and satisficing net-benefit planners on the original problems with explicit soft goals. Furthermore, we show that penalties, or negative preferences expressing conditions to avoid, can also be compiled away using a similar idea.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3492",
        "title": "ParamILS: An Automatic Algorithm Configuration Framework",
        "authors": [
            "Frank Hutter",
            "Thomas Stuetzle",
            "Kevin Leyton-Brown",
            "Holger H. Hoos"
        ],
        "abstract": "The identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm configuration problem. More formally, we provide methods for optimizing a target algorithm's performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for SAT. We also present what is, to our knowledge, the first published work on automatically configuring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identified with considerable effort. Nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3493",
        "title": "Predicting the Performance of IDA* using Conditional Distributions",
        "authors": [
            "Uzi Zahavi",
            "Ariel Felner",
            "Neil Burch",
            "Robert C. Holte"
        ],
        "abstract": "Korf, Reid, and Edelkamp introduced a formula to predict the number of nodes IDA* will expand on a single iteration for a given consistent heuristic, and experimentally demonstrated that it could make very accurate predictions. In this paper we show that, in addition to requiring the heuristic to be consistent, their formulas predictions are accurate only at levels of the brute-force search tree where the heuristic values obey the unconditional distribution that they defined and then used in their formula. We then propose a new formula that works well without these requirements, i.e., it can  make accurate predictions of IDA*s performance for inconsistent heuristics and if the heuristic values in any\nlevel do not obey the unconditional distribution. In order to achieve this we introduce the conditional distribution of heuristic values which is a generalization of their unconditional heuristic distribution. We also provide extensions of our formula that handle individual start states and the augmentation of IDA* with bidirectional pathmax (BPMX), a technique for propagating heuristic values when inconsistent heuristics are used. Experimental results demonstrate the accuracy of our new method and all its variations.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3626",
        "title": "Modeling Concept Combinations in a Quantum-theoretic Framework",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "We present modeling for conceptual combinations which uses the mathematical formalism of quantum theory. Our model faithfully describes a large amount of experimental data collected by different scholars on concept conjunctions and disjunctions. Furthermore, our approach sheds a new light on long standing drawbacks connected with vagueness, or fuzziness, of concepts, and puts forward a completely novel possible solution to the 'combination problem' in concept theory. Additionally, we introduce an explanation for the occurrence of quantum structures in the mechanisms and dynamics of concepts and, more generally, in cognitive and decision processes, according to which human thought is a well structured superposition of a 'logical thought' and a 'conceptual thought', and the latter usually prevails over the former, at variance with some widespread beliefs\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3825",
        "title": "Reasoning About the Transfer of Control",
        "authors": [
            "Wiebe van der Hoek",
            "Dirk Walther",
            "Michael Wooldridge"
        ],
        "abstract": " We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents stem from a distribution of atomic Boolean variables to individual agents -- the choices available to a coalition correspond to assignments to the variables the coalition controls. The basic modal constructs of DCL-PC are of the form coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic logic modalities in which atomic programs are of the form agent i gives control of variable p to agent j; as usual in dynamic logic, these atomic programs may be combined using sequence, iteration, choice, and test operators to form complex programs. By combining such dynamic transfer programs with cooperation modalities, it becomes possible to reason about how the power of agents and coalitions is affected by the transfer of control. We give two alternative semantics for the logic: a direct semantics, in which we capture the distributions of Boolean variables to agents; and a more conventional Kripke semantics. We prove that these semantics are equivalent, and then present an axiomatization for the logic. We investigate the computational complexity of model checking and satisfiability for DCL-PC, and show that both problems are PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally, we investigate the characterisation of control in DCL-PC. We distinguish between first-order control -- the ability of an agent or coalition to control some state of affairs through the assignment of values to the variables under the control of the agent or coalition -- and second-order control -- the ability of an agent to exert control over the control that other agents have by transferring variables to other agents. We give a logical characterisation of second-order control.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3827",
        "title": "Efficient Planning under Uncertainty with Macro-actions",
        "authors": [
            "Ruijie He",
            "Emma Brunskill",
            "Nicholas Roy"
        ],
        "abstract": "Deciding how to act in partially observable environments remains an active area of research. Identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states. Towards addressing this challenge, we present an online, forward-search algorithm called the Posterior Belief Distribution (PBD). PBD leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken, given the set of observation sequences that could be received during this process. This method allows us to efficiently evaluate the expected reward of a sequence of primitive actions, which we refer to as macro-actions. We present a formal analysis of our approach, and examine its performance on two very large simulation experiments: scientific exploration and a target monitoring domain. We also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment, which suggests that our approach has practical potential for planning in real-world, large partially observable domains where a multi-step lookahead is required to achieve good performance.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3830",
        "title": "Interactive Cost Configuration Over Decision Diagrams",
        "authors": [
            "Henrik Reif Andersen",
            "Tarik Hadzic",
            "David Pisinger"
        ],
        "abstract": "In many AI domains such as product configuration, a user should interactively specify a solution that must satisfy  a set of constraints. In such scenarios, offline compilation of feasible solutions into a tractable representation is an important approach to delivering efficient backtrack-free  user interaction online. In particular,binary decision diagrams (BDDs) have been successfully used as a compilation target for product and service configuration. In this paper we discuss how to extend BDD-based configuration to scenarios involving cost functions which express user preferences.\n",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3831",
        "title": "An Investigation into Mathematical Programming for Finite Horizon Decentralized POMDPs",
        "authors": [
            "Raghav Aras",
            "Alain Dutech"
        ],
        "abstract": "Decentralized planning in uncertain environments is a complex task generally dealt with by using a decision-theoretic approach, mainly through the framework of Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). Although DEC-POMDPS are a general and powerful modeling tool, solving them is a task with an overwhelming complexity that can be doubly exponential. In this paper, we study an alternate formulation of DEC-POMDPs relying on a sequence-form representation of policies. From this formulation, we show how to derive Mixed Integer Linear Programming (MILP) problems that, once solved, give exact optimal solutions to the DEC-POMDPs. We show that these MILPs can be derived either by using some combinatorial characteristics of the optimal solutions of the DEC-POMDPs or by using concepts borrowed from game theory. Through an experimental validation on classical test problems from the DEC-POMDP literature, we compare our approach to existing algorithms. Results show that mathematical programming outperforms dynamic programming but is less efficient than forward search, except for some particular problems.\nThe main contributions of this work are the use of mathematical programming for DEC-POMDPs and a better understanding of DEC-POMDPs and of their solutions. Besides, we argue that our alternate representation of DEC-POMDPs could be helpful for designing novel algorithms looking for approximate solutions to DEC-POMDPs.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3833",
        "title": "Active Tuples-based Scheme for Bounding Posterior Beliefs",
        "authors": [
            "Bozhena Bidyuk",
            "Rina Dechter",
            "Emma Rollon"
        ],
        "abstract": "The paper presents a scheme for computing lower and upper bounds on the posterior marginals in Bayesian networks with discrete variables. Its power lies in its ability to use any available scheme that bounds the probability of evidence or posterior marginals and enhance its performance in an anytime manner. The scheme uses the cutset conditioning principle to tighten existing bounding schemes  and to facilitate anytime behavior, utilizing  a fixed  number of cutset tuples. The accuracy of the bounds improves as the number of used cutset tuples increases and so does the computation time. We demonstrate empirically the value of our scheme for bounding posterior marginals and probability of evidence using a variant of the bound propagation algorithm as a plug-in scheme.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3835",
        "title": "On Action Theory Change",
        "authors": [
            "Ivan Jos\u00e9 Varzinczak"
        ],
        "abstract": "As historically acknowledged in the Reasoning about Actions and Change community, intuitiveness of a logical domain description cannot be fully automated. Moreover, like any other logical theory, action theories may also evolve, and thus knowledge engineers need revision methods to help in accommodating new incoming information about the behavior of actions in an adequate manner. The present work is about changing action domain descriptions in multimodal logic. Its contribution is threefold: first we revisit the semantics of action theory contraction proposed in previous work, giving more robust operators that express minimal change based on a notion of distance between Kripke-models. Second we give algorithms for syntactical action theory contraction and establish their correctness with respect to our semantics for those action theories that satisfy a principle of modularity investigated in previous work. Since modularity can be ensured for every action theory and, as we show here, needs to be computed at most once during the evolution of a domain description, it does not represent a limitation at all to the method here studied. Finally we state AGM-like postulates for action theory contraction and assess the behavior of our operators with respect to them. Moreover, we also address the revision counterpart of action theory change, showing that it benefits from our semantics for contraction.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3838",
        "title": "Change in Abstract Argumentation Frameworks: Adding an Argument",
        "authors": [
            "Claudette Cayrol",
            "Florence Dupin de Saint-Cyr",
            "Marie-Christine Lagasquie-Schiex"
        ],
        "abstract": "In this paper, we address the problem of change in an abstract argumentation system. We focus on a particular change: the addition of  a new argument which interacts with previous arguments. We study the impact of such an addition on the outcome of the argumentation system, more particularly on the set of its extensions. Several properties for this change operation are defined by comparing the new set of extensions to the initial one, these properties are called structural when the comparisons are based on set-cardinality or set-inclusion relations. Several other properties are proposed where comparisons are based on the status of some particular arguments: the accepted arguments; these properties refer to the evolution of this status during the change, e.g., Monotony and Priority to Recency. All these  properties may be more or less desirable according to specific applications. They are studied under two particular semantics: the grounded and preferred semantics.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3839",
        "title": "The LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks",
        "authors": [
            "Silvia Richter",
            "Matthias Westphal"
        ],
        "abstract": "LAMA is a classical planning system based on heuristic forward search. Its core feature is the use of a pseudo-heuristic derived from landmarks, propositional formulas that must be true in every solution of a planning task. LAMA builds on the Fast Downward planning system, using finite-domain rather than binary state variables and multi-heuristic search. The latter is employed to combine the landmark heuristic with a variant of the well-known FF heuristic. Both heuristics are cost-sensitive, focusing on high-quality solutions in the case where actions have non-uniform cost. A weighted A* search is used with iteratively decreasing weights, so that the planner continues to search for plans of better quality until the search is terminated.\nLAMA showed best performance among all planners in the sequential satisficing track of the International Planning Competition 2008. In this paper we present the system in detail and investigate which features of LAMA are crucial for its performance. We present individual results for some of the domains used at the competition, demonstrating good and bad cases for the techniques implemented in LAMA. Overall, we find that using landmarks improves performance, whereas the incorporation of action costs into the heuristic estimators proves not to be beneficial. We show that in some domains a search that ignores cost solves far more problems, raising the question of how to deal with action costs more effectively in the future. The iterated weighted A* search greatly improves results, and shows synergy effects with the use of landmarks.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3841",
        "title": "Narrative Planning: Balancing Plot and Character",
        "authors": [
            "Mark Owen Riedl",
            "Robert Michael Young"
        ],
        "abstract": "Narrative, and in particular storytelling, is an important part of the human experience.  Consequently, computational systems that can reason about narrative can be more effective communicators, entertainers, educators, and trainers.  One of the central challenges in computational narrative reasoning is narrative generation, the automated creation of meaningful event sequences.  There are many factors -- logical and aesthetic -- that contribute to the success of a narrative artifact.  Central to this success is its understandability.  We argue that the following two attributes of narratives are universal: (a) the logical causal progression of plot, and (b) character believability.  Character believability is the perception by the audience that the actions performed by characters do not negatively impact the audiences suspension of disbelief.  Specifically, characters must be perceived by the audience to be intentional agents.  In this article, we explore the use of refinement search as a technique for solving the narrative generation problem -- to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold. We describe a novel refinement search planning algorithm -- the Intent-based Partial Order Causal Link (IPOCL) planner -- that, in addition to creating causally sound plot progression, reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals. We present the results of an empirical evaluation that demonstrates that narrative plans generated by the IPOCL algorithm support audience comprehension of character intentions better than plans generated by conventional partial-order planners.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3842",
        "title": "Developing Approaches for Solving a Telecommunications Feature Subscription Problem",
        "authors": [
            "David Lesaint",
            "Deepak Mehta",
            "Barry O'Sullivan",
            "Luis Quesada",
            "Nic Wilson"
        ],
        "abstract": "Call control features (e.g., call-divert, voice-mail) are primitive options to which users can subscribe off-line to personalise their  service. The configuration of a feature subscription involves choosing and sequencing features from a catalogue and is subject to  constraints that prevent undesirable feature interactions at run-time. When the subscription requested by a user is inconsistent, one  problem is to find an optimal relaxation,  which is a generalisation of the feedback vertex  set problem on directed graphs, and thus it is an NP-hard task. We present several constraint programming formulations of the problem. We also present formulations using partial  weighted maximum Boolean satisfiability and mixed integer linear programming.  We study all these formulations by experimentally comparing them  on a variety of randomly generated instances of the feature subscription problem.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3846",
        "title": "Fast Set Bounds Propagation Using a BDD-SAT Hybrid",
        "authors": [
            "Graeme Gange",
            "Peter James Stuckey",
            "Vitaly Lagoon"
        ],
        "abstract": "Binary Decision Diagram (BDD) based set bounds propagation is a powerful approach to solving set-constraint satisfaction problems. However, prior BDD based techniques in- cur the significant overhead of constructing and manipulating graphs during search. We present a set-constraint solver which combines BDD-based set-bounds propagators with the learning abilities of a modern SAT solver. Together with a number of improvements beyond the basic algorithm, this solver is highly competitive with existing propagation based set constraint solvers.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3847",
        "title": "Automatic Induction of Bellman-Error Features for Probabilistic Planning",
        "authors": [
            "Jia-Hong Wu",
            "Robert Givan"
        ],
        "abstract": "Domain-specific features are important in representing problem structure throughout machine learning and decision-theoretic planning. In planning, once state features are provided, domain-independent algorithms such as approximate value iteration can learn weighted combinations of those features that often perform well as heuristic estimates of state value (e.g., distance to the goal). Successful applications in real-world domains often require features crafted by human experts. Here, we propose automatic processes for learning useful domain-specific feature sets with little or no human intervention. Our methods select and add features that describe state-space regions of high inconsistency in the Bellman equation (statewise Bellman error) during approximate value iteration. Our method can be applied using any real-valued-feature hypothesis space and corresponding learning method for selecting features from training sets of state-value pairs. We evaluate the method with hypothesis spaces defined by both relational and propositional feature languages, using nine probabilistic planning domains. We show that approximate value iteration using a relational feature space performs at the state-of-the-art in domain-independent stochastic relational planning. Our method provides the first domain-independent approach that plays Tetris successfully (without human-engineered features).\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3848",
        "title": "Approximate Model-Based Diagnosis Using Greedy Stochastic Search",
        "authors": [
            "Alexander Feldman",
            "Gregory Provan",
            "Arjan van Gemund"
        ],
        "abstract": "We propose a StochAstic Fault diagnosis AlgoRIthm, called SAFARI, which trades off guarantees of computing minimal diagnoses for computational efficiency. We empirically demonstrate, using the 74XXX and ISCAS-85 suites of benchmark combinatorial circuits, that SAFARI achieves several orders-of-magnitude speedup over two well-known deterministic algorithms, CDA* and HA*, for multiple-fault diagnoses; further, SAFARI can compute a range of multiple-fault diagnoses that CDA* and HA* cannot. We also prove that SAFARI is optimal for a range of propositional fault models, such as the widely-used weak-fault models (models with ignorance of abnormal behavior). We discuss the optimality of SAFARI in a class of strong-fault circuit models with stuck-at failure modes. By modeling the algorithm itself as a Markov chain, we provide exact bounds on the minimality of the diagnosis computed. SAFARI also displays strong anytime behavior, and will return a diagnosis after any non-trivial inference time.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3850",
        "title": "A Model-Based Active Testing Approach to Sequential Diagnosis",
        "authors": [
            "Alexander Feldman",
            "Gregory Provan",
            "Arjan van Gemund"
        ],
        "abstract": "Model-based diagnostic reasoning often leads to a large number of diagnostic hypotheses. The set of diagnoses can be reduced by taking into account extra observations (passive monitoring), measuring additional variables (probing) or executing additional tests (sequential diagnosis/test sequencing). In this paper we combine the above approaches with techniques from Automated Test Pattern Generation (ATPG) and Model-Based Diagnosis (MBD) into a framework called FRACTAL (FRamework for ACtive Testing ALgorithms). Apart from the inputs and outputs that connect a system to its environment, in active testing we consider additional input variables to which a sequence of test vectors can be supplied. We address the computationally hard problem of computing optimal control assignments (as defined in FRACTAL) in terms of a greedy approximation algorithm called FRACTAL-G. We compare the decrease in the number of remaining minimal cardinality diagnoses of FRACTAL-G to that of two more FRACTAL algorithms: FRACTAL-ATPG and FRACTAL-P. FRACTAL-ATPG is based on ATPG and sequential diagnosis while FRACTAL-P is based on probing and, although not an active testing algorithm, provides a baseline for comparing the lower bound on the number of reachable diagnoses for the FRACTAL algorithms. We empirically evaluate the trade-offs of the three FRACTAL algorithms by performing extensive experimentation on the ISCAS85/74XXX benchmark of combinational circuits.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3851",
        "title": "Intrusion Detection using Continuous Time Bayesian Networks",
        "authors": [
            "Jing Xu",
            "Christian R. Shelton"
        ],
        "abstract": "Intrusion detection systems (IDSs) fall into two high-level categories: network-based systems (NIDS) that monitor network behaviors, and host-based systems (HIDS) that monitor system calls. In this work, we present a general technique for both systems. We use anomaly detection, which identifies patterns not conforming to a historic norm. In both types of systems, the rates of change vary dramatically over time (due to burstiness) and over components (due to service difference). To efficiently model such systems, we use continuous time Bayesian networks (CTBNs) and avoid specifying a fixed update interval common to discrete-time models. We build generative models from the normal training data, and abnormal behaviors are flagged based on their likelihood under this norm. For NIDS, we construct a hierarchical CTBN model for the network packet traces and use Rao-Blackwellized particle filtering to learn the parameters. We illustrate the power of our method through experiments on detecting real worms and identifying hosts on two publicly available network traces, the MAWI dataset and the LBNL dataset. For HIDS, we develop a novel learning method to deal with the finite resolution of system log file time stamps, without losing the benefits of our continuous time model. We demonstrate the method by detecting intrusions in the DARPA 1998 BSM dataset.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3853",
        "title": "Implicit Abstraction Heuristics",
        "authors": [
            "Michael Katz",
            "Carmel Domshlak"
        ],
        "abstract": "State-space search with explicit abstraction heuristics is at the state of the art of cost-optimal planning. These heuristics are inherently limited, nonetheless, because the size of the abstract space must be bounded by some, even if a very large, constant. Targeting this shortcoming, we introduce the notion of (additive) implicit abstractions, in which the planning task is abstracted by instances of tractable fragments of optimal planning. We then introduce a concrete setting of this framework, called fork-decomposition, that is based on two novel fragments of tractable cost-optimal planning. The induced admissible heuristics are then studied formally and empirically. This  study testifies for the accuracy of the fork decomposition heuristics, yet our empirical evaluation also stresses the tradeoff between their accuracy and the runtime complexity of computing them. Indeed, some of the power of the explicit abstraction heuristics comes from precomputing the heuristic function offline and then determining h(s) for each evaluated state s by a very fast lookup in a database. By contrast, while fork-decomposition heuristics can be calculated in polynomial time,  computing them is far from being fast. To address this problem, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics  can be successfully overcome. We demonstrate that an equivalent of the explicit abstraction notion of a database exists for the fork-decomposition abstractions as well, despite their exponential-size abstract spaces. We then verify empirically that heuristic search with the databased\" fork-decomposition heuristics favorably competes with the state of the art of cost-optimal planning.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3854",
        "title": "A Constraint Satisfaction Framework for Executing Perceptions and Actions in Diagrammatic Reasoning",
        "authors": [
            "Bonny Banerjee",
            "B. Chandrasekaran"
        ],
        "abstract": "Diagrammatic reasoning (DR) is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on language-like representations. The research reported in this paper is a contribution to building a general purpose DR system as an extension to a SOAR-like problem solving architecture. The work is in a framework in which DR is modeled as a process where subtasks are solved, as appropriate, either by inference from symbolic representations or by interaction with a diagram, i.e., perceiving specified information from a diagram or modifying/creating objects in a diagram in specified ways according to problem solving needs. The perceptions and actions in most DR systems built so far are hand-coded for the specific application, even when the rest of the system is built using the general architecture. The absence of a general framework for executing perceptions/actions poses as a major hindrance to using them opportunistically -- the essence of open-ended search in problem solving.\nOur goal is to develop a framework for executing a wide variety of specified perceptions and actions across tasks/domains without human intervention. We observe that the domain/task-specific visual perceptions/actions can be transformed into domain/task-independent spatial problems. We specify a spatial problem as a quantified constraint satisfaction problem in the real domain using an open-ended vocabulary of properties, relations and actions involving three kinds of diagrammatic objects -- points, curves, regions. Solving a spatial problem from this specification requires computing the equivalent simplified quantifier-free expression, the complexity of which is inherently doubly exponential. We represent objects as configuration of simple elements to facilitate decomposition of complex problems into simpler and similar subproblems. We show that, if the symbolic solution to a subproblem can be expressed concisely, quantifiers can be eliminated from spatial problems in low-order polynomial time using similar previously solved subproblems. This requires determining the similarity of two problems, the existence of a mapping between them computable in polynomial time, and designing a memory for storing previously solved problems so as to facilitate search. The efficacy of the idea is shown by time complexity analysis. We demonstrate the proposed approach by executing perceptions and actions involved in DR tasks in two army applications.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3857",
        "title": "Case-Based Subgoaling in Real-Time Heuristic Search for Video Game Pathfinding",
        "authors": [
            "Vadim Bulitko",
            "Yngvi Bj\u00f6rnsson",
            "Ramon Lawrence"
        ],
        "abstract": "Real-time heuristic search algorithms satisfy a constant bound on the amount of planning per action, independent of problem size. As a result, they scale up well as problems become larger. This property would make them well suited for video games where Artificial Intelligence controlled agents must react quickly to user commands and to other agents actions. On the downside, real-time search algorithms employ learning methods that frequently lead to poor solution quality and cause the agent to appear irrational by re-visiting the same problem states repeatedly. The situation changed recently with a new algorithm, D LRTA*, which attempted to eliminate learning by automatically selecting subgoals. D LRTA* is well poised for video games, except it has a complex and memory-demanding pre-computation phase during which it builds a database of subgoals. In this paper, we propose a simpler and more memory-efficient way of pre-computing subgoals thereby eliminating the main obstacle to applying state-of-the-art real-time search methods in video games. The new algorithm solves a number of randomly chosen problems off-line, compresses the solutions into a series of subgoals and stores them in a database. When presented with a novel problem on-line, it queries the database for the most similar previously solved case and uses its subgoals to solve the problem. In the domain of pathfinding on four large video game maps, the new algorithm delivers solutions eight times better while using 57 times less memory and requiring 14% less pre-computation time.\n\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3859",
        "title": "A Utility-Theoretic Approach to Privacy in Online Services",
        "authors": [
            "Andreas Krause",
            "Eric Horvitz"
        ],
        "abstract": "Online offerings such as web search, news portals, and e-commerce applications face the challenge of providing high-quality service to a large, heterogeneous user base. Recent efforts have highlighted the potential to improve performance by introducing methods to personalize services based on special knowledge about users and their context. For example, a users demographics, location, and past search and browsing may be useful in enhancing the results offered in response to web search queries.  However, reasonable concerns about privacy by both users, providers, and government agencies acting on behalf of citizens, may limit access by services to such information.   We introduce and explore an economics of privacy in personalization, where people can opt to share personal information, in a standing or on-demand manner, in return for expected enhancements in the quality of an online service. We focus on the example of web search and formulate realistic objective functions for search efficacy and privacy. We demonstrate how we can find a provably near-optimal optimization of the utility-privacy tradeoff in an efficient manner.  We evaluate our methodology on data drawn from a log of the search activity of volunteer participants. We separately assess users' preferences about privacy and utility via a large-scale survey, aimed at eliciting preferences about peoples' willingness to trade the sharing of personal data in returns for gains in search efficiency. We show that a significant level of personalization can be achieved using a relatively small amount of information about users.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3860",
        "title": "Planning with Noisy Probabilistic Relational Rules",
        "authors": [
            "Tobias Lang",
            "Marc Toussaint"
        ],
        "abstract": "Noisy probabilistic relational rules are a promising world model representation for several reasons. They are compact and generalize over world instantiations. They are usually interpretable and they can be learned effectively from the action experiences in complex worlds. We investigate reasoning with such rules in grounded relational domains. Our algorithms exploit the compactness of rules for efficient and flexible decision-theoretic planning. As a first approach, we combine these rules with the Upper Confidence Bounds applied to Trees (UCT) algorithm based on look-ahead trees. Our second approach converts these rules into a structured dynamic Bayesian network representation and predicts the effects of action sequences using approximate inference and beliefs over world states. We evaluate the effectiveness of our approaches for planning in a simulated complex 3D robot manipulation scenario with an articulated manipulator and realistic physics and in domains of the probabilistic planning competition. Empirical results show that our methods can solve problems where existing methods fail.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3861",
        "title": "Best-First Heuristic Search for Multicore Machines",
        "authors": [
            "Ethan Burns",
            "Sofia Lemons",
            "Wheeler Ruml",
            "Rong Zhou"
        ],
        "abstract": "To harness modern multicore processors, it is imperative to develop parallel versions of fundamental algorithms. In this paper, we compare different approaches to parallel best-first search in a shared-memory setting. We present a new method, PBNF, that uses abstraction to partition the state space and to detect duplicate states without requiring frequent locking. PBNF allows speculative expansions when necessary to keep threads busy. We identify and fix potential livelock conditions in our approach, proving its correctness using temporal logic. Our approach is general, allowing it to extend easily to suboptimal and anytime heuristic search. In an empirical comparison on STRIPS planning, grid pathfinding, and sliding tile puzzle problems using 8-core machines, we show that A*, weighted A* and Anytime weighted A* implemented using PBNF yield faster search than improved versions of previous parallel search proposals.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3863",
        "title": "An Effective Algorithm for and Phase Transitions of the Directed Hamiltonian Cycle Problem",
        "authors": [
            "Gerold J\u00e4ger",
            "Weixiong Zhang"
        ],
        "abstract": "The Hamiltonian cycle problem (HCP) is an important combinatorial problem with applications in many areas. It is among the first  problems used for studying intrinsic properties, including phase transitions,  of combinatorial problems. While thorough theoretical and experimental analyses have been made on the HCP in undirected graphs, a limited  amount of work has been done for the HCP in directed graphs (DHCP). \nThe main contribution of this work is an effective algorithm for the DHCP. Our algorithm explores and exploits the close relationship between the DHCP and the  Assignment Problem (AP) and utilizes a technique based on Boolean satisfiability (SAT). By combining effective algorithms for the AP and SAT, our algorithm significantly outperforms previous exact DHCP algorithms, including an algorithm based on the award-winning Concorde TSP algorithm. The second result of the current study is an experimental analysis of phase transitions of the DHCP, verifying and refining a known phase  transition of the DHCP. \n\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3866",
        "title": "Automated Search for Impossibility Theorems in Social Choice Theory: Ranking Sets of Objects",
        "authors": [
            "Christian Geist",
            "Ulle Endriss"
        ],
        "abstract": "We present a method for using standard techniques from satisfiability checking to automatically verify and discover theorems in an area of economic theory known as ranking sets of objects. The key question in this area, which has important applications in social choice theory and decision making under uncertainty, is how to extend an agents preferences over a number of objects to a preference relation over nonempty sets of such objects. Certain combinations of seemingly natural principles for this kind of preference extension can result in logical inconsistencies, which has led to a number of important impossibility theorems. We first prove a general result that shows that for a wide range of such principles, characterised by their syntactic form when expressed in a many-sorted first-order logic, any impossibility exhibited at a fixed (small) domain size will necessarily extend to the general case. We then show how to formulate candidates for impossibility theorems at a fixed domain size in propositional logic, which in turn enables us to automatically search for (general) impossibility theorems using a SAT solver. When applied to a space of 20 principles for preference extension familiar from the literature, this method yields a total of 84 impossibility theorems, including both known and nontrivial new results.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3867",
        "title": "Iterated Belief Change Due to Actions and Observations",
        "authors": [
            "Aaron Hunter",
            "James P. Delgrande"
        ],
        "abstract": "In action domains where agents may have erroneous beliefs, reasoning about the effects of actions involves reasoning about belief change.  In this paper, we use a transition system approach to reason about the evolution of an agents beliefs as actions are executed.  Some  actions cause an agent to perform belief revision while others cause an agent to perform belief update, but the interaction between revision and update can be non-elementary.  We present a set of rationality properties describing the interaction between revision and update, and we introduce a new class of belief change operators for reasoning about alternating sequences of revisions and updates.  Our belief change operators can be characterized in terms of a natural shifting operation on total pre-orderings over interpretations.  We compare our approach with related work on iterated belief change due to action, and we conclude with some directions for future research.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3871",
        "title": "Non-Deterministic Policies in Markovian Decision Processes",
        "authors": [
            "Mahdi Milani Fard",
            "Joelle Pineau"
        ],
        "abstract": "Markovian processes have long been used to model stochastic environments. Reinforcement learning has emerged as a framework to solve sequential planning and decision-making problems in such environments. In recent years, attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in Markovian environments. Although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decision-making, they cannot be applied in their current form to decision support systems, such as those in medical domains, as they suggest policies that are often highly prescriptive and leave little room for the users input. Without the ability to provide flexible guidelines, it is unlikely that these methods can gain ground with users of such systems. This paper introduces the new concept of non-deterministic policies to allow more flexibility in the users decision-making process, while constraining decisions to remain near optimal solutions. We provide two algorithms to compute non-deterministic policies in discrete domains. We study the output and running time of these method on a set of synthetic and real-world problems. In an experiment with human subjects, we show that humans assisted by hints based on non-deterministic policies outperform both human-only and computer-only agents in a web navigation task.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3872",
        "title": "Second-Order Consistencies",
        "authors": [
            "Christophe Lecoutre",
            "Stephane Cardon",
            "Julien Vion"
        ],
        "abstract": "In this paper, we propose a comprehensive study of second-order consistencies (i.e., consistencies identifying inconsistent pairs of values) for constraint satisfaction. We build a full picture of the relationships existing between four basic second-order consistencies, namely path consistency (PC), 3-consistency (3C), dual consistency (DC) and 2-singleton arc consistency (2SAC), as well as their conservative and strong variants. Interestingly, dual consistency is an original property that can be established by using the outcome of the enforcement of generalized arc consistency (GAC), which makes it rather easy to obtain since constraint solvers typically maintain GAC during search.  On binary constraint  networks, DC is equivalent to PC, but its restriction to existing constraints, called conservative dual consistency (CDC), is strictly stronger than  traditional conservative consistencies derived from path consistency, namely partial path consistency (PPC) and conservative path consistency (CPC).  After introducing a general algorithm to enforce strong (C)DC, we present the results of an experimentation over a wide range of benchmarks that demonstrate the interest of (conservative) dual consistency.  In particular, we show that enforcing (C)DC before search clearly improves the performance of MAC (the algorithm that maintains GAC during search) on several binary and non-binary structured problems.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3875",
        "title": "On-line Planning and Scheduling: An Application to Controlling Modular Printers",
        "authors": [
            "Wheeler Ruml",
            "Minh Binh Do",
            "Rong Zhou",
            "Markus P.J. Fromherz"
        ],
        "abstract": "We present a case study of artificial intelligence techniques applied to the control of production printing equipment.  Like many other real-world applications, this complex domain requires high-speed autonomous decision-making and robust continual operation.  To our knowledge, this work represents the first successful industrial application of embedded domain-independent temporal planning.  Our system handles execution failures and multi-objective preferences.  At its heart is an on-line algorithm that combines techniques from state-space planning and partial-order scheduling.  We suggest that this general architecture may prove useful in other applications as more intelligent systems operate in continual, on-line settings.  Our system has been used to drive several commercial prototypes and has enabled a new product architecture for our industrial partner.  When compared with state-of-the-art off-line planners, our system is hundreds of times faster and often finds better plans.  Our experience demonstrates that domain-independent AI planning based on heuristic search can flexibly handle time, resources, replanning, and multiple objectives in a high-speed practical application without requiring hand-coded control knowledge.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3879",
        "title": "Soft Constraints of Difference and Equality",
        "authors": [
            "Emmanuel Hebrard",
            "D\u00e1niel Marx",
            "Barry O'Sullivan",
            "Igor Razgon"
        ],
        "abstract": "In many combinatorial problems one may need to model the diversity or similarity of assignments in a solution. For example, one may wish to maximise or minimise the number of distinct values in a solution. To formulate problems of this type, we can use soft variants of the well known AllDifferent and AllEqual constraints.  We present a taxonomy of six soft global constraints, generated by combining the two latter ones and the two standard cost functions, which are either maximised or minimised. We characterise the complexity of achieving arc and bounds consistency on these constraints, resolving those cases for which NP-hardness was neither proven nor disproven. In particular, we explore in depth the constraint ensuring that at least k pairs of variables have a common value. We show that achieving arc consistency is NP-hard, however achieving bounds consistency can be done in polynomial time through dynamic programming. Moreover, we show that the maximum number of pairs of equal variables can be approximated by a factor 1/2 with a linear time greedy algorithm. Finally, we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains. Interestingly, this taxonomy shows that enforcing equality is harder than enforcing difference.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3881",
        "title": "Value of Information Lattice: Exploiting Probabilistic Independence for Effective Feature Subset Acquisition",
        "authors": [
            "Mustafa Bilgic",
            "Lise Getoor"
        ],
        "abstract": "We address the cost-sensitive feature acquisition problem, where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features. Because acquiring the features is costly as well, the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized. We describe the Value of Information Lattice (VOILA), an optimal and efficient feature subset acquisition framework. Unlike the common practice, which is to acquire features greedily, VOILA can reason with subsets of features. VOILA efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process. Through empirical evaluation on five medical datasets, we show that the greedy strategy is often reluctant to acquire features, as it cannot forecast the benefit of acquiring multiple features in combination.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3882",
        "title": "Probabilistic Relational Planning with First Order Decision Diagrams",
        "authors": [
            "Saket Joshi",
            "Roni Khardon"
        ],
        "abstract": "Dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations, in particular algebraic decision diagrams, to capture domain dynamics and value functions.  Work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes.  Recent work introduced a first order variant of decision diagrams (FODD) and developed a value iteration algorithm for this representation. This paper develops several improvements to the FODD algorithm that make the approach practical. These include, new reduction operators that decrease the size of the representation, several speedup techniques, and techniques for value approximation.  Incorporating these, the paper presents a planning system, FODD-Planner, for solving relational stochastic planning problems.  The system is evaluated on several domains, including problems from the recent international planning competition, and shows competitive performance with top ranking systems. This is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning.\n\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3885",
        "title": "Scaling up Heuristic Planning with Relational Decision Trees",
        "authors": [
            "Tomas De la Rosa",
            "Sergio Jimenez",
            "Raquel Fuentetaja",
            "Daniel Borrajo"
        ],
        "abstract": "Current evaluation functions for heuristic planning are expensive to compute. In numerous planning problems these functions provide good guidance to the solution, so they are worth the expense. However, when  evaluation functions are misguiding or when planning problems are large enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. Particularly, we define the task of learning search control for heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. Our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain. These planning contexts are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and the static predicates of the planning task. This paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. The first one consists of using the resulting classifier as an action policy. The second one consists of applying the classifier to generate lookahead states within a Best First Search algorithm. Experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.\n\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3886",
        "title": "Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference",
        "authors": [
            "Wei Li",
            "Pascal Poupart",
            "Peter van Beek"
        ],
        "abstract": "Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference.  In this paper, we present techniques for improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations---two relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. In particular, we present two SAT encodings for noisy-OR and two encodings for noisy-MAX that exploit the structure or semantics of the relations to improve both time and space efficiency, and we prove the correctness of the encodings. We experimentally evaluated our techniques on large-scale real and randomly generated Bayesian networks.  On these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisy-OR/MAX relations and scaled up to larger networks. As well, our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach.\n\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3887",
        "title": "The Complexity of Integer Bound Propagation",
        "authors": [
            "Lucas Bordeaux",
            "George Katsirelos",
            "Nina Narodytska",
            "Moshe Y. Vardi"
        ],
        "abstract": "Bound propagation is an important Artificial Intelligence technique used in Constraint Programming tools to deal with numerical constraints. It is typically embedded within a search procedure (\"branch and prune\") and used at every node of the search tree to narrow down the search space, so it is critical that it be fast. The procedure invokes constraint propagators until a common fixpoint is reached, but the known algorithms for this have a pseudo-polynomial worst-case time complexity: they are fast indeed when the variables have a small numerical range, but they have the well-known problem of being prohibitively slow when these ranges are large. An important question is therefore whether strongly-polynomial algorithms exist that compute the common bound consistent fixpoint of a set of constraints. This paper answers this question. In particular we show that this fixpoint computation is in fact NP-complete, even when restricted to binary linear constraints.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3890",
        "title": "Analyzing Search Topology Without Running Any Search: On the Connection Between Causal Graphs and h+",
        "authors": [
            "Joerg Hoffmann"
        ],
        "abstract": "The ignoring delete lists relaxation is of paramount importance for both satisficing and optimal planning. In earlier work, it was observed that the optimal relaxation heuristic h+ has amazing qualities in many classical planning benchmarks, in particular pertaining to the complete absence of local minima. The proofs of this are hand-made, raising the question whether such proofs can be lead automatically by domain analysis techniques. In contrast to earlier disappointing results -- the analysis method has exponential runtime and succeeds only in two extremely simple benchmark domains -- we herein answer this question in the affirmative. We establish connections between causal graph structure and h+ topology. This results in low-order polynomial time analysis methods, implemented in a tool we call TorchLight. Of the 12 domains where the absence of local minima has been proved, TorchLight gives strong success guarantees in 8 domains. Empirically, its analysis exhibits strong performance in a further 2 of these domains, plus in 4 more domains where local minima may exist but are rare. In this way, TorchLight can distinguish easy domains from hard ones. By summarizing structural reasons for analysis failure, TorchLight also provides diagnostic output indicating domain aspects that may cause local minima.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3892",
        "title": "Sequential Diagnosis by Abstraction",
        "authors": [
            "Sajjad Ahmed Siddiqi",
            "Jinbo Huang"
        ],
        "abstract": "When a system behaves abnormally, sequential diagnosis takes a sequence of measurements of the system until the faults causing the abnormality are identified, and the goal is to reduce the diagnostic cost, defined here as the number of measurements. To propose measurement points, previous work employs a heuristic based on reducing the entropy over a computed set of diagnoses. This approach generally has good performance in terms of diagnostic cost, but can fail to diagnose large systems when the set of diagnoses is too large. Focusing on a smaller set of probable diagnoses scales the approach but generally leads to increased average diagnostic costs. In this paper, we propose a new diagnostic framework employing four new techniques, which scales to much larger systems with good performance in terms of diagnostic cost.\nFirst, we propose a new heuristic for measurement point selection that can be computed efficiently, without requiring the set of diagnoses, once the system is modeled as a Bayesian network and compiled into a logical form known as d-DNNF. Second, we extend hierarchical diagnosis, a technique based on system abstraction from our previous work, to handle probabilities so that it can be applied to sequential diagnosis to allow larger systems to be diagnosed. Third, for the largest systems where even hierarchical diagnosis fails, we propose a novel method that converts the system into one that has a smaller abstraction and whose diagnoses form a superset of those of the original system; the new system can then be diagnosed and the result mapped back to the original system. Finally, we propose a novel cost estimation function which can be used to choose an abstraction of the system that is more likely to provide optimal average cost. Experiments with ISCAS-85 benchmark circuits indicate that our approach scales to all circuits in the suite except one that has a flat structure not susceptible to useful abstraction. \n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3893",
        "title": "Most Relevant Explanation in Bayesian Networks",
        "authors": [
            "Changhe Yuan",
            "Heejin Lim",
            "Tsai-Ching Lu"
        ],
        "abstract": "A major inference task in Bayesian networks is explaining why some variables are observed in their particular states using a set of target variables. Existing methods for solving this problem often generate explanations that are either too simple (underspecified) or too complex (overspecified). In this paper, we introduce a method called Most Relevant Explanation (MRE) which finds a partial instantiation of the target variables that maximizes the generalized Bayes factor (GBF) as the best explanation for the given evidence. Our study shows that GBF has several theoretical properties that enable MRE to automatically identify the most relevant target variables in forming its explanation. In particular, conditional Bayes factor (CBF), defined as the GBF of a new explanation conditioned on an existing explanation, provides a soft measure on the degree of relevance of the variables in the new explanation in explaining the evidence given the existing explanation. As a result, MRE is able to automatically prune less relevant variables from its explanation. We also show that CBF is able to capture well the explaining-away phenomenon that is often represented in Bayesian networks. Moreover, we define two dominance relations between the candidate solutions and use the relations to generalize MRE to find a set of top explanations that is both diverse and representative. Case studies on several benchmark diagnostic Bayesian networks show that MRE is often able to find explanatory hypotheses that are not only precise but also concise.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3895",
        "title": "On the Intertranslatability of Argumentation Semantics",
        "authors": [
            "Wolfgang Dvorak",
            "Stefan Woltran"
        ],
        "abstract": "Translations between different nonmonotonic formalisms always have been an important topic in the field, in particular to understand the knowledge-representation capabilities those formalisms offer.  We provide such an investigation in terms of different semantics proposed for abstract argumentation frameworks, a nonmonotonic yet simple formalism which received increasing interest within the last decade. Although the properties of these different semantics are nowadays well understood, there are no explicit results about intertranslatability. We provide such translations wrt. different properties and also give a few novel complexity results which underlie some negative results.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3899",
        "title": "Representing and Reasoning with Qualitative Preferences for Compositional Systems",
        "authors": [
            "Ganesh Ram Santhanam",
            "Samik Basu",
            "Vasant Honavar"
        ],
        "abstract": "Many applications, e.g., Web service composition, complex system design, team formation, etc., rely on methods for identifying collections of objects or entities satisfying some functional requirement. Among the collections that satisfy the functional requirement, it is often necessary to identify one or more collections that are optimal with respect to user preferences over a set of attributes that describe the non-functional properties of the collection.\n",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3902",
        "title": "On the Link between Partial Meet, Kernel, and Infra Contraction and its Application to Horn Logic",
        "authors": [
            "Richard Booth",
            "Thomas Meyer",
            "Ivan Varzinczak",
            "Renata Wassermann"
        ],
        "abstract": "Standard belief change assumes an underlying logic containing full classical propositional logic. However, there are good reasons for considering belief change in less expressive logics as well. In this paper we build on recent investigations by Delgrande on contraction for Horn logic. We show that the standard basic form of contraction, partial meet, is too strong in the Horn case. This result stands in contrast to Delgrande's conjecture that orderly maxichoice is the appropriate form of contraction for Horn logic. We then define a more appropriate notion of basic contraction for the Horn case, influenced by the convexity property holding for full propositional logic and which we refer to as infra contraction. The main contribution of this work is a result which shows that the construction method for Horn contraction for belief sets based on our infra remainder sets corresponds exactly to Hansson's classical kernel contraction for belief sets, when restricted to Horn logic. This result is obtained via a detour through contraction for belief bases. We prove that kernel contraction for belief bases produces precisely the same results as the belief base version of infra contraction. The use of belief bases to obtain this result provides evidence for the conjecture that Horn belief change is best viewed as a hybrid version of belief set change and belief base change. One of the consequences of the link with base contraction is the provision of a representation result for Horn contraction for belief sets in which a version of the Core-retainment postulate features.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3905",
        "title": "MAPP: a Scalable Multi-Agent Path Planning Algorithm with Tractability and Completeness Guarantees",
        "authors": [
            "Ko-Hsin Cindy Wang",
            "Adi Botea"
        ],
        "abstract": "Multi-agent path planning is a challenging problem with numerous real-life applications.  Running a centralized search such as A* in the combined state space of all units is complete and cost-optimal, but scales poorly, as the state space size is exponential in the number of mobile units.  Traditional decentralized approaches, such as FAR and  WHCA*, are faster and more scalable, being based on problem decomposition.  However, such methods are incomplete and provide no guarantees with respect to the running time or the solution quality.  They are not necessarily able to tell in a reasonable time whether they would succeed in finding a solution to a given instance.\nWe introduce MAPP, a tractable algorithm for multi-agent path planning on undirected graphs.  We present a basic version and several extensions.\nThey have low-polynomial worst-case upper bounds for the running time, the memory requirements, and the length of solutions.  Even though all algorithmic versions are incomplete in the general case, each provides formal guarantees on problems it can solve.  For each version, we discuss the algorithms completeness with respect to clearly defined subclasses of instances.\nExperiments were run on realistic game grid maps.  MAPP solved 99.86% of all mobile units, which is 18--22% better than the percentage of FAR and WHCA*.  MAPP marked 98.82% of all units as provably solvable during the first stage of plan computation.  Parts of MAPPs computation can be re-used across instances on the same map.  Speed-wise, MAPP is competitive or significantly faster than WHCA*, depending on whether MAPP performs all computations from scratch.  When data that MAPP can re-use are preprocessed offline and readily available, MAPP is slower than the very fast FAR algorithm by a factor of 2.18 on average.  MAPPs solutions are on average 20% longer than FARs solutions and 7--31% longer than WHCA*s solutions.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3906",
        "title": "Making Decisions Using Sets of Probabilities: Updating, Time Consistency, and Calibration",
        "authors": [
            "Peter D Grunwald",
            "Joseph Y Halpern"
        ],
        "abstract": "We consider how an agent should update her beliefs when her beliefs are represented by a set P of probability distributions, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. We adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from P. We consider two reasonable games that differ in what the bookie knows when he makes his choice. Anomalies that have been observed before, like time inconsistency, can be understood as arising because different games are being played, against bookies with different information. We characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. Finally, we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities. Our results emphasize the key role of the rectangularity condition of Epstein and Schneider.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3909",
        "title": "Scheduling Bipartite Tournaments to Minimize Total Travel Distance",
        "authors": [
            "Richard Hoshino",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "In many professional sports leagues, teams from opposing leagues/conferences compete against one another, playing inter-league games.  This is an example of a bipartite tournament.  In this paper, we consider the problem of reducing the total travel distance of bipartite tournaments, by analyzing inter-league scheduling from the perspective of discrete optimization.  This research has natural applications to sports scheduling, especially for leagues such as the National Basketball Association (NBA) where teams must travel long distances across North America to play all their games, thus consuming much time, money, and greenhouse gas emissions.\nWe introduce the Bipartite Traveling Tournament Problem (BTTP), the inter-league variant of the well-studied Traveling Tournament Problem. We prove that the 2n-team BTTP is NP-complete, but for small values of n, a distance-optimal inter-league schedule can be generated from an algorithm based on minimum-weight 4-cycle-covers.  We apply our theoretical results to the 12-team Nippon Professional Baseball (NPB) league in Japan, producing a provably-optimal schedule requiring 42950 kilometres of total team travel, a 16% reduction compared to the actual distance traveled by these teams during the 2010 NPB season.  We also develop a nearly-optimal inter-league tournament for the 30-team NBA league, just 3.8% higher than the trivial theoretical lower bound.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3910",
        "title": "Topological Value Iteration Algorithms",
        "authors": [
            "Peng Dai",
            "Mausam",
            "Daniel Sabby Weld",
            "Judy Goldsmith"
        ],
        "abstract": "Value iteration is a powerful yet inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, ILAO* and variants of RTDP are state-of-the-art ones. These methods use reachability analysis and heuristic search to avoid some unnecessary backups. However, none of these approaches build the graphical structure of the state transitions in a pre-processing step or use the structural information to systematically decompose a problem, whereby generating an intelligent backup sequence of the state space. In this paper, we present two optimal MDP algorithms. The first algorithm,  topological value iteration (TVI), detects the structure of MDPs and backs up states based on topological sequences. It (1) divides an MDP into strongly-connected components (SCCs), and (2) solves these components sequentially. TVI outperforms VI and other state-of-the-art algorithms vastly when an MDP has multiple, close-to-equal-sized SCCs. The second algorithm,  focused  topological value iteration (FTVI), is an extension of TVI. FTVI restricts its attention to connected components that are relevant for solving the MDP. Specifically, it uses a small amount of heuristic search to eliminate provably sub-optimal actions; this pruning allows FTVI to find smaller connected components, thus running faster.  We demonstrate that FTVI outperforms TVI by an order of magnitude, averaged across several domains. Surprisingly, FTVI also significantly outperforms popular heuristically-informed MDP algorithms such as ILAO*, LRTDP, BRTDP and Bayesian-RTDP in many domains, sometimes by as much as two orders of magnitude. Finally, we characterize the type of domains where FTVI excels --- suggesting a way to an informed choice of solver.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4144",
        "title": "Arguments using ontological and causal knowledge",
        "authors": [
            "Philippe Besnard",
            "Marie-Odile Cordier",
            "Yves Moinard"
        ],
        "abstract": "We investigate an approach to reasoning about causes through argumentation. We consider a causal model for a physical system, and look for arguments about facts. Some arguments are meant to provide explanations of facts whereas some challenge these explanations and so on. At the root of argumentation here, are causal links ({A_1, ... ,A_n} causes B) and ontological links (o_1 is_a o_2). We present a system that provides a candidate explanation ({A_1, ... ,A_n} explains {B_1, ... ,B_m}) by resorting to an underlying causal link substantiated with appropriate ontological links. Argumentation is then at work from these various explaining links. A case study is developed: a severe storm Xynthia that devastated part of France in 2010, with an unaccountably high number of casualties.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4539",
        "title": "Solving the Minimum Common String Partition Problem with the Help of Ants",
        "authors": [
            "S.M. Ferdous",
            "M. Sohel Rahman"
        ],
        "abstract": "In this paper, we consider the problem of finding a minimum common partition of two strings. The problem has its application in genome comparison. As it is an NP-hard, discrete combinatorial optimization problem, we employ a metaheuristic technique, namely, MAX-MIN ant system to solve this problem. To achieve better efficiency we first map the problem instance into a special kind of graph. Subsequently, we employ a MAX-MIN ant system to achieve high quality solutions for the problem. Experimental results show the superiority of our algorithm in comparison with the state of art algorithm in the literature. The improvement achieved is also justified by standard statistical test.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4590",
        "title": "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks",
        "authors": [
            "Enrique Amig\u00f3",
            "Julio Gonzalo",
            "Javier Artiles",
            "Felisa Verdejo"
        ],
        "abstract": "Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen's F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted.\nBesides discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. Remarkably, our experiments show that UIR can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4592",
        "title": "Proximity-Based Non-uniform Abstractions for Approximate Planning",
        "authors": [
            "Jiri Baum",
            "Ann E. Nicholson",
            "Trevor I. Dix"
        ],
        "abstract": "In a deterministic world, a planning agent can be certain of the consequences of its planned sequence of actions. Not so, however, in dynamic, stochastic domains where Markov decision processes are commonly used. Unfortunately these suffer from the curse of dimensionality: if the state space is a Cartesian product of many small sets (dimensions), planning is exponential in the number of those dimensions.\nOur new technique exploits the intuitive strategy of selectively ignoring various dimensions in different parts of the state space. The resulting non-uniformity has strong implications, since the approximation is no longer Markovian, requiring the use of a modified planner. We also use a spatial and temporal proximity measure, which responds to continued planning as well as movement of the agent through the state space, to dynamically adapt the abstraction as planning progresses.\nWe present qualitative and quantitative results across a range of experimental domains showing that an agent exploiting this novel approximation method successfully finds solutions to the planning problem using much less than the full state space. We assess and analyse the features of domains which our method can exploit.\n\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4595",
        "title": "Robust Local Search for Solving RCPSP/max with Durational Uncertainty",
        "authors": [
            "Na Fu",
            "Hoong Chuin Lau",
            "Pradeep R. Varakantham",
            "Fei Xiao"
        ],
        "abstract": "Scheduling problems in manufacturing, logistics and project management have frequently been modeled using the framework of Resource Constrained Project Scheduling Problems with minimum and maximum time lags (RCPSP/max). Due to the importance of these problems, providing scalable solution schedules for RCPSP/max problems is a topic of extensive research. However, all existing methods for solving RCPSP/max assume that durations of activities are known with certainty, an assumption that does not hold in real world scheduling problems where unexpected external events such as manpower availability, weather changes, etc. lead to delays or advances in completion of activities. Thus, in this paper, our focus is on providing a scalable method for solving RCPSP/max problems with durational uncertainty. To that end, we introduce the robust local search method consisting of three key ideas: (a) Introducing and studying the properties of two decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty; (b) Deriving the expression for robust makespan of an execution strategy based on decision rule approximations; and (c) A robust local search mechanism to efficiently compute activity execution strategies that are robust against durational uncertainty. Furthermore, we also provide enhancements to local search that exploit temporal dependencies between activities. Our experimental results illustrate that robust local search is able to provide robust execution strategies efficiently.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4597",
        "title": "Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs",
        "authors": [
            "Matthew L. Ginsberg"
        ],
        "abstract": "We describe ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4598",
        "title": "SAS+ Planning as Satisfiability",
        "authors": [
            "Ruoyun Huang",
            "Yixin Chen",
            "Weixiong Zhang"
        ],
        "abstract": "Planning as satisfiability is a principal approach to planning with many eminent advantages. The existing planning as satisfiability techniques usually use encodings compiled from STRIPS. We introduce a novel SAT encoding scheme (SASE) based on the SAS+ formalism. The new scheme exploits the structural information in SAS+, resulting in an encoding that is both more compact and efficient for planning. We prove the correctness of the new encoding by establishing an isomorphism between the solution plans of SASE and that of STRIPS based encodings. We further analyze the transition variables newly introduced in SASE to explain why it accommodates modern SAT solving algorithms and improves performance. We give empirical statistical results to support our analysis. We also develop a number of techniques to further reduce the encoding size of SASE, and conduct experimental studies to show the strength of each individual technique. Finally, we report extensive experimental results to demonstrate significant improvements of SASE over the state-of-the-art STRIPS based encoding schemes in terms of both time and memory efficiency.\n\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4600",
        "title": "Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams",
        "authors": [
            "Yifeng Zeng",
            "Prashant Doshi"
        ],
        "abstract": " We focus on  the problem of sequential decision  making in partially observable environments shared with  other agents of uncertain types having  similar or  conflicting objectives.   This problem  has been previously  formalized by multiple  frameworks one  of which  is the interactive  dynamic   influence   diagram  (I-DID),   which generalizes  the  well-known  influence  diagram to  the  multiagent setting.  I-DIDs are graphical models and may be used to compute the policy  of an agent  given its  belief over  the physical  state and others models, which changes as  the agent acts and observes in the  multiagent setting.\n",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4601",
        "title": "Counting-Based Search: Branching Heuristics for Constraint Satisfaction Problems",
        "authors": [
            "Gilles Pesant",
            "Claude-Guy Quimper",
            "Alessandro Zanarini"
        ],
        "abstract": "Designing a search heuristic for constraint programming that is reliable across problem domains has been an important research topic in recent years. This paper concentrates on one family of candidates: counting-based search. Such heuristics seek to make branching decisions that preserve most of the solutions by determining what proportion of solutions to each individual constraint agree with that decision. Whereas most generic search heuristics in constraint programming rely on local information at the level of the individual variable, our search heuristics are based on more global information at the constraint level. We design several algorithms that are used to count the number of solutions to specific families of constraints and propose some search heuristics exploiting such information. The experimental part of the paper considers eight problem domains ranging from well-established benchmark puzzles to rostering and sport scheduling. An initial empirical analysis identifies heuristic maxSD as a robust candidate among our ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4603",
        "title": "Semantic Similarity Measures Applied to an Ontology for Human-Like Interaction",
        "authors": [
            "Esperanza Albacete",
            "Javier Calle",
            "Elena Castro",
            "Dolores Cuadra"
        ],
        "abstract": "The focus of this paper is the calculation of similarity between two concepts from an ontology for a Human-Like Interaction system. In order to facilitate this calculation, a similarity function is proposed based on five dimensions (sort, compositional, essential, restrictive and descriptive) constituting the structure of ontological knowledge. The paper includes a proposal for computing a similarity function for each dimension of knowledge. Later on, the similarity values obtained are weighted and aggregated to obtain a global similarity measure. In order to calculate those weights associated to each dimension, four training methods have been proposed. The training methods differ in the element to fit: the user, concepts or pairs of concepts, and a hybrid approach. For evaluating the proposal, the knowledge base was fed from WordNet and extended by using a knowledge editing toolkit (Cognos). The evaluation of the proposal is carried out through the comparison of system responses with those given by human test subjects, both providing a measure of the soundness of the procedure and revealing ways in which the proposal may be improved.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4604",
        "title": "Completeness Guarantees for Incomplete Ontology Reasoners: Theory and Practice",
        "authors": [
            "Bernardo Cuenca Grau",
            "Boris Motik",
            "Giorgos Stoilos",
            "Ian Horrocks"
        ],
        "abstract": "To achieve scalability of query answering, the developers of Semantic Web applications are often forced to use incomplete OWL 2 reasoners, which fail to derive all answers for at least one query, ontology, and data set. The lack of completeness guarantees, however, may be unacceptable for applications in areas such as health care and defence, where missing answers can adversely affect the applications functionality. Furthermore, even if an application can tolerate some level of incompleteness, it is often advantageous to estimate how many and what kind of answers are being lost.\nIn this paper, we present a novel logic-based framework that allows one to check whether a reasoner is complete for a given query Q and ontology T---that is, whether the reasoner is guaranteed to compute all answers to Q w.r.t. T and an arbitrary data set A. Since ontologies and typical queries are often fixed at application design time, our approach allows application developers to check whether a reasoner known to be incomplete in general is actually complete for the kinds of input relevant for the application.\nWe also present a technique that, given a query Q, an ontology T, and reasoners R_1 and R_2 that satisfy certain assumptions, can be used to determine whether, for each data set A, reasoner R_1 computes more answers to Q w.r.t. T and A than reasoner R_2. This allows application developers to select the reasoner that provides the highest degree of completeness for Q and T that is compatible with the applications scalability requirements.\nOur results thus provide a theoretical and practical foundation for the design of future ontology-based information systems that maximise scalability while minimising or even eliminating incompleteness of query answers.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4605",
        "title": "Consistency Techniques for Flow-Based Projection-Safe Global Cost Functions in Weighted Constraint Satisfaction",
        "authors": [
            "J.H.M. Lee",
            "Ka Lun Leung"
        ],
        "abstract": "Many combinatorial problems deal with preferences and violations, the goal of which is to find solutions with the minimum cost.  Weighted constraint satisfaction is a framework for modeling such problems, which consists of a set of cost functions to measure the degree of violation or preferences of different combinations of variable assignments.  Typical solution methods for weighted constraint satisfaction problems (WCSPs) are based on branch-and-bound search, which are made practical through the use of powerful consistency techniques such as AC*, FDAC*, EDAC* to deduce hidden cost information and value pruning during search.  These techniques, however, are designed to be efficient only on binary and ternary cost functions which are represented in table form.  In tackling many real-life problems, high arity (or global) cost functions are required.  We investigate efficient representation scheme and algorithms to bring the benefits of the consistency techniques to also high arity cost functions, which are often derived from hard global constraints from classical constraint satisfaction. The literature suggests some global cost functions can be represented as flow networks, and the minimum cost flow algorithm can be used to compute the minimum costs of such networks in polynomial time.  We show that naive adoption of this flow-based algorithmic method for global cost functions can result in a stronger form of null-inverse consistency.  We further show how the method can be modified to handle cost projections and extensions to maintain generalized versions of AC* and FDAC* for cost functions with more than two variables.  Similar generalization for the stronger EDAC* is less straightforward.  We reveal the oscillation problem when enforcing EDAC* on cost functions sharing more than one variable.  To avoid oscillation, we propose a weak version of EDAC* and generalize it to weak EDGAC* for non-binary cost functions. Using various benchmarks involving the soft variants of hard global constraints ALLDIFFERENT, GCC, SAME, and REGULAR, empirical results demonstrate that our proposal gives improvements of up to an order of magnitude when compared with the traditional constraint optimization approach, both in terms of time and pruning.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4606",
        "title": "Drake: An Efficient Executive for Temporal Plans with Choice",
        "authors": [
            "Patrick Raymond Conrad",
            "Brian Williams"
        ],
        "abstract": "This work presents Drake, a dynamic executive for temporal plans with choice. Dynamic plan execution strategies allow an autonomous agent to react quickly to unfolding events, improving the robustness of the agent. Prior work developed methods for dynamically dispatching Simple Temporal Networks, and further research enriched the expressiveness of the plans executives could handle, including discrete choices, which are the focus of this work. However, in some approaches to date, these additional choices induce significant storage or latency requirements to make flexible execution possible. \nDrake is designed to leverage the low latency made possible by a preprocessing step called compilation, while avoiding high memory costs through a compact representation. We leverage the concepts of labels and environments, taken from prior work in Assumption-based Truth Maintenance Systems (ATMS), to concisely record the implications of the discrete choices, exploiting the structure of the plan to avoid redundant reasoning or storage. Our labeling and maintenance scheme, called the Labeled Value Set Maintenance System, is distinguished by its focus on properties fundamental to temporal problems, and, more generally, weighted graph algorithms. In particular, the maintenance system focuses on maintaining a minimal representation of non-dominated constraints. We benchmark Drakes performance on random structured problems, and find that Drake reduces the size of the compiled representation by a factor of over 500 for large problems, while incurring only a modest increase in run-time latency, compared to prior work in compiled executives for temporal plans with discrete choices.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4607",
        "title": "Reformulating the Situation Calculus and the Event Calculus in the General Theory of Stable Models and in Answer Set Programming",
        "authors": [
            "Joohyung Lee",
            "Ravi Palla"
        ],
        "abstract": "Circumscription and logic programs under the stable model semantics are two well-known nonmonotonic formalisms. The former has served as a basis of classical logic based action formalisms, such as the situation calculus, the event calculus and temporal action logics; the latter has served as a basis of a family of action languages, such as language A and several of its descendants. Based on the discovery that circumscription and the stable model semantics coincide on a class of canonical formulas, we reformulate the situation calculus and the event calculus in the general theory of stable models. We also present a translation that turns the reformulations further into answer set programs, so that efficient answer set solvers can be applied to compute the situation calculus and the event calculus. \n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4613",
        "title": "Local Consistency and SAT-Solvers",
        "authors": [
            "Peter Jeavons",
            "Justyna Petke"
        ],
        "abstract": "Local consistency techniques such as k-consistency are a key  component of specialised solvers for constraint satisfaction problems. In this paper we show that the power of using k-consistency techniques on a constraint satisfaction problem is precisely captured by using a particular inference rule, which we call negative-hyper-resolution, on the standard direct encoding of the problem into Boolean clauses. We also show that current clause-learning SAT-solvers will discover in expected polynomial time any inconsistency that can be deduced from a given set of clauses using negative-hyper-resolvents of a fixed size. We combine these two results to show that, without being explicitly designed to do so, current clause-learning SAT-solvers efficiently simulate k-consistency techniques, for all fixed values of k. We then give some experimental results to show that this feature allows clause-learning SAT-solvers to efficiently solve certain families of constraint problems which are challenging for conventional constraint-programming solvers.\n\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4942",
        "title": "Info-computational constructivism in modelling of life as cognition",
        "authors": [
            "Gordana Dodig-Crnkovic"
        ],
        "abstract": "This paper addresses the open question formulated as: Which levels of abstraction are appropriate in the synthetic modelling of life and cognition? within the framework of info-computational constructivism, treating natural phenomena as computational processes on informational structures. At present we lack the common understanding of the processes of life and cognition in living organisms with the details of co-construction of informational structures and computational processes in embodied, embedded cognizing agents, both living and artifactual ones. Starting with the definition of an agent as an entity capable of acting on its own behalf, as an actor in Hewitt Actor model of computation, even so simple systems as molecules can be modelled as actors exchanging messages (information). We adopt Kauffmans view of a living agent as something that can reproduce and undergoes at least one thermodynamic work cycle. This definition of living agents leads to the Maturana and Varelas identification of life with cognition. Within the info-computational constructive approach to living beings as cognizing agents, from the simplest to the most complex living systems, mechanisms of cognition can be studied in order to construct synthetic model classes of artifactual cognizing agents on different levels of organization.\n    ",
        "submission_date": "2013-11-02T00:00:00",
        "last_modified_date": "2013-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5031",
        "title": "A Scalable Conditional Independence Test for Nonlinear, Non-Gaussian Data",
        "authors": [
            "Joseph D. Ramsey"
        ],
        "abstract": "Many relations of scientific interest are nonlinear, and even in linear systems distributions are often non-Gaussian, for example in fMRI BOLD data. A class of search procedures for causal relations in high dimensional data relies on sample derived conditional independence decisions. The most common applications rely on Gaussian tests that can be systematically erroneous in nonlinear non-Gaussian cases. Recent work (Gretton et al. (2009), Tillman et al. (2009), Zhang et al. (2011)) has proposed conditional independence tests using Reproducing Kernel Hilbert Spaces (RKHS). Among these, perhaps the most efficient has been KCI (Kernel Conditional Independence, Zhang et al. (2011)), with computational requirements that grow effectively at least as O(N3), placing it out of range of large sample size analysis, and restricting its applicability to high dimensional data sets. We propose a class of O(N2) tests using conditional correlation independence (CCI) that require a few seconds on a standard workstation for tests that require tens of minutes to hours for the KCI method, depending on degree of parallelization, with similar accuracy. For accuracy on difficult nonlinear, non-Gaussian data sets, we also compare a recent test due to Harris & Drton (2012), applicable to nonlinear, non-Gaussian distributions in the Gaussian copula, as well as to partial correlation, a linear Gaussian test.\n    ",
        "submission_date": "2014-01-20T00:00:00",
        "last_modified_date": "2014-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5156",
        "title": "Harmony Search Algorithm for Curriculum-Based Course Timetabling Problem",
        "authors": [
            "Juliana Wahid",
            "Naimah Mohd Hussin"
        ],
        "abstract": "In this paper, harmony search algorithm is applied to curriculum-based course timetabling. The implementation, specifically the process of improvisation consists of memory consideration, random consideration and pitch adjustment. In memory consideration, the value of the course number for new solution was selected from all other course number located in the same column of the Harmony Memory. This research used the highest occurrence of the course number to be scheduled in a new harmony. The remaining courses that have not been scheduled by memory consideration will go through random consideration, i.e. will select any feasible location available to be scheduled in the new harmony solution. Each course scheduled out of memory consideration is examined as to whether it should be pitch adjusted with probability of eight procedures. However, the algorithm produced results that were not comparatively better than those previously known as best solution. With proper modification in terms of the approach in this algorithm would make the algorithm perform better on curriculum-based course timetabling.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2014-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5157",
        "title": "Skill Analysis with Time Series Image Data",
        "authors": [
            "Toshiyuki Maeda",
            "Masanori Fujii",
            "Isao Hayashi"
        ],
        "abstract": "We present a skill analysis with time series image data using data mining methods, focused on table tennis. We do not use body model, but use only hi-speed movies, from which time series data are obtained and analyzed using data mining methods such as C4.5 and so on. We identify internal models for technical skills as evaluation skillfulness for the forehand stroke of table tennis, and discuss mono and meta-functional skills for improving skills.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2014-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5334",
        "title": "A Microkernel Architecture for Constraint Programming",
        "authors": [
            "Laurent Michel",
            "Pascal Van Hentenryck"
        ],
        "abstract": "This paper presents a microkernel architecture for constraint programming organized around a number of small number of core functionalities and minimal interfaces. The architecture contrasts with the monolithic nature of many implementations. Experimental results indicate that the software engineering benefits are not incompatible with runtime efficiency.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2014-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5341",
        "title": "Domain Views for Constraint Programming",
        "authors": [
            "Pascal Van Hentenryck",
            "Laurent Michel"
        ],
        "abstract": "Views are a standard abstraction in constraint programming: They make it possible to implement a single version of each constraint, while avoiding to create new variables and constraints that would slow down propagation. Traditional constraint-programming systems provide the concept of {\\em variable views} which implement a view of the type $y = f(x)$ by delegating all (domain and constraint) operations on variable $y$ to variable $x$. This paper proposes the alternative concept of {\\em domain views} which only delegate domain operations. Domain views preserve the benefits of variable views but simplify the implementation of value-based propagation. Domain views also support non-injective views compositionally, expanding the scope of views significantly. Experimental results demonstrate the practical benefits of domain views.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2014-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5424",
        "title": "Real Time Strategy Language",
        "authors": [
            "Roy Hayes",
            "Peter Beling",
            "William Scherer"
        ],
        "abstract": "Real Time Strategy (RTS) games provide complex domain to test the latest artificial intelligence (AI) research. In much of the literature, AI systems have been limited to playing one game. Although, this specialization has resulted in stronger AI gaming systems it does not address the key concerns of AI researcher. AI researchers seek the development of AI agents that can autonomously interpret learn, and apply new knowledge. To achieve human level performance, current AI systems rely on game specific knowledge of an expert. The paper presents the full RTS language in hopes of shifting the current research focus to the development of general RTS agents. General RTS agents are AI gaming systems that can play any RTS games, defined in the RTS language. This prevents game specific knowledge from being hard coded into the system, thereby facilitating research that addresses the fundamental concerns of artificial intelligence.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2014-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5813",
        "title": "GGP with Advanced Reasoning and Board Knowledge Discovery",
        "authors": [
            "Adrian \u0141a\u0144cucki"
        ],
        "abstract": "Quality of General Game Playing (GGP) matches suffers from slow state-switching and weak knowledge modules. Instantiation and Propositional Networks offer great performance gains over Prolog-based reasoning, but do not scale well. In this publication mGDL, a variant of GDL stripped of function constants, has been defined as a basis for simple reasoning machines. mGDL allows to easily map rules to C++ functions. 253 out of 270 tested GDL rule sheets conformed to mGDL without any modifications; the rest required minor changes. A revised (m)GDL to C++ translation scheme has been reevaluated; it brought gains ranging from 28% to 7300% over YAP Prolog, managing to compile even demanding rule sheets under few seconds. For strengthening game knowledge, spatial features inspired by similar successful techniques from computer Go have been proposed. For they required an Euclidean metric, a small board extension to GDL has been defined through a set of ground atomic sentences. An SGA-based genetic algorithm has been designed for tweaking game parameters and conducting self-plays, so the features could be mined from meaningful game records. The approach has been tested on a small cluster, giving performance gains up to 20% more wins against the baseline UCT player. Implementations of proposed ideas constitutes the core of GGP Spatium - a small C++/Python GGP framework, created for developing compact GGP Players and problem solvers.\n    ",
        "submission_date": "2014-01-22T00:00:00",
        "last_modified_date": "2014-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5848",
        "title": "Algorithms and Limits for Compact Plan Representations",
        "authors": [
            "Christer B\u00e4ckstr\u00f6m",
            "Peter Jonsson"
        ],
        "abstract": "Compact representations of objects is a common concept in  computer science.  Automated planning can be viewed as a case of this concept: a planning instance is a compact implicit representation of a graph and the problem is to find a path (a plan) in this graph.  While the graphs themselves are represented compactly as planning instances, the paths are usually represented explicitly as sequences of actions.  Some cases are known where the plans always have compact representations, for example, using macros.  We show that these results do not extend to the general case, by proving a number of bounds for compact representations of plans under various criteria, like efficient sequential or random access of actions.  In addition to this, we show that our results have consequences for what can be gained from reformulating planning into some other problem.  As a contrast to this we also prove a number of positive results, demonstrating restricted cases where plans do have useful compact representations, as well as proving that macro plans have favourable access properties.  Our results are finally discussed in relation to other relevant contexts.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5852",
        "title": "Algorithms for Generating Ordered Solutions for Explicit AND/OR Structures",
        "authors": [
            "Priyankar Ghosh",
            "Amit Sharma",
            "P.P. Chakrabarti",
            "Pallab Dasgupta"
        ],
        "abstract": "We present algorithms for generating alternative solutions for explicit acyclic AND/OR structures in non-decreasing order of cost. The proposed algorithms use a best first search technique and report the solutions using an implicit representation ordered by cost. In this paper, we present two versions of the search algorithm -- (a) an initial version of the best first search algorithm, ASG, which may present one solution more than once while generating the ordered solutions, and (b) another version, LASG, which avoids the construction of the duplicate solutions. The actual solutions can be reconstructed quickly from the implicit compact representation used. We have applied the methods on a few test domains, some of them are synthetic while the others are based on well known problems including the search space of the 5-peg Tower of Hanoi problem, the matrix-chain multiplication problem and the problem of finding secondary structure of RNA. Experimental results show the efficacy of the proposed algorithms over the existing approach. Our proposed algorithms have potential use in various domains ranging from knowledge based frameworks to service composition, where the AND/OR structure is widely used for representing problems.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5853",
        "title": "Reasoning over Ontologies with Hidden Content: The Import-by-Query Approach",
        "authors": [
            "Bernardo Cuenca Grau",
            "Boris Motik"
        ],
        "abstract": "There is currently a growing interest in techniques for hiding parts of the signature of an ontology Kh that is being reused by another ontology Kv. Towards this goal, in this paper we propose the import-by-query framework, which makes the content of Kh accessible through a limited query interface. If Kv reuses the symbols from Kh in a certain restricted way, one can reason over Kv U Kh by accessing only Kv and the query interface. We map out the landscape of the import-by-query problem. In particular, we outline the limitations of our framework and prove that certain restrictions on the expressivity of Kh and the way in which Kv reuses symbols from Kh are strictly necessary to enable reasoning in our setting. We also identify cases in which reasoning is possible and we present suitable import-by-query reasoning algorithms.\n\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5854",
        "title": "Avoiding and Escaping Depressions in Real-Time Heuristic Search",
        "authors": [
            "Carlos Hern\u00e1ndez",
            "Jorge A Baier"
        ],
        "abstract": "Heuristics used for solving hard real-time search problems have regions with depressions.  Such regions are bounded areas of the search space in which the heuristic function is inaccurate compared to the actual cost to reach a solution. Early real-time search algorithms, like LRTA*, easily become trapped in those regions since the heuristic values of their states may need to be updated multiple times, which results in costly solutions. State-of-the-art real-time search algorithms, like LSS-LRTA* or LRTA*(k), improve LRTA*s mechanism to update the heuristic, resulting in improved performance. Those algorithms, however, do not guide search towards avoiding depressed regions. This paper presents depression avoidance, a simple real-time search principle to guide search towards avoiding states that have been marked as part of a heuristic depression. We propose two ways in which depression avoidance can be implemented: mark-and-avoid and move-to-border. We implement these strategies on top of LSS-LRTA* and RTAA*, producing 4 new real-time heuristic search algorithms: aLSS-LRTA*, daLSS-LRTA*, aRTAA*, and daRTAA*. When the objective is to find a single solution by running the real-time search algorithm once, we show that daLSS-LRTA* and daRTAA* outperform their predecessors sometimes by one order of magnitude. Of the four new algorithms, daRTAA* produces the best solutions given a fixed deadline on the average time allowed per planning episode. We prove all our algorithms have good theoretical properties: in finite search spaces, they find a solution if one exists, and converge to an optimal after a number of trials.\n\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5856",
        "title": "Narrative Planning: Compilations to Classical Planning",
        "authors": [
            "Patrik Haslum"
        ],
        "abstract": "A model of story generation recently proposed by Riedl and Young casts it as planning, with the additional condition that story characters behave intentionally. This means that characters have perceivable motivation for the actions they take. I show that this condition can be compiled away (in more ways than one) to produce a classical planning problem that can be solved by an off-the-shelf classical planner, more efficiently than by Riedl and Youngs specialised planner.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5857",
        "title": "COLIN: Planning with Continuous Linear Numeric Change",
        "authors": [
            "Amanda J. Coles",
            "Andrew I. Coles",
            "Maria Fox",
            "Derek Long"
        ],
        "abstract": "In this paper we describe COLIN, a forward-chaining heuristic search planner, capable of reasoning with COntinuous LINear numeric change, in addition to the full temporal semantics of PDDL.  Through this work we make two advances to the state-of-the-art in terms of expressive reasoning capabilities of planners: the handling of continuous linear change, and the handling of duration-dependent effects in combination with duration inequalities, both of which require tightly coupled temporal and numeric reasoning during planning.  COLIN combines FF-style forward chaining search, with the use of a Linear Program (LP) to check the consistency of the interacting temporal and numeric constraints at each state.  The LP is used to compute bounds on the values of variables in each state, reducing the range of actions that need to be considered for application.  In addition, we develop an extension of the Temporal Relaxed Planning Graph heuristic of CRIKEY3, to support reasoning directly with continuous change.  We extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action.  Finally, we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan, once a solution has been found. To support this, we further contribute a selection of extended benchmark domains that include continuous numeric effects.  We present results for COLIN that demonstrate its scalability on a range of benchmarks, and compare to existing state-of-the-art planners. \n\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5858",
        "title": "SAP Speaks PDDL: Exploiting a Software-Engineering Model for Planning in Business Process Management",
        "authors": [
            "Joerg Hoffman",
            "Ingo Weber",
            "Frank Michael Kraft"
        ],
        "abstract": "Planning is concerned with the automated solution of action sequencing problems described in declarative languages giving the action preconditions and effects. One important application area for such technology is the creation of new processes in Business Process Management (BPM), which is essential in an ever more dynamic business environment. A major obstacle for the application of Planning in this area lies in the modeling. Obtaining a suitable model to plan with -- ideally a description in PDDL, the most commonly used planning language -- is often prohibitively complicated and/or costly. Our core observation in this work is that this problem can be ameliorated by leveraging synergies with model-based software development. Our application at SAP, one of the leading vendors of enterprise software, demonstrates that even one-to-one model re-use is possible.\nThe model in question is called Status and Action Management (SAM). It describes the behavior of Business Objects (BO), i.e., large-scale data structures, at a level of abstraction  corresponding to the language of business experts. SAM covers more than 400 kinds of BOs, each of which is described in terms of a set of status variables and how their values are required for, and affected by, processing steps (actions) that are atomic from a business perspective. SAM was developed by SAP as part of a major model-based software engineering effort.  We show herein that one can use this same model for planning, thus obtaining a BPM planning application that incurs no modeling overhead at all.\nWe compile SAM into a variant of PDDL, and adapt an off-the-shelf planner to solve this kind of problem. Thanks to the resulting technology, business experts may create new processes simply by specifying the desired behavior in terms of status variable value changes: effectively, by describing the process in their own language.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5859",
        "title": "Plan-based Policies for Efficient Multiple Battery Load Management",
        "authors": [
            "Maria Fox",
            "Derek Long",
            "Daniele Magazzeni"
        ],
        "abstract": "Efficient use of multiple batteries is a practical problem with wide and growing application. The problem can be cast as a planning problem under uncertainty. We describe the approach we have adopted to modelling and solving this problem, seen as a Markov Decision Problem, building effective policies for battery switching in the face of stochastic load profiles. \nOur solution exploits and adapts several existing techniques: planning for deterministic mixed discrete-continuous problems and Monte Carlo sampling for policy learning. The paper describes the development of planning techniques to allow solution of the non-linear continuous dynamic models capturing the battery behaviours. This approach depends on carefully handled discretisation of the temporal dimension. The construction of policies is performed using a classification approach and this idea offers opportunities for wider exploitation in other problems. The approach and its generality are described in the paper.\nApplication of the approach leads to construction of policies that, in simulation, significantly outperform those that are currently in use and the best published solutions to the battery management problem. We achieve solutions that achieve more than 99% efficiency in simulation compared with the theoretical limit and do so with far fewer battery switches than existing policies. Behaviour of physical batteries does not exactly match the simulated models for many reasons, so to confirm that our theoretical results can lead to real measured improvements in performance we also conduct and report experiments using a physical test system. These results demonstrate that we can obtain 5%-15% improvement in lifetimes in the case of a two battery system.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5860",
        "title": "A New Look at BDDs for Pseudo-Boolean Constraints",
        "authors": [
            "Ignasi Ab\u00edo",
            "Robert Nieuwenhuis",
            "Albert Oliveras",
            "Enric Rodriguez-Carbonell",
            "Valentin Mayer-Eichberger"
        ],
        "abstract": "Pseudo-Boolean constraints are omnipresent in practical applications, and thus a significant effort has been devoted to the development of good SAT encoding techniques for them. Some of these encodings first construct a Binary Decision Diagram (BDD) for the constraint, and then encode the BDD into a propositional formula.  These BDD-based approaches have some important advantages, such as not being dependent on the size of the coefficients, or being able to share the same BDD for representing many constraints.\nWe first focus on the size of the resulting BDDs, which was considered to be an open problem in our research community. We report on previous work where it was proved that there are Pseudo-Boolean constraints for which no polynomial BDD exists. We also give an alternative and simpler proof assuming that NP is different from Co-NP.  More interestingly, here we also show how to overcome the possible exponential blowup of BDDs by phcoefficient decomposition. This allows us to give the first polynomial generalized arc-consistent ROBDD-based encoding for Pseudo-Boolean constraints.\nFinally, we focus on practical issues: we show how to efficiently construct such ROBDDs, how to encode them into SAT with only 2 clauses per node, and present experimental results that confirm that our approach is competitive with other encodings and state-of-the-art Pseudo-Boolean solvers.\n\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5861",
        "title": "Online Speedup Learning for Optimal Planning",
        "authors": [
            "Carmel Domshlak",
            "Erez Karpas",
            "Shaul Markovitch"
        ],
        "abstract": "Domain-independent planning is one of the foundational areas in the field of Artificial Intelligence. A description of a planning task consists of an initial world state, a goal, and a set of actions for modifying the world state. The objective is to find a sequence of actions, that is, a plan, that transforms the initial world state into a goal state. In optimal planning, we are interested in finding not just a plan, but one of the cheapest plans. A prominent approach to optimal planning these days is heuristic\nstate-space search, guided by admissible heuristic functions. Numerous admissible heuristics have been developed, each with its own strengths and weaknesses, and it is well known that there is no single \"best heuristic for optimal planning in general.  Thus, which heuristic to choose for a given planning task is a difficult question. This difficulty can be avoided by combining several heuristics, but that requires computing numerous heuristic estimates at each state, and the tradeoff between the time spent doing so and the time saved by the combined advantages of the different heuristics might be high. We present a novel method that reduces the cost of combining admissible heuristics for optimal planning, while maintaining its benefits.  Using an idealized search space model, we formulate a decision rule for choosing the best heuristic to compute at each state. We then present an active online learning approach for learning a classifier with that decision rule as the target concept, and employ the learned classifier to decide  which heuristic to compute at each state. We evaluate this technique empirically, and show that it substantially outperforms the standard method for combining several heuristics via their pointwise maximum.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5869",
        "title": "An Enhanced Branch-and-bound Algorithm for the Talent Scheduling Problem",
        "authors": [
            "Zizhen Zhang",
            "Hu Qin",
            "Xiaocong Liang",
            "Andrew Lim"
        ],
        "abstract": "The talent scheduling problem is a simplified version of the real-world film shooting problem, which aims to determine a shooting sequence so as to minimize the total cost of the actors involved. In this article, we first formulate the problem as an integer linear programming model. Next, we devise a branch-and-bound algorithm to solve the problem. The branch-and-bound algorithm is enhanced by several accelerating techniques, including preprocessing, dominance rules and caching search states. Extensive experiments over two sets of benchmark instances suggest that our algorithm is superior to the current best exact algorithm. Finally, the impacts of different parameter settings are disclosed by some additional experiments.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6048",
        "title": "Replanning in Domains with Partial Information and Sensing Actions",
        "authors": [
            "Ronen I. Brafman",
            "Guy Shani"
        ],
        "abstract": "Replanning via determinization is a recent, popular approach for online planning in MDPs. In this paper we adapt this idea to classical, non-stochastic domains with partial information and sensing actions, presenting a new planner: SDR (Sample, Determinize, Replan). At each step we generate a  solution plan to a classical planning problem induced by the original problem. We execute this plan as long as it is safe to do so. When this is no longer the case, we replan. The classical planning problem we generate is based on the translation-based approach for conformant planning introduced by Palacios and Geffner. The state of the classical planning problem generated in this approach captures the belief state of the agent in the original problem. Unfortunately, when this method is applied to planning problems with sensing, it yields a non-deterministic planning problem that is typically very large. Our main contribution is the introduction of state sampling techniques for overcoming these two problems. In addition, we introduce a novel, lazy, regression-based method for querying the agents belief state during run-time.  We provide a comprehensive experimental evaluation of the planner, showing that it scales better than the state-of-the-art CLG planner on existing benchmark problems, but also highlighting its weaknesses with new domains.  We also discuss its theoretical guarantees.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6049",
        "title": "Generating Approximate Solutions to the TTP using a Linear Distance Relaxation",
        "authors": [
            "Richard Hoshino",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "In some domestic professional sports leagues, the home stadiums are located in cities connected by a common train line running in one direction. For these instances, we can incorporate this geographical information to determine optimal or nearly-optimal solutions to the n-team Traveling Tournament Problem (TTP), an NP-hard sports scheduling problem whose solution is a double round-robin tournament schedule that minimizes the sum total of distances traveled by all n teams.\nWe introduce the Linear Distance Traveling Tournament Problem (LD-TTP), and solve it for n=4 and n=6, generating the complete set of possible solutions through elementary combinatorial techniques. For larger n, we propose a novel \"expander construction\" that generates an approximate solution to the LD-TTP. For n congruent to 4 modulo 6, we show that our expander construction produces a feasible double round-robin tournament schedule whose total distance is guaranteed to be no worse than 4/3 times the optimal solution, regardless of where the n teams are located. This 4/3-approximation for the LD-TTP is stronger than the currently best-known ratio of 5/3 + epsilon for the general TTP.\nWe conclude the paper by applying this linear distance relaxation to general (non-linear) n-team TTP instances, where we develop fast approximate solutions by simply \"assuming\" the n teams lie on a straight line and solving the modified problem. We show that this technique surprisingly generates the distance-optimal tournament on all benchmark sets on 6 teams, as well as close-to-optimal schedules for larger n, even when the teams are located around a circle or positioned in three-dimensional space.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6098",
        "title": "An adaptive Simulated Annealing-based satellite observation scheduling method combined with a dynamic task clustering strategy",
        "authors": [
            "Guohua Wu",
            "Huilin Wang",
            "Haifeng Li",
            "Witold Pedrycz",
            "Dishan Qiu",
            "Manhao Ma",
            "Jin Liu"
        ],
        "abstract": "Efficient scheduling is of great significance to rationally make use of scarce satellite resources. Task clustering has been demonstrated to realize an effective strategy to improve the efficiency of satellite scheduling. However, the previous task clustering strategy is static. That is, it is integrated into the scheduling in a two-phase manner rather than in a dynamic fashion, without expressing its full potential in improving the satellite scheduling performance. In this study, we present an adaptive Simulated Annealing based scheduling algorithm aggregated with a dynamic task clustering strategy (or ASA-DTC for short) for satellite observation scheduling problems (SOSPs). First, we develop a formal model for the scheduling of Earth observing satellites. Second, we analyze the related constraints involved in the observation task clustering process. Thirdly, we detail an implementation of the dynamic task clustering strategy and the adaptive Simulated Annealing algorithm. The adaptive Simulated Annealing algorithm is efficient, with the endowment of some sophisticated mechanisms, i.e. adaptive temperature control, tabu-list based revisiting avoidance mechanism, and intelligent combination of neighborhood structures. Finally, we report on experimental simulation studies to demonstrate the competitive performance of ASA-DTC. Moreover, we show that ASA-DTC is especially effective when SOSPs contain a large number of targets or these targets are densely distributed in a certain area.\n    ",
        "submission_date": "2014-01-14T00:00:00",
        "last_modified_date": "2014-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6679",
        "title": "Quality of Geographic Information: Ontological approach and Artificial Intelligence Tools",
        "authors": [
            "Robert Jeansoulin",
            "Nic Wilson"
        ],
        "abstract": "The objective is to present one important aspect of the European IST-FET project \"REV!GIS\"1: the methodology which has been developed for the translation (interpretation) of the quality of the data into a \"fitness for use\" information, that we can confront to the user needs in its application. This methodology is based upon the notion of \"ontologies\" as a conceptual framework able to capture the explicit and implicit knowledge involved in the application. We do not address the general problem of formalizing such ontologies, instead, we rather try to illustrate this with three applications which are particular cases of the more general \"data fusion\" problem. In each application, we show how to deploy our methodology, by comparing several possible solutions, and we try to enlighten where are the quality issues, and what kind of solution to privilege, even at the expense of a highly complex computational approach. The expectation of the REV!GIS project is that computationally tractable solutions will be available among the next generation AI tools.\n    ",
        "submission_date": "2014-01-26T00:00:00",
        "last_modified_date": "2014-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6686",
        "title": "Perturbed Message Passing for Constraint Satisfaction Problems",
        "authors": [
            "Siamak Ravanbakhsh",
            "Russell Greiner"
        ],
        "abstract": "We introduce an efficient message passing scheme for solving Constraint Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and directly produce a single satisfying assignment. Our first CSP solver, called Perturbed Blief Propagation, smoothly interpolates two well-known inference procedures; it starts as BP and ends as a Gibbs sampler, which produces a single sample from the set of solutions. Moreover we apply a similar perturbation scheme to SP to produce another CSP solver, Perturbed Survey Propagation. Experimental results on random and real-world CSPs show that Perturbed BP is often more successful and at the same time tens to hundreds of times more efficient than standard BP guided decimation. Perturbed BP also compares favorably with state-of-the-art SP-guided decimation, which has a computational complexity that generally scales exponentially worse than our method (wrt the cardinality of variable domains and constraints). Furthermore, our experiments with random satisfiability and coloring problems demonstrate that Perturbed SP can outperform SP-guided decimation, making it the best incomplete random CSP-solver in difficult regimes.\n    ",
        "submission_date": "2014-01-26T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7249",
        "title": "Fuzzy Controller Design for Assisted Omni-Directional Treadmill Therapy",
        "authors": [
            "Atif Ali Khan",
            "Oumair Naseer",
            "Daciana Iliescu",
            "Evor Hines"
        ],
        "abstract": "One of the defining characteristic of human being is their ability to walk upright. Loss or restriction of such ability whether due to the accident, spine problem, stroke or other neurological injuries can cause tremendous stress on the patients and hence will contribute negatively to their quality of life. Modern research shows that physical exercise is very important for maintaining physical fitness and adopting a healthier life style. In modern days treadmill is widely used for physical exercises and training which enables the user to set up an exercise regime that can be adhered to irrespective of the weather conditions. Among the users of treadmills today are medical facilities such as hospitals, rehabilitation centres, medical and physiotherapy clinics etc. The process of assisted training or doing rehabilitation exercise through treadmill is referred to as treadmill therapy. A modern treadmill is an automated machine having built in functions and predefined features. Most of the treadmills used today are one dimensional and user can only walk in one direction. This paper presents the idea of using omnidirectional treadmills which will be more appealing to the patients as they can walk in any direction, hence encouraging them to do exercises more frequently. This paper proposes a fuzzy control design and possible implementation strategy to assist patients in treadmill therapy. By intelligently controlling the safety belt attached to the treadmill user, one can help them steering left, right or in any direction. The use of intelligent treadmill therapy can help patients to improve their walking ability without being continuously supervised by the specialists. The patients can walk freely within a limited space and the support system will provide continuous evaluation of their position and can adjust the control parameters of treadmill accordingly to provide best possible assistance.\n    ",
        "submission_date": "2014-01-24T00:00:00",
        "last_modified_date": "2014-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7463",
        "title": "Propagators and Violation Functions for Geometric and Workload Constraints Arising in Airspace Sectorisation",
        "authors": [
            "Pierre Flener",
            "Justin Pearson"
        ],
        "abstract": "Airspace sectorisation provides a partition of a given airspace into sectors, subject to geometric constraints and workload constraints, so that some cost metric is minimised. We make a study of the constraints that arise in airspace sectorisation. For each constraint, we give an analysis of what algorithms and properties are required under systematic search and stochastic local search.\n    ",
        "submission_date": "2014-01-29T00:00:00",
        "last_modified_date": "2014-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7941",
        "title": "Exploiting Causality for Selective Belief Filtering in Dynamic Bayesian Networks",
        "authors": [
            "Stefano V. Albrecht",
            "Subramanian Ramamoorthy"
        ],
        "abstract": "Dynamic Bayesian networks (DBNs) are a general model for stochastic processes with partially observed states. Belief filtering in DBNs is the task of inferring the belief state (i.e. the probability distribution over process states) based on incomplete and noisy observations. This can be a hard problem in complex processes with large state spaces. In this article, we explore the idea of accelerating the filtering task by automatically exploiting causality in the process. We consider a specific type of causal relation, called passivity, which pertains to how state variables cause changes in other variables. We present the Passivity-based Selective Belief Filtering (PSBF) method, which maintains a factored belief representation and exploits passivity to perform selective updates over the belief factors. PSBF produces exact belief states under certain assumptions and approximate belief states otherwise, where the approximation error is bounded by the degree of uncertainty in the process. We show empirically, in synthetic processes with varying sizes and degrees of passivity, that PSBF is faster than several alternative methods while achieving competitive accuracy. Furthermore, we demonstrate how passivity occurs naturally in a complex system such as a multi-robot warehouse, and how PSBF can exploit this to accelerate the filtering task.\n    ",
        "submission_date": "2014-01-30T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.8175",
        "title": "Equilibrium Points of an AND-OR Tree: under Constraints on Probability",
        "authors": [
            "Toshio Suzuki",
            "Yoshinao Niida"
        ],
        "abstract": "We study a probability distribution d on the truth assignments to a uniform binary AND-OR tree. Liu and Tanaka [2007, Inform. Process. Lett.] showed the following: If d achieves the equilibrium among independent distributions (ID) then d is an independent identical distribution (IID). We show a stronger form of the above result. Given a real number r such that 0 < r < 1, we consider a constraint that the probability of the root node having the value 0 is r. Our main result is the following: When we restrict ourselves to IDs satisfying this constraint, the above result of Liu and Tanaka still holds. The proof employs clever tricks of induction. In particular, we show two fundamental relationships between expected cost and probability in an IID on an OR-AND tree: (1) The ratio of the cost to the probability (of the root having the value 0) is a decreasing function of the probability x of the leaf. (2) The ratio of derivative of the cost to the derivative of the probability is a decreasing function of x, too.\n    ",
        "submission_date": "2014-01-31T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0557",
        "title": "Optimal Rectangle Packing: An Absolute Placement Approach",
        "authors": [
            "Eric Huang",
            "Richard E. Korf"
        ],
        "abstract": "We consider the problem of finding all enclosing rectangles of minimum area that can contain a given set of rectangles without overlap.  Our rectangle packer chooses the x-coordinates of all the rectangles before any of the y-coordinates. We then transform the problem into a perfect-packing problem with no empty space by adding additional rectangles. To determine the y-coordinates, we branch on the different rectangles that can be placed in each empty position. Our packer allows us to extend the known solutions for a consecutive-square benchmark from 27 to 32 squares. We also introduce three new benchmarks, avoiding properties that make a benchmark easy, such as rectangles with shared dimensions. Our third benchmark consists of rectangles of increasingly high precision. To pack them efficiently, we limit the rectangles coordinates and the bounding box dimensions to the set of subset sums of the rectangles dimensions. Overall, our algorithms represent the current state-of-the-art for this problem, outperforming other algorithms by orders of magnitude, depending on the benchmark.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0558",
        "title": "Parameterized Complexity Results for Exact Bayesian Network Structure Learning",
        "authors": [
            "Sebastian Ordyniak",
            "Stefan Szeider"
        ],
        "abstract": "Bayesian network structure learning is the notoriously difficult problem of discovering a Bayesian network that optimally represents a given set of training data.  In this paper we study the computational worst-case complexity of exact Bayesian network structure learning under graph theoretic restrictions on the (directed) super-structure.  The super-structure is an undirected graph that contains as subgraphs the skeletons of solution networks. We introduce the directed super-structure as a natural generalization of its undirected counterpart. Our results apply to several variants of score-based Bayesian network structure learning where the score of a network decomposes into local scores of its nodes.\nResults: We show that exact Bayesian network structure learning can be carried out in non-uniform polynomial time if the super-structure has bounded treewidth, and in linear time if in addition the super-structure has bounded maximum degree. Furthermore, we show that if the directed super-structure is acyclic, then exact Bayesian network structure learning can be carried out in quadratic time. We complement these positive results with a number of hardness results. We show that both restrictions (treewidth and degree) are essential and cannot be dropped without loosing uniform polynomial time tractability (subject to a complexity-theoretic assumption). Similarly, exact Bayesian network structure learning remains NP-hard for \"almost acyclic\" directed super-structures.  Furthermore, we show that the restrictions remain essential if we do not search for a globally optimal network but aim to improve a given network by means of at most k arc additions, arc deletions, or arc reversals (k-neighborhood local search).\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0559",
        "title": "Short and Long Supports for Constraint Propagation",
        "authors": [
            "Peter Nightingale",
            "Ian Philip Gent",
            "Christopher Jefferson",
            "Ian Miguel"
        ],
        "abstract": "Special-purpose constraint propagation algorithms frequently make implicit use of short supports -- by examining a subset of the variables, they can infer support (a justification that a variable-value pair may still form part of an assignment that satisfies the constraint) for all other variables and values and save substantial work -- but short supports have not been studied in their own right. The two main contributions of this paper are the identification of short supports as important for constraint propagation, and the introduction of HaggisGAC, an efficient and effective general purpose propagation algorithm for exploiting short supports. Given the complexity of HaggisGAC, we present it as an optimised version of a simpler algorithm ShortGAC. Although experiments demonstrate the efficiency of ShortGAC compared with other general-purpose propagation algorithms where a compact set of short supports is available, we show theoretically and experimentally that HaggisGAC is even better. We also find that HaggisGAC performs better than GAC-Schema on full-length supports. We also introduce a variant algorithm HaggisGAC-Stable, which is adapted to avoid work on backtracking and in some cases can be faster and have significant reductions in memory use. All the proposed algorithms are excellent for propagating disjunctions of constraints. In all experiments with disjunctions we found our algorithms to be faster than Constructive Or and GAC-Schema by at least an order of magnitude, and up to three orders of magnitude.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0561",
        "title": "Irrelevant and independent natural extension for sets of desirable gambles",
        "authors": [
            "Gert de Cooman",
            "Enrique Miranda"
        ],
        "abstract": "The results in this paper add useful tools to the theory of sets of desirable gambles, a growing toolbox for reasoning with partial probability assessments. We investigate how to combine a number of marginal coherent sets of desirable gambles into a joint set using the properties of epistemic irrelevance and independence. We provide formulas for the smallest such joint, called their independent natural extension, and study its main properties. The independent natural extension of maximal coherent sets of desirable gambles allows us to define the strong product of sets of desirable gambles. Finally, we explore an easy way to generalise these results to also apply for the conditional versions of epistemic irrelevance and independence.   Having such a set of tools that are easily implemented in computer programs is clearly beneficial to fields, like AI, with a clear interest in coherent reasoning under uncertainty using general and robust uncertainty models that require no full specification.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0564",
        "title": "A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in Planning",
        "authors": [
            "Amanda Jane Coles",
            "Andrew Ian Coles",
            "Maria Fox",
            "Derek Long"
        ],
        "abstract": "Although the use of metric fluents is fundamental to many practical planning problems, the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored. The most widely used heuristic is the relaxation of metric fluents into interval-valued variables --- an idea first proposed a decade ago. Other heuristics depend on domain encodings that supply additional information about fluents, such as capacity constraints or other resource-related annotations. \nA particular challenge to these approaches is in handling interactions between metric fluents that represent exchange, such as the transformation of quantities of raw materials into quantities of processed goods, or trading of money for materials. The usual relaxation of metric fluents is often very poor in these situations, since it does not recognise that resources, once spent, are no longer available to be spent again.\nWe present a heuristic for numeric planning problems building on the propositional relaxed planning graph, but using a mathematical program for numeric reasoning.  We define a class of producer--consumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program (MIP).  This MIP is then combined with a metric Relaxed Planning Graph (RPG) heuristic to produce an integrated hybrid heuristic. The MIP tracks resource use more accurately than the usual relaxation, but relaxes the ordering of actions, while the RPG captures the causal propositional aspects of the problem.  We discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the MIP. We show that encoding a limited subset of the propositional problem to augment the MIP can yield more accurate guidance, partly by exploiting structure such as propositional landmarks and propositional resources.  Our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0565",
        "title": "Lifted Variable Elimination: Decoupling the Operators from the Constraint Language",
        "authors": [
            "Nima Taghipour",
            "Daan Fierens",
            "Jesse Davis",
            "Hendrik Blockeel"
        ],
        "abstract": "Lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. More specifically, they identify groups of interchangeable variables and perform inference once per group, as opposed to once per variable. The groups are defined by means of constraints, so the flexibility of the grouping is determined by the expressivity of the constraint language. Existing approaches for exact lifted inference use specific languages for (in)equality constraints, which often have limited expressivity. In this article, we decouple lifted inference from the constraint language. We define operators for lifted inference in terms of relational algebra operators, so that they operate on the semantic level (the constraints extension) rather than on the syntactic level, making them language-independent. As a result, lifted inference can be performed using more powerful constraint languages, which provide more opportunities for lifting. We empirically demonstrate that this can improve inference efficiency by orders of magnitude, allowing exact inference where until now only approximate inference was feasible.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0566",
        "title": "Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs",
        "authors": [
            "Frans Adriaan Oliehoek",
            "Matthijs T.J. Spaan",
            "Christopher Amato",
            "Shimon Whiteson"
        ],
        "abstract": "This article presents the state-of-the-art in optimal solution methods for decentralized partially observable Markov decision processes (Dec-POMDPs), which are general models for collaborative multiagent planning under uncertainty. Building off the generalized multiagent A* (GMAA*) algorithm, which reduces the problem to a tree of one-shot collaborative Bayesian games (CBGs), we describe several advances that greatly expand the range of Dec-POMDPs that can be solved optimally.  First, we introduce lossless incremental clustering of the CBGs solved by GMAA*, which achieves exponential speedups without sacrificing optimality.  Second, we introduce incremental expansion of nodes in the GMAA* search tree, which avoids the need to expand all children, the number of which is in the worst case doubly exponential in the nodes depth.  This is particularly beneficial when little clustering is possible.  In addition, we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger Dec-POMDPs.  We provide theoretical guarantees that, when a suitable heuristic is used, both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent.  Finally, we present extensive empirical results demonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can optimally solve Dec-POMDPs of unprecedented size.\n\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0568",
        "title": "Boolean Equi-propagation for Concise and Efficient SAT Encodings of Combinatorial Problems",
        "authors": [
            "Amit Metodi",
            "Michael Codish",
            "Peter James Stuckey"
        ],
        "abstract": "We present an approach to propagation-based SAT encoding of combinatorial problems, Boolean equi-propagation, where constraints are modeled as Boolean functions which propagate information about equalities between Boolean literals.  This information is then applied to simplify the CNF encoding of the constraints.  A key factor is that considering only a small fragment of a constraint model at one time enables us to apply stronger, and even complete, reasoning to detect equivalent literals in that fragment. Once detected, equivalences apply to simplify the entire constraint model and facilitate further reasoning on other fragments. Equi-propagation in combination with partial evaluation and constraint simplification provide the foundation for a powerful approach to SAT-based finite domain constraint solving.  We introduce a tool called BEE (Ben-Gurion Equi-propagation Encoder) based on these ideas and demonstrate for a variety of benchmarks that our approach leads to a considerable reduction in the size of CNF encodings and subsequent speed-ups in SAT solving times.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0569",
        "title": "Description Logic Knowledge and Action Bases",
        "authors": [
            "Babak Bagheri Hariri",
            "Diego Calvanese",
            "Marco Montali",
            "Giuseppe De Giacomo",
            "Riccardo De Masellis",
            "Paolo Felli"
        ],
        "abstract": "Description logic Knowledge and Action Bases (KAB) are a mechanism for providing both a semantically rich representation of the information on the domain of interest in terms of a description logic knowledge base and actions to change such information over time, possibly introducing new objects. We resort to a variant of DL-Lite where the unique name assumption is not enforced and where equality between objects may be asserted and inferred. Actions are specified as sets of conditional effects, where conditions are based on epistemic queries over the knowledge base (TBox and ABox), and effects are expressed in terms of new ABoxes. In this setting, we address verification of temporal properties expressed in a variant of first-order mu-calculus with quantification across states. Notably, we show decidability of verification, under a suitable restriction inspired by the notion of weak acyclicity in data exchange.\n\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0571",
        "title": "Analysis of Watson's Strategies for Playing Jeopardy!",
        "authors": [
            "Gerald Tesauro",
            "David C. Gondek",
            "Jonathan Lenchner",
            "James Fan",
            "John M. Prager"
        ],
        "abstract": "Major advances in Question Answering technology were needed for\nIBM Watson to play Jeopardy! at championship level -- the show requires rapid-fire answers to challenging natural language questions, broad general knowledge, high precision, and accurate confidence estimates.  In addition, Jeopardy! features four types of decision making carrying great strategic importance: (1) Daily Double wagering; (2) Final Jeopardy wagering; (3) selecting the next square when in control of the board; (4) deciding whether to attempt to answer, i.e., \"buzz in.\"  Using sophisticated strategies for these decisions, that properly account for the game state and future event probabilities, can significantly boost a players overall chances to win, when compared with simple \"rule of thumb\" strategies.\nThis article presents our approach to developing Watsons game-playing strategies, comprising development of a faithful simulation model, and then using learning and Monte-Carlo methods within the simulator to optimize Watsons strategic decision-making. After giving a detailed description of each of our game-strategy algorithms, we then focus in particular on validating the accuracy of the simulators predictions, and documenting performance improvements using our methods. Quantitative performance benefits are shown with respect to both simple heuristic strategies, and actual human contestant performance in historical episodes.  We further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0573",
        "title": "Identifying the Class of Maxi-Consistent Operators in Argumentation",
        "authors": [
            "Srdjan Vesic"
        ],
        "abstract": "Dung's abstract argumentation theory can be seen as a general framework for non-monotonic reasoning. An important question is then: what is the class of logics that can be subsumed as instantiations of this theory? The goal of this paper is to identify and study the large class of logic-based instantiations of Dung's theory which correspond to the maxi-consistent operator, i.e. to the function which returns maximal consistent subsets of an inconsistent knowledge base. In other words, we study the class of instantiations where very extension of the argumentation system corresponds to exactly one maximal consistent subset of the knowledge base. We show that an attack relation belonging to this class must be conflict-dependent, must not be valid, must not be conflict-complete, must not be symmetric etc. Then, we show that some attack relations serve as lower or upper bounds of the class (e.g. if an attack relation contains canonical undercut then it is not a member of this class). By using our results, we show for all existing attack relations whether or not they belong to this class. We also define new attack relations which are members of this class. Finally, we interpret our results and discuss more general questions, like: what is the added value of argumentation in such a setting? We believe that this work is a first step towards achieving our long-term goal, which is to better understand the role of argumentation and, particularly, the expressivity of logic-based instantiations of Dung-style argumentation frameworks.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0575",
        "title": "Reasoning about Explanations for Negative Query Answers in DL-Lite",
        "authors": [
            "Diego Calvanese",
            "Magdalena Ortiz",
            "Mantas Simkus",
            "Giorgio Stefanoni"
        ],
        "abstract": "In order to meet usability requirements, most logic-based applications provide explanation facilities for reasoning services. This holds also for Description Logics, where research has focused on the explanation of both TBox reasoning and, more recently, query answering. Besides explaining the  presence of a tuple in a query answer, it is important to explain also why a   given tuple is missing. We address the latter problem for instance and  conjunctive query answering over DL-Lite ontologies by adopting abductive  reasoning; that is, we look for additions to the ABox that force a given  tuple to be in the result. As reasoning tasks we consider existence and recognition of an explanation, and relevance and necessity of a given  assertion for an explanation. We characterize the computational complexity of  these problems for arbitrary, subset minimal, and cardinality minimal explanations.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0579",
        "title": "Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk",
        "authors": [
            "Masahiro Ono",
            "Brian C. Williams",
            "L. Blackmore"
        ],
        "abstract": "This paper presents a model-based planner called the Probabilistic Sulu Planner  or the p-Sulu Planner, which controls stochastic systems in a goal directed manner within user-specified risk bounds. The objective of the p-Sulu Planner is to allow users to command continuous, stochastic systems, such as unmanned aerial and space vehicles, in a manner that is both intuitive and safe. To this end, we first develop a new plan representation called a chance-constrained qualitative state plan (CCQSP), through which users can specify the desired evolution of the plant state as well as the acceptable level of risk. An example of a CCQSP statement is go to A through B within 30 minutes, with less than 0.001% probability of failure.\"  We then develop the p-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to enable CCQSP planning, we develop the following two capabilities in this paper: 1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a continuous domain with temporal constraints. The first capability is to ensures that the probability of failure is bounded. The second capability is essential for the planner to solve problems with a continuous state space such as vehicle path planning. We demonstrate the capabilities of the p-Sulu Planner by simulations on two real-world scenarios: the path planning and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo spacecraft. \n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0581",
        "title": "Qualitative Order of Magnitude Energy-Flow-Based Failure Modes and Effects Analysis",
        "authors": [
            "Neal Andrew Snooke",
            "Mark H Lee"
        ],
        "abstract": "This paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow. The modelling is split into two parts.\nPower flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements from the components of a system. The power flow analysis is a platform for higher-level behaviour prediction of energy related aspects using local component behaviour models to capture a state-based representation with a global time. The primary application is Failure Modes and Effects Analysis (FMEA) and a form of exaggeration reasoning is used, combined with an order of magnitude representation to derive the worst case failure modes.  \nThe novel aspects of the work are an order of magnitude(OM) qualitative network analyser to represent any power domain and topology, including multiple power sources, a feature that was not required for earlier specialised electrical versions of the approach. Secondly, the representation of generalised energy related behaviour as state-based local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equation-based ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0582",
        "title": "Scheduling a Dynamic Aircraft Repair Shop with Limited Repair Resources",
        "authors": [
            "Maliheh Aramon Bajestani",
            "J. Christopher Beck"
        ],
        "abstract": "We address a dynamic repair shop scheduling problem in the context of military aircraft fleet management where the goal is to maintain a full complement of aircraft over the long-term. A number of flights, each with a requirement for a specific number and type of aircraft, are already scheduled over a long horizon. We need to assign aircraft to flights and schedule repair activities while considering the flights requirements, repair capacity, and aircraft failures. The number of aircraft awaiting repair dynamically changes over time due to failures and it is therefore necessary to rebuild the repair schedule online. To solve the problem, we view the dynamic repair shop as successive static repair scheduling sub-problems over shorter time periods. We propose a complete approach based on the logic-based Benders decomposition to solve the static sub-problems, and design different rescheduling policies to schedule the dynamic repair shop. Computational experiments demonstrate that the Benders model is able to find and prove optimal solutions on average four times faster than a mixed integer programming model. The rescheduling approach having both aspects of scheduling over a longer horizon and quickly adjusting the schedule increases aircraft available in the long term by 10% compared to the approaches having either one of the aspects alone.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0584",
        "title": "NuMVC: An Efficient Local Search Algorithm for Minimum Vertex Cover",
        "authors": [
            "Shaowei Cai",
            "Kaile Su",
            "Chuan Luo",
            "Abdul Sattar"
        ],
        "abstract": "The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial optimization problem of great importance in both theory and application. Local search has proved successful for this problem. However, there are two main drawbacks in state-of-the-art MVC local search algorithms. First, they select a pair of vertices to exchange simultaneously, which is time-consuming. Secondly, although using edge weighting techniques to diversify the search, these algorithms lack mechanisms for decreasing the weights. To address these issues, we propose two new strategies: two-stage exchange and edge weighting with forgetting. The two-stage exchange strategy selects two vertices to exchange separately and performs the exchange in two stages. The strategy of edge weighting with forgetting not only increases weights of uncovered edges, but also decreases some weights for each edge periodically. These two strategies are used in designing a new MVC local search algorithm, which is referred to as NuMVC.\nWe conduct extensive experimental studies on the standard benchmarks, namely DIMACS and BHOSLIB. The experiment comparing NuMVC with state-of-the-art heuristic algorithms show that NuMVC is at least competitive with the nearest competitor namely PLS on the DIMACS benchmark, and clearly dominates all competitors on the BHOSLIB benchmark. Also, experimental results indicate that NuMVC finds an optimal solution much faster than the current best exact algorithm for Maximum Clique on random instances as well as some structured ones. Moreover, we study the effectiveness of the two strategies and the run-time behaviour through experimental analysis.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0585",
        "title": "AI Methods in Algorithmic Composition: A Comprehensive Survey",
        "authors": [
            "Jose David Fernandez",
            "Francisco Vico"
        ],
        "abstract": "Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, different computational techniques related to Artificial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in Artificial Intelligence.\n\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0587",
        "title": "Asymmetric Distributed Constraint Optimization Problems",
        "authors": [
            "Tal Grinshpoun",
            "Alon Grubshtein",
            "Roie Zivan",
            "Arnon Netzer",
            "Amnon Meisels"
        ],
        "abstract": "Distributed Constraint Optimization (DCOP) is a powerful framework for representing and solving distributed combinatorial problems, where the variables of the problem are owned by different agents. Many multi-agent problems include constraints that produce different gains (or costs) for the participating agents. Asymmetric gains of constrained agents cannot be naturally represented by the standard DCOP model.\nThe present paper proposes a general framework for Asymmetric DCOPs (ADCOPs). In ADCOPs different agents may have different valuations for constraints that they are involved in. The new framework bridges the gap between multi-agent problems which tend to have asymmetric structure and the standard symmetric DCOP model. The benefits of the proposed model over previous attempts to generalize the DCOP model are discussed and evaluated.\nInnovative algorithms that apply to the special properties of the proposed ADCOP model are presented in detail. These include complete algorithms that have a substantial advantage in terms of runtime and network load over existing algorithms (for standard DCOPs) which use alternative representations.  Moreover, standard incomplete algorithms (i.e., local search algorithms) are inapplicable to the existing DCOP representations of asymmetric constraints and when they are applied to the new ADCOP framework they often fail to converge to a local optimum and yield poor results. The local search algorithms proposed in the present paper converge to high quality solutions. The experimental evidence that is presented reveals that the proposed local search algorithms for ADCOPs achieve high quality solutions while preserving a high level of privacy.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0588",
        "title": "A Refined View of Causal Graphs and Component Sizes: SP-Closed Graph Classes and Beyond",
        "authors": [
            "Christer B\u00e4ckstr\u00f6m",
            "Peter Jonsson"
        ],
        "abstract": "The causal graph of a planning instance is an important tool for planning both in practice and in theory. The theoretical studies of causal graphs have largely analysed the computational complexity of planning for instances where the causal graph has a certain structure, often in combination with other parameters like the domain size of the variables. Chen and Gimand#233;nez ignored even the structure and considered only the size of the weakly connected components. They proved that planning is tractable if the components are bounded by a constant and otherwise intractable. Their intractability result was, however, conditioned by an assumption from parameterised complexity theory that has no known useful relationship with the standard complexity classes. We approach the same problem from the perspective of standard complexity classes, and prove that planning is NP-hard for classes with unbounded components under an additional restriction we refer to as SP-closed. We then argue that most NP-hardness theorems for causal graphs are difficult to apply and, thus, prove a more general result; even if the component sizes grow slowly and the class is not densely populated with graphs, planning still cannot be tractable unless the polynomial hierachy collapses. Both these results still hold when restricted to the class of acyclic causal graphs. We finally give a partial characterization of the borderline between NP-hard and NP-intermediate classes, giving further insight into the problem.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0589",
        "title": "Protecting Privacy through Distributed Computation in Multi-agent Decision Making",
        "authors": [
            "Thomas Leaute",
            "Boi Faltings"
        ],
        "abstract": "As large-scale theft of data from corporate servers is becoming increasingly common, it becomes interesting to examine alternatives to the paradigm of centralizing sensitive data into large databases. Instead, one could use cryptography and distributed computation so that sensitive data can be supplied and processed in encrypted form, and only the final result is made known. In this paper, we examine how such a paradigm can be used to implement constraint satisfaction, a technique that can solve a broad class of AI problems such as resource allocation, planning, scheduling, and diagnosis. Most previous work on privacy in constraint satisfaction only attempted to protect specific types of information, in particular the feasibility of particular combinations of decisions. We formalize and extend these restricted notions of privacy by introducing four types of private information, including the feasibility of decisions and the final decisions made, but also the identities of the participants and the topology of the problem. We present distributed algorithms that allow computing solutions to constraint satisfaction problems while maintaining these four types of privacy. We formally prove the privacy properties of these algorithms, and show experiments that compare their respective performance on benchmark problems.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0590",
        "title": "A Survey of Multi-Objective Sequential Decision-Making",
        "authors": [
            "Diederik Marijn Roijers",
            "Peter Vamplew",
            "Shimon Whiteson",
            "Richard Dazeley"
        ],
        "abstract": "Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0591",
        "title": "Learning by Observation of Agent Software Images",
        "authors": [
            "Paulo Roberto Costa",
            "Lu\u00eds Miguel Botelho"
        ],
        "abstract": "Learning by observation can be of key importance whenever agents sharing similar features want to learn from each other. This paper presents an agent architecture that enables software agents to learn by direct observation of the actions executed by expert agents while they are performing a task. This is possible because the proposed architecture displays information that is essential for observation, making it possible for software agents to observe each other. \nThe agent architecture supports a learning process that covers all aspects of learning by observation, such as discovering and observing experts, learning from the observed data, applying the acquired knowledge and evaluating the agents progress. The evaluation provides control over the decision to obtain new knowledge or apply the acquired knowledge to new problems. \nWe combine two methods for learning from the observed information. The first one, the recall method, uses the sequence on which the actions were observed to solve new problems. The second one, the classification method, categorizes the information in the observed data and determines to which set of categories the new problems belong. \nResults show that agents are able to learn in conditions where common supervised learning algorithms fail, such as when agents do not know the results of their actions a priori or when not all the effects of the actions are visible. The results also show that our approach provides better results than other learning methods since it requires shorter learning periods.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1361",
        "title": "Combining finite and continuous solvers",
        "authors": [
            "Jean-Guillaume Fages",
            "Gilles Chabert",
            "Charles Prud'homme"
        ],
        "abstract": "Combining efficiency with reliability within CP systems is one of the main concerns of CP developers. This paper presents a simple and efficient way to connect Choco and Ibex, two CP solvers respectively specialised on finite and continuous domains. This enables to take advantage of the most recent advances of the continuous community within Choco while saving development and maintenance resources, hence ensuring a better software quality.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1500",
        "title": "Co-clustering of Fuzzy Lagged Data",
        "authors": [
            "Eran Shaham",
            "David Sarne",
            "Boaz Ben-Moshe"
        ],
        "abstract": "The paper focuses on mining patterns that are characterized by a fuzzy lagged relationship between the data objects forming them. Such a regulatory mechanism is quite common in real life settings. It appears in a variety of fields: finance, gene expression, neuroscience, crowds and collective movements are but a limited list of examples. Mining such patterns not only helps in understanding the relationship between objects in the domain, but assists in forecasting their future behavior. For most interesting variants of this problem, finding an optimal fuzzy lagged co-cluster is an NP-complete problem. We thus present a polynomial-time Monte-Carlo approximation algorithm for mining fuzzy lagged co-clusters. We prove that for any data matrix, the algorithm mines a fuzzy lagged co-cluster with fixed probability, which encompasses the optimal fuzzy lagged co-cluster by a maximum 2 ratio columns overhead and completely no rows overhead. Moreover, the algorithm handles noise, anti-correlations, missing values and overlapping patterns. The algorithm was extensively evaluated using both artificial and real datasets. The results not only corroborate the ability of the algorithm to efficiently mine relevant and accurate fuzzy lagged co-clusters, but also illustrate the importance of including the fuzziness in the lagged-pattern model.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1956",
        "title": "Revisiting the Learned Clauses Database Reduction Strategies",
        "authors": [
            "Said Jabbour",
            "Jerry Lonlac",
            "Lakhdar Sais",
            "Yakoub Salhi"
        ],
        "abstract": "In this paper, we revisit an important issue of CDCL-based SAT solvers, namely the learned clauses database management policies. Our motivation takes its source from a simple observation on the remarkable performances of both random and size-bounded reduction strategies. We first derive a simple reduction strategy, called Size-Bounded Randomized strategy (in short SBR), that combines maintaing short clauses (of size bounded by k), while deleting randomly clauses of size greater than k. The resulting strategy outperform the state-of-the-art, namely the LBD based one, on SAT instances taken from the last SAT competition. Reinforced by the interest of keeping short clauses, we propose several new dynamic variants, and we discuss their performances.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2014-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1958",
        "title": "Better Optimism By Bayes: Adaptive Planning with Rich Models",
        "authors": [
            "Arthur Guez",
            "David Silver",
            "Peter Dayan"
        ],
        "abstract": "The computational costs of inference and planning have confined Bayesian model-based reinforcement learning to one of two dismal fates: powerful Bayes-adaptive planning but only for simplistic models, or powerful, Bayesian non-parametric models but using simple, myopic planning strategies such as Thompson sampling. We ask whether it is feasible and truly beneficial to combine rich probabilistic models with a closer approximation to fully Bayesian planning. First, we use a collection of counterexamples to show formal problems with the over-optimism inherent in Thompson sampling. Then we leverage state-of-the-art techniques in efficient Bayes-adaptive planning and non-parametric Bayesian methods to perform qualitatively better than both existing conventional algorithms and Thompson sampling on two contextual bandit-like problems.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2014-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1986",
        "title": "Recommandation mobile, sensible au contexte de contenus \u00e9volutifs: Contextuel-E-Greedy",
        "authors": [
            "Djallel Bouneffouf"
        ],
        "abstract": "We introduce in this paper an algorithm named Contextuel-E-Greedy that tackles the dynamicity of the user's content. It is based on dynamic exploration/exploitation tradeoff and can adaptively balance the two aspects by deciding which situation is most relevant for exploration or exploitation. The experimental results demonstrate that our algorithm outperforms surveyed algorithms.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2014-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3490",
        "title": "D numbers theory: a generalization of Dempster-Shafer theory",
        "authors": [
            "Xinyang Deng",
            "Yong Deng"
        ],
        "abstract": "Dempster-Shafer theory is widely applied to uncertainty modelling and knowledge reasoning due to its ability of expressing uncertain information. However, some conditions, such as exclusiveness hypothesis and completeness constraint, limit its development and application to a large extend. To overcome these shortcomings in Dempster-Shafer theory and enhance its capability of representing uncertain information, a novel theory called D numbers theory is systematically proposed in this paper. Within the proposed theory, uncertain information is expressed by D numbers, reasoning and synthesization of information are implemented by D numbers combination rule. The proposed D numbers theory is an generalization of Dempster-Shafer theory, which inherits the advantage of Dempster-Shafer theory and strengthens its capability of uncertainty modelling.\n    ",
        "submission_date": "2014-02-14T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3578",
        "title": "Learning-assisted Theorem Proving with Millions of Lemmas",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban"
        ],
        "abstract": "Large formal mathematical libraries consist of millions of atomic inference steps that give rise to a corresponding number of proved statements (lemmas). Analogously to the informal mathematical practice, only a tiny fraction of such statements is named and re-used in later proofs by formal mathematicians. In this work, we suggest and implement criteria defining the estimated usefulness of the HOL Light lemmas for proving further theorems. We use these criteria to mine the large inference graph of the lemmas in the HOL Light and Flyspeck libraries, adding up to millions of the best lemmas to the pool of statements that can be re-used in later proofs. We show that in combination with learning-based relevance filtering, such methods significantly strengthen automated theorem proving of new conjectures over large formal mathematical libraries such as Flyspeck.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3613",
        "title": "Finding Coordinated Paths for Multiple Holonomic Agents in 2-d Polygonal Environment",
        "authors": [
            "Pavel Janovsk\u00fd",
            "Michal \u010c\u00e1p",
            "Ji\u0159\u00ed Vok\u0159\u00ednek"
        ],
        "abstract": "Avoiding collisions is one of the vital tasks for systems of autonomous mobile agents. We focus on the problem of finding continuous coordinated paths for multiple mobile disc agents in a 2-d environment with polygonal obstacles. The problem is PSPACE-hard, with the state space growing exponentially in the number of agents. Therefore, the state of the art methods include mainly reactive techniques and sampling-based iterative algorithms.\n",
        "submission_date": "2014-02-14T00:00:00",
        "last_modified_date": "2014-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3664",
        "title": "Parameter estimation based on interval-valued belief structures",
        "authors": [
            "Xinyang Deng",
            "Yong Hu",
            "Felix Chan",
            "Sankaran Mahadevan",
            "Yong Deng"
        ],
        "abstract": "Parameter estimation based on uncertain data represented as belief structures is one of the latest problems in the Dempster-Shafer theory. In this paper, a novel method is proposed for the parameter estimation in the case where belief structures are uncertain and represented as interval-valued belief structures. Within our proposed method, the maximization of likelihood criterion and minimization of estimated parameter's uncertainty are taken into consideration simultaneously. As an illustration, the proposed method is employed to estimate parameters for deterministic and uncertain belief structures, which demonstrates its effectiveness and versatility.\n    ",
        "submission_date": "2014-02-15T00:00:00",
        "last_modified_date": "2014-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4157",
        "title": "Conservative collision prediction and avoidance for stochastic trajectories in continuous time and space",
        "authors": [
            "Jan-Peter Calliess",
            "Michael Osborne",
            "Stephen Roberts"
        ],
        "abstract": "Existing work in multi-agent collision prediction and avoidance typically assumes discrete-time trajectories with Gaussian uncertainty or that are completely deterministic. We propose an approach that allows detection of collisions even between continuous, stochastic trajectories with the only restriction that means and variances can be computed. To this end, we employ probabilistic bounds to derive criterion functions whose negative sign provably is indicative of probable collisions. For criterion functions that are Lipschitz, an algorithm is provided to rapidly find negative values or prove their absence. We propose an iterative policy-search approach that avoids prior discretisations and yields collision-free trajectories with adjustably high certainty. We test our method with both fixed-priority and auction-based protocols for coordinating the iterative planning process. Results are provided in collision-avoidance simulations of feedback controlled plants.\n    ",
        "submission_date": "2014-02-17T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4413",
        "title": "Towards Ultra Rapid Restarts",
        "authors": [
            "Shai Haim",
            "Marijn Heule"
        ],
        "abstract": "We observe a trend regarding restart strategies used in SAT solvers. A few years ago, most state-of-the-art solvers restarted on average after a few thousands of backtracks. Currently, restarting after a dozen backtracks results in much better performance. The main reason for this trend is that heuristics and data structures have become more restart-friendly. We expect further continuation of this trend, so future SAT solvers will restart even more rapidly. Additionally, we present experimental results to support our observations.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4525",
        "title": "Off-Policy General Value Functions to Represent Dynamic Role Assignments in RoboCup 3D Soccer Simulation",
        "authors": [
            "Saminda Abeyruwan",
            "Andreas Seekircher",
            "Ubbo Visser"
        ],
        "abstract": "Collecting and maintaining accurate world knowledge in a dynamic, complex, adversarial, and stochastic environment such as the RoboCup 3D Soccer Simulation is a challenging task. Knowledge should be learned in real-time with time constraints. We use recently introduced Off-Policy Gradient Descent algorithms within Reinforcement Learning that illustrate learnable knowledge representations for dynamic role assignments. The results show that the agents have learned competitive policies against the top teams from the RoboCup 2012 competitions for three vs three, five vs five, and seven vs seven agents. We have explicitly used subsets of agents to identify the dynamics and the semantics for which the agents learn to maximize their performance measures, and to gather knowledge about different objectives, so that all agents participate effectively and efficiently within the group.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4914",
        "title": "Building fast Bayesian computing machines out of intentionally stochastic, digital parts",
        "authors": [
            "Vikash Mansinghka",
            "Eric Jonas"
        ],
        "abstract": "The brain interprets ambiguous sensory information faster and more reliably than modern computers, using neurons that are slower and less reliable than logic gates. But Bayesian inference, which underpins many computational models of perception and cognition, appears computationally challenging even given modern transistor speeds and energy budgets. The computational principles and structures needed to narrow this gap are unknown. Here we show how to build fast Bayesian computing machines using intentionally stochastic, digital parts, narrowing this efficiency gap by multiple orders of magnitude. We find that by connecting stochastic digital components according to simple mathematical rules, one can build massively parallel, low precision circuits that solve Bayesian inference problems and are compatible with the Poisson firing statistics of cortical neurons. We evaluate circuits for depth and motion perception, perceptual learning and causal reasoning, each performing inference over 10,000+ latent variables in real time - a 1,000x speed advantage over commodity microprocessors. These results suggest a new role for randomness in the engineering and reverse-engineering of intelligent computation.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5034",
        "title": "Using the Crowd to Generate Content for Scenario-Based Serious-Games",
        "authors": [
            "Sigal Sina",
            "Sarit Kraus",
            "Avi Rosenfeld"
        ],
        "abstract": "In the last decade, scenario-based serious-games have become a main tool for learning new skills and capabilities. An important factor in the development of such systems is the overhead in time, cost and human resources to manually create the content for these scenarios. We focus on how to create content for scenarios in medical, military, commerce and gaming applications where maintaining the integrity and coherence of the content is integral for the system's success. To do so, we present an automatic method for generating content about everyday activities through combining computer science techniques with the crowd. We use the crowd in three basic ways: to capture a database of scenarios of everyday activities, to generate a database of likely replacements for specific events within that scenario, and to evaluate the resulting scenarios. We found that the generated scenarios were rated as reliable and consistent by the crowd when compared to the scenarios that were originally captured. We also compared the generated scenarios to those created by traditional planning techniques. We found that both methods were equally effective in generated reliable and consistent scenarios, yet the main advantages of our approach is that the content we generate is more varied and much easier to create. We have begun integrating this approach within a scenario-based training application for novice investigators within the law enforcement departments to improve their questioning skills.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5037",
        "title": "Assessing the Reach and Impact of Game-Based Learning Approaches to Cultural Competency and Behavioural Change",
        "authors": [
            "Ian Dunwell",
            "Panagiotis Petridis",
            "Petros Lameras",
            "Maurice Hendrix",
            "Stella Doukianou",
            "Mark Gaved"
        ],
        "abstract": "As digital games continue to be explored as solutions to educational and behavioural challenges, the need for evaluation methodologies which support both the unique nature of the format and the need for comparison with other approaches continues to increase. In this workshop paper, a range of challenges are described related specifically to the case of cultural learning using digital games, in terms of how it may best be assessed, understood, and sustained through an iterative process supported by research. An evaluation framework is proposed, identifying metrics for reach and impact and their associated challenges, as well as presenting ethical considerations and the means to utilize evaluation outcomes within an iterative cycle, and to provide feedback to learners. Presenting as a case study a serious game from the Mobile Assistance for Social Inclusion and Empowerment of Immigrants with Persuasive Learning Technologies and Social Networks (MASELTOV) project, the use of the framework in the context of an integrative project is discussed, with emphasis on the need to view game-based learning as a blended component of the cultural learning process, rather than a standalone solution. The particular case of mobile gaming is also considered within this case study, providing a platform by which to deliver and update content in response to evaluation outcomes. Discussion reflects upon the general challenges related to the assessment of cultural learning, and behavioural change in more general terms, suggesting future work should address the need to provide sustainable, research-driven platforms for game-based learning content.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5039",
        "title": "Interpreting social cues to generate credible affective reactions of virtual job interviewers",
        "authors": [
            "Haza\u007fel Jones",
            "Nicolas Sabouret",
            "Ionut Damian",
            "Tobias Baur",
            "Elisabeth Andr\u00e9",
            "Ka\u015bka Porayska-Pomsta",
            "Paola Rizzo"
        ],
        "abstract": "In this paper we describe a mechanism of generating credible affective reactions in a virtual recruiter during an interaction with a user. This is done using communicative performance computation based on the behaviours of the user as detected by a recognition module. The proposed software pipeline is part of the TARDIS system which aims to aid young job seekers in acquiring job interview related social skills. In this context, our system enables the virtual recruiter to realistically adapt and react to the user in real-time.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5043",
        "title": "A logical model of Theory of Mind for virtual agents in the context of job interview simulation",
        "authors": [
            "Marwen Belkaid",
            "Nicolas Sabouret"
        ],
        "abstract": "Job interview simulation with a virtual agents aims at improving people's social skills and supporting professional inclusion. In such simulators, the virtual agent must be capable of representing and reasoning about the user's mental state based on social cues that inform the system about his/her affects and social attitude. In this paper, we propose a formal model of Theory of Mind (ToM) for virtual agent in the context of human-agent interaction that focuses on the affective dimension. It relies on a hybrid ToM that combines the two major paradigms of the domain. Our framework is based on modal logic and inference rules about the mental states, emotions and social relations of both actors. Finally, we present preliminary results regarding the impact of such a model on natural interaction in the context of job interviews simulation.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5161",
        "title": "Statistical Constraints",
        "authors": [
            "Roberto Rossi",
            "Steven Prestwich",
            "S. Armagan Tarim"
        ],
        "abstract": "We introduce statistical constraints, a declarative modelling tool that links statistics and constraint programming. We discuss two statistical constraints and some associated filtering algorithms. Finally, we illustrate applications to standard problems encountered in statistics and to a novel inspection scheduling problem in which the aim is to find inspection plans with desirable statistical properties.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5358",
        "title": "Extended Breadth-First Search Algorithm",
        "authors": [
            "Tam\u00e1s K\u00e1dek",
            "J\u00e1nos P\u00e1novics"
        ],
        "abstract": "The task of artificial intelligence is to provide representation techniques for describing problems, as well as search algorithms that can be used to answer our questions. A widespread and elaborated model is state-space representation, which, however, has some shortcomings. Classical search algorithms are not applicable in practice when the state space contains even only a few tens of thousands of states. We can give remedy to this problem by defining some kind of heuristic knowledge. In case of classical state-space representation, heuristic must be defined so that it qualifies an arbitrary state based on its \"goodness,\" which is obviously not trivial. In our paper, we introduce an algorithm that gives us the ability to handle huge state spaces and to use a heuristic concept which is easier to embed into search algorithms.\n    ",
        "submission_date": "2014-02-21T00:00:00",
        "last_modified_date": "2014-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5379",
        "title": "What Is It Like to Be a Brain Simulation?",
        "authors": [
            "Eray \u00d6zkural"
        ],
        "abstract": "We frame the question of what kind of subjective experience a brain simulation would have in contrast to a biological brain. We discuss the brain prosthesis thought experiment. We evaluate how the experience of the brain simulation might differ from the biological, according to a number of hypotheses about experience and the properties of simulation. Then, we identify finer questions relating to the original inquiry, and answer them from both a general physicalist, and panexperientialist perspective.\n    ",
        "submission_date": "2014-02-01T00:00:00",
        "last_modified_date": "2014-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5380",
        "title": "Godseed: Benevolent or Malevolent?",
        "authors": [
            "Eray \u00d6zkural"
        ],
        "abstract": "It is hypothesized by some thinkers that benign looking AI objectives may result in powerful AI drives that may pose an existential risk to human society. We analyze this scenario and find the underlying assumptions to be unlikely. We examine the alternative scenario of what happens when universal goals that are not human-centric are used for designing AI agents. We follow a design approach that tries to exclude malevolent motivations from AI agents, however, we see that objectives that seem benevolent may pose significant risk. We consider the following meta-rules: preserve and pervade life and culture, maximize the number of free minds, maximize intelligence, maximize wisdom, maximize energy production, behave like human, seek pleasure, accelerate evolution, survive, maximize control, and maximize capital. We also discuss various solution approaches for benevolent behavior including selfless goals, hybrid designs, Darwinism, universal constraints, semi-autonomy, and generalization of robot laws. A \"prime directive\" for AI may help in formulating an encompassing constraint for avoiding malicious behavior. We hypothesize that social instincts for autonomous robots may be effective such as attachment learning. We mention multiple beneficial scenarios for an advanced semi-autonomous AGI agent in the near future including space exploration, automation of industries, state functions, and cities. We conclude that a beneficial AI agent with intelligence beyond human-level is possible and has many practical use cases.\n    ",
        "submission_date": "2014-02-01T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5436",
        "title": "Characterizing and computing stable models of logic programs: The non-stratified case",
        "authors": [
            "Gianpaolo Brignoli",
            "Stefania Costantini",
            "Ottavio D'Antona",
            "Alessandro Provetti"
        ],
        "abstract": "Stable Logic Programming (SLP) is an emergent, alternative style of logic programming: each solution to a problem is represented by a stable model of a deductive database/function-free logic program encoding the problem itself. Several implementations now exist for stable logic programming, and their performance is rapidly improving. To make SLP generally applicable, it should be possible to check for consistency (i.e., existence of stable models) of the input program before attempting to answer queries. In the literature, only rather strong sufficient conditions have been proposed for consistency, e.g., stratification. This paper extends these results in several directions. First, the syntactic features of programs, viz. cyclic negative dependencies, affecting the existence of stable models are characterized, and their relevance is discussed. Next, a new graph representation of logic programs, the Extended Dependency Graph (EDG), is introduced, which conveys enough information for reasoning about stable models (while the traditional Dependency Graph does not). Finally, we show that the problem of the existence of stable models can be reformulated in terms of coloring of the EDG.\n    ",
        "submission_date": "2014-02-21T00:00:00",
        "last_modified_date": "2014-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5458",
        "title": "Information Aggregation in Exponential Family Markets",
        "authors": [
            "Jacob Abernethy",
            "Sindhu Kutty",
            "S\u00e9bastien Lahaie",
            "Rahul Sami"
        ],
        "abstract": "We consider the design of prediction market mechanisms known as automated market makers. We show that we can design these mechanisms via the mold of \\emph{exponential family distributions}, a popular and well-studied probability distribution template used in statistics. We give a full development of this relationship and explore a range of benefits. We draw connections between the information aggregation of market prices and the belief aggregation of learning agents that rely on exponential family distributions. We develop a very natural analysis of the market behavior as well as the price equilibrium under the assumption that the traders exhibit risk aversion according to exponential utility. We also consider similar aspects under alternative models, such as when traders are budget constrained.\n    ",
        "submission_date": "2014-02-22T00:00:00",
        "last_modified_date": "2014-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5593",
        "title": "Reciprocity in Gift-Exchange-Games",
        "authors": [
            "Rustam Tagiew",
            "Dmitry I. Ignatov"
        ],
        "abstract": "This paper presents an analysis of data from a gift-exchange-game experiment. The experiment was described in `The Impact of Social Comparisons on Reciprocity' by G\u00e4chter et al. 2012. Since this paper uses state-of-art data science techniques, the results provide a different point of view on the problem. As already shown in relevant literature from experimental economics, human decisions deviate from rational payoff maximization. The average gift rate was $31$%. Gift rate was under no conditions zero. Further, we derive some special findings and calculate their significance.\n    ",
        "submission_date": "2014-02-23T00:00:00",
        "last_modified_date": "2014-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5684",
        "title": "Discriminative Functional Connectivity Measures for Brain Decoding",
        "authors": [
            "Orhan Firat",
            "Mete Ozay",
            "Ilke Oztekin",
            "Fatos T. Yarman Vural"
        ],
        "abstract": "We propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning method, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using a functional neighbourhood concept. In order to define the functional neighbourhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighbouring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Functional Connectivity aware Local Relational Features (FC-LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn) and Support Vector Machine (SVM), are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The classification performance of the Functional Mesh Learning model, which range in 62%-71% is superior to the classical multi-voxel pattern analysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.\n    ",
        "submission_date": "2014-02-23T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6028",
        "title": "Algorithms for multi-armed bandit problems",
        "authors": [
            "Volodymyr Kuleshov",
            "Doina Precup"
        ],
        "abstract": "Although many algorithms for the multi-armed bandit problem are well-understood theoretically, empirical confirmation of their effectiveness is generally scarce. This paper presents a thorough empirical study of the most popular multi-armed bandit algorithms. Three important observations can be made from our results. Firstly, simple heuristics such as epsilon-greedy and Boltzmann exploration outperform theoretically sound algorithms on most settings by a significant margin. Secondly, the performance of most algorithms varies dramatically with the parameters of the bandit problem. Our study identifies for each algorithm the settings where it performs well, and the settings where it performs poorly. Thirdly, the algorithms' performance relative each to other is affected only by the number of bandit arms and the variance of the rewards. This finding may guide the design of subsequent empirical evaluations. In the second part of the paper, we turn our attention to an important area of application of bandit algorithms: clinical trials. Although the design of clinical trials has been one of the principal practical problems motivating research on multi-armed bandits, bandit algorithms have never been evaluated as potential treatment allocation strategies. Using data from a real study, we simulate the outcome that a 2001-2002 clinical trial would have had if bandit algorithms had been used to allocate patients to treatments. We find that an adaptive trial would have successfully treated at least 50% more patients, while significantly reducing the number of adverse effects and increasing patient retention. At the end of the trial, the best treatment could have still been identified with a high level of statistical confidence. Our findings demonstrate that bandit algorithms are attractive alternatives to current adaptive treatment allocation strategies.\n    ",
        "submission_date": "2014-02-25T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6560",
        "title": "Even more generic solution construction in Valuation-Based Systems",
        "authors": [
            "Jordi Roca-Lacostena",
            "Jesus Cerquides"
        ],
        "abstract": "Valuation algebras abstract a large number of formalisms for automated reasoning and enable the definition of generic inference procedures. Many of these formalisms provide some notions of solutions. Typical examples are satisfying assignments in constraint systems, models in logics or solutions to linear equation systems.\n",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6663",
        "title": "Enaction-Based Artificial Intelligence: Toward Coevolution with Humans in the Loop",
        "authors": [
            "Pierre De Loor",
            "Kristen Manach",
            "Jacques Tisseau"
        ],
        "abstract": "This article deals with the links between the enaction paradigm and artificial intelligence. Enaction is considered a metaphor for artificial intelligence, as a number of the notions which it deals with are deemed incompatible with the phenomenal field of the virtual. After explaining this stance, we shall review previous works regarding this issue in terms of artifical life and robotics. We shall focus on the lack of recognition of co-evolution at the heart of these approaches. We propose to explicitly integrate the evolution of the environment into our approach in order to refine the ontogenesis of the artificial system, and to compare it with the enaction paradigm. The growing complexity of the ontogenetic mechanisms to be activated can therefore be compensated by an interactive guidance system emanating from the environment. This proposition does not however resolve that of the relevance of the meaning created by the machine (sense-making). Such reflections lead us to integrate human interaction into this environment in order to construct relevant meaning in terms of participative artificial intelligence. This raises a number of questions with regards to setting up an enactive interaction. The article concludes by exploring a number of issues, thereby enabling us to associate current approaches with the principles of morphogenesis, guidance, the phenomenology of interactions and the use of minimal enactive interfaces in setting up experiments which will deal with the problem of artificial intelligence in a variety of enaction-based ways.\n    ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.7276",
        "title": "Robot Location Estimation in the Situation Calculus",
        "authors": [
            "Vaishak Belle",
            "Hector Levesque"
        ],
        "abstract": "Location estimation is a fundamental sensing task in robotic applications, where the world is uncertain, and sensors and effectors are noisy. Most systems make various assumptions about the dependencies between state variables, and especially about how these dependencies change as a result of actions. Building on a general framework by Bacchus, Halpern and Levesque for reasoning about degrees of belief in the situation calculus, and a recent extension to it for continuous domains, in this paper we illustrate location estimation in the presence of a rich theory of actions using an example. We also show that while actions might affect prior distributions in nonstandard ways, suitable posterior beliefs are nonetheless entailed as a side-effect of the overall specification.\n    ",
        "submission_date": "2014-02-28T00:00:00",
        "last_modified_date": "2014-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0034",
        "title": "Tractable Epistemic Reasoning with Functional Fluents, Static Causal Laws and Postdiction",
        "authors": [
            "Manfred Eppe"
        ],
        "abstract": "We present an epistemic action theory for tractable epistemic reasoning as an extension to the h-approximation (HPX) theory. In contrast to existing tractable approaches, the theory supports functional fluents and postdictive reasoning with static causal laws. We argue that this combination is particularly synergistic because it allows one not only to perform direct postdiction about the conditions of actions, but also indirect postdiction about the conditions of static causal laws. We show that despite the richer expressiveness, the temporal projection problem remains tractable (polynomial), and therefore the planning problem remains in NP. We present the operational semantics of our theory as well as its formulation as Answer Set Programming.\n    ",
        "submission_date": "2014-03-01T00:00:00",
        "last_modified_date": "2020-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0036",
        "title": "Dynamic Decision Process Modeling and Relation-line Handling in Distributed Cooperative Modeling System",
        "authors": [
            "Menghan Wang"
        ],
        "abstract": "The Distributed Cooperative Modeling System (DCMS) solves complex decision problems involving a lot of participants with different viewpoints by network based distributed modeling and multi-template aggregation.\n",
        "submission_date": "2014-03-01T00:00:00",
        "last_modified_date": "2014-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0504",
        "title": "A Compilation Target for Probabilistic Programming Languages",
        "authors": [
            "Brooks Paige",
            "Frank Wood"
        ],
        "abstract": "Forward inference techniques such as sequential Monte Carlo and particle Markov chain Monte Carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes, forking, mutexes, and shared memory. Exploiting this we have defined, developed, and tested a probabilistic programming language intermediate representation language we call probabilistic C, which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient, scalable, portable probabilistic programming compilation target. This opens up a new hardware and systems research path for optimizing probabilistic programming systems.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0522",
        "title": "Expert System Based On Neural-Fuzzy Rules for Thyroid Diseases Diagnosis",
        "authors": [
            "Ahmad Taher Azar",
            "Aboul Ella Hassanien"
        ],
        "abstract": "The thyroid, an endocrine gland that secretes hormones in the blood, circulates its products to all tissues of the body, where they control vital functions in every cell. Normal levels of thyroid hormone help the brain, heart, intestines, muscles and reproductive system function normally. Thyroid hormones control the metabolism of the body. Abnormalities of thyroid function are usually related to production of too little thyroid hormone (hypothyroidism) or production of too much thyroid hormone (hyperthyroidism). Therefore, the correct diagnosis of these diseases is very important topic. In this study, Linguistic Hedges Neural-Fuzzy Classifier with Selected Features (LHNFCSF) is presented for diagnosis of thyroid diseases. The performance evaluation of this system is estimated by using classification accuracy and k-fold cross-validation. The results indicated that the classification accuracy without feature selection was 98.6047% and 97.6744% during training and testing phases, respectively with RMSE of 0.02335. After applying feature selection algorithm, LHNFCSF achieved 100% for all cluster sizes during training phase. However, in the testing phase LHNFCSF achieved 88.3721% using one cluster for each class, 90.6977% using two clusters, 91.8605% using three clusters and 97.6744% using four clusters for each class and 12 fuzzy rules. The obtained classification accuracy was very promising with regard to the other classification applications in literature for this problem.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0541",
        "title": "Representing, reasoning and answering questions about biological pathways - various applications",
        "authors": [
            "Saadat Anwar"
        ],
        "abstract": "Biological organisms are composed of numerous interconnected biochemical processes. Diseases occur when normal functionality of these processes is disrupted. Thus, understanding these biochemical processes and their interrelationships is a primary task in biomedical research and a prerequisite for diagnosing diseases, and drug development. Scientists studying these processes have identified various pathways responsible for drug metabolism, and signal transduction, etc.\n",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0613",
        "title": "On Redundant Topological Constraints",
        "authors": [
            "Sanjiang Li",
            "Zhiguo Long",
            "Weiming Liu",
            "Matt Duckham",
            "Alan Both"
        ],
        "abstract": "The Region Connection Calculus (RCC) is a well-known calculus for representing part-whole and topological relations. It plays an important role in qualitative spatial reasoning, geographical information science, and ontology. The computational complexity of reasoning with RCC5 and RCC8 (two fragments of RCC) as well as other qualitative spatial/temporal calculi has been investigated in depth in the literature. Most of these works focus on the consistency of qualitative constraint networks. In this paper, we consider the important problem of redundant qualitative constraints. For a set $\\Gamma$ of qualitative constraints, we say a constraint $(x R y)$ in $\\Gamma$ is redundant if it is entailed by the rest of $\\Gamma$. A prime subnetwork of $\\Gamma$ is a subset of $\\Gamma$ which contains no redundant constraints and has the same solution set as $\\Gamma$. It is natural to ask how to compute such a prime subnetwork, and when it is unique.\n",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0764",
        "title": "Clustering Concept Chains from Ordered Data without Path Descriptions",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper describes a process for clustering concepts into chains from data presented randomly to an evaluating system. There are a number of rules or guidelines that help the system to determine more accurately what concepts belong to a particular chain and what ones do not, but it should be possible to write these in a generic way. This mechanism also uses a flat structure without any hierarchical path information, where the link between two concepts is made at the level of the concept itself. It does not require related metadata, but instead, a simple counting mechanism is used. Key to this is a count for both the concept itself and also the group or chain that it belongs to. To test the possible success of the mechanism, concept chain parts taken randomly from a larger ontology were presented to the system, but only at a depth of 2 concepts each time. That is - root concept plus a concept that it is linked to. The results show that this can still lead to very variable structures being formed and can also accommodate some level of randomness.\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0778",
        "title": "Dynamic Move Chains -- a Forward Pruning Approach to Tree Search in Computer Chess",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper proposes a new mechanism for pruning a search game-tree in computer chess. The algorithm stores and then reuses chains or sequences of moves, built up from previous searches. These move sequences have a built-in forward-pruning mechanism that can radically reduce the search space. A typical search process might retrieve a move from a Transposition Table, where the decision of what move to retrieve would be based on the position itself. This algorithm stores move sequences based on what previous sequences were better, or caused cutoffs. This is therefore position independent and so it could also be useful in games with imperfect information or uncertainty, where the whole situation is not known at any one time. Over a small set of tests, the algorithm was shown to clearly out-perform Transposition Tables, both in terms of search reduction and game-play results.\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1076",
        "title": "Is Intelligence Artificial?",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "Our understanding of intelligence is directed primarily at the human level. This paper attempts to give a more unifying definition that can be applied to the natural world in general and then Artificial Intelligence. The definition would be used more to qualify than quantify it and might help when making judgements on the matter. While correct behaviour is the preferred definition, a metric that is grounded in Kolmogorov's Complexity Theory is suggested, which leads to a measurement about entropy. A version of an accepted AI test is then put forward as the 'acid test' and might be what a free-thinking program would try to achieve. Recent work by the author has been more from a direction of mechanical processes, or ones that might operate automatically. This paper agrees that intelligence is a pro-active event, but also notes a second aspect to it that is in the background and mechanical. The paper suggests looking at intelligence and the conscious as being slightly different, where the conscious is this more mechanical aspect. In fact, a surprising conclusion can be a passive but intelligent brain being invoked by active and less intelligent senses.\n    ",
        "submission_date": "2014-03-05T00:00:00",
        "last_modified_date": "2024-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1080",
        "title": "New Ideas for Brain Modelling",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper describes some biologically-inspired processes that could be used to build the sort of networks that we associate with the human brain. New to this paper, a 'refined' neuron will be proposed. This is a group of neurons that by joining together can produce a more analogue system, but with the same level of control and reliability that a binary neuron would have. With this new structure, it will be possible to think of an essentially binary system in terms of a more variable set of values. The paper also shows how recent research associated with the new model, can be combined with established theories, to produce a more complete picture. The propositions are largely in line with conventional thinking, but possibly with one or two more radical suggestions. An earlier cognitive model can be filled in with more specific details, based on the new research results, where the components appear to fit together almost seamlessly. The intention of the research has been to describe plausible 'mechanical' processes that can produce the appropriate brain structures and mechanisms, but that could be used without the magical 'intelligence' part that is still not fully understood. There are also some important updates from an earlier version of this paper.\n    ",
        "submission_date": "2014-03-05T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1169",
        "title": "A proof challenge: multiple alignment and information compression",
        "authors": [
            "J Gerard Wolff"
        ],
        "abstract": "These notes pose a \"proof challenge\": a proof, or disproof, of the proposition that \"For any given body of information, I, expressed as a one-dimensional sequence of atomic symbols, a multiple alignment concept, described in the document, provides a means of encoding all the redundancy that may exist in I. Aspects of the challenge are described.\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1497",
        "title": "Active Learning for Autonomous Intelligent Agents: Exploration, Curiosity, and Interaction",
        "authors": [
            "Manuel Lopes",
            "Luis Montesano"
        ],
        "abstract": "In this survey we present different approaches that allow an intelligent agent to explore autonomous its environment to gather information and learn multiple tasks. Different communities proposed different solutions, that are in many cases, similar and/or complementary. These solutions include active learning, exploration/exploitation, online-learning and social learning. The common aspect of all these approaches is that it is the agent to selects and decides what information to gather next. Applications for these approaches already include tutoring systems, autonomous grasping learning, navigation and mapping and human-robot interaction. We discuss how these approaches are related, explaining their similarities and their differences in terms of problem assumptions and metrics of success. We consider that such an integrated discussion will improve inter-disciplinary research and applications.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1521",
        "title": "Approximation Models of Combat in StarCraft 2",
        "authors": [
            "Ian Helmke",
            "Daniel Kreymer",
            "Karl Wiegand"
        ],
        "abstract": "Real-time strategy (RTS) games make heavy use of artificial intelligence (AI), especially in the design of computerized opponents. Because of the computational complexity involved in managing all aspects of these games, many AI opponents are designed to optimize only a few areas of playing style. In games like StarCraft 2, a very popular and recently released RTS, most AI strategies revolve around economic and building efficiency: AI opponents try to gather and spend all resources as quickly and effectively as possible while ensuring that no units are idle. The aim of this work was to help address the need for AI combat strategies that are not computationally intensive. Our goal was to produce a computationally efficient model that is accurate at predicting the results of complex battles between diverse armies, including which army will win and how many units will remain. Our results suggest it may be possible to develop a relatively simple approximation model of combat that can accurately predict many battles that do not involve micromanagement. Future designs of AI opponents may be able to incorporate such an approximation model into their decision and planning systems to provide a challenge that is strategically balanced across all aspects of play.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1618",
        "title": "Design a Persian Automated Plagiarism Detector (AMZPPD)",
        "authors": [
            "Maryam Mahmoodi",
            "Mohammad Mahmoodi Varnamkhasti"
        ],
        "abstract": "Currently there are lots of plagiarism detection approaches. But few of them implemented and adapted for Persian languages. In this paper, our work on designing and implementation of a plagiarism detection system based on pre-processing and NLP technics will be described. And the results of testing on a corpus will be presented.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2498",
        "title": "Cognitive Internet of Things: A New Paradigm beyond Connection",
        "authors": [
            "Qihui Wu",
            "Guoru Ding",
            "Yuhua Xu",
            "Shuo Feng",
            "Zhiyong Du",
            "Jinlong Wang",
            "Keping Long"
        ],
        "abstract": "Current research on Internet of Things (IoT) mainly focuses on how to enable general objects to see, hear, and smell the physical world for themselves, and make them connected to share the observations. In this paper, we argue that only connected is not enough, beyond that, general objects should have the capability to learn, think, and understand both physical and social worlds by themselves. This practical need impels us to develop a new paradigm, named Cognitive Internet of Things (CIoT), to empower the current IoT with a `brain' for high-level intelligence. Specifically, we first present a comprehensive definition for CIoT, primarily inspired by the effectiveness of human cognition. Then, we propose an operational framework of CIoT, which mainly characterizes the interactions among five fundamental cognitive tasks: perception-action cycle, massive data analytics, semantic derivation and knowledge discovery, intelligent decision-making, and on-demand service provisioning. Furthermore, we provide a systematic tutorial on key enabling techniques involved in the cognitive tasks. In addition, we also discuss the design of proper performance metrics on evaluating the enabling techniques. Last but not least, we present the research challenges and open issues ahead. Building on the present work and potentially fruitful future studies, CIoT has the capability to bridge the physical world (with objects, resources, etc.) and the social world (with human demand, social behavior, etc.), and enhance smart resource allocation, automatic network operation, and intelligent service provisioning.\n    ",
        "submission_date": "2014-03-11T00:00:00",
        "last_modified_date": "2014-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2541",
        "title": "Turing: Then, Now and Still Key",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper looks at Turing's postulations about Artificial Intelligence in his paper 'Computing Machinery and Intelligence', published in 1950. It notes how accurate they were and how relevant they still are today. This paper notes the arguments and mechanisms that he suggested and tries to expand on them further. The paper however is mostly about describing the essential ingredients for building an intelligent model and the problems related with that. The discussion includes recent work by the author himself, who adds his own thoughts on the matter that come from a purely technical investigation into the problem. These are personal and quite speculative, but provide an interesting insight into the mechanisms that might be used for building an intelligent system.\n    ",
        "submission_date": "2014-03-11T00:00:00",
        "last_modified_date": "2014-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3084",
        "title": "Emerging archetypes in massive artificial societies for literary purposes using genetic algorithms",
        "authors": [
            "R.H. Garc\u00eda-Ortega",
            "P. Garc\u00eda-S\u00e1nchez",
            "J. J. Merelo"
        ],
        "abstract": "The creation of fictional stories is a very complex task that usually implies a creative process where the author has to combine characters, conflicts and plots to create an engaging narrative. This work presents a simulated environment with hundreds of characters that allows the study of coherent and interesting literary archetypes (or behaviours), plots and sub-plots. We will use this environment to perform a study about the number of profiles (parameters that define the personality of a character) needed to create two emergent scenes of archetypes: \"natality control\" and \"revenge\". A Genetic Algorithm (GA) will be used to find the fittest number of profiles and parameter configuration that enables the existence of the desired archetypes (played by the characters without their explicit knowledge). The results show that parametrizing this complex system is possible and that these kind of archetypes can emerge in the given environment.\n    ",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3807",
        "title": "Sensing Subjective Well-being from Social Media",
        "authors": [
            "Bibo Hao",
            "Lin Li",
            "Rui Gao",
            "Ang Li",
            "Tingshao Zhu"
        ],
        "abstract": "Subjective Well-being(SWB), which refers to how people experience the quality of their lives, is of great use to public policy-makers as well as economic, sociological research, etc. Traditionally, the measurement of SWB relies on time-consuming and costly self-report questionnaires. Nowadays, people are motivated to share their experiences and feelings on social media, so we propose to sense SWB from the vast user generated data on social media. By utilizing 1785 users' social media data with SWB labels, we train machine learning models that are able to \"sense\" individual SWB from users' social media. Our model, which attains the state-by-art prediction accuracy, can then be used to identify SWB of large population of social media users in time with very low cost.\n    ",
        "submission_date": "2014-03-15T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5142",
        "title": "Interactive Debugging of ASP Programs",
        "authors": [
            "Kostyantyn Shchekotykhin"
        ],
        "abstract": "Broad application of answer set programming (ASP) for declarative problem solving requires the development of tools supporting the coding process. Program debugging is one of the crucial activities within this process. Recently suggested ASP debugging approaches allow efficient computation of possible explanations of a fault. However, even for a small program a debugger might return a large number of possible explanations and selection of the correct one must be done manually. In this paper we present an interactive query-based ASP debugging method which extends previous approaches and finds a preferred explanation by means of observations. The system queries a programmer whether a set of ground atoms must be true in all (cautiously) or some (bravely) answer sets of the program. Since some queries can be more informative than the others, we discuss query selection strategies which, given user's preferences for an explanation, can find the best query. That is, the query an answer of which reduces the overall number of queries required for the identification of a preferred explanation.\n    ",
        "submission_date": "2014-03-20T00:00:00",
        "last_modified_date": "2014-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5169",
        "title": "Defuzzify firstly or finally: Dose it matter in fuzzy DEMATEL under uncertain environment?",
        "authors": [
            "Yunpeng Li",
            "Ya Li",
            "Jie Liu",
            "Yong Deng"
        ],
        "abstract": "Decision-Making Trial and Evaluation Laboratory (DEMATEL) method is widely used in many real applications. With the desirable property of efficient handling with the uncertain information in decision making, the fuzzy DEMATEL is heavily studied. Recently, Dytczak and Ginda suggested to defuzzify the fuzzy numbers firstly and then use the classical DEMATEL to obtain the final result. In this short paper, we show that it is not reasonable in some situations. The results of defuzzification at the first step are not coincide with the results of defuzzification at the final ",
        "submission_date": "2014-03-20T00:00:00",
        "last_modified_date": "2014-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5508",
        "title": "Towards Active Logic Programming",
        "authors": [
            "Stefania Costantini"
        ],
        "abstract": "In this paper we present the new logic programming language DALI, aimed at defining agents and agent systems. A main design objective for DALI has been that of introducing in a declarative fashion all the essential features, while keeping the language as close as possible to the syntax and semantics of the plain Horn--clause language. Special atoms and rules have been introduced, for representing: external events, to which the agent is able to respond (reactivity); actions (reactivity and proactivity); internal events (previous conclusions which can trigger further activity); past and present events (to be aware of what has happened). An extended resolution is provided, so that a DALI agent is able to answer queries like in the plain Horn--clause language, but is also able to cope with the different kinds of events, and exhibit a (rational) reactive and proactive behaviour.\n    ",
        "submission_date": "2014-03-21T00:00:00",
        "last_modified_date": "2014-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5618",
        "title": "Belief-Rule-Based Expert Systems for Evaluation of E- Government: A Case Study",
        "authors": [
            "Shahadat Hossein",
            "Par-Ola Zander",
            "Md. Kamal",
            "Linkon Chowdhury"
        ],
        "abstract": "Little knowledge exists on the impact and results associated with e-government projects in many specific use domains. Therefore it is necessary to evaluate the efficiency and effectiveness of e-government systems. Since the development of e-government is a continuous process of improvement, it requires continuous evaluation of the overall e-government system as well as evaluation of its various dimensions such as determinants, characteristics and results. E-government development is often complex with multiple stakeholders, large user bases and complex goals. Consequently, even experts have difficulties in evaluating these systems, especially in an integrated and comprehensive way as well as on an aggregate level. Expert systems are a candidate solution to evaluate such complex e-government systems. However, it is difficult for expert systems to cope with uncertain evaluation data that are vague, inconsistent, highly subjective or in other ways challenging to formalize. This paper presents an approach that can handle uncertainty in e-government evaluation: The combination of Belief Rule Base (BRB) knowledge representation and Evidential Reasoning (ES). This approach is illustrated with a concrete prototype, known as Belief Rule Based Expert System (BRBES) and put to use in the local e-government of Bangladesh. The results have been compared with a recently developed method of evaluating e-Government, and it is shown that the results of BRBES are more accurate and reliable. BRBES can be used to identify the factors that need to be improved to achieve the overall aim of an e-government project. In addition, various \"what if\" scenarios can be generated and developers and managers can get a forecast of the outcomes. In this way, the system can be used to facilitate decision making processes under uncertainty.\n    ",
        "submission_date": "2014-03-22T00:00:00",
        "last_modified_date": "2015-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5701",
        "title": "Cortex simulation system proposal using distributed computer network environments",
        "authors": [
            "Boris Tomas"
        ],
        "abstract": "In the dawn of computer science and the eve of neuroscience we participate in rebirth of neuroscience due to new technology that allows us to deeply and precisely explore whole new world that dwells in our brains.\n    ",
        "submission_date": "2014-03-22T00:00:00",
        "last_modified_date": "2014-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5753",
        "title": "D-CFPR: D numbers extended consistent fuzzy preference relations",
        "authors": [
            "Xinyang Deng",
            "Felix T.S. Chan",
            "Rehan Sadiq",
            "Sankaran Mahadevan",
            "Yong Deng"
        ],
        "abstract": "How to express an expert's or a decision maker's preference for alternatives is an open issue. Consistent fuzzy preference relation (CFPR) is with big advantages to handle this problem due to it can be construed via a smaller number of pairwise comparisons and satisfies additive transitivity property. However, the CFPR is incapable of dealing with the cases involving uncertain and incomplete information. In this paper, a D numbers extended consistent fuzzy preference relation (D-CFPR) is proposed to overcome the weakness. The D-CFPR extends the classical CFPR by using a new model of expressing uncertain information called D numbers. The D-CFPR inherits the merits of classical CFPR and can be totally reduced to the classical CFPR. This study can be integrated into our previous study about D-AHP (D numbers extended AHP) model to provide a systematic solution for multi-criteria decision making (MCDM).\n    ",
        "submission_date": "2014-03-23T00:00:00",
        "last_modified_date": "2014-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6036",
        "title": "Adaptive MCMC-Based Inference in Probabilistic Logic Programs",
        "authors": [
            "Arun Nampally",
            "C. R. Ramakrishnan"
        ],
        "abstract": "Probabilistic Logic Programming (PLP) languages enable programmers to specify systems that combine logical models with statistical knowledge. The inference problem, to determine the probability of query answers in PLP, is intractable in general, thereby motivating the need for approximate techniques. In this paper, we present a technique for approximate inference of conditional probabilities for PLP queries. It is an Adaptive Markov Chain Monte Carlo (MCMC) technique, where the distribution from which samples are drawn is modified as the Markov Chain is explored. In particular, the distribution is progressively modified to increase the likelihood that a generated sample is consistent with evidence. In our context, each sample is uniquely characterized by the outcomes of a set of random variables. Inspired by reinforcement learning, our technique propagates rewards to random variable/outcome pairs used in a sample based on whether the sample was consistent or not. The cumulative rewards of each outcome is used to derive a new \"adapted distribution\" for each random variable. For a sequence of samples, the distributions are progressively adapted after each sample. For a query with \"Markovian evaluation structure\", we show that the adapted distribution of samples converges to the query's conditional probability distribution. For Markovian queries, we present a modified adaptation process that can be used in adaptive MCMC as well as adaptive independent sampling. We empirically evaluate the effectiveness of the adaptive sampling methods for queries with and without Markovian evaluation structure.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6348",
        "title": "Updating Formulas and Algorithms for Computing Entropy and Gini Index from Time-Changing Data Streams",
        "authors": [
            "Blaz Sovdat"
        ],
        "abstract": "Despite growing interest in data stream mining the most successful incremental learners, such as VFDT, still use periodic recomputation to update attribute information gains and Gini indices. This note provides simple incremental formulas and algorithms for computing entropy and Gini index from time-changing data streams.\n    ",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2016-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7292",
        "title": "A Mining Method to Create Knowledge Map by Analysing the Data Resource",
        "authors": [
            "Arti Gupta",
            "Prof. N.T Deotale"
        ],
        "abstract": "The fundamental step in measuring the robustness of a system is the synthesis of the so called Process ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7373",
        "title": "Difficulty Rating of Sudoku Puzzles: An Overview and Evaluation",
        "authors": [
            "Radek Pel\u00e1nek"
        ],
        "abstract": "How can we predict the difficulty of a Sudoku puzzle? We give an overview of difficulty rating metrics and evaluate them on extensive dataset on human problem solving (more then 1700 Sudoku puzzles, hundreds of solvers). The best results are obtained using a computational model of human solving activity. Using the model we show that there are two sources of the problem difficulty: complexity of individual steps (logic operations) and structure of dependency among steps. We also describe metrics based on analysis of solutions under relaxed constraints -- a novel approach inspired by phase transition phenomenon in the graph coloring problem. In our discussion we focus not just on the performance of individual metrics on the Sudoku puzzle, but also on their generalizability and applicability to other problems.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7426",
        "title": "An Overview of Hierarchical Task Network Planning",
        "authors": [
            "Ilche Georgievski",
            "Marco Aiello"
        ],
        "abstract": "Hierarchies are the most common structure used to understand the world better. In galaxies, for instance, multiple-star systems are organised in a hierarchical system. Then, governmental and company organisations are structured using a hierarchy, while the Internet, which is used on a daily basis, has a space of domain names arranged hierarchically. Since Artificial Intelligence (AI) planning portrays information about the world and reasons to solve some of world's problems, Hierarchical Task Network (HTN) planning has been introduced almost 40 years ago to represent and deal with hierarchies. Its requirement for rich domain knowledge to characterise the world enables HTN planning to be very useful, but also to perform well. However, the history of almost 40 years obfuscates the current understanding of HTN planning in terms of accomplishments, planning models, similarities and differences among hierarchical planners, and its current and objective image. On top of these issues, attention attracts the ability of hierarchical planning to truly cope with the requirements of applications from the real world. We propose a framework-based approach to remedy this situation. First, we provide a basis for defining different formal models of hierarchical planning, and define two models that comprise a large portion of HTN planners. Second, we provide a set of concepts that helps to interpret HTN planners from the aspect of their search space. Then, we analyse and compare the planners based on a variety of properties organised in five segments, namely domain authoring, expressiveness, competence, performance and applicability. Furthermore, we select Web service composition as a real-world and current application, and classify and compare the approaches that employ HTN planning to solve the problem of service composition. Finally, we conclude with our findings and present directions for future work.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7465",
        "title": "Shiva: A Framework for Graph Based Ontology Matching",
        "authors": [
            "Iti Mathur",
            "Nisheeth Joshi",
            "Hemant Darbari",
            "Ajai Kumar"
        ],
        "abstract": "Since long, corporations are looking for knowledge sources which can provide structured description of data and can focus on meaning and shared understanding. Structures which can facilitate open world assumptions and can be flexible enough to incorporate and recognize more than one name for an entity. A source whose major purpose is to facilitate human communication and interoperability. Clearly, databases fail to provide these features and ontologies have emerged as an alternative choice, but corporations working on same domain tend to make different ontologies. The problem occurs when they want to share their data/knowledge. Thus we need tools to merge ontologies into one. This task is termed as ontology matching. This is an emerging area and still we have to go a long way in having an ideal matcher which can produce good results. In this paper we have shown a framework to matching ontologies using graphs.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7766",
        "title": "Enhancing Automated Decision Support across Medical and Oral Health Domains with Semantic Web Technologies",
        "authors": [
            "Tejal Shah",
            "Fethi Rabhi",
            "Pradeep Ray",
            "Kerry Taylor"
        ],
        "abstract": "Research has shown that the general health and oral health of an individual are closely related. Accordingly, current practice of isolating the information base of medical and oral health domains can be dangerous and detrimental to the health of the individual. However, technical issues such as heterogeneous data collection and storage formats, limited sharing of patient information and lack of decision support over the shared information are the principal reasons for the current state of affairs. To address these issues, the following research investigates the development and application of a cross-domain ontology and rules to build an evidence-based and reusable knowledge base consisting of the inter-dependent conditions from the two domains. Through example implementation of the knowledge base in Protege, we demonstrate the effectiveness of our approach in reasoning over and providing decision support for cross-domain patient information.\n    ",
        "submission_date": "2014-03-30T00:00:00",
        "last_modified_date": "2014-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.8046",
        "title": "Chemlambda, universality and self-multiplication",
        "authors": [
            "Marius Buliga",
            "Louis H. Kauffman"
        ],
        "abstract": "We present chemlambda (or the chemical concrete machine), an artificial chemistry with the following properties: (a) is Turing complete, (b) has a model of decentralized, distributed computing associated to it, (c) works at the level of individual (artificial) molecules, subject of reversible, but otherwise deterministic interactions with a small number of enzymes, (d) encodes information in the geometrical structure of the molecules and not in their numbers, (e) all interactions are purely local in space and time. This is part of a larger project to create computing, artificial chemistry and artificial life in a distributed context, using topological and graphical languages.\n    ",
        "submission_date": "2014-03-31T00:00:00",
        "last_modified_date": "2014-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0099",
        "title": "Venture: a higher-order probabilistic programming platform with programmable inference",
        "authors": [
            "Vikash Mansinghka",
            "Daniel Selsam",
            "Yura Perov"
        ],
        "abstract": "We describe Venture, an interactive virtual machine for probabilistic programming that aims to be sufficiently expressive, extensible, and efficient for general-purpose use. Like Church, probabilistic models and inference problems in Venture are specified via a Turing-complete, higher-order probabilistic language descended from Lisp. Unlike Church, Venture also provides a compositional language for custom inference strategies built out of scalable exact and approximate techniques. We also describe four key aspects of Venture's implementation that build on ideas from probabilistic graphical models. First, we describe the stochastic procedure interface (SPI) that specifies and encapsulates primitive random variables. The SPI supports custom control flow, higher-order probabilistic procedures, partially exchangeable sequences and ``likelihood-free'' stochastic simulators. It also supports external models that do inference over latent variables hidden from Venture. Second, we describe probabilistic execution traces (PETs), which represent execution histories of Venture programs. PETs capture conditional dependencies, existential dependencies and exchangeable coupling. Third, we describe partitions of execution histories called scaffolds that factor global inference problems into coherent sub-problems. Finally, we describe a family of stochastic regeneration algorithms for efficiently modifying PET fragments contained within scaffolds. Stochastic regeneration linear runtime scaling in cases where many previous approaches scaled quadratically. We show how to use stochastic regeneration and the SPI to implement general-purpose inference strategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals based on particle Markov chain Monte Carlo and mean-field variational inference techniques.\n    ",
        "submission_date": "2014-04-01T00:00:00",
        "last_modified_date": "2014-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0540",
        "title": "Modeling contaminant intrusion in water distribution networks based on D numbers",
        "authors": [
            "Li Gou",
            "Yong Deng",
            "Rehan Sadiq",
            "Sankaran Mahadevan"
        ],
        "abstract": "Efficient modeling on uncertain information plays an important role in estimating the risk of contaminant intrusion in water distribution networks. Dempster-Shafer evidence theory is one of the most commonly used methods. However, the Dempster-Shafer evidence theory has some hypotheses including the exclusive property of the elements in the frame of discernment, which may not be consistent with the real world. In this paper, based on a more effective representation of uncertainty, called D numbers, a new method that allows the elements in the frame of discernment to be non-exclusive is proposed. To demonstrate the efficiency of the proposed method, we apply it to the water distribution networks to estimate the risk of contaminant intrusion.\n    ",
        "submission_date": "2014-04-02T00:00:00",
        "last_modified_date": "2014-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0640",
        "title": "Conceptive Artificial Intelligence: Insights from design theory",
        "authors": [
            "Akin Osman Kazakci"
        ],
        "abstract": "The current paper offers a perspective on what we term conceptive intelligence - the capacity of an agent to continuously think of new object definitions (tasks, problems, physical systems, etc.) and to look for methods to realize them. The framework, called a Brouwer machine, is inspired by previous research in design theory and modeling, with its roots in the constructivist mathematics of intuitionism. The dual constructivist perspective we describe offers the possibility to create novelty both in terms of the types of objects and the methods for constructing objects. More generally, the theoretical work on which Brouwer machines are based is called imaginative constructivism. Based on the framework and the theory, we discuss many paradigms and techniques omnipresent in AI research and their merits and shortcomings for modeling aspects of design, as described by imaginative constructivism. To demonstrate and explain the type of creative process expressed by the notion of a Brouwer machine, we compare this concept with a system using genetic algorithms for scientific law discovery.\n    ",
        "submission_date": "2014-04-02T00:00:00",
        "last_modified_date": "2014-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1140",
        "title": "Scalable Planning and Learning for Multiagent POMDPs: Extended Version",
        "authors": [
            "Christopher Amato",
            "Frans A. Oliehoek"
        ],
        "abstract": "Online, sample-based planning algorithms for POMDPs have shown great promise in scaling to problems with large state spaces, but they become intractable for large action and observation spaces. This is particularly problematic in multiagent POMDPs where the action and observation space grows exponentially with the number of agents. To combat this intractability, we propose a novel scalable approach based on sample-based planning and factored value functions that exploits structure present in many multiagent settings. This approach applies not only in the planning case, but also in the Bayesian reinforcement learning setting. Experimental results show that we are able to provide high quality solutions to large multiagent planning and learning problems.\n    ",
        "submission_date": "2014-04-04T00:00:00",
        "last_modified_date": "2014-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1511",
        "title": "MTD(f), A Minimax Algorithm Faster Than NegaScout",
        "authors": [
            "Aske Plaat"
        ],
        "abstract": "MTD(f) is a new minimax search algorithm, simpler and more efficient than previous algorithms. In tests with a number of tournament game playing programs for chess, checkers and Othello it performed better, on average, than NegaScout/PVS (the AlphaBeta variant used in practically all good chess, checkers, and Othello programs). One of the strongest chess programs of the moment, MIT's parallel chess program Cilkchess uses MTD(f) as its search algorithm, replacing NegaScout, which was used in StarSocrates, the previous version of the program.\n    ",
        "submission_date": "2014-04-05T00:00:00",
        "last_modified_date": "2014-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1515",
        "title": "A New Paradigm for Minimax Search",
        "authors": [
            "Aske Plaat",
            "Jonathan Schaeffer",
            "Wim Pijls",
            "Arie de Bruin"
        ],
        "abstract": "This paper introduces a new paradigm for minimax game-tree search algo- rithms. MT is a memory-enhanced version of Pearls Test procedure. By changing the way MT is called, a number of best-first game-tree search algorithms can be simply and elegantly constructed (including SSS*). Most of the assessments of minimax search algorithms have been based on simulations. However, these simulations generally do not address two of the key ingredients of high performance game-playing programs: iterative deepening and memory usage. This paper presents experimental data from three game-playing programs (checkers, Othello and chess), covering the range from low to high branching factor. The improved move ordering due to iterative deepening and memory usage results in significantly different results from those portrayed in the literature. Whereas some simulations show Alpha-Beta expanding almost 100% more leaf nodes than other algorithms [12], our results showed variations of less than 20%. One new instance of our framework (MTD-f) out-performs our best alpha- beta searcher (aspiration NegaScout) on leaf nodes, total nodes and execution time. To our knowledge, these are the first reported results that compare both depth-first and best-first algorithms given the same amount of memory\n    ",
        "submission_date": "2014-04-05T00:00:00",
        "last_modified_date": "2014-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1517",
        "title": "SSS* = Alpha-Beta + TT",
        "authors": [
            "Aske Plaat",
            "Jonathan Schaeffer",
            "Wim Pijls",
            "Arie de Bruin"
        ],
        "abstract": "In 1979 Stockman introduced the SSS* minimax search algorithm that domi- nates Alpha-Beta in the number of leaf nodes expanded. Further investigation of the algorithm showed that it had three serious drawbacks, which prevented its use by practitioners: it is difficult to understand, it has large memory requirements, and it is slow. This paper presents an alternate formulation of SSS*, in which it is implemented as a series of Alpha-Beta calls that use a transposition table (AB- SSS*). The reformulation solves all three perceived drawbacks of SSS*, making it a practical algorithm. Further, because the search is now based on Alpha-Beta, the extensive research on minimax search enhancements can be easily integrated into AB-SSS*. To test AB-SSS* in practise, it has been implemented in three state-of-the- art programs: for checkers, Othello and chess. AB-SSS* is comparable in performance to Alpha-Beta on leaf node count in all three games, making it a viable alternative to Alpha-Beta in practise. Whereas SSS* has usually been regarded as being entirely different from Alpha-Beta, it turns out to be just an Alpha-Beta enhancement, like null-window searching. This runs counter to published simulation results. Our research leads to the surprising result that iterative deepening versions of Alpha-Beta can expand fewer leaf nodes than iterative deepening versions of SSS* due to dynamic move re-ordering.\n    ",
        "submission_date": "2014-04-05T00:00:00",
        "last_modified_date": "2014-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1518",
        "title": "Nearly Optimal Minimax Tree Search?",
        "authors": [
            "Aske Plaat",
            "Jonathan Schaeffer",
            "Wim Pijls",
            "Arie de Bruin"
        ],
        "abstract": "Knuth and Moore presented a theoretical lower bound on the number of leaves that any fixed-depth minimax tree-search algorithm traversing a uniform tree must explore, the so-called minimal tree. Since real-life minimax trees are not uniform, the exact size of this tree is not known for most applications. Further, most games have transpositions, implying that there exists a minimal graph which is smaller than the minimal tree. For three games (chess, Othello and checkers) we compute the size of the minimal tree and the minimal graph. Empirical evidence shows that in all three games, enhanced Alpha-Beta search is capable of building a tree that is close in size to that of the minimal graph. Hence, it appears game-playing programs build nearly optimal search trees. However, the conventional definition of the minimal graph is wrong. There are ways in which the size of the minimal graph can be reduced: by maximizing the number of transpositions in the search, and generating cutoffs using branches that lead to smaller search trees. The conventional definition of the minimal graph is just a left-most approximation. Calculating the size of the real minimal graph is too computationally intensive. However, upper bound approximations show it to be significantly smaller than the left-most minimal graph. Hence, it appears that game-playing programs are not searching as efficiently as is widely believed. Understanding the left-most and real minimal search graphs leads to some new ideas for enhancing Alpha-Beta search. One of them, enhanced transposition cutoffs, is shown to significantly reduce search tree size.\n    ",
        "submission_date": "2014-04-05T00:00:00",
        "last_modified_date": "2014-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1685",
        "title": "Thou Shalt is not You Will",
        "authors": [
            "Guido Governatori"
        ],
        "abstract": "In this paper we discuss some reasons why temporal logic might not be suitable to model real life norms. To show this, we present a novel deontic logic contrary-to-duty/derived permission paradox based on the interaction of obligations, permissions and contrary-to-duty obligations. The paradox is inspired by real life norms.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2015-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1718",
        "title": "Applications of Algorithmic Probability to the Philosophy of Mind",
        "authors": [
            "Gabriel Leuenberger"
        ],
        "abstract": "This paper presents formulae that can solve various seemingly hopeless philosophical conundrums. We discuss the simulation argument, teleportation, mind-uploading, the rationality of utilitarianism, and the ethics of exploiting artificial general intelligence. Our approach arises from combining the essential ideas of formalisms such as algorithmic probability, the universal intelligence measure, space-time-embedded intelligence, and Hutter's observer localization. We argue that such universal models can yield the ultimate solutions, but a novel research direction would be required in order to find computationally efficient approximations thereof.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2017-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1812",
        "title": "Determining the Consistency factor of Autopilot using Rough Set Theory",
        "authors": [
            "Anugrah Kumar"
        ],
        "abstract": "Autopilot is a system designed to guide a vehicle without aid. Due to increase in flight hours and complexity of modern day flight it has become imperative to equip the aircrafts with autopilot. Thus reliability and consistency of an Autopilot system becomes a crucial role in a flight. But the increased complexity and demand for better accuracy has made the process of evaluating the autopilot for consistency a difficult process .A vast amount of imprecise data has been involved. Rough sets can be a potent tool for such kind of Applications containing vague data. This paper proposes an approach towards Consistency factor determination using Rough Set Theory. The seventeen basic factors, that are crucial in determining the consistency of an Autopilot system, are grouped into five Payloads based on their functionality. Consistency Factor is evaluated through these payloads, using Rough Set Theory. Consistency Factor determines the consistency and reliability of an autopilot system and the conditions under which manual override becomes imperative. Using Rough set Theory the most and the least influential factors towards Autopilot system are also determined.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1884",
        "title": "Plug and Play! A Simple, Universal Model for Energy Disaggregation",
        "authors": [
            "Guoming Tang",
            "Kui Wu",
            "Jingsheng Lei",
            "Jiuyang Tang"
        ],
        "abstract": "Energy disaggregation is to discover the energy consumption of individual appliances from their aggregated energy values. To solve the problem, most existing approaches rely on either appliances' signatures or their state transition patterns, both hard to obtain in practice. Aiming at developing a simple, universal model that works without depending on sophisticated machine learning techniques or auxiliary equipments, we make use of easily accessible knowledge of appliances and the sparsity of the switching events to design a Sparse Switching Event Recovering (SSER) method. By minimizing the total variation (TV) of the (sparse) event matrix, SSER can effectively recover the individual energy consumption values from the aggregated ones. To speed up the process, a Parallel Local Optimization Algorithm (PLOA) is proposed to solve the problem in active epochs of appliance activities in parallel. Using real-world trace data, we compare the performance of our method with that of the state-of-the-art solutions, including Least Square Estimation (LSE) and iterative Hidden Markov Model (HMM). The results show that our approach has an overall higher detection accuracy and a smaller overhead.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2116",
        "title": "Rational Counterfactuals",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "abstract": "This paper introduces the concept of rational countefactuals which is an idea of identifying a counterfactual from the factual (whether perceived or real) that maximizes the attainment of the desired consequent. In counterfactual thinking if we have a factual statement like: Saddam Hussein invaded Kuwait and consequently George Bush declared war on Iraq then its counterfactuals is: If Saddam Hussein did not invade Kuwait then George Bush would not have declared war on Iraq. The theory of rational counterfactuals is applied to identify the antecedent that gives the desired consequent necessary for rational decision making. The rational countefactual theory is applied to identify the values of variables Allies, Contingency, Distance, Major Power, Capability, Democracy, as well as Economic Interdependency that gives the desired consequent Peace.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2162",
        "title": "The NNN Formalization: Review and Development of Guideline Specification in the Care Domain",
        "authors": [
            "Georg Kaes",
            "J\u00fcrgen Manger",
            "Stefanie Rinderle-Ma",
            "Ralph Vigne"
        ],
        "abstract": "Due to an ageing society, it can be expected that less nursing personnel will be responsible for an increasing number of patients in the future. One way to address this challenge is to provide system-based support for nursing personnel in creating, executing, and adapting patient care processes. In care practice, these processes are following the general care process definition and individually specified according to patient-specific data as well as diagnoses and guidelines from the NANDA, NIC, and NOC (NNN) standards. In addition, adaptations to running patient processes become necessary frequently and are to be conducted by nursing personnel including NNN knowledge. In order to provide semi-automatic support for design and adaption of care processes, a formalization of NNN knowledge is indispensable. This technical report presents the NNN formalization that is developed targeting at goals such as completeness, flexibility, and later exploitation for creating and adapting patient care processes. The formalization also takes into consideration an extensive evaluation of existing formalization standards for clinical guidelines. The NNN formalization as well as its usage are evaluated based on case study FATIGUE.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2267",
        "title": "Transparallel mind: Classical computing with quantum power",
        "authors": [
            "Peter A. van der Helm"
        ],
        "abstract": "Inspired by the extraordinary computing power promised by quantum computers, the quantum mind hypothesis postulated that quantum mechanical phenomena are the source of neuronal synchronization, which, in turn, might underlie consciousness. Here, I present an alternative inspired by a classical computing method with quantum power. This method relies on special distributed representations called hyperstrings. Hyperstrings are superpositions of up to an exponential number of strings, which -- by a single-processor classical computer -- can be evaluated in a transparallel fashion, that is, simultaneously as if only one string were concerned. Building on a neurally plausible model of human visual perceptual organization, in which hyperstrings are formal counterparts of transient neural assemblies, I postulate that synchronization in such assemblies is a manifestation of transparallel information processing. This accounts for the high combinatorial capacity and speed of human visual perceptual organization and strengthens ideas that self-organizing cognitive architecture bridges the gap between neurons and consciousness.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2015-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2313",
        "title": "Outer-Product Hidden Markov Model and Polyphonic MIDI Score Following",
        "authors": [
            "Eita Nakamura",
            "Tomohiko Nakamura",
            "Yasuyuki Saito",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "abstract": "We present a polyphonic MIDI score-following algorithm capable of following performances with arbitrary repeats and skips, based on a probabilistic model of musical performances. It is attractive in practical applications of score following to handle repeats and skips which may be made arbitrarily during performances, but the algorithms previously described in the literature cannot be applied to scores of practical length due to problems with large computational complexity. We propose a new type of hidden Markov model (HMM) as a performance model which can describe arbitrary repeats and skips including performer tendencies on distributed score positions before and after them, and derive an efficient score-following algorithm that reduces computational complexity without pruning. A theoretical discussion on how much such information on performer tendencies improves the score-following results is given. The proposed score-following algorithm also admits performance mistakes and is demonstrated to be effective in practical situations by carrying out evaluations with human performances. The proposed HMM is potentially valuable for other topics in information processing and we also provide a detailed description of inference algorithms.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2314",
        "title": "A Stochastic Temporal Model of Polyphonic MIDI Performance with Ornaments",
        "authors": [
            "Eita Nakamura",
            "Nobutaka Ono",
            "Shigeki Sagayama",
            "Kenji Watanabe"
        ],
        "abstract": "We study indeterminacies in realization of ornaments and how they can be incorporated in a stochastic performance model applicable for music information processing such as score-performance matching. We point out the importance of temporal information, and propose a hidden Markov model which describes it explicitly and represents ornaments with several state types. Following a review of the indeterminacies, they are carefully incorporated into the model through its topology and parameters, and the state construction for quite general polyphonic scores is explained in detail. By analyzing piano performance data, we find significant overlaps in inter-onset-interval distributions of chordal notes, ornaments, and inter-chord events, and the data is used to determine details of the model. The model is applied for score following and offline score-performance matching, yielding highly accurate matching for performances with many ornaments and relatively frequent errors, repeats, and skips.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2768",
        "title": "Verification of confliction and unreachability in rule-based expert systems with model checking",
        "authors": [
            "Einollah pira",
            "Mohammad Reza Zand Miralvand",
            "Fakhteh Soltani"
        ],
        "abstract": "It is important to find optimal solutions for structural errors in rule-based expert systems .Solutions to discovering such errors by using model checking techniques have already been proposed, but these solutions have problems such as state space explosion. In this paper, to overcome these problems, we model the rule-based systems as finite state transition systems and express confliction and unreachability as Computation Tree Logic (CTL) logic formula and then use the technique of model checking to detect confliction and unreachability in rule-based systems with the model checker UPPAAL.\n    ",
        "submission_date": "2014-04-10T00:00:00",
        "last_modified_date": "2014-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2984",
        "title": "Distribution-Aware Sampling and Weighted Model Counting for SAT",
        "authors": [
            "Supratik Chakraborty",
            "Daniel J. Fremont",
            "Kuldeep S. Meel",
            "Sanjit A. Seshia",
            "Moshe Y. Vardi"
        ],
        "abstract": "Given a CNF formula and a weight for each assignment of values to variables, two natural problems are weighted model counting and distribution-aware sampling of satisfying assignments. Both problems have a wide variety of important applications. Due to the inherent complexity of the exact versions of the problems, interest has focused on solving them approximately. Prior work in this area scaled only to small problems in practice, or failed to provide strong theoretical guarantees, or employed a computationally-expensive maximum a posteriori probability (MAP) oracle that assumes prior knowledge of a factored representation of the weight distribution. We present a novel approach that works with a black-box oracle for weights of assignments and requires only an {\\NP}-oracle (in practice, a SAT-solver) to solve both the counting and sampling problems. Our approach works under mild assumptions on the distribution of weights of satisfying assignments, provides strong theoretical guarantees, and scales to problems involving several thousand variables. We also show that the assumptions can be significantly relaxed while improving computational efficiency if a factored representation of the weights is known.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3141",
        "title": "Datalog Rewritability of Disjunctive Datalog Programs and its Applications to Ontology Reasoning",
        "authors": [
            "Mark Kaminski",
            "Yavor Nenov",
            "Bernardo Cuenca Grau"
        ],
        "abstract": "We study the problem of rewriting a disjunctive datalog program into plain datalog. We show that a disjunctive program is rewritable if and only if it is equivalent to a linear disjunctive program, thus providing a novel characterisation of datalog rewritability. Motivated by this result, we propose weakly linear disjunctive datalog---a novel rule-based KR language that extends both datalog and linear disjunctive datalog and for which reasoning is tractable in data complexity. We then explore applications of weakly linear programs to ontology reasoning and propose a tractable extension of OWL 2 RL with disjunctive axioms. Our empirical results suggest that many non-Horn ontologies can be reduced to weakly linear programs and that query answering over such ontologies using a datalog engine is feasible in practice.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3285",
        "title": "An Integer Programming Model for the Dynamic Location and Relocation of Emergency Vehicles: A Case Study",
        "authors": [
            "Mahdi Moeini",
            "Zied Jemai",
            "Evren Sahin"
        ],
        "abstract": "In this paper, we address the dynamic Emergency Medical Service (EMS) systems. A dynamic location model is presented that tries to locate and relocate the ambulances. The proposed model controls the movements and locations of ambulances in order to provide a better coverage of the demand points under different fluctuation patterns that may happen during a given period of time. Some numerical experiments have been carried out by using some real-world data sets that have been collected through the French EMS system.\n    ",
        "submission_date": "2014-04-12T00:00:00",
        "last_modified_date": "2014-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3301",
        "title": "Efficient Inference and Learning in a Large Knowledge Base: Reasoning with Extracted Information using a Locally Groundable First-Order Probabilistic Logic",
        "authors": [
            "William Yang Wang",
            "Kathryn Mazaitis",
            "Ni Lao",
            "Tom Mitchell",
            "William W. Cohen"
        ],
        "abstract": "One important challenge for probabilistic logics is reasoning with very large knowledge bases (KBs) of imperfect information, such as those produced by modern web-scale information extraction systems. One scalability problem shared by many probabilistic logics is that answering queries involves \"grounding\" the query---i.e., mapping it to a propositional representation---and the size of a \"grounding\" grows with database size. To address this bottleneck, we present a first-order probabilistic language called ProPPR in which that approximate \"local groundings\" can be constructed in time independent of database size. Technically, ProPPR is an extension to stochastic logic programs (SLPs) that is biased towards short derivations; it is also closely related to an earlier relational learning algorithm called the path ranking algorithm (PRA). We show that the problem of constructing proofs for this logic is related to computation of personalized PageRank (PPR) on a linearized version of the proof space, and using on this connection, we develop a proveably-correct approximate grounding scheme, based on the PageRank-Nibble algorithm. Building on this, we develop a fast and easily-parallelized weight-learning algorithm for ProPPR. In experiments, we show that learning for ProPPR is orders magnitude faster than learning for Markov logic networks; that allowing mutual recursion (joint learning) in KB inference leads to improvements in performance; and that ProPPR can learn weights for a mutually recursive program with hundreds of clauses, which define scores of interrelated predicates, over a KB containing one million entities.\n    ",
        "submission_date": "2014-04-12T00:00:00",
        "last_modified_date": "2014-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3370",
        "title": "Distance function of D numbers",
        "authors": [
            "Meizhu Li",
            "Qi Zhang",
            "Xinyang Deng",
            "Yong Deng"
        ],
        "abstract": "Dempster-Shafer theory is widely applied in uncertainty modelling and knowledge reasoning due to its ability of expressing uncertain information. A distance between two basic probability assignments(BPAs) presents a measure of performance for identification algorithms based on the evidential theory of Dempster-Shafer. However, some conditions lead to limitations in practical application for Dempster-Shafer theory, such as exclusiveness hypothesis and completeness constraint. To overcome these shortcomings, a novel theory called D numbers theory is proposed. A distance function of D numbers is proposed to measure the distance between two D numbers. The distance function of D numbers is an generalization of distance between two BPAs, which inherits the advantage of Dempster-Shafer theory and strengthens the capability of uncertainty modeling. An illustrative case is provided to demonstrate the effectiveness of the proposed function.\n    ",
        "submission_date": "2014-04-13T00:00:00",
        "last_modified_date": "2014-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3659",
        "title": "Avoiding Undesired Choices Using Intelligent Adaptive Systems",
        "authors": [
            "Amir Konigsberg"
        ],
        "abstract": "We propose a number of heuristics that can be used for identifying when intransitive choice behaviour is likely to occur in choice situations. We also suggest two methods for avoiding undesired choice behaviour, namely transparent communication and adaptive choice-set generation. We believe that these two ways can contribute to the avoidance of decision biases in choice situations that may often be regretted.\n    ",
        "submission_date": "2014-04-10T00:00:00",
        "last_modified_date": "2014-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3675",
        "title": "On Backdoors To Tractable Constraint Languages",
        "authors": [
            "Clement Carbonnel",
            "Martin C. Cooper",
            "Emmanuel Hebrard"
        ],
        "abstract": "In the context of CSPs, a strong backdoor is a subset of variables such that every complete assignment yields a residual instance guaranteed to have a specified property. If the property allows efficient solving, then a small strong backdoor provides a reasonable decomposition of the original instance into easy instances. An important challenge is the design of algorithms that can find quickly a small strong backdoor if one exists. We present a systematic study of the parameterized complexity of backdoor detection when the target property is a restricted type of constraint language defined by means of a family of polymorphisms. In particular, we show that under the weak assumption that the polymorphisms are idempotent, the problem is unlikely to be FPT when the parameter is either r (the constraint arity) or k (the size of the backdoor) unless P = NP or FPT = W[2]. When the parameter is k+r, however, we are able to identify large classes of languages for which the problem of finding a small backdoor is FPT.\n    ",
        "submission_date": "2014-04-14T00:00:00",
        "last_modified_date": "2014-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4089",
        "title": "On the Role of Canonicity in Bottom-up Knowledge Compilation",
        "authors": [
            "Guy Van den Broeck",
            "Adnan Darwiche"
        ],
        "abstract": "We consider the problem of bottom-up compilation of knowledge bases, which is usually predicated on the existence of a polytime function for combining compilations using Boolean operators (usually called an Apply function). While such a polytime Apply function is known to exist for certain languages (e.g., OBDDs) and not exist for others (e.g., DNNF), its existence for certain languages remains unknown. Among the latter is the recently introduced language of Sentential Decision Diagrams (SDDs), for which a polytime Apply function exists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonical SDDs). We resolve this open question in this paper and consider some of its theoretical and practical implications. Some of the findings we report question the common wisdom on the relationship between bottom-up compilation, language canonicity and the complexity of the Apply function.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4258",
        "title": "An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized Approximate Linear Programming Approximation Accuracy",
        "authors": [
            "Gavin Taylor",
            "Connor Geer",
            "David Piekut"
        ],
        "abstract": "Recent interest in the use of $L_1$ regularization in the use of value function approximation includes Petrik et al.'s introduction of $L_1$-Regularized Approximate Linear Programming (RALP). RALP is unique among $L_1$-regularized approaches in that it approximates the optimal value function using off-policy samples. Additionally, it produces policies which outperform those of previous methods, such as LSPI. RALP's value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program, and by the distribution from which samples are drawn; however, there has been no discussion of these considerations in the previous literature. In this paper, we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality, using both theoretical and experimental illustrations. The results provide insight not only onto these effects, but also provide intuition into the types of MDPs which are especially well suited for approximation with RALP.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4274",
        "title": "Managing Change in Graph-structured Data Using Description Logics (long version with appendix)",
        "authors": [
            "Shqiponja Ahmetaj",
            "Diego Calvanese",
            "Magdalena Ortiz",
            "Mantas Simkus"
        ],
        "abstract": "In this paper, we consider the setting of graph-structured data that evolves as a result of operations carried out by users or applications. We study different reasoning problems, which range from ensuring the satisfaction of a given set of integrity constraints after a given sequence of updates, to deciding the (non-)existence of a sequence of actions that would take the data to an (un)desirable state, starting either from a specific data instance or from an incomplete description of it. We consider an action language in which actions are finite sequences of conditional insertions and deletions of nodes and labels, and use Description Logics for describing integrity constraints and (partial) states of the data. We then formalize the above data management problems as a static verification problem and several planning problems. We provide algorithms and tight complexity bounds for the formalized problems, both for an expressive DL and for a variant of DL-Lite.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4785",
        "title": "Ontology as a Source for Rule Generation",
        "authors": [
            "Olegs Verhodubs"
        ],
        "abstract": "This paper discloses the potential of OWL (Web Ontology Language) ontologies for generation of rules. The main purpose of this paper is to identify new types of rules, which may be generated from OWL ontologies. Rules, generated from OWL ontologies, are necessary for the functioning of the Semantic Web Expert System. It is expected that the Semantic Web Expert System (SWES) will be able to process ontologies from the Web with the purpose to supplement or even to develop its knowledge base.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4789",
        "title": "A new combination approach based on improved evidence distance",
        "authors": [
            "Hongming Mo",
            "Yong Deng"
        ],
        "abstract": "Dempster-Shafer evidence theory is a powerful tool in information fusion. When the evidence are highly conflicting, the counter-intuitive results will be presented. To adress this open issue, a new method based on evidence distance of Jousselme and Hausdorff distance is proposed. Weight of each evidence can be computed, preprocess the original evidence to generate a new evidence. The Dempster's combination rule is used to combine the new evidence. Comparing with the existing methods, the new proposed method is efficient.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4801",
        "title": "Generalized Evidence Theory",
        "authors": [
            "Yong Deng"
        ],
        "abstract": "Conflict management is still an open issue in the application of Dempster Shafer evidence theory. A lot of works have been presented to address this issue. In this paper, a new theory, called as generalized evidence theory (GET), is proposed. Compared with existing methods, GET assumes that the general situation is in open world due to the uncertainty and incomplete knowledge. The conflicting evidence is handled under the framework of GET. It is shown that the new theory can explain and deal with the conflicting evidence in a more reasonable way.\n    ",
        "submission_date": "2014-04-17T00:00:00",
        "last_modified_date": "2014-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4884",
        "title": "Causal Interfaces",
        "authors": [
            "David A. Eubanks"
        ],
        "abstract": "The interaction of two binary variables, assumed to be empirical observations, has three degrees of freedom when expressed as a matrix of frequencies. Usually, the size of causal influence of one variable on the other is calculated as a single value, as increase in recovery rate for a medical treatment, for example. We examine what is lost in this simplification, and propose using two interface constants to represent positive and negative implications separately. Given certain assumptions about non-causal outcomes, the set of resulting epistemologies is a continuum. We derive a variety of particular measures and contrast them with the one-dimensional index.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4893",
        "title": "CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit",
        "authors": [
            "Daniele Codecasa",
            "Fabio Stella"
        ],
        "abstract": "Continuous time Bayesian network classifiers are designed for temporal classification of multivariate streaming data when time duration of events matters and the class does not change over time. This paper introduces the CTBNCToolkit: an open source Java toolkit which provides a stand-alone application for temporal classification and a library for continuous time Bayesian network classifiers. CTBNCToolkit implements the inference algorithm, the parameter learning algorithm, and the structural learning algorithm for continuous time Bayesian network classifiers. The structural learning algorithm is based on scoring functions: the marginal log-likelihood score and the conditional log-likelihood score are provided. CTBNCToolkit provides also an implementation of the expectation maximization algorithm for clustering purpose. The paper introduces continuous time Bayesian network classifiers. How to use the CTBNToolkit from the command line is described in a specific section. Tutorial examples are included to facilitate users to understand how the toolkit must be used. A section dedicate to the Java library is proposed to help further code extensions.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4983",
        "title": "Shiva++: An Enhanced Graph based Ontology Matcher",
        "authors": [
            "Iti Mathur",
            "Nisheeth Joshi",
            "Hemant Darbari",
            "Ajai Kumar"
        ],
        "abstract": "With the web getting bigger and assimilating knowledge about different concepts and domains, it is becoming very difficult for simple database driven applications to capture the data for a domain. Thus developers have come out with ontology based systems which can store large amount of information and can apply reasoning and produce timely information. Thus facilitating effective knowledge management. Though this approach has made our lives easier, but at the same time has given rise to another problem. Two different ontologies assimilating same knowledge tend to use different terms for the same concepts. This creates confusion among knowledge engineers and workers, as they do not know which is a better term then the other. Thus we need to merge ontologies working on same domain so that the engineers can develop a better application over it. This paper shows the development of one such matcher which merges the concepts available in two ontologies at two levels; 1) at string level and 2) at semantic level; thus producing better merged ontologies. We have used a graph matching technique which works at the core of the system. We have also evaluated the system and have tested its performance with its predecessor which works only on string matching. Thus current approach produces better results.\n    ",
        "submission_date": "2014-04-19T00:00:00",
        "last_modified_date": "2014-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5078",
        "title": "TurKPF: TurKontrol as a Particle Filter",
        "authors": [
            "Ethan Petuchowski",
            "Matthew Lease"
        ],
        "abstract": "TurKontrol, and algorithm presented in (Dai et al. 2010), uses a POMDP to model and control an iterative workflow for crowdsourced work. Here, TurKontrol is re-implemented as \"TurKPF,\" which uses a Particle Filter to reduce computation time & memory usage. Most importantly, in our experimental environment with default parameter settings, the action is chosen nearly instantaneously. Through a series of experiments we see that TurKPF and TurKontrol perform similarly.\n    ",
        "submission_date": "2014-04-20T00:00:00",
        "last_modified_date": "2014-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5454",
        "title": "Stochastic Privacy",
        "authors": [
            "Adish Singla",
            "Eric Horvitz",
            "Ece Kamar",
            "Ryen White"
        ],
        "abstract": "Online services such as web search and e-commerce applications typically rely on the collection of data about users, including details of their activities on the web. Such personal data is used to enhance the quality of service via personalization of content and to maximize revenues via better targeting of advertisements and deeper engagement of users on sites. To date, service providers have largely followed the approach of either requiring or requesting consent for opting-in to share their data. Users may be willing to share private information in return for better quality of service or for incentives, or in return for assurances about the nature and extend of the logging of data. We introduce \\emph{stochastic privacy}, a new approach to privacy centering on a simple concept: A guarantee is provided to users about the upper-bound on the probability that their personal data will be used. Such a probability, which we refer to as \\emph{privacy risk}, can be assessed by users as a preference or communicated as a policy by a service provider. Service providers can work to personalize and to optimize revenues in accordance with preferences about privacy risk. We present procedures, proofs, and an overall system for maximizing the quality of services, while respecting bounds on allowable or communicated privacy risk. We demonstrate the methodology with a case study and evaluation of the procedures applied to web search personalization. We show how we can achieve near-optimal utility of accessing information with provable guarantees on the probability of sharing data.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5643",
        "title": "A Formal Analysis of Required Cooperation in Multi-agent Planning",
        "authors": [
            "Yu Zhang",
            "Subbarao Kambhampati"
        ],
        "abstract": "Research on multi-agent planning has been popular in recent years. While previous research has been motivated by the understanding that, through cooperation, multi-agent systems can achieve tasks that are unachievable by single-agent systems, there are no formal characterizations of situations where cooperation is required to achieve a goal, thus warranting the application of multi-agent systems. In this paper, we provide such a formal discussion from the planning aspect. We first show that determining whether there is required cooperation (RC) is intractable is general. Then, by dividing the problems that require cooperation (referred to as RC problems) into two classes -- problems with heterogeneous and homogeneous agents, we aim to identify all the conditions that can cause RC in these two classes. We establish that when none of these identified conditions hold, the problem is single-agent solvable. Furthermore, with a few assumptions, we provide an upper bound on the minimum number of agents required for RC problems with homogeneous agents. This study not only provides new insights into multi-agent planning, but also has many applications. For example, in human-robot teaming, when a robot cannot achieve a task, it may be due to RC. In such cases, the human teammate should be informed and, consequently, coordinate with other available robots for a solution.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5668",
        "title": "An Adversarial Interpretation of Information-Theoretic Bounded Rationality",
        "authors": [
            "Pedro A. Ortega",
            "Daniel D. Lee"
        ],
        "abstract": "Recently, there has been a growing interest in modeling planning with information constraints. Accordingly, an agent maximizes a regularized expected utility known as the free energy, where the regularizer is given by the information divergence from a prior to a posterior policy. While this approach can be justified in various ways, including from statistical mechanics and information theory, it is still unclear how it relates to decision-making against adversarial environments. This connection has previously been suggested in work relating the free energy to risk-sensitive control and to extensive form games. Here, we show that a single-agent free energy optimization is equivalent to a game between the agent and an imaginary adversary. The adversary can, by paying an exponential penalty, generate costs that diminish the decision maker's payoffs. It turns out that the optimal strategy of the adversary consists in choosing costs so as to render the decision maker indifferent among its choices, which is a definining property of a Nash equilibrium, thus tightening the connection between free energy optimization and game theory.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6036",
        "title": "Gradual Classical Logic for Attributed Objects",
        "authors": [
            "Ryuta Arisaka"
        ],
        "abstract": "There is knowledge. There is belief. And there is tacit agreement.' 'We may talk about objects. We may talk about attributes of the objects. Or we may talk both about objects and their attributes.' This work inspects tacit agreements on assumptions about the relation between objects and their attributes, and studies a way of expressing them, presenting as the result what we term gradual logic in which the sense of truth gradually shifts. It extends classical logic instances with a new logical connective capturing the object-attribute relation. A formal semantics is presented. Decidability is proved. Para- consistent/epistemic/conditional/intensional/description/combined logics are compared.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6059",
        "title": "A Comparative study Between Fuzzy Clustering Algorithm and Hard Clustering Algorithm",
        "authors": [
            "Dibya Jyoti Bora",
            "Dr. Anil Kumar Gupta"
        ],
        "abstract": "Data clustering is an important area of data mining. This is an unsupervised study where data of similar types are put into one cluster while data of another types are put into different cluster. Fuzzy C means is a very important clustering technique based on fuzzy logic. Also we have some hard clustering techniques available like K-means among the popular ones. In this paper a comparative study is done between Fuzzy clustering algorithm and hard clustering algorithm\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6445",
        "title": "Belief merging within fragments of propositional logic",
        "authors": [
            "Nadia Creignou",
            "Odile Papini",
            "Stefan R\u00fcmmele",
            "Stefan Woltran"
        ],
        "abstract": "Recently, belief change within the framework of fragments of propositional logic has gained increasing attention. Previous works focused on belief contraction and belief revision on the Horn fragment. However, the problem of belief merging within fragments of propositional logic has been neglected so far. This paper presents a general approach to define new merging operators derived from existing ones such that the result of merging remains in the fragment under consideration. Our approach is not limited to the case of Horn fragment but applicable to any fragment of propositional logic characterized by a closure property on the sets of models of its formulae. We study the logical properties of the proposed operators in terms of satisfaction of merging postulates, considering in particular distance-based merging operators for Horn and Krom fragments.\n    ",
        "submission_date": "2014-04-25T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6566",
        "title": "On the Non-Monotonic Description Logic $\\mathcal{ALC}$+T$_{\\mathsf{min}}$",
        "authors": [
            "Oliver Fern\u00e1ndez Gil"
        ],
        "abstract": "In the last 20 years many proposals have been made to incorporate non-monotonic reasoning into description logics, ranging from approaches based on default logic and circumscription to those based on preferential semantics. In particular, the non-monotonic description logic $\\mathcal{ALC}$+T$_{\\mathsf{min}}$ uses a combination of the preferential semantics with minimization of a certain kind of concepts, which represent atypical instances of a class of elements. One of its drawbacks is that it suffers from the problem known as the \\emph{property blocking inheritance}, which can be seen as a weakness from an inferential point of view. In this paper we propose an extension of $\\mathcal{ALC}$+T$_{\\mathsf{min}}$, namely $\\mathcal{ALC}$+T$^+_{\\mathsf{min}}$, with the purpose to solve the mentioned problem. In addition, we show the close connection that exists between $\\mathcal{ALC}$+T$^+_{\\mathsf{min}}$ and concept-circumscribed knowledge bases. Finally, we study the complexity of deciding the classical reasoning tasks in $\\mathcal{ALC}$+T$^+_{\\mathsf{min}}$.\n    ",
        "submission_date": "2014-04-25T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6567",
        "title": "Une approche CSP pour l'aide \u00e0 la localisation d'erreurs",
        "authors": [
            "Mohammed Bekkouche",
            "H\u00e9l\u00e8ne Collavizza",
            "Michel Rueher"
        ],
        "abstract": "We introduce in this paper a new CP-based approach to support errors location in a program for which a counter-example is available, i.e. an instantiation of the input variables that violates the post-condition. To provide helpful information for error location, we generate a constraint system for the paths of the CFG (Control Flow Graph) for which at most k conditional statements may be erroneous. Then, we calculate Minimal Correction Sets (MCS) of bounded size for each of these paths. The removal of one of these sets of constraints yields a maximal satisfiable subset, in other words, a maximal subset of constraints satisfying the post condition. We extend the algorithm proposed by Liffiton and Sakallah \\cite{LiS08} to handle programs with numerical statements more efficiently. We present preliminary experimental results that are quite encouraging.\n    ",
        "submission_date": "2014-04-25T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6696",
        "title": "Hybrid Metaheuristics for the Clustered Vehicle Routing Problem",
        "authors": [
            "Thibaut Vidal",
            "Maria Battarra",
            "Anand Subramanian",
            "G\u00fcne\u015f Erdo\u01e7an"
        ],
        "abstract": "The Clustered Vehicle Routing Problem (CluVRP) is a variant of the Capacitated Vehicle Routing Problem in which customers are grouped into clusters. Each cluster has to be visited once, and a vehicle entering a cluster cannot leave it until all customers have been visited. This article presents two alternative hybrid metaheuristic algorithms for the CluVRP. The first algorithm is based on an Iterated Local Search algorithm, in which only feasible solutions are explored and problem-specific local search moves are utilized. The second algorithm is a Hybrid Genetic Search, for which the shortest Hamiltonian path between each pair of vertices within each cluster should be precomputed. Using this information, a sequence of clusters can be used as a solution representation and large neighborhoods can be efficiently explored by means of bi-directional dynamic programming, sequence concatenations, by using appropriate data structures. Extensive computational experiments are performed on benchmark instances from the literature, as well as new large scale ones. Recommendations on promising algorithm choices are provided relatively to average cluster size.\n    ",
        "submission_date": "2014-04-26T00:00:00",
        "last_modified_date": "2014-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6784",
        "title": "On Strong and Default Negation in Logic Program Updates (Extended Version)",
        "authors": [
            "Martin Slota",
            "Martin Bal\u00e1z",
            "Jo\u00e3o Leite"
        ],
        "abstract": "Existing semantics for answer-set program updates fall into two categories: either they consider only strong negation in heads of rules, or they primarily rely on default negation in heads of rules and optionally provide support for strong negation by means of a syntactic transformation. In this paper we pinpoint the limitations of both these approaches and argue that both types of negation should be first-class citizens in the context of updates. We identify principles that plausibly constrain their interaction but are not simultaneously satisfied by any existing rule update semantics. Then we extend one of the most advanced semantics with direct support for strong negation and show that it satisfies the outlined principles as well as a variety of other desirable properties.\n    ",
        "submission_date": "2014-04-27T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6883",
        "title": "Credulous and Skeptical Argument Games for Complete Semantics in Conflict Resolution based Argumentation",
        "authors": [
            "Jozef Frt\u00fas"
        ],
        "abstract": "Argumentation is one of the most popular approaches of defining a~non-monotonic formalism and several argumentation based semantics were proposed for defeasible logic programs. Recently, a new approach based on notions of conflict resolutions was proposed, however with declarative semantics only. This paper gives a more procedural counterpart by developing skeptical and credulous argument games for complete semantics and soundness and completeness theorems for both games are provided. After that, distribution of defeasible logic program into several contexts is investigated and both argument games are adapted for multi-context system.\n    ",
        "submission_date": "2014-04-28T00:00:00",
        "last_modified_date": "2014-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6974",
        "title": "Deontic Logic for Human Reasoning",
        "authors": [
            "Ulrich Furbach",
            "Claudia Schon"
        ],
        "abstract": "Deontic logic is shown to be applicable for modelling human reasoning. For this the Wason selection task and the suppression task are discussed in detail. Different versions of modelling norms with deontic logic are introduced and in the case of the Wason selection task it is demonstrated how differences in the performance of humans in the abstract and in the social contract case can be explained. Furthermore it is shown that an automated theorem prover can be used as a reasoning tool for deontic logic.\n    ",
        "submission_date": "2014-04-28T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6999",
        "title": "Preliminary Report on WASP 2.0",
        "authors": [
            "Mario Alviano",
            "Carmine Dodaro",
            "Francesco Ricca"
        ],
        "abstract": "Answer Set Programming (ASP) is a declarative programming paradigm. The intrinsic complexity of the evaluation of ASP programs makes the development of more effective and faster systems a challenging research topic. This paper reports on the recent improvements of the ASP solver WASP. WASP is undergoing a refactoring process which will end up in the release of a new and more performant version of the software. In particular the paper focus on the improvements to the core evaluation algorithms working on normal programs. A preliminary experiment on benchmarks from the 3rd ASP competition belonging to the NP class is reported. The previous version of WASP was often not competitive with alternative solutions on this class. The new version of WASP shows a substantial increase in performance.\n    ",
        "submission_date": "2014-04-28T00:00:00",
        "last_modified_date": "2014-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7173",
        "title": "Nonmonotonic Reasoning as a Temporal Activity",
        "authors": [
            "Daniel G. Schwartz"
        ],
        "abstract": "A {\\it dynamic reasoning system} (DRS) is an adaptation of a conventional formal logical system that explicitly portrays reasoning as a temporal activity, with each extralogical input to the system and each inference rule application being viewed as occurring at a distinct time step. Every DRS incorporates some well-defined logic together with a controller that serves to guide the reasoning process in response to user inputs. Logics are generic, whereas controllers are application-specific. Every controller does, nonetheless, provide an algorithm for nonmonotonic belief revision. The general notion of a DRS comprises a framework within which one can formulate the logic and algorithms for a given application and prove that the algorithms are correct, i.e., that they serve to (i) derive all salient information and (ii) preserve the consistency of the belief set. This paper illustrates the idea with ordinary first-order predicate calculus, suitably modified for the present purpose, and an example. The example revisits some classic nonmonotonic reasoning puzzles (Opus the Penguin, Nixon Diamond) and shows how these can be resolved in the context of a DRS, using an expanded version of first-order logic that incorporates typed predicate symbols. All concepts are rigorously defined and effectively computable, thereby providing the foundation for a future software implementation.\n    ",
        "submission_date": "2014-04-28T00:00:00",
        "last_modified_date": "2014-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7205",
        "title": "Generalizing Modular Logic Programs",
        "authors": [
            "Jo\u00e3o Moura",
            "Carlos Dam\u00e1sio"
        ],
        "abstract": "Even though modularity has been studied extensively in conventional logic programming, there are few approaches on how to incorporate modularity into Answer Set Programming, a prominent rule-based declarative programming paradigm. A major approach is Oikarinnen and Janhunen's Gaifman-Shapiro-style architecture of program modules, which provides the composition of program modules. Their module theorem properly strengthens Lifschitz and Turner's splitting set theorem for normal logic programs. However, this approach is limited by module conditions that are imposed in order to ensure the compatibility of their module system with the stable model semantics, namely forcing output signatures of composing modules to be disjoint and disallowing positive cyclic dependencies between different modules. These conditions turn out to be too restrictive in practice and in this paper we discuss alternative ways of lift both restrictions independently, effectively solving the first, widening the applicability of this framework and the scope of the module theorem.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7279",
        "title": "Assessing the players'performance in the game of bridge: A fuzzy logic approach",
        "authors": [
            "Michael Gr. Voskoglou"
        ],
        "abstract": "Contract bridge occupies nowadays a position of great prestige being, together with chess, the only mind games officially recognized by the International Olympic Committee. In the present paper an innovative method for assessing the total performance of bridge- players' belonging to groups of special interest(e.g. different bridge clubs during a tournament, men and women, new and old players, etc) is introduced, which is based on principles of fuzzy logic. For this, the cohorts under assessment are represented as fuzzy subsets of a set of linguistic labels characterizing their performance and the centroid defuzzification method is used to convert the fuzzy data collected from the game to a crisp number. This new method of assessment could be used informally as a complement of the official bridge-scoring methods for statistical and other obvious reasons. Two real applications related to simultaneous tournaments with pre-dealt boards, organized by the Hellenic Bridge Federation, are also presented, illustrating the importance of our results in practice.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7428",
        "title": "Analysis of Dialogical Argumentation via Finite State Machines",
        "authors": [
            "Anthony Hunter"
        ],
        "abstract": "Dialogical argumentation is an important cognitive activity by which agents exchange arguments and counterarguments as part of some process such as discussion, debate, persuasion and negotiation. Whilst numerous formal systems have been proposed, there is a lack of frameworks for implementing and evaluating these proposals. First-order executable logic has been proposed as a general framework for specifying and analysing dialogical argumentation. In this paper, we investigate how we can implement systems for dialogical argumentation using propositional executable logic. Our approach is to present and evaluate an algorithm that generates a finite state machine that reflects a propositional executable logic specification for a dialogical argumentation together with an initial state. We also consider how the finite state machines can be analysed, with the minimax strategy being used as an illustration of the kinds of empirical analysis that can be undertaken.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7541",
        "title": "An Approach to Forgetting in Disjunctive Logic Programs that Preserves Strong Equivalence",
        "authors": [
            "James P. Delgrande",
            "Kewen Wang"
        ],
        "abstract": "In this paper we investigate forgetting in disjunctive logic programs, where forgetting an atom from a program amounts to a reduction in the signature of that program. The goal is to provide an approach that is syntax-independent, in that if two programs are strongly equivalent, then the results of forgetting an atom in each program should also be strongly equivalent. Our central definition of forgetting is impractical but satisfies this goal: Forgetting an atom is characterised by the set of SE consequences of the program that do not mention the atom to be forgotten. We then provide an equivalent, practical definition, wherein forgetting an atom $p$ is given by those rules in the program that don't mention $p$, together with rules obtained by a single inference step from rules that do mention $p$. Forgetting is shown to have appropriate properties; as well, the finite characterisation results in a modest (at worst quadratic) blowup. Finally we have also obtained a prototype implementation of this approach to forgetting.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7719",
        "title": "An argumentation system for reasoning with conflict-minimal paraconsistent ALC",
        "authors": [
            "Wenzhao Qiao",
            "Nico Roos"
        ],
        "abstract": "The semantic web is an open and distributed environment in which it is hard to guarantee consistency of knowledge and information. Under the standard two-valued semantics everything is entailed if knowledge and information is inconsistent. The semantics of the paraconsistent logic LP offers a solution. However, if the available knowledge and information is consistent, the set of conclusions entailed under the three-valued semantics of the paraconsistent logic LP is smaller than the set of conclusions entailed under the two-valued semantics. Preferring conflict-minimal three-valued interpretations eliminates this difference.\n",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7734",
        "title": "Compact Argumentation Frameworks",
        "authors": [
            "Ringo Baumann",
            "Wolfgang Dvor\u00e1k",
            "Thomas Linsbichler",
            "Hannes Strass",
            "Stefan Woltran"
        ],
        "abstract": "Abstract argumentation frameworks (AFs) are one of the most studied formalisms in AI. In this work, we introduce a certain subclass of AFs which we call compact. Given an extension-based semantics, the corresponding compact AFs are characterized by the feature that each argument of the AF occurs in at least one extension. This not only guarantees a certain notion of fairness; compact AFs are thus also minimal in the sense that no argument can be removed without changing the outcome. We address the following questions in the paper: (1) How are the classes of compact AFs related for different semantics? (2) Under which circumstances can AFs be transformed into equivalent compact ones? (3) Finally, we show that compact AFs are indeed a non-trivial subclass, since the verification problem remains coNP-hard for certain semantics.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0034",
        "title": "Belief Revision and Trust",
        "authors": [
            "Aaron Hunter"
        ],
        "abstract": "Belief revision is the process in which an agent incorporates a new piece of information together with a pre-existing set of beliefs. When the new information comes in the form of a report from another agent, then it is clear that we must first determine whether or not that agent should be trusted. In this paper, we provide a formal approach to modeling trust as a pre-processing step before belief revision. We emphasize that trust is not simply a relation between agents; the trust that one agent has in another is often restricted to a particular domain of expertise. We demonstrate that this form of trust can be captured by associating a state-partition with each agent, then relativizing all reports to this state partition before performing belief revision. In this manner, we incorporate only the part of a report that falls under the perceived domain of expertise of the reporting agent. Unfortunately, state partitions based on expertise do not allow us to compare the relative strength of trust held with respect to different agents. To address this problem, we introduce pseudometrics over states to represent differing degrees of trust. This allows us to incorporate simultaneous reports from multiple agents in a way that ensures the most trusted reports will be believed.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0054",
        "title": "LTLf and LDLf Monitoring: A Technical Report",
        "authors": [
            "Giuseppe De Giacomo",
            "Riccardo De Masellis",
            "Marco Grasso",
            "Fabrizio Maggi",
            "Marco Montali"
        ],
        "abstract": "Runtime monitoring is one of the central tasks to provide operational decision support to running business processes, and check on-the-fly whether they comply with constraints and rules. We study runtime monitoring of properties expressed in LTL on finite traces (LTLf) and in its extension LDLf. LDLf is a powerful logic that captures all monadic second order logic on finite traces, which is obtained by combining regular expressions and LTLf, adopting the syntax of propositional dynamic logic (PDL). Interestingly, in spite of its greater expressivity, LDLf has exactly the same computational complexity of LTLf. We show that LDLf is able to capture, in the logic itself, not only the constraints to be monitored, but also the de-facto standard RV-LTL monitors. This makes it possible to declaratively capture monitoring metaconstraints, and check them by relying on usual logical services instead of ad-hoc algorithms. This, in turn, enables to flexibly monitor constraints depending on the monitoring state of other constraints, e.g., \"compensation\" constraints that are only checked when others are detected to be violated. In addition, we devise a direct translation of LDLf formulas into nondeterministic automata, avoiding to detour to Buechi automata or alternating automata, and we use it to implement a monitoring plug-in for the PROM suite.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0406",
        "title": "Extension-based Semantics of Abstract Dialectical Frameworks",
        "authors": [
            "Sylwia Polberg"
        ],
        "abstract": "One of the most prominent tools for abstract argumentation is the Dung's framework, AF for short. It is accompanied by a variety of semantics including grounded, complete, preferred and stable. Although powerful, AFs have their shortcomings, which led to development of numerous enrichments. Among the most general ones are the abstract dialectical frameworks, also known as the ADFs. They make use of the so-called acceptance conditions to represent arbitrary relations. This level of abstraction brings not only new challenges, but also requires addressing existing problems in the field. One of the most controversial issues, recognized not only in argumentation, concerns the support cycles. In this paper we introduce a new method to ensure acyclicity of the chosen arguments and present a family of extension-based semantics built on it. We also continue our research on the semantics that permit cycles and fill in the gaps from the previous works. Moreover, we provide ADF versions of the properties known from the Dung setting. Finally, we also introduce a classification of the developed sub-semantics and relate them to the existing labeling-based approaches.\n    ",
        "submission_date": "2014-05-02T00:00:00",
        "last_modified_date": "2014-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0423",
        "title": "Representation of a Sentence using a Polar Fuzzy Neutrosophic Semantic Net",
        "authors": [
            "Sachin Lakra",
            "T.V. Prasad",
            "G. Ramakrishna"
        ],
        "abstract": "A semantic net can be used to represent a sentence. A sentence in a language contains semantics which are polar in nature, that is, semantics which are positive, neutral and negative. Neutrosophy is a relatively new field of science which can be used to mathematically represent triads of concepts. These triads include truth, indeterminacy and falsehood, and so also positivity, neutrality and negativity. Thus a conventional semantic net has been extended in this paper using neutrosophy into a Polar Fuzzy Neutrosophic Semantic Net. A Polar Fuzzy Neutrosophic Semantic Net has been implemented in MATLAB and has been used to illustrate a polar sentence in English language. The paper demonstrates a method for the representation of polarity in a computers memory. Thus, polar concepts can be applied to imbibe a machine such as a robot, with emotions, making machine emotion representation possible.\n    ",
        "submission_date": "2014-05-02T00:00:00",
        "last_modified_date": "2014-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0446",
        "title": "Solving the undirected feedback vertex set problem by local search",
        "authors": [
            "Shao-Meng Qin",
            "Hai-Jun Zhou"
        ],
        "abstract": "An undirected graph consists of a set of vertices and a set of undirected edges between vertices. Such a graph may contain an abundant number of cycles, then a feedback vertex set (FVS) is a set of vertices intersecting with each of these cycles. Constructing a FVS of cardinality approaching the global minimum value is a optimization problem in the nondeterministic polynomial-complete complexity class, therefore it might be extremely difficult for some large graph instances. In this paper we develop a simulated annealing local search algorithm for the undirected FVS problem. By defining an order for the vertices outside the FVS, we replace the global cycle constraints by a set of local vertex constraints on this order. Under these local constraints the cardinality of the focal FVS is then gradually reduced by the simulated annealing dynamical process. We test this heuristic algorithm on large instances of Er\u00f6dos-Renyi random graph and regular random graph, and find that this algorithm is comparable in performance to the belief propagation-guided decimation algorithm.\n    ",
        "submission_date": "2014-05-01T00:00:00",
        "last_modified_date": "2014-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0546",
        "title": "Kaggle LSHTC4 Winning Solution",
        "authors": [
            "Antti Puurula",
            "Jesse Read",
            "Albert Bifet"
        ],
        "abstract": "Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores. This document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package. A package omitting precomputed result files is also distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies.\n    ",
        "submission_date": "2014-05-03T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0720",
        "title": "Probabilistic Inductive Logic Programming Based on Answer Set Programming",
        "authors": [
            "Matthias Nickles",
            "Alessandra Mileo"
        ],
        "abstract": "We propose a new formal language for the expressive representation of probabilistic knowledge based on Answer Set Programming (ASP). It allows for the annotation of first-order formulas as well as ASP rules and facts with probabilities and for learning of such weights from data (parameter estimation). Weighted formulas are given a semantics in terms of soft and hard constraints which determine a probability distribution over answer sets. In contrast to related approaches, we approach inference by optionally utilizing so-called streamlining XOR constraints, in order to reduce the number of computed answer sets. Our approach is prototypically implemented. Examples illustrate the introduced concepts and point at issues and topics for future research.\n    ",
        "submission_date": "2014-05-04T00:00:00",
        "last_modified_date": "2014-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0795",
        "title": "Belief revision in the propositional closure of a qualitative algebra (extended version)",
        "authors": [
            "Valmi Dufour-Lussier",
            "Alice Hermann",
            "Florence Le Ber",
            "Jean Lieber"
        ],
        "abstract": "Belief revision is an operation that aims at modifying old beliefs so that they become consistent with new ones. The issue of belief revision has been studied in various formalisms, in particular, in qualitative algebras (QAs) in which the result is a disjunction of belief bases that is not necessarily representable in a QA. This motivates the study of belief revision in formalisms extending QAs, namely, their propositional closures: in such a closure, the result of belief revision belongs to the formalism. Moreover, this makes it possible to define a contraction operator thanks to the Harper identity. Belief revision in the propositional closure of QAs is studied, an algorithm for a family of revision operators is designed, and an open-source implementation is made freely available on the web. (This is the extended version of an article originally presented at the 14th International Conference on Principles of Knowledge Representation and Reasoning.)\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0805",
        "title": "On the Relative Expressiveness of Argumentation Frameworks, Normal Logic Programs and Abstract Dialectical Frameworks",
        "authors": [
            "Hannes Strass"
        ],
        "abstract": "We analyse the expressiveness of the two-valued semantics of abstract argumentation frameworks, normal logic programs and abstract dialectical frameworks. By expressiveness we mean the ability to encode a desired set of two-valued interpretations over a given propositional signature using only atoms from that signature. While the computational complexity of the two-valued model existence problem for all these languages is (almost) the same, we show that the languages form a neat hierarchy with respect to their expressiveness.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0809",
        "title": "Implementing Default and Autoepistemic Logics via the Logic of GK",
        "authors": [
            "Jianmin Ji",
            "Hannes Strass"
        ],
        "abstract": "The logic of knowledge and justified assumptions, also known as logic of grounded knowledge (GK), was proposed by Lin and Shoham as a general logic for nonmonotonic reasoning. To date, it has been used to embed in it default logic (propositional case), autoepistemic logic, Turner's logic of universal causation, and general logic programming under stable model semantics. Besides showing the generality of GK as a logic for nonmonotonic reasoning, these embeddings shed light on the relationships among these other logics. In this paper, for the first time, we show how the logic of GK can be embedded into disjunctive logic programming in a polynomial but non-modular translation with new variables. The result can then be used to compute the extension/expansion semantics of default logic, autoepistemic logic and Turner's logic of universal causation by disjunctive ASP solvers such as claspD(-2), DLV, GNT and cmodels.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0868",
        "title": "Finding Inner Outliers in High Dimensional Space",
        "authors": [
            "Zhana Bao"
        ],
        "abstract": "Outlier detection in a large-scale database is a significant and complex issue in knowledge discovering field. As the data distributions are obscure and uncertain in high dimensional space, most existing solutions try to solve the issue taking into account the two intuitive points: first, outliers are extremely far away from other points in high dimensional space; second, outliers are detected obviously different in projected-dimensional subspaces. However, for a complicated case that outliers are hidden inside the normal points in all dimensions, existing detection methods fail to find such inner outliers. In this paper, we propose a method with twice dimension-projections, which integrates primary subspace outlier detection and secondary point-projection between subspaces, and sums up the multiple weight values for each point. The points are computed with local density ratio separately in twice-projected dimensions. After the process, outliers are those points scoring the largest values of weight. The proposed method succeeds to find all inner outliers on the synthetic test datasets with the dimension varying from 100 to 10000. The experimental results also show that the proposed algorithm can work in low dimensional space and can achieve perfect performance in high dimensional space. As for this reason, our proposed approach has considerable potential to apply it in multimedia applications helping to process images or video with large-scale attributes.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0869",
        "title": "Robust Subspace Outlier Detection in High Dimensional Space",
        "authors": [
            "Zhana Bao"
        ],
        "abstract": "Rare data in a large-scale database are called outliers that reveal significant information in the real world. The subspace-based outlier detection is regarded as a feasible approach in very high dimensional space. However, the outliers found in subspaces are only part of the true outliers in high dimensional space, indeed. The outliers hidden in normal-clustered points are sometimes neglected in the projected dimensional subspace. In this paper, we propose a robust subspace method for detecting such inner outliers in a given dataset, which uses two dimensional-projections: detecting outliers in subspaces with local density ratio in the first projected dimensions; finding outliers by comparing neighbor's positions in the second projected dimensions. Each point's weight is calculated by summing up all related values got in the two steps projected dimensions, and then the points scoring the largest weight values are taken as outliers. By taking a series of experiments with the number of dimensions from 10 to 10000, the results show that our proposed method achieves high precision in the case of extremely high dimensional space, and works well in low dimensional space.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0876",
        "title": "The Multi-engine ASP Solver ME-ASP: Progress Report",
        "authors": [
            "Marco Maratea",
            "Luca Pulina",
            "Francesco Ricca"
        ],
        "abstract": "MEASP is a multi-engine solver for ground ASP programs. It exploits algorithm selection techniques based on classification to select one among a set of out-of-the-box heterogeneous ASP solvers used as black-box engines. In this paper we report on (i) a new optimized implementation of MEASP; and (ii) an attempt of applying algorithm selection to non-ground programs. An experimental analysis reported in the paper shows that (i) the new implementation of \\measp is substantially faster than the previous version; and (ii) the multi-engine recipe can be applied to the evaluation of non-ground programs with some benefits.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0915",
        "title": "Reasoning with Probabilistic Logics",
        "authors": [
            "Riccardo Zese"
        ],
        "abstract": "The interest in the combination of probability with logics for modeling the world has rapidly increased in the last few years. One of the most effective approaches is the Distribution Semantics which was adopted by many logic programming languages and in Descripion Logics. In this paper, we illustrate the work we have done in this research field by presenting a probabilistic semantics for description logics and reasoning and learning algorithms. In particular, we present in detail the system TRILL P, which computes the probability of queries w.r.t. probabilistic knowledge bases, which has been implemented in Prolog. Note: An extended abstract / full version of a paper accepted to be presented at the Doctoral Consortium of the 30th International Conference on Logic Programming (ICLP 2014), July 19-22, Vienna, Austria\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2015-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0936",
        "title": "Design Of Fuzzy Logic Traffic Controller For Isolated Intersections With Emergency Vehicle Priority System Using MATLAB Simulation",
        "authors": [
            "Mohit Jha",
            "Shailja Shukla"
        ],
        "abstract": "Traffic is the chief puzzle problem which every country faces because of the enhancement in number of vehicles throughout the world, especially in large urban towns. Hence the need arises for simulating and optimizing traffic control algorithms to better accommodate this increasing demand. Fuzzy optimization deals with finding the values of input parameters of a complex simulated system which result in desired output. This paper presents a MATLAB simulation of fuzzy logic traffic controller for controlling flow of traffic in isolated intersections. This controller is based on the waiting time and queue length of vehicles at present green phase and vehicles queue lengths at the other phases. The controller controls the traffic light timings and phase difference to ascertain sebaceous flow of traffic with least waiting time and queue length. In this paper, the isolated intersection model used consists of two alleyways in each approach. Every outlook has different value of queue length and waiting time, systematically, at the intersection. The maximum value of waiting time and vehicle queue length has to be selected by using proximity sensors as inputs to controller for the ameliorate control traffic flow at the intersection. An intelligent traffic model and fuzzy logic traffic controller are developed to evaluate the performance of traffic controller under different pre-defined conditions for oleaginous flow of traffic. Additionally, this fuzzy logic traffic controller has emergency vehicle siren sensors which detect emergency vehicle movement like ambulance, fire brigade, Police Van etc. and gives maximum priority to him and pass preferred signal to it.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0941",
        "title": "Towards a Benchmark of Natural Language Arguments",
        "authors": [
            "Elena Cabrio",
            "Serena Villata"
        ],
        "abstract": "The connections among natural language processing and argumentation theory are becoming stronger in the latest years, with a growing amount of works going in this direction, in different scenarios and applying heterogeneous techniques. In this paper, we present two datasets we built to cope with the combination of the Textual Entailment framework and bipolar abstract argumentation. In our approach, such datasets are used to automatically identify through a Textual Entailment system the relations among the arguments (i.e., attack, support), and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0961",
        "title": "Do we need Asimov's Laws?",
        "authors": [
            "Ulrike Barthelmess",
            "Ulrich Furbach"
        ],
        "abstract": "In this essay the stance on robots is discussed. The attitude against robots in history, starting in Ancient Greek culture until the industrial revolution is described. The uncanny valley and some possible explanations are given. Some differences in Western and Asian understanding of robots are listed and finally we answer the question raised with the title.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0999",
        "title": "KR$^3$: An Architecture for Knowledge Representation and Reasoning in Robotics",
        "authors": [
            "Shiqi Zhang",
            "Mohan Sridharan",
            "Michael Gelfond",
            "Jeremy Wyatt"
        ],
        "abstract": "This paper describes an architecture that combines the complementary strengths of declarative programming and probabilistic graphical models to enable robots to represent, reason with, and learn from, qualitative and quantitative descriptions of uncertainty and knowledge. An action language is used for the low-level (LL) and high-level (HL) system descriptions in the architecture, and the definition of recorded histories in the HL is expanded to allow prioritized defaults. For any given goal, tentative plans created in the HL using default knowledge and commonsense reasoning are implemented in the LL using probabilistic algorithms, with the corresponding observations used to update the HL history. Tight coupling between the two levels enables automatic selection of relevant variables and generation of suitable action policies in the LL for each HL action, and supports reasoning with violation of defaults, noisy observations and unreliable actions in large and complex domains. The architecture is evaluated in simulation and on physical robots transporting objects in indoor domains; the benefit on robots is a reduction in task execution time of 39% compared with a purely probabilistic, but still hierarchical, approach.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1027",
        "title": "K-NS: Section-Based Outlier Detection in High Dimensional Space",
        "authors": [
            "Zhana Bao"
        ],
        "abstract": "Finding rare information hidden in a huge amount of data from the Internet is a necessary but complex issue. Many researchers have studied this issue and have found effective methods to detect anomaly data in low dimensional space. However, as the dimension increases, most of these existing methods perform poorly in detecting outliers because of \"high dimensional curse\". Even though some approaches aim to solve this problem in high dimensional space, they can only detect some anomaly data appearing in low dimensional space and cannot detect all of anomaly data which appear differently in high dimensional space. To cope with this problem, we propose a new k-nearest section-based method (k-NS) in a section-based space. Our proposed approach not only detects outliers in low dimensional space with section-density ratio but also detects outliers in high dimensional space with the ratio of k-nearest section against average value. After taking a series of experiments with the dimension from 10 to 10000, the experiment results show that our proposed method achieves 100% precision and 100% recall result in the case of extremely high dimensional space, and better improvement in low dimensional space compared to our previously proposed method.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1071",
        "title": "Revisiting Chase Termination for Existential Rules and their Extension to Nonmonotonic Negation",
        "authors": [
            "Jean-Fran\u00e7ois Baget",
            "Fabien Garreau",
            "Marie-Laure Mugnier",
            "Swan Rocher"
        ],
        "abstract": "Existential rules have been proposed for representing ontological knowledge, specifically in the context of Ontology- Based Data Access. Entailment with existential rules is undecidable. We focus in this paper on conditions that ensure the termination of a breadth-first forward chaining algorithm known as the chase. Several variants of the chase have been proposed. In the first part of this paper, we propose a new tool that allows to extend existing acyclicity conditions ensuring chase termination, while keeping good complexity properties. In the second part, we study the extension to existential rules with nonmonotonic negation under stable model semantics, discuss the relevancy of the chase variants for these rules and further extend acyclicity results obtained in the positive case.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1124",
        "title": "An ASP-Based Architecture for Autonomous UAVs in Dynamic Environments: Progress Report",
        "authors": [
            "Marcello Balduccini",
            "William C. Regli",
            "Duc N. Nguyen"
        ],
        "abstract": "Traditional AI reasoning techniques have been used successfully in many domains, including logistics, scheduling and game playing. This paper is part of a project aimed at investigating how such techniques can be extended to coordinate teams of unmanned aerial vehicles (UAVs) in dynamic environments. Specifically challenging are real-world environments where UAVs and other network-enabled devices must communicate to coordinate---and communication actions are neither reliable nor free. Such network-centric environments are common in military, public safety and commercial applications, yet most research (even multi-agent planning) usually takes communications among distributed agents as a given. We address this challenge by developing an agent architecture and reasoning algorithms based on Answer Set Programming (ASP). ASP has been chosen for this task because it enables high flexibility of representation, both of knowledge and of reasoning tasks. Although ASP has been used successfully in a number of applications, and ASP-based architectures have been studied for about a decade, to the best of our knowledge this is the first practical application of a complete ASP-based agent architecture. It is also the first practical application of ASP involving a combination of centralized reasoning, decentralized reasoning, execution monitoring, and reasoning about network communications. This work has been empirically validated using a distributed network-centric software evaluation testbed and the results provide guidance to designers in how to understand and control intelligent systems that operate in these environments.\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1183",
        "title": "Some thoughts about benchmarks for NMR",
        "authors": [
            "Daniel Le Berre"
        ],
        "abstract": "The NMR community would like to build a repository of benchmarks to push forward the design of systems implementing NMR as it has been the case for many other areas in AI. There are a number of lessons which can be learned from the experience of other communi- ties. Here are a few thoughts about the requirements and choices to make before building such a repository.\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1287",
        "title": "Semantics and Compilation of Answer Set Programming with Generalized Atoms",
        "authors": [
            "Mario Alviano",
            "Wolfgang Faber"
        ],
        "abstract": "Answer Set Programming (ASP) is logic programming under the stable model or answer set semantics. During the last decade, this paradigm has seen several extensions by generalizing the notion of atom used in these programs. Among these, there are aggregate atoms, HEX atoms, generalized quantifiers, and abstract constraints. In this paper we refer to these constructs collectively as generalized atoms. The idea common to all of these constructs is that their satisfaction depends on the truth values of a set of (non-generalized) atoms, rather than the truth value of a single (non-generalized) atom. Motivated by several examples, we argue that for some of the more intricate generalized atoms, the previously suggested semantics provide unintuitive results and provide an alternative semantics, which we call supportedly stable or SFLP answer sets. We show that it is equivalent to the major previously proposed semantics for programs with convex generalized atoms, and that it in general admits more intended models than other semantics in the presence of non-convex generalized atoms. We show that the complexity of supportedly stable models is on the second level of the polynomial hierarchy, similar to previous proposals and to stable models of disjunctive logic programs. Given these complexity results, we provide a compilation method that compactly transforms programs with generalized atoms in disjunctive normal form to programs without generalized atoms. Variants are given for the new supportedly stable and the existing FLP semantics, for which a similar compilation technique has not been known so far.\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1397",
        "title": "Analysis Tool for UNL-Based Knowledge Representation",
        "authors": [
            "Shamim Ripon",
            "Aoyan Barua",
            "Mohammad Salah Uddin"
        ],
        "abstract": "The fundamental issue in knowledge representation is to provide a precise definition of the knowledge that they possess in a manner that is independent of procedural considerations, context free and easy to manipulate, exchange and reason about. Knowledge must be accessible to everyone regardless of their native languages. Universal Networking Language (UNL) is a declarative formal language and a generalized form of human language in a machine independent digital platform for defining, recapitulating, amending, storing and dissipating knowledge among people of different affiliations. UNL extracts semantic data from a native language for Interlingua machine translation. This paper presents the development of a graphical tool that incorporates UNL to provide a visual mean to represent the semantic data available in a native text. UNL represents the semantics of a sentence as a conceptual hyper-graph. We translate this information into XML format and create a graph from XML, representing the actual concepts available in the native language\n    ",
        "submission_date": "2014-05-04T00:00:00",
        "last_modified_date": "2014-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1520",
        "title": "claspfolio 2: Advances in Algorithm Selection for Answer Set Programming",
        "authors": [
            "Holger Hoos",
            "Marius Lindauer",
            "Torsten Schaub"
        ],
        "abstract": "To appear in Theory and Practice of Logic Programming (TPLP). Building on the award-winning, portfolio-based ASP solver claspfolio, we present claspfolio 2, a modular and open solver architecture that integrates several different portfolio-based algorithm selection approaches and techniques. The claspfolio 2 solver framework supports various feature generators, solver selection approaches, solver portfolios, as well as solver-schedule-based pre-solving techniques. The default configuration of claspfolio 2 relies on a light-weight version of the ASP solver clasp to generate static and dynamic instance features. The flexible open design of claspfolio 2 is a distinguishing factor even beyond ASP. As such, it provides a unique framework for comparing and combining existing portfolio-based algorithm selection approaches and techniques in a single, unified framework. Taking advantage of this, we conducted an extensive experimental study to assess the impact of different feature sets, selection approaches and base solver portfolios. In addition to gaining substantial insights into the utility of the various approaches and techniques, we identified a default configuration of claspfolio 2 that achieves substantial performance gains not only over clasp's default configuration and the earlier version of claspfolio 2, but also over manually tuned configurations of clasp.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2014-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1524",
        "title": "An expert system for recommending suitable ornamental fish addition to an aquarium based on aquarium condition",
        "authors": [
            "Mohammad Mohammadi",
            "Shahram Jafari"
        ],
        "abstract": "Expert systems prove to be suitable replacement for human experts when human experts are unavailable for different reasons. Various expert system has been developed for wide range of application. Although some expert systems in the field of fishery and aquaculture has been developed but a system that aids user in process of selecting a new addition to their aquarium tank never been designed. This paper proposed an expert system that suggests new addition to an aquarium tank based on current environmental condition of aquarium and currently existing fishes in aquarium. The system suggest the best fit for aquarium condition and most compatible to other fishes in aquarium.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2014-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1544",
        "title": "Transalg: a Tool for Translating Procedural Descriptions of Discrete Functions to SAT",
        "authors": [
            "Ilya Otpuschennikov",
            "Alexander Semenov",
            "Stepan Kochemazov"
        ],
        "abstract": "In this paper we present the Transalg system, designed to produce SAT encodings for discrete functions, written as programs in a specific language. Translation of such programs to SAT is based on propositional encoding methods for formal computing models and on the concept of symbolic execution. We used the Transalg system to make SAT encodings for a number of cryptographic functions.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2015-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1675",
        "title": "Structured Learning Modulo Theories",
        "authors": [
            "Stefano Teso",
            "Roberto Sebastiani",
            "Andrea Passerini"
        ],
        "abstract": "Modelling problems containing a mixture of Boolean and numerical variables is a long-standing interest of Artificial Intelligence. However, performing inference and learning in hybrid domains is a particularly daunting task. The ability to model this kind of domains is crucial in \"learning to design\" tasks, that is, learning applications where the goal is to learn from examples how to perform automatic {\\em de novo} design of novel objects. In this paper we present Structured Learning Modulo Theories, a max-margin approach for learning in hybrid domains based on Satisfiability Modulo Theories, which allows to combine Boolean reasoning and optimization over continuous linear arithmetical constraints. The main idea is to leverage a state-of-the-art generalized Satisfiability Modulo Theory solver for implementing the inference and separation oracles of Structured Output SVMs. We validate our method on artificial and real world scenarios.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2058",
        "title": "Joint Tabling of Logic Program Abductions and Updates",
        "authors": [
            "Ari Saptawijaya",
            "Lu\u00eds Moniz Pereira"
        ],
        "abstract": "Abductive logic programs offer a formalism to declaratively represent and reason about problems in a variety of areas: diagnosis, decision making, hypothetical reasoning, etc. On the other hand, logic program updates allow us to express knowledge changes, be they internal (or self) and external (or world) changes. Abductive logic programs and logic program updates thus naturally coexist in problems that are susceptible to hypothetical reasoning about change. Taking this as a motivation, in this paper we integrate abductive logic programs and logic program updates by jointly exploiting tabling features of logic programming. The integration is based on and benefits from the two implementation techniques we separately devised previously, viz., tabled abduction and incremental tabling for query-driven propagation of logic program updates. A prototype of the integrated system is implemented in XSB Prolog.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2501",
        "title": "Using Tabled Logic Programming to Solve the Petrobras Planning Problem",
        "authors": [
            "Roman Bart\u00e1k",
            "Neng-Fa Zhou"
        ],
        "abstract": "Tabling has been used for some time to improve efficiency of Prolog programs by memorizing answered queries. The same idea can be naturally used to memorize visited states during search for planning. In this paper we present a planner developed in the Picat language to solve the Petrobras planning problem. Picat is a novel Prolog-like language that provides pattern matching, deterministic and non-deterministic rules, and tabling as its core modelling and solving features. We demonstrate these capabilities using the Petrobras problem, where the goal is to plan transport of cargo items from ports to platforms using vessels with limited capacity. Monte Carlo Tree Search has been so far the best technique to tackle this problem and we will show that by using tabling we can achieve much better runtime efficiency and better plan quality.\n    ",
        "submission_date": "2014-05-11T00:00:00",
        "last_modified_date": "2014-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2590",
        "title": "Efficient Computation of the Well-Founded Semantics over Big Data",
        "authors": [
            "Ilias Tachmazidis",
            "Grigoris Antoniou",
            "Wolfgang Faber"
        ],
        "abstract": "Data originating from the Web, sensor readings and social media result in increasingly huge datasets. The so called Big Data comes with new scientific and technological challenges while creating new opportunities, hence the increasing interest in academia and industry. Traditionally, logic programming has focused on complex knowledge structures/programs, so the question arises whether and how it can work in the face of Big Data. In this paper, we examine how the well-founded semantics can process huge amounts of data through mass parallelization. More specifically, we propose and evaluate a parallel approach using the MapReduce framework. Our experimental results indicate that our approach is scalable and that well-founded semantics can be applied to billions of facts. To the best of our knowledge, this is the first work that addresses large scale nonmonotonic reasoning without the restriction of stratification for predicates of arbitrary arity. To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2014-05-11T00:00:00",
        "last_modified_date": "2014-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2600",
        "title": "Learning from networked examples",
        "authors": [
            "Yuyi Wang",
            "Jan Ramon",
            "Zheng-Chu Guo"
        ],
        "abstract": "Many machine learning algorithms are based on the assumption that training examples are drawn independently. However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects, and hence share the features of these shared objects. We show that the classic approach of ignoring this problem potentially can have a harmful effect on the accuracy of statistics, and then consider alternatives. One of these is to only use independent examples, discarding other information. However, this is clearly suboptimal. We analyze sample error bounds in this networked setting, providing significantly improved results. An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities.\n    ",
        "submission_date": "2014-05-11T00:00:00",
        "last_modified_date": "2017-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2664",
        "title": "FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test",
        "authors": [
            "Ji Zhao",
            "Deyu Meng"
        ],
        "abstract": "The maximum mean discrepancy (MMD) is a recently proposed test statistic for two-sample test. Its quadratic time complexity, however, greatly hampers its availability to large-scale applications. To accelerate the MMD calculation, in this study we propose an efficient method called FastMMD. The core idea of FastMMD is to equivalently transform the MMD with shift-invariant kernels into the amplitude expectation of a linear combination of sinusoid components based on Bochner's theorem and Fourier transform (Rahimi & Recht, 2007). Taking advantage of sampling of Fourier transform, FastMMD decreases the time complexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$ are the size and dimension of the sample set, respectively. Here $L$ is the number of basis functions for approximating kernels which determines the approximation accuracy. For kernels that are spherically invariant, the computation can be further accelerated to $O(L N \\log d)$ by using the Fastfood technique (Le et al., 2013). The uniform convergence of our method has also been theoretically proved in both unbiased and biased estimates. We have further provided a geometric explanation for our method, namely ensemble of circular discrepancy, which facilitates us to understand the insight of MMD, and is hopeful to help arouse more extensive metrics for assessing two-sample test. Experimental results substantiate that FastMMD is with similar accuracy as exact MMD, while with faster computation speed and lower variance than the existing MMD approximation methods.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2878",
        "title": "Approximate Policy Iteration Schemes: A Comparison",
        "authors": [
            "Bruno Scherrer"
        ],
        "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\\infty$), and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all algorithms, we describe performance bounds, and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API/API($\\alpha$), but this comes at the cost of a relative---exponential in $\\frac{1}{\\epsilon}$---increase of the number of iterations. 2) PSDP$_\\infty$ enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP$_\\infty$ is proportional to their number of iterations, which may be problematic when the discount factor $\\gamma$ is close to 1 or the approximation error $\\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2883",
        "title": "The Metrics Matter! On the Incompatibility of Different Flavors of Replanning",
        "authors": [
            "Kartik Talamadupula",
            "David E. Smith",
            "Subbarao Kambhampati"
        ],
        "abstract": "When autonomous agents are executing in the real world, the state of the world as well as the objectives of the agent may change from the agent's original model. In such cases, the agent's planning process must modify the plan under execution to make it amenable to the new conditions, and to resume execution. This brings up the replanning problem, and the various techniques that have been proposed to solve it. In all, three main techniques -- based on three different metrics -- have been proposed in prior automated planning work. An open question is whether these metrics are interchangeable; answering this requires a normalized comparison of the various replanning quality metrics. In this paper, we show that it is possible to support such a comparison by compiling all the respective techniques into a single substrate. Using this novel compilation, we demonstrate that these different metrics are not interchangeable, and that they are not good surrogates for each other. Thus we focus attention on the incompatibility of the various replanning flavors with each other, founded in the differences between the metrics that they respectively seek to optimize.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3175",
        "title": "D numbers theory: a generalization of Dempster-Shafer evidence theory",
        "authors": [
            "Yong Deng"
        ],
        "abstract": "Efficient modeling of uncertain information in real world is still an open issue. Dempster-Shafer evidence theory is one of the most commonly used methods. However, the Dempster-Shafer evidence theory has the assumption that the hypothesis in the framework of discernment is exclusive of each other. This condition can be violated in real applications, especially in linguistic decision making since the linguistic variables are not exclusive of each others essentially. In this paper, a new theory, called as D numbers theory (DNT), is systematically developed to address this issue. The combination rule of two D numbers is presented. An coefficient is defined to measure the exclusive degree among the hypotheses in the framework of discernment. The combination rule of two D numbers is presented. If the exclusive coefficient is one which means that the hypothesis in the framework of discernment is exclusive of each other totally, the D combination is degenerated as the classical Dempster combination rule. Finally, a linguistic variables transformation of D numbers is presented to make a decision. A numerical example on linguistic evidential decision making is used to illustrate the efficiency of the proposed D numbers theory.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3218",
        "title": "Lifted Variable Elimination for Probabilistic Logic Programming",
        "authors": [
            "Elena Bellodi",
            "Evelina Lamma",
            "Fabrizio Riguzzi",
            "Vitor Santos Costa",
            "Riccardo Zese"
        ],
        "abstract": "Lifted inference has been proposed for various probabilistic logical frameworks in order to compute the probability of queries in a time that depends on the size of the domains of the random variables rather than the number of instances. Even if various authors have underlined its importance for probabilistic logic programming (PLP), lifted inference has been applied up to now only to relational languages outside of logic programming. In this paper we adapt Generalized Counting First Order Variable Elimination (GC-FOVE) to the problem of computing the probability of queries to probabilistic logic programs under the distribution semantics. In particular, we extend the Prolog Factor Language (PFL) to include two new types of factors that are needed for representing ProbLog programs. These factors take into account the existing causal independence relationships among random variables and are managed by the extension to variable elimination proposed by Zhang and Poole for dealing with convergent variables and heterogeneous factors. Two new operators are added to GC-FOVE for treating heterogeneous factors. The resulting algorithm, called LP$^2$ for Lifted Probabilistic Logic Programming, has been implemented by modifying the PFL implementation of GC-FOVE and tested on three benchmarks for lifted inference. A comparison with PITA and ProbLog2 shows the potential of the approach.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3250",
        "title": "Understanding the Complexity of Lifted Inference and Asymmetric Weighted Model Counting",
        "authors": [
            "Eric Gribkoff",
            "Guy Van den Broeck",
            "Dan Suciu"
        ],
        "abstract": "In this paper we study lifted inference for the Weighted First-Order Model Counting problem (WFOMC), which counts the assignments that satisfy a given sentence in first-order logic (FOL); it has applications in Statistical Relational Learning (SRL) and Probabilistic Databases (PDB). We present several results. First, we describe a lifted inference algorithm that generalizes prior approaches in SRL and PDB. Second, we provide a novel dichotomy result for a non-trivial fragment of FO CNF sentences, showing that for each sentence the WFOMC problem is either in PTIME or #P-hard in the size of the input domain; we prove that, in the first case our algorithm solves the WFOMC problem in PTIME, and in the second case it fails. Third, we present several properties of the algorithm. Finally, we discuss limitations of lifted inference for symmetric probabilistic databases (where the weights of ground literals depend only on the relation name, and not on the constants of the domain), and prove the impossibility of a dichotomy result for the complexity of probabilistic inference for the entire language FOL.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3318",
        "title": "Adaptive Monte Carlo via Bandit Allocation",
        "authors": [
            "James Neufeld",
            "Andr\u00e1s Gy\u00f6rgy",
            "Dale Schuurmans",
            "Csaba Szepesv\u00e1ri"
        ],
        "abstract": "We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3342",
        "title": "An Agent-based Modeling Framework for Sociotechnical Simulation of Water Distribution Contamination Events",
        "authors": [
            "M. Ehsan Shafiee",
            "Emily M. Zechman"
        ],
        "abstract": "In the event that a bacteriological or chemical toxin is intro- duced to a water distribution network, a large population of consumers may become exposed to the contaminant. A contamination event may be poorly predictable dynamic process due to the interactions of consumers and utility managers during an event. Consumers that become aware of a threat may select protective actions that change their water demands from typical demand patterns, and new hydraulic conditions can arise that differ from conditions that are predicted when demands are considered as exogenous inputs. Consequently, the movement of the contaminant plume in the pipe network may shift from its expected trajectory. A sociotechnical model is developed here to integrate agent-based models of consumers with an engineering water distribution system model and capture the dynamics between consumer behaviors and the water distribution system for predicting contaminant transport and public exposure. Consumers are simulated as agents with behaviors defined for water use activities, mobility, word-of-mouth communication, and demand reduction, based on a set of rules representing an agents autonomy and reaction to health impacts, the environment, and the actions of other agents. As consumers decrease their water use, the demand exerted on the water distribution system is updated; as the flow directions and volumes shift in response, the location of the contaminant plume is updated and the amount of contaminant consumed by each agent is calculated. The framework is tested through simulating realistic contamination scenarios for a virtual city and water distribution system.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3362",
        "title": "Grounding Bound Founded Answer Set Programs",
        "authors": [
            "Rehan Abdul Aziz",
            "Geoffrey Chu",
            "Peter James Stuckey"
        ],
        "abstract": "To appear in Theory and Practice of Logic Programming (TPLP)\n",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3367",
        "title": "Bound Founded Answer Set Programming",
        "authors": [
            "Rehan Abdul Aziz"
        ],
        "abstract": "Answer Set Programming (ASP) is a powerful modelling formalism that is very efficient in solving combinatorial problems. ASP solvers implement the stable model semantics that eliminates circular derivations between Boolean variables from the solutions of a logic program. Due to this, ASP solvers are better suited than propositional satisfiability (SAT) and Constraint Programming (CP) solvers to solve a certain class of problems whose specification includes inductive definitions such as reachability in a graph. On the other hand, ASP solvers suffer from the grounding bottleneck that occurs due to their inability to model finite domain variables. Furthermore, the existing stable model semantics are not sufficient to disallow circular reasoning on the bounds of numeric variables. An example where this is required is in modelling shortest paths between nodes in a graph. Just as reachability can be encoded as an inductive definition with one or more base cases and recursive rules, shortest paths between nodes can also be modelled with similar base cases and recursive rules for their upper bounds. This deficiency of stable model semantics introduces another type of grounding bottleneck in ASP systems that cannot be removed by naively merging ASP with CP solvers, but requires a theoretical extension of the semantics from Booleans and normal rules to bounds over numeric variables and more general rules. In this work, we propose Bound Founded Answer Set Programming (BFASP) that resolves this issue and consequently, removes all types of grounding bottleneck inherent in ASP systems.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3376",
        "title": "Probabilistic Argumentation with Epistemic Extensions and Incomplete Information",
        "authors": [
            "Anthony Hunter",
            "Matthias Thimm"
        ],
        "abstract": "Abstract argumentation offers an appealing way of representing and evaluating arguments and counterarguments. This approach can be enhanced by a probability assignment to each argument. There are various interpretations that can be ascribed to this assignment. In this paper, we regard the assignment as denoting the belief that an agent has that an argument is justifiable, i.e., that both the premises of the argument and the derivation of the claim of the argument from its premises are valid. This leads to the notion of an epistemic extension which is the subset of the arguments in the graph that are believed to some degree (which we defined as the arguments that have a probability assignment greater than 0.5). We consider various constraints on the probability assignment. Some constraints correspond to standard notions of extensions, such as grounded or stable extensions, and some constraints give us new kinds of extensions.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3451",
        "title": "Developing Corpus-based Translation Methods between Informal and Formal Mathematics: Project Description",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban",
            "Jiri Vyskocil",
            "Herman Geuvers"
        ],
        "abstract": "The goal of this project is to (i) accumulate annotated informal/formal mathematical corpora suitable for training semi-automated translation between informal and formal mathematics by statistical machine-translation methods, (ii) to develop such methods oriented at the formalization task, and in particular (iii) to combine such methods with learning-assisted automated reasoning that will serve as a strong semantic component. We describe these ideas, the initial set of corpora, and some initial experiments done over them.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3486",
        "title": "ESmodels: An Epistemic Specification Solver",
        "authors": [
            "Zhizheng Zhang",
            "Kaikai Zhao"
        ],
        "abstract": "(To appear in Theory and Practice of Logic Programming (TPLP))\n",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3487",
        "title": "COCOpf: An Algorithm Portfolio Framework",
        "authors": [
            "Petr Baudi\u0161"
        ],
        "abstract": "Algorithm portfolios represent a strategy of composing multiple heuristic algorithms, each suited to a different class of problems, within a single general solver that will choose the best suited algorithm for each input. This approach recently gained popularity especially for solving combinatoric problems, but optimization applications are still emerging. The COCO platform of the BBOB workshop series is the current standard way to measure performance of continuous black-box optimization algorithms.\n",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3539",
        "title": "Pattern Recognition in Narrative: Tracking Emotional Expression in Context",
        "authors": [
            "Fionn Murtagh",
            "Adam Ganz"
        ],
        "abstract": "Using geometric data analysis, our objective is the analysis of narrative, with narrative of emotion being the focus in this work. The following two principles for analysis of emotion inform our work. Firstly, emotion is revealed not as a quality in its own right but rather through interaction. We study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the 3-way relationship of Emma, Charles and Rodolphe in the novel {\\em Madame Bovary}. Secondly, emotion, that is expression of states of mind of subjects, is formed and evolves within the narrative that expresses external events and (personal, social, physical) context. In addition to the analysis methodology with key aspects that are innovative, the input data used is crucial. We use, firstly, dialogue, and secondly, broad and general description that incorporates dialogue. In a follow-on study, we apply our unsupervised narrative mapping to data streams with very low emotional expression. We map the narrative of Twitter streams. Thus we demonstrate map analysis of general narratives.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3546",
        "title": "Anytime Computation of Cautious Consequences in Answer Set Programming",
        "authors": [
            "Mario Alviano",
            "Carmine Dodaro",
            "Francesco Ricca"
        ],
        "abstract": "Query answering in Answer Set Programming (ASP) is usually solved by computing (a subset of) the cautious consequences of a logic program. This task is computationally very hard, and there are programs for which computing cautious consequences is not viable in reasonable time. However, current ASP solvers produce the (whole) set of cautious consequences only at the end of their computation. This paper reports on strategies for computing cautious consequences, also introducing anytime algorithms able to produce sound answers during the computation.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3570",
        "title": "Exchanging Conflict Resolution in an Adaptable Implementation of ACT-R",
        "authors": [
            "Daniel Gall",
            "Thom Fr\u00fchwirth"
        ],
        "abstract": "In computational cognitive science, the cognitive architecture ACT-R is very popular. It describes a model of cognition that is amenable to computer implementation, paving the way for computational psychology. Its underlying psychological theory has been investigated in many psychological experiments, but ACT-R lacks a formal definition of its underlying concepts from a mathematical-computational point of view. Although the canonical implementation of ACT-R is now modularized, this production rule system is still hard to adapt and extend in central components like the conflict resolution mechanism (which decides which of the applicable rules to apply next).\n",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3637",
        "title": "Vicious Circle Principle and Logic Programs with Aggregates",
        "authors": [
            "Michael Gelfond",
            "Yuanlin Zhang"
        ],
        "abstract": "The paper presents a knowledge representation language $\\mathcal{A}log$ which extends ASP with aggregates. The goal is to have a language based on simple syntax and clear intuitive and mathematical semantics. We give some properties of $\\mathcal{A}log$, an algorithm for computing its answer sets, and comparison with other approaches.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3710",
        "title": "The Design of the Fifth Answer Set Programming Competition",
        "authors": [
            "Francesco Calimeri",
            "Martin Gebser",
            "Marco Maratea",
            "Francesco Ricca"
        ],
        "abstract": "Answer Set Programming (ASP) is a well-established paradigm of declarative programming that has been developed in the field of logic programming and nonmonotonic reasoning. Advances in ASP solving technology are customarily assessed in competition events, as it happens for other closely-related problem-solving technologies like SAT/SMT, QBF, Planning and Scheduling. ASP Competitions are (usually) biennial events; however, the Fifth ASP Competition departs from tradition, in order to join the FLoC Olympic Games at the Vienna Summer of Logic 2014, which is expected to be the largest event in the history of logic. This edition of the ASP Competition series is jointly organized by the University of Calabria (Italy), the Aalto University (Finland), and the University of Genova (Italy), and is affiliated with the 30th International Conference on Logic Programming (ICLP 2014). It features a completely re-designed setup, with novelties involving the design of tracks, the scoring schema, and the adherence to a fixed modeling language in order to push the adoption of the ASP-Core-2 standard. Benchmark domains are taken from past editions, and best system packages submitted in 2013 are compared with new versions and solvers.\n",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3713",
        "title": "Contextual Abductive Reasoning with Side-Effects",
        "authors": [
            "Lu\u00eds Moniz Pereira",
            "Emmanuelle-Anna Dietz",
            "Steffen H\u00f6lldobler"
        ],
        "abstract": "The belief bias effect is a phenomenon which occurs when we think that we judge an argument based on our reasoning, but are actually influenced by our beliefs and prior knowledge. Evans, Barston and Pollard carried out a psychological syllogistic reasoning task to prove this effect. Participants were asked whether they would accept or reject a given syllogism. We discuss one specific case which is commonly assumed to be believable but which is actually not logically valid. By introducing abnormalities, abduction and background knowledge, we adequately model this case under the weak completion semantics. Our formalization reveals new questions about possible extensions in abductive reasoning. For instance, observations and their explanations might include some relevant prior abductive contextual information concerning some side-effect or leading to a contestable or refutable side-effect. A weaker notion indicates the support of some relevant consequences by a prior abductive context. Yet another definition describes jointly supported relevant consequences, which captures the idea of two observations containing mutually supportive side-effects. Though motivated with and exemplified by the running psychology application, the various new general abductive context definitions are introduced here and given a declarative semantics for the first time, and have a much wider scope of application. Inspection points, a concept introduced by Pereira and Pinto, allows us to express these definitions syntactically and intertwine them into an operational semantics.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3729",
        "title": "Building a Classification Model for Enrollment In Higher Educational Courses using Data Mining Techniques",
        "authors": [
            "Priyanka Saini"
        ],
        "abstract": "Data Mining is the process of extracting useful patterns from the huge amount of database and many data mining techniques are used for mining these patterns. Recently, one of the remarkable facts in higher educational institute is the rapid growth data and this educational data is expanding quickly without any advantage to the educational management. The main aim of the management is to refine the education standard; therefore by applying the various data mining techniques on this data one can get valuable information. This research study proposed the \"classification model for the student's enrollment process in higher educational courses using data mining techniques\". Additionally, this study contributes to finding some patterns that are meaningful to management.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3790",
        "title": "Transaction Logic with (Complex) Events",
        "authors": [
            "Ana Sofia Gomes",
            "Jos\u00e9 J\u00falio Alferes"
        ],
        "abstract": "This work deals with the problem of combining reactive features, such as the ability to respond to events and define complex events, with the execution of transactions over general Knowledge Bases (KBs).\n",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3824",
        "title": "Multi-Criteria Optimal Planning for Energy Policies in CLP",
        "authors": [
            "Marco Gavanelli",
            "Stefano Bragaglia",
            "Michela Milano",
            "Federico Chesani",
            "Elisa Marengo",
            "Paolo Cagnoli"
        ],
        "abstract": "In the policy making process a number of disparate and diverse issues such as economic development, environmental aspects, as well as the social acceptance of the policy, need to be considered. A single person might not have all the required expertises, and decision support systems featuring optimization components can help to assess policies. Leveraging on previous work on Strategic Environmental Assessment, we developed a fully-fledged system that is able to provide optimal plans with respect to a given objective, to perform multi-objective optimization and provide sets of Pareto optimal plans, and to visually compare them. Each plan is environmentally assessed and its footprint is evaluated. The heart of the system is an application developed in a popular Constraint Logic Programming system on the Reals sort. It has been equipped with a web service module that can be queried through standard interfaces, and an intuitive graphic user interface.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3826",
        "title": "Application of Methods for Syntax Analysis of Context-Free Languages to Query Evaluation of Logic Programs",
        "authors": [
            "Heike Stephan"
        ],
        "abstract": "My research goal is to employ a parser generation algorithm based on the Earley parsing algorithm to the evaluation and compilation of queries to logic programs, especially to deductive databases. By means of partial deduction, from a query to a logic program a parameterized automaton is to be generated that models the evaluation of this query. This automaton can be compiled to executable code; thus we expect a speedup in runtime of query evaluation. An extended abstract/ full version of a paper accepted to be presented at the Doctoral Consortium of the 30th International Conference on Logic Programming (ICLP 2014), July 19-22, Vienna, Austria\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3896",
        "title": "Properties of Stable Model Semantics Extensions",
        "authors": [
            "M\u00e1rio Abrantes",
            "Lu\u00eds Moniz Pereira"
        ],
        "abstract": "The stable model (SM) semantics lacks the properties of existence, relevance and cumulativity. If we prospectively consider the class of conservative extensions of SM semantics (i.e., semantics that for each normal logic program P retrieve a superset of the set of stable models of P), one may wander how do the semantics of this class behave in what concerns the aforementioned properties. That is the type of issue dealt with in this paper. We define a large class of conservative extensions of the SM semantics, dubbed affix stable model semantics, ASM, and study the above referred properties into two non-disjoint subfamilies of the class ASM, here dubbed ASMh and ASMm. From this study a number of results stem which facilitate the assessment of semantics in the class ASMh U ASMm with respect to the properties of existence, relevance and cumulativity, whilst unveiling relations among these properties. As a result of the approach taken in our work, light is shed on the characterization of the SM semantics, as we show that the properties of (lack of) existence and (lack of) cautious monotony are equivalent, which opposes statements on this issue that may be found in the literature; we also characterize the relevance failure of SM semantics in a more clear way than usually stated in the literature.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4138",
        "title": "Empirical Study of Artificial Fish Swarm Algorithm",
        "authors": [
            "Reza Azizi"
        ],
        "abstract": "Artificial fish swarm algorithm (AFSA) is one of the swarm intelligence optimization algorithms that works based on population and stochastic search. In order to achieve acceptable result, there are many parameters needs to be adjusted in AFSA. Among these parameters, visual and step are very significant in view of the fact that artificial fish basically move based on these parameters. In standard AFSA, these two parameters remain constant until the algorithm termination. Large values of these parameters increase the capability of algorithm in global search, while small values improve the local search ability of the algorithm. In this paper, we empirically study the performance of the AFSA and different approaches to balance between local and global exploration have been tested based on the adaptive modification of visual and step during algorithm execution. The proposed approaches have been evaluated based on the four well-known benchmark functions. Experimental results show considerable positive impact on the performance of AFSA.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4180",
        "title": "Algorithm for Adapting Cases Represented in a Tractable Description Logic",
        "authors": [
            "Liang Chang",
            "Uli Sattler",
            "Tianlong Gu"
        ],
        "abstract": "Case-based reasoning (CBR) based on description logics (DLs) has gained a lot of attention lately. Adaptation is a basic task in the CBR inference that can be modeled as the knowledge base revision problem and solved in propositional logic. However, in DLs, it is still a challenge problem since existing revision operators only work well for strictly restricted DLs of the \\emph{DL-Lite} family, and it is difficult to design a revision algorithm which is syntax-independent and fine-grained. In this paper, we present a new method for adaptation based on the DL $\\mathcal{EL_{\\bot}}$. Following the idea of adaptation as revision, we firstly extend the logical basis for describing cases from propositional logic to the DL $\\mathcal{EL_{\\bot}}$, and present a formalism for adaptation based on $\\mathcal{EL_{\\bot}}$. Then we present an adaptation algorithm for this formalism and demonstrate that our algorithm is syntax-independent and fine-grained. Our work provides a logical basis for adaptation in CBR systems where cases and domain knowledge are described by the tractable DL $\\mathcal{EL_{\\bot}}$.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4206",
        "title": "Model revision inference for extensions of first order logic",
        "authors": [
            "Joachim Jansen"
        ],
        "abstract": "I am Joachim Jansen and this is my research summary, part of my application to the Doctoral Consortium at ICLP'14. I am a PhD student in the Knowledge Representation and Reasoning (KRR) research group, a subgroup of the Declarative Languages and Artificial Intelligence (DTAI) group at the department of Computer Science at KU Leuven. I started my PhD in September 2012. My promotor is prof. dr. ir. Gerda Janssens and my co-promotor is prof. dr. Marc Denecker. I can be contacted at ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5048",
        "title": "Towards A Theory-Of-Mind-Inspired Generic Decision-Making Framework",
        "authors": [
            "Mihai Polceanu",
            "C\u00e9dric Buche"
        ],
        "abstract": "Simulation is widely used to make model-based predictions, but few approaches have attempted this technique in dynamic physical environments of medium to high complexity or in general contexts. After an introduction to the cognitive science concepts from which this work is inspired and the current development in the use of simulation as a decision-making technique, we propose a generic framework based on theory of mind, which allows an agent to reason and perform actions using multiple simulations of automatically created or externally inputted models of the perceived environment. A description of a partial implementation is given, which aims to solve a popular game within the IJCAI2013 AIBirds contest. Results of our approach are presented, in comparison with the competition benchmark. Finally, future developments regarding the framework are discussed.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5066",
        "title": "An optimization algorithm inspired by the States of Matter that improves the balance between exploration and exploitation",
        "authors": [
            "Erik Cuevas",
            "Alonso Echavarria",
            "Marte A. Ramirez-Ortegon"
        ],
        "abstract": "The ability of an Evolutionary Algorithm (EA) to find a global optimal solution depends on its capacity to find a good rate between exploitation of found so far elements and exploration of the search space. Inspired by natural phenomena, researchers have developed many successful evolutionary algorithms which, at original versions, define operators that mimic the way nature solves complex problems, with no actual consideration of the exploration/exploitation balance. In this paper, a novel nature-inspired algorithm called the States of Matter Search (SMS) is introduced. The SMS algorithm is based on the simulation of the states of matter phenomenon. In SMS, individuals emulate molecules which interact to each other by using evolutionary operations which are based on the physical principles of the thermal-energy motion mechanism. The algorithm is devised by considering each state of matter at one different exploration/exploitation ratio. The evolutionary process is divided into three phases which emulate the three states of matter: gas, liquid and solid. In each state, molecules (individuals) exhibit different movement capacities. Beginning from the gas state (pure exploration), the algorithm modifies the intensities of exploration and exploitation until the solid state (pure exploitation) is reached. As a result, the approach can substantially improve the balance between exploration/exploitation, yet preserving the good search capabilities of an evolutionary approach.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5172",
        "title": "Opposition Based ElectromagnetismLike for Global Optimization",
        "authors": [
            "Erik Cuevas",
            "Diego Oliva",
            "Daniel Zaldivar",
            "Marco Perez",
            "Gonzalo Pajares"
        ],
        "abstract": "Electromagnetismlike Optimization (EMO) is a global optimization algorithm, particularly well suited to solve problems featuring nonlinear and multimodal cost functions. EMO employs searcher agents that emulate a population of charged particles which interact to each other according to electromagnetisms laws of attraction and repulsion. However, EMO usually requires a large number of iterations for a local search procedure; any reduction or cancelling over such number, critically perturb other issues such as convergence, exploration, population diversity and accuracy. This paper presents an enhanced EMO algorithm called OBEMO, which employs the Opposition-Based Learning (OBL) approach to accelerate the global convergence speed. OBL is a machine intelligence strategy which considers the current candidate solution and its opposite value at the same time, achieving a faster exploration of the search space. The proposed OBEMO method significantly reduces the required computational effort yet avoiding any detriment to the good search capabilities of the original EMO algorithm. Experiments are conducted over a comprehensive set of benchmark functions, showing that OBEMO obtains promising performance for most of the discussed test problems.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5358",
        "title": "Off-Policy Shaping Ensembles in Reinforcement Learning",
        "authors": [
            "Anna Harutyunyan",
            "Tim Brys",
            "Peter Vrancx",
            "Ann Nowe"
        ],
        "abstract": "Recent advances of gradient temporal-difference methods allow to learn off-policy multiple value functions in parallel with- out sacrificing convergence guarantees or computational efficiency. This opens up new possibilities for sound ensemble techniques in reinforcement learning. In this work we propose learning an ensemble of policies related through potential-based shaping rewards. The ensemble induces a combination policy by using a voting mechanism on its components. Learning happens in real time, and we empirically show the combination policy to outperform the individual policies of the ensemble.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5443",
        "title": "Towards an ASP-Based Architecture for Autonomous UAVs in Dynamic Environments (Extended Abstract)",
        "authors": [
            "Marcello Balduccini",
            "William C. Regli",
            "Duc N. Nguyen"
        ],
        "abstract": "Traditional AI reasoning techniques have been used successfully in many domains, including logistics, scheduling and game playing. This paper is part of a project aimed at investigating how such techniques can be extended to coordinate teams of unmanned aerial vehicles (UAVs) in dynamic environments. Specifically challenging are real-world environments where UAVs and other network-enabled devices must communicate to coordinate -- and communication actions are neither reliable nor free. Such network-centric environments are common in military, public safety and commercial applications, yet most research (even multi-agent planning) usually takes communications among distributed agents as a given. We address this challenge by developing an agent architecture and reasoning algorithms based on Answer Set Programming (ASP). Although ASP has been used successfully in a number of applications, to the best of our knowledge this is the first practical application of a complete ASP-based agent architecture. It is also the first practical application of ASP involving a combination of centralized reasoning, decentralized reasoning, execution monitoring, and reasoning about network communications.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5459",
        "title": "Projective simulation applied to the grid-world and the mountain-car problem",
        "authors": [
            "Alexey A. Melnikov",
            "Adi Makmal",
            "Hans J. Briegel"
        ],
        "abstract": "We study the model of projective simulation (PS) which is a novel approach to artificial intelligence (AI). Recently it was shown that the PS agent performs well in a number of simple task environments, also when compared to standard models of reinforcement learning (RL). In this paper we study the performance of the PS agent further in more complicated scenarios. To that end we chose two well-studied benchmarking problems, namely the \"grid-world\" and the \"mountain-car\" problem, which challenge the model with large and continuous input space. We compare the performance of the PS agent model with those of existing models and show that the PS agent exhibits competitive performance also in such scenarios.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5643",
        "title": "Interactive Reference Point-Based Guided Local Search for the Bi-objective Inventory Routing Problem",
        "authors": [
            "Sandra Huber",
            "Martin Josef Geiger",
            "Marc Sevaux"
        ],
        "abstract": "Eliciting preferences of a decision maker is a key factor to successfully combine search and decision making in an interactive method. Therefore, the progressively integration and simulation of the decision maker is a main concern in an application. We contribute in this direction by proposing an interactive method based on a reference point-based guided local search to the bi-objective Inventory Routing Problem. A local search metaheuristic, working on the delivery intervals, and the Clarke & Wright savings heuristic is employed for the subsequently obtained Vehicle Routing Problem. To elicit preferences, the decision maker selects a reference point to guide the search in interesting subregions. Additionally, the reference point is used as a reservation point to discard solutions outside the cone, introduced as a convergence criterion. Computational results of the reference point-based guided local search are reported and analyzed on benchmark data in order to show the applicability of the approach.\n    ",
        "submission_date": "2014-05-22T00:00:00",
        "last_modified_date": "2014-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5646",
        "title": "Mathematical Programming Strategies for Solving the Minimum Common String Partition Problem",
        "authors": [
            "Christian Blum",
            "Jos\u00e9 A. Lozano",
            "Pedro Pinacho Davidson"
        ],
        "abstract": "The minimum common string partition problem is an NP-hard combinatorial optimization problem with applications in computational biology. In this work we propose the first integer linear programming model for solving this problem. Moreover, on the basis of the integer linear programming model we develop a deterministic 2-phase heuristic which is applicable to larger problem instances. The results show that provenly optimal solutions can be obtained for problem instances of small and medium size from the literature by solving the proposed integer linear programming model with CPLEX. Furthermore, new best-known solutions are obtained for all considered problem instances from the literature. Concerning the heuristic, we were able to show that it outperforms heuristic competitors from the related literature.\n    ",
        "submission_date": "2014-05-22T00:00:00",
        "last_modified_date": "2014-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6142",
        "title": "A Computational Theory of Subjective Probability",
        "authors": [
            "Phil Maguire",
            "Philippe Moser",
            "Rebecca Maguire",
            "Mark Keane"
        ],
        "abstract": "In this article we demonstrate how algorithmic probability theory is applied to situations that involve uncertainty. When people are unsure of their model of reality, then the outcome they observe will cause them to update their beliefs. We argue that classical probability cannot be applied in such cases, and that subjective probability must instead be used. In Experiment 1 we show that, when judging the probability of lottery number sequences, people apply subjective rather than classical probability. In Experiment 2 we examine the conjunction fallacy and demonstrate that the materials used by Tversky and Kahneman (1983) involve model uncertainty. We then provide a formal mathematical proof that, for every uncertain model, there exists a conjunction of outcomes which is more subjectively probable than either of its constituents in isolation.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6369",
        "title": "HEPGAME and the Simplification of Expressions",
        "authors": [
            "Ben Ruijl",
            "Jos Vermaseren",
            "Aske Plaat",
            "Jaap van den Herik"
        ],
        "abstract": "Advances in high energy physics have created the need to increase computational capacity. Project HEPGAME was composed to address this challenge. One of the issues is that numerical integration of expressions of current interest have millions of terms and takes weeks to compute. We have investigated ways to simplify these expressions, using Horner schemes and common subexpression elimination. Our approach applies MCTS, a search procedure that has been successful in AI. We use it to find near-optimal Horner schemes. Although MCTS finds better solutions, this approach gives rise to two further challenges. (1) MCTS (with UCT) introduces a constant, $C_p$ that governs the balance between exploration and exploitation. This constant has to be tuned manually. (2) There should be more guided exploration at the bottom of the tree, since the current approach reduces the quality of the solution towards the end of the expression. We investigate NMCS (Nested Monte Carlo Search) to address both issues, but find that NMCS is computationally unfeasible for our problem. Then, we modify the MCTS formula by introducing a dynamic exploration-exploitation parameter $T$ that decreases linearly with the iteration number. Consequently, we provide a performance analysis. We observe that a variable $C_p$ solves our domain: it yields more exploration at the bottom and as a result the tuning problem has been simplified. The region in $C_p$ for which good values are found is increased by more than a tenfold. This result encourages us to continue our research to solve other prominent problems in High Energy Physics.\n    ",
        "submission_date": "2014-05-25T00:00:00",
        "last_modified_date": "2014-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6509",
        "title": "Judgment Aggregation in Multi-Agent Argumentation",
        "authors": [
            "Edmond Awad",
            "Richard Booth",
            "Fernando Tohme",
            "Iyad Rahwan"
        ],
        "abstract": "Given a set of conflicting arguments, there can exist multiple plausible opinions about which arguments should be accepted, rejected, or deemed undecided. We study the problem of how multiple such judgments can be aggregated. We define the problem by adapting various classical social-choice-theoretic properties for the argumentation domain. We show that while argument-wise plurality voting satisfies many properties, it fails to guarantee the collective rationality of the outcome, and struggles with ties. We then present more general results, proving multiple impossibility results on the existence of any good aggregation operator. After characterising the sufficient and necessary conditions for satisfying collective rationality, we study whether restricting the domain of argument-wise plurality voting to classical semantics allows us to escape the impossibility result. We close by listing graph-theoretic restrictions under which argument-wise plurality rule does produce collectively rational outcomes. In addition to identifying fundamental barriers to collective argument evaluation, our results open up the door for a new research agenda for the argumentation and computational social choice communities.\n    ",
        "submission_date": "2014-05-26T00:00:00",
        "last_modified_date": "2015-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6662",
        "title": "Cognitive-mapping and contextual pyramid based Digital Elevation Model Registration and its effective storage using fractal based compression",
        "authors": [
            "Suma Dawn",
            "Vikas Saxena",
            "Bhudev Sharma"
        ],
        "abstract": "Digital Elevation models (DEM) are images having terrain information embedded into them. Using cognitive mapping concepts for DEM registration, has evolved from this basic idea of using the mapping between the space to objects and defining their relationships to form the basic landmarks that need to be marked, stored and manipulated in and about the environment or other candidate environments, namely, in our case, the DEMs. The progressive two-level encapsulation of methods of geo-spatial cognition includes landmark knowledge and layout knowledge and can be useful for DEM registration. Space-based approach, that emphasizes on explicit extent of the environment under consideration, and object-based approach, that emphasizes on the relationships between objects in the local environment being the two paradigms of cognitive mapping can be methodically integrated in this three-architecture for DEM registration. Initially, P-model based segmentation is performed followed by landmark formation for contextual mapping that uses contextual pyramid formation. Apart from landmarks being used for registration key-point finding, Euclidean distance based deformation calculation has been used for transformation and change detection. Landmarks have been categorized to belong to either being flat-plain areas without much variation in the land heights; peaks that can be found when there is gradual increase in height as compared to the flat areas; valleys, marked with gradual decrease in the height seen in DEM; and finally, ripple areas with very shallow crests and nadirs. Fractal based compression was used for storage of co-registered DEMs. This method may further be extended for DEM-topographic map and DEM-to-remote sensed image registration. Experimental results further cement the fact that DEM registration may be effectively done using the proposed method.\n    ",
        "submission_date": "2014-05-09T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7076",
        "title": "On minimal sets of graded attribute implications",
        "authors": [
            "Vilem Vychodil"
        ],
        "abstract": "We explore the structure of non-redundant and minimal sets consisting of graded if-then rules. The rules serve as graded attribute implications in object-attribute incidence data and as similarity-based functional dependencies in a similarity-based generalization of the relational model of data. Based on our observations, we derive a polynomial-time algorithm which transforms a given finite set of rules into an equivalent one which has the least size in terms of the number of rules.\n    ",
        "submission_date": "2014-05-27T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7192",
        "title": "The PeerRank Method for Peer Assessment",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "We propose the PeerRank method for peer assessment. This constructs a grade for an agent based on the grades proposed by the agents evaluating the agent. Since the grade of an agent is a measure of their ability to grade correctly, the PeerRank method weights grades by the grades of the grading agent. The PeerRank method also provides an incentive for agents to grade correctly. As the grades of an agent depend on the grades of the grading agents, and as these grades themselves depend on the grades of other agents, we define the PeerRank method by a fixed point equation similar to the PageRank method for ranking web-pages. We identify some formal properties of the PeerRank method (for example, it satisfies axioms of unanimity, no dummy, no discrimination and symmetry), discuss some examples, compare with related work and evaluate the performance on some synthetic data. Our results show considerable promise, reducing the error in grade predictions by a factor of 2 or more in many cases over the natural baseline of averaging peer grades.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7295",
        "title": "On the cost-complexity of multi-context systems",
        "authors": [
            "Peter Nov\u00e1k",
            "Cees Witteveen"
        ],
        "abstract": "Multi-context systems provide a powerful framework for modelling information-aggregation systems featuring heterogeneous reasoning components. Their execution can, however, incur non-negligible cost. Here, we focus on cost-complexity of such systems. To that end, we introduce cost-aware multi-context systems, an extension of non-monotonic multi-context systems framework taking into account costs incurred by execution of semantic operators of the individual contexts. We formulate the notion of cost-complexity for consistency and reasoning problems in MCSs. Subsequently, we provide a series of results related to gradually more and more constrained classes of MCSs and finally introduce an incremental cost-reducing algorithm solving the reasoning problem for definite MCSs.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7567",
        "title": "Analogy-Based and Case-Based Reasoning: Two sides of the same coin",
        "authors": [
            "Michael Gr. Voskoglou",
            "Abdel-Badeeh M. Salem"
        ],
        "abstract": "Analogy-Based (or Analogical) and Case-Based Reasoning (ABR and CBR) are two similar problem solving processes based on the adaptation of the solution of past problems for use with a new analogous problem. In this paper we review these two processes and we give some real world examples with emphasis to the field of Medicine, where one can find some of the most common and useful CBR applications. We also underline the differences between CBR and the classical rule-induction algorithms, we discuss the criticism for CBR methods and we focus on the future trends of research in the area of CBR.\n    ",
        "submission_date": "2014-05-29T00:00:00",
        "last_modified_date": "2014-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7944",
        "title": "Military Simulator - A Case Study of Behaviour Tree and Unity based architecture",
        "authors": [
            "Shruti Jadon",
            "Anubhav Singhal",
            "Suma Dawn"
        ],
        "abstract": "In this paper we show how the combination of Behaviour Tree and Utility Based AI architecture can be used to design more realistic bots for Military Simulators. In this work, we have designed a mathematical model of a simulator system which in turn helps in analyzing the results and finding out the various spaces on which our favorable situation might exist, this is done geometrically. In the mathematical model, we have explained the matrix formation and its significance followed up in dynamic programming approach we explained the possible graph formation which will led improvisation of AI, latter we explained the possible geometrical structure of the matrix operations and its impact on a particular decision, we also explained the conditions under which it tend to fail along with a possible solution in future works.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7964",
        "title": "Neutrosophic soft sets with applications in decision making",
        "authors": [
            "Faruk Karaaslan"
        ],
        "abstract": "We firstly present definitions and properties in study of Maji \\cite{maji-2013} on neutrosophic soft sets. We then give a few notes on his study. Next, based on \u00c7a\u011fman \\cite{cagman-2014}, we redefine the notion of neutrosophic soft set and neutrosophic soft set operations to make more functional. By using these new definitions we construct a decision making method and a group decision making method which selects a set of optimum elements from the alternatives. We finally present examples which shows that the methods can be successfully applied to many problems that contain uncertainties.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0062",
        "title": "Towards a Multiagent Decision Support System for crisis Management",
        "authors": [
            "Fahem Kebair",
            "Fr\u00e9d\u00e9ric Serin"
        ],
        "abstract": "Crisis management is a complex problem raised by the scientific community currently. Decision support systems are a suitable solution for such issues, they are indeed able to help emergency managers to prevent and to manage crisis in emergency situations. However, they should be enough flexible and adaptive in order to be reliable to solve complex problems that are plunged in dynamic and unpredictable environments. The approach we propose in this paper addresses this challenge. We expose here a modelling of information for an emergency environment and an architecture of a multiagent decision support system that deals with these information in order to prevent and to manage the occur of a crisis in emergency situations. We focus on the first level of the system mechanism which intends to perceive and to reflect the evolution of the current situation. The general approach and experimentations are provided here.\n    ",
        "submission_date": "2014-05-31T00:00:00",
        "last_modified_date": "2014-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0155",
        "title": "On the measure of conflicts: A MUS-Decomposition Based Framework",
        "authors": [
            "Said Jabbour",
            "Yue Ma",
            "Badran Raddaoui",
            "Lakhdar Sais",
            "Yakoub Salhi"
        ],
        "abstract": "Measuring inconsistency is viewed as an important issue related to handling inconsistencies. Good measures are supposed to satisfy a set of rational properties. However, defining sound properties is sometimes problematic. In this paper, we emphasize one such property, named Decomposability, rarely discussed in the literature due to its modeling difficulties. To this end, we propose an independent decomposition which is more intuitive than existing proposals. To analyze inconsistency in a more fine-grained way, we introduce a graph representation of a knowledge base and various MUSdecompositions. One particular MUS-decomposition, named distributable MUS-decomposition leads to an interesting partition of inconsistencies in a knowledge base such that multiple experts can check inconsistencies in parallel, which is impossible under existing measures. Such particular MUSdecomposition results in an inconsistency measure that satisfies a number of desired properties. Moreover, we give an upper bound complexity of the measure that can be computed using 0/1 linear programming or Min Cost Satisfiability problems, and conduct preliminary experiments to show its feasibility.\n    ",
        "submission_date": "2014-06-01T00:00:00",
        "last_modified_date": "2014-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0486",
        "title": "Monte Carlo Tree Search with Heuristic Evaluations using Implicit Minimax Backups",
        "authors": [
            "Marc Lanctot",
            "Mark H.M. Winands",
            "Tom Pepels",
            "Nathan R. Sturtevant"
        ],
        "abstract": "Monte Carlo Tree Search (MCTS) has improved the performance of game engines in domains such as Go, Hex, and general game playing. MCTS has been shown to outperform classic alpha-beta search in games where good heuristic evaluations are difficult to obtain. In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately. Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during the MCTS simulations. These minimax values are then used to guide future simulations. We show that using implicit minimax backups leads to stronger play performance in Kalah, Breakthrough, and Lines of Action.\n    ",
        "submission_date": "2014-06-02T00:00:00",
        "last_modified_date": "2014-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0941",
        "title": "Augmentative Message Passing for Traveling Salesman Problem and Graph Partitioning",
        "authors": [
            "Siamak Ravanbakhsh",
            "Reihaneh Rabbany",
            "Russell Greiner"
        ],
        "abstract": "The cutting plane method is an augmentative constrained optimization procedure that is often used with continuous-domain optimization techniques such as linear and convex programs. We investigate the viability of a similar idea within message passing -- which produces integral solutions -- in the context of two combinatorial problems: 1) For Traveling Salesman Problem (TSP), we propose a factor-graph based on Held-Karp formulation, with an exponential number of constraint factors, each of which has an exponential but sparse tabular form. 2) For graph-partitioning (a.k.a., community mining) using modularity optimization, we introduce a binary variable model with a large number of constraints that enforce formation of cliques. In both cases we are able to derive surprisingly simple message updates that lead to competitive solutions on benchmark instances. In particular for TSP we are able to find near-optimal solutions in the time that empirically grows with N^3, demonstrating that augmentation is practical and efficient.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0955",
        "title": "Cascading A*: a Parallel Approach to Approximate Heuristic Search",
        "authors": [
            "Yan Gu"
        ],
        "abstract": "In this paper, we proposed a new approximate heuristic search algorithm: Cascading A*, which is a two-phrase algorithm combining A* and IDA* by a new concept \"envelope ball\". The new algorithm CA* is efficient, able to generate approximate solution and any-time solution, and parallel friendly.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2016-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1061",
        "title": "A Methodology for Empirical Analysis of LOD Datasets",
        "authors": [
            "Vit Novacek"
        ],
        "abstract": "CoCoE stands for Complexity, Coherence and Entropy, and presents an extensible methodology for empirical analysis of Linked Open Data (i.e., RDF graphs). CoCoE can offer answers to questions like: Is dataset A better than B for knowledge discovery since it is more complex and informative?, Is dataset X better than Y for simple value lookups due its flatter structure?, etc. In order to address such questions, we introduce a set of well-founded measures based on complementary notions from distributional semantics, network analysis and information theory. These measures are part of a specific implementation of the CoCoE methodology that is available for download. Last but not least, we illustrate CoCoE by its application to selected biomedical RDF datasets.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1128",
        "title": "A self-organizing system for urban traffic control based on predictive interval microscopic model",
        "authors": [
            "Bartlomiej Placzek"
        ],
        "abstract": "This paper introduces a self-organizing traffic signal system for an urban road network. The key elements of this system are agents that control traffic signals at intersections. Each agent uses an interval microscopic traffic model to predict effects of its possible control actions in a short time horizon. The executed control action is selected on the basis of predicted delay intervals. Since the prediction results are represented by intervals, the agents can recognize and suspend those control actions, whose positive effect on the performance of traffic control is uncertain. Evaluation of the proposed traffic control system was performed in a simulation environment. The simulation experiments have shown that the proposed approach results in an improved performance, particularly for non-uniform traffic streams.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1411",
        "title": "Advances in Learning Bayesian Networks of Bounded Treewidth",
        "authors": [
            "Siqi Nie",
            "Denis Deratani Maua",
            "Cassio Polpo de Campos",
            "Qiang Ji"
        ],
        "abstract": "This work presents novel algorithms for learning Bayesian network structures with bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that $k$-tree. Some properties of these methods are discussed and proven. The approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables. The experiments show that our exact algorithm outperforms the state of the art, and that the approximate approach is fairly accurate.\n    ",
        "submission_date": "2014-06-05T00:00:00",
        "last_modified_date": "2014-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1556",
        "title": "Enhancements to ACL2 in Versions 6.2, 6.3, and 6.4",
        "authors": [
            "Matt Kaufmann",
            "J Strother Moore"
        ],
        "abstract": "We report on improvements to ACL2 made since the 2013 ACL2 Workshop.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2014-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1559",
        "title": "Initial Experiments with TPTP-style Automated Theorem Provers on ACL2 Problems",
        "authors": [
            "Sebastiaan Joosten",
            "Cezary Kaliszyk",
            "Josef Urban"
        ],
        "abstract": "This paper reports our initial experiments with using external ATP on some corpora built with the ACL2 system. This is intended to provide the first estimate about the usefulness of such external reasoning and AI systems for solving ACL2 problems.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2014-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1638",
        "title": "Automated Generation of Geometric Theorems from Images of Diagrams",
        "authors": [
            "Xiaoyu Chen",
            "Dan Song",
            "Dongming Wang"
        ],
        "abstract": "We propose an approach to generate geometric theorems from electronic images of diagrams automatically. The approach makes use of techniques of Hough transform to recognize geometric objects and their labels and of numeric verification to mine basic geometric relations. Candidate propositions are generated from the retrieved information by using six strategies and geometric theorems are obtained from the candidates via algebraic computation. Experiments with a preliminary implementation illustrate the effectiveness and efficiency of the proposed approach for generating nontrivial theorems from images of diagrams. This work demonstrates the feasibility of automated discovery of profound geometric knowledge from simple image data and has potential applications in geometric knowledge management and education.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2014-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1697",
        "title": "Multiscale probability transformation of basic probability assignment",
        "authors": [
            "Meizhu Li",
            "Qi Zhang",
            "Yong Deng"
        ],
        "abstract": "Decision making is still an open issue in the application of Dempster-Shafer evidence theory. A lot of works have been presented for it. In the transferable belief model (TBM), pignistic probabilities based on the basic probability as- signments are used for decision making. In this paper, multiscale probability transformation of basic probability assignment based on the belief function and the plausibility function is proposed, which is a generalization of the pignistic probability transformation. In the multiscale probability function, a factor q based on the Tsallis entropy is used to make the multiscale prob- abilities diversified. An example is shown that the multiscale probability transformation is more reasonable in the decision making.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2014-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1767",
        "title": "Changing the Environment Based on Empowerment as Intrinsic Motivation",
        "authors": [
            "Christoph Salge",
            "Cornelius Glackin",
            "Daniel Polani"
        ],
        "abstract": "One aspect of intelligence is the ability to restructure your own environment so that the world you live in becomes more beneficial to you. In this paper we investigate how the information-theoretic measure of agent empowerment can provide a task-independent, intrinsic motivation to restructure the world. We show how changes in embodiment and in the environment change the resulting behaviour of the agent and the artefacts left in the world. For this purpose, we introduce an approximation of the established empowerment formalism based on sparse sampling, which is simpler and significantly faster to compute for deterministic dynamics. Sparse sampling also introduces a degree of randomness into the decision making process, which turns out to beneficial for some cases. We then utilize the measure to generate agent behaviour for different agent embodiments in a Minecraft-inspired three dimensional block world. The paradigmatic results demonstrate that empowerment can be used as a suitable generic intrinsic motivation to not only generate actions in given static environments, as shown in the past, but also to modify existing environmental conditions. In doing so, the emerging strategies to modify an agent's environment turn out to be meaningful to the specific agent capabilities, i.e., de facto to its embodiment.\n    ",
        "submission_date": "2014-06-03T00:00:00",
        "last_modified_date": "2014-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2000",
        "title": "Introduction to Neutrosophic Statistics",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "Neutrosophic Statistics means statistical analysis of population or sample that has indeterminate (imprecise, ambiguous, vague, incomplete, unknown) data. For example, the population or sample size might not be exactly determinate because of some individuals that partially belong to the population or sample, and partially they do not belong, or individuals whose appurtenance is completely unknown. Also, there are population or sample individuals whose data could be indeterminate. In this book, we develop the 1995 notion of neutrosophic statistics. We present various practical examples. It is possible to define the neutrosophic statistics in many ways, because there are various types of indeterminacies, depending on the problem to solve.\n    ",
        "submission_date": "2014-06-08T00:00:00",
        "last_modified_date": "2014-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2023",
        "title": "Rational Closure in SHIQ",
        "authors": [
            "Laura Giordano",
            "Valentina Gliozzi",
            "Nicola Olivetti",
            "Gian Luca Pozzato"
        ],
        "abstract": "We define a notion of rational closure for the logic SHIQ, which does not enjoys the finite model property, building on the notion of rational closure introduced by Lehmann and Magidor in [23]. We provide a semantic characterization of rational closure in SHIQ in terms of a preferential semantics, based on a finite rank characterization of minimal models. We show that the rational closure of a TBox can be computed in EXPTIME using entailment in SHIQ.\n    ",
        "submission_date": "2014-06-08T00:00:00",
        "last_modified_date": "2014-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2128",
        "title": "A bio-inspired algorithm for fuzzy user equilibrium problem by aid of Physarum Polycephalum",
        "authors": [
            "Yang Liu",
            "Xiaoge Zhang",
            "Yong Deng"
        ],
        "abstract": "The user equilibrium in traffic assignment problem is based on the fact that travelers choose the minimum-cost path between every origin-destination pair and on the assumption that such a behavior will lead to an equilibrium of the traffic network. In this paper, we consider this problem when the traffic network links are fuzzy cost. Therefore, a Physarum-type algorithm is developed to unify the Physarum network and the traffic network for taking full of advantage of Physarum Polycephalum's adaptivity in network design to solve the user equilibrium problem. Eventually, some experiments are used to test the performance of this method. The results demonstrate that our approach is competitive when compared with other existing algorithms.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2358",
        "title": "Conjunction and Negation of Natural Concepts: A Quantum-theoretic Modeling",
        "authors": [
            "Sandro Sozzo"
        ],
        "abstract": "We perform two experiments with the aim to investigate the effects of negation on the combination of natural concepts. In the first experiment, we test the membership weights of a list of exemplars with respect to two concepts, e.g., {\\it Fruits} and {\\it Vegetables}, and their conjunction {\\it Fruits And Vegetables}. In the second experiment, we test the membership weights of the same list of exemplars with respect to the same two concepts, but negating the second, e.g., {\\it Fruits} and {\\it Not Vegetables}, and again their conjunction {\\it Fruits And Not Vegetables}. The collected data confirm existing results on conceptual combination, namely, they show dramatic deviations from the predictions of classical (fuzzy set) logic and probability theory. More precisely, they exhibit conceptual vagueness, gradeness of membership, overextension and double overextension of membership weights with respect to the given conjunctions. Then, we show that the quantum probability model in Fock space recently elaborated to model Hampton's data on concept conjunction (Hampton, 1988a) and disjunction (Hampton, 1988b) faithfully accords with the collected data. Our quantum-theoretic modeling enables to describe these non-classical effects in terms of genuine quantum effects, namely `contextuality', `superposition', `interference' and `emergence'. The obtained results confirm and strenghten the analysis in Aerts (2009a) and Sozzo (2014) on the identification of quantum aspects in experiments on conceptual vagueness. Our results can be inserted within the general research on the identification of quantum structures in cognitive and decision processes.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2014-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2395",
        "title": "ExpertBayes: Automatically refining manually built Bayesian networks",
        "authors": [
            "Ezilda Almeida",
            "Pedro Ferreira",
            "Tiago Vinhoza",
            "In\u00eas Dutra",
            "Jingwei Li",
            "Yirong Wu",
            "Elizabeth Burnside"
        ],
        "abstract": "Bayesian network structures are usually built using only the data and starting from an empty network or from a naive Bayes structure. Very often, in some domains, like medicine, a prior structure knowledge is already known. This structure can be automatically or manually refined in search for better performance models. In this work, we take Bayesian networks built by specialists and show that minor perturbations to this original network can yield better classifiers with a very small computational cost, while maintaining most of the intended meaning of the original model.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2720",
        "title": "The Effect of Social Learning on Individual Learning and Evolution",
        "authors": [
            "Chris Marriott",
            "Jobran Chebib"
        ],
        "abstract": "We consider the effects of social learning on the individual learning and genetic evolution of a colony of artificial agents capable of genetic, individual and social modes of adaptation. We confirm that there is strong selection pressure to acquire traits of individual learning and social learning when these are adaptive traits. We show that selection pressure for learning of either kind can supress selection pressure for reproduction or greater fitness. We show that social learning differs from individual learning in that it can support a second evolutionary system that is decoupled from the biological evolutionary system. This decoupling leads to an emergent interaction where immature agents are more likely to engage in learning activities than mature agents.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2858",
        "title": "Quantum POMDPs",
        "authors": [
            "Jennifer Barry",
            "Daniel T. Barry",
            "Scott Aaronson"
        ],
        "abstract": "We present quantum observable Markov decision processes (QOMDPs), the quantum analogues of partially observable Markov decision processes (POMDPs). In a QOMDP, an agent's state is represented as a quantum state and the agent can choose a superoperator to apply. This is similar to the POMDP belief state, which is a probability distribution over world states and evolves via a stochastic matrix. We show that the existence of a policy of at least a certain value has the same complexity for QOMDPs and POMDPs in the polynomial and infinite horizon cases. However, we also prove that the existence of a policy that can reach a goal state is decidable for goal POMDPs and undecidable for goal QOMDPs.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3047",
        "title": "Tree-like Queries in OWL 2 QL: Succinctness and Complexity Results",
        "authors": [
            "Meghyn Bienvenu",
            "Stanislav Kikot",
            "Vladimir Podolskii"
        ],
        "abstract": "This paper investigates the impact of query topology on the difficulty of answering conjunctive queries in the presence of OWL 2 QL ontologies. Our first contribution is to clarify the worst-case size of positive existential (PE), non-recursive Datalog (NDL), and first-order (FO) rewritings for various classes of tree-like conjunctive queries, ranging from linear queries to bounded treewidth queries. Perhaps our most surprising result is a superpolynomial lower bound on the size of PE-rewritings that holds already for linear queries and ontologies of depth 2. More positively, we show that polynomial-size NDL-rewritings always exist for tree-shaped queries with a bounded number of leaves (and arbitrary ontologies), and for bounded treewidth queries paired with bounded depth ontologies. For FO-rewritings, we equate the existence of polysize rewritings with well-known problems in Boolean circuit complexity. As our second contribution, we analyze the computational complexity of query answering and establish tractability results (either NL- or LOGCFL-completeness) for a range of query-ontology pairs. Combining our new results with those from the literature yields a complete picture of the succinctness and complexity landscapes for the considered classes of queries and ontologies.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3124",
        "title": "Guarantees and Limits of Preprocessing in Constraint Satisfaction and Reasoning",
        "authors": [
            "Serge Gaspers",
            "Stefan Szeider"
        ],
        "abstract": "We present a first theoretical analysis of the power of polynomial-time preprocessing for important combinatorial problems from various areas in AI. We consider problems from Constraint Satisfaction, Global Constraints, Satisfiability, Nonmonotonic and Bayesian Reasoning under structural restrictions. All these problems involve two tasks: (i) identifying the structure in the input as required by the restriction, and (ii) using the identified structure to solve the reasoning task efficiently. We show that for most of the considered problems, task (i) admits a polynomial-time preprocessing to a problem kernel whose size is polynomial in a structural problem parameter of the input, in contrast to task (ii) which does not admit such a reduction to a problem kernel of polynomial size, subject to a complexity theoretic assumption. As a notable exception we show that the consistency problem for the AtMost-NValue constraint admits a polynomial kernel consisting of a quadratic number of variables and domain values. Our results provide a firm worst-case guarantees and theoretical boundaries for the performance of polynomial-time preprocessing algorithms for the considered problems.\n    ",
        "submission_date": "2014-06-12T00:00:00",
        "last_modified_date": "2014-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3191",
        "title": "An eigenvector-based hotspot detection",
        "authors": [
            "Hadi Fanaee-T",
            "Joao Gama"
        ],
        "abstract": "Space and time are two critical components of many real world systems. For this reason, analysis of anomalies in spatiotemporal data has been a great of interest. In this work, application of tensor decomposition and eigenspace techniques on spatiotemporal hotspot detection is investigated. An algorithm called SST-Hotspot is proposed which accounts for spatiotemporal variations in data and detect hotspots using matching of eigenvector elements of two cases and population tensors. The experimental results reveal the interesting application of tensor decomposition and eigenvector-based techniques in hotspot analysis.\n    ",
        "submission_date": "2014-06-12T00:00:00",
        "last_modified_date": "2014-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3266",
        "title": "Event and Anomaly Detection Using Tucker3 Decomposition",
        "authors": [
            "Hadi Fanaee-T",
            "M\u00e1rcia D. B. Oliveira",
            "Jo\u00e3o Gama",
            "Simon Malinowski",
            "Ricardo Morla"
        ],
        "abstract": "Failure detection in telecommunication networks is a vital task. So far, several supervised and unsupervised solutions have been provided for discovering failures in such networks. Among them unsupervised approaches has attracted more attention since no label data is required. Often, network devices are not able to provide information about the type of failure. In such cases the type of failure is not known in advance and the unsupervised setting is more appropriate for diagnosis. Among unsupervised approaches, Principal Component Analysis (PCA) is a well-known solution which has been widely used in the anomaly detection literature and can be applied to matrix data (e.g. Users-Features). However, one of the important properties of network data is their temporal sequential nature. So considering the interaction of dimensions over a third dimension, such as time, may provide us better insights into the nature of network failures. In this paper we demonstrate the power of three-way analysis to detect events and anomalies in time-evolving network data.\n    ",
        "submission_date": "2014-06-12T00:00:00",
        "last_modified_date": "2014-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3339",
        "title": "Algorithms for CVaR Optimization in MDPs",
        "authors": [
            "Yinlam Chow",
            "Mohammad Ghavamzadeh"
        ],
        "abstract": "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.\n    ",
        "submission_date": "2014-06-12T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3496",
        "title": "EigenEvent: An Algorithm for Event Detection from Complex Data Streams in Syndromic Surveillance",
        "authors": [
            "Hadi Fanaee-T",
            "Jo\u00e3o Gama"
        ],
        "abstract": "Syndromic surveillance systems continuously monitor multiple pre-diagnostic daily streams of indicators from different regions with the aim of early detection of disease outbreaks. The main objective of these systems is to detect outbreaks hours or days before the clinical and laboratory confirmation. The type of data that is being generated via these systems is usually multivariate and seasonal with spatial and temporal dimensions. The algorithm What's Strange About Recent Events (WSARE) is the state-of-the-art method for such problems. It exhaustively searches for contrast sets in the multivariate data and signals an alarm when find statistically significant rules. This bottom-up approach presents a much lower detection delay comparing the existing top-down approaches. However, WSARE is very sensitive to the small-scale changes and subsequently comes with a relatively high rate of false alarms. We propose a new approach called EigenEvent that is neither fully top-down nor bottom-up. In this method, we instead of top-down or bottom-up search, track changes in data correlation structure via eigenspace techniques. This new methodology enables us to detect both overall changes (via eigenvalue) and dimension-level changes (via eigenvectors). Experimental results on hundred sets of benchmark data reveals that EigenEvent presents a better overall performance comparing state-of-the-art, in particular in terms of the false alarm rate.\n    ",
        "submission_date": "2014-06-13T00:00:00",
        "last_modified_date": "2014-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3497",
        "title": "Multi-objective Reinforcement Learning with Continuous Pareto Frontier Approximation Supplementary Material",
        "authors": [
            "Matteo Pirotta",
            "Simone Parisi",
            "Marcello Restelli"
        ],
        "abstract": "This document contains supplementary material for the paper \"Multi-objective Reinforcement Learning with Continuous Pareto Frontier Approximation\", published at the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15). The paper is about learning a continuous approximation of the Pareto frontier in Multi-Objective Markov Decision Problems (MOMDPs). We propose a policy-based approach that exploits gradient information to generate solutions close to the Pareto ones. Differently from previous policy-gradient multi-objective algorithms, where n optimization routines are use to have n solutions, our approach performs a single gradient-ascent run that at each step generates an improved continuous approximation of the Pareto frontier. The idea is to exploit a gradient-based approach to optimize the parameters of a function that defines a manifold in the policy parameter space so that the corresponding image in the objective space gets as close as possible to the Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non-trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two interesting MOMDPs.\n    ",
        "submission_date": "2014-06-13T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3506",
        "title": "Eigenspace Method for Spatiotemporal Hotspot Detection",
        "authors": [
            "Hadi Fanaee-T",
            "Jo\u00e3o Gama"
        ],
        "abstract": "Hotspot detection aims at identifying subgroups in the observations that are unexpected, with respect to the some baseline information. For instance, in disease surveillance, the purpose is to detect sub-regions in spatiotemporal space, where the count of reported diseases (e.g. Cancer) is higher than expected, with respect to the population. The state-of-the-art method for this kind of problem is the Space-Time Scan Statistics (STScan), which exhaustively search the whole space through a sliding window looking for significant spatiotemporal clusters. STScan makes some restrictive assumptions about the distribution of data, the shape of the hotspots and the quality of data, which can be unrealistic for some nontraditional data sources. A novel methodology called EigenSpot is proposed where instead of an exhaustive search over the space, tracks the changes in a space-time correlation structure. Not only does the new approach presents much more computational efficiency, but also makes no assumption about the data distribution, hotspot shape or the data quality. The principal idea is that with the joint combination of abnormal elements in the principal spatial and the temporal singular vectors, the location of hotspots in the spatiotemporal space can be approximated. A comprehensive experimental evaluation, both on simulated and real data sets reveals the effectiveness of the proposed method.\n    ",
        "submission_date": "2014-06-13T00:00:00",
        "last_modified_date": "2014-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3793",
        "title": "Neural tuning size is a key factor underlying holistic face processing",
        "authors": [
            "Cheston Tan",
            "Tomaso Poggio"
        ],
        "abstract": "Faces are a class of visual stimuli with unique significance, for a variety of reasons. They are ubiquitous throughout the course of a person's life, and face recognition is crucial for daily social interaction. Faces are also unlike any other stimulus class in terms of certain physical stimulus characteristics. Furthermore, faces have been empirically found to elicit certain characteristic behavioral phenomena, which are widely held to be evidence of \"holistic\" processing of faces. However, little is known about the neural mechanisms underlying such holistic face processing. In other words, for the processing of faces by the primate visual system, the input and output characteristics are relatively well known, but the internal neural computations are not. The main aim of this work is to further the fundamental understanding of what causes the visual processing of faces to be different from that of objects. In this computational modeling work, we show that a single factor - \"neural tuning size\" - is able to account for three key phenomena that are characteristic of face processing, namely the Composite Face Effect (CFE), Face Inversion Effect (FIE) and Whole-Part Effect (WPE). Our computational proof-of-principle provides specific neural tuning properties that correspond to the poorly-understood notion of holistic face processing, and connects these neural properties to psychophysical behavior. Overall, our work provides a unified and parsimonious theoretical account for the disparate empirical data on face-specific processing, deepening the fundamental understanding of face processing.\n    ",
        "submission_date": "2014-06-15T00:00:00",
        "last_modified_date": "2014-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3877",
        "title": "Argument Ranking with Categoriser Function",
        "authors": [
            "Fuan Pu",
            "Jian Luo",
            "Yulai Zhang",
            "Guiming Luo"
        ],
        "abstract": "Recently, ranking-based semantics is proposed to rank-order arguments from the most acceptable to the weakest one(s), which provides a graded assessment to arguments. In general, the ranking on arguments is derived from the strength values of the arguments. Categoriser function is a common approach that assigns a strength value to a tree of arguments. When it encounters an argument system with cycles, then the categoriser strength is the solution of the non-linear equations. However, there is no detail about the existence and uniqueness of the solution, and how to find the solution (if exists). In this paper, we will cope with these issues via fixed point technique. In addition, we define the categoriser-based ranking semantics in light of categoriser strength, and investigate some general properties of it. Finally, the semantics is shown to satisfy some of the axioms that a ranking-based semantics should satisfy.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4067",
        "title": "Automatic Channel Fault Detection and Diagnosis System for a Small Animal APD-Based Digital PET Scanner",
        "authors": [
            "Jonathan Charest",
            "Jean-Fran\u00e7ois Beaudoin",
            "Jules Cadorette",
            "Roger Lecomte",
            "Charles-Antoine Brunet",
            "R\u00e9jean Fontaine"
        ],
        "abstract": "Fault detection and diagnosis is critical to many applications in order to ensure proper operation and performance over time. Positron emission tomography (PET) systems that require regular calibrations by qualified scanner operators are good candidates for such continuous improvements. Furthermore, for scanners employing one-to-one coupling of crystals to photodetectors to achieve enhanced spatial resolution and contrast, the calibration task is even more daunting because of the large number of independent channels involved. To cope with the additional complexity of the calibration and quality control procedures of these scanners, an intelligent system (IS) was designed to perform fault detection and diagnosis (FDD) of malfunctioning channels. The IS can be broken down into four hierarchical modules: parameter extraction, channel fault detection, fault prioritization and diagnosis. Of these modules, the first two have previously been reported and this paper focuses on fault prioritization and diagnosis. The purpose of the fault prioritization module is to help the operator to zero in on the faults that need immediate attention. The fault diagnosis module will then identify the causes of the malfunction and propose an explanation of the reasons that lead to the diagnosis. The FDD system was implemented on a LabPET avalanche photodiode (APD)-based digital PET scanner. Experiments demonstrated a FDD Sensitivity of 99.3 % (with a 95% confidence interval (CI) of: [98.7, 99.9]) for major faults. Globally, the Balanced Accuracy of the diagnosis for varying fault severities is 92 %. This suggests the IS can greatly benefit the operators in their maintenance task.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4200",
        "title": "Lifted Tree-Reweighted Variational Inference",
        "authors": [
            "Hung Hai Bui",
            "Tuyen N. Huynh",
            "David Sontag"
        ],
        "abstract": "We analyze variational inference for highly symmetric graphical models such as those arising from first-order probabilistic models. We first show that for these graphical models, the tree-reweighted variational objective lends itself to a compact lifted formulation which can be solved much more efficiently than the standard TRW formulation for the ground graphical model. Compared to earlier work on lifted belief propagation, our formulation leads to a convex optimization problem for lifted marginal inference and provides an upper bound on the partition function. We provide two approaches for improving the lifted TRW upper bound. The first is a method for efficiently computing maximum spanning trees in highly symmetric graphs, which can be used to optimize the TRW edge appearance probabilities. The second is a method for tightening the relaxation of the marginal polytope using lifted cycle inequalities and novel exchangeable cluster consistency constraints.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4287",
        "title": "Identifying roles of clinical pharmacy with survey evaluation",
        "authors": [
            "Andreja \u010cufar",
            "Ale\u0161 Mrhar",
            "Marko Robnik-\u0160ikonja"
        ],
        "abstract": "The survey data sets are important sources of data and their successful exploitation is of key importance for informed policy-decision making. We present how a survey analysis approach initially developed for customer satisfaction research in marketing can be adapted for the introduction of clinical pharmacy services into hospital. We use two analytical approaches to extract relevant managerial consequences. With OrdEval algorithm we first evaluate the importance of competences for the users of clinical pharmacy and extract their nature according to the users expectations. Next, we build a model for predicting a successful introduction of clinical pharmacy to the clinical departments. We the wards with the highest probability of successful cooperation with a clinical pharmacist. We obtain useful managerially relevant information from a relatively small sample of highly relevant respondents. We show how the OrdEval algorithm exploits the information hidden in the ordering of class and attribute values and their inherent correlation. Its output can be effectively visualized and complemented with confidence intervals.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4324",
        "title": "Towards a theory of granular sets",
        "authors": [
            "Garimella Rama Murthy"
        ],
        "abstract": "Motivated by the application problem of sensor fusion the author introduced the concept of graded set. It is reasoned that in classification problem arising in an information system (represented by information table), a novel set called Granular set naturally arises. It is realized that in any hierarchical classification problem, Granular set naturally arises. Also when the target set of objects forms a graded set the lower and upper approximations of target sets form a graded set. This generalizes the concept of rough set. It is hoped that a detailed theory of granular/ graded sets finds several applications.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4462",
        "title": "Soccer League Optimization: A heuristic Algorithm Inspired by the Football System in European Countries",
        "authors": [
            "Erfan Khaji"
        ],
        "abstract": "In this paper a new heuristic optimization algorithm has been introduced based on the performance of the major football leagues within each season in EU countries. The algorithm starts with an initial population including three different groups of teams: the wealthiest (strongest), the regular, the poorest (weakest). Each individual of population constitute a football team while each player is an indication of a player in a post. The optimization can hopefully occurs when the competition among the teams in all the leagues is imitated as the strongest teams usually purchase the best players of the regular teams and in turn, regular teams purchase the best players of the weakest who should always discover young players instead of buying professionals. It has been shown that the algorithm can hopefully converge to an acceptable solution solving various benchmarks. Key words: Heuristic Algorithms\n    ",
        "submission_date": "2014-06-15T00:00:00",
        "last_modified_date": "2014-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4472",
        "title": "Notes on hierarchical ensemble methods for DAG-structured taxonomies",
        "authors": [
            "Giorgio Valentini"
        ],
        "abstract": "Several real problems ranging from text classification to computational biology are characterized by hierarchical multi-label classification tasks. Most of the methods presented in literature focused on tree-structured taxonomies, but only few on taxonomies structured according to a Directed Acyclic Graph (DAG). In this contribution novel classification ensemble algorithms for DAG-structured taxonomies are introduced. In particular Hierarchical Top-Down (HTD-DAG) and True Path Rule (TPR-DAG) for DAGs are presented and discussed.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4679",
        "title": "The Propagation Depth of Local Consistency",
        "authors": [
            "Christoph Berkholz"
        ],
        "abstract": "We establish optimal bounds on the number of nested propagation steps in $k$-consistency tests. It is known that local consistency algorithms such as arc-, path- and $k$-consistency are not efficiently parallelizable. Their inherent sequential nature is caused by long chains of nested propagation steps, which cannot be executed in parallel. This motivates the question \"What is the minimum number of nested propagation steps that have to be performed by $k$-consistency algorithms on (binary) constraint networks with $n$ variables and domain size $d$?\"\n",
        "submission_date": "2014-06-18T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4682",
        "title": "Exact Decoding on Latent Variable Conditional Models is NP-Hard",
        "authors": [
            "Xu Sun"
        ],
        "abstract": "Latent variable conditional models, including the latent conditional random fields as a special case, are popular models for many natural language processing and vision processing tasks. The computational complexity of the exact decoding/inference in latent conditional random fields is unclear. In this paper, we try to clarify the computational complexity of the exact decoding. We analyze the complexity and demonstrate that it is an NP-hard problem even on a sequential labeling setting. Furthermore, we propose the latent-dynamic inference (LDI-Naive) method and its bounded version (LDI-Bounded), which are able to perform exact-inference or almost-exact-inference by using top-$n$ search and dynamic programming.\n    ",
        "submission_date": "2014-06-18T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4881",
        "title": "Architecture of a Fuzzy Expert System Used for Dyslalic Children Therapy",
        "authors": [
            "Ovidiu-Andrei Schipor",
            "Stefan-Gheorghe Pentiuc",
            "Maria-Doina Schipor"
        ],
        "abstract": "In this paper we present architecture of a fuzzy expert system used for therapy of dyslalic children. With fuzzy approach we can create a better model for speech therapist decisions. A software interface was developed for validation of the system. The main objectives of this task are: personalized therapy (the therapy must be in according with child's problems level, context and possibilities), speech therapist assistant (the expert system offer some suggestion regarding what exercises are better for a specific moment and from a specific child), (self) teaching (when system's conclusion is different that speech therapist's conclusion the last one must have the knowledge base change possibility).\n    ",
        "submission_date": "2014-05-29T00:00:00",
        "last_modified_date": "2014-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4882",
        "title": "Knowledge Base of an Expert System Used for Dyslalic Children Therapy",
        "authors": [
            "Ovidiu-Andrei Schipor",
            "Stefan-Gheorghe Pentiuc",
            "Doina-Maria Schipor"
        ],
        "abstract": "In order to improve children speech therapy, we develop a Fuzzy Expert System based on a speech therapy guide. This guide, write in natural language, was formalized using fuzzy logic paradigm. In this manner we obtain a knowledge base with over 150 rules and 19 linguistic variables. All these researches, including expert system validation, are part of TERAPERS project.\n    ",
        "submission_date": "2014-05-29T00:00:00",
        "last_modified_date": "2014-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4973",
        "title": "Racing Multi-Objective Selection Probabilities",
        "authors": [
            "Ga\u00e9tan Marceau",
            "Marc Schoenauer"
        ],
        "abstract": "In the context of Noisy Multi-Objective Optimization, dealing with uncertainties requires the decision maker to define some preferences about how to handle them, through some statistics (e.g., mean, median) to be used to evaluate the qualities of the solutions, and define the corresponding Pareto set. Approximating these statistics requires repeated samplings of the population, drastically increasing the overall computational cost. To tackle this issue, this paper proposes to directly estimate the probability of each individual to be selected, using some Hoeffding races to dynamically assign the estimation budget during the selection step. The proposed racing approach is validated against static budget approaches with NSGA-II on noisy versions of the ZDT benchmark functions.\n    ",
        "submission_date": "2014-06-19T00:00:00",
        "last_modified_date": "2014-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6045",
        "title": "Cognitive Surveillance: Why does it never appear among the AVSS Conferences topics?",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "Video Surveillance is a fast evolving field of research and development (R&D) driven by the urgent need for public security and safety (due to the growing threats of terrorism, vandalism, and anti-social behavior). Traditionally, surveillance systems are comprised of two components - video cameras distributed over the guarded area and human observer watching and analyzing the incoming video. Explosive growth of installed cameras and limited human operator's ability to process the delivered video content raise an urgent demand for developing surveillance systems with human like cognitive capabilities, that is - Cognitive surveillance systems. The growing interest in this issue is testified by the tens of workshops, symposiums and conferences held over the world each year. The IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS) is certainly one of them. However, for unknown reasons, the term Cognitive Surveillance does never appear among its topics. As to me, the explanation for this is simple - the complexity and the indefinable nature of the term \"Cognition\". In this paper, I am trying to resolve the problem providing a novel definition of cognition equally suitable for biological as well as technological applications. I hope my humble efforts will be helpful.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2014-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6102",
        "title": "Random Logic Programs: Linear Model",
        "authors": [
            "Kewen Wang",
            "Lian Wen",
            "Kedian Mu"
        ],
        "abstract": "This paper proposes a model, the linear model, for randomly generating logic programs with low density of rules and investigates statistical properties of such random logic programs. It is mathematically shown that the average number of answer sets for a random program converges to a constant when the number of atoms approaches infinity. Several experimental results are also reported, which justify the suitability of the linear model. It is also experimentally shown that, under this model, the size distribution of answer sets for random programs tends to a normal distribution when the number of atoms is sufficiently large.\n    ",
        "submission_date": "2014-06-23T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6764",
        "title": "A factorization criterion for acyclic directed mixed graphs",
        "authors": [
            "Thomas S. Richardson"
        ],
        "abstract": "Acyclic directed mixed graphs, also known as semi-Markov models represent the conditional independence structure induced on an observed margin by a DAG model with latent variables. In this paper we present a factorization criterion for these models that is equivalent to the global Markov property given by (the natural extension of) d-separation.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6901",
        "title": "Pattern-wave model of brain. Mechanisms of information processing, memory organization",
        "authors": [
            "Alexey Redozubov"
        ],
        "abstract": "The structure of the axon-dendrite connections of neurons of the brain creates a rich spatial structure in which provided various combinations of signals surrounding neurons. Structure of dendritic trees and shape of dendritic spines allow repeatedly increase combinatorial component through cross synapses influence neighboring neurons. In this paper it is shown that the diffuse spreading of neurotransmitters allows neurons to detect and remember significant set of environmental activity patterns. As a core element fixation described extrasynaptic metabotropic receptive clusters. The described mechanism leads to the appearance of wave processes, based on the propagation of the front-line areas of spontaneous activity. In the proposed model, any compact pattern of neural activity is seen as a source emitting a diverging wave endogenous spikes. It is shown that the spike pattern of the wave front is strictly unique and uniquely defined pattern that started the wave. The propagation of waves with a unique pattern allows anywhere in nature undergoing brain wave patterns there to judge the whole brain processes information. In these assumptions naturally described mechanism of projection information between regions of the cortex. Performed computer simulations show the high effectiveness of such information model.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6973",
        "title": "Communicating and resolving entity references",
        "authors": [
            "R.V.Guha"
        ],
        "abstract": "Statements about entities occur everywhere, from newspapers and web pages to structured databases. Correlating references to entities across systems that use different identifiers or names for them is a widespread problem. In this paper, we show how shared knowledge between systems can be used to solve this problem. We present \"reference by description\", a formal model for resolving references. We provide some results on the conditions under which a randomly chosen entity in one system can, with high probability, be mapped to the same entity in a different system.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7196",
        "title": "Set Constraint Model and Automated Encoding into SAT: Application to the Social Golfer Problem",
        "authors": [
            "Fr\u00e9d\u00e9ric Lardeux",
            "Eric Monfroy",
            "Broderick Crawford",
            "Ricardo Soto"
        ],
        "abstract": "On the one hand, Constraint Satisfaction Problems allow one to declaratively model problems. On the other hand, propositional satisfiability problem (SAT) solvers can handle huge SAT instances. We thus present a technique to declaratively model set constraint problems and to encode them automatically into SAT instances. We apply our technique to the Social Golfer Problem and we also use it to break symmetries of the problem. Our technique is simpler, more declarative, and less error-prone than direct and improved hand modeling. The SAT instances that we automatically generate contain less clauses than improved hand-written instances such as in [20], and with unit propagation they also contain less variables. Moreover, they are well-suited for SAT solvers and they are solved faster as shown when solving difficult instances of the Social Golfer Problem.\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1041",
        "title": "n-Valued Refined Neutrosophic Logic and Its Applications to Physics",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "In this paper we present a short history of logics: from particular cases of 2-symbol or numerical valued logic to the general case of n-symbol or numerical valued logic. We show generalizations of 2-valued Boolean logic to fuzzy logic, also from the Kleene and Lukasiewicz 3-symbol valued logics or Belnap 4-symbol valued logic to the most general n-symbol or numerical valued refined neutrosophic logic. Two classes of neutrosophic norm (n-norm) and neutrosophic conorm (n-conorm) are defined. Examples of applications of neutrosophic logic to physics are listed in the last section. Similar generalizations can be done for n-Valued Refined Neutrosophic Set, and respectively n- Valued Refined Neutrosopjhic Probability.\n    ",
        "submission_date": "2014-07-03T00:00:00",
        "last_modified_date": "2014-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1408",
        "title": "The Complexity of Reasoning with FODD and GFODD",
        "authors": [
            "Benjamin J. Hescott",
            "Roni Khardon"
        ],
        "abstract": "Recent work introduced Generalized First Order Decision Diagrams (GFODD) as a knowledge representation that is useful in mechanizing decision theoretic planning in relational domains. GFODDs generalize function-free first order logic and include numerical values and numerical generalizations of existential and universal quantification. Previous work presented heuristic inference algorithms for GFODDs and implemented these heuristics in systems for decision theoretic planning. In this paper, we study the complexity of the computational problems addressed by such implementations. In particular, we study the evaluation problem, the satisfiability problem, and the equivalence problem for GFODDs under the assumption that the size of the intended model is given with the problem, a restriction that guarantees decidability. Our results provide a complete characterization placing these problems within the polynomial hierarchy. The same characterization applies to the corresponding restriction of problems in first order logic, giving an interesting new avenue for efficient inference when the number of objects is bounded. Our results show that for $\\Sigma_k$ formulas, and for corresponding GFODDs, evaluation and satisfiability are $\\Sigma_k^p$ complete, and equivalence is $\\Pi_{k+1}^p$ complete. For $\\Pi_k$ formulas evaluation is $\\Pi_k^p$ complete, satisfiability is one level higher and is $\\Sigma_{k+1}^p$ complete, and equivalence is $\\Pi_{k+1}^p$ complete.\n    ",
        "submission_date": "2014-07-05T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1474",
        "title": "Fuzzy Model on Human Emotions Recognition",
        "authors": [
            "Kaveh Bakhtiyari",
            "Hafizah Husain"
        ],
        "abstract": "This paper discusses a fuzzy model for multi-level human emotions recognition by computer systems through keyboard keystrokes, mouse and touchscreen interactions. This model can also be used to detect the other possible emotions at the time of recognition. Accuracy measurements of human emotions by the fuzzy model are discussed through two methods; the first is accuracy analysis and the second is false positive rate analysis. This fuzzy model detects more emotions, but on the other hand, for some of emotions, a lower accuracy was obtained with the comparison with the non-fuzzy human emotions detection methods. This system was trained and tested by Support Vector Machine (SVM) to recognize the users' emotions. Overall, this model represents a closer similarity between human brain detection of emotions and computer systems.\n    ",
        "submission_date": "2014-07-06T00:00:00",
        "last_modified_date": "2014-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1584",
        "title": "A Coordinated MDP Approach to Multi-Agent Planning for Resource Allocation, with Applications to Healthcare",
        "authors": [
            "Hadi Hosseini",
            "Jesse Hoey",
            "Robin Cohen"
        ],
        "abstract": "This paper considers a novel approach to scalable multiagent resource allocation in dynamic settings. We propose an approximate solution in which each resource consumer is represented by an independent MDP-based agent that models expected utility using an average model of its expected access to resources given only limited information about all other agents. A global auction-based mechanism is proposed for allocations based on expected regret. We assume truthful bidding and a cooperative coordination mechanism, as we are considering healthcare scenarios. We illustrate the performance of our coordinated MDP approach against a Monte-Carlo based planning algorithm intended for large-scale applications, as well as other approaches suitable for allocating medical resources. The evaluations show that the global utility value across all consumer agents is closer to optimal when using our algorithms under certain time constraints, with low computational cost. As such, we offer a promising approach for addressing complex resource allocation problems that arise in healthcare settings.\n    ",
        "submission_date": "2014-07-07T00:00:00",
        "last_modified_date": "2014-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2506",
        "title": "Discovery of Important Crossroads in Road Network using Massive Taxi Trajectories",
        "authors": [
            "Ming Xu",
            "Jianping Wu",
            "Yiman Du",
            "Haohan Wang",
            "Geqi Qi",
            "Kezhen Hu",
            "Yunpeng Xiao"
        ],
        "abstract": "A major problem in road network analysis is discovery of important crossroads, which can provide useful information for transport planning. However, none of existing approaches addresses the problem of identifying network-wide important crossroads in real road network. In this paper, we propose a novel data-driven based approach named CRRank to rank important crossroads. Our key innovation is that we model the trip network reflecting real travel demands with a tripartite graph, instead of solely analysis on the topology of road network. To compute the importance scores of crossroads accurately, we propose a HITS-like ranking algorithm, in which a procedure of score propagation on our tripartite graph is performed. We conduct experiments on CRRank using a real-world dataset of taxi trajectories. Experiments verify the utility of CRRank.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2646",
        "title": "Learning Probabilistic Programs",
        "authors": [
            "Yura N. Perov",
            "Frank D. Wood"
        ],
        "abstract": "We develop a technique for generalising from data in which models are samplers represented as program text. We establish encouraging empirical results that suggest that Markov chain Monte Carlo probabilistic programming inference techniques coupled with higher-order probabilistic programming languages are now sufficiently powerful to enable successful inference of this kind in nontrivial domains. We also introduce a new notion of probabilistic program compilation and show how the same machinery might be used in the future to compile probabilistic programs for efficient reusable predictive inference.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2873",
        "title": "Possibilities of technologization of philosophical knowledge",
        "authors": [
            "Sergey Kulikov"
        ],
        "abstract": "Article purpose is the analysis of a question of possibility of technologization of philosophical knowledge. We understand the organization of cognitive activity which is guided by the set of methods guaranteed bringing to successful (i.e. to precisely corresponding set parameters) to applied results as technologization. Transformation of sense of philosophy allows revealing possibilities of its technologization. The leading role in this process is played by philosophy of science which creates conditions for such transformation. At the same time there is justified an appeal to branch combination theory of the directions of scientific knowledge and partial refusal of understanding of philosophy as synthetic knowledge in which the main task is permission, instead of generation of paradoxes.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3130",
        "title": "Allocation in Practice",
        "authors": [
            "Toby Walsh"
        ],
        "abstract": "How do we allocate scarcere sources? How do we fairly allocate costs? These are two pressing challenges facing society today. I discuss two recent projects at NICTA concerning resource and cost allocation. In the first, we have been working with FoodBank Local, a social startup working in collaboration with food bank charities around the world to optimise the logistics of collecting and distributing donated food. Before we can distribute this food, we must decide how to allocate it to different charities and food kitchens. This gives rise to a fair division problem with several new dimensions, rarely considered in the literature. In the second, we have been looking at cost allocation within the distribution network of a large multinational company. This also has several new dimensions rarely considered in the literature.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3208",
        "title": "Decision-Making with Complex Data Structures using Probabilistic Programming",
        "authors": [
            "Brian E. Ruttenberg",
            "Avi Pfeffer"
        ],
        "abstract": "Existing decision-theoretic reasoning frameworks such as decision networks use simple data structures and processes. However, decisions are often made based on complex data structures, such as social networks and protein sequences, and rich processes involving those structures. We present a framework for representing decision problems with complex data structures using probabilistic programming, allowing probabilistic models to be created with programming language constructs such as data structures and control flow. We provide a way to use arbitrary data types with minimal effort from the user, and an approximate decision-making algorithm that is effective even when the information space is very large or infinite. Experimental results show our algorithm working on problems with very large information spaces.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3211",
        "title": "Possibility neutrosophic soft sets with applications in decision making and similarity measure",
        "authors": [
            "Faruk Karaaslan"
        ],
        "abstract": "In this paper, concept of possibility neutrosophic soft set and its operations are defined, and their properties are studied. An application of this theory in decision making is investigated. Also a similarity measure of two possibility neutrosophic soft sets is introduced and discussed. Finally an application of this similarity measure is given to select suitable person for position in a firm.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3269",
        "title": "Multiple chaotic central pattern generators with learning for legged locomotion and malfunction compensation",
        "authors": [
            "Guanjiao Ren",
            "Weihai Chen",
            "Sakyasingha Dasgupta",
            "Christoph Kolodziejski",
            "Florentin W\u00f6rg\u00f6tter",
            "Poramate Manoonpong"
        ],
        "abstract": "An originally chaotic system can be controlled into various periodic dynamics. When it is implemented into a legged robot's locomotion control as a central pattern generator (CPG), sophisticated gait patterns arise so that the robot can perform various walking behaviors. However, such a single chaotic CPG controller has difficulties dealing with leg malfunction. Specifically, in the scenarios presented here, its movement permanently deviates from the desired trajectory. To address this problem, we extend the single chaotic CPG to multiple CPGs with learning. The learning mechanism is based on a simulated annealing algorithm. In a normal situation, the CPGs synchronize and their dynamics are identical. With leg malfunction or disability, the CPGs lose synchronization leading to independent dynamics. In this case, the learning mechanism is applied to automatically adjust the remaining legs' oscillation frequencies so that the robot adapts its locomotion to deal with the malfunction. As a consequence, the trajectory produced by the multiple chaotic CPGs resembles the original trajectory far better than the one produced by only a single CPG. The performance of the system is evaluated first in a physical simulation of a quadruped as well as a hexapod robot and finally in a real six-legged walking machine called AMOSII. The experimental results presented here reveal that using multiple CPGs with learning is an effective approach for adaptive locomotion generation where, for instance, different body parts have to perform independent movements for malfunction compensation.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3341",
        "title": "Extreme State Aggregation Beyond MDPs",
        "authors": [
            "Marcus Hutter"
        ],
        "abstract": "We consider a Reinforcement Learning setup where an agent interacts with an environment in observation-reward-action cycles without any (esp.\\ MDP) assumptions on the environment. State aggregation and more generally feature reinforcement learning is concerned with mapping histories/raw-states to reduced/aggregated states. The idea behind both is that the resulting reduced process (approximately) forms a small stationary finite-state MDP, which can then be efficiently solved or learnt. We considerably generalize existing aggregation results by showing that even if the reduced process is not an MDP, the (q-)value functions and (optimal) policies of an associated MDP with same state-space size solve the original problem, as long as the solution can approximately be represented as a function of the reduced states. This implies an upper bound on the required state space size that holds uniformly for all RL problems. It may also explain why RL algorithms designed for MDPs sometimes perform well beyond MDPs.\n    ",
        "submission_date": "2014-07-12T00:00:00",
        "last_modified_date": "2014-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3512",
        "title": "A New Rational Algorithm for View Updating in Relational Databases",
        "authors": [
            "Radhakrishnan Delhibabu",
            "Andreas Behrend"
        ],
        "abstract": "The dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information. In order to apply the rationality result of belief dynamics theory to various practical problems, it should be generalized in two respects: first it should allow a certain part of belief to be declared as immutable; and second, the belief state need not be deductively closed. Such a generalization of belief dynamics, referred to as base dynamics, is presented in this paper, along with the concept of a generalized revision algorithm for knowledge bases (Horn or Horn logic with stratified negation). We show that knowledge base dynamics has an interesting connection with kernel change via hitting set and abduction. In this paper, we show how techniques from disjunctive logic programming can be used for efficient (deductive) database updates. The key idea is to transform the given database together with the update request into a disjunctive (datalog) logic program and apply disjunctive techniques (such as minimal model reasoning) to solve the original update problem. The approach extends and integrates standard techniques for efficient query answering and integrity checking. The generation of a hitting set is carried out through a hyper tableaux calculus and magic set that is focused on the goal of minimality.\n    ",
        "submission_date": "2014-07-13T00:00:00",
        "last_modified_date": "2014-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3832",
        "title": "Non-Monotonic Reasoning and Story Comprehension",
        "authors": [
            "Irene-Anna Diakidoy",
            "Antonis Kakas",
            "Loizos Michael",
            "Rob Miller"
        ],
        "abstract": "This paper develops a Reasoning about Actions and Change framework integrated with Default Reasoning, suitable as a Knowledge Representation and Reasoning framework for Story Comprehension. The proposed framework, which is guided strongly by existing knowhow from the Psychology of Reading and Comprehension, is based on the theory of argumentation from AI. It uses argumentation to capture appropriate solutions to the frame, ramification and qualification problems and generalizations of these problems required for text comprehension. In this first part of the study the work concentrates on the central problem of integration (or elaboration) of the explicit information from the narrative in the text with the implicit (in the readers mind) common sense world knowledge pertaining to the topic(s) of the story given in the text. We also report on our empirical efforts to gather background common sense world knowledge used by humans when reading a story and to evaluate, through a prototype system, the ability of our approach to capture both the majority and the variability of understanding of a story by the human readers in the experiments.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3836",
        "title": "Imparo is complete by inverse subsumption",
        "authors": [
            "David Toth"
        ],
        "abstract": "In Inverse subsumption for complete explanatory induction Yamamoto et al. investigate which inductive logic programming systems can learn a correct hypothesis $H$ by using the inverse subsumption instead of inverse entailment. We prove that inductive logic programming system Imparo is complete by inverse subsumption for learning a correct definite hypothesis $H$ wrt the definite background theory $B$ and ground atomic examples $E$, by establishing that there exists a connected theory $T$ for $B$ and $E$ such that $H$ subsumes $T$.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3896",
        "title": "Abduction and Dialogical Proof in Argumentation and Logic Programming",
        "authors": [
            "Richard Booth",
            "Dov Gabbay",
            "Souhila Kaci",
            "Tjitze Rienstra",
            "Leendert van der Torre"
        ],
        "abstract": "We develop a model of abduction in abstract argumentation, where changes to an argumentation framework act as hypotheses to explain the support of an observation. We present dialogical proof theories for the main decision problems (i.e., finding hypothe- ses that explain skeptical/credulous support) and we show that our model can be instantiated on the basis of abductive logic programs.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2014-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3926",
        "title": "Strategy Synthesis for General Deductive Games Based on SAT Solving",
        "authors": [
            "Miroslav Klimos",
            "Antonin Kucera"
        ],
        "abstract": "We propose a general framework for modelling and solving deductive games, where one player selects a secret code and the other player strives to discover this code using a minimal number of allowed experiments that reveal some partial information about the code. The framework is implemented in a software tool Cobra, and its functionality is demonstrated by producing new results about existing deductive games.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4139",
        "title": "Subjectivity, Bayesianism, and Causality",
        "authors": [
            "Pedro A. Ortega"
        ],
        "abstract": "Bayesian probability theory is one of the most successful frameworks to model reasoning under uncertainty. Its defining property is the interpretation of probabilities as degrees of belief in propositions about the state of the world relative to an inquiring subject. This essay examines the notion of subjectivity by drawing parallels between Lacanian theory and Bayesian probability theory, and concludes that the latter must be enriched with causal interventions to model agency. The central contribution of this work is an abstract model of the subject that accommodates causal interventions in a measure-theoretic formalisation. This formalisation is obtained through a game-theoretic Ansatz based on modelling the inside and outside of the subject as an extensive-form game with imperfect information between two players. Finally, I illustrate the expressiveness of this model with an example of causal induction.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4234",
        "title": "A Plausibility Semantics for Abstract Argumentation Frameworks",
        "authors": [
            "Emil Weydert"
        ],
        "abstract": "We propose and investigate a simple ranking-measure-based extension semantics for abstract argumentation frameworks based on their generic instantiation by default knowledge bases and the ranking construction semantics for default reasoning. In this context, we consider the path from structured to logical to shallow semantic instantiations. The resulting well-justified JZ-extension semantics diverges from more traditional approaches.\n    ",
        "submission_date": "2014-07-16T00:00:00",
        "last_modified_date": "2014-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4360",
        "title": "Data Assimilation by Artificial Neural Networks for an Atmospheric General Circulation Model: Conventional Observation",
        "authors": [
            "Rosangela S. Cintra",
            "Haroldo F. de Campos Velho"
        ],
        "abstract": "This paper presents an approach for employing artificial neural networks (NN) to emulate an ensemble Kalman filter (EnKF) as a method of data assimilation. The assimilation methods are tested in the Simplified Parameterizations PrimitivE-Equation Dynamics (SPEEDY) model, an atmospheric general circulation model (AGCM), using synthetic observational data simulating localization of balloon soundings. For the data assimilation scheme, the supervised NN, the multilayer perceptrons (MLP-NN), is applied. The MLP-NN are able to emulate the analysis from the local ensemble transform Kalman filter (LETKF). After the training process, the method using the MLP-NN is seen as a function of data assimilation. The NN were trained with data from first three months of 1982, 1983, and 1984. A hind-casting experiment for the 1985 data assimilation cycle using MLP-NN were performed with synthetic observations for January 1985. The numerical results demonstrate the effectiveness of the NN technique for atmospheric data assimilation. The results of the NN analyses are very close to the results from the LETKF analyses, the differences of the monthly average of absolute temperature analyses is of order 0.02. The simulations show that the major advantage of using the MLP-NN is better computational performance, since the analyses have similar quality. The CPU-time cycle assimilation with MLP-NN is 90 times faster than cycle assimilation with LETKF for the numerical experiment.\n    ",
        "submission_date": "2014-07-16T00:00:00",
        "last_modified_date": "2014-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4364",
        "title": "One-Step or Two-Step Optimization and the Overfitting Phenomenon: A Case Study on Time Series Classification",
        "authors": [
            "Muhammad Marwan Muhammad Fuad"
        ],
        "abstract": "For the last few decades, optimization has been developing at a fast rate. Bio-inspired optimization algorithms are metaheuristics inspired by nature. These algorithms have been applied to solve different problems in engineering, economics, and other domains. Bio-inspired algorithms have also been applied in different branches of information technology such as networking and software engineering. Time series data mining is a field of information technology that has its share of these applications too. In previous works we showed how bio-inspired algorithms such as the genetic algorithms and differential evolution can be used to find the locations of the breakpoints used in the symbolic aggregate approximation of time series representation, and in another work we showed how we can utilize the particle swarm optimization, one of the famous bio-inspired algorithms, to set weights to the different segments in the symbolic aggregate approximation representation. In this paper we present, in two different approaches, a new meta optimization process that produces optimal locations of the breakpoints in addition to optimal weights of the segments. The experiments of time series classification task that we conducted show an interesting example of how the overfitting phenomenon, a frequently encountered problem in data mining which happens when the model overfits the training set, can interfere in the optimization process and hide the superior performance of an optimization algorithm.\n    ",
        "submission_date": "2014-07-16T00:00:00",
        "last_modified_date": "2014-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4490",
        "title": "Virus Detection in Multiplexed Nanowire Arrays using Hidden Semi-Markov models",
        "authors": [
            "Shalini Ghosh",
            "Patrick Lincoln",
            "Christian Petersen",
            "Alfonso Valdes"
        ],
        "abstract": "In this paper, we address the problem of real-time detection of viruses docking to nanowires, especially when multiple viruses dock to the same nano-wire. The task becomes more complicated when there is an array of nanowires coated with different antibodies, where different viruses can dock to each coated nanowire at different binding strengths. We model the array response to a viral agent as a pattern of conductance change over nanowires with known modifier --- this representation permits analysis of the output of such an array via belief network (Bayes) methods, as well as novel generative models like the Hidden Semi-Markov Model (HSMM).\n    ",
        "submission_date": "2014-07-16T00:00:00",
        "last_modified_date": "2014-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4709",
        "title": "Flow for Meta Control",
        "authors": [
            "Vadim Bulitko"
        ],
        "abstract": "The psychological state of flow has been linked to optimizing human performance. A key condition of flow emergence is a match between the human abilities and complexity of the task. We propose a simple computational model of flow for Artificial Intelligence (AI) agents. The model factors the standard agent-environment state into a self-reflective set of the agent's abilities and a socially learned set of the environmental complexity. Maximizing the flow serves as a meta control for the agent. We show how to apply the meta-control policy to a broad class of AI control policies and illustrate our approach with a specific implementation. Results in a synthetic testbed are promising and open interesting directions for future work.\n    ",
        "submission_date": "2014-07-17T00:00:00",
        "last_modified_date": "2014-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4833",
        "title": "$OntoMath^{PRO}$ Ontology: A Linked Data Hub for Mathematics",
        "authors": [
            "Olga Nevzorova",
            "Nikita Zhiltsov",
            "Alexander Kirillovich",
            "Evgeny Lipachev"
        ],
        "abstract": "In this paper, we present an ontology of mathematical knowledge concepts that covers a wide range of the fields of mathematics and introduces a balanced representation between comprehensive and sensible models. We demonstrate the applications of this representation in information extraction, semantic search, and education. We argue that the ontology can be a core of future integration of math-aware data sets in the Web of Data and, therefore, provide mappings onto relevant datasets, such as DBpedia and ScienceWISE.\n    ",
        "submission_date": "2014-07-17T00:00:00",
        "last_modified_date": "2014-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4863",
        "title": "A Comparative Study of Meta-heuristic Algorithms for Solving Quadratic Assignment Problem",
        "authors": [
            "Gamal Abd El-Nasser A. Said",
            "Abeer M. Mahmoud",
            "El-Sayed M. El-Horbaty"
        ],
        "abstract": "Quadratic Assignment Problem (QAP) is an NP-hard combinatorial optimization problem, therefore, solving the QAP requires applying one or more of the meta-heuristic algorithms. This paper presents a comparative study between Meta-heuristic algorithms: Genetic Algorithm, Tabu Search, and Simulated annealing for solving a real-life (QAP) and analyze their performance in terms of both runtime efficiency and solution quality. The results show that Genetic Algorithm has a better solution quality while Tabu Search has a faster execution time in comparison with other Meta-heuristic algorithms for solving QAP.\n    ",
        "submission_date": "2014-07-18T00:00:00",
        "last_modified_date": "2014-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5212",
        "title": "Context Aware Dynamic Traffic Signal Optimization",
        "authors": [
            "Kandarp Khandwala",
            "Rudra Sharma",
            "Snehal Rao"
        ],
        "abstract": "Conventional urban traffic control systems have been based on historical traffic data. Later advancements made use of detectors, which enabled the gathering of real time traffic data, in order to reorganize and calibrate traffic signalization programs. Further evolvement provided the ability to forecast traffic conditions, in order to develop traffic signalization programs and strategies precomputed and applied at the most appropriate time frame for the optimal control of the current traffic conditions. We, propose the next generation of traffic control systems based on principles of Artificial Intelligence and Context Awareness. Most of the existing algorithms use average waiting time or length of the queue to assess an algorithms performance. However, a low average waiting time may come at the cost of delaying other vehicles indefinitely. In our algorithm, besides the vehicle queue, we use fairness also as an important performance metric to assess an algorithms performance.\n    ",
        "submission_date": "2014-07-19T00:00:00",
        "last_modified_date": "2014-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5380",
        "title": "Representing and Reasoning about Game Strategies",
        "authors": [
            "Dongmo Zhang",
            "Michael Thielsher"
        ],
        "abstract": "As a contribution to the challenge of building game-playing AI systems, we develop and analyse a formal language for representing and reasoning about strategies. Our logical language builds on the existing general Game Description Language (GDL) and extends it by a standard modality for linear time along with two dual connectives to express preferences when combining strategies. The semantics of the language is provided by a standard state-transition model. As such, problems that require reasoning about games can be solved by the standard methods for reasoning about actions and change. We also endow the language with a specific semantics by which strategy formulas are understood as move recommendations for a player. To illustrate how our formalism supports automated reasoning about strategies, we demonstrate two example methods of implementation\\/: first, we formalise the semantic interpretation of our language in conjunction with game rules and strategy rules in the Situation Calculus; second, we show how the reasoning problem can be solved with Answer Set Programming.\n    ",
        "submission_date": "2014-07-21T00:00:00",
        "last_modified_date": "2014-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5574",
        "title": "A Novel Hybrid Crossover based Artificial Bee Colony Algorithm for Optimization Problem",
        "authors": [
            "Sandeep Kumar",
            "Vivek Kumar Sharma",
            "Rajani Kumari"
        ],
        "abstract": "Artificial bee colony (ABC) algorithm has proved its importance in solving a number of problems including engineering optimization problems. ABC algorithm is one of the most popular and youngest member of the family of population based nature inspired meta-heuristic swarm intelligence method. ABC has been proved its superiority over some other Nature Inspired Algorithms (NIA) when applied for both benchmark functions and real world problems. The performance of search process of ABC depends on a random value which tries to balance exploration and exploitation phase. In order to increase the performance it is required to balance the exploration of search space and exploitation of optimal solution of the ABC. This paper outlines a new hybrid of ABC algorithm with Genetic Algorithm. The proposed method integrates crossover operation from Genetic Algorithm (GA) with original ABC algorithm. The proposed method is named as Crossover based ABC (CbABC). The CbABC strengthens the exploitation phase of ABC as crossover enhances exploration of search space. The CbABC tested over four standard benchmark functions and a popular continuous optimization problem.\n    ",
        "submission_date": "2014-07-21T00:00:00",
        "last_modified_date": "2014-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5656",
        "title": "PGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical Data Problems",
        "authors": [
            "Khalifeh AlJadda",
            "Mohammed Korayem",
            "Camilo Ortiz",
            "Trey Grainger",
            "John A. Miller",
            "William S. York"
        ],
        "abstract": "In the big data era, scalability has become a crucial requirement for any useful computational model. Probabilistic graphical models are very useful for mining and discovering data insights, but they are not scalable enough to be suitable for big data problems. Bayesian Networks particularly demonstrate this limitation when their data is represented using few random variables while each random variable has a massive set of values. With hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, Bayesian networks become infeasible for representing the probability distributions for the following reasons: i) Each level represents a single random variable with hundreds of thousands of values, ii) The number of levels is usually small, so there are also few random variables, and iii) The structure of the network is predefined since the dependency is modeled top-down from each parent to each of its child nodes, so the network would contain a single linear path for the random variables from each parent to each child node. In this paper we present a scalable probabilistic graphical model to overcome these limitations for massive hierarchical data. We believe the proposed model will lead to an easily-scalable, more readable, and expressive implementation for problems that require probabilistic-based solutions for massive amounts of hierarchical data. We successfully applied this model to solve two different challenging probabilistic-based problems on massive hierarchical data sets for different domains, namely, bioinformatics and latent semantic discovery over search logs.\n    ",
        "submission_date": "2014-07-21T00:00:00",
        "last_modified_date": "2014-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5754",
        "title": "Tree-based iterated local search for Markov random fields with applications in image analysis",
        "authors": [
            "Truyen Tran",
            "Dinh Phung",
            "Svetha Venkatesh"
        ],
        "abstract": "The \\emph{maximum a posteriori} (MAP) assignment for general structure Markov random fields (MRFs) is computationally intractable. In this paper, we exploit tree-based methods to efficiently address this problem. Our novel method, named Tree-based Iterated Local Search (T-ILS) takes advantage of the tractability of tree-structures embedded within MRFs to derive strong local search in an ILS framework. The method efficiently explores exponentially large neighborhood and does so with limited memory without any requirement on the cost functions. We evaluate the T-ILS in a simulation of Ising model and two real-world problems in computer vision: stereo matching, image denoising. Experimental results demonstrate that our methods are competitive against state-of-the-art rivals with a significant computational gain.\n    ",
        "submission_date": "2014-07-22T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6090",
        "title": "Social and Business Intelligence Analysis Using PSO",
        "authors": [
            "Jyoti Chaturvedi",
            "Anubha Parashar",
            "Amrita A Manjrekar",
            "Vinay S Bhaskar"
        ],
        "abstract": "The goal of this paper is to elaborate swarm intelligence for business intelligence decision making and the business rules management improvement. .The swarm optimization, which is highly influenced by the behavior of creature, performs in group. The Spatial data is defined as data that is represented by 2D or 3D images. SQL Server supports only 2D images till now. As we know that location is an essential part of any organizational data as well as business data enterprises maintain customer address lists, own property, ship goods from and to warehouses, manage transport flows among their workforce, and perform many other activities. By means to say a lot of spatial data is used and processed by enterprises, organizations and other bodies in order to make the things more visible and self descriptive. From the experiments, we found that PSO is can facilitate the intelligence in social and business behavior.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6166",
        "title": "M-best solutions for a class of fuzzy constraint satisfaction problems",
        "authors": [
            "Michail Schlesinger",
            "Boris Flach",
            "Evgeniy Vodolazskiy"
        ],
        "abstract": "The article considers one of the possible generalizations of constraint satisfaction problems where relations are replaced by multivalued membership functions. In this case operations of disjunction and conjunction are replaced by maximum and minimum, and consistency of a solution becomes multivalued rather than binary. The article studies the problem of finding d most admissible solutions for a given d. A tractable subclass of these problems is defined by the concepts of invariants and polymorphisms similar to the classic constraint satisfaction approach. These concepts are adapted in two ways. Firstly, the correspondence of \"invariant-polymorphism\" is generalized to (min,max) semirings. Secondly, we consider non-uniform polymorphisms, where each variable has its own operator, in contrast to the case of one operator common for all variables. The article describes an algorithm that finds $d$ most admissible solutions in polynomial time, provided that the problem is invariant with respect to some non-uniform majority operator. It is essential that this operator needs not to be known for the algorithm to work. Moreover, even a guarantee for the existence of such an operator is not necessary. The algorithm either finds the solution or discards the problem. The latter is possible only if the problem has no majority polymorphism.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6315",
        "title": "Quadratically constrained quadratic programming for classification using particle swarms and applications",
        "authors": [
            "Deepak Kumar",
            "A G Ramakrishnan"
        ],
        "abstract": "Particle swarm optimization is used in several combinatorial optimization problems. In this work, particle swarms are used to solve quadratic programming problems with quadratic constraints. The approach of particle swarms is an example for interior point methods in optimization as an iterative technique. This approach is novel and deals with classification problems without the use of a traditional classifier. Our method determines the optimal hyperplane or classification boundary for a data set. In a binary classification problem, we constrain each class as a cluster, which is enclosed by an ellipsoid. The estimation of the optimal hyperplane between the two clusters is posed as a quadratically constrained quadratic problem. The optimization problem is solved in distributed format using modified particle swarms. Our method has the advantage of using the direction towards optimal solution rather than searching the entire feasible region. Our results on the Iris, Pima, Wine, and Thyroid datasets show that the proposed method works better than a neural network and the performance is close to that of SVM.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6699",
        "title": "Fuzzy inference system for integrated VVC in isolated power systems",
        "authors": [
            "Eduardo Vega-Fuentes",
            "Juan Manuel Cerezo-Sanchez",
            "Sonia Leon-del Rosario",
            "Aurelio Vega-Martinez"
        ],
        "abstract": "This paper presents a fuzzy inference system for integrated volt/var control (VVC) in distribution substations. The purpose is go forward to automation distribution applying conservation voltage reduction (CVR) in isolated power systems where control capabilities are limited. A fuzzy controller has been designed. Working as an on-line tool, it has been tested under real conditions and it has managed the operation during a whole day in a distribution substation. Within the limits of control capabilities of the system, the controller maintained successfully an acceptable voltage profile, power factor values over 0,98 and it has ostensibly improved the performance given by an optimal power flow based automation system. CVR savings during the test are evaluated and the aim to integrate it in the VVC is presented.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6885",
        "title": "Extending Acyclicity Notions for Existential Rules (\\emph{long version})",
        "authors": [
            "Jean-Francois Baget",
            "Fabien Garreau",
            "Marie-Laure Mugnier",
            "Swan Rocher"
        ],
        "abstract": "Existential rules have been proposed for representing ontological knowledge, specifically in the context of Ontology-Based Query Answering. Entailment with existential rules is undecidable. We focus in this paper on conditions that ensure the termination of a breadth-first forward chaining algorithm known as the chase. First, we propose a new tool that allows to extend existing acyclicity conditions ensuring chase termination, while keeping good complexity properties. Second, we consider the extension to existential rules with nonmonotonic negation under stable model semantics and further extend acyclicity results obtained in the positive case.\n    ",
        "submission_date": "2014-07-25T00:00:00",
        "last_modified_date": "2014-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7008",
        "title": "Modeling and Recognition of Smart Grid Faults by a Combined Approach of Dissimilarity Learning and One-Class Classification",
        "authors": [
            "Enrico De Santis",
            "Lorenzo Livi",
            "Alireza Sadeghian",
            "Antonello Rizzi"
        ],
        "abstract": "Detecting faults in electrical power grids is of paramount importance, either from the electricity operator and consumer viewpoints. Modern electric power grids (smart grids) are equipped with smart sensors that allow to gather real-time information regarding the physical status of all the component elements belonging to the whole infrastructure (e.g., cables and related insulation, transformers, breakers and so on). In real-world smart grid systems, usually, additional information that are related to the operational status of the grid itself are collected such as meteorological information. Designing a suitable recognition (discrimination) model of faults in a real-world smart grid system is hence a challenging task. This follows from the heterogeneity of the information that actually determine a typical fault condition. The second point is that, for synthesizing a recognition model, in practice only the conditions of observed faults are usually meaningful. Therefore, a suitable recognition model should be synthesized by making use of the observed fault conditions only. In this paper, we deal with the problem of modeling and recognizing faults in a real-world smart grid system, which supplies the entire city of Rome, Italy. Recognition of faults is addressed by following a combined approach of multiple dissimilarity measures customization and one-class classification techniques. We provide here an in-depth study related to the available data and to the models synthesized by the proposed one-class classifier. We offer also a comprehensive analysis of the fault recognition results by exploiting a fuzzy set based reliability decision rule.\n    ",
        "submission_date": "2014-07-25T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7138",
        "title": "Data granulation by the principles of uncertainty",
        "authors": [
            "Lorenzo Livi",
            "Alireza Sadeghian"
        ],
        "abstract": "Researches in granular modeling produced a variety of mathematical models, such as intervals, (higher-order) fuzzy sets, rough sets, and shadowed sets, which are all suitable to characterize the so-called information granules. Modeling of the input data uncertainty is recognized as a crucial aspect in information granulation. Moreover, the uncertainty is a well-studied concept in many mathematical settings, such as those of probability theory, fuzzy set theory, and possibility theory. This fact suggests that an appropriate quantification of the uncertainty expressed by the information granule model could be used to define an invariant property, to be exploited in practical situations of information granulation. In this perspective, a procedure of information granulation is effective if the uncertainty conveyed by the synthesized information granule is in a monotonically increasing relation with the uncertainty of the input data. In this paper, we present a data granulation framework that elaborates over the principles of uncertainty introduced by Klir. Being the uncertainty a mesoscopic descriptor of systems and data, it is possible to apply such principles regardless of the input data type and the specific mathematical setting adopted for the information granules. The proposed framework is conceived (i) to offer a guideline for the synthesis of information granules and (ii) to build a groundwork to compare and quantitatively judge over different data granulation procedures. To provide a suitable case study, we introduce a new data granulation technique based on the minimum sum of distances, which is designed to generate type-2 fuzzy sets. We analyze the procedure by performing different experiments on two distinct data types: feature vectors and labeled graphs. Results show that the uncertainty of the input data is suitably conveyed by the generated type-2 fuzzy set models.\n    ",
        "submission_date": "2014-07-26T00:00:00",
        "last_modified_date": "2015-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7180",
        "title": "Defining Relative Likelihood in Partially-Ordered Preferential Structures",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "Starting with a likelihood or preference order on worlds, we extend it to a likelihood ordering on sets of worlds in a natural way, and examine the resulting logic.  Lewis (1973) earlier considered such a notion of relative likelihood in the context of studying counterfactuals, but he assumed a total preference order on worlds.  Complications arise when examining partial orders that are not present for total orders.  There are subtleties involving the exact approach to lifting the order on worlds to an order on sets of worlds. In addition, the axiomatization of the logic of relative likelihood in the case of partial orders gives insight into the connection between relative likelihood and default reasoning.\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7182",
        "title": "Conditional Plausibility Measures and Bayesian Networks",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "A general notion of algebraic conditional plausibility measures is defined.  Probability measures, ranking functions, possibility measures, and (under the appropriate definitions) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures.  It is shown that the technology of Bayesian networks can be applied to algebraic conditional plausibility measures.\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7183",
        "title": "Updating Probabilities",
        "authors": [
            "Peter D. Grunwald",
            "Joseph Y. Halpern"
        ],
        "abstract": "As examples such as the Monty Hall puzzle show, applying conditioning to update a probability distribution on a ``naive space', which does not take into account the protocol used, can often lead to counterintuitive results.  Here we examine why.  A criterion known as CAR (coarsening at random) in the statistical literature  characterizes when ``naive' conditioning in a naive space works.  We show that the CAR condition holds rather infrequently.  We then consider more generalized notions of update such as Jeffrey conditioning and minimizing relative entropy (MRE).  We give a generalization of the CAR condition that characterizes when Jeffrey conditioning leads to appropriate answers, but show that there are no such conditions for MRE. This generalizes and interconnects previous results obtained in the literature on CAR and MRE.\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7184",
        "title": "Reasoning about Expectation",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "abstract": "Expectation is a central notion in probability theory. The notion of expectation also makes sense for other notions of uncertainty.  We introduce a propositional logic for reasoning about expectation, where the semantics depends on the underlying representation  of uncertainty.  We give sound and complete axiomatizations for the logic in the case that the underlying representation is (a) probability, (b) sets of probability measures, (c) belief functions, and (d) possibility measures.  We show that this logic is more expressive than the corresponding logic for reasoning about likelihood in the case of sets of probability measures, but equi-expressive in the case of probability, belief, and possibility. Finally, we show that satisfiability for these logics is NP-complete, no harder than satisfiability for propositional logic.  \n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7185",
        "title": "A Logic for Reasoning about Evidence",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "abstract": "We introduce a logic for reasoning about evidence, that essentially views     evidence as a function from prior beliefs (before making an observation) to     posterior beliefs (after making the observation). We provide a sound and     complete axiomatization for the logic, and consider the complexity of the     decision problem. Although the reasoning in the logic is mainly propositional,     we allow variables representing numbers and quantification over them. This     expressive power seems necessary to capture important properties of evidence\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7188",
        "title": "When Ignorance is Bliss",
        "authors": [
            "Peter D. Grunwald",
            "Joseph Y. Halpern"
        ],
        "abstract": "It is commonly-accepted wisdom that more information is better, and that information should never be ignored. Here we argue, using both a Bayesian and a non-Bayesian analysis, that in some situations you are better off ignoring information if your uncertainty is represented by a set of probability measures. These include situations in which the information is relevant for the prediction task at hand. In the non-Bayesian analysis, we show how ignoring information avoids dilation, the phenomenon that additional pieces of information sometimes lead to an increase in uncertainty. In the Bayesian analysis, we show that for small sample sizes and certain prediction tasks, the Bayesian posterior based on a noninformative prior yields worse predictions than simply ignoring the given information.\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7189",
        "title": "Evidence with Uncertain Likelihoods",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "abstract": "An agent often has a number of hypotheses, and must choose among them based on observations, or outcomes of experiments. Each of these observations can be viewed as providing evidence for or against various hypotheses. All the attempts to formalize this intuition up to now have assumed that associated with each hypothesis h there is a likelihood function {\\mu}h, which is a probability measure that intuitively describes how likely each observation is, conditional on h being the correct hypothesis. We consider an extension of this framework where there is uncertainty as to which of a number of likelihood functions is appropriate, and discuss how one formal approach to defining evidence, which views evidence as a function from priors to posteriors, can be generalized to accommodate this uncertainty.\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7190",
        "title": "A Game-Theoretic Analysis of Updating Sets of Probabilities",
        "authors": [
            "Peter D. Grunwald",
            "Joseph Y. Halpern"
        ],
        "abstract": "We consider how an agent should update her uncertainty when it is represented by a set P of probability distributions and the agent observes that a random variable X takes on value x, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. We adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from P. We consider two reasonable games that differ in what the bookie knows when he makes his choice. Anomalies that have been observed before, like time inconsistency, can be understood as arising because different games are being played, against bookies with different information. We characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. Finally, we consider the relationship between conditioning and calibration when uncertainty is described by sets of probabilities.\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7191",
        "title": "MDPs with Unawareness",
        "authors": [
            "Joseph Y. Halpern",
            "Nan Rong",
            "Ashutosh Saxena"
        ],
        "abstract": "Markov decision processes (MDPs) are widely used for modeling decision-making problems in robotics, automated control, and economics. Traditional MDPs assume that the decision maker (DM) knows all states and actions. However, this may not be true in many situations of interest. We define a new framework, MDPs with unawareness (MDPUs) to deal with the possibilities that a DM may not be aware of all possible actions. We provide a complete characterization of when a DM can learn to play near-optimally in an MDPU, and give an algorithm that learns to play near-optimally when it is possible to do so, as efficiently as possible. In particular, we characterize when a near-optimal solution can be found in polynomial time.\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7281",
        "title": "Modular Belief Updates and Confusion about Measures of Certainty in Artificial Intelligence Research",
        "authors": [
            "Eric J. Horvitz",
            "David Heckerman"
        ],
        "abstract": "Over the last decade, there has been growing interest in the use or measures or change in belief for reasoning with uncertainty in artificial intelligence research. An important characteristic of several methodologies that reason with changes in belief or belief updates, is a property that we term modularity. We call updates that satisfy this property modular updates. Whereas probabilistic measures of belief update - which satisfy the modularity property were first discovered in the nineteenth century, knowledge and discussion of these quantities remains obscure in artificial intelligence research. We define modular updates and discuss their inappropriate use in two influential expert systems.\n    ",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7933",
        "title": "Graph Transformation Planning via Abstraction",
        "authors": [
            "Steffen Ziegert"
        ],
        "abstract": "Modern software systems increasingly incorporate self-* behavior to adapt to changes in the environment at runtime. Such adaptations often involve reconfiguring the software architecture of the system. Many systems also need to manage their architecture themselves, i.e., they need a planning component to autonomously decide which reconfigurations to execute to reach a desired target configuration. For the specification of reconfigurations, we employ graph transformations systems (GTS) due to the close relation of graphs and UML object diagrams. We solve the resulting planning problems with a planning system that works directly on a GTS. It features a domain-independent heuristic that uses the solution length of an abstraction of the original problem as an estimate. Finally, we provide experimental results on two different domains, which confirm that our heuristic performs better than another domain-independent heuristic which resembles heuristics employed in related work.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7934",
        "title": "Backwards State-space Reduction for Planning in Dynamic Knowledge Bases",
        "authors": [
            "Valerio Senni",
            "Michele Stawowy"
        ],
        "abstract": "In this paper we address the problem of planning in rich domains, where knowledge representation is a key aspect for managing the complexity and size of the planning domain. We follow the approach of Description Logic (DL) based Dynamic Knowledge Bases, where a state of the world is represented concisely by a (possibly changing) ABox and a (fixed) TBox containing the axioms, and actions that allow to change the content of the ABox. The plan goal is given in terms of satisfaction of a DL query. In this paper we start from a traditional forward planning algorithm and we propose a much more efficient variant by combining backward and forward search. In particular, we propose a Backward State-space Reduction technique that consists in two phases: first, an Abstract Planning Graph P is created by using the Abstract Backward Planning Algorithm (ABP), then the abstract planning graph P is instantiated into a corresponding planning graph P by using the Forward Plan Instantiation Algorithm (FPI). The advantage is that in the preliminary ABP phase we produce a symbolic plan that is a pattern to direct the search of the concrete plan. This can be seen as a kind of informed search where the preliminary backward phase is useful to discover properties of the state-space that can be used to direct the subsequent forward phase. We evaluate the effectiveness of our ABP+FPI algorithm in the reduction of the explored planning domain by comparing it to a standard forward planning algorithm and applying both of them to a concrete business case study.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8151",
        "title": "Consistent transformations of belief functions",
        "authors": [
            "Fabio Cuzzolin"
        ],
        "abstract": "Consistent belief functions represent collections of coherent or non-contradictory pieces of evidence, but most of all they are the counterparts of consistent knowledge bases in belief calculus. The use of consistent transformations cs[.] in a reasoning process to guarantee coherence can therefore be desirable, and generalizes similar techniques in classical logic. Transformations can be obtained by minimizing an appropriate distance measure between the original belief function and the collection of consistent ones. We focus here on the case in which distances are measured using classical Lp norms, in both the \"mass space\" and the \"belief space\" representation of belief functions. While mass consistent approximations reassign the mass not focussed on a chosen element of the frame either to the whole frame or to all supersets of the element on an equal basis, approximations in the belief space do distinguish these focal elements according to the \"focussed consistent transformation\" principle. The different approximations are interpreted and compared, with the help of examples.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8392",
        "title": "MONEYBaRL: Exploiting pitcher decision-making using Reinforcement Learning",
        "authors": [
            "Gagan Sidhu",
            "Brian Caffo"
        ],
        "abstract": "This manuscript uses machine learning techniques to exploit baseball pitchers' decision making, so-called \"Baseball IQ,\" by modeling the at-bat information, pitch selection and counts, as a Markov Decision Process (MDP). Each state of the MDP models the pitcher's current pitch selection in a Markovian fashion, conditional on the information immediately prior to making the current pitch. This includes the count prior to the previous pitch, his ensuing pitch selection, the batter's ensuing action and the result of the pitch.\n    ",
        "submission_date": "2014-07-31T00:00:00",
        "last_modified_date": "2014-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0032",
        "title": "Calculating Ultra-Strong and Extended Solutions for Nine Men's Morris, Morabaraba, and Lasker",
        "authors": [
            "G\u00e1bor E. G\u00e9vay",
            "G\u00e1bor Danner"
        ],
        "abstract": "The strong solutions of Nine Men's Morris and its variant, Lasker Morris are well-known results (the starting positions are draws). We re-examined both of these games, and calculated extended strong solutions for them. By this we mean the game-theoretic values of all possible game states that could be reached from certain starting positions where the number of stones to be placed by the players is different from the standard rules. These were also calculated for a previously unsolved third variant, Morabaraba, with interesting results: most of the starting positions where the players can place an equal number of stones (including the standard starting position) are wins for the first player (as opposed to the above games, where these are usually draws).\n",
        "submission_date": "2014-07-31T00:00:00",
        "last_modified_date": "2015-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0328",
        "title": "Weakly monotone averaging functions",
        "authors": [
            "Tim Wilkin",
            "Gleb Beliakov"
        ],
        "abstract": "Monotonicity with respect to all arguments is fundamental to the definition of aggregation functions. It is also a limiting property that results in many important non-monotonic averaging functions being excluded from the theoretical framework. This work proposes a definition for weakly monotonic averaging functions, studies some properties of this class of functions and proves that several families of important non-monotonic means are actually weakly monotonic averaging functions. Specifically we provide sufficient conditions for weak monotonicity of the Lehmer mean and generalised mixture operators. We establish weak monotonicity of several robust estimators of location and conditions for weak monotonicity of a large class of penalty-based aggregation functions. These results permit a proof of the weak monotonicity of the class of spatial-tonal filters that include important members such as the bilateral filter and anisotropic diffusion. Our concept of weak monotonicity provides a sound theoretical and practical basis by which (monotone) aggregation functions and non-monotone averaging functions can be related within the same framework, allowing us to bridge the gap between these previously disparate areas of research.\n    ",
        "submission_date": "2014-08-02T00:00:00",
        "last_modified_date": "2014-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0595",
        "title": "Correlating and Cross-linking Knowledge Threads in Informledge System for Creating New Knowledge",
        "authors": [
            "T. R. Gopalakrishnan Nair",
            "Meenakshi Malhotra"
        ],
        "abstract": "There has been a considerable advance in computing, to mimic the way in which the brain tries to comprehend and structure the information to retrieve meaningful knowledge. It is identified that neuronal entities hold whole of the knowledge that the species makes use of. We intended to develop a modified knowledge based system, termed as Informledge System (ILS) with autonomous nodes and intelligent links that integrate and structure the pieces of knowledge. We conceive that every piece of knowledge is a cluster of cross-linked and correlated structure. In this paper, we put forward the theory of the nodes depicting concepts, referred as Entity Concept State which in turn is dealt with Concept State Diagrams (CSD). This theory is based on an abstract framework provided by the concepts. The framework represents the ILS as the weighted graph where the weights attached with the linked nodes help in knowledge retrieval by providing the direction of connectivity of autonomous nodes present in knowledge thread traversal. Here for the first time in the process of developing Informledge, we apply tenor computation for creating intelligent combinatorial knowledge with cross mutation to create fresh knowledge which looks to be the fundamentals of a typical thought process.\n    ",
        "submission_date": "2014-08-04T00:00:00",
        "last_modified_date": "2014-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0651",
        "title": "Boundary properties of the inconsistency of pairwise comparisons in group decisions",
        "authors": [
            "Matteo Brunelli",
            "Michele Fedrizzi"
        ],
        "abstract": "This paper proposes an analysis of the effects of consensus and preference aggregation on the consistency of pairwise comparisons. We define some boundary properties for the inconsistency of group preferences and investigate their relation with different inconsistency indices. Some results are presented on more general dependencies between properties of inconsistency indices and the satisfaction of boundary properties. In the end, given three boundary properties and nine indices among the most relevant ones, we will be able to present a complete analysis of what indices satisfy what properties and offer a reflection on the interpretation of the inconsistency of group preferences.\n    ",
        "submission_date": "2014-08-04T00:00:00",
        "last_modified_date": "2014-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1479",
        "title": "Logarithmic-Time Updates and Queries in Probabilistic Networks",
        "authors": [
            "Arthur L. Delcher",
            "Adam J. Grove",
            "Simon Kasif",
            "Judea Pearl"
        ],
        "abstract": "In this paper we propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks (causal trees and polytrees).  In the conventional algorithms, new evidence in absorbed in time O(1) and queries are processed in time O(N), where N is the size of the network.  We propose a practical algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(logn N) time per evidence absorption.  The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1480",
        "title": "Query DAGs: A Practical Paradigm for Implementing Belief Network Inference",
        "authors": [
            "Adnan Darwiche",
            "Gregory M. Provan"
        ],
        "abstract": "We describe a new paradigm for implementing inference in belief networks, which relies on compiling a belief network into an arithmetic expression called a Query DAG (Q-DAG).  Each non-leaf node of a Q-DAG represents a numeric operation, a number, or a symbol for evidence.  Each leaf node of a Q-DAG represents the answer to a network query, that is, the probability of some event of interest.  It appears that Q-DAGs can be generated using any of the algorithms for exact inference in belief networks --- we show how they can be generated using clustering and conditioning algorithms.  The time and space complexity of a Q-DAG generation algorithm is no worse than the time complexity of the inference algorithm on which it is based; that of a Q-DAG on-line evaluation algorithm is linear in the size of the Q-DAG, and such inference amounts to a standard evaluation of the arithmetic expression it represents.  The main value of Q-DAGs is in reducing the software and hardware resources required to utilize belief networks in on-line, real-world applications.  The proposed framework also facilitates the development of on-line inference on different software and hardware platforms, given the simplicity of the Q-DAG evaluation algorithm.  This paper describes this new paradigm for probabilistic inference, explaining how it works, its uses, and outlines some of the research directions that it leads to.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1481",
        "title": "Generalized Qualitative Probability: Savage Revisited",
        "authors": [
            "Daniel Lehmann"
        ],
        "abstract": "Preferences among acts are analyzed in the style of L. Savage, but as partially ordered. The rationality postulates considered are weaker than Savage's on three counts.  The Sure Thing Principle is derived in this setting.  The postulates are shown to lead to a characterization of generalized qualitative probability that includes and blends both traditional qualitative probability and the ranked structures used in logical approaches. \n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1482",
        "title": "Axiomatizing Causal Reasoning",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "Causal models defined in terms of a collection of equations, as defined by Pearl, are axiomatized here.  Axiomatizations are provided for three successively more general classes of causal models: (1) the class of recursive theories (those without feedback), (2) the class of theories where the solutions to the equations are unique, (3) arbitrary theories (where the equations may not have solutions and, if they do, they are not necessarily unique).  It is shown that to reason about causality in the most general third class, we must extend the language used by Galles and Pearl. In addition, the complexity of the decision procedures is examined for all the languages and classes of models considered.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1483",
        "title": "Random Algorithms for the Loop Cutset Problem",
        "authors": [
            "Ann Becker",
            "Reuven Bar-Yehuada",
            "Dan Geiger"
        ],
        "abstract": "We show how to find a minimum loop cutset in a Bayesian network with high probability.  Finding such a loop cutset is the first step in Pearl's method of conditioning for inference.  Our random algorithm for finding a loop cutset, called \"Repeated WGuessI\", outputs a minimum loop cutset, after O(c 6^k k n) steps, with probability at least 1-(1 over{6^k})^{c 6^k}), where c>1 is a constant specified by the user, k is the size of a minimum weight loop cutset, and n is the number of vertices.  We also show empirically that a variant of this algorithm, called WRA, often finds a loop cutset that is closer to the minimum loop cutset than the ones found by the best deterministic algorithms known.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1484",
        "title": "Learning to Cooperate via Policy Search",
        "authors": [
            "Leonid Peshkin",
            "Kee-Eung Kim",
            "Nicolas Meuleau",
            "Leslie Pack Kaelbling"
        ],
        "abstract": "Cooperative games are those in which both agents share the same payoff structure. Value-based reinforcement-learning algorithms, such as variants of Q-learning, have been applied to learning cooperative games, but they only apply when the game state is completely observable to both agents. Policy search methods are a reasonable alternative to value-based methods for partially observable environments. In this paper, we provide a gradient-based distributed policy-search method for cooperative games and compare the notion of local optimum to that of Nash equilibrium. We demonstrate the effectiveness of this method experimentally in a small, partially observable simulated soccer domain.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1485",
        "title": "A Logic for Reasoning about Upper Probabilities",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "abstract": "We present a propositional logic to reason about the uncertainty of events, where the uncertainty is modeled by a set of probability measures assigning an interval of probability to each event. We give a sound and complete axiomatization for the logic, and show  that the satisfiability problem is NP-complete, no harder than satisfiability for propositional logic.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1487",
        "title": "Robust Feature Selection by Mutual Information Distributions",
        "authors": [
            "Marco Zaffalon",
            "Marcus Hutter"
        ],
        "abstract": "Mutual information is widely used in artificial intelligence, in a descriptive way, to measure the stochastic dependence of discrete random variables. In order to address questions such as the reliability of the empirical value, one must consider sample-to-population inferential approaches. This paper deals with the distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean and an analytical approximation of the variance are reported. Asymptotic approximations of the distribution are proposed. The results are applied to the problem of selecting features for incremental learning and classification of the naive Bayes classifier. A fast, newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets. Finally, a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1488",
        "title": "Updating with incomplete observations",
        "authors": [
            "Gert de Cooman",
            "Marco Zaffalon"
        ],
        "abstract": "Currently, there is renewed interest in the problem, raised by Shafer in 1985, of updating probabilities when observations are incomplete (or set-valued). This is a fundamental problem, and of particular interest for Bayesian networks. Recently, Grunwald and Halpern have shown that commonly used updating strategies fail here, except under very special assumptions. We propose a new rule for updating probabilities with incomplete observations. Our approach is deliberately conservative: we make no or weak assumptions about the so-called incompleteness mechanism that produces incomplete observations. We model our ignorance about this mechanism by a vacuous lower prevision, a tool from the theory of imprecise probabilities, and we derive a new updating rule using coherence arguments. In general, our rule produces lower posterior probabilities, as well as partially determinate decisions. This is a logical consequence of the ignorance about the incompleteness mechanism. We show how the new rule can properly address the apparent paradox in the 'Monty Hall' puzzle. In addition, we apply it to the classification of new evidence in Bayesian networks constructed using expert knowledge. We provide an exact algorithm for this task with linear-time complexity, also for multiply connected nets.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1489",
        "title": "Renewal Strings for Cleaning Astronomical Databases",
        "authors": [
            "Amos J. Storkey",
            "Nigel C. Hambly",
            "Christopher K. I. Williams",
            "Robert G. Mann"
        ],
        "abstract": "Large astronomical databases obtained from sky surveys such as the SuperCOSMOS Sky Surveys (SSS) invariably suffer from a small number of spurious records coming from artefactual effects of the telescope, satellites and junk objects in orbit around earth and physical defects on the photographic plate or CCD. Though relatively small in number these spurious records present a significant problem in many situations where they can become a large proportion of the records potentially of interest to a given astronomer. In this paper we focus on the four most common causes of unwanted records in the SSS: satellite or aeroplane tracks, scratches fibres and other linear phenomena introduced to the plate, circular halos around bright stars due to internal reflections within the telescope and diffraction spikes near to bright stars. Accurate and robust techniques are needed for locating and flagging such spurious objects. We have developed renewal strings, a probabilistic technique combining the Hough transform, renewal processes and hidden Markov models which have proven highly effective in this context. The methods are applied to the SSS data to develop a dataset of spurious object detections, along with confidence measures, which can allow this unwanted data to be removed from consideration. These methods are general and can be adapted to any future astronomical survey data.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1664",
        "title": "A Parallel Algorithm for Exact Bayesian Structure Discovery in Bayesian Networks",
        "authors": [
            "Yetian Chen",
            "Jin Tian",
            "Olga Nikolova",
            "Srinivas Aluru"
        ],
        "abstract": "Exact Bayesian structure discovery in Bayesian networks requires exponential time and space. Using dynamic programming (DP), the fastest known sequential algorithm computes the exact posterior probabilities of structural features in $O(2(d+1)n2^n)$ time and space, if the number of nodes (variables) in the Bayesian network is $n$ and the in-degree (the number of parents) per node is bounded by a constant $d$. Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. That is, if $p=2^k$ processors are used, the run-time reduces to $O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per processor. Our algorithm is based the observation that the subproblems in the sequential DP algorithm constitute a $n$-$D$ hypercube. We take a delicate way to coordinate the computation of correlated DP procedures such that large amount of data exchange is suppressed. Further, we develop parallel techniques for two variants of the well-known \\emph{zeta transform}, which have applications outside the context of Bayesian networks. We demonstrate the capability of our algorithm on datasets with up to 33 variables and its scalability on up to 2048 processors. We apply our algorithm to a biological data set for discovering the yeast pheromone response pathways.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2016-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1692",
        "title": "When do Numbers Really Matter?",
        "authors": [
            "Hei Chan",
            "Adnan Darwiche"
        ],
        "abstract": "Common wisdom has it that small distinctions in the probabilities quantifying a Bayesian network do not matter much for the resultsof probabilistic queries. However, one can easily develop realistic scenarios under which small variations in network probabilities can lead to significant changes in computed queries. A pending theoretical question is then to analytically characterize parameter changes that do or do not matter. In this paper, we study the sensitivity of probabilistic queries to changes in network parameters and prove some tight bounds on the impact that such parameters can have on queries. Our analytical results pinpoint some interesting situations under which parameter changes do or do not matter. These results are important for knowledge engineers as they help them identify influential network parameters. They are also important for approximate inference algorithms that preprocessnetwork CPTs to eliminate small distinctions in probabilities.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1913",
        "title": "Using Learned Predictions as Feedback to Improve Control and Communication with an Artificial Limb: Preliminary Findings",
        "authors": [
            "Adam S. R. Parker",
            "Ann L. Edwards",
            "Patrick M. Pilarski"
        ],
        "abstract": "Many people suffer from the loss of a limb. Learning to get by without an arm or hand can be very challenging, and existing prostheses do not yet fulfil the needs of individuals with amputations. One promising solution is to provide greater communication between a prosthesis and its user. Towards this end, we present a simple machine learning interface to supplement the control of a robotic limb with feedback to the user about what the limb will be experiencing in the near future. A real-time prediction learner was implemented to predict impact-related electrical load experienced by a robot limb; the learning system's predictions were then communicated to the device's user to aid in their interactions with a workspace. We tested this system with five able-bodied subjects. Each subject manipulated the robot arm while receiving different forms of vibrotactile feedback regarding the arm's contact with its workspace. Our trials showed that communicable predictions could be learned quickly during human control of the robot arm. Using these predictions as a basis for feedback led to a statistically significant improvement in task performance when compared to purely reactive feedback from the device. Our study therefore contributes initial evidence that prediction learning and machine intelligence can benefit not just control, but also feedback from an artificial limb. We expect that a greater level of acceptance and ownership can be achieved if the prosthesis itself takes an active role in transmitting learned knowledge about its state and its situation of use.\n    ",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2014-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2027",
        "title": "A Heuristic Search Algorithm for Solving First-Order MDPs",
        "authors": [
            "Eldar Karabaev",
            "Olga Skvortsova"
        ],
        "abstract": "We present a heuristic search algorithm for solving first-order MDPs (FOMDPs). Our approach combines first-order state abstraction that avoids evaluating states individually, and heuristic search that avoids evaluating all states. Firstly, we apply state abstraction directly on the FOMDP avoiding propositionalization. Such kind of abstraction is referred to as firstorder state abstraction. Secondly, guided by an admissible heuristic, the search is restricted only to those states that are reachable from the initial state. We demonstrate the usefullness of the above techniques for solving FOMDPs on a system, referred to as FCPlanner, that entered the probabilistic track of the International Planning Competition (IPC'2004).\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2028",
        "title": "Bandit Algorithms for Tree Search",
        "authors": [
            "Pierre-Arnuad Coquelin",
            "Remi Munos"
        ],
        "abstract": "Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go [6]. Their efficient exploration of the tree enables to re- turn rapidly a good value, and improve preci- sion if more time is provided. The UCT algo- rithm [8], a tree search method based on Up- per Confidence Bounds (UCB) [2], is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is \"over-optimistic\" in some sense, leading to a worst-case regret that may be very poor. We propose alternative bandit algorithms for tree search. First, a modification of UCT us- ing a confidence sequence that scales expo- nentially in the horizon depth is analyzed. We then consider Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce and analyze a Bandit Algorithm for Smooth Trees (BAST) which takes into account ac- tual smoothness of the rewards for perform- ing efficient \"cuts\" of sub-optimal branches with high confidence. Finally, we present an incremental tree expansion which applies when the full tree is too big (possibly in- finite) to be entirely represented and show that with high probability, only the optimal branches are indefinitely developed. We illus- trate these methods on a global optimization problem of a continuous function, given noisy values.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2029",
        "title": "Sensitivity analysis for finite Markov chains in discrete time",
        "authors": [
            "Gert de Cooman",
            "Filip Hermans",
            "Erik Quaeghebeur"
        ],
        "abstract": "When the initial and transition probabilities of a finite Markov chain in discrete time are not well known, we should perform a sensitivity analysis. This is done by considering as basic uncertainty models the so-called credal sets that these probabilities are known or believed to belong to, and by allowing the probabilities to vary over such sets. This leads to the definition of an imprecise Markov chain. We show that the time evolution of such a system can be studied very efficiently using so-called lower and upper expectations. We also study how the inferred credal set about the state at time n evolves as n->infinity: under quite unrestrictive conditions, it converges to a uniquely invariant credal set, regardless of the credal set given for the initial state. This leads to a non-trivial generalisation of the classical Perron-Frobenius Theorem to imprecise Markov chains.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2030",
        "title": "On the Conditional Independence Implication Problem: A Lattice-Theoretic Approach",
        "authors": [
            "Mathias Niepert",
            "Dirk Van Gucht",
            "Marc Gyssens"
        ],
        "abstract": "A lattice-theoretic framework is introduced that permits the study of the conditional independence (CI) implication problem relative to the class of discrete probability measures. Semi-lattices are associated with CI statements and a finite, sound and complete inference system relative to semi-lattice inclusions is presented. This system is shown to be (1) sound and complete for saturated CI statements, (2) complete for general CI statements, and (3) sound and complete for stable CI statements. These results yield a criterion that can be used to falsify instances of the implication problem and several heuristics are derived that approximate this \"lattice-exclusion\" criterion in polynomial time. Finally, we provide experimental results that relate our work to results obtained from other existing inference algorithms.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2034",
        "title": "Approximate inference on planar graphs using Loop Calculus and Belief Propagation",
        "authors": [
            "Vicenc Gomez",
            "Hilbert Kappen",
            "Michael Chertkov"
        ],
        "abstract": "We introduce novel results for approximate inference on planar graphical models using the loop calculus framework. The loop calculus (Chertkov and Chernyak, 2006b) allows to express the exact partition function Z of a graphical model as a finite sum of terms that can be evaluated once the belief propagation (BP) solution is known. In general, full summation over all correction terms is intractable. We develop an algorithm for the approach presented in Chertkov et al. (2008) which represents an efficient truncation scheme on planar graphs and a new representation of the series in terms of Pfaffians of matrices. We analyze in detail both the loop series and the Pfaffian series for models with binary variables and pairwise interactions, and show that the first term of the Pfaffian series can provide very accurate approximations. The algorithm outperforms previous truncation schemes of the loop series and is competitive with other state-of-the-art methods for approximate inference.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2035",
        "title": "Quantum Annealing for Clustering",
        "authors": [
            "Kenichi Kurihara",
            "Shu Tanaka",
            "Seiji Miyashita"
        ],
        "abstract": "This paper studies quantum annealing (QA) for clustering, which can be seen as an extension of simulated annealing (SA). We derive a QA algorithm for clustering and propose an annealing schedule, which is crucial in practice. Experiments show the proposed QA algorithm finds better clustering assignments than SA. Furthermore, QA is as easy as SA to implement.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2046",
        "title": "Decentralized Data Fusion and Active Sensing with Mobile Sensors for Modeling and Predicting Spatiotemporal Traffic Phenomena",
        "authors": [
            "Jie Chen",
            "Kian Hsiang Low",
            "Colin Keng-Yan Tan",
            "Ali Oran",
            "Patrick Jaillet",
            "John Dolan",
            "Gaurav Sukhatme"
        ],
        "abstract": "The problem of modeling and predicting spatiotemporal traffic phenomena over an urban road network is important to many traffic applications such as detecting and forecasting congestion hotspots. This paper presents a decentralized data fusion and active sensing (D2FAS) algorithm for mobile sensors to actively explore the road network to gather and assimilate the most informative data for predicting the traffic phenomenon. We analyze the time and communication complexity of D2FAS and demonstrate that it can scale well with a large number of observations and sensors. We provide a theoretical guarantee on its predictive performance to be equivalent to that of a sophisticated centralized sparse approximation for the Gaussian process (GP) model: The computation of such a sparse approximate GP model can thus be parallelized and distributed among the mobile sensors (in a Google-like MapReduce paradigm), thereby achieving efficient and scalable prediction. We also theoretically guarantee its active sensing performance that improves under various practical environmental conditions. Empirical evaluation on real-world urban road network data shows that our D2FAS algorithm is significantly more time-efficient and scalable than state-oftheart centralized algorithms while achieving comparable predictive performance.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2048",
        "title": "Selecting Computations: Theory and Applications",
        "authors": [
            "Nicholas Hay",
            "Stuart Russell",
            "David Tolpin",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Sequential decision problems are often approximately solvable by simulating possible future action sequences. Metalevel decision procedures have been developed for selecting which action sequences to simulate, based on estimating the expected improvement in decision quality that would result from any particular simulation; an example is the recent work on using bandit algorithms to control Monte Carlo tree search in the game of Go. In this paper we develop a theoretical basis for metalevel decisions in the statistical framework of Bayesian selection problems, arguing (as others have done) that this is more appropriate than the bandit framework. We derive a number of basic results applicable to Monte Carlo selection problems, including the first finite sampling bounds for optimal policies in certain cases; we also provide a simple counterexample to the intuitive conjecture that an optimal policy will necessarily reach a decision in all cases. We then derive heuristic approximations in both Bayesian and distribution-free settings and demonstrate their superiority to bandit-based heuristics in one-shot decision problems and in Go.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2052",
        "title": "Markov Chains on Orbits of Permutation Groups",
        "authors": [
            "Mathias Niepert"
        ],
        "abstract": "We present a novel approach to detecting and utilizing symmetries in probabilistic graphical models with two main contributions. First, we present a scalable approach to computing generating sets of permutation groups representing the symmetries of graphical models. Second, we introduce orbital Markov chains, a novel family of Markov chains leveraging model symmetries to reduce mixing times. We establish an insightful connection between model symmetries and rapid mixing of orbital Markov chains. Thus, we present the first lifted MCMC algorithm for probabilistic graphical models. Both analytical and empirical results demonstrate the effectiveness and efficiency of the approach.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2053",
        "title": "Predicting the behavior of interacting humans by fusing data from multiple sources",
        "authors": [
            "Erik J. Schlicht",
            "Ritchie Lee",
            "David H. Wolpert",
            "Mykel J. Kochenderfer",
            "Brendan Tracey"
        ],
        "abstract": "Multi-fidelity methods combine inexpensive low-fidelity simulations with costly but highfidelity simulations to produce an accurate model of a system of interest at minimal cost. They have proven useful in modeling physical systems and have been applied to engineering problems such as wing-design optimization. During human-in-the-loop experimentation, it has become increasingly common to use online platforms, like Mechanical Turk, to run low-fidelity experiments to gather human performance data in an efficient manner. One concern with these experiments is that the results obtained from the online environment generalize poorly to the actual domain of interest. To address this limitation, we extend traditional multi-fidelity approaches to allow us to combine fewer data points from high-fidelity human-in-the-loop experiments with plentiful but less accurate data from low-fidelity experiments to produce accurate models of how humans interact. We present both model-based and model-free methods, and summarize the predictive performance of each method under dierent conditions.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2056",
        "title": "Active Sensing as Bayes-Optimal Sequential Decision Making",
        "authors": [
            "Sheeraz Ahmad",
            "Angela Yu"
        ],
        "abstract": "Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller). Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy [Najemnik & Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and effort. We simulate these algorithms on a simple visual search task to illustrate scenarios in which context-sensitivity is particularly beneficial and optimization with respect to generic statistical objectives particularly inadequate. Motivated by the geometric properties of the C-DAC policy, we present both parametric and non-parametric approximations, which retain context-sensitivity while significantly reducing computational complexity. These approximations enable us to investigate the more complex problem involving peripheral vision, and we notice that the difference between C-DAC and statistical policies becomes even more evident in this scenario.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2057",
        "title": "Scoring and Searching over Bayesian Networks with Causal and Associative Priors",
        "authors": [
            "Giorgos Borboudakis",
            "Ioannis Tsamardinos"
        ],
        "abstract": "A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. In this paper, a method is presented for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between pairs of variables. This type of knowledge naturally arises from prior experimental and observational data, among others. In addition, a novel search-operator is proposed to take advantage of such prior knowledge. Experiments show that, using path beliefs improves the learning of the skeleton, as well as the edge directions in the network.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2058",
        "title": "POMDPs under Probabilistic Semantics",
        "authors": [
            "Krishnendu Chatterjee",
            "Martin Chmelik"
        ],
        "abstract": "We consider partially observable Markov decision processes (POMDPs) with limit-average payoff, where a reward value in the interval [0,1] is associated to every transition, and the payoff of an infinite path is the long-run average of the rewards. We consider two types of path constraints: (i) quantitative constraint defines the set of paths where the payoff is at least a given threshold lambda_1 in (0,1]; and (ii) qualitative constraint which is a special case of quantitative constraint with lambda_1=1. We consider the computation of the almost-sure winning set, where the controller needs to ensure that the path constraint is satisfied with probability 1. Our main results for qualitative path constraint are as follows: (i) the problem of deciding the existence of a finite-memory controller is EXPTIME-complete; and (ii) the problem of deciding the existence of an infinite-memory controller is undecidable. For quantitative path constraint we show that the problem of deciding the existence of a finite-memory controller is undecidable.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2063",
        "title": "From Ordinary Differential Equations to Structural Causal Models: the deterministic case",
        "authors": [
            "Joris Mooij",
            "Dominik Janzing",
            "Bernhard Schoelkopf"
        ],
        "abstract": "We show how, and under which conditions, the equilibrium states of a first-order Ordinary Differential Equation (ODE) system can be described with a deterministic Structural Causal Model (SCM). Our exposition sheds more light on the concept of causality as expressed within the framework of Structural Causal Models, especially for cyclic models.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3002",
        "title": "The New Approach on Fuzzy Decision Trees",
        "authors": [
            "Jooyeol Yun",
            "Jun won Seo",
            "Taeseon Yoon"
        ],
        "abstract": "Decision trees have been widely used in machine learning. However, due to some reasons, data collecting in real world contains a fuzzy and uncertain form. The decision tree should be able to handle such fuzzy data. This paper presents a method to construct fuzzy decision tree. It proposes a fuzzy decision tree induction method in iris flower data set, obtaining the entropy from the distance between an average value and a particular value. It also presents an experiment result that shows the accuracy compared to former ID3.\n    ",
        "submission_date": "2014-08-13T00:00:00",
        "last_modified_date": "2014-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5241",
        "title": "A two-stage architecture for stock price forecasting by combining SOM and fuzzy-SVM",
        "authors": [
            "Duc-Hien Nguyen",
            "Manh-Thanh Le"
        ],
        "abstract": "This paper proposed a model to predict the stock price based on combining Self-Organizing Map (SOM) and fuzzy-Support Vector Machines (f-SVM). Extraction of fuzzy rules from raw data based on the combining of statistical machine learning models is base of this proposed approach. In the proposed model, SOM is used as a clustering algorithm to partition the whole input space into the several disjoint regions. For each partition, a set of fuzzy rules is extracted based on a f-SVM combining model. Then fuzzy rules sets are used to predict the test data using fuzzy inference algorithms. The performance of the proposed approach is compared with other models using four data sets\n    ",
        "submission_date": "2014-08-22T00:00:00",
        "last_modified_date": "2014-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5265",
        "title": "A Bayesian Ensemble Regression Framework on the Angry Birds Game",
        "authors": [
            "Nikolaos Tziortziotis",
            "Georgios Papagiannis",
            "Konstantinos Blekas"
        ],
        "abstract": "An ensemble inference mechanism is proposed on the Angry Birds domain. It is based on an efficient tree structure for encoding and representing game screenshots, where it exploits its enhanced modeling capability. This has the advantage to establish an informative feature space and modify the task of game playing to a regression analysis problem. To this direction, we assume that each type of object material and bird pair has its own Bayesian linear regression model. In this way, a multi-model regression framework is designed that simultaneously calculates the conditional expectations of several objects and makes a target decision through an ensemble of regression models. Learning procedure is performed according to an online estimation strategy for the model parameters. We provide comparative experimental results on several game levels that empirically illustrate the efficiency of the proposed methodology.\n    ",
        "submission_date": "2014-08-22T00:00:00",
        "last_modified_date": "2014-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5377",
        "title": "Dynamic Sweep Filtering Algorithm for FlexC",
        "authors": [
            "Alban Derrien",
            "Thierry Petit",
            "Stephane Zampelli"
        ],
        "abstract": "We investigate cumulative scheduling in uncertain environments, using constraint programming. We detail in this paper the dynamic sweep filtering algorithm of the FlexC global constraint.\n    ",
        "submission_date": "2014-08-22T00:00:00",
        "last_modified_date": "2014-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5490",
        "title": "New Ideas for Brain Modelling 2",
        "authors": [
            "Kieran Greer"
        ],
        "abstract": "This paper describes a relatively simple way of allowing a brain model to self-organise its concept patterns through nested structures. For a simulation, time reduction is helpful and it would be able to show how patterns may form and then fire in sequence, as part of a search or thought process. It uses a very simple equation to show how the inhibitors in particular, can switch off certain areas, to allow other areas to become the prominent ones and thereby define the current brain state. This allows for a small amount of control over what appears to be a chaotic structure inside of the brain. It is attractive because it is still mostly mechanical and therefore can be added as an automatic process, or the modelling of that. The paper also describes how the nested pattern structure can be used as a basic counting mechanism. Another mathematical conclusion provides a basis for maintaining memory or concept patterns. The self-organisation can space itself through automatic processes. This might allow new neurons to be added in a more even manner and could help to maintain the concept integrity. The process might also help with finding memory structures afterwards. This extended version integrates further with the existing cognitive model and provides some new conclusions.\n    ",
        "submission_date": "2014-08-23T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5507",
        "title": "Soft Neutrosophic Algebraic Structures and Their Generalization",
        "authors": [
            "Florentin Smarandache",
            "Mumtaz Ali",
            "Muhammad Shabir"
        ],
        "abstract": "Study of soft sets was first proposed by Molodtsov in 1999 to deal with uncertainty in a non-parametric manner. The researchers did not pay attention to soft set theory at that time but now the soft set theory has been developed in many areas of mathematics. Algebraic structures using soft set theory are very rapidly developed. In this book we developed soft neutrosophic algebraic structures by using soft sets and neutrosophic algebraic structures. In this book we study soft neutrosophic groups, soft neutrosophic semigroups, soft neutrosophic loops, soft neutrosophic LA-semigroups, and their generalizations respectively.\n    ",
        "submission_date": "2014-08-23T00:00:00",
        "last_modified_date": "2014-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6127",
        "title": "A Complete framework for ambush avoidance in realistic environments",
        "authors": [
            "Emmanuel Boidot",
            "Aude Marzuoli",
            "Eric Feron"
        ],
        "abstract": "Operating vehicles in adversarial environments between a recurring origin-destination pair requires new planning techniques. A two players zero-sum game is introduced. The goal of the first player is to minimize the expected casualties undergone by a convoy. The goal of the second player is to maximize this damage. The outcome of the game is obtained via a linear program that solves the corresponding minmax optimization problem over this outcome. Different environment models are defined in order to compute routing strategies over unstructured environments. To compare these methods for increasingly accurate representations of the environment, a grid-based model is chosen to represent the environment and the existence of a sufficient network size is highlighted. A global framework for the generation of realistic routing strategies between any two points is described. This framework requires a good assessment of the potential casualties at any location, therefore the most important parameters are identified. Finally the framework is tested on real world environments.\n    ",
        "submission_date": "2014-08-26T00:00:00",
        "last_modified_date": "2014-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6186",
        "title": "Consensus and Consistency Level Optimization of Fuzzy Preference Relation: A Soft Computing Approach",
        "authors": [
            "Sujit Das",
            "Samarjit Kar"
        ],
        "abstract": "In group decision making (GDM) problems fuzzy preference relations (FPR) are widely used for representing decision makers' opinions on the set of alternatives. In order to avoid misleading solutions, the study of consistency and consensus has become a very important aspect. This article presents a simulated annealing (SA) based soft computing approach to optimize the consistency/consensus level (CCL) of a complete fuzzy preference relation in order to solve a GDM problem. Consistency level indicates as expert's preference quality and consensus level measures the degree of agreement among experts' opinions. This study also suggests the set of experts for the necessary modifications in their prescribed preference structures without intervention of any moderator.\n    ",
        "submission_date": "2014-08-26T00:00:00",
        "last_modified_date": "2014-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6520",
        "title": "Knowledge Engineering for Planning-Based Hypothesis Generation",
        "authors": [
            "Shirin Sohrabi",
            "Octavian Udrea",
            "Anton V. Riabov"
        ],
        "abstract": "In this paper, we address the knowledge engineering problems for hypothesis generation motivated by applications that require timely exploration of hypotheses under unreliable observations. We looked at two applications: malware detection and intensive care delivery. In intensive care, the goal is to generate plausible hypotheses about the condition of the patient from clinical observations and further refine these hypotheses to create a recovery plan for the patient. Similarly, preventing malware spread within a corporate network involves generating hypotheses from network traffic data and selecting preventive actions. To this end, building on the already established characterization and use of AI planning for similar problems, we propose use of planning for the hypothesis generation problem. However, to deal with uncertainty, incomplete model description and unreliable observations, we need to use a planner capable of generating multiple high-quality plans. To capture the model description we propose a language called LTS++ and a web-based tool that enables the specification of the LTS++ model and a set of observations. We also proposed a 9-step process that helps provide guidance to the domain expert in specifying the LTS++ model. The hypotheses are then generated by running a planner on the translated LTS++ model and the provided trace. The hypotheses can be visualized and shown to the analyst or can be further investigated automatically.\n    ",
        "submission_date": "2014-08-27T00:00:00",
        "last_modified_date": "2014-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6706",
        "title": "Equilibrium States in Numerical Argumentation Networks",
        "authors": [
            "D. Gabbay",
            "O. Rodrigues"
        ],
        "abstract": "Given an argumentation network with initial values to the arguments, we look for algorithms which can yield extensions compatible with such initial values. We find that the best way of tackling this problem is to offer an iteration formula that takes the initial values and the attack relation and iterates a sequence of intermediate values that eventually converges leading to an extension. The properties surrounding the application of the iteration formula and its connection with other numerical and non-numerical techniques proposed by others are thoroughly investigated in this paper.\n    ",
        "submission_date": "2014-08-28T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6806",
        "title": "Mathematical Knowledge Representation: Semantic Models and Formalisms",
        "authors": [
            "Alexander Elizarov",
            "Alexander Kirillovich",
            "Evgeny Lipachev",
            "Olga Nevzorova",
            "Valery Solovyev",
            "Nikita Zhiltsov"
        ],
        "abstract": "The paper provides a survey of semantic methods for solution of fundamental tasks in mathematical knowledge management. Ontological models and formalisms are discussed. We propose an ontology of mathematical knowledge, covering a wide range of fields of mathematics. We demonstrate applications of this representation in mathematical formula search, and learning.\n    ",
        "submission_date": "2014-08-28T00:00:00",
        "last_modified_date": "2014-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6908",
        "title": "AI Evaluation: past, present and future",
        "authors": [
            "Jose Hernandez-Orallo"
        ],
        "abstract": "Artificial intelligence develops techniques and systems whose performance must be evaluated on a regular basis in order to certify and foster progress in the discipline. We will describe and critically assess the different ways AI systems are evaluated. We first focus on the traditional task-oriented evaluation approach. We see that black-box (behavioural evaluation) is becoming more and more common, as AI systems are becoming more complex and unpredictable. We identify three kinds of evaluation: Human discrimination, problem benchmarks and peer confrontation. We describe the limitations of the many evaluation settings and competitions in these three categories and propose several ideas for a more systematic and robust evaluation. We then focus on a less customary (and challenging) ability-oriented evaluation approach, where a system is characterised by its (cognitive) abilities, rather than by the tasks it is designed to solve. We discuss several possibilities: the adaptation of cognitive tests used for humans and animals, the development of tests derived from algorithmic information theory or more general approaches under the perspective of universal psychometrics.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2016-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.7092",
        "title": "Challenges in Bridging Social Semantics and Formal Semantics on the Web",
        "authors": [
            "Fabien Lucien Gandon",
            "Michel Buffa",
            "Elena Cabrio",
            "Catherine Faron-Zucker",
            "Alain Giboin",
            "Nhan Le Thanh",
            "Isabelle Mirbel",
            "Peter Sander",
            "Andrea G. B. Tettamanzi",
            "Serena Villata"
        ],
        "abstract": "This paper describes several results of Wimmics, a research lab which names stands for: web-instrumented man-machine interactions, communities, and semantics. The approaches introduced here rely on graph-oriented knowledge representation, reasoning and operationalization to model and support actors, actions and interactions in web-based epistemic communities. The re-search results are applied to support and foster interactions in online communities and manage their resources.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2014-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0069",
        "title": "Personalization of Itineraries search using Ontology and Rules to Avoid Congestion in Urban Areas",
        "authors": [
            "Amir Zidi",
            "Amna Bouhana",
            "Afef Fekih",
            "Mourad Abed"
        ],
        "abstract": "There is a relatively small amount of research covering urban freight movements. Most research dealing with the subject of urban mobility focuses on passenger vehicles, not commercial vehicles hauling freight. However, in many ways, urban freight transport contributes to congestion, air pollution, noise, accident and more fuel consumption which raises logistic costs, and hence the price of products. The main focus of this paper is to propose a new solution for congestion in order to improve the distribution process of goods in urban areas and optimize transportation cost, time of delivery, fuel consumption, and environmental impact, while guaranteeing the safety of goods and passengers. A novel technique for personalization in itinerary search based on city logistics ontology and rules is proposed to overcome this problem. The integration of personalization plays a key role in capturing or inferring the needs of each stakeholder (user), and then satisfying these needs in a given context. The proposed approach is implemented to an itinerary search problem for freight transportation in urban areas to demonstrate its ability in facilitating intelligent decision support by retrieving the best itinerary that satisfies the most users preferences (stakeholders).\n    ",
        "submission_date": "2014-08-30T00:00:00",
        "last_modified_date": "2014-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0703",
        "title": "On computable abstractions (a conceptual introduction)",
        "authors": [
            "Alejandro Sanchez Guinea"
        ],
        "abstract": "This paper introduces abstractions that are meaningful for computers and that can be built and used according to computers' own criteria, i.e., computable abstractions. It is analyzed how abstractions can be seen to serve as the building blocks for the creation of one own's understanding of things, which is essential in performing intellectual tasks. Thus, abstractional machines are defined, which following a mechanical process can, based on computable abstractions, build and use their own understanding of things. Abstractional machines are illustrated through an example that outlines their application to the task of natural language processing.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2015-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1045",
        "title": "A Fuzzy Directional Distance Measure",
        "authors": [
            "Josie C. McCullochy",
            "Chris J. Hinde",
            "Christian Wagner",
            "Uwe Aickelin"
        ],
        "abstract": "The measure of distance between two fuzzy sets is a fundamental tool within fuzzy set theory, however, distance measures currently within the literature use a crisp value to represent the distance between fuzzy sets. A real valued distance measure is developed into a fuzzy distance measure which better reflects the uncertainty inherent in fuzzy sets and a fuzzy directional distance measure is presented, which accounts for the direction of change between fuzzy sets. A multiplicative version is explored as a full maximal assignment is computationally intractable so an intermediate solution is offered.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1046",
        "title": "Analysing Fuzzy Sets Through Combining Measures of Similarity and Distance",
        "authors": [
            "Josie McCulloch",
            "Christian Wagner",
            "Uwe Aickelin"
        ],
        "abstract": "Reasoning with fuzzy sets can be achieved through measures such as similarity and distance. However, these measures can often give misleading results when considered independently, for example giving the same value for two different pairs of fuzzy sets. This is particularly a problem where many fuzzy sets are generated from real data, and while two different measures may be used to automatically compare such fuzzy sets, it is difficult to interpret two different results. This is especially true where a large number of fuzzy sets are being compared as part of a reasoning system. This paper introduces a method for combining the results of multiple measures into a single measure for the purpose of analysing and comparing fuzzy sets. The combined measure alleviates ambiguous results and aids in the automatic comparison of fuzzy sets. The properties of the combined measure are given, and demonstrations are presented with discussions on the advantages over using a single measure.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1170",
        "title": "Hybrid Systems Knowledge Representation Using Modelling Environment System Techniques Artificial Intelligence",
        "authors": [
            "Kamran Latif"
        ],
        "abstract": "Knowledge-based or Artificial Intelligence techniques are used increasingly as alternatives to more classical techniques to model ENVIRONMENTAL SYSTEMS. Use of Artificial Intelligence (AI) in environmental modelling has increased with recognition of its potential. In this paper we examine the DIFFERENT TECHNIQUES of Artificial intelligence with profound examples of human perception, learning and reasoning to solve complex problems. However with the increase of complexity better methods are required. Keeping in view of the above some researchers introduced the idea of hybrid mechanism in which two or more methods can be combined which seems to be a positive effort for creating a more complex; advanced and intelligent system which has the capability to in- cooperate human decisions thus driving the landscape changes.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1456",
        "title": "Accurate, fully-automated NMR spectral profiling for metabolomics",
        "authors": [
            "Siamak Ravanbakhsh",
            "Philip Liu",
            "Trent Bjorndahl",
            "Rupasri Mandal",
            "Jason R. Grant",
            "Michael Wilson",
            "Roman Eisner",
            "Igor Sinelnikov",
            "Xiaoyu Hu",
            "Claudio Luchinat",
            "Russell Greiner",
            "David S. Wishart"
        ],
        "abstract": "Many diseases cause significant changes to the concentrations of small molecules (aka metabolites) that appear in a person's biofluids, which means such diseases can often be readily detected from a person's \"metabolic profile\". This information can be extracted from a biofluid's NMR spectrum. Today, this is often done manually by trained human experts, which means this process is relatively slow, expensive and error-prone. This paper presents a tool, Bayesil, that can quickly, accurately and autonomously produce a complex biofluid's (e.g., serum or CSF) metabolic profile from a 1D1H NMR spectrum. This requires first performing several spectral processing steps then matching the resulting spectrum against a reference compound library, which contains the \"signatures\" of each relevant metabolite. Many of these steps are novel algorithms and our matching step views spectral matching as an inference problem within a probabilistic graphical model that rapidly approximates the most probable metabolic profile. Our extensive studies on a diverse set of complex mixtures, show that Bayesil can autonomously find the concentration of all NMR-detectable metabolites accurately (~90% correct identification and ~10% quantification error), in <5minutes on a single CPU. These results demonstrate that Bayesil is the first fully-automatic publicly-accessible system that provides quantitative NMR spectral profiling effectively -- with an accuracy that meets or exceeds the performance of trained experts. We anticipate this tool will usher in high-throughput metabolomics and enable a wealth of new applications of NMR in clinical settings. Available at ",
        "submission_date": "2014-09-04T00:00:00",
        "last_modified_date": "2014-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1686",
        "title": "Simulating Non Stationary Operators in Search Algorithms",
        "authors": [
            "Adrien Go\u00ebffon",
            "Fr\u00e9d\u00e9ric Lardeux",
            "Fr\u00e9d\u00e9ric Saubion"
        ],
        "abstract": "In this paper, we propose a model for simulating search operators whose behaviour often changes continuously during the search. In these scenarios, the performance of the operators decreases when they are applied. This is motivated by the fact that operators for optimization problems are often roughly classified into exploitation operators and exploration operators. Our simulation model is used to compare the different performances of operator selection policies and clearly identify their ability to adapt to such specific operators behaviours. The experimental study provides interesting results on the respective behaviours of operator selection policies when faced to such non stationary search scenarios.\n    ",
        "submission_date": "2014-09-05T00:00:00",
        "last_modified_date": "2014-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2650",
        "title": "Combining the analytical hierarchy process and the genetic algorithm to solve the timetable problem",
        "authors": [
            "Ihab Sbeity",
            "Mohamed Dbouk",
            "Habib Kobeissi"
        ],
        "abstract": "The main problems of school course timetabling are time, curriculum, and classrooms. In addition there are other problems that vary from one institution to another. This paper is intended to solve the problem of satisfying the teachers preferred schedule in a way that regards the importance of the teacher to the supervising institute, i.e. his score according to some criteria. Genetic algorithm (GA) has been presented as an elegant method in solving timetable problem (TTP) in order to produce solutions with no conflict. In this paper, we consider the analytic hierarchy process (AHP) to efficiently obtain a score for each teacher, and consequently produce a GA-based TTP solution that satisfies most of the teachers preferences.\n    ",
        "submission_date": "2014-09-09T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2821",
        "title": "Ambiguity-Driven Fuzzy C-Means Clustering: How to Detect Uncertain Clustered Records",
        "authors": [
            "Meysam Ghaffari",
            "Nasser Ghadiri"
        ],
        "abstract": "As a well-known clustering algorithm, Fuzzy C-Means (FCM) allows each input sample to belong to more than one cluster, providing more flexibility than non-fuzzy clustering methods. However, the accuracy of FCM is subject to false detections caused by noisy records, weak feature selection and low certainty of the algorithm in some cases. The false detections are very important in some decision-making application domains like network security and medical diagnosis, where weak decisions based on such false detections may lead to catastrophic outcomes. They are mainly emerged from making decisions about a subset of records that do not provide enough evidence to make a good decision. In this paper, we propose a method for detecting such ambiguous records in FCM by introducing a certainty factor to decrease invalid detections. This approach enables us to send the detected ambiguous records to another discrimination method for a deeper investigation, thus increasing the accuracy by lowering the error rate. Most of the records are still processed quickly and with low error rate which prevents performance loss compared to similar hybrid methods. Experimental results of applying the proposed method on several datasets from different domains show a significant decrease in error rate as well as improved sensitivity of the algorithm.\n    ",
        "submission_date": "2014-09-09T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3653",
        "title": "On Minimax Optimal Offline Policy Evaluation",
        "authors": [
            "Lihong Li",
            "Remi Munos",
            "Csaba Szepesvari"
        ],
        "abstract": "This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a minimax risk lower bound, and analyze the risk of two standard estimators. It is shown, and verified in simulation, that one is minimax optimal up to a constant, while another can be arbitrarily worse, despite its empirical success and popularity. The results are applied to related problems in contextual bandits and fixed-horizon Markov decision processes, and are also related to semi-supervised learning.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4161",
        "title": "Crowdsourcing Pareto-Optimal Object Finding by Pairwise Comparisons",
        "authors": [
            "Abolfazl Asudeh",
            "Gensheng Zhang",
            "Naeemul Hassan",
            "Chengkai Li",
            "Gergely V. Zaruba"
        ],
        "abstract": "This is the first study on crowdsourcing Pareto-optimal object finding, which has applications in public opinion collection, group decision making, and information exploration. Departing from prior studies on crowdsourcing skyline and ranking queries, it considers the case where objects do not have explicit attributes and preference relations on objects are strict partial orders. The partial orders are derived by aggregating crowdsourcers' responses to pairwise comparison questions. The goal is to find all Pareto-optimal objects by the fewest possible questions. It employs an iterative question-selection framework. Guided by the principle of eagerly identifying non-Pareto optimal objects, the framework only chooses candidate questions which must satisfy three conditions. This design is both sufficient and efficient, as it is proven to find a short terminal question sequence. The framework is further steered by two ideas---macro-ordering and micro-ordering. By different micro-ordering heuristics, the framework is instantiated into several algorithms with varying power in pruning questions. Experiment results using both real crowdsourcing marketplace and simulations exhibited not only orders of magnitude reductions in questions when compared with a brute-force approach, but also close-to-optimal performance from the most efficient instantiation.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4164",
        "title": "Dispute Resolution Using Argumentation-Based Mediation",
        "authors": [
            "Tomas Trescak",
            "Carles Sierra",
            "Simeon Simoff",
            "Ramon Lopez de Mantaras"
        ],
        "abstract": "Mediation is a process, in which both parties agree to resolve their dispute by negotiating over alternative solutions presented by a mediator. In order to construct such solutions, mediation brings more information and knowledge, and, if possible, resources to the negotiation table. The contribution of this paper is the automated mediation machinery which does that. It presents an argumentation-based mediation approach that extends the logic-based approach to argumentation-based negotiation involving BDI agents. The paper describes the mediation algorithm. For comparison it illustrates the method with a case study used in an earlier work. It demonstrates how the computational mediator can deal with realistic situations in which the negotiating agents would otherwise fail due to lack of knowledge and/or resources.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4814",
        "title": "ICE: Enabling Non-Experts to Build Models Interactively for Large-Scale Lopsided Problems",
        "authors": [
            "Patrice Simard",
            "David Chickering",
            "Aparna Lakshmiratan",
            "Denis Charles",
            "Leon Bottou",
            "Carlos Garcia Jurado Suarez",
            "David Grangier",
            "Saleema Amershi",
            "Johan Verwey",
            "Jina Suh"
        ],
        "abstract": "Quick interaction between a human teacher and a learning machine presents numerous benefits and challenges when working with web-scale data. The human teacher guides the machine towards accomplishing the task of interest. The learning machine leverages big data to find examples that maximize the training value of its interaction with the teacher. When the teacher is restricted to labeling examples selected by the machine, this problem is an instance of active learning. When the teacher can provide additional information to the machine (e.g., suggestions on what examples or predictive features should be used) as the learning task progresses, then the problem becomes one of interactive learning.\n",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2014-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5166",
        "title": "A Tabu Search Algorithm for the Multi-period Inspector Scheduling Problem",
        "authors": [
            "Hu Qin",
            "Zizhen Zhang",
            "Yubin Xie",
            "Andrew Lim"
        ],
        "abstract": "This paper introduces a multi-period inspector scheduling problem (MPISP), which is a new variant of the multi-trip vehicle routing problem with time windows (VRPTW). In the MPISP, each inspector is scheduled to perform a route in a given multi-period planning horizon. At the end of each period, each inspector is not required to return to the depot but has to stay at one of the vertices for recuperation. If the remaining time of the current period is insufficient for an inspector to travel from his/her current vertex $A$ to a certain vertex B, he/she can choose either waiting at vertex A until the start of the next period or traveling to a vertex C that is closer to vertex B. Therefore, the shortest transit time between any vertex pair is affected by the length of the period and the departure time. We first describe an approach of computing the shortest transit time between any pair of vertices with an arbitrary departure time. To solve the MPISP, we then propose several local search operators adapted from classical operators for the VRPTW and integrate them into a tabu search framework. In addition, we present a constrained knapsack model that is able to produce an upper bound for the problem. Finally, we evaluate the effectiveness of our algorithm with extensive experiments based on a set of test instances. Our computational results indicate that our approach generates high-quality solutions.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5189",
        "title": "Solving Graph Coloring Problems with Abstraction and Symmetry",
        "authors": [
            "Michael Codish",
            "Michael Frank",
            "Avraham Itzhakov",
            "Alice Miller"
        ],
        "abstract": "This paper introduces a general methodology, based on abstraction and symmetry, that applies to solve hard graph edge-coloring problems and demonstrates its use to provide further evidence that the Ramsey number $R(4,3,3)=30$. The number $R(4,3,3)$ is often presented as the unknown Ramsey number with the best chances of being found \"soon\". Yet, its precise value has remained unknown for more than 50 years. We illustrate our approach by showing that: (1) there are precisely 78{,}892 $(3,3,3;13)$ Ramsey colorings; and (2) if there exists a $(4,3,3;30)$ Ramsey coloring then it is (13,8,8) regular. Specifically each node has 13 edges in the first color, 8 in the second, and 8 in the third. We conjecture that these two results will help provide a proof that no $(4,3,3;30)$ Ramsey coloring exists implying that $R(4,3,3)=30$.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5223",
        "title": "Why Local Search Excels in Expression Simplification",
        "authors": [
            "Ben Ruijl",
            "Aske Plaat",
            "Jos Vermaseren",
            "Jaap van den Herik"
        ],
        "abstract": "Simplifying expressions is important to make numerical integration of large expressions from High Energy Physics tractable. To this end, Horner's method can be used. Finding suitable Horner schemes is assumed to be hard, due to the lack of local heuristics. Recently, MCTS was reported to be able to find near optimal schemes. However, several parameters had to be fine-tuned manually. In this work, we investigate the state space properties of Horner schemes and find that the domain is relatively flat and contains only a few local minima. As a result, the Horner space is appropriate to be explored by Stochastic Local Search (SLS), which has only two parameters: the number of iterations (computation time) and the neighborhood structure. We found a suitable neighborhood structure, leaving only the allowed computation time as a parameter. We performed a range of experiments. The results obtained by SLS are similar or better than those obtained by MCTS. Furthermore, we show that SLS obtains the good results at least 10 times faster. Using SLS, we can speed up numerical integration of many real-world large expressions by at least a factor of 24. For High Energy Physics this means that numerical integrations that took weeks can now be done in hours.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5317",
        "title": "A Bayesian model for recognizing handwritten mathematical expressions",
        "authors": [
            "Scott MacLean",
            "George Labahn"
        ],
        "abstract": "Recognizing handwritten mathematics is a challenging classification problem, requiring simultaneous identification of all the symbols comprising an input as well as the complex two-dimensional relationships between symbols and subexpressions. Because of the ambiguity present in handwritten input, it is often unrealistic to hope for consistently perfect recognition accuracy. We present a system which captures all recognizable interpretations of the input and organizes them in a parse forest from which individual parse trees may be extracted and reported. If the top-ranked interpretation is incorrect, the user may request alternates and select the recognition result they desire. The tree extraction step uses a novel probabilistic tree scoring strategy in which a Bayesian network is constructed based on the structure of the input, and each joint variable assignment corresponds to a different parse tree. Parse trees are then reported in order of decreasing probability. Two accuracy evaluations demonstrate that the resulting recognition system is more accurate than previous versions (which used non-probabilistic methods) and other academic math recognizers.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5340",
        "title": "Belief revision by examples",
        "authors": [
            "Paolo Liberatore"
        ],
        "abstract": "A common assumption in belief revision is that the reliability of the information sources is either given, derived from temporal information, or the same for all. This article does not describe a new semantics for integration but the problem of obtaining the reliability of the sources given the result of a previous merging. As an example, the relative reliability of two sensors can be assessed given some certain observation, and allows for subsequent mergings of data coming from them.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5671",
        "title": "A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion Systems",
        "authors": [
            "Ebru Aydin Gol",
            "Ezio Bartocci",
            "Calin Belta"
        ],
        "abstract": "We propose a technique to detect and generate patterns in a network of locally interacting dynamical systems. Central to our approach is a novel spatial superposition logic, whose semantics is defined over the quad-tree of a partitioned image. We show that formulas in this logic can be efficiently learned from positive and negative examples of several types of patterns. We also demonstrate that pattern detection, which is implemented as a model checking algorithm, performs very well for test data sets different from the learning sets. We define a quantitative semantics for the logic and integrate the model checking algorithm with particle swarm optimization in a computational framework for synthesis of parameters leading to desired patterns in reaction-diffusion systems.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5719",
        "title": "Local Optimal Sets and Bounded Archiving on Multi-objective NK-Landscapes with Correlated Objectives",
        "authors": [
            "Manuel L\u00f3pez-Ib\u00e1\u00f1ez",
            "Arnaud Liefooghe",
            "S\u00e9bastien Verel"
        ],
        "abstract": "The properties of local optimal solutions in multi-objective combinatorial optimization problems are crucial for the effectiveness of local search algorithms, particularly when these algorithms are based on Pareto dominance. Such local search algorithms typically return a set of mutually nondominated Pareto local optimal (PLO) solutions, that is, a PLO-set. This paper investigates two aspects of PLO-sets by means of experiments with Pareto local search (PLS). First, we examine the impact of several problem characteristics on the properties of PLO-sets for multi-objective NK-landscapes with correlated objectives. In particular, we report that either increasing the number of objectives or decreasing the correlation between objectives leads to an exponential increment on the size of PLO-sets, whereas the variable correlation has only a minor effect. Second, we study the running time and the quality reached when using bounding archiving methods to limit the size of the archive handled by PLS, and thus, the maximum size of the PLO-set found. We argue that there is a clear relationship between the running time of PLS and the difficulty of a problem instance.\n    ",
        "submission_date": "2014-09-19T00:00:00",
        "last_modified_date": "2014-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5752",
        "title": "On the Impact of Multiobjective Scalarizing Functions",
        "authors": [
            "Bilel Derbel",
            "Dimo Brockhoff",
            "Arnaud Liefooghe",
            "S\u00e9bastien Verel"
        ],
        "abstract": "Recently, there has been a renewed interest in decomposition-based approaches for evolutionary multiobjective optimization. However, the impact of the choice of the underlying scalarizing function(s) is still far from being well understood. In this paper, we investigate the behavior of different scalarizing functions and their parameters. We thereby abstract firstly from any specific algorithm and only consider the difficulty of the single scalarized problems in terms of the search ability of a (1+lambda)-EA on biobjective NK-landscapes. Secondly, combining the outcomes of independent single-objective runs allows for more general statements on set-based performance measures. Finally, we investigate the correlation between the opening angle of the scalarizing function's underlying contour lines and the position of the final solution in the objective space. Our analysis is of fundamental nature and sheds more light on the key characteristics of multiobjective scalarizing functions.\n    ",
        "submission_date": "2014-09-19T00:00:00",
        "last_modified_date": "2014-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6052",
        "title": "Oblivious Bounds on the Probability of Boolean Functions",
        "authors": [
            "Wolfgang Gatterbauer",
            "Dan Suciu"
        ],
        "abstract": "This paper develops upper and lower bounds for the probability of Boolean functions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach dissociation and give an exact characterization of optimal oblivious bounds, i.e. when the new probabilities are chosen independent of the probabilities of all other variables. Our motivation comes from the weighted model counting problem (or, equivalently, the problem of computing the probability of a Boolean function), which is #P-hard in general. By performing several dissociations, one can transform a Boolean formula whose probability is difficult to compute, into one whose probability is easy to compute, and which is guaranteed to provide an upper or lower bound on the probability of the original formula by choosing appropriate probabilities for the dissociated variables. Our new bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We also show how our theory allows a standard relational database management system (DBMS) to both upper and lower bound hard probabilistic queries in guaranteed polynomial time.\n    ",
        "submission_date": "2014-09-21T00:00:00",
        "last_modified_date": "2014-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6287",
        "title": "On tensor rank of conditional probability tables in Bayesian networks",
        "authors": [
            "Ji\u0159\u00ed Vomlel",
            "Petr Tichavsk\u00fd"
        ],
        "abstract": "A difficult task in modeling with Bayesian networks is the elicitation of numerical parameters of Bayesian networks. A large number of parameters is needed to specify a conditional probability table (CPT) that has a larger parent set. In this paper we show that, most CPTs from real applications of Bayesian networks can actually be very well approximated by tables that require substantially less parameters. This observation has practical consequence not only for model elicitation but also for efficient probabilistic reasoning with these networks.\n    ",
        "submission_date": "2014-09-22T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6359",
        "title": "Neighborhood Selection and Rules Identification for Cellular Automata: A Rough Sets Approach",
        "authors": [
            "Bartlomiej Placzek"
        ],
        "abstract": "In this paper a method is proposed which uses data mining techniques based on rough sets theory to select neighborhood and determine update rule for cellular automata (CA). According to the proposed approach, neighborhood is detected by reducts calculations and a rule-learning algorithm is applied to induce a set of decision rules that define the evolution of CA. Experiments were performed with use of synthetic as well as real-world data sets. The results show that the introduced method allows identification of both deterministic and probabilistic CA-based models of real-world phenomena.\n    ",
        "submission_date": "2014-09-22T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6831",
        "title": "The Application of Differential Privacy for Rank Aggregation: Privacy and Accuracy",
        "authors": [
            "Shang Shang",
            "Tiance Wang",
            "Paul Cuff",
            "Sanjeev Kulkarni"
        ],
        "abstract": "The potential risk of privacy leakage prevents users from sharing their honest opinions on social platforms. This paper addresses the problem of privacy preservation if the query returns the histogram of rankings. The framework of differential privacy is applied to rank aggregation. The error probability of the aggregated ranking is analyzed as a result of noise added in order to achieve differential privacy. Upper bounds on the error rates for any positional ranking rule are derived under the assumption that profiles are uniformly distributed. Simulation results are provided to validate the probabilistic analysis.\n    ",
        "submission_date": "2014-09-24T00:00:00",
        "last_modified_date": "2014-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7186",
        "title": "Feature-based tuning of simulated annealing applied to the curriculum-based course timetabling problem",
        "authors": [
            "Ruggero Bellio",
            "Sara Ceschia",
            "Luca Di Gaspero",
            "Andrea Schaerf",
            "Tommaso Urli"
        ],
        "abstract": "We consider the university course timetabling problem, which is one of the most studied problems in educational timetabling. In particular, we focus our attention on the formulation known as the curriculum-based course timetabling problem, which has been tackled by many researchers and for which there are many available benchmarks.\n",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7281",
        "title": "Causal Graph Justifications of Logic Programs",
        "authors": [
            "Pedro Cabalar",
            "Jorge Fandinno",
            "Michael Fink"
        ],
        "abstract": "In this work we propose a multi-valued extension of logic programs under the stable models semantics where each true atom in a model is associated with a set of justifications. These justifications are expressed in terms of causal graphs formed by rule labels and edges that represent their application ordering. For positive programs, we show that the causal justifications obtained for a given atom have a direct correspon- dence to (relevant) syntactic proofs of that atom using the program rules involved in the graphs. The most interesting contribution is that this causal information is obtained in a purely semantic way, by algebraic op- erations (product, sum and application) on a lattice of causal values whose ordering relation expresses when a justification is stronger than another. Finally, for programs with negation, we define the concept of causal stable model by introducing an analogous transformation to Gelfond and Lifschitz's program reduct. As a result, default negation behaves as \"absence of proof\" and no justification is derived from negative liter\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7410",
        "title": "Revisiting Algebra and Complexity of Inference in Graphical Models",
        "authors": [
            "Siamak Ravanbakhsh",
            "Russell Greiner"
        ],
        "abstract": "This paper studies the form and complexity of inference in graphical models using the abstraction offered by algebraic structures. In particular, we broadly formalize inference problems in graphical models by viewing them as a sequence of operations based on commutative semigroups. We then study the computational complexity of inference by organizing various problems into an \"inference hierarchy\". When the underlying structure of an inference problem is a commutative semiring -- i.e. a combination of two commutative semigroups with the distributive law -- a message passing procedure called belief propagation can leverage this distributive law to perform polynomial-time inference for certain problems. After establishing the NP-hardness of inference in any commutative semiring, we investigate the relation between algebraic properties in this setting and further show that polynomial-time inference using distributive law does not (trivially) extend to inference problems that are expressed using more than two commutative semigroups. We then extend the algebraic treatment of message passing procedures to survey propagation, providing a novel perspective using a combination of two commutative semirings. This formulation generalizes the application of survey propagation to new settings.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7777",
        "title": "Using Answer Set Programming for pattern mining",
        "authors": [
            "Thomas Guyet",
            "Yves Moinard",
            "Ren\u00e9 Quiniou"
        ],
        "abstract": "Serial pattern mining consists in extracting the frequent sequential patterns from a unique sequence of itemsets. This paper explores the ability of a declarative language, such as Answer Set Programming (ASP), to solve this issue efficiently. We propose several ASP implementations of the frequent sequential pattern mining task: a non-incremental and an incremental resolution. The results show that the incremental resolution is more efficient than the non-incremental one, but both ASP programs are less efficient than dedicated algorithms. Nonetheless, this approach can be seen as a first step toward a generic framework for sequential pattern mining with constraints.\n    ",
        "submission_date": "2014-09-27T00:00:00",
        "last_modified_date": "2014-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7830",
        "title": "How good is the Shapley value-based approach to the influence maximization problem?",
        "authors": [
            "Kamil Adamczewski",
            "Szymon Matejczyk",
            "Tomasz P. Michalak"
        ],
        "abstract": "The Shapley value has been recently advocated as a method to choose the seed nodes for the process of information diffusion. Intuitively, since the Shapley value evaluates the average marginal contribution of a player to the coalitional game, it can be used in the network context to evaluate the marginal contribution of a node in the process of information diffusion given various groups of already 'infected' nodes. Although the above direction of research seems promising, the current liter- ature is missing a throughout assessment of its performance. The aim of this work is to provide such an assessment of the existing Shapley value-based approaches to information diffusion.\n    ",
        "submission_date": "2014-09-27T00:00:00",
        "last_modified_date": "2014-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8027",
        "title": "Autonomous robots and the SP theory of intelligence",
        "authors": [
            "J. Gerard Wolff"
        ],
        "abstract": "This article is about how the \"SP theory of intelligence\" and its realisation in the \"SP machine\" (both outlined in the article) may help to solve computer-related problems in the design of autonomous robots, meaning robots that do not depend on external intelligence or power supplies, are mobile, and are designed to exhibit as much human-like intelligence as possible. The article is about: how to increase the computational and energy efficiency of computers and reduce their bulk; how to achieve human-like versatility in intelligence; and likewise for human-like adaptability in intelligence. The SP system has potential for substantial gains in computational and energy efficiency and reductions in the bulkiness of computers: by reducing the size of data to be processed; by exploiting statistical information that the system gathers; and via an updated version of Donald Hebb's concept of a \"cell assembly\". Towards human-like versatility in intelligence, the SP system has strengths in unsupervised learning, natural language processing, pattern recognition, information retrieval, several kinds of reasoning, planning, problem solving, and more, with seamless integration amongst structures and functions. The SP system's strengths in unsupervised learning and other aspects of intelligence may help to achieve human-like adaptability in intelligence via: the learning of natural language; learning to see; building 3D models of objects and of a robot's surroundings; learning regularities in the workings of a robot and in the robot's environment; exploration and play; learning major skills; and secondary forms of learning. Also discussed are: how the SP system may process parallel streams of information; generalisation of knowledge, correction of over-generalisations, and learning from dirty data; how to cut the cost of learning; and reinforcements, motivations, goals, and demonstration.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8053",
        "title": "Medical diagnosis as pattern recognition in a framework of information compression by multiple alignment, unification and search",
        "authors": [
            "J. Gerard Wolff"
        ],
        "abstract": "This paper describes a novel approach to medical diagnosis based on the SP theory of computing and cognition. The main attractions of this approach are: a format for representing diseases that is simple and intuitive; an ability to cope with errors and uncertainties in diagnostic information; the simplicity of storing statistical information as frequencies of occurrence of diseases; a method for evaluating alternative diagnostic hypotheses that yields true probabilities; and a framework that should facilitate unsupervised learning of medical knowledge and the integration of medical diagnosis with other AI applications.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2014-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8470",
        "title": "Interference Effects in Quantum Belief Networks",
        "authors": [
            "Catarina Moreira",
            "Andreas Wichert"
        ],
        "abstract": "Probabilistic graphical models such as Bayesian Networks are one of the most powerful structures known by the Computer Science community for deriving probabilistic inferences. However, modern cognitive psychology has revealed that human decisions could not follow the rules of classical probability theory, because humans cannot process large amounts of data in order to make judgements. Consequently, the inferences performed are based on limited data coupled with several heuristics, leading to violations of the law of total probability. This means that probabilistic graphical models based on classical probability theory are too limited to fully simulate and explain various aspects of human decision making.\n",
        "submission_date": "2014-09-30T00:00:00",
        "last_modified_date": "2014-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0210",
        "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0281",
        "title": "Modeling Creativity: Case Studies in Python",
        "authors": [
            "Tom De Smedt"
        ],
        "abstract": "Modeling Creativity (doctoral dissertation, 2013) explores how creativity can be represented using computational approaches. Our aim is to construct computer models that exhibit creativity in an artistic context, that is, that are capable of generating or evaluating an artwork (visual or linguistic), an interesting new idea, a subjective opinion. The research was conducted in 2008-2012 at the Computational Linguistics Research Group (CLiPS, University of Antwerp) under the supervision of Prof. Walter Daelemans. Prior research was also conducted at the Experimental Media Research Group (EMRG, St. Lucas University College of Art & Design Antwerp) under the supervision of Lucas Nijs.\n",
        "submission_date": "2014-08-10T00:00:00",
        "last_modified_date": "2014-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0369",
        "title": "The Universe of Minds",
        "authors": [
            "Roman V. Yampolskiy"
        ],
        "abstract": "The paper attempts to describe the space of possible mind designs by first equating all minds to software. Next it proves some interesting properties of the mind design space such as infinitude of minds, size and representation complexity of minds. A survey of mind design taxonomies is followed by a proposal for a new field of investigation devoted to study of minds, intellectology, a list of open problems for this new field is presented.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0572",
        "title": "Algebraic Semantics of Proto-Transitive Rough Sets",
        "authors": [
            "A. Mani"
        ],
        "abstract": "Rough sets over generalized transitive relations like proto-transitive ones had been initiated by the present author in the year 2012. Subsequently, approximation of proto-transitive relations by other relations was investigated and the relation with rough approximations was developed towards constructing semantics that can handle fragments of structure. It was also proved that difference of approximations induced by some approximate relations need not induce rough structures. In this research we develop different semantics of proto transitive rough sets (PRAX) after characterizing the structure of rough objects and also develop a theory of dependence for general rough sets and use it to internalize the Nelson-algebra based approximate semantics developed earlier. The theory of rough dependence initiated later by the present author is extended in the process. This monograph is reasonably self-contained and includes proofs and extensions of representation of objects that were not part of earlier papers.\n    ",
        "submission_date": "2014-10-02T00:00:00",
        "last_modified_date": "2014-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1231",
        "title": "Bayesian regression and Bitcoin",
        "authors": [
            "Devavrat Shah",
            "Kang Zhang"
        ],
        "abstract": "In this paper, we discuss the method of Bayesian regression and its efficacy for predicting price variation of Bitcoin, a recently popularized virtual, cryptographic currency. Bayesian regression refers to utilizing empirical data as proxy to perform Bayesian inference. We utilize Bayesian regression for the so-called \"latent source model\". The Bayesian regression for \"latent source model\" was introduced and discussed by Chen, Nikolov and Shah (2013) and Bresler, Chen and Shah (2014) for the purpose of binary classification. They established theoretical as well as empirical efficacy of the method for the setting of binary classification.\n",
        "submission_date": "2014-10-06T00:00:00",
        "last_modified_date": "2014-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1776",
        "title": "Ontology-based Representation and Reasoning on Process Models: A Logic Programming Approach",
        "authors": [
            "Fabrizio Smith",
            "Maurizio Proietti"
        ],
        "abstract": "We propose a framework grounded in Logic Programming for representing and reasoning about business processes from both the procedural and ontological point of views. In particular, our goal is threefold: (1) define a logical language and a formal semantics for process models enriched with ontology-based annotations; (2) provide an effective inference mechanism that supports the combination of reasoning services dealing with the structural definition of a process model, its behavior, and the domain knowledge related to the participating business entities; (3) implement such a theoretical framework into a process modeling and reasoning platform. To this end we define a process ontology coping with a relevant fragment of the popular BPMN modeling notation. The behavioral semantics of a process is defined as a state transition system by following an approach similar to the Fluent Calculus, and allows us to specify state change in terms of preconditions and effects of the enactment of activities. Then we show how the procedural process knowledge can be seamlessly integrated with the domain knowledge specified by using the OWL 2 RL rule-based ontology language. Our framework provides a wide range of reasoning services, including CTL model checking, which can be performed by using standard Logic Programming inference engines through a goal-oriented, efficient, sound and complete evaluation procedure. We also present a software environment implementing the proposed framework, and we report on an experimental evaluation of the system, whose results are encouraging and show the viability of the approach.\n    ",
        "submission_date": "2014-10-07T00:00:00",
        "last_modified_date": "2014-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2056",
        "title": "An improved multimodal PSO method based on electrostatic interaction using n- nearest-neighbor local search",
        "authors": [
            "Taymaz Rahkar-Farshi",
            "Sara Behjat-Jamal",
            "Mohammad-Reza Feizi-Derakhshi"
        ],
        "abstract": "In this paper, an improved multimodal optimization (MMO) algorithm,called LSEPSO,has been proposed. LSEPSO combined Electrostatic Particle Swarm Optimization (EPSO) algorithm and a local search method and then made some modification on them. It has been shown to improve global and local optima finding ability of the algorithm. This algorithm useda modified local search to improve particle's personal best, which used n-nearest-neighbour instead of nearest-neighbour. Then, by creating n new points among each particle and n nearest particles, it tried to find a point which could be the alternative of particle's personal best. This method prevented particle's attenuation and following a specific particle by its neighbours. The performed tests on a number of benchmark functions clearly demonstrated that the improved algorithm is able to solve MMO problems and outperform other tested algorithms in this article.\n    ",
        "submission_date": "2014-10-08T00:00:00",
        "last_modified_date": "2014-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2063",
        "title": "Committment-Based Data-Aware Multi-Agent-Contexts Systems",
        "authors": [
            "Stefania Costantini"
        ],
        "abstract": "Communication and interaction among agents have been the subject of extensive investigation since many years. Commitment-based communication, where communicating agents are seen as a debtor agent who is committed to a creditor agent to bring about something (possibly under some conditions) is now very well-established. The approach of DACMAS (Data-Aware Commitment-based MAS) lifts commitment-related approaches proposed in the literature from a propositional to a first-order setting via the adoption the DRL-Lite Description Logic. Notably, DACMASs provide, beyond commitments, simple forms of inter-agent event-based communication. Yet, the aspect is missing of making a MAS able to acquire knowledge from contexts which are not agents and which are external to the MAS. This topic is coped with in Managed MCSs (Managed Multi-Context Systems), where however exchanges are among knowledge bases and not agents. In this paper, we propose the new approach of DACmMCMASs (Data-Aware Commitment-based managed Multi- Context MAS), so as to obtain a commitment-based first-order agent system which is able to interact with heterogeneous external information sources. We show that DACmMCMASs retain the nice formal properties of the original approaches.\n    ",
        "submission_date": "2014-10-08T00:00:00",
        "last_modified_date": "2014-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2442",
        "title": "Realizing RCC8 networks using convex regions",
        "authors": [
            "Steven Schockaert",
            "Sanjiang Li"
        ],
        "abstract": "RCC8 is a popular fragment of the region connection calculus, in which qualitative spatial relations between regions, such as adjacency, overlap and parthood, can be expressed. While RCC8 is essentially dimensionless, most current applications are confined to reasoning about two-dimensional or three-dimensional physical space. In this paper, however, we are mainly interested in conceptual spaces, which typically are high-dimensional Euclidean spaces in which the meaning of natural language concepts can be represented using convex regions. The aim of this paper is to analyze how the restriction to convex regions constrains the realizability of networks of RCC8 relations. First, we identify all ways in which the set of RCC8 base relations can be restricted to guarantee that consistent networks can be convexly realized in respectively 1D, 2D, 3D, and 4D. Most surprisingly, we find that if the relation 'partially overlaps' is disallowed, all consistent atomic RCC8 networks can be convexly realized in 4D. If instead refinements of the relation 'part of' are disallowed, all consistent atomic RCC8 relations can be convexly realized in 3D. We furthermore show, among others, that any consistent RCC8 network with 2n+1 variables can be realized using convex regions in the n-dimensional Euclidean space.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2014-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3125",
        "title": "Relational Linear Programs",
        "authors": [
            "Kristian Kersting",
            "Martin Mladenov",
            "Pavel Tokmakov"
        ],
        "abstract": "We propose relational linear programming, a simple framework for combing linear programs (LPs) and logic programs. A relational linear program (RLP) is a declarative LP template defining the objective and the constraints through the logical concepts of objects, relations, and quantified variables. This allows one to express the LP objective and constraints relationally for a varying number of individuals and relations among them without enumerating them. Together with a logical knowledge base, effectively a logical program consisting of logical facts and rules, it induces a ground LP. This ground LP is solved using lifted linear programming. That is, symmetries within the ground LP are employed to reduce its dimensionality, if possible, and the reduced program is solved using any off-the-shelf LP solver. In contrast to mainstream LP template languages like AMPL, which features a mixture of declarative and imperative programming styles, RLP's relational nature allows a more intuitive representation of optimization problems over relational domains. We illustrate this empirically by experiments on approximate inference in Markov logic networks using LP relaxations, on solving Markov decision processes, and on collective inference using LP support vector machines.\n    ",
        "submission_date": "2014-10-12T00:00:00",
        "last_modified_date": "2014-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3862",
        "title": "Presence-absence reasoning for evolutionary phenotypes",
        "authors": [
            "James P. Balhoff",
            "T. Alexander Dececchi",
            "Paula M. Mabee",
            "Hilmar Lapp"
        ],
        "abstract": "Nearly invariably, phenotypes are reported in the scientific literature in meticulous detail, utilizing the full expressivity of natural language. Often it is particularly these detailed observations (facts) that are of interest, and thus specific to the research questions that motivated observing and reporting them. However, research aiming to synthesize or integrate phenotype data across many studies or even fields is often faced with the need to abstract from detailed observations so as to construct phenotypic concepts that are common across many datasets rather than specific to a few. Yet, observations or facts that would fall under such abstracted concepts are typically not directly asserted by the original authors, usually because they are \"obvious\" according to common domain knowledge, and thus asserting them would be deemed redundant by anyone with sufficient domain knowledge. For example, a phenotype describing the length of a manual digit for an organism implicitly means that the organism must have had a hand, and thus a forelimb; the presence or absence of a forelimb may have supporting data across a far wider range of taxa than the length of a particular manual digit. Here we describe how within the Phenoscape project we use a pipeline of OWL axiom generation and reasoning steps to infer taxon-specific presence/absence of anatomical entities from anatomical phenotypes. Although presence/absence is all but one, and a seemingly simple way to abstract phenotypes across data sources, it can nonetheless be powerful for linking genotype to phenotype, and it is particularly relevant for constructing synthetic morphological supermatrices for comparative analysis; in fact presence/absence is one of the prevailing character observation types in published character matrices.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3916",
        "title": "Memory Networks",
        "authors": [
            "Jason Weston",
            "Sumit Chopra",
            "Antoine Bordes"
        ],
        "abstract": "We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4182",
        "title": "Analysis of corporate environmental reports using statistical techniques and data mining",
        "authors": [
            "J. R. Modapothala",
            "B. Issac"
        ],
        "abstract": "Measuring the effectiveness of corporate environmental reports, it being highly qualitative and less regulated, is often considered as a daunting task. The task becomes more complex if comparisons are to be performed. This study is undertaken to overcome the physical verification problems by implementing data mining technique. It further explores on the effectiveness by performing exploratory analysis and structural equation model to bring out the significant linkages between the selected 10 variables. Samples of five hundred and thirty nine reports across various countries are used from an international directory to perform the statistical analysis like: One way ANOVA (Analysis of Variance), MDA (Multivariate Discriminant Analysis) and SEM (Structural Equation Modeling). The results indicate the significant differences among the various types of industries in their environmental reporting, and the exploratory factors like stakeholder, organization strategy and industrial oriented factors, proved significant. The major accomplishment is that the findings correlate with the conceptual frame work of GRI.\n    ",
        "submission_date": "2014-10-08T00:00:00",
        "last_modified_date": "2014-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5078",
        "title": "Learning Vague Concepts for the Semantic Web",
        "authors": [
            "Paolo Pareti",
            "Ewan Klein"
        ],
        "abstract": "Ontologies can be a powerful tool for structuring knowledge, and they are currently the subject of extensive research. Updating the contents of an ontology or improving its interoperability with other ontologies is an important but difficult process. In this paper, we focus on the presence of vague concepts, which are pervasive in natural language, within the framework of formal ontologies. We will adopt a framework in which vagueness is captured via numerical restrictions that can be automatically adjusted. Since updating vague concepts, either through ontology alignment or ontology evolution, can lead to inconsistent sets of axioms, we define and implement a method to detecting and repairing such inconsistencies in a local fashion.\n    ",
        "submission_date": "2014-10-19T00:00:00",
        "last_modified_date": "2014-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5215",
        "title": "Interactive Error Correction in Implicative Theories",
        "authors": [
            "Sergei O. Kuznetsov",
            "Artem Revenko"
        ],
        "abstract": "Errors in implicative theories coming from binary data are studied. First, two classes of errors that may affect implicative theories are singled out. Two approaches for finding errors of these classes are proposed, both of them based on methods of Formal Concept Analysis. The first approach uses the cardinality minimal (canonical or Duquenne-Guigues) implication base. The construction of such a base is computationally intractable. Using an alternative approach one checks possible errors on the fly in polynomial time via computing closures of subsets of attributes. Both approaches are interactive, based on questions about the validity of certain implications. Results of computer experiments are presented and discussed.\n    ",
        "submission_date": "2014-10-20T00:00:00",
        "last_modified_date": "2014-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5738",
        "title": "Investigation of A Collective Decision Making System of Different Neighbourhood-Size Based on Hyper-Geometric Distribution",
        "authors": [
            "Debdipta Goswami",
            "Heiko Hamann"
        ],
        "abstract": "The study of collective decision making system has become the central part of the Swarm- Intelligence Related research in recent years. The most challenging task of modelling a collec- tive decision making system is to develop the macroscopic stochastic equation from its microscopic model. In this report we have investigated the behaviour of a collective decision making system with specified microscopic rules that resemble the chemical reaction and used different group size. Then we ventured to derive a generalized analytical model of a collective-decision system using hyper-geometric distribution.\n",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2014-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5859",
        "title": "Towards a Model Theory for Distributed Representations",
        "authors": [
            "Ramanathan Guha"
        ],
        "abstract": "Distributed representations (such as those based on embeddings) and discrete representations (such as those based on logic) have complementary strengths. We explore one possible approach to combining these two kinds of representations. We present a model theory/semantics for first order logic based on vectors of reals. We describe the model theory, discuss some interesting properties of such a system and present a simple approach to query answering.\n    ",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5996",
        "title": "Log-Optimal Portfolio Selection Using the Blackwell Approachability Theorem",
        "authors": [
            "Vladimir V'yugin"
        ],
        "abstract": "We present a method for constructing the log-optimal portfolio using the well-calibrated forecasts of market values. Dawid's notion of calibration and the Blackwell approachability theorem are used for computing well-calibrated forecasts. We select a portfolio using this \"artificial\" probability distribution of market values. Our portfolio performs asymptotically at least as well as any stationary portfolio that redistributes the investment at each round using a continuous function of side information. Unlike in classical mathematical finance theory, no stochastic assumptions are made about market values.\n    ",
        "submission_date": "2014-10-22T00:00:00",
        "last_modified_date": "2015-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6142",
        "title": "The Lovelace 2.0 Test of Artificial Creativity and Intelligence",
        "authors": [
            "Mark O. Riedl"
        ],
        "abstract": "Observing that the creation of certain types of artistic artifacts necessitate intelligence, we present the Lovelace 2.0 Test of creativity as an alternative to the Turing Test as a means of determining whether an agent is intelligent. The Lovelace 2.0 Test builds off prior tests of creativity and additionally provides a means of directly comparing the relative intelligence of different agents.\n    ",
        "submission_date": "2014-10-22T00:00:00",
        "last_modified_date": "2014-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6519",
        "title": "Justifying and Improving Meta-Agent Conflict-Based Search",
        "authors": [
            "David Tolpin"
        ],
        "abstract": "The Meta-Agent Conflict-Based Search~(MA-CBS) is a recently proposed algorithm for the multi-agent path finding problem. The algorithm is an extension of Conflict-Based Search~(CBS), which automatically merges conflicting agents into meta-agents if the number of conflicts exceeds a certain threshold. However, the decision to merge agents is made according to an empirically chosen fixed threshold on the number of conflicts. The best threshold depends both on the domain and on the number of agents, and the nature of the dependence is not clearly understood.\n",
        "submission_date": "2014-10-23T00:00:00",
        "last_modified_date": "2014-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6641",
        "title": "Partial Optimality by Pruning for MAP-Inference with General Graphical Models",
        "authors": [
            "Paul Swoboda",
            "Alexander Shekhovtsov",
            "J\u00f6rg Hendrik Kappes",
            "Christoph Schn\u00f6rr",
            "Bogdan Savchynskyy"
        ],
        "abstract": "We consider the energy minimization problem for undirected graphical models, also known as MAP-inference problem for Markov random fields which is NP-hard in general. We propose a novel polynomial time algorithm to obtain a part of its optimal non-relaxed integral solution. Our algorithm is initialized with variables taking integral values in the solution of a convex relaxation of the MAP-inference problem and iteratively prunes those, which do not satisfy our criterion for partial optimality. We show that our pruning strategy is in a certain sense theoretically optimal. Also empirically our method outperforms previous approaches in terms of the number of persistently labelled variables. The method is very general, as it is applicable to models with arbitrary factors of an arbitrary order and can employ any solver for the considered relaxed problem. Our method's runtime is determined by the runtime of the convex relaxation solver for the MAP-inference problem.\n    ",
        "submission_date": "2014-10-24T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6671",
        "title": "Augmenting Ordered Binary Decision Diagrams with Conjunctive Decomposition",
        "authors": [
            "Yong Lai",
            "Dayou Liu",
            "Minghao Yin"
        ],
        "abstract": "This paper augments OBDD with conjunctive decomposition to propose a generalization called OBDD[$\\wedge$]. By imposing reducedness and the finest $\\wedge$-decomposition bounded by integer $i$ ($\\wedge_{\\widehat{i}}$-decomposition) on OBDD[$\\wedge$], we identify a family of canonical languages called ROBDD[$\\wedge_{\\widehat{i}}$], where ROBDD[$\\wedge_{\\widehat{0}}$] is equivalent to ROBDD. We show that the succinctness of ROBDD[$\\wedge_{\\widehat{i}}$] is strictly increasing when $i$ increases. We introduce a new time-efficiency criterion called rapidity which reflects that exponential operations may be preferable if the language can be exponentially more succinct, and show that: the rapidity of each operation on ROBDD[$\\wedge_{\\widehat{i}}$] is increasing when $i$ increases; particularly, the rapidity of some operations (e.g., conjoining) is strictly increasing. Finally, our empirical results show that: a) the size of ROBDD[$\\wedge_{\\widehat{i}}$] is normally not larger than that of the equivalent \\ROBDDC{\\widehat{i+1}}; b) conjoining two ROBDD[$\\wedge_{\\widehat{1}}$]s is more efficient than conjoining two ROBDD[$\\wedge_{\\widehat{0}}$]s in most cases, where the former is NP-hard but the latter is in P; and c) the space-efficiency of ROBDD[$\\wedge_{\\widehat{\\infty}}$] is comparable with that of d-DNNF and that of another canonical generalization of \\ROBDD{} called SDD.\n    ",
        "submission_date": "2014-10-24T00:00:00",
        "last_modified_date": "2014-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6690",
        "title": "On the Complexity of Optimization Problems based on Compiled NNF Representations",
        "authors": [
            "Daniel Le Berre",
            "Emmanuel Lonca",
            "Pierre Marquis"
        ],
        "abstract": "Optimization is a key task in a number of applications. When the set of feasible solutions under consideration is of combinatorial nature and described in an implicit way as a set of constraints, optimization is typically NP-hard. Fortunately, in many problems, the set of feasible solutions does not often change and is independent from the user's request. In such cases, compiling the set of constraints describing the set of feasible solutions during an off-line phase makes sense, if this compilation step renders computationally easier the generation of a non-dominated, yet feasible solution matching the user's requirements and preferences (which are only known at the on-line step). In this article, we focus on propositional constraints. The subsets L of the NNF language analyzed in Darwiche and Marquis' knowledge compilation map are considered. A number of families F of representations of objective functions over propositional variables, including linear pseudo-Boolean functions and more sophisticated ones, are considered. For each language L and each family F, the complexity of generating an optimal solution when the constraints are compiled into L and optimality is to be considered w.r.t. a function from F is identified.\n    ",
        "submission_date": "2014-10-24T00:00:00",
        "last_modified_date": "2014-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6854",
        "title": "The Quantum Nature of Identity in Human Thought: Bose-Einstein Statistics for Conceptual Indistinguishability",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "Increasing experimental evidence shows that humans combine concepts in a way that violates the rules of classical logic and probability theory. On the other hand, mathematical models inspired by the formalism of quantum theory are in accordance with data on concepts and their combinations. In this paper, we investigate a novel type of concept combination were a number is combined with a noun, e.g., `Eleven Animals. Our aim is to study 'conceptual identity' and the effects of 'indistinguishability' - in the combination 'Eleven Animals', the 'animals' are identical and indistinguishable - on the mechanisms of conceptual combination. We perform experiments on human subjects and find significant evidence of deviation from the predictions of classical statistical theories, more specifically deviations with respect to Maxwell-Boltzmann statistics. This deviation is of the 'same type' of the deviation of quantum mechanical from classical mechanical statistics, due to indistinguishability of microscopic quantum particles, i.e we find convincing evidence of the presence of Bose-Einstein statistics. We also present preliminary promising evidence of this phenomenon in a web-based study.\n    ",
        "submission_date": "2014-10-24T00:00:00",
        "last_modified_date": "2014-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6960",
        "title": "Parameterizing the semantics of fuzzy attribute implications by systems of isotone Galois connections",
        "authors": [
            "Vilem Vychodil"
        ],
        "abstract": "We study the semantics of fuzzy if-then rules called fuzzy attribute implications parameterized by systems of isotone Galois connections. The rules express dependencies between fuzzy attributes in object-attribute incidence data. The proposed parameterizations are general and include as special cases the parameterizations by linguistic hedges used in earlier approaches. We formalize the general parameterizations, propose bivalent and graded notions of semantic entailment of fuzzy attribute implications, show their characterization in terms of least models and complete axiomatization, and provide characterization of bases of fuzzy attribute implications derived from data.\n    ",
        "submission_date": "2014-10-25T00:00:00",
        "last_modified_date": "2014-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7063",
        "title": "Towards a General Framework for Actual Causation Using CP-logic",
        "authors": [
            "Sander Beckers",
            "Joost Vennekens"
        ],
        "abstract": "Since Pearl's seminal work on providing a formal language for causality, the subject has garnered a lot of interest among philosophers and researchers in artificial intelligence alike. One of the most debated topics in this context regards the notion of actual causation, which concerns itself with specific - as opposed to general - causal claims. The search for a proper formal definition of actual causation has evolved into a controversial debate, that is pervaded with ambiguities and confusion. The goal of our research is twofold. First, we wish to provide a clear way to compare competing definitions. Second, we also want to improve upon these definitions so they can be applied to a more diverse range of instances, including non-deterministic ones. To achieve these goals we will provide a general, abstract definition of actual causation, formulated in the context of the expressive language of CP-logic (Causal Probabilistic logic). We will then show that three recent definitions by Ned Hall (originally formulated for structural models) and a definition of our own (formulated for CP-logic directly) can be viewed and directly compared as instantiations of this abstract definition, which allows them to deal with a broader range of examples.\n    ",
        "submission_date": "2014-10-26T00:00:00",
        "last_modified_date": "2015-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7100",
        "title": "Estimating the intrinsic dimension in fMRI space via dataset fractal analysis - Counting the `cpu cores' of the human brain",
        "authors": [
            "Harris V. Georgiou"
        ],
        "abstract": "Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive tool for localizing and analyzing brain activity. This study focuses on one very important aspect of the functional properties of human brain, specifically the estimation of the level of parallelism when performing complex cognitive tasks. Using fMRI as the main modality, the human brain activity is investigated through a purely data-driven signal processing and dimensionality analysis approach. Specifically, the fMRI signal is treated as a multi-dimensional data space and its intrinsic `complexity' is studied via dataset fractal analysis and blind-source separation (BSS) methods. One simulated and two real fMRI datasets are used in combination with Independent Component Analysis (ICA) and fractal analysis for estimating the intrinsic (true) dimensionality, in order to provide data-driven experimental evidence on the number of independent brain processes that run in parallel when visual or visuo-motor tasks are performed. Although this number is can not be defined as a strict threshold but rather as a continuous range, when a specific activation level is defined, a corresponding number of parallel processes or the casual equivalent of `cpu cores' can be detected in normal human brain activity.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7223",
        "title": "The probatilistic Quantifier Fuzzification Mechanism FA: A theoretical analysis",
        "authors": [
            "Felix Diaz-Hermida",
            "Alberto Bugarin",
            "David E. Losada"
        ],
        "abstract": "The main goal of this work is to analyze the behaviour of the FA quantifier fuzzification mechanism. As we prove in the paper, this model has a very solid theorethical behaviour, superior to most of the models defined in the literature. Moreover, we show that the underlying probabilistic interpretation has very interesting consequences.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7849",
        "title": "A mutli-thread tabu search algorithm",
        "authors": [
            "A.M.Connor"
        ],
        "abstract": "This paper describes a novel refinement to a Tabu search algorithm that has been implemented in an attempt to improve the robustness of the search when applied to particularly complex problems. In this approach, two Tabu searches are carried out in parallel. Each search thread is characterised by it's own short term memory which forces that point out of local optima. However, the two search threads share an intermediate term memory so allowing a degree of information to be passed between them. Results are presented for both unconstrained and constrained numerical functions as well as a problem in the field of hydraulic circuit optimization. Simulation of hydraulic circuit performance is achieved by linking the optimization algorithm to the commercial simulation package Bathfp.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7856",
        "title": "A Statistical Decision-Theoretic Framework for Social Choice",
        "authors": [
            "Hossein Azari Soufiani",
            "David C. Parkes",
            "Lirong Xia"
        ],
        "abstract": "In this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents. In our framework, we are given a statistical ranking model, a decision space, and a loss function defined on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize expected loss. This suggests a general framework for the design and analysis of new social choice mechanisms. We compare Bayesian estimators, which minimize Bayesian expected loss, for the Mallows model and the Condorcet model respectively, and the Kemeny rule. We consider various normative properties, in addition to computational complexity and asymptotic behavior. In particular, we show that the Bayesian estimator for the Condorcet model satisfies some desired properties such as anonymity, neutrality, and monotonicity, can be computed in polynomial time, and is asymptotically different from the other two rules when the data are generated from the Condorcet model for some ground truth parameter.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2016-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7953",
        "title": "Reasoning for ALCQ extended with a flexible meta-modelling hierarchy",
        "authors": [
            "Regina Motz",
            "Edelweis Rohrer",
            "Paula Severi"
        ],
        "abstract": "This works is motivated by a real-world case study where it is necessary to integrate and relate existing ontologies through meta- modelling. For this, we introduce the Description Logic ALCQM which is obtained from ALCQ by adding statements that equate individuals to concepts in a knowledge base. In this new extension, a concept can be an individual of another concept (called meta-concept) which themselves can be individuals of yet another concept (called meta meta-concept) and so on. We define a tableau algorithm for checking consistency of an ontology in ALCQM and prove its correctness.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8027",
        "title": "Towards a Visual Turing Challenge",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "As language and visual understanding by machines progresses rapidly, we are observing an increasing interest in holistic architectures that tightly interlink both modalities in a joint learning and inference process. This trend has allowed the community to progress towards more challenging and open tasks and refueled the hope at achieving the old AI dream of building machines that could pass a turing test in open domains. In order to steadily make progress towards this goal, we realize that quantifying performance becomes increasingly difficult. Therefore we ask how we can precisely define such challenges and how we can evaluate different algorithms on this open tasks? In this paper, we summarize and discuss such challenges as well as try to give answers where appropriate options are available in the literature. We exemplify some of the solutions on a recently presented dataset of question-answering task based on real-world indoor images that establishes a visual turing challenge. Finally, we argue despite the success of unique ground-truth annotation, we likely have to step away from carefully curated dataset and rather rely on 'social consensus' as the main driving force to create suitable benchmarks. Providing coverage in this inherently ambiguous output space is an emerging challenge that we face in order to make quantifiable progress in this area.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8233",
        "title": "Do Artificial Reinforcement-Learning Agents Matter Morally?",
        "authors": [
            "Brian Tomasik"
        ],
        "abstract": "Artificial reinforcement learning (RL) is a widely used technique in artificial intelligence that provides a general method for training agents to perform a wide variety of behaviours. RL as used in computer science has striking parallels to reward and punishment learning in animal and human brains. I argue that present-day artificial RL agents have a very small but nonzero degree of ethical importance. This is particularly plausible for views according to which sentience comes in degrees based on the abilities and complexities of minds, but even binary views on consciousness should assign nonzero probability to RL programs having morally relevant experiences. While RL programs are not a top ethical priority today, they may become more significant in the coming decades as RL is increasingly applied to industry, robotics, video games, and other areas. I encourage scientists, philosophers, and citizens to begin a conversation about our ethical duties to reduce the harm that we inflict on powerless, voiceless RL agents.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8536",
        "title": "Tasks that Require, or can Benefit from, Matching Blank Nodes",
        "authors": [
            "Christina Lantzaki",
            "Yannis Tzitzikas"
        ],
        "abstract": "In various domains and cases, we observe the creation and usage of information elements which are unnamed. Such elements do not have a name, or may have a name that is not externally referable (usually meaningless and not persistent over time). This paper discusses why we will never `escape' from the problem of having to construct mappings between such unnamed elements in information systems. Since unnamed elements nowadays occur very often in the framework of the Semantic Web and Linked Data as blank nodes, the paper describes scenarios that can benefit from methods that compute mappings between the unnamed elements. For each scenario, the corresponding bnode matching problem is formally defined. Based on this analysis, we try to reach to more a general formulation of the problem, which can be useful for guiding the required technological advances. To this end, the paper finally discusses methods to realize blank node matching, the implementations that exist, and identifies open issues and challenges.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8808",
        "title": "A Semantic Web of Know-How: Linked Data for Community-Centric Tasks",
        "authors": [
            "Paolo Pareti",
            "Ewan Klein",
            "Adam Barker"
        ],
        "abstract": "This paper proposes a novel framework for representing community know-how on the Semantic Web. Procedural knowledge generated by web communities typically takes the form of natural language instructions or videos and is largely unstructured. The absence of semantic structure impedes the deployment of many useful applications, in particular the ability to discover and integrate know-how automatically. We discuss the characteristics of community know-how and argue that existing knowledge representation frameworks fail to represent it adequately. We present a novel framework for representing the semantic structure of community know-how and demonstrate the feasibility of our approach by providing a concrete implementation which includes a method for automatically acquiring procedural knowledge for real-world tasks.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0149",
        "title": "How Many Workers to Ask? Adaptive Exploration for Collecting High Quality Labels",
        "authors": [
            "Ittai Abraham",
            "Omar Alonso",
            "Vasilis Kandylas",
            "Rajesh Patel",
            "Steven Shelford",
            "Aleksandrs Slivkins"
        ],
        "abstract": "Crowdsourcing has been part of the IR toolbox as a cheap and fast mechanism to obtain labels for system development and evaluation. Successful deployment of crowdsourcing at scale involves adjusting many variables, a very important one being the number of workers needed per human intelligence task (HIT). We consider the crowdsourcing task of learning the answer to simple multiple-choice HITs, which are representative of many relevance experiments. In order to provide statistically significant results, one often needs to ask multiple workers to answer the same HIT. A stopping rule is an algorithm that, given a HIT, decides for any given set of worker answers if the system should stop and output an answer or iterate and ask one more worker. Knowing the historic performance of a worker in the form of a quality score can be beneficial in such a scenario. In this paper we investigate how to devise better stopping rules given such quality scores. We also suggest adaptive exploration as a promising approach for scalable and automatic creation of ground truth. We conduct a data analysis on an industrial crowdsourcing platform, and use the observations from this analysis to design new stopping rules that use the workers' quality scores in a non-trivial manner. We then perform a simulation based on a real-world workload, showing that our algorithm performs better than the more naive approaches.\n    ",
        "submission_date": "2014-11-01T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0156",
        "title": "Surrogate Search As a Way to Combat Harmful Effects of Ill-behaved Evaluation Functions",
        "authors": [
            "William Cushing",
            "J. Benton",
            "Patrick Eyerich",
            "Subbarao Kambhampati"
        ],
        "abstract": "Recently, several researchers have found that cost-based satisficing search with A* often runs into problems. Although some \"work arounds\" have been proposed to ameliorate the problem, there has been little concerted effort to pinpoint its origin. In this paper, we argue that the origins of this problem can be traced back to the fact that most planners that try to optimize cost also use cost-based evaluation functions (i.e., f(n) is a cost estimate). We show that cost-based evaluation functions become ill-behaved whenever there is a wide variance in action costs; something that is all too common in planning domains. The general solution to this malady is what we call a surrogatesearch, where a surrogate evaluation function that doesn't directly track the cost objective, and is resistant to cost-variance, is used. We will discuss some compelling choices for surrogate evaluation functions that are based on size rather that cost. Of particular practical interest is a cost-sensitive version of size-based evaluation function -- where the heuristic estimates the size of cheap paths, as it provides attractive quality vs. speed tradeoffs\n    ",
        "submission_date": "2014-11-01T00:00:00",
        "last_modified_date": "2014-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0359",
        "title": "NESTA, The NICTA Energy System Test Case Archive",
        "authors": [
            "Carleton Coffrin",
            "Dan Gordon",
            "Paul Scott"
        ],
        "abstract": "In recent years the power systems research community has seen an explosion of work applying operations research techniques to challenging power network optimization problems. Regardless of the application under consideration, all of these works rely on power system test cases for evaluation and validation. However, many of the well established power system test cases were developed as far back as the 1960s with the aim of testing AC power flow algorithms. It is unclear if these power flow test cases are suitable for power system optimization studies. This report surveys all of the publicly available AC transmission system test cases, to the best of our knowledge, and assess their suitability for optimization tasks. It finds that many of the traditional test cases are missing key network operation constraints, such as line thermal limits and generator capability curves. To incorporate these missing constraints, data driven models are developed from a variety of publicly available data sources. The resulting extended test cases form a compressive archive, NESTA, for the evaluation and validation of power system optimization algorithms.\n    ",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2019-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0406",
        "title": "GC-SROIQ(C) : Expressive Constraint Modelling and Grounded Circumscription for SROIQ",
        "authors": [
            "Arjun Bhardwaj",
            "Sangeetha"
        ],
        "abstract": "Developments in semantic web technologies have promoted ontological encoding of knowledge from diverse domains. However, modelling many practical domains requires more expressive representations schemes than what the standard description logics(DLs) support. We extend the DL SROIQ with constraint networks and grounded circumscription. Applications of constraint modelling include embedding ontologies with temporal or spatial information, while grounded circumscription allows defeasible inference and closed world reasoning. This paper overcomes restrictions on existing constraint modelling approaches by introducing expressive constructs. Grounded circumscription allows concept and role minimization and is decidable for DL. We provide a general and intuitive algorithm for the framework of grounded circumscription that can be applied to a whole range of logics. We present the resulting logic: GC-SROIQ(C), and describe a tableau decision procedure for it.\n    ",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2017-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0440",
        "title": "Modelling serendipity in a computational context",
        "authors": [
            "Joseph Corneli",
            "Anna Jordanous",
            "Christian Guckelsberger",
            "Alison Pease",
            "Simon Colton"
        ],
        "abstract": "The term serendipity describes a creative process that develops, in context, with the active participation of a creative agent, but not entirely within that agent's control. While a system cannot be made to perform serendipitously on demand, we argue that its $\\mathit{serendipity\\ potential}$ can be increased by means of a suitable system architecture and other design choices. We distil a unified description of serendipitous occurrences from historical theorisations of serendipity and creativity. This takes the form of a framework with six phases: $\\mathit{perception}$, $\\mathit{attention}$, $\\mathit{interest}$, $\\mathit{explanation}$, $\\mathit{bridge}$, and $\\mathit{valuation}$. We then use this framework to organise a survey of literature in cognitive science, philosophy, and computing, which yields practical definitions of the six phases, along with heuristics for implementation. We use the resulting model to evaluate the serendipity potential of four existing systems developed by others, and two systems previously developed by two of the authors. Most existing research that considers serendipity in a computing context deals with serendipity as a service; here we relate theories of serendipity to the development of autonomous systems and computational creativity practice. We argue that serendipity is not teleologically blind, and outline representative directions for future applications of our model. We conclude that it is feasible to equip computational systems with the potential for serendipity, and that this could be beneficial in varied computational creativity/AI applications, particularly those designed to operate responsively in real-world contexts.\n    ",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2020-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0967",
        "title": "A Multi-Heuristic Approach for Solving the Pre-Marshalling Problem",
        "authors": [
            "Raka Jovanovic",
            "Milan Tuba",
            "Stefan Voss"
        ],
        "abstract": "Minimizing the number of reshuffling operations at maritime container terminals incorporates the Pre-Marshalling Problem (PMP) as an important problem. Based on an analysis of existing solution approaches we develop new heuristics utilizing specific properties of problem instances of the PMP. We show that the heuristic performance is highly dependent on these properties. We introduce a new method that exploits a greedy heuristic of four stages, where for each of these stages several different heuristics may be applied. Instead of using randomization to improve the performance of the heuristic, we repetitively generate a number of solutions by using a combination of different heuristics for each stage. In doing so, only a small number of solutions is generated for which we intend that they do not have undesirable properties, contrary to the case when simple randomization is used. Our experiments show that such a deterministic algorithm significantly outperforms the original nondeterministic method when the quality of found solutions is observed, with a much lower number of generated solutions.\n    ",
        "submission_date": "2014-11-02T00:00:00",
        "last_modified_date": "2014-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1080",
        "title": "A Heuristic Method for Solving the Problem of Partitioning Graphs with Supply and Demand",
        "authors": [
            "Raka Jovanovic",
            "Abdelkader Bousselham",
            "Stefan Voss"
        ],
        "abstract": "In this paper we present a greedy algorithm for solving the problem of the maximum partitioning of graphs with supply and demand (MPGSD). The goal of the method is to solve the MPGSD for large graphs in a reasonable time limit. This is done by using a two stage greedy algorithm, with two corresponding types of heuristics. The solutions acquired in this way are improved by applying a computationally inexpensive, hill climbing like, greedy correction procedure. In our numeric experiments we analyze different heuristic functions for each stage of the greedy algorithm, and show that their performance is highly dependent on the properties of the specific instance. Our tests show that by exploring a relatively small number of solutions generated by combining different heuristic functions, and applying the proposed correction procedure we can find solutions within only a few percent of the optimal ones.\n    ",
        "submission_date": "2014-11-02T00:00:00",
        "last_modified_date": "2014-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1112",
        "title": "Learning of Agent Capability Models with Applications in Multi-agent Planning",
        "authors": [
            "Yu Zhang",
            "Subbarao Kambhampati"
        ],
        "abstract": "One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of each other. An important aspect of these models of other agents is that they are often partial and incomplete. Thus far, there are two common representations of agent models: MDP based and action based, which are both based on action modeling. In many applications, agent models may not have been given, and hence must be learnt. While it may seem convenient to use either MDP based or action based models for learning, in this paper, we introduce a new representation based on capability models, which has several unique advantages. First, we show that learning capability models can be performed efficiently online via Bayesian learning, and the learning process is robust to high degrees of incompleteness in plan execution traces (e.g., with only start and end states). While high degrees of incompleteness in plan execution traces presents learning challenges for MDP based and action based models, capability models can still learn to {\\em abstract} useful information out of these traces. As a result, capability models are useful in applications in which such incompleteness is common, e.g., robot learning human model from observations and interactions. Furthermore, when used in multi-agent planning (with each agent modeled separately), capability models provide flexible abstraction of actions. The limitation, however, is that the synthesized plan is incomplete and abstract.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1373",
        "title": "Ethical Artificial Intelligence",
        "authors": [
            "Bill Hibbard"
        ],
        "abstract": "This book-length article combines several peer reviewed papers and new material to analyze the issues of ethical artificial intelligence (AI). The behavior of future AI systems can be described by mathematical equations, which are adapted to analyze possible unintended AI behaviors and ways that AI designs can avoid them. This article makes the case for utility-maximizing agents and for avoiding infinite sets in agent definitions. It shows how to avoid agent self-delusion using model-based utility functions and how to avoid agents that corrupt their reward generators (sometimes called \"perverse instantiation\") using utility functions that evaluate outcomes at one point in time from the perspective of humans at a different point in time. It argues that agents can avoid unintended instrumental actions (sometimes called \"basic AI drives\" or \"instrumental goals\") by accurately learning human values. This article defines a self-modeling agent framework and shows how it can avoid problems of resource limits, being predicted by other agents, and inconsistency between the agent's utility function and its definition (one version of this problem is sometimes called \"motivated value selection\"). This article also discusses how future AI will differ from current AI, the politics of AI, and the ultimate use of AI to help understand the nature of the universe and our place in it.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1497",
        "title": "The Spaces of Data, Information, and Knowledge",
        "authors": [
            "Xiaoyu Chen",
            "Dongming Wang"
        ],
        "abstract": "We study the data space $D$ of any given data set $X$ and explain how functions and relations are defined over $D$. From $D$ and for a specific domain $\\Delta$ we construct the information space $I$ of $X$ by interpreting variables, functions, and explicit relations over $D$ in $\\Delta$ and by including other relations that $D$ implies under the interpretation in $\\Delta$. Then from $I$ we build up the knowledge space $K$ of $X$ as the product of two spaces $K_T$ and $K_P$, where $K_T$ is obtained from $I$ by using the induction principle to generalize propositional relations to quantified relations, the deduction principle to generate new relations, and standard mechanisms to validate relations and $K_P$ is the space of specifications of methods with operational instructions which are valid in $K_T$. Through our construction of the three topological spaces the following key observation is made clear: the retrieval of information from the given data set for $\\Delta$ consists essentially in mining domain objects and relations, and the discovery of knowledge from the retrieved information consists essentially in applying the induction and deduction principles to generate propositions, synthesizing and modeling the information to generate specifications of methods with operational instructions, and validating the propositions and specifications. Based on this observation, efficient approaches may be designed to discover profound knowledge automatically from simple data, as demonstrated by the result of our study in the case of geometry.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1507",
        "title": "Scalable Parallel Numerical CSP Solver",
        "authors": [
            "Daisuke Ishii",
            "Kazuki Yoshizoe",
            "Toyotaro Suzumura"
        ],
        "abstract": "We present a parallel solver for numerical constraint satisfaction problems (NCSPs) that can scale on a number of cores. Our proposed method runs worker solvers on the available cores and simultaneously the workers cooperate for the search space distribution and balancing. In the experiments, we attained up to 119-fold speedup using 256 cores of a parallel computer.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1629",
        "title": "The Limitations of Standardized Science Tests as Benchmarks for Artificial Intelligence Research: Position Paper",
        "authors": [
            "Ernest Davis"
        ],
        "abstract": "In this position paper, I argue that standardized tests for elementary science such as SAT or Regents tests are not very good benchmarks for measuring the progress of artificial intelligence systems in understanding basic science. The primary problem is that these tests are designed to test aspects of knowledge and ability that are challenging for people; the aspects that are challenging for AI systems are very different. In particular, standardized tests do not test knowledge that is obvious for people; none of this knowledge can be assumed in AI systems. Individual standardized tests also have specific features that are not necessarily appropriate for an AI benchmark. I analyze the Physics subject SAT in some detail and the New York State Regents Science test more briefly. I also argue that the apparent advantages offered by using standardized tests are mostly either minor or illusory. The one major real advantage is that the significance is easily explained to the public; but I argue that even this is a somewhat mixed blessing. I conclude by arguing that, first, more appropriate collections of exam style problems could be assembled, and second, that there are better kinds of benchmarks than exam-style problems. In an appendix I present a collection of sample exam-style problems that test kinds of knowledge missing from the standardized tests.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1999",
        "title": "Azhary: An Arabic Lexical Ontology",
        "authors": [
            "Hossam Ishkewy",
            "Hany Harb",
            "Hassan Farahat"
        ],
        "abstract": "Arabic language is the most spoken languages in the Semitic languages group, and one of the most common languages in the world spoken by more than 422 million. It is also of paramount importance to Muslims, it is a sacred language of the Islamic Holly Book (Quran) and prayer (and other acts of worship) in Islam is performed only by mastering some of Arabic words. Arabic is also a major ritual language of a number of Christian churches in the Arab world and it is also used in writing several intellectual and religious Jewish books in the Middle Ages. Despite this, there is no semantic Arabic lexicon which researchers can depend on. In this paper we introduce Azhary as a lexical ontology for the Arabic language. It groups Arabic words into sets of synonyms called synsets, and records a number of relationships between words such as synonym, antonym, hypernym, hyponym, meronym, holonym and association relations. The ontology contains 26,195 words organized in 13,328 synsets. It has been developed and contrasted against AWN which is the most common available Arabic lexical ontology.\n    ",
        "submission_date": "2014-11-07T00:00:00",
        "last_modified_date": "2014-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2499",
        "title": "Comparative Study of View Update Algorithms in Rational Choice Theory",
        "authors": [
            "Radhakrishnan Delhibabu"
        ],
        "abstract": "The dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information. We show that knowledge base dynamics has interesting connection with kernel change via hitting set and abduction. The approach extends and integrates standard techniques for efficient query answering and integrity checking. The generation of hitting set is carried out through a hyper tableaux calculus and magic set that is focused on the goal of minimality. Many different view update algorithms have been proposed in the literature to address this problem. The present paper provides a comparative study of view update algorithms in rational approach.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2516",
        "title": "Answering Conjunctive Queries over $\\mathcal{EL}$ Knowledge Bases with Transitive and Reflexive Roles",
        "authors": [
            "Giorgio Stefanoni",
            "Boris Motik"
        ],
        "abstract": "Answering conjunctive queries (CQs) over $\\mathcal{EL}$ knowledge bases (KBs) with complex role inclusions is PSPACE-hard and in PSPACE in certain cases; however, if complex role inclusions are restricted to role transitivity, the tight upper complexity bound has so far been unknown. Furthermore, the existing algorithms cannot handle reflexive roles, and they are not practicable. Finally, the problem is tractable for acyclic CQs and $\\mathcal{ELH}$, and NP-complete for unrestricted CQs and $\\mathcal{ELHO}$ KBs. In this paper we complete the complexity landscape of CQ answering for several important cases. In particular, we present a practicable NP algorithm for answering CQs over $\\mathcal{ELHO}^s$ KBs---a logic containing all of OWL 2 EL, but with complex role inclusions restricted to role transitivity. Our preliminary evaluation suggests that the algorithm can be suitable for practical use. Moreover, we show that, even for a restricted class of so-called arborescent acyclic queries, CQ answering over $\\mathcal{EL}$ KBs becomes NP-hard in the presence of either transitive or reflexive roles. Finally, we show that answering arborescent CQs over $\\mathcal{ELHO}$ KBs is tractable, whereas answering acyclic CQs is NP-hard.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2800",
        "title": "Exploiting Parallelism for Hard Problems in Abstract Argumentation: Technical Report",
        "authors": [
            "Federico Cerutti",
            "Ilias Tachmazidis",
            "Mauro Vallati",
            "Sotirios Batsakis",
            "Massimiliano Giacomin",
            "Grigoris Antoniou"
        ],
        "abstract": "Abstract argumentation framework (\\AFname) is a unifying framework able to encompass a variety of nonmonotonic reasoning approaches, logic programming and computational argumentation. Yet, efficient approaches for most of the decision and enumeration problems associated to \\AFname s are missing, thus potentially limiting the efficacy of argumentation-based approaches in real domains. In this paper, we present an algorithm for enumerating the preferred extensions of abstract argumentation frameworks which exploits parallel computation. To this purpose, the SCC-recursive semantics definition schema is adopted, where extensions are defined at the level of specific sub-frameworks. The algorithm shows significant performance improvements in large frameworks, in terms of number of solutions found and speedup.\n    ",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3197",
        "title": "Warranty Cost Estimation Using Bayesian Network",
        "authors": [
            "Karamjit Singh",
            "Puneet Agarwal",
            "Gautam Shroff"
        ],
        "abstract": "All multi-component product manufacturing companies face the problem of warranty cost estimation. Failure rate analysis of components plays a key role in this problem. Data source used for failure rate analysis has traditionally been past failure data of components. However, failure rate analysis can be improved by means of fusion of additional information, such as symptoms observed during after-sale service of the product, geographical information (hilly or plains areas), and information from tele-diagnostic analytics. In this paper, we propose an approach, which learns dependency between part-failures and symptoms gleaned from such diverse sources of information, to predict expected number of failures with better accuracy. We also indicate how the optimum warranty period can be computed. We demonstrate, through empirical results, that our method can improve the warranty cost estimates significantly.\n    ",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2014-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3320",
        "title": "On Sparse Discretization for Graphical Games",
        "authors": [
            "Luis E. Ortiz"
        ],
        "abstract": "This short paper concerns discretization schemes for representing and computing approximate Nash equilibria, with emphasis on graphical games, but briefly touching on normal-form and poly-matrix games. The main technical contribution is a representation theorem that informally states that to account for every exact Nash equilibrium using a nearby approximate Nash equilibrium on a grid over mixed strategies, a uniform discretization size linear on the inverse of the approximation quality and natural game-representation parameters suffices. For graphical games, under natural conditions, the discretization is logarithmic in the game-representation size, a substantial improvement over the linear dependency previously required. The paper has five other objectives: (1) given the venue, to highlight the important, but often ignored, role that work on constraint networks in AI has in simplifying the derivation and analysis of algorithms for computing approximate Nash equilibria; (2) to summarize the state-of-the-art on computing approximate Nash equilibria, with emphasis on relevance to graphical games; (3) to help clarify the distinction between sparse-discretization and sparse-support techniques; (4) to illustrate and advocate for the deliberate mathematical simplicity of the formal proof of the representation theorem; and (5) to list and discuss important open problems, emphasizing graphical-game generalizations, which the AI community is most suitable to solve.\n    ",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2014-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3346",
        "title": "Membership Function Assignment for Elements of Single OWL Ontology",
        "authors": [
            "Olegs Verhodubs"
        ],
        "abstract": "This paper develops the idea of membership function assignment for OWL (Web Ontology Language) ontology elements in order to subsequently generate fuzzy rules from this ontology. The task of membership function assignment for OWL ontology elements had already been partially described, but this concerned the case, when several OWL ontologies of the same domain were available, and they were merged into a single ontology. The purpose of this paper is to present the way of membership function assignment for OWL ontology elements in the case, when there is the only one available ontology. Fuzzy rules, generated from the OWL ontology, are necessary for supplement of the SWES (Semantic Web Expert System) knowledge base. SWES is an expert system, which will be able to extract knowledge from OWL ontologies, found in the Web, and will serve as a universal expert for the user.\n    ",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2014-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3792",
        "title": "An Approach to Model Checking of Multi-agent Data Analysis",
        "authors": [
            "Natalia Garanina",
            "Eugene Bodin",
            "Elena Sidorova"
        ],
        "abstract": "The paper presents an approach to verification of a multi-agent data analysis algorithm. We base correct simulation of the multi-agent system by a finite integer model. For verification we use model checking tool SPIN. Protocols of agents are written in Promela language and properties of the multi-agent data analysis system are expressed in logic LTL. We run several experiments with SPIN and the model.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3806",
        "title": "Integrating Fuzzy and Ant Colony System for Fuzzy Vehicle Routing Problem with Time Windows",
        "authors": [
            "Sandhya Bansal",
            "V. Katiyar"
        ],
        "abstract": "In this paper fuzzy VRPTW with an uncertain travel time is considered. Credibility theory is used to model the problem and specifies a preference index at which it is desired that the travel times to reach the customers fall into their time windows. We propose the integration of fuzzy and ant colony system based evolutionary algorithm to solve the problem while preserving the constraints. Computational results for certain benchmark problems having short and long time horizons are presented to show the effectiveness of the algorithm. Comparison between different preferences indexes have been obtained to help the user in making suitable decisions.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3880",
        "title": "Optimal Cost Almost-sure Reachability in POMDPs",
        "authors": [
            "Krishnendu Chatterjee",
            "Martin Chmel\u00edk",
            "Raghav Gupta",
            "Ayush Kanodia"
        ],
        "abstract": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and every transition is associated with an integer cost. The optimization objective we study asks to minimize the expected total cost till the target set is reached, while ensuring that the target set is reached almost-surely (with probability 1). We show that for integer costs approximating the optimal cost is undecidable. For positive costs, our results are as follows: (i) we establish matching lower and upper bounds for the optimal cost and the bound is double exponential; (ii) we show that the problem of approximating the optimal cost is decidable and present approximation algorithms developing on the existing algorithms for POMDPs with finite-horizon objectives. While the worst-case running time of our algorithm is double exponential, we also present efficient stopping criteria for the algorithm and show experimentally that it performs well in many examples of interest.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4023",
        "title": "Automatic Generation of Alternative Starting Positions for Simple Traditional Board Games",
        "authors": [
            "Umair Z. Ahmed",
            "Krishnendu Chatterjee",
            "Sumit Gulwani"
        ],
        "abstract": "Simple board games, like Tic-Tac-Toe and CONNECT-4, play an important role not only in the development of mathematical and logical skills, but also in the emotional and social development. In this paper, we address the problem of generating targeted starting positions for such games. This can facilitate new approaches for bringing novice players to mastery, and also leads to discovery of interesting game variants. We present an approach that generates starting states of varying hardness levels for player~$1$ in a two-player board game, given rules of the board game, the desired number of steps required for player~$1$ to win, and the expertise levels of the two players. Our approach leverages symbolic methods and iterative simulation to efficiently search the extremely large state space. We present experimental results that include discovery of states of varying hardness levels for several simple grid-based board games. The presence of such states for standard game variants like $4 \\times 4$ Tic-Tac-Toe opens up new games to be played that have never been played as the default start state is heavily biased.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4156",
        "title": "Using Description Logics for RDF Constraint Checking and Closed-World Recognition",
        "authors": [
            "Peter F. Patel-Schneider"
        ],
        "abstract": "RDF and Description Logics work in an open-world setting where absence of information is not information about absence. Nevertheless, Description Logic axioms can be interpreted in a closed-world setting and in this setting they can be used for both constraint checking and closed-world recognition against information sources. When the information sources are expressed in well-behaved RDF or RDFS (i.e., RDF graphs interpreted in the RDF or RDFS semantics) this constraint checking and closed-world recognition is simple to describe. Further this constraint checking can be implemented as SPARQL querying and thus effectively performed.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2015-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4192",
        "title": "Introduction to ROSS: A New Representational Scheme",
        "authors": [
            "Glenn R. Hofford"
        ],
        "abstract": "ROSS (\"Representation, Ontology, Structure, Star\") is introduced as a new method for knowledge representation that emphasizes representational constructs for physical structure. The ROSS representational scheme includes a language called \"Star\" for the specification of ontology classes. The ROSS method also includes a formal scheme called the \"instance model\". Instance models are used in the area of natural language meaning representation to represent situations. This paper provides both the rationale and the philosophical background for the ROSS method.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4194",
        "title": "ROSS User's Guide and Reference Manual (Version 1.0)",
        "authors": [
            "Glenn R. Hofford"
        ],
        "abstract": "The ROSS method is a new approach in the area of knowledge representation that is useful for many artificial intelligence and natural language understanding representation and reasoning tasks. (ROSS stands for \"Representation\", \"Ontology\", \"Structure\", \"Star\" language). ROSS is a physical symbol-based representational scheme. ROSS provides a complex model for the declarative representation of physical structure and for the representation of processes and causality. From the metaphysical perspective, the ROSS view of external reality involves a 4D model, wherein discrete single-time-point unit-sized locations with states are the basis for all objects, processes and aspects that can be modeled. ROSS includes a language called \"Star\" for the specification of ontology classes. The ROSS method also includes a formal scheme called the \"instance model\". Instance models are used in the area of natural language meaning representation to represent situations. This document is an in-depth specification of the ROSS method.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4516",
        "title": "Verification of Relational Multiagent Systems with Data Types (Extended Version)",
        "authors": [
            "Diego Calvanese",
            "Giorgio Delzanno",
            "Marco Montali"
        ],
        "abstract": "We study the extension of relational multiagent systems (RMASs), where agents manipulate full-fledged relational databases, with data types and facets equipped with domain-specific, rigid relations (such as total orders). Specifically, we focus on design-time verification of RMASs against rich first-order temporal properties expressed in a variant of first-order mu-calculus with quantification across states. We build on previous decidability results under the \"state-bounded\" assumption, i.e., in each single state only a bounded number of data objects is stored in the agent databases, while unboundedly many can be encountered over time. We recast this condition, showing decidability in presence of dense, linear orders, and facets defined on top of them. Our approach is based on the construction of a finite-state, sound and complete abstraction of the original system, in which dense linear orders are reformulated as non-rigid relations working on the active domain of the system only. We also show undecidability when including a data type equipped with the successor relation.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4616",
        "title": "A Note on Systematic Conflict Generation in CA-EN-type Causal Structures",
        "authors": [
            "Antoni Lig\u0119za"
        ],
        "abstract": "This paper is aimed at providing a very first, more \"global\", systematic point of view with respect to possible conflict generation in CA-EN-like causal structures. For simplicity, only the outermost level of graphs is taken into account. Localization of the \"conflict area\", diagnostic preferences, and bases for systematic conflict generation are considered. A notion of {\\em Potential Conflict Structure} ({\\em PCS}) constituting a basic tool for identification of possible conflicts is proposed and its use is discussed.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4619",
        "title": "Aggregating partial rankings with applications to peer grading in massive online open courses",
        "authors": [
            "Ioannis Caragiannis",
            "George A. Krimpas",
            "Alexandros A. Voudouris"
        ],
        "abstract": "We investigate the potential of using ordinal peer grading for the evaluation of students in massive online open courses (MOOCs). According to such grading schemes, each student receives a few assignments (by other students) which she has to rank. Then, a global ranking (possibly translated into numerical scores) is produced by combining the individual ones. This is a novel application area for social choice concepts and methods where the important problem to be solved is as follows: how should the assignments be distributed so that the collected individual rankings can be easily merged into a global one that is as close as possible to the ranking that represents the relative performance of the students in the assignment? Our main theoretical result suggests that using very simple ways to distribute the assignments so that each student has to rank only $k$ of them, a Borda-like aggregation method can recover a $1-O(1/k)$ fraction of the true ranking when each student correctly ranks the assignments she receives. Experimental results strengthen our analysis further and also demonstrate that the same method is extremely robust even when students have imperfect capabilities as graders. We believe that our results provide strong evidence that ordinal peer grading can be a highly effective and scalable solution for evaluation in MOOCs.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4823",
        "title": "Automated Reasoning in Deontic Logic",
        "authors": [
            "Ulrich Furbach",
            "Claudia Schon",
            "Frieder Stolzenburg"
        ],
        "abstract": "Deontic logic is a very well researched branch of mathematical logic and philosophy. Various kinds of deontic logics are discussed for different application domains like argumentation theory, legal reasoning, and acts in multi-agent systems. In this paper, we show how standard deontic logic can be stepwise transformed into description logic and DL- clauses, such that it can be processed by Hyper, a high performance theorem prover which uses a hypertableau calculus. Two use cases, one from multi-agent research and one from the development of normative system are investigated.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4825",
        "title": "Cognitive Systems and Question Answering",
        "authors": [
            "Ulrich Furbach",
            "Claudia Schon",
            "Frieder Stolzenburg"
        ],
        "abstract": "This paper briefly characterizes the field of cognitive computing. As an exemplification, the field of natural language question answering is introduced together with its specific challenges. A possibility to master these challenges is illustrated by a detailed presentation of the LogAnswer system, which is a successful representative of the field of natural language question answering.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4925",
        "title": "Linguistic Descriptions for Automatic Generation of Textual Short-Term Weather Forecasts on Real Prediction Data",
        "authors": [
            "A. Ramos-Soto",
            "A. Bugar\u00edn",
            "S. Barro",
            "J. Taboada"
        ],
        "abstract": "We present in this paper an application which automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data together with a natural language generation (NLG) system. The application, named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the natural language generation system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation GALiWeather will be released as a real service offering custom forecasts for a wide public.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5007",
        "title": "A Unified View of Large-scale Zero-sum Equilibrium Computation",
        "authors": [
            "Kevin Waugh",
            "J. Andrew Bagnell"
        ],
        "abstract": "The task of computing approximate Nash equilibria in large zero-sum extensive-form games has received a tremendous amount of attention due mainly to the Annual Computer Poker Competition. Immediately after its inception, two competing and seemingly different approaches emerged---one an application of no-regret online learning, the other a sophisticated gradient method applied to a convex-concave saddle-point formulation. Since then, both approaches have grown in relative isolation with advancements on one side not effecting the other. In this paper, we rectify this by dissecting and, in a sense, unify the two views.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5220",
        "title": "Existential Rule Languages with Finite Chase: Complexity and Expressiveness",
        "authors": [
            "Heng Zhang",
            "Yan Zhang",
            "Jia-Huai You"
        ],
        "abstract": "Finite chase, or alternatively chase termination, is an important condition to ensure the decidability of existential rule languages. In the past few years, a number of rule languages with finite chase have been studied. In this work, we propose a novel approach for classifying the rule languages with finite chase. Using this approach, a family of decidable rule languages, which extend the existing languages with the finite chase property, are naturally defined. We then study the complexity of these languages. Although all of them are tractable for data complexity, we show that their combined complexity can be arbitrarily high. Furthermore, we prove that all the rule languages with finite chase that extend the weakly acyclic language are of the same expressiveness as the weakly acyclic one, while rule languages with higher combined complexity are in general more succinct than those with lower combined complexity.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2015-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5313",
        "title": "Ontology Module Extraction via Datalog Reasoning",
        "authors": [
            "Ana Armas Romero",
            "Mark Kaminski",
            "Bernardo Cuenca Grau",
            "Ian Horrocks"
        ],
        "abstract": "Module extraction - the task of computing a (preferably small) fragment M of an ontology T that preserves entailments over a signature S - has found many applications in recent years. Extracting modules of minimal size is, however, computationally hard, and often algorithmically infeasible. Thus, practical techniques are based on approximations, where M provably captures the relevant entailments, but is not guaranteed to be minimal. Existing approximations, however, ensure that M preserves all second-order entailments of T w.r.t. S, which is stronger than is required in many applications, and may lead to large modules in practice. In this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog. Our approach not only generalises existing approximations in an elegant way, but it can also be tailored to preserve only specific kinds of entailments, which allows us to extract significantly smaller modules. An evaluation on widely-used ontologies has shown very encouraging results.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5326",
        "title": "Compress and Control",
        "authors": [
            "Joel Veness",
            "Marc G. Bellemare",
            "Marcus Hutter",
            "Alvin Chua",
            "Guillaume Desjardins"
        ],
        "abstract": "This paper describes a new information-theoretic policy evaluation technique for reinforcement learning. This technique converts any compression or density model into a corresponding estimate of value. Under appropriate stationarity and ergodicity conditions, we show that the use of a sufficiently powerful model gives rise to a consistent value function estimator. We also study the behavior of this technique when applied to various Atari 2600 video games, where the use of suboptimal modeling techniques is unavoidable. We consider three fundamentally different models, all too limited to perfectly model the dynamics of the system. Remarkably, we find that our technique provides sufficiently accurate value estimates for effective on-policy control. We conclude with a suggestive study highlighting the potential of our technique to scale to large problems.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5410",
        "title": "Stable Model Counting and Its Application in Probabilistic Logic Programming",
        "authors": [
            "Rehan Abdul Aziz",
            "Geoffrey Chu",
            "Christian Muise",
            "Peter Stuckey"
        ],
        "abstract": "Model counting is the problem of computing the number of models that satisfy a given propositional theory. It has recently been applied to solving inference tasks in probabilistic logic programming, where the goal is to compute the probability of given queries being true provided a set of mutually independent random variables, a model (a logic program) and some evidence. The core of solving this inference task involves translating the logic program to a propositional theory and using a model counter. In this paper, we show that for some problems that involve inductive definitions like reachability in a graph, the translation of logic programs to SAT can be expensive for the purpose of solving inference tasks. For such problems, direct implementation of stable model semantics allows for more efficient solving. We present two implementation techniques, based on unfounded set detection, that extend a propositional model counter to a stable model counter. Our experiments show that for particular problems, our approach can outperform a state-of-the-art probabilistic logic programming solver by several orders of magnitude in terms of running time and space requirements, and can solve instances of significantly larger sizes on which the current solver runs out of time or memory.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5416",
        "title": "Recommending the Most Encompassing Opposing and Endorsing Arguments in Debates",
        "authors": [
            "Marius C. Silaghi",
            "Roussi Roussev"
        ],
        "abstract": "Arguments are essential objects in DirectDemocracyP2P, where they can occur both in association with signatures for petitions, or in association with other debated decisions, such as bug sorting by importance. The arguments of a signer on a given issue are grouped into one single justification, are classified by the type of signature (e.g., supporting or opposing), and can be subject to various types of threading.\n",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5635",
        "title": "Justifying Answer Sets using Argumentation",
        "authors": [
            "Claudia Schulz",
            "Francesca Toni"
        ],
        "abstract": "An answer set is a plain set of literals which has no further structure that would explain why certain literals are part of it and why others are not. We show how argumentation theory can help to explain why a literal is or is not contained in a given answer set by defining two justification methods, both of which make use of the correspondence between answer sets of a logic program and stable extensions of the Assumption-Based Argumentation (ABA) framework constructed from the same logic program. Attack Trees justify a literal in argumentation-theoretic terms, i.e. using arguments and attacks between them, whereas ABA-Based Answer Set Justifications express the same justification structure in logic programming terms, that is using literals and their relationships. Interestingly, an ABA-Based Answer Set Justification corresponds to an admissible fragment of the answer set in question, and an Attack Tree corresponds to an admissible fragment of the stable extension corresponding to this answer set.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5899",
        "title": "Falling Rule Lists",
        "authors": [
            "Fulton Wang",
            "Cynthia Rudin"
        ],
        "abstract": "Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6300",
        "title": "Discrete Bayesian Networks: The Exact Posterior Marginal Distributions",
        "authors": [
            "Do Le Paul Minh"
        ],
        "abstract": "In a Bayesian network, we wish to evaluate the marginal probability of a query variable, which may be conditioned on the observed values of some evidence variables. Here we first present our \"border algorithm,\" which converts a BN into a directed chain. For the polytrees, we then present in details, with some modifications and within the border algorithm framework, the \"revised polytree algorithm\" by Peot & Shachter (1991). Finally, we present our \"parentless polytree method,\" which, coupled with the border algorithm, converts any Bayesian network into a polytree, rendering the complexity of our inferences independent of the size of network, and linear with the number of its evidence and query variables. All quantities in this paper have probabilistic interpretations.\n    ",
        "submission_date": "2014-11-23T00:00:00",
        "last_modified_date": "2014-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6593",
        "title": "Rational Deployment of Multiple Heuristics in IDA*",
        "authors": [
            "David Tolpin",
            "Oded Betzalel",
            "Ariel Felner",
            "Solomon Eyal Shimony"
        ],
        "abstract": "Recent advances in metareasoning for search has shown its usefulness in improving numerous search algorithms. This paper applies rational metareasoning to IDA* when several admissible heuristics are available. The obvious basic approach of taking the maximum of the heuristics is improved upon by lazy evaluation of the heuristics, resulting in a variant known as Lazy IDA*. We introduce a rational version of lazy IDA* that decides whether to compute the more expensive heuristics or to bypass it, based on a myopic expected regret estimate. Empirical evaluation in several domains supports the theoretical results, and shows that rational lazy IDA* is a state-of-the-art heuristic combination method.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6651",
        "title": "A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network Structure",
        "authors": [
            "Amir Arsalan Soltani"
        ],
        "abstract": "In this report paper we first present a report of the Advanced Machine Learning Course Project on the provided data set and then present a novel heuristic algorithm for exact Bayesian network (BN) structure discovery that uses decomposable scoring functions. Our algorithm follows a different approach to solve the problem of BN structure discovery than the previously used methods such as Dynamic Programming (DP) and Branch and Bound to reduce the search space and find the global optima space for the problem. The algorithm we propose has some degree of flexibility that can make it more or less greedy. The more the algorithm is set to be greedy, the more the speed of the algorithm will be, and the less optimal the final structure. Our algorithm runs in a much less time than the previously known methods and guarantees to have an optimality of close to 99%. Therefore, it sacrifices less than one percent of score of an optimal structure in order to gain a much lower running time and make the algorithm feasible for large data sets (we may note that we never used any toolbox except for result validation)\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6754",
        "title": "HCRS: A hybrid clothes recommender system based on user ratings and product features",
        "authors": [
            "Xiaosong Hu",
            "Wen Zhu",
            "Qing Li"
        ],
        "abstract": "Nowadays, online clothes-selling business has become popular and extremely attractive because of its convenience and cheap-and-fine price. Good examples of these successful Web sites include ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6794",
        "title": "Some Reflections on the Set-based and the Conditional-based Interpretations of Statements in Syllogistic Reasoning",
        "authors": [
            "M. Pereira-Fari\u00f1a"
        ],
        "abstract": "Two interpretations about syllogistic statements are described in this paper. One is the so-called set-based interpretation, which assumes that quantified statements and syllogisms talk about quantity-relationships between sets. The other one, the so-called conditional interpretation, assumes that quantified propositions talk about conditional propositions and how strong are the links between the antecedent and the consequent. Both interpretations are compared attending to three different questions (existential import, singular statements and non-proportional quantifiers) from the point of view of their impact on the further development of this type of reasoning.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6998",
        "title": "Solving the Periodic Timetabling Problem using a Genetic Algorithm",
        "authors": [
            "Diego Arenas",
            "Remy Chevirer",
            "Said Hanafi",
            "Joaquin Rodriguez"
        ],
        "abstract": "In railway operations, a timetable is established to determine the departure and arrival times for the trains or other rolling stock at the different stations or relevant points inside the rail network or a subset of this network. The elaboration of this timetable is done to respond to the commercial requirements for both passenger and freight traffic, but also it must respect a set of security and capacity constraints associated with the railway network, rolling stock and legislation. Combining these requirements and constraints, as well as the important number of trains and schedules to plan, makes the preparation of a feasible timetable a complex and time-consuming process, that normally takes several months to be completed. This article addresses the problem of generating periodic timetables, which means that the involved trains operate in a recurrent pattern. For instance, the trains belonging to the same train line, depart from some station every 15 minutes or one hour. To tackle the problem, we present a constraint-based model suitable for this kind of problem. Then, we propose a genetic algorithm, allowing a rapid generation of feasible periodic timetables. Finally, two case studies are presented, the first, describing a sub-set of the Netherlands rail network, and the second a large portion of the Nord-pas-de-Calais regional rail network, both of them are then solved using our algorithm and the results are presented and discussed.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7149",
        "title": "A Fuzzy Syllogistic Reasoning Schema for Generalized Quantifiers",
        "authors": [
            "M. Pereira-Fari\u00f1a",
            "Juan C. Vidal",
            "F. D\u00edaz-Hermida",
            "A. Bugar\u00edn"
        ],
        "abstract": "In this paper, a new approximate syllogistic reasoning schema is described that expands some of the approaches expounded in the literature into two ways: (i) a number of different types of quantifiers (logical, absolute, proportional, comparative and exception) taken from Theory of Generalized Quantifiers and similarity quantifiers, taken from statistics, are considered and (ii) any number of premises can be taken into account within the reasoning process. Furthermore, a systematic reasoning procedure to solve the syllogism is also proposed, interpreting it as an equivalent mathematical optimization problem, where the premises constitute the constraints of the searching space for the quantifier in the conclusion.\n    ",
        "submission_date": "2014-11-26T00:00:00",
        "last_modified_date": "2014-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7441",
        "title": "Pattern Decomposition with Complex Combinatorial Constraints: Application to Materials Discovery",
        "authors": [
            "Stefano Ermon",
            "Ronan Le Bras",
            "Santosh K. Suram",
            "John M. Gregoire",
            "Carla Gomes",
            "Bart Selman",
            "Robert B. van Dover"
        ],
        "abstract": "Identifying important components or factors in large amounts of noisy data is a key problem in machine learning and data mining. Motivated by a pattern decomposition problem in materials discovery, aimed at discovering new materials for renewable energy, e.g. for fuel and solar cells, we introduce CombiFD, a framework for factor based pattern decomposition that allows the incorporation of a-priori knowledge as constraints, including complex combinatorial constraints. In addition, we propose a new pattern decomposition algorithm, called AMIQO, based on solving a sequence of (mixed-integer) quadratic programs. Our approach considerably outperforms the state of the art on the materials discovery problem, scaling to larger datasets and recovering more precise and physically meaningful decompositions. We also show the effectiveness of our approach for enforcing background knowledge on other application domains.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7480",
        "title": "Unweighted Stochastic Local Search can be Effective for Random CSP Benchmarks",
        "authors": [
            "Christopher D. Rosin"
        ],
        "abstract": "We present ULSA, a novel stochastic local search algorithm for random binary constraint satisfaction problems (CSP). ULSA is many times faster than the prior state of the art on a widely-studied suite of random CSP benchmarks. Unlike the best previous methods for these benchmarks, ULSA is a simple unweighted method that does not require dynamic adaptation of weights or penalties. ULSA obtains new record best solutions satisfying 99 of 100 variables in the challenging frb100-40 benchmark instance.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7525",
        "title": "On the analysis of set-based fuzzy quantified reasoning using classical syllogistics",
        "authors": [
            "M. Pereira-Fari\u00f1a",
            "F. D\u00edaz-Hermida",
            "A. Bugar\u00edn"
        ],
        "abstract": "Syllogism is a type of deductive reasoning involving quantified statements. The syllogistic reasoning scheme in the classical Aristotelian framework involves three crisp term sets and four linguistic quantifiers, for which the main support is the linguistic properties of the quantifiers. A number of fuzzy approaches for defining an approximate syllogism have been proposed for which the main support is cardinality calculus. In this paper we analyze fuzzy syllogistic models previously described by Zadeh and Dubois et al. and compare their behavior with that of the classical Aristotelian framework to check which of the 24 classical valid syllogistic reasoning patterns or moods are particular crisp cases of these fuzzy approaches. This allows us to assess to what extent these approaches can be considered as either plausible extensions of the classical crisp syllogism or a basis for a general approach to the problem of approximate syllogism.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7812",
        "title": "Elections with Few Voters: Candidate Control Can Be Easy",
        "authors": [
            "Jiehua Chen",
            "Piotr Faliszewski",
            "Rolf Niedermeier",
            "Nimrod Talmon"
        ],
        "abstract": "We study the computational complexity of candidate control in elections with few voters, that is, we consider the parameterized complexity of candidate control in elections with respect to the number of voters as a parameter. We consider both the standard scenario of adding and deleting candidates, where one asks whether a given candidate can become a winner (or, in the destructive case, can be precluded from winning) by adding or deleting few candidates, as well as a combinatorial scenario where adding/deleting a candidate automatically means adding or deleting a whole group of candidates. Considering several fundamental voting rules, our results show that the parameterized complexity of candidate control, with the number of voters as the parameter, is much more varied than in the setting with many voters.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2017-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7974",
        "title": "Solving Games with Functional Regret Estimation",
        "authors": [
            "Kevin Waugh",
            "Dustin Morrill",
            "J. Andrew Bagnell",
            "Michael Bowling"
        ],
        "abstract": "We propose a novel online learning method for minimizing regret in large extensive-form games. The approach learns a function approximator online to estimate the regret for choosing a particular action. A no-regret algorithm uses these estimates in place of the true regrets to define a sequence of policies.\n",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0315",
        "title": "Lifted Probabilistic Inference for Asymmetric Graphical Models",
        "authors": [
            "Guy Van den Broeck",
            "Mathias Niepert"
        ],
        "abstract": "Lifted probabilistic inference algorithms have been successfully applied to a large number of symmetric graphical models. Unfortunately, the majority of real-world graphical models is asymmetric. This is even the case for relational representations when evidence is given. Therefore, more recent work in the community moved to making the models symmetric and then applying existing lifted inference algorithms. However, this approach has two shortcomings. First, all existing over-symmetric approximations require a relational representation such as Markov logic networks. Second, the induced symmetries often change the distribution significantly, making the computed probabilities highly biased. We present a framework for probabilistic sampling-based inference that only uses the induced approximate symmetries to propose steps in a Metropolis-Hastings style Markov chain. The framework, therefore, leads to improved probability estimates while remaining unbiased. Experiments demonstrate that the approach outperforms existing MCMC algorithms.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2014-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0691",
        "title": "RoboBrain: Large-Scale Knowledge Engine for Robots",
        "authors": [
            "Ashutosh Saxena",
            "Ashesh Jain",
            "Ozan Sener",
            "Aditya Jami",
            "Dipendra K. Misra",
            "Hema S. Koppula"
        ],
        "abstract": "In this paper we introduce a knowledge engine, which learns and shares knowledge representations, for robots to carry out a variety of tasks. Building such an engine brings with it the challenge of dealing with multiple data modalities including symbols, natural language, haptic senses, robot trajectories, visual features and many others. The \\textit{knowledge} stored in the engine comes from multiple sources including physical interactions that robots have while performing tasks (perception, planning and control), knowledge bases from the Internet and learned representations from several robotics research groups.\n",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2015-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0773",
        "title": "Expressiveness of Logic Programs under General Stable Model Semantics",
        "authors": [
            "Heng Zhang",
            "Yan Zhang"
        ],
        "abstract": "The stable model semantics had been recently generalized to non-Herbrand structures by several works, which provides a unified framework and solid logical foundations for answer set programming. This paper focuses on the expressiveness of normal and disjunctive programs under the general stable model semantics. A translation from disjunctive programs to normal programs is proposed for infinite structures. Over finite structures, some disjunctive programs are proved to be intranslatable to normal programs if the arities of auxiliary predicates and functions are bounded in a certain way. The equivalence of the expressiveness of normal programs and disjunctive programs over arbitrary structures is also shown to coincide with that over finite structures, and coincide with whether NP is closed under complement. Moreover, to capture the exact expressiveness, some intertranslatability results between logic program classes and fragments of second-order logic are obtained.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0854",
        "title": "Semantic HMC for Big Data Analysis",
        "authors": [
            "Thomas Hassan",
            "Rafael Peixoto",
            "Christophe Cruz",
            "Aurlie Bertaux",
            "Nuno Silva"
        ],
        "abstract": "Analyzing Big Data can help corporations to im-prove their efficiency. In this work we present a new vision to derive Value from Big Data using a Semantic Hierarchical Multi-label Classification called Semantic HMC based in a non-supervised Ontology learning process. We also proposea Semantic HMC process, using scalable Machine-Learning techniques and Rule-based reasoning.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1044",
        "title": "Problem Theory",
        "authors": [
            "Ram\u00f3n Casares"
        ],
        "abstract": "The Turing machine, as it was presented by Turing himself, models the calculations done by a person. This means that we can compute whatever any Turing machine can compute, and therefore we are Turing complete. The question addressed here is why, Why are we Turing complete? Being Turing complete also means that somehow our brain implements the function that a universal Turing machine implements. The point is that evolution achieved Turing completeness, and then the explanation should be evolutionary, but our explanation is mathematical. The trick is to introduce a mathematical theory of problems, under the basic assumption that solving more problems provides more survival opportunities. So we build a problem theory by fusing set and computing theories. Then we construct a series of resolvers, where each resolver is defined by its computing capacity, that exhibits the following property: all problems solved by a resolver are also solved by the next resolver in the series if certain condition is satisfied. The last of the conditions is to be Turing complete. This series defines a resolvers hierarchy that could be seen as a framework for the evolution of cognition. Then the answer to our question would be: to solve most problems. By the way, the problem theory defines adaptation, perception, and learning, and it shows that there are just three ways to resolve any problem: routine, trial, and analogy. And, most importantly, this theory demonstrates how problems can be used to found mathematics and computing on biology.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1913",
        "title": "A Portfolio Approach to Algorithm Selection for Discrete Time-Cost Trade-off Problem",
        "authors": [
            "Santosh Mungle"
        ],
        "abstract": "It is a known fact that the performance of optimization algorithms for NP-Hard problems vary from instance to instance. We observed the same trend when we comprehensively studied multi-objective evolutionary algorithms (MOEAs) on a six benchmark instances of discrete time-cost trade-off problem (DTCTP) in a construction project. In this paper, instead of using a single algorithm to solve DTCTP, we use a portfolio approach that takes multiple algorithms as its constituent. We proposed portfolio comprising of four MOEAs, Non-dominated Sorting Genetic Algorithm II (NSGA-II), the strength Pareto Evolutionary Algorithm II (SPEA-II), Pareto archive evolutionary strategy (PAES) and Niched Pareto Genetic Algorithm II (NPGA-II) to solve DTCTP. The result shows that the portfolio approach is computationally fast and qualitatively superior to its constituent algorithms for all benchmark instances. Moreover, portfolio approach provides an insight in selecting the best algorithm for all benchmark instances of DTCTP.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2114",
        "title": "Chases and Escapes, and Optimization Problems",
        "authors": [
            "Toru Ohira"
        ],
        "abstract": "We propose a new approach for solving combinatorial optimization problem by utilizing the mechanism of chases and escapes, which has a long history in mathematics. In addition to the well-used steepest descent and neighboring search, we perform a chase and escape game on the \"landscape\" of the cost function. We have created a concrete algorithm for the Traveling Salesman Problem. Our preliminary test indicates a possibility that this new fusion of chases and escapes problem into combinatorial optimization search is fruitful.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2014-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2226",
        "title": "Possible and Necessary Allocations via Sequential Mechanisms",
        "authors": [
            "Haris Aziz",
            "Toby Walsh",
            "Lirong Xia"
        ],
        "abstract": "A simple mechanism for allocating indivisible resources is sequential allocation in which agents take turns to pick items. We focus on possible and necessary allocation problems, checking whether allocations of a given form occur in some or all mechanisms for several commonly used classes of sequential allocation mechanisms. In particular, we consider whether a given agent receives a given item, a set of items, or a subset of items for five natural classes of sequential allocation mechanisms: balanced, recursively balanced, balanced alternating, strictly alternating and all policies. We identify characterizations of allocations produced balanced, recursively balanced, balanced alternating policies and strictly alternating policies respectively, which extend the well-known characterization by Brams and King [2005] for policies without restrictions. In addition, we examine the computational complexity of possible and necessary allocation problems for these classes.\n    ",
        "submission_date": "2014-12-06T00:00:00",
        "last_modified_date": "2014-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2328",
        "title": "On the Behavioural Formalization of the Cognitive Middleware AWDRAT",
        "authors": [
            "Muhammad Taimoor Khan",
            "Dimitrios Serpanos",
            "Howard Shrobe"
        ],
        "abstract": "We present our ongoing work and initial results towards the (behavioral) correctness analysis of the cognitive middleware AWDRAT. Since, the (provable) behavioral correctness of a software system is a fundamental pre-requisite of the system's security. Therefore, the goal of the work is to first formalize the behavioral semantics of the middleware as a pre-requisite for our proof of the behavioral correctness. However, in this paper, we focus only on the core and critical component of the middleware, i.e. Execution Monitor which is a part of the module \"Architectural Differencer\" of AWDRAT. The role of the execution monitor is to identify inconsistencies between runtime observations of the target system and predictions of the specification System Architectural Model of the system. As a starting point we have defined the formal (denotational) semantics of the observations (runtime events) and predictions (executable specifications as of System Architectural Model); then based on the aforementioned formal semantices, we have formalized the behavior of the \"Execution Monitor\" of the middleware.\n    ",
        "submission_date": "2014-12-07T00:00:00",
        "last_modified_date": "2014-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2620",
        "title": "Cells in Multidimensional Recurrent Neural Networks",
        "authors": [
            "G. Leifert",
            "T. Strau\u00df",
            "T. Gr\u00fcning",
            "R. Labahn"
        ],
        "abstract": "The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist temporal classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2672",
        "title": "When Computer Vision Gazes at Cognition",
        "authors": [
            "Tao Gao",
            "Daniel Harari",
            "Joshua Tenenbaum",
            "Shimon Ullman"
        ],
        "abstract": "Joint attention is a core, early-developing form of social interaction. It is based on our ability to discriminate the third party objects that other people are looking at. While it has been shown that people can accurately determine whether another person is looking directly at them versus away, little is known about human ability to discriminate a third person gaze directed towards objects that are further away, especially in unconstraint cases where the looker can move her head and eyes freely. In this paper we address this question by jointly exploring human psychophysics and a cognitively motivated computer vision model, which can detect the 3D direction of gaze from 2D face images. The synthesis of behavioral study and computer vision yields several interesting discoveries. (1) Human accuracy of discriminating targets 8\u00b0-10\u00b0 of visual angle apart is around 40% in a free looking gaze task; (2) The ability to interpret gaze of different lookers vary dramatically; (3) This variance can be captured by the computational model; (4) Human outperforms the current model significantly. These results collectively show that the acuity of human joint attention is indeed highly impressive, given the computational challenge of the natural looking task. Moreover, the gap between human and model performance, as well as the variability of gaze interpretation across different lookers, require further understanding of the underlying mechanisms utilized by humans for this challenging task.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2824",
        "title": "Plan or not: Remote Human-robot Teaming with Incomplete Task Information",
        "authors": [
            "Vignesh Narayanan",
            "Yu Zhang",
            "Nathaniel Mendoza",
            "Subbarao Kambhampati"
        ],
        "abstract": "Human-robot interaction can be divided into two categories based on the physical distance between the human and robot: remote and proximal. In proximal interaction, the human and robot often engage in close coordination; in remote interaction, the human and robot are less coupled due to communication constraints. As a result, providing automation for the robot in remote interaction becomes more important. Thus far, human factor studies on automation in remote human-robot interaction have been restricted to various forms of supervision, in which the robot is essentially being used as a smart mobile manipulation platform with sensing capabilities. In this paper, we investigate the incorporation of general planning capability into the robot to facilitate peer-to-peer human-robot teaming, in which the human and robot are viewed as teammates that are physically separated. The human and robot share the same global goal and collaborate to achieve it. Note that humans may feel uncomfortable at such robot autonomy, which can potentially reduce teaming performance. One important difference between peer-to-peer teaming and supervised teaming is that an autonomous robot in peer-to-peer teaming can achieve the goal alone when the task information is completely specified. However, incompleteness often exists, which implies information asymmetry. While information asymmetry can be desirable sometimes, it may also lead to the robot choosing improper actions that negatively influence the teaming performance. We aim to investigate the various trade-offs, e.g., mental workload and situation awareness, between these two types of remote human-robot teaming.\n    ",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2985",
        "title": "Cause, Responsibility, and Blame: oA Structural-Model Approach",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "A definition of causality introduced by Halpern and Pearl, which uses structural equations, is reviewed. A more refined definition is then considered, which takes into account issues of normality and typicality, which are well known to affect causal ascriptions. Causality is typically an all-or-nothing notion: either A is a cause of B or it is not. An extension of the definition of causality to capture notions of degree of responsibility and degree of blame, due to Chockler and Halpern, is reviewed. For example, if someone wins an election 11-0, then each person who votes for him is less responsible for the victory than if he had won 6-5. Degree of blame takes into account an agent's epistemic state. Roughly speaking, the degree of blame of A for B is the expected degree of responsibility of A for B, taken over the epistemic state of an agent. Finally, the structural-equations definition of causality is compared to Wright's NESS test.\n    ",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3076",
        "title": "The Computational Complexity of Structure-Based Causality",
        "authors": [
            "Gadi Aleksandrowicz",
            "Hana Chockler",
            "Joseph Y. Halpern",
            "Alexander Ivrii"
        ],
        "abstract": "Halpern and Pearl introduced a definition of actual causality; Eiter and Lukasiewicz showed that computing whether X=x is a cause of Y=y is NP-complete in binary models (where all variables can take on only two values) and\\ Sigma_2^P-complete in general models. In the final version of their paper, Halpern and Pearl slightly modified the definition of actual cause, in order to deal with problems pointed by Hopkins and Pearl. As we show, this modification has a nontrivial impact on the complexity of computing actual cause. To characterize the complexity, a new family D_k^P, k= 1, 2, 3, ..., of complexity classes is introduced, which generalizes the class DP introduced by Papadimitriou and Yannakakis (DP is just D_1^P). %joe2 %We show that the complexity of computing causality is $\\D_2$-complete %under the new definition. Chockler and Halpern \\citeyear{CH04} extended the We show that the complexity of computing causality under the updated definition is $D_2^P$-complete.\n",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3079",
        "title": "Computoser - rule-based, probability-driven algorithmic music composition",
        "authors": [
            "Bozhidar Bozhanov"
        ],
        "abstract": "This paper presents the Computoser hybrid probability/rule based algorithm for music composition (",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3137",
        "title": "Rule reasoning for legal norm validation of FSTP facts",
        "authors": [
            "Naouel Karam",
            "Shashishekar Ramakrishna",
            "Adrian Paschke"
        ],
        "abstract": "Non-obviousness or inventive step is a general requirement for patentability in most patent law systems. An invention should be at an adequate distance beyond its prior art in order to be patented. This short paper provides an overview on a methodology proposed for legal norm validation of FSTP facts using rule reasoning approach.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2014-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3138",
        "title": "Computational Protein Design Using AND/OR Branch-and-Bound Search",
        "authors": [
            "Yichao Zhou",
            "Yuexin Wu",
            "Jianyang Zeng"
        ],
        "abstract": "The computation of the global minimum energy conformation (GMEC) is an important and challenging topic in structure-based computational protein design. In this paper, we propose a new protein design algorithm based on the AND/OR branch-and-bound (AOBB) search, which is a variant of the traditional branch-and-bound search algorithm, to solve this combinatorial optimization problem. By integrating with a powerful heuristic function, AOBB is able to fully exploit the graph structure of the underlying residue interaction network of a backbone template to significantly accelerate the design process. Tests on real protein data show that our new protein design algorithm is able to solve many prob- lems that were previously unsolvable by the traditional exact search algorithms, and for the problems that can be solved with traditional provable algorithms, our new method can provide a large speedup by several orders of magnitude while still guaranteeing to find the global minimum energy conformation (GMEC) solution.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3191",
        "title": "Bach in 2014: Music Composition with Recurrent Neural Network",
        "authors": [
            "I-Ting Liu",
            "Bhiksha Ramakrishnan"
        ],
        "abstract": "We propose a framework for computer music composition that uses resilient propagation (RProp) and long short term memory (LSTM) recurrent neural network. In this paper, we show that LSTM network learns the structure and characteristics of music pieces properly by demonstrating its ability to recreate music. We also show that predicting existing music using RProp outperforms Back propagation through time (BPTT).\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2014-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3279",
        "title": "The category of networks of ontologies",
        "authors": [
            "J\u00e9r\u00f4me Euzenat"
        ],
        "abstract": "The semantic web has led to the deployment of ontologies on the web connected through various relations and, in particular, alignments of their vocabularies. There exists several semantics for alignments which make difficult interoperation between different interpretation of networks of ontologies. Here we present an abstraction of these semantics which allows for defining the notions of closure and consistency for networks of ontologies independently from the precise semantics. We also show that networks of ontologies with specific notions of morphisms define categories of networks of ontologies.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2014-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3409",
        "title": "Teaching Deep Convolutional Neural Networks to Play Go",
        "authors": [
            "Christopher Clark",
            "Amos Storkey"
        ],
        "abstract": "Mastering the game of Go has remained a long standing challenge to the field of AI. Modern computer Go systems rely on processing millions of possible future positions to play well, but intuitively a stronger and more 'humanlike' way to play the game would be to rely on pattern recognition abilities rather then brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to 'hard code' symmetries that are expect to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction programs have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go, indicating it is state of the art among programs that do not use Monte Carlo Tree Search. It is also able to win some games against state of the art Go playing program Fuego while using a fraction of the play time. This success at playing Go indicates high level principles of the game were learned.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3518",
        "title": "Appropriate Causal Models and the Stability of Causation",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "Causal models defined in terms of structural equations have proved to be quite a powerful way of representing knowledge regarding causality. However, a number of authors have given examples that seem to show that the Halpern-Pearl (HP) definition of causality gives intuitively unreasonable answers. Here it is shown that, for each of these examples, we can give two stories consistent with the description in the example, such that intuitions regarding causality are quite different for each story. By adding additional variables, we can disambiguate the stories. Moreover, in the resulting causal models, the HP definition of causality gives the intuitively correct answer. It is also shown that, by adding extra variables, a modification to the original HP definition made to deal with an example of Hopkins and Pearl may not be necessary. Given how much can be done by adding extra variables, there might be a concern that the notion of causality is somewhat unstable. Can adding extra variables in a \"conservative\" way (i.e., maintaining all the relations between the variables in the original model) cause the answer to the question \"Is X=x a cause of Y=y\" to alternate between \"yes\" and \"no\"? It is shown that we can have such alternation infinitely often, but if we take normality into consideration, we cannot. Indeed, under appropriate normality assumptions. adding an extra variable can change the answer from \"yes\" to \"no\", but after that, it cannot cannot change back to \"yes\".\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3802",
        "title": "Turing Test for the Internet of Things",
        "authors": [
            "Neil Rubens"
        ],
        "abstract": "How smart is your kettle? How smart are things in your kitchen, your house, your neighborhood, on the internet? With the advent of Internet of Things, and the move of making devices `smart' by utilizing AI, a natural question arrises, how can we evaluate the progress. The standard way of evaluating AI is through the Turing Test. While Turing Test was designed for AI; the device that it was tailored to was a computer. Applying the test to variety of devices that constitute Internet of Things poses a number of challenges which could be addressed through a number of adaptations.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2014-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3908",
        "title": "Belief revision in the propositional closure of a qualitative algebra",
        "authors": [
            "Valmi Dufour-Lussier",
            "Alice Hermann",
            "Florence Le Ber",
            "Jean Lieber"
        ],
        "abstract": "Belief revision is an operation that aims at modifying old be-liefs so that they become consistent with new ones. The issue of belief revision has been studied in various formalisms, in particular, in qualitative algebras (QAs) in which the result is a disjunction of belief bases that is not necessarily repre-sentable in a QA. This motivates the study of belief revision in formalisms extending QAs, namely, their propositional clo-sures: in such a closure, the result of belief revision belongs to the formalism. Moreover, this makes it possible to define a contraction operator thanks to the Harper identity. Belief revision in the propositional closure of QAs is studied, an al-gorithm for a family of revision operators is designed, and an open-source implementation is made freely available on the web.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4271",
        "title": "Multi-Context Models for Reasoning under Partial Knowledge: Generative Process and Inference Grammar",
        "authors": [
            "Ardavan Salehi Nobandegani",
            "Ioannis N. Psaromiligkos"
        ],
        "abstract": "Arriving at the complete probabilistic knowledge of a domain, i.e., learning how all variables interact, is indeed a demanding task. In reality, settings often arise for which an individual merely possesses partial knowledge of the domain, and yet, is expected to give adequate answers to a variety of posed queries. That is, although precise answers to some queries, in principle, cannot be achieved, a range of plausible answers is attainable for each query given the available partial knowledge. In this paper, we propose the Multi-Context Model (MCM), a new graphical model to represent the state of partial knowledge as to a domain. MCM is a middle ground between Probabilistic Logic, Bayesian Logic, and Probabilistic Graphical Models. For this model we discuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., to form partial beliefs regarding a domain in a gradual and probabilistically consistent way, and (ii) how to perform inference, i.e., to evaluate a probability of interest involving some variables of the domain.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4465",
        "title": "Generating Graphical Chain by Mutual Matching of Bayesian Network and Extracted Rules of Bayesian Network Using Genetic Algorithm",
        "authors": [
            "Mostafa Sepahvand",
            "Ghasem Alikhajeh",
            "Meysam Ghaffari",
            "Abdolreza Mirzaei"
        ],
        "abstract": "With the technology development, the need of analyze and extraction of useful information is increasing. Bayesian networks contain knowledge from data and experts that could be used for decision making processes But they are not easily understandable thus the rule extraction methods have been used but they have high computation costs. To overcome this problem we extract rules from Bayesian network using genetic algorithm. Then we generate the graphical chain by mutually matching the extracted rules and Bayesian network. This graphical chain could shows the sequence of events that lead to the target which could help the decision making process. The experimental results on small networks show that the proposed method has comparable results with brute force method which has a significantly higher computation cost.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4485",
        "title": "Worst-case Optimal Query Answering for Greedy Sets of Existential Rules and Their Subclasses",
        "authors": [
            "Sebastian Rudolph",
            "Micha\u00ebl Thomazo",
            "Jean-Fran\u00e7ois Baget",
            "Marie-Laure Mugnier"
        ],
        "abstract": "The need for an ontological layer on top of data, associated with advanced reasoning mechanisms able to exploit the semantics encoded in ontologies, has been acknowledged both in the database and knowledge representation communities. We focus in this paper on the ontological query answering problem, which consists of querying data while taking ontological knowledge into account. More specifically, we establish complexities of the conjunctive query entailment problem for classes of existential rules (also called tuple-generating dependencies, Datalog+/- rules, or forall-exists-rules. Our contribution is twofold. First, we introduce the class of greedy bounded-treewidth sets (gbts) of rules, which covers guarded rules, and their most well-known generalizations. We provide a generic algorithm for query entailment under gbts, which is worst-case optimal for combined complexity with or without bounded predicate arity, as well as for data complexity and query complexity. Secondly, we classify several gbts classes, whose complexity was unknown, with respect to combined complexity (with both unbounded and bounded predicate arity) and data complexity to obtain a comprehensive picture of the complexity of existential rule fragments that are based on diverse guardedness notions. Upper bounds are provided by showing that the proposed algorithm is optimal for all of them.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4802",
        "title": "Neutrosophic information in the framework of multi-valued representation",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "The paper presents some steps for multi-valued representation of neutrosophic information. These steps are provided in the framework of multi-valued logics using the following logical value: true, false, neutral, unknown and saturated. Also, this approach provides some calculus formulae for the following neutrosophic features: truth, falsity, neutrality, ignorance, under-definedness, over-definedness, saturation and entropy. In addition, it was defined net truth, definedness and neutrosophic score.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2014-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4972",
        "title": "Max-Product Belief Propagation for Linear Programming: Applications to Combinatorial Optimization",
        "authors": [
            "Sejun Park",
            "Jinwoo Shin"
        ],
        "abstract": "The max-product {belief propagation} (BP) is a popular message-passing heuristic for approximating a maximum-a-posteriori (MAP) assignment in a joint distribution represented by a graphical model (GM). In the past years, it has been shown that BP can solve a few classes of linear programming (LP) formulations to combinatorial optimization problems including maximum weight matching, shortest path and network flow, i.e., BP can be used as a message-passing solver for certain combinatorial optimizations. However, those LPs and corresponding BP analysis are very sensitive to underlying problem setups, and it has been not clear what extent these results can be generalized to. In this paper, we obtain a generic criteria that BP converges to the optimal solution of given LP, and show that it is satisfied in LP formulations associated to many classical combinatorial optimization problems including maximum weight perfect matching, shortest path, traveling salesman, cycle packing, vertex/edge cover and network flow.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2017-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5077",
        "title": "A Multi-criteria neutrosophic group decision making metod based TOPSIS for supplier selection",
        "authors": [
            "R\u0131dvan \u015eahin",
            "Muhammed Yi\u011fider"
        ],
        "abstract": "The process of multiple criteria decision making (MCDM) is of determining the best choice among all of the probable alternatives. The problem of supplier selection on which decision maker has usually vague and imprecise knowledge is a typical example of multi criteria group decision-making problem. The conventional crisp techniques has not much effective for solving MCDM problems because of imprecise or fuzziness nature of the linguistic assessments. To find the exact values for MCDM problems is both difficult and impossible in more cases in real world. So, it is more reasonable to consider the values of alternatives according to the criteria as single valued neutrosophic sets (SVNS). This paper deal with the technique for order preference by similarity to ideal solution (TOPSIS) approach and extend the TOPSIS method to MCDM problem with single valued neutrosophic information. The value of each alternative and the weight of each criterion are characterized by single valued neutrosophic numbers. Here, the importance of criteria and alternatives is identified by aggregating individual opinions of decision makers (DMs) via single valued neutrosophic weighted averaging (IFWA) operator. The proposed method is, easy use, precise and practical for solving MCDM problem with single valued neutrosophic data. Finally, to show the applicability of the developed method, a numerical experiment for supplier choice is given as an application of single valued neutrosophic TOPSIS method at end of this paper.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5202",
        "title": "Multi-criteria neutrosophic decision making method based on score and accuracy functions under neutrosophic environment",
        "authors": [
            "R\u0131dvan \u015eahin"
        ],
        "abstract": "A neutrosophic set is a more general platform, which can be used to present uncertainty, imprecise, incomplete and inconsistent. In this paper a score function and an accuracy function for single valued neutrosophic sets is firstly proposed to make the distinction between them. Then the idea is extended to interval neutrosophic sets. A multi-criteria decision making method based on the developed score-accuracy functions is established in which criterion values for alternatives are single valued neutrosophic sets and interval neutrosophic sets. In decision making process, the neutrosophic weighted aggregation operators (arithmetic and geometric average operators) are adopted to aggregate the neutrosophic information related to each alternative. Thus, we can rank all alternatives and make the selection of the best of one(s) according to the score-accuracy functions. Finally, some illustrative examples are presented to verify the developed approach and to demonstrate its practicality and effectiveness.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5980",
        "title": "GraATP: A Graph Theoretic Approach for Automated Theorem Proving in Plane Geometry",
        "authors": [
            "Mohammad Murtaza Mahmud",
            "Swakkhar Shatabda",
            "Mohammad Nurul Huda"
        ],
        "abstract": "Automated Theorem Proving (ATP) is an established branch of Artificial Intelligence. The purpose of ATP is to design a system which can automatically figure out an algorithm either to prove or disprove a mathematical claim, on the basis of a set of given premises, using a set of fundamental postulates and following the method of logical inference. In this paper, we propose GraATP, a generalized framework for automated theorem proving in plane geometry. Our proposed method translates the geometric entities into nodes of a graph and the relations between them as edges of that graph. The automated system searches for different ways to reach the conclusion for a claim via graph traversal by which the validity of the geometric theorem is examined.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5984",
        "title": "Stochastic Local Search for Pattern Set Mining",
        "authors": [
            "Muktadir Hossain",
            "Tajkia Tasnim",
            "Swakkhar Shatabda",
            "Dewan M. Farid"
        ],
        "abstract": "Local search methods can quickly find good quality solutions in cases where systematic search methods might take a large amount of time. Moreover, in the context of pattern set mining, exhaustive search methods are not applicable due to the large search space they have to explore. In this paper, we propose the application of stochastic local search to solve the pattern set mining. Specifically, to the task of concept learning. We applied a number of local search algorithms on a standard benchmark instances for pattern set mining and the results show the potentials for further exploration.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6060",
        "title": "Combinatorial Structure of the Deterministic Seriation Method with Multiple Subset Solutions",
        "authors": [
            "Mark E. Madsen",
            "Carl P. Lipo"
        ],
        "abstract": "Seriation methods order a set of descriptions given some criterion (e.g., unimodality or minimum distance between similarity scores). Seriation is thus inherently a problem of finding the optimal solution among a set of permutations of objects. In this short technical note, we review the combinatorial structure of the classical seriation problem, which seeks a single solution out of a set of objects. We then extend those results to the iterative frequency seriation approach introduced by Lipo (1997), which finds optimal subsets of objects which each satisfy the unimodality criterion within each subset. The number of possible solutions across multiple solution subsets is larger than $n!$, which underscores the need to find new algorithms and heuristics to assist in the deterministic frequency seriation problem.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6141",
        "title": "Efficient Decision-Making by Volume-Conserving Physical Object",
        "authors": [
            "Song-Ju Kim",
            "Masashi Aono",
            "Etsushi Nameda"
        ],
        "abstract": "We demonstrate that any physical object, as long as its volume is conserved when coupled with suitable operations, provides a sophisticated decision-making capability. We consider the problem of finding, as accurately and quickly as possible, the most profitable option from a set of options that gives stochastic rewards. These decisions are made as dictated by a physical object, which is moved in a manner similar to the fluctuations of a rigid body in a tug-of-war game. Our analytical calculations validate statistical reasons why our method exhibits higher efficiency than conventional algorithms.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6413",
        "title": "Towards a Consistent, Sound and Complete Conceptual Knowledge",
        "authors": [
            "Gowri Shankar Ramaswamy",
            "F Sagayaraj Francis"
        ],
        "abstract": "Knowledge is only good if it is sound, consistent and complete. The same holds true for conceptual knowledge, which holds knowledge about concepts and its association. Conceptual knowledge no matter what format they are represented in, must be consistent, sound and complete in order to realise its practical use. This paper discusses consistency, soundness and completeness in the ambit of conceptual knowledge and the need to consider these factors as fundamental to the development of conceptual knowledge.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6545",
        "title": "KF metamodel formalization",
        "authors": [
            "Pablo R. Fillottrani",
            "C. Maria Keet"
        ],
        "abstract": "The KF metamodel is a comprehensive unifying metamodel covering the static structural entities and constraints of UML Class Diagrams (v2.4.1), ER, EER, ORM, and ORM2, and intended to boost interoperability of common conceptual data modelling languages. It was originally designed in UML with textual constraints, and in this report we present its formalisations in FOL and OWL, which accompanies the paper that describes, discusses, and analyses the KF metamodel in detail. These new formalizations contribute to give a precise meaning to the metamodel, to understand its complexity properties and to provide a basis for future implementations.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6649",
        "title": "Qualitative shape representation based on the qualitative relative direction and distance calculus eOPRAm",
        "authors": [
            "Christopher H. Dorr",
            "Reinhard Moratz"
        ],
        "abstract": "This document serves as a brief technical report, detailing the processes used to represent and reconstruct simplified polygons using qualitative spatial descriptions, as defined by the eOPRAm qualitative spatial calculus.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2014-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6703",
        "title": "Quantifying Natural and Artificial Intelligence in Robots and Natural Systems with an Algorithmic Behavioural Test",
        "authors": [
            "Hector Zenil"
        ],
        "abstract": "One of the most important aims of the fields of robotics, artificial intelligence and artificial life is the design and construction of systems and machines as versatile and as reliable as living organisms at performing high level human-like tasks. But how are we to evaluate artificial systems if we are not certain how to measure these capacities in living systems, let alone how to define life or intelligence? Here I survey a concrete metric towards measuring abstract properties of natural and artificial systems, such as the ability to react to the environment and to control one's own behaviour.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2014-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6973",
        "title": "Decision-theoretic rough sets-based three-way approximations of interval-valued fuzzy sets",
        "authors": [
            "Guangming Lang"
        ],
        "abstract": "In practical situations, interval-valued fuzzy sets are frequently encountered. In this paper, firstly, we present shadowed sets for interpreting and understanding interval fuzzy sets. We also provide an analytic solution to computing the pair of thresholds by searching for a balance of uncertainty in the framework of shadowed sets. Secondly, we construct errors-based three-way approximations of interval-valued fuzzy sets. We also provide an alternative decision-theoretic formulation for calculating the pair of thresholds by transforming interval-valued loss functions into single-valued loss functions, in which the required thresholds are computed by minimizing decision costs. Thirdly, we compute errors-based three-way approximations of interval-valued fuzzy sets by using interval-valued loss functions. Finally, we employ several examples to illustrate that how to take an action for an object with interval-valued membership grade by using interval-valued loss functions.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2014-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7585",
        "title": "Converting Instance Checking to Subsumption: A Rethink for Object Queries over Practical Ontologies",
        "authors": [
            "Jia Xu",
            "Patrick Shironoshita",
            "Ubbo Visser",
            "Nigel John",
            "Mansur Kabuka"
        ],
        "abstract": "Efficiently querying Description Logic (DL) ontologies is becoming a vital task in various data-intensive DL applications. Considered as a basic service for answering object queries over DL ontologies, instance checking can be realized by using the most specific concept (MSC) method, which converts instance checking into subsumption problems. This method, however, loses its simplicity and efficiency when applied to large and complex ontologies, as it tends to generate very large MSC's that could lead to intractable reasoning. In this paper, we propose a revision to this MSC method for DL SHI, allowing it to generate much simpler and smaller concepts that are specific-enough to answer a given query. With independence between computed MSC's, scalability for query answering can also be achieved by distributing and parallelizing the computations. An empirical evaluation shows the efficacy of our revised MSC method and the significant efficiency achieved when using it for answering object queries.\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7961",
        "title": "Reasoning for Improved Sensor Data Interpretation in a Smart Home",
        "authors": [
            "Marjan Alirezaie",
            "Amy Loutfi"
        ],
        "abstract": "In this paper an ontological representation and reasoning paradigm has been proposed for interpretation of time-series signals. The signals come from sensors observing a smart environment. The signal chosen for the annotation process is a set of unintuitive and complex gas sensor data. The ontology of this paradigm is inspired form the SSN ontology (Semantic Sensor Network) and used for representation of both the sensor data and the contextual information. The interpretation process is mainly done by an incremental ASP solver which as input receives a logic program that is generated from the contents of the ontology. The contextual information together with high level domain knowledge given in the ontology are used to infer explanations (answer sets) for changes in the ambient air detected by the gas sensors.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7964",
        "title": "Knowledge Propagation in Contextualized Knowledge Repositories: an Experimental Evaluation",
        "authors": [
            "Loris Bozzato",
            "Luciano Serafini"
        ],
        "abstract": "As the interest in the representation of context dependent knowledge in the Semantic Web has been recognized, a number of logic based solutions have been proposed in this regard. In our recent works, in response to this need, we presented the description logic-based Contextualized Knowledge Repository (CKR) framework. CKR is not only a theoretical framework, but it has been effectively implemented over state-of-the-art tools for the management of Semantic Web data: inference inside and across contexts has been realized in the form of forward SPARQL-based rules over different RDF named graphs. In this paper we present the first evaluation results for such CKR implementation. In particular, in first experiment we study its scalability with respect to different reasoning regimes. In a second experiment we analyze the effects of knowledge propagation on the computation of inferences.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7965",
        "title": "Adding Context to Knowledge and Action Bases",
        "authors": [
            "Diego Calvanese",
            "\u0130smail \u0130lkan Ceylan",
            "Marco Montali",
            "Ario Santoso"
        ],
        "abstract": "Knowledge and Action Bases (KABs) have been recently proposed as a formal framework to capture the dynamics of systems which manipulate Description Logic (DL) Knowledge Bases (KBs) through action execution. In this work, we enrich the KAB setting with contextual information, making use of different context dimensions. On the one hand, context is determined by the environment using context-changing actions that make use of the current state of the KB and the current context. On the other hand, it affects the set of TBox assertions that are relevant at each time point, and that have to be considered when processing queries posed over the KAB. Here we extend to our enriched setting the results on verification of rich temporal properties expressed in mu-calculus, which had been established for standard KABs. Specifically, we show that under a run-boundedness condition, verification stays decidable.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7967",
        "title": "Different Types of Conflicting Knowledge in AmI Environments",
        "authors": [
            "Martin Homola",
            "Theodore Patkos"
        ],
        "abstract": "We characterize different types of conflicts that may occur in complex distributed multi-agent scenarios, such as in Ambient Intelligence (AmI) environments, and we argue that these conflicts should be resolved in a suitable order and with the appropriate strategies for each individual conflict type. We call for further research with the goal of turning conflict resolution in AmI environments and similar multi-agent domains into a more coordinated and agreed upon process.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7968",
        "title": "Context-Aware Analytics in MOM Applications",
        "authors": [
            "Martin Ringsquandl",
            "Steffen Lamparter",
            "Raffaello Lepratti"
        ],
        "abstract": "Manufacturing Operations Management (MOM) systems are complex in the sense that they integrate data from heterogeneous systems inside the automation pyramid. The need for context-aware analytics arises from the dynamics of these systems that influence data generation and hamper comparability of analytics, especially predictive models (e.g. predictive maintenance), where concept drift affects application of these models in the future. Recently, an increasing amount of research has been directed towards data integration using semantic context models. Manual construction of such context models is an elaborate and error-prone task. Therefore, we pose the challenge to apply combinations of knowledge extraction techniques in the domain of analytics in MOM, which comprises the scope of data integration within Product Life-cycle Management (PLM), Enterprise Resource Planning (ERP), and Manufacturing Execution Systems (MES). We describe motivations, technological challenges and show benefits of context-aware analytics, which leverage from and regard the interconnectedness of semantic context data. Our example scenario shows the need for distribution and effective change tracking of context information.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7978",
        "title": "The Computational Theory of Intelligence: Information Entropy",
        "authors": [
            "Daniel Kovach"
        ],
        "abstract": "This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications.\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2014-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8529",
        "title": "A note about the generalisation of the C-tests",
        "authors": [
            "Jose Hernandez-Orallo"
        ],
        "abstract": "In this exploratory note we ask the question of what a measure of performance for all tasks is like if we use a weighting of tasks based on a difficulty function. This difficulty function depends on the complexity of the (acceptable) solution for the task (instead of a universal distribution over tasks or an adaptive test). The resulting aggregations and decompositions are (now retrospectively) seen as the natural (and trivial) interactive generalisation of the C-tests.\n    ",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8531",
        "title": "Workshop Notes of the 6th International Workshop on Acquisition, Representation and Reasoning about Context with Logic (ARCOE-Logic 2014)",
        "authors": [
            "Michael Fink",
            "Martin Homola",
            "Alessandra Mileo"
        ],
        "abstract": "ARCOE-Logic 2014, the 6th International Workshop on Acquisition, Representation and Reasoning about Context with Logic, was held in co-location with the 19th International Conference on Knowledge Engineering and Knowledge Management (EKAW 2014) on November 25, 2014 in Link\u00f6ping, Sweden. These notes contain the five papers which were accepted and presented at the workshop.\n    ",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8704",
        "title": "Quantum Structure in Cognition and the Foundations of Human Reasoning",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "Traditional cognitive science rests on a foundation of classical logic and probability theory. This foundation has been seriously challenged by several findings in experimental psychology on human decision making. Meanwhile, the formalism of quantum theory has provided an efficient resource for modeling these classically problematical situations. In this paper, we start from our successful quantum-theoretic approach to the modeling of concept combinations to formulate a unifying explanatory hypothesis. In it, human reasoning is the superposition of two processes -- a conceptual reasoning, whose nature is emergence of new conceptuality, and a logical reasoning, founded on an algebraic calculus of the logical type. In most cognitive processes however, the former reasoning prevails over the latter. In this perspective, the observed deviations from classical logical reasoning should not be interpreted as biases but, rather, as natural expressions of emergence in its deepest form.\n    ",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0102",
        "title": "A DDoS-Aware IDS Model Based on Danger Theory and Mobile Agents",
        "authors": [
            "Mahdi Zamani",
            "Mahnush Movahedi",
            "Mohammad Ebadzadeh",
            "Hossein Pedram"
        ],
        "abstract": "We propose an artificial immune model for intrusion detection in distributed systems based on a relatively recent theory in immunology called Danger theory. Based on Danger theory, immune response in natural systems is a result of sensing corruption as well as sensing unknown substances. In contrast, traditional self-nonself discrimination theory states that immune response is only initiated by sensing nonself (unknown) patterns. Danger theory solves many problems that could only be partially explained by the traditional model. Although the traditional model is simpler, such problems result in high false positive rates in immune-inspired intrusion detection systems. We believe using danger theory in a multi-agent environment that computationally emulates the behavior of natural immune systems is effective in reducing false positive rates. We first describe a simplified scenario of immune response in natural systems based on danger theory and then, convert it to a computational model as a network protocol. In our protocol, we define several immune signals and model cell signaling via message passing between agents that emulate cells. Most messages include application-specific patterns that must be meaningfully extracted from various system properties. We show how to model these messages in practice by performing a case study on the problem of detecting distributed denial-of-service attacks in wireless sensor networks. We conduct a set of systematic experiments to find a set of performance metrics that can accurately distinguish malicious patterns. The results indicate that the system can be efficiently used to detect malicious patterns with a high level of accuracy.\n    ",
        "submission_date": "2013-12-31T00:00:00",
        "last_modified_date": "2014-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0166",
        "title": "Medical Image Fusion: A survey of the state of the art",
        "authors": [
            "A.P. James",
            "B. V. Dasarathy"
        ],
        "abstract": "Medical image fusion is the process of registering and combining multiple images from single or multiple imaging modalities to improve the imaging quality and reduce randomness and redundancy in order to increase the clinical applicability of medical images for diagnosis and assessment of medical problems. Multi-modal medical image fusion algorithms and devices have shown notable achievements in improving clinical accuracy of decisions based on medical images. This review article provides a factual listing of methods and summarizes the broad scientific challenges faced in the field of medical image fusion. We characterize the medical image fusion research based on (1) the widely used image fusion methods, (2) imaging modalities, and (3) imaging of organs that are under study. This review concludes that even though there exists several open ended technological and scientific challenges, the fusion of medical images has proved to be useful for advancing the clinical reliability of using medical imaging for medical diagnostics and analysis, and is a scientific discipline that has the potential to significantly grow in the coming years.\n    ",
        "submission_date": "2013-12-31T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0282",
        "title": "Design of a GIS-based Assistant Software Agent for the Incident Commander to Coordinate Emergency Response Operations",
        "authors": [
            "Reza Nourjou",
            "Michinori Hatayama",
            "Stephen F. Smith",
            "Atabak Sadeghi",
            "Pedro Szekely"
        ],
        "abstract": "Problem: This paper addresses the design of an intelligent software system for the IC (incident commander) of a team in order to coordinate actions of agents (field units or robots) in the domain of emergency/crisis response operations. Objective: This paper proposes GICoordinator. It is a GIS-based assistant software agent that assists and collaborates with the human planner in strategic planning and macro tasks assignment for centralized multi-agent coordination. Method: Our approach to design GICoordinator was to: analyze the problem, design a complete data model, design an architecture of GICoordinator, specify required capabilities of human and system in coordination problem solving, specify development tools, and deploy. Result: The result was an architecture/design of GICoordinator that contains system requirements. Findings: GICoordinator efficiently integrates geoinformatics with artifice intelligent techniques in order to provide a spatial intelligent coordinator system for an IC to efficiently coordinate and control agents by making macro/strategic decisions. Results define a framework for future works to develop this system.\n    ",
        "submission_date": "2014-01-01T00:00:00",
        "last_modified_date": "2014-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0708",
        "title": "Quantitative methods for Phylogenetic Inference in Historical Linguistics: An experimental case study of South Central Dravidian",
        "authors": [
            "Taraka Rama",
            "Sudheer Kolachina",
            "Lakshmi Bai B"
        ],
        "abstract": "In this paper we examine the usefulness of two classes of algorithms Distance Methods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widely used in genetics, for predicting the family relationships among a set of related languages and therefore, diachronic language change. Applying these algorithms to the data on the numbers of shared cognates- with-change and changed as well as unchanged cognates for a group of six languages belonging to a Dravidian language sub-family given in Krishnamurti et al. (1983), we observed that the resultant phylogenetic trees are largely in agreement with the linguistic family tree constructed using the comparative method of reconstruction with only a few minor differences. Furthermore, we studied these minor differences and found that they were cases of genuine ambiguity even for a well-trained historical linguist. We evaluated the trees obtained through our experiments using a well-defined criterion and report the results here. We finally conclude that quantitative methods like the ones we examined are quite useful in predicting family relationships among languages. In addition, we conclude that a modest degree of confidence attached to the intuition that there could indeed exist a parallelism between the processes of linguistic and genetic change is not totally misplaced.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0742",
        "title": "Data Smashing",
        "authors": [
            "Ishanu Chattopadhyay",
            "Hod Lipson"
        ],
        "abstract": "Investigation of the underlying physics or biology from empirical data requires a quantifiable notion of similarity - when do two observed data sets indicate nearly identical generating processes, and when they do not. The discriminating characteristics to look for in data is often determined by heuristics designed by experts, $e.g.$, distinct shapes of \"folded\" lightcurves may be used as \"features\" to classify variable stars, while determination of pathological brain states might require a Fourier analysis of brainwave activity. Finding good features is non-trivial. Here, we propose a universal solution to this problem: we delineate a principle for quantifying similarity between sources of arbitrary data streams, without a priori knowledge, features or training. We uncover an algebraic structure on a space of symbolic models for quantized data, and show that such stochastic generators may be added and uniquely inverted; and that a model and its inverse always sum to the generator of flat white noise. Therefore, every data stream has an anti-stream: data generated by the inverse model. Similarity between two streams, then, is the degree to which one, when summed to the other's anti-stream, mutually annihilates all statistical structure to noise. We call this data smashing. We present diverse applications, including disambiguation of brainwaves pertaining to epileptic seizures, detection of anomalous cardiac rhythms, and classification of astronomical objects from raw photometry. In our examples, the data smashing principle, without access to any domain knowledge, meets or exceeds the performance of specialized algorithms tuned by domain experts.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0943",
        "title": "LB2CO: A Semantic Ontology Framework for B2C eCommerce Transaction on the Internet",
        "authors": [
            "Adeyinka K Akanbi"
        ],
        "abstract": "Business ontology can enhance the successful development of complex enterprise system; this is being achieved through knowledge sharing and the ease of communication between every entity in the domain. Through human semantic interaction with the web resources, machines to interpret the data published in a machine interpretable form under web. However, the theoretical practice of business ontology in eCommerce domain is quite a few especially in the section of electronic transaction, and the various techniques used to obtain efficient communication across spheres are error prone and are not always guaranteed to be efficient in obtaining desired result due to poor semantic integration between entities. To overcome the poor semantic integration this research focuses on proposed ontology called LB2CO, which combines the framework of IDEF5 & SNAP as an analysis tool, for automated recommendation of product and services and create effective ontological framework for B2C transaction & communication across different business domains that facilitates the interoperability & integration of B2C transactions over the web.\n    ",
        "submission_date": "2014-01-05T00:00:00",
        "last_modified_date": "2014-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1031",
        "title": "Constraint Solvers for User Interface Layout",
        "authors": [
            "Noreen Jamil"
        ],
        "abstract": "Constraints have played an important role in the construction of GUIs, where they are mainly used to define the layout of the widgets. Resizing behavior is very important in GUIs because areas have domain specific parameters such as form the resizing of windows. If linear objective function is used and window is resized then error is not distributed equally. To distribute the error equally, a quadratic objective function is introduced. Different algorithms are widely used for solving linear constraints and quadratic problems in a variety of different scientific areas. The linear relxation, Kaczmarz, direct and linear programming methods are common methods for solving linear constraints for GUI layout. The interior point and active set methods are most commonly used techniques to solve quadratic programming problems. Current constraint solvers designed for GUI layout do not use interior point methods for solving a quadratic objective function subject to linear equality and inequality constraints. In this paper, performance aspects and the convergence speed of interior point and active set methods are compared along with one most commonly used linear programming method when they are implemented for graphical user interface layout. The performance and convergence of the proposed algorithms are evaluated empirically using randomly generated UI layout specifications of various sizes. The results show that the interior point algorithms perform significantly better than the Simplex method and QOCA-solver, which uses the active set method implementation for solving quadratic optimization.\n    ",
        "submission_date": "2014-01-06T00:00:00",
        "last_modified_date": "2014-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1475",
        "title": "Belief Revision in Structured Probabilistic Argumentation",
        "authors": [
            "Paulo Shakarian",
            "Gerardo I. Simari",
            "Marcelo A. Falappa"
        ],
        "abstract": "In real-world applications, knowledge bases consisting of all the information at hand for a specific domain, along with the current state of affairs, are bound to contain contradictory data coming from different sources, as well as data with varying degrees of uncertainty attached. Likewise, an important aspect of the effort associated with maintaining knowledge bases is deciding what information is no longer useful; pieces of information (such as intelligence reports) may be outdated, may come from sources that have recently been discovered to be of low quality, or abundant evidence may be available that contradicts them. In this paper, we propose a probabilistic structured argumentation framework that arises from the extension of Presumptive Defeasible Logic Programming (PreDeLP) with probabilistic models, and argue that this formalism is capable of addressing the basic issues of handling contradictory and uncertain data. Then, to address the last issue, we focus on the study of non-prioritized belief revision operations over probabilistic PreDeLP programs. We propose a set of rationality postulates -- based on well-known ones developed for classical knowledge bases -- that characterize how such operations should behave, and study a class of operators along with theoretical relationships with the proposed postulates, including a representation theorem stating the equivalence between this class and the class of operators characterized by the postulates.\n    ",
        "submission_date": "2014-01-07T00:00:00",
        "last_modified_date": "2014-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1549",
        "title": "Optimal Demand Response Using Device Based Reinforcement Learning",
        "authors": [
            "Zheng Wen",
            "Daniel O'Neill",
            "Hamid Reza Maei"
        ],
        "abstract": "Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS's rescheduling problem as a reinforcement learning (RL) problem, and argue that this RL problem can be approximately solved by decomposing it over device clusters. Compared with existing formulations, our new formulation (1) does not require explicitly modeling the user's dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1560",
        "title": "Beyond One-Step-Ahead Forecasting: Evaluation of Alternative Multi-Step-Ahead Forecasting Models for Crude Oil Prices",
        "authors": [
            "Tao Xiong",
            "Yukun Bao",
            "Zhongyi Hu"
        ],
        "abstract": "An accurate prediction of crude oil prices over long future horizons is challenging and of great interest to governments, enterprises, and investors. This paper proposes a revised hybrid model built upon empirical mode decomposition (EMD) based on the feed-forward neural network (FNN) modeling framework incorporating the slope-based method (SBM), which is capable of capturing the complex dynamic of crude oil prices. Three commonly used multi-step-ahead prediction strategies proposed in the literature, including iterated strategy, direct strategy, and MIMO (multiple-input multiple-output) strategy, are examined and compared, and practical considerations for the selection of a prediction strategy for multi-step-ahead forecasting relating to crude oil prices are identified. The weekly data from the WTI (West Texas Intermediate) crude oil spot price are used to compare the performance of the alternative models under the EMD-SBM-FNN modeling framework with selected counterparts. The quantitative and comprehensive assessments are performed on the basis of prediction accuracy and computational cost. The results obtained in this study indicate that the proposed EMD-SBM-FNN model using the MIMO strategy is the best in terms of prediction accuracy with accredited computational load.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1752",
        "title": "Speeding up SOR Solvers for Constraint-based GUIs with a Warm-Start Strategy",
        "authors": [
            "Noreen Jamil",
            "Johannes M\u00fcller",
            "Christof Lutteroth",
            "Gerald Weber"
        ],
        "abstract": "Many computer programs have graphical user interfaces (GUIs), which need good layout to make efficient use of the available screen real estate. Most GUIs do not have a fixed layout, but are resizable and able to adapt themselves. Constraints are a powerful tool for specifying adaptable GUI layouts: they are used to specify a layout in a general form, and a constraint solver is used to find a satisfying concrete layout, e.g.\\ for a specific GUI size. The constraint solver has to calculate a new layout every time a GUI is resized or changed, so it needs to be efficient to ensure a good user experience. One approach for constraint solvers is based on the Gauss-Seidel algorithm and successive over-relaxation (SOR).\n",
        "submission_date": "2014-01-06T00:00:00",
        "last_modified_date": "2014-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1926",
        "title": "A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters Optimization",
        "authors": [
            "Yukun Bao",
            "Zhongyi Hu",
            "Tao Xiong"
        ],
        "abstract": "Addressing the issue of SVMs parameters optimization, this study proposes an efficient memetic algorithm based on Particle Swarm Optimization algorithm (PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is responsible for exploration of the search space and the detection of the potential regions with optimum solutions, while pattern search (PS) is used to produce an effective exploitation on the potential regions obtained by PSO. Moreover, a novel probabilistic selection strategy is proposed to select the appropriate individuals among the current population to undergo local refinement, keeping a well balance between exploration and exploitation. Experimental results confirm that the local refinement with PS and our proposed selection strategy are effective, and finally demonstrate effectiveness and robustness of the proposed PSO-PS based MA for SVMs parameters optimization.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2482",
        "title": "STIMONT: A core ontology for multimedia stimuli description",
        "authors": [
            "Marko Horvat",
            "Nikola Bogunovi\u0107",
            "Kre\u0161imir \u0106osi\u0107"
        ],
        "abstract": "Affective multimedia documents such as images, sounds or videos elicit emotional responses in exposed human subjects. These stimuli are stored in affective multimedia databases and successfully used for a wide variety of research in psychology and neuroscience in areas related to attention and emotion processing. Although important all affective multimedia databases have numerous deficiencies which impair their applicability. These problems, which are brought forward in the paper, result in low recall and precision of multimedia stimuli retrieval which makes creating emotion elicitation procedures difficult and labor-intensive. To address these issues a new core ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and extends W3C EmotionML format with an expressive and formal representation of affective concepts, high-level semantics, stimuli document metadata and the elicited physiology. The advantages of ontology in description of affective multimedia stimuli are demonstrated in a document retrieval experiment and compared against contemporary keyword-based querying methods. Also, a software tool Intelligent Stimulus Generator for retrieval of affective multimedia and construction of stimuli sequences is presented.\n    ",
        "submission_date": "2014-01-10T00:00:00",
        "last_modified_date": "2014-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2657",
        "title": "The Missing Ones: Key Ingredients Towards Effective Ambient Assisted Living Systems",
        "authors": [
            "Hong Sun",
            "Vincenzo De Florio",
            "Ning Gui",
            "Chris Blondia"
        ],
        "abstract": "The population of elderly people keeps increasing rapidly, which becomes a predominant aspect of our societies. As such, solutions both efficacious and cost-effective need to be sought. Ambient Assisted Living (AAL) is a new approach which promises to address the needs from elderly people. In this paper, we claim that human participation is a key ingredient towards effective AAL systems, which not only saves social resources, but also has positive relapses on the psychological health of the elderly people. Challenges in increasing the human participation in ambient assisted living are discussed in this paper and solutions to meet those challenges are also proposed. We use our proposed mutual assistance community, which is built with service oriented approach, as an example to demonstrate how to integrate human tasks in AAL systems. Our preliminary simulation results are presented, which support the effectiveness of human participation.\n    ",
        "submission_date": "2014-01-12T00:00:00",
        "last_modified_date": "2014-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3426",
        "title": "Networks of Influence Diagrams: A Formalism for Representing Agents' Beliefs and Decision-Making Processes",
        "authors": [
            "Yaakov Gal",
            "Avi Pfeffer"
        ],
        "abstract": "This paper presents Networks of Influence Diagrams (NID), a compact, natural and highly expressive language for reasoning about agents beliefs and decision-making processes. NIDs are graphical structures in which agents mental models are represented as nodes in a network; a mental model for an agent may itself use descriptions of the mental models of other agents. NIDs are demonstrated by examples, showing how they can be used to describe conflicting and cyclic belief structures, and certain forms of bounded rationality. In an opponent modeling domain, NIDs were able to outperform other computational agents whose strategies were not known in advance. NIDs are equivalent in representation to Bayesian games but they are more compact and structured than this formalism. In particular, the equilibrium definition for NIDs makes an explicit distinction between agents optimal strategies, and how they actually behave in reality.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3427",
        "title": "Analogical Dissimilarity: Definition, Algorithms and Two Experiments in Machine Learning",
        "authors": [
            "Laurent Miclet",
            "Sabri Bayoudh",
            "Arnaud Delhay"
        ],
        "abstract": "This paper defines the notion of analogical dissimilarity between four objects, with a special focus on objects structured as sequences. Firstly, it studies the case where the four objects have a null analogical dissimilarity, i.e. are in analogical proportion. Secondly, when one of these objects is unknown, it gives algorithms to compute it. Thirdly, it tackles the problem of defining analogical dissimilarity, which is a measure of how far four objects are from being in analogical proportion. In particular, when objects are sequences, it gives a definition and an algorithm based on an optimal alignment of the four sequences. It gives also learning algorithms, i.e. methods to find the triple of objects in a learning sample which has the least analogical dissimilarity with a given object. Two practical experiments are described: the first is a classification problem on benchmarks of binary and nominal data, the second shows how the generation of sequences by solving analogical equations enables a handwritten character recognition system to rapidly be adapted to a new writer.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3441",
        "title": "Transductive Rademacher Complexity and its Applications",
        "authors": [
            "Ran El-Yaniv",
            "Dmitry Pechyony"
        ],
        "abstract": "We develop a technique for deriving data-dependent error bounds for transductive learning algorithms based on transductive Rademacher complexity. Our technique is based on a novel general error bound for transduction in terms of transductive Rademacher complexity, together with a novel bounding technique for Rademacher averages for particular algorithms, in terms of their \"unlabeled-labeled\" representation. This technique is relevant to many advanced graph-based transductive algorithms and we demonstrate its effectiveness by deriving error bounds to three well known algorithms. Finally, we present a new PAC-Bayesian bound for mixtures of transductive algorithms based on our Rademacher bounds.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3462",
        "title": "Efficient Informative Sensing using Multiple Robots",
        "authors": [
            "Amarjeet Singh",
            "Andreas Krause",
            "Carlos Guestrin",
            "William J. Kaiser"
        ],
        "abstract": "The need for efficient monitoring of spatio-temporal dynamics in large environmental applications, such as the water quality monitoring in rivers and lakes, motivates the use of robotic sensors in order to achieve sufficient spatial coverage. Typically, these robots have bounded resources, such as limited battery or limited amounts of time to obtain measurements. Thus, careful coordination of their paths is required in order to maximize the amount of information collected, while respecting the resource constraints. In this paper, we present an efficient approach for near-optimally solving the NP-hard optimization problem of planning such informative paths. In particular, we first develop eSIP (efficient Single-robot Informative Path planning), an approximation algorithm for optimizing the path of a single robot. Hereby, we use a Gaussian Process to model the underlying phenomenon, and use the mutual information between the visited locations and remainder of the space to quantify the amount of information collected. We prove that the mutual information collected using paths obtained by using eSIP is close to the information obtained by an optimal solution. We then provide a general technique, sequential allocation, which can be used to extend any single robot planning algorithm, such as eSIP, for the multi-robot problem. This procedure approximately generalizes any guarantees for the single-robot problem to the multi-robot case. We extensively evaluate the effectiveness of our approach on several experiments performed in-field for two important environmental sensing applications, lake and river monitoring, and simulation experiments performed using several real world sensor network data sets.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3463",
        "title": "Automated Reasoning in Modal and Description Logics via SAT Encoding: the Case Study of K(m)/ALC-Satisfiability",
        "authors": [
            "Roberto Sebastiani",
            "Michele Vescovi"
        ],
        "abstract": "In the last two decades, modal and description logics have been applied to numerous areas of computer science, including knowledge representation, formal verification, database theory, distributed computing and, more recently, semantic web and ontologies. For this reason, the problem of automated reasoning in modal and description logics has been thoroughly investigated. In particular, many approaches have been proposed for efficiently handling the satisfiability of the core normal modal logic K(m), and of its notational variant, the description logic ALC. Although simple in structure, K(m)/ALC is computationally very hard to reason on, its satisfiability being PSPACE-complete.  \n",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3464",
        "title": "Learning Bayesian Network Equivalence Classes with Ant Colony Optimization",
        "authors": [
            "R\u00f3n\u00e1n Daly",
            "Qiang Shen"
        ],
        "abstract": "Bayesian networks are a useful tool in the representation of uncertain knowledge. This paper proposes a new algorithm called ACO-E, to learn the structure of a Bayesian network. It does this by conducting a search through the space of equivalence classes of Bayesian networks using Ant Colony Optimization (ACO). To this end, two novel extensions of traditional ACO techniques are proposed and implemented. Firstly, multiple types of moves are allowed. Secondly, moves can be given in terms of indices that are not based on construction graph nodes. The results of testing show that ACO-E performs better than a greedy search and other state-of-the-art and metaheuristic algorithms whilst searching in the space of equivalence classes.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3466",
        "title": "An Anytime Algorithm for Optimal Coalition Structure Generation",
        "authors": [
            "Talal Rahwan",
            "Sarvapali Dyanand Ramchurn",
            "Nicholas Robert Jennings",
            "Andrea Giovannucci"
        ],
        "abstract": "Coalition formation is a fundamental type of interaction that involves the creation of coherent groupings of distinct, autonomous, agents in order to efficiently achieve their individual or collective goals. Forming effective coalitions is a major research challenge in the field of  multi-agent systems. Central to this endeavour is the problem of determining which of the many possible coalitions to form in order to achieve some goal. This usually requires calculating a value for every possible coalition, known as the coalition value, which indicates how beneficial that coalition would be if it was formed. Once these values  are calculated, the agents usually need to find a combination of  coalitions, in which every agent belongs to exactly one coalition, and by which the overall outcome of the system is maximized. However, this coalition structure generation problem is extremely challenging due to the number of possible solutions that need to be examined, which grows exponentially with the number of agents involved. To date, therefore, many algorithms have been proposed to solve this problem using different techniques ranging from dynamic programming, to integer programming, to stochastic search all of which suffer from major limitations relating to execution time, solution quality, and memory requirements.\n",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3472",
        "title": "Variable Forgetting in Reasoning about Knowledge",
        "authors": [
            "Kaile Su",
            "Abdul Sattar",
            "Guanfeng Lv",
            "Yan Zhang"
        ],
        "abstract": "In this paper, we investigate knowledge reasoning within a simple framework called knowledge structure. We use variable forgetting as a basic operation for one agent to reason about its own or other agents\\ knowledge. In our framework, two notions namely agents\\ observable variables and the weakest sufficient condition play important roles in knowledge reasoning. Given a background knowledge base and a set of observable variables for each agent, we show that the notion of an agent knowing a formula can be defined as a weakest sufficient condition of the formula under background knowledge base. Moreover, we show how to capture the notion of common knowledge by using a generalized notion of weakest sufficient condition. Also, we show that public announcement operator can be conveniently dealt with via our notion of knowledge structure. Further, we explore the computational complexity of the problem whether an epistemic formula is realized in a knowledge structure. In the general case, this problem is PSPACE-hard; however, for some interesting subcases, it can be reduced to co-NP. Finally, we discuss possible applications of our framework in some interesting domains such as the automated analysis of the well-known muddy children puzzle and the verification of the revised Needham-Schroeder protocol. We believe that there are many scenarios where the natural presentation of the available information about knowledge is under the form of a knowledge structure. What makes it valuable compared with the corresponding multi-agent S5 Kripke structure is that it can be much more succinct.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3475",
        "title": "Prime Implicates and Prime Implicants: From Propositional to Modal Logic",
        "authors": [
            "Meghyn Bienvenu"
        ],
        "abstract": "Prime implicates and prime implicants have proven relevant to a number of areas of artificial intelligence, most notably abductive reasoning and knowledge compilation. The purpose of this paper is to examine how these notions might be appropriately extended from propositional logic to the modal logic K. We begin the paper by considering a number of potential definitions of clauses and terms for K. The different definitions are evaluated with respect to a set of syntactic, semantic, and complexity-theoretic properties characteristic of the propositional definition.  We then compare the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce. While there is no definition that perfectly generalizes the propositional notions, we show that there does exist one definition which satisfies many of the desirable properties of the propositional case. In the second half of the paper, we consider the computational properties of the selected definition. To this end, we provide sound and complete algorithms for generating and recognizing prime implicates, and we show the prime implicate recognition task to be PSPACE-complete. We also prove upper and lower bounds on the size and number of prime implicates. While the paper focuses on the logic K, all of our results hold equally well for multi-modal K and for concept expressions in the description logic ALC. \n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3476",
        "title": "The Complexity of Circumscription in DLs",
        "authors": [
            "Piero A. Bonatti",
            "Carsten Lutz",
            "Frank Wolter"
        ],
        "abstract": "As fragments of first-order logic, Description logics (DLs) do not provide nonmonotonic features such as defeasible inheritance and default rules. Since many applications would benefit from the availability of such features, several families of nonmonotonic DLs have been developed that are mostly based on default logic and autoepistemic logic. In this paper, we consider circumscription as an interesting alternative approach to nonmonotonic DLs that, in particular, supports defeasible inheritance in a natural way. We study DLs extended with circumscription under different language restrictions and under different constraints on the sets of minimized, fixed, and varying predicates, and pinpoint the exact computational complexity of reasoning for DLs ranging from ALC to ALCIO and ALCQO.  When the minimized and fixed predicates include only concept names but no role names, then reasoning is complete for NExpTime^NP.  It becomes complete for NP^NExpTime when the number of minimized and fixed predicates is bounded by a constant.  If roles can be minimized or fixed, then complexity ranges from NExpTime^NP to undecidability.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3478",
        "title": "Efficient Markov Network Structure Discovery Using Independence Tests",
        "authors": [
            "Facundo Bromberg",
            "Dimitris Margaritis",
            "Vasant Honavar"
        ],
        "abstract": "  We present two algorithms for learning the structure of a Markov network from data:  GSMN* and GSIMN.  Both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests.  Until very recently, algorithms for structure learning were based on maximum likelihood estimation, which has been proved to be NP-hard for Markov networks due to the difficulty of estimating the parameters of the network, needed for the computation of the data likelihood.  The independence-based approach does not require the computation of the likelihood, and thus both GSMN* and GSIMN can compute the structure efficiently (as shown in our experiments).  GSMN* is an adaptation of the Grow-Shrink algorithm of Margaritis and Thrun for learning the structure of Bayesian networks.  GSIMN extends GSMN* by additionally exploiting Pearls well-known properties of the conditional independence relation to infer novel independences from known ones, thus avoiding the performance of statistical tests to estimate them.  To accomplish this efficiently GSIMN uses the Triangle theorem, also introduced in this work, which is a simplified version of the set of Markov axioms.  Experimental comparisons on artificial and real-world data sets show GSIMN can yield significant savings with respect to GSMN*, while generating a Markov network with comparable or in some cases improved quality.  We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH, that produces all possible conditional independences resulting from repeatedly applying  Pearls theorems on the known conditional independence tests.   The results of this comparison show that GSIMN, by the sole use of the Triangle theorem, is nearly optimal in terms of the set of independences tests that it infers.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3482",
        "title": "Enhancing QA Systems with Complex Temporal Question Processing Capabilities",
        "authors": [
            "Estela Saquete",
            "Jose Luis Vicedo",
            "Patricio Mart\u00ednez-Barco",
            "Rafael Mu\u00f1oz",
            "Hector Llorens"
        ],
        "abstract": "This paper presents a multilayered architecture that enhances the capabilities of current QA systems and allows different types of complex questions or queries to be processed. The answers to these questions need to be gathered from factual information scattered throughout different documents. Specifically, we designed a specialized layer to process the different types of temporal questions. Complex temporal questions are first decomposed into simple questions, according to the temporal relations expressed in the original question. In the same way, the answers to the resulting simple questions are recomposed, fulfilling the temporal restrictions of the original complex question. A novel aspect of this approach resides in the decomposition which uses a minimal quantity of resources, with the final aim of obtaining a portable platform that is easily extensible to other languages. In this paper we also present a methodology for evaluation of the decomposition of the questions as well as the ability of the implemented temporal layer to perform at a multilingual level. The temporal layer was first performed for English, then evaluated and compared with: a) a general purpose QA system (F-measure 65.47% for QA plus English temporal layer vs. 38.01% for the general QA system), and b) a well-known QA system. Much better results were obtained for temporal questions with the multilayered system. This system was therefore extended to Spanish and very good results were again obtained in the evaluation (F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general QA system).\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3484",
        "title": "Modularity Aspects of Disjunctive Stable Models",
        "authors": [
            "Tomi Janhunen",
            "Emilia Oikarinen",
            "Hans Tompits",
            "Stefan Woltran"
        ],
        "abstract": "Practically all programming languages allow the programmer to split a program into several modules which brings along several advantages in software development. In this paper, we are interested in the area of answer-set programming where fully declarative and nonmonotonic languages are applied. In this context, obtaining a modular structure for programs is by no means straightforward since the output of an entire program cannot in general be composed from the output of its components. To better understand the effects of disjunctive information on modularity we restrict the scope of analysis to the case of disjunctive logic programs (DLPs) subject to stable-model semantics. We define the notion of a DLP-function, where a well-defined input/output interface is provided, and establish a novel module theorem which indicates the compositionality of stable-model semantics for DLP-functions. The module theorem extends the well-known splitting-set theorem and enables the decomposition of DLP-functions given their strongly connected components based on positive dependencies induced by rules. In this setting, it is also possible to split shared disjunctive rules among components using a generalized shifting technique. The concept of modular equivalence is introduced for the mutual comparison of DLP-functions using a generalization of a translation-based verification method.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3485",
        "title": "Hypertableau Reasoning for Description Logics",
        "authors": [
            "Boris Motik",
            "Rob Shearer",
            "Ian Horrocks"
        ],
        "abstract": "We present a novel reasoning calculus for the description logic SHOIQ^+---a knowledge representation formalism with applications in areas such as the Semantic Web. Unnecessary nondeterminism and the construction of large models are two primary sources of inefficiency in the tableau-based reasoning calculi used in state-of-the-art reasoners. In order to reduce nondeterminism, we base our calculus on hypertableau and hyperresolution calculi, which we extend with a blocking condition to ensure termination. In order to reduce the size of the constructed models, we introduce anywhere pairwise blocking. We also present an improved nominal introduction rule that ensures termination in the presence of nominals, inverse roles, and number restrictions---a combination of DL constructs that has proven notoriously difficult to handle. Our implementation shows significant performance improvements over state-of-the-art reasoners on several well-known ontologies.\n\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3487",
        "title": "The DL-Lite Family and Relations",
        "authors": [
            "Alessandro Artale",
            "Diego Calvanese",
            "Roman Kontchakov",
            "Michael Zakharyaschev"
        ],
        "abstract": "The recently introduced series of description logics under the common moniker DL-Lite has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference, on the one hand, and the ability to represent  conceptual modeling formalisms, on the other.  The main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original DL-Lite logics along five axes: by (i) adding the Boolean connectives and (ii) number restrictions to concept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry, asymmetry, reflexivity, irreflexivity and transitivity constraints, and (v) adopting or dropping  the unique same assumption.  We analyze the combined complexity of satisfiability for the resulting logics, as well as the data complexity of instance checking and answering positive existential queries.  Our approach is based on embedding DL-Lite logics in suitable fragments of the one-variable first-order logic, which provides useful insights into their properties and, in particular, computational behavior.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3531",
        "title": "Highly comparative feature-based time-series classification",
        "authors": [
            "Ben D. Fulcher",
            "Nick S. Jones"
        ],
        "abstract": "A highly comparative, feature-based approach to time series classification is introduced that uses an extensive database of algorithms to extract thousands of interpretable features from time series. These features are derived from across the scientific time-series analysis literature, and include summaries of time series in terms of their correlation structure, distribution, entropy, stationarity, scaling properties, and fits to a range of time-series models. After computing thousands of features for each time series in a training set, those that are most informative of the class structure are selected using greedy forward feature selection with a linear classifier. The resulting feature-based classifiers automatically learn the differences between classes using a reduced number of time-series properties, and circumvent the need to calculate distances between time series. Representing time series in this way results in orders of magnitude of dimensionality reduction, allowing the method to perform well on very large datasets containing long time series or time series of different lengths. For many of the datasets studied, classification performance exceeded that of conventional instance-based classifiers, including one nearest neighbor classifiers using Euclidean distances and dynamic time warping and, most importantly, the features selected provide an understanding of the properties of the dataset, insight that can guide further scientific investigation.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3579",
        "title": "A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An Actor-Critic Approach",
        "authors": [
            "Keyvan Yahya"
        ],
        "abstract": "This paper aims to find an algorithmic structure that affords to predict and explain economical choice behaviour particularly under uncertainty(random policies) by manipulating the prevalent Actor-Critic learning method to comply with the requirements we have been entrusted ever since the field of neuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics that seem relevant to our discussion, we will try to outline some of the important works which have so far been done to simulate choice making processes. Concerning neurological findings that suggest the existence of two specific functions that are executed through Basal Ganglia all the way up to sub- cortical areas, namely 'rewards' and 'beliefs', we will offer a modified version of actor/critic algorithm to shed a light on the relation between these functions and most importantly resolve what is referred to as a challenge for actor-critic algorithms, that is, the lack of inheritance or hierarchy which avoids the system being evolved in continuous time tasks whence the convergence might not be emerged.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2020-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3840",
        "title": "Grounding FO and FO(ID) with Bounds",
        "authors": [
            "Johan Wittocx",
            "Maarten Mari\u00ebn",
            "Marc Denecker"
        ],
        "abstract": "Grounding is the task of reducing a first-order theory and finite domain to an equivalent propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning. \nBesides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounders output can often be reduced significantly. A common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. In this paper we present a method to compute and add such redundant information automatically. Our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems.\nWe first present our method for classical first-order logic (FO) theories. Then we extend it to FO(ID), the extension of FO with inductive definitions, which allows for more concise and comprehensive input theories. We discuss implementation issues and experimentally validate the practical applicability of our method.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3843",
        "title": "Theta*: Any-Angle Path Planning on Grids",
        "authors": [
            "Kenny Daniel",
            "Alex Nash",
            "Sven Koenig",
            "Ariel Felner"
        ],
        "abstract": "  Grids with blocked and unblocked cells are often used to represent terrain in robotics and video games. However, paths formed by grid edges can be longer than true shortest paths in the terrain since their headings are artificially constrained. We present two new correct and complete any-angle path-planning algorithms that avoid this shortcoming.  Basic Theta* and Angle-Propagation Theta* are both variants of A* that propagate information along grid edges without constraining paths to grid edges. Basic Theta* is simple to understand and implement, fast and finds short paths. However, it is not guaranteed to find true shortest paths. Angle-Propagation Theta* achieves a better worst-case complexity per vertex expansion than Basic Theta* by propagating angle ranges when it expands vertices, but is more complex, not as fast and finds slightly longer paths. We refer to Basic Theta* and Angle-Propagation Theta* collectively as Theta*. Theta* has unique properties, which we analyze in detail. We show experimentally that it finds shorter paths than both A* with post-smoothed paths and Field D* (the only other version of A* we know of that propagates information along grid edges without constraining paths to grid edges) with a runtime comparable to that of A* on grids. Finally, we extend Theta* to grids that contain unblocked cells with non-uniform traversal costs and introduce variants of Theta* which provide different tradeoffs between path length and runtime.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3844",
        "title": "Multiattribute Auctions Based on Generalized Additive Independence",
        "authors": [
            "Yagil Engel",
            "Michael P. Wellman"
        ],
        "abstract": "We develop multiattribute auctions that accommodate generalized additive independent (GAI) preferences. We propose an iterative auction mechanism that maintains prices on potentially overlapping GAI clusters of attributes, thus decreases elicitation and computational burden, and creates an open competition among suppliers over a multidimensional domain. Most significantly, the auction is guaranteed to achieve surplus which approximates optimal welfare up to a small additive factor, under reasonable equilibrium strategies of traders. The main departure of GAI auctions from previous literature is to accommodate non-additive trader preferences, hence allowing traders to condition their evaluation of specific attributes on the value of other attributes. At the same time, the GAI structure supports a compact representation of prices, enabling a tractable auction process. We perform a simulation study, demonstrating and quantifying the significant efficiency advantage of more expressive preference modeling. We draw random GAI-structured utility functions with various internal structures, generate additive functions that approximate the GAI utility, and compare the performance of the auctions using the two representations. We find that allowing traders to express existing dependencies among attributes improves the economic efficiency of multiattribute auctions.\n\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3845",
        "title": "Resource-Driven Mission-Phasing Techniques for Constrained Agents in Stochastic Environments",
        "authors": [
            "Jianhui Wu",
            "Edmund H. Durfee"
        ],
        "abstract": "Because an agents resources dictate what actions it can possibly take, it should plan which resources it holds over time carefully, considering its inherent limitations (such as power or payload restrictions), the competing needs of other agents for the same resources, and the stochastic nature of the environment. Such agents can, in general, achieve more of their objectives if they can use --- and even create --- opportunities to change which resources they hold at various times.  Driven by resource constraints, the agents could break their overall missions into an optimal series of phases, optimally reconfiguring their resources at each phase, and optimally using their assigned resources in each phase, given their knowledge of the stochastic environment.\nIn this paper, we formally define and analyze this constrained, sequential optimization problem in both the single-agent and multi-agent contexts. We present a family of mixed integer linear programming (MILP) formulations of this problem that can optimally create phases (when phases are not predefined) accounting for costs and limitations in phase creation.  Because our formulations multaneously also find the optimal allocations of resources at each phase and the optimal policies for using the allocated resources at each phase, they exploit structure across these coupled problems. This allows them to find solutions significantly faster(orders of magnitude faster in larger problems) than alternative solution techniques, as we demonstrate empirically.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3849",
        "title": "Nominals, Inverses, Counting, and Conjunctive Queries or: Why Infinity is your Friend!",
        "authors": [
            "Sebastian Rudolph",
            "Birte Glimm"
        ],
        "abstract": "Description Logics are knowledge representation formalisms that provide, for example, the logical underpinning of the W3C OWL standards. Conjunctive queries, the standard query language in databases, have recently gained significant attention as an expressive formalism for querying Description Logic knowledge bases. Several different techniques for deciding conjunctive query entailment are available for a wide range of DLs. Nevertheless, the combination of nominals, inverse roles, and number restrictions in OWL 1 and OWL 2 DL causes unsolvable problems for the techniques hitherto available. We tackle this problem and present a decidability result for entailment of unions of conjunctive queries in the DL ALCHOIQb that contains all three problematic constructors simultaneously. Provided that queries contain only simple roles, our result also shows decidability of entailment of (unions of) conjunctive queries in the logic that underpins OWL 1 DL and we believe that the presented results will pave the way for further progress towards conjunctive query entailment decision procedures for the Description Logics underlying the OWL standards.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3855",
        "title": "Algorithms for Closed Under Rational Behavior (CURB) Sets",
        "authors": [
            "Michael Benisch",
            "George B. Davis",
            "Tuomas Sandholm"
        ],
        "abstract": "We provide a series of algorithms demonstrating that solutions according to the fundamental game-theoretic solution concept of closed under rational behavior (CURB) sets in two-player, normal-form games can be computed in polynomial time (we also discuss extensions to n-player games). First, we describe an algorithm that identifies all of a player's best responses conditioned on the belief that the other player will play from within a given subset of its strategy space. This algorithm serves as a subroutine in a series of polynomial-time algorithms for finding all minimal CURB sets, one minimal CURB set, and the smallest minimal CURB set in a game. We then show that the complexity of finding a Nash equilibrium can be exponential only in the size of a game's smallest CURB set. Related to this, we show that the smallest CURB set can be an arbitrarily small portion of the game, but it can also be arbitrarily larger than the supports of its only enclosed Nash equilibrium. We test our algorithms empirically and find that most commonly studied academic games tend to have either very large or very small minimal CURB sets.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3858",
        "title": "Logical Foundations of RDF(S) with Datatypes",
        "authors": [
            "Jos de Bruijn",
            "Stijn Heymans"
        ],
        "abstract": "The Resource Description Framework (RDF) is a Semantic Web standard that provides a data language, simply called RDF, as well as a lightweight ontology language, called RDF Schema. We investigate embeddings of RDF in logic and show how standard logic programming and description logic technology can be used for reasoning with RDF. We subsequently consider extensions of RDF with datatype support, considering D entailment, defined in the RDF semantics specification, and D* entailment, a semantic weakening of D entailment, introduced by ter Horst. We use the embeddings and properties of the logics to establish novel upper bounds for the complexity of deciding entailment. We subsequently establish two novel lower bounds, establishing that RDFS entailment is PTime-complete and that simple-D entailment is coNP-hard, when considering arbitrary datatypes, both in the size of the entailing graph. The results indicate that RDFS may not be as lightweight as one may expect.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3864",
        "title": "A Logical Study of Partial Entailment",
        "authors": [
            "Yi Zhou",
            "Yan Zhang"
        ],
        "abstract": "We introduce a novel logical notion--partial entailment--to propositional logic. In contrast with classical entailment, that a formula P partially entails another formula Q with respect to a background formula set \\Gamma intuitively means that under the circumstance of \\Gamma, if P is true then some \"part\" of Q will also be true. We distinguish three different kinds of partial entailments and formalize them by using an extended notion of prime implicant. We study their semantic properties, which show that, surprisingly, partial entailments fail for many simple inference rules. Then, we study the related computational properties, which indicate that partial entailments are relatively difficult to be computed. Finally, we consider a potential application of partial entailments in reasoning about rational agents.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3868",
        "title": "Clause-Learning Algorithms with Many Restarts and Bounded-Width Resolution",
        "authors": [
            "Albert Atserias",
            "Johannes Klaus Fichte",
            "Marc Thurley"
        ],
        "abstract": " We offer a new understanding of some aspects of practical SAT-solvers that are based on DPLL with unit-clause propagation, clause-learning, and restarts. We do so by analyzing a concrete algorithm which we claim is faithful to what practical solvers do. In particular, before making any new decision or restart, the solver repeatedly applies the unit-resolution rule until saturation, and leaves no component to the mercy of non-determinism except for some internal randomness. We prove the perhaps surprising fact that, although the solver is not explicitly designed for it, with high probability it ends up behaving as width-k resolution after no more than O(n^2k+2) conflicts and restarts, where n is the number of variables. In other words, width-k resolution can be thought of as O(n^2k+2) restarts of the unit-resolution rule with learning.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3870",
        "title": "Learning to Make Predictions In Partially Observable Environments Without a Generative Model",
        "authors": [
            "Erik Talvitie",
            "Satinder Singh"
        ],
        "abstract": "When faced with the problem of learning a model of a high-dimensional environment, a common approach is to limit the model to make only a restricted set of predictions, thereby simplifying the learning problem. These partial models may be directly useful for making decisions or may be combined together to form a more complete, structured model. However, in partially observable (non-Markov) environments, standard model-learning methods learn generative models, i.e. models that provide a probability distribution over all possible futures (such as POMDPs). It is not straightforward to restrict such models to make only certain predictions, and doing so does not always simplify the learning problem. In this paper we present prediction profile models: non-generative partial models for partially observable systems that make only a given set of predictions, and are therefore far simpler than generative models in some cases. We formalize the problem of learning a prediction profile model as a transformation of the original model-learning problem, and show empirically that one can learn prediction profile models that make a small set of important predictions even in systems that are too complex for standard generative models.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3877",
        "title": "Properties of Bethe Free Energies and Message Passing in Gaussian Models",
        "authors": [
            "Botond Cseke",
            "Tom Heskes"
        ],
        "abstract": "We address the problem of computing approximate marginals in Gaussian probabilistic models by using mean field and fractional Bethe approximations. We define the Gaussian fractional Bethe free energy in terms of the moment parameters of the approximate marginals, derive a lower and an upper bound on the fractional Bethe free energy and establish a necessary condition for the lower bound to be bounded from below. It turns out that the condition is identical to the pairwise normalizability condition, which is known to be a sufficient condition for the convergence of the message passing algorithm. We show that stable fixed points of the Gaussian message passing algorithm are local minima of the Gaussian Bethe free energy. By a counterexample, we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3878",
        "title": "Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories",
        "authors": [
            "Alessandro Cimatti",
            "Alberto Griggio",
            "Roberto Sebastiani"
        ],
        "abstract": "The problem of finding small unsatisfiable cores for SAT formulas has  recently received a lot of interest, mostly for its applications in formal verification.  However, propositional logic is often not expressive  enough for representing many interesting verification problems, which can be more naturally addressed in the framework of Satisfiability Modulo Theories, SMT.  Surprisingly, the problem of finding unsatisfiable cores in SMT has received very little attention in the literature.\nIn this paper we present a novel approach to this problem, called the Lemma-Lifting approach. The main idea is to combine an SMT solver with an external propositional core extractor. The SMT solver produces the theory lemmas found during the search, dynamically lifting the suitable amount of theory information to the Boolean level. The core extractor is then called on the Boolean abstraction of the original SMT problem and of the theory lemmas. This results in an unsatisfiable core for the original SMT problem, once the remaining theory lemmas are removed.\nThe approach is conceptually interesting, and has several advantages in practice. In fact, it is extremely simple to implement and to update, and it can be interfaced with every propositional core extractor in a plug-and-play manner, so as to benefit for free of all unsat-core reduction techniques which have been or will be made available.\nWe have evaluated our algorithm with a very extensive empirical test on SMT-LIB benchmarks, which confirms the validity and potential of this approach.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3894",
        "title": "Efficient Multi-Start Strategies for Local Search Algorithms",
        "authors": [
            "Andr\u00e1s Gy\u00f6rgy",
            "Levente Kocsis"
        ],
        "abstract": "Local search algorithms applied to optimization problems often suffer from getting trapped in a local optimum. The common solution for this deficiency is to restart the algorithm when no progress is observed. Alternatively, one can start multiple instances of a local search algorithm, and allocate computational resources (in particular, processing time) to the instances depending on their behavior. Hence, a multi-start strategy has to decide (dynamically) when to allocate additional resources to a particular instance and when to start new instances. In this paper we propose multi-start strategies motivated by works on multi-armed bandit problems and Lipschitz optimization with an unknown constant. The strategies continuously estimate the potential performance of each algorithm instance by supposing a convergence rate of the local search algorithm up to an unknown constant, and in every phase allocate resources to those instances that could converge to the optimum for a particular range of the constant. Asymptotic bounds are given on the performance of the strategies. In particular, we prove that at most a quadratic increase in the number of times the target function is evaluated is needed to achieve the performance of a local search algorithm started from the attraction region of the optimum. Experiments are provided using SPSA (Simultaneous Perturbation Stochastic Approximation) and k-means as local search algorithms, and the results indicate that the proposed strategies work well in practice, and, in all cases studied, need only logarithmically more evaluations of the target function as opposed to the theoretically suggested quadratic increase.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3897",
        "title": "Interpolable Formulas in Equilibrium Logic and Answer Set Programming",
        "authors": [
            "Dov Gabbay",
            "David Pearce",
            "Agust\u00edn Valverde"
        ],
        "abstract": "Interpolation is an important property of classical and many non-classical logics that has been shown to have interesting applications in computer science and AI. Here we study the Interpolation Property for the the non-monotonic system of equilibrium logic, establishing weaker or stronger forms of interpolation depending on the precise interpretation of the inference relation. These results also yield a form of interpolation for ground logic programs under the answer sets semantics. For disjunctive logic programs we also study the property of uniform interpolation that is closely related to the concept of variable forgetting. The first-order version of equilibrium logic has analogous Interpolation properties whenever the collection of equilibrium models is (first-order) definable. Since this is the case for so-called safe programs and theories, it applies to the usual situations that arise in practical answer set programming.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3898",
        "title": "First-Order Stable Model Semantics and First-Order Loop Formulas",
        "authors": [
            "Joohyung Lee",
            "Yunsong Meng"
        ],
        "abstract": "Lin and Zhaos theorem on loop formulas states that in the propositional case the stable model semantics of a logic program can be completely characterized by propositional loop formulas, but this result does not fully carry over to the first-order case. We investigate the precise relationship between the first-order stable model semantics and first-order loop formulas, and study conditions under which the former can be represented by the latter. In order to facilitate the comparison, we extend the definition of a first-order loop formula which was limited to a nondisjunctive program, to a disjunctive program and to an arbitrary first-order theory. Based on the studied relationship we extend the syntax of a logic program with explicit quantifiers, which allows us to do reasoning involving non-Herbrand stable models using first-order reasoners. Such programs can be viewed as a special class of first-order theories under the stable model semantics, which yields more succinct loop formulas than the general language due to their restricted syntax.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3900",
        "title": "Decidability and Undecidability Results for Propositional Schemata",
        "authors": [
            "Vincent Aravantinos",
            "Ricardo Caferra",
            "Nicolas Peltier"
        ],
        "abstract": "We define a logic of propositional formula schemata adding to the syntax of propositional logic indexed propositions and iterated connectives ranging over intervals parameterized by arithmetic variables.  The satisfiability problem is shown to be undecidable for this new logic, but we introduce a very general class of schemata, called bound-linear, for which this problem becomes decidable.  This result is obtained by reduction to a particular class of schemata called regular, for which we provide a sound and complete terminating proof procedure.  This schemata calculus allows one to capture proof patterns corresponding to a large class of problems specified in propositional logic. We also show that the satisfiability problem becomes again undecidable for slight extensions of this class, thus demonstrating that bound-linear schemata represent a good compromise between expressivity and decidability.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3901",
        "title": "Defeasible Inclusions in Low-Complexity DLs",
        "authors": [
            "Piero A. Bonatti",
            "Marco Faella",
            "Luigi Sauro"
        ],
        "abstract": "Some of the applications of OWL and RDF (e.g. biomedical knowledge representation and semantic policy formulation) call for extensions of these languages with nonmonotonic constructs such as inheritance with overriding.  Nonmonotonic description logics have been studied for many years, however no practical such knowledge representation languages exist, due to a combination of semantic difficulties and high computational complexity. Independently, low-complexity description logics such as DL-lite and EL have been introduced and incorporated in the OWL standard.  Therefore, it is interesting to see whether the syntactic restrictions characterizing DL-lite and EL bring computational benefits to their nonmonotonic versions, too. In this paper we extensively investigate the computational complexity of Circumscription when knowledge bases are formulated in DL-lite_R, EL, and fragments thereof.  We identify fragments whose complexity ranges from P to the second level of the polynomial hierarchy, as well as fragments whose complexity raises to PSPACE and beyond.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4082",
        "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
        "authors": [
            "Danilo Jimenez Rezende",
            "Shakir Mohamed",
            "Daan Wierstra"
        ],
        "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4593",
        "title": "Location-Based Reasoning about Complex Multi-Agent Behavior",
        "authors": [
            "Adam Sadilek",
            "Henry Kautz"
        ],
        "abstract": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the activities of interest. We, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities. We model the domain using Markov logic, a statistical-relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as a player capturing an enemy. Our unified model combines constraints imposed by the geometry of the game area, the motion model of the players, and by the rules and dynamics of the game in a probabilistically and logically sound fashion. We show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it. Further, we show that given a model of successfully performed multi-agent activities, along with a set of examples of failed attempts at the same activities, our system automatically learns an augmented model that is capable of recognizing success and failure, as well as goals of peoples actions with high accuracy. We compare our approach with other alternatives and show that our unified model, which takes into account not only relationships among individual players, but also relationships among activities over the entire length of a game, although more computationally costly, is significantly more accurate. Finally, we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4596",
        "title": "Unfounded Sets and Well-Founded Semantics of Answer Set Programs with Aggregates",
        "authors": [
            "Mario Alviano",
            "Francesco Calimeri",
            "Wolfgang Faber",
            "Nicola Leone",
            "Simona Perri"
        ],
        "abstract": "Logic programs with aggregates (LPA) are one of the major linguistic extensions to Logic Programming (LP). In this work, we propose a generalization of the notions of unfounded set and well-founded semantics for programs with monotone and antimonotone aggregates (LPAma programs). In particular, we present a new notion of unfounded set for LPAma programs, which is a sound generalization of the original definition for standard (aggregate-free) LP. On this basis, we define a well-founded operator for LPAma programs, the fixpoint of which is called well-founded model (or well-founded semantics) for LPAma programs. The most important properties of unfounded sets and the well-founded semantics for standard LP are retained by this generalization, notably existence and uniqueness of the well-founded model, together with a strong relationship to the answer set semantics for LPAma programs. We show that one of the D-well-founded semantics, defined by Pelov, Denecker, and Bruynooghe for a broader class of aggregates using approximating operators, coincides with the well-founded model as defined in this work on LPAma programs. We also discuss some complexity issues, most importantly we give a formal proof of tractable computation of the well-founded model for LPA programs. Moreover, we prove that for general LPA programs, which may contain aggregates that are neither monotone nor antimonotone, deciding satisfaction of aggregate expressions with respect to partial interpretations is coNP-complete. As a consequence, a well-founded semantics for general LPA programs that allows for tractable computation is unlikely to exist, which justifies the restriction on LPAma programs. Finally, we present a prototype system extending DLV, which supports the well-founded semantics for LPAma programs, at the time of writing the only implemented system that does so. Experiments with this prototype show significant computational advantages of aggregate constructs over equivalent aggregate-free encodings.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4599",
        "title": "Learning and Reasoning with Action-Related Places for Robust Mobile Manipulation",
        "authors": [
            "Freek Stulp",
            "Andreas Fedrizzi",
            "Lorenz M\u00f6senlechner",
            "Michael Beetz"
        ],
        "abstract": "We propose the concept of Action-Related Place (ARPlace) as a powerful and flexible representation of task-related place in the context of mobile manipulation. ARPlace represents robot base locations not as a single position, but rather as a collection of positions, each with an associated probability that the manipulation action will succeed when located there. ARPlaces are generated using a predictive model that is acquired through experience-based learning, and take into account the uncertainty the robot has about its own location and the location of the object to be manipulated.\nWhen executing the task, rather than choosing one specific goal position based only on the initial knowledge about the task context, the robot instantiates an ARPlace, and bases its decisions on this ARPlace, which is updated as new information about the task becomes available. To show the advantages of this least-commitment approach, we present a transformational planner that reasons about ARPlaces in order to optimize symbolic plans.  Our empirical evaluation demonstrates that using ARPlaces leads to more robust and efficient mobile manipulation in the face of state estimation uncertainty on our simulated robot.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4609",
        "title": "Computing All-Pairs Shortest Paths by Leveraging Low Treewidth",
        "authors": [
            "L\u00e9on R. Planken",
            "Mathijs M. de Weerdt",
            "Roman P.J. van der Krogt"
        ],
        "abstract": "We present two new and efficient algorithms for computing all-pairs shortest paths.  The algorithms operate on directed graphs with real (possibly negative) weights.  They make use of directed path consistency along a vertex ordering d. Both algorithms run in O(n^2 w_d) time, where w_d is the graph width induced by this vertex ordering.  For graphs of constant treewidth, this yields O(n^2) time, which is optimal.  On chordal graphs, the algorithms run in O(nm) time. In addition, we present a variant that exploits graph separators to arrive at a run time of  O(n w_d^2 + n^2 s_d) on general graphs, where s_d andlt= w_d is the size of the largest minimal separator induced by the vertex ordering d. We show empirically that on both constructed and realistic benchmarks, in many cases the algorithms outperform Floyd-Warshalls as well as Johnsons algorithm, which represent the current state of the art with a run time of O(n^3) and O(nm + n^2 log n), respectively. Our algorithms can be used for spatial and temporal reasoning, such as for the Simple Temporal Problem, which underlines their relevance to the planning and scheduling community.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4869",
        "title": "Does Syntactic Knowledge help English-Hindi SMT?",
        "authors": [
            "Taraka Rama",
            "Karthik Gali",
            "Avinesh PVS"
        ],
        "abstract": "In this paper we explore various parameter settings of the state-of-art Statistical Machine Translation system to improve the quality of the translation for a `distant' language pair like English-Hindi. We proposed new techniques for efficient reordering. A slight improvement over the baseline is reported using these techniques. We also show that a simple pre-processing step can improve the quality of the translation significantly.\n    ",
        "submission_date": "2014-01-20T00:00:00",
        "last_modified_date": "2014-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5054",
        "title": "An\u00e1lisis e implementaci\u00f3n de algoritmos evolutivos para la optimizaci\u00f3n de simulaciones en ingenier\u00eda civil. (draft)",
        "authors": [
            "Jos\u00e9 Alberto Garc\u00eda Guti\u00e9rrez",
            "Alejandro Mateo Hern\u00e1ndez D\u00edaz"
        ],
        "abstract": "This paper studies the applicability of evolutionary algorithms, particularly, the evolution strategies family in order to estimate a degradation parameter in the shear design of reinforced concrete members. This problem represents a great computational task and is highly relevant in the framework of the structural engineering that for the first time is solved using genetic algorithms.\n",
        "submission_date": "2014-01-20T00:00:00",
        "last_modified_date": "2014-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5327",
        "title": "Compositional Operators in Distributional Semantics",
        "authors": [
            "Dimitri Kartsaklis"
        ],
        "abstract": "This survey presents in some detail the main advances that have been recently taking place in Computational Linguistics towards the unification of the two prominent semantic paradigms: the compositional formal semantics view and the distributional models of meaning based on vector spaces. After an introduction to these two approaches, I review the most important models that aim to provide compositionality in distributional semantics. Then I proceed and present in more detail a particular framework by Coecke, Sadrzadeh and Clark (2010) based on the abstract mathematical setting of category theory, as a more complete example capable to demonstrate the diversity of techniques and scientific disciplines that this kind of research can draw from. This paper concludes with a discussion about important open issues that need to be addressed by the researchers in the future.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2014-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5390",
        "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework",
        "authors": [
            "S.R.K. Branavan",
            "David Silver",
            "Regina Barzilay"
        ],
        "abstract": "Domain knowledge is crucial for effective performance in autonomous control systems.  Typically, human effort is required to encode this knowledge into a control algorithm.  In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance.  Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application.  To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure.  This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space.  We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer.  Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games.  We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide.  Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5849",
        "title": "Interactions between Knowledge and Time in a First-Order Logic for Multi-Agent Systems: Completeness Results",
        "authors": [
            "Francesco Belardinelli",
            "Alessio Lomuscio"
        ],
        "abstract": "We investigate a class of first-order temporal-epistemic logics for reasoning about multi-agent systems. We encode typical properties of systems including perfect recall, synchronicity, no learning, and having a unique initial state in terms of variants of quantified interpreted systems, a first-order extension of interpreted systems. We identify several monodic fragments of first-order temporal-epistemic logic and show their completeness with respect to their corresponding classes of quantified interpreted systems.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5850",
        "title": "The Logical Difference for the Lightweight Description Logic EL",
        "authors": [
            "Boris Konev",
            "Michel Ludwig",
            "Dirk Walther",
            "Frank Wolter"
        ],
        "abstract": "We study a logic-based approach to versioning of ontologies. Under this view, ontologies provide answers to queries about some vocabulary of interest. The difference between two versions of an ontology is given by the set of queries that receive different answers.\nWe investigate this approach for terminologies given in the description logic EL extended with role inclusions and domain and range restrictions for three distinct types of queries: subsumption, instance, and conjunctive queries. In all three cases, we present polynomial-time algorithms that decide whether two terminologies give the same answers to queries over a given vocabulary and compute a succinct representation of the difference if it is non-\nempty. We present an implementation, CEX2, of the developed algorithms for subsumption and instance queries and apply it to distinct versions of Snomed CT and the NCI ontology.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5855",
        "title": "Tractable Triangles and Cross-Free Convexity in Discrete Optimisation",
        "authors": [
            "Martin C. Cooper",
            "Stanislav \u017divn\u00fd"
        ],
        "abstract": "The minimisation problem of a sum of unary and pairwise functions of discrete variables is a general NP-hard problem with wide applications such as computing MAP configurations in Markov Random Fields (MRF), minimising Gibbs energy, or solving binary Valued Constraint Satisfaction Problems (VCSPs).\n",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5980",
        "title": "Reasoning about Meaning in Natural Language with Compact Closed Categories and Frobenius Algebras",
        "authors": [
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh",
            "Stephen Pulman",
            "Bob Coecke"
        ],
        "abstract": "Compact closed categories have found applications in modeling quantum information protocols by Abramsky-Coecke. They also provide semantics for Lambek's pregroup algebras, applied to formalizing the grammatical structure of natural language, and are implicit in a distributional model of word meaning based on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh used the product category of pregroups with vector spaces and provided a distributional model of meaning for sentences. We recast this theory in terms of strongly monoidal functors and advance it via Frobenius algebras over vector spaces. The former are used to formalize topological quantum field theories by Atiyah and Baez-Dolan, and the latter are used to model classical data in quantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us to work in a single space in which meanings of words, phrases, and sentences of any structure live. Hence we can compare meanings of different language constructs and enhance the applicability of the theory. We report on experimental results on a number of language tasks and verify the theoretical predictions.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6226",
        "title": "Using Neural Network to Propose Solutions to Threats in Attack Patterns",
        "authors": [
            "Adetunji Adebiyi",
            "Chris Imafidon"
        ],
        "abstract": "In the last decade, a lot of effort has been put into securing software application during development in the software industry. Software security is a research field in this area which looks at how security can be weaved into software at each phase of software development lifecycle (SDLC). The use of attack patterns is one of the approaches that have been proposed for integrating security during the design phase of SDLC. While this approach help developers in identify security flaws in their software designs, the need to apply the proper security capability that will mitigate the threat identified is very important. To assist in this area, the uses of security patterns have been proposed to help developers to identify solutions to recurring security problems. However due to different types of security patterns and their taxonomy, software developers are faced with the challenge of finding and selecting appropriate security patterns that addresses the security risks in their design. In this paper, we propose a tool based on Neural Network for proposing solutions in form of security patterns to threats in attack patterns matching attacking patterns. From the result of performance of the neural network, we found out that the neural network was able to match attack patterns to security patterns that can mitigate the threat in the attack pattern. With this information developers are better informed in making decision on the solution for securing their application.\n    ",
        "submission_date": "2014-01-24T00:00:00",
        "last_modified_date": "2014-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6307",
        "title": "Hypergraph Acyclicity and Propositional Model Counting",
        "authors": [
            "Florent Capelli",
            "Arnaud Durand",
            "Stefan Mengel"
        ],
        "abstract": "We show that the propositional model counting problem #SAT for CNF- formulas with hypergraphs that allow a disjoint branches decomposition can be solved in polynomial time. We show that this class of hypergraphs is incomparable to hypergraphs of bounded incidence cliquewidth which were the biggest class of hypergraphs for which #SAT was known to be solvable in polynomial time so far. Furthermore, we present a polynomial time algorithm that computes a disjoint branches decomposition of a given hypergraph if it exists and rejects otherwise. Finally, we show that some slight extensions of the class of hypergraphs with disjoint branches decompositions lead to intractable #SAT, leaving open how to generalize the counting result of this paper.\n    ",
        "submission_date": "2014-01-24T00:00:00",
        "last_modified_date": "2014-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.8269",
        "title": "Experiments with Three Approaches to Recognizing Lexical Entailment",
        "authors": [
            "Peter D. Turney",
            "Saif M. Mohammad"
        ],
        "abstract": "Inference in natural language often involves recognizing lexical entailment (RLE); that is, identifying whether one word entails another. For example, \"buy\" entails \"own\". Two general strategies for RLE have been proposed: One strategy is to manually construct an asymmetric similarity measure for context vectors (directional similarity) and another is to treat RLE as a problem of learning to recognize semantic relations using supervised machine learning techniques (relation classification). In this paper, we experiment with two recent state-of-the-art representatives of the two general strategies. The first approach is an asymmetric similarity measure (an instance of the directional similarity strategy), designed to capture the degree to which the contexts of a word, a, form a subset of the contexts of another word, b. The second approach (an instance of the relation classification strategy) represents a word pair, a:b, with a feature vector that is the concatenation of the context vectors of a and b, and then applies supervised learning to a training set of labeled feature vectors. Additionally, we introduce a third approach that is a new instance of the relation classification strategy. The third approach represents a word pair, a:b, with a feature vector in which the features are the differences in the similarities of a and b to a set of reference words. All three approaches use vector space models (VSMs) of semantics, based on word-context matrices. We perform an extensive evaluation of the three approaches using three different datasets. The proposed new approach (similarity differences) performs significantly better than the other two approaches on some datasets and there is no dataset for which it is significantly worse. Our results suggest it is beneficial to make connections between the research in lexical entailment and the research in semantic relation classification.\n    ",
        "submission_date": "2014-01-31T00:00:00",
        "last_modified_date": "2014-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0052",
        "title": "Performance of the Survey Propagation-guided decimation algorithm for the random NAE-K-SAT problem",
        "authors": [
            "David Gamarnik",
            "Madhu Sudan"
        ],
        "abstract": "We show that the Survey Propagation-guided decimation algorithm fails to find satisfying assignments on random instances of the \"Not-All-Equal-$K$-SAT\" problem if the number of message passing iterations is bounded by a constant independent of the size of the instance and the clause-to-variable ratio is above $(1+o_K(1)){2^{K-1}\\over K}\\log^2 K$ for sufficiently large $K$. Our analysis in fact applies to a broad class of algorithms described as \"sequential local algorithms\". Such algorithms iteratively set variables based on some local information and then recurse on the reduced instance. Survey Propagation-guided as well as Belief Propagation-guided decimation algorithms - two widely studied message passing based algorithms, fall under this category of algorithms provided the number of message passing iterations is bounded by a constant. Another well-known algorithm falling into this category is the Unit Clause algorithm. Our work constitutes the first rigorous analysis of the performance of the SP-guided decimation algorithm.\n",
        "submission_date": "2014-02-01T00:00:00",
        "last_modified_date": "2014-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0402",
        "title": "Customizable Contraction Hierarchies",
        "authors": [
            "Julian Dibbelt",
            "Ben Strasser",
            "Dorothea Wagner"
        ],
        "abstract": "We consider the problem of quickly computing shortest paths in weighted graphs given auxiliary data derived in an expensive preprocessing phase. By adding a fast weight-customization phase, we extend Contraction Hierarchies by Geisberger et al to support the three-phase workflow introduced by Delling et al. Our Customizable Contraction Hierarchies use nested dissection orders as suggested by Bauer et al. We provide an in-depth experimental analysis on large road and game maps that clearly shows that Customizable Contraction Hierarchies are a very practicable solution in scenarios where edge weights often change.\n    ",
        "submission_date": "2014-02-03T00:00:00",
        "last_modified_date": "2015-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0560",
        "title": "Safe Exploration of State and Action Spaces in Reinforcement Learning",
        "authors": [
            "Javier Garcia",
            "Fernando Fernandez"
        ],
        "abstract": "In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0574",
        "title": "Learning to Predict from Textual Data",
        "authors": [
            "Kira Radinsky",
            "Sagie Davidovich",
            "Shaul Markovitch"
        ],
        "abstract": "Given a current news event, we tackle the problem of generating plausible predictions of future events it might cause.  We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor.  To obtain precisely labeled causality examples, we mine 150 years of news articles and apply semantic natural language modeling techniques to headlines containing certain predefined causality patterns.  For generalization, the model uses a vast number of world knowledge ontologies.  Empirical evaluation on real news articles shows that our Pundit algorithm performs as well as non-expert humans.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0576",
        "title": "Optimizing SPARQL Query Answering over OWL Ontologies",
        "authors": [
            "Ilianna Kollia",
            "Birte Glimm"
        ],
        "abstract": "The SPARQL query language is currently being extended by the World Wide Web Consortium (W3C) with so-called entailment regimes. An entailment regime defines how queries are evaluated under more expressive semantics than SPARQLs standard simple entailment, which is based on subgraph matching. The queries are very expressive since variables can occur within complex concepts and can also bind to concept or role names. In this paper, we describe a sound and complete algorithm for the OWL Direct Semantics entailment regime. We further propose several novel optimizations such as strategies for determining a good query execution order, query rewriting techniques, and show how specialized OWL reasoning tasks and the concept and role hierarchy can be used to reduce the query execution time. For determining a good execution order, we propose a cost-based model, where the costs are based on information about the instances of concepts and roles that are extracted from a model abstraction built by an OWL reasoner. We present two ordering strategies: a static and a dynamic one. For the dynamic case, we improve the performance by exploiting an individual clustering approach that allows for computing the cost functions based on one individual sample from a cluster. We provide a prototypical implementation and evaluate the efficiency of the proposed optimizations. Our experimental study shows that the static ordering usually outperforms the dynamic one when accurate statistics are available. This changes, however, when the statistics are less accurate, e.g., due to nondeterministic reasoning decisions. For queries that go beyond conjunctive instance queries we observe an improvement of up to three orders of magnitude due to the proposed optimizations.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0635",
        "title": "Generalization and Exploration via Randomized Value Functions",
        "authors": [
            "Ian Osband",
            "Benjamin Van Roy",
            "Zheng Wen"
        ],
        "abstract": "We propose randomized least-squares value iteration (RLSVI) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2016-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0672",
        "title": "User Friendly Line CAPTCHAs",
        "authors": [
            "A. K. B. Karunathilake",
            "B. M. D. Balasuriya",
            "R. G. Ragel"
        ],
        "abstract": "CAPTCHAs or reverse Turing tests are real-time assessments used by programs (or computers) to tell humans and machines apart. This is achieved by assigning and assessing hard AI problems that could only be solved easily by human but not by machines. Applications of such assessments range from stopping spammers from automatically filling online forms to preventing hackers from performing dictionary attack. Today, the race between makers and breakers of CAPTCHAs is at a juncture, where the CAPTCHAs proposed are not even answerable by humans. We consider such CAPTCHAs as non user friendly. In this paper, we propose a novel technique for reverse Turing test - we call it the Line CAPTCHAs - that mainly focuses on user friendliness while not compromising the security aspect that is expected to be provided by such a system.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1757",
        "title": "Frequency-Based Patrolling with Heterogeneous Agents and Limited Communication",
        "authors": [
            "Tao Mao",
            "Laura Ray"
        ],
        "abstract": "This paper investigates multi-agent frequencybased patrolling of intersecting, circle graphs under conditions where graph nodes have non-uniform visitation requirements and agents have limited ability to communicate. The task is modeled as a partially observable Markov decision process, and a reinforcement learning solution is developed. Each agent generates its own policy from Markov chains, and policies are exchanged only when agents occupy the same or adjacent nodes. This constraint on policy exchange models sparse communication conditions over large, unstructured environments. Empirical results provide perspectives on convergence properties, agent cooperation, and generalization of learned patrolling policies to new instances of the task. The emergent behavior indicates learned coordination strategies between heterogeneous agents for patrolling large, unstructured regions as well as the ability to generalize to dynamic variation in node visitation requirements.\n    ",
        "submission_date": "2014-02-07T00:00:00",
        "last_modified_date": "2014-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1921",
        "title": "A Hybrid Loss for Multiclass and Structured Prediction",
        "authors": [
            "Qinfeng Shi",
            "Mark Reid",
            "Tiberio Caetano",
            "Anton van den Hengel",
            "Zhenhua Wang"
        ],
        "abstract": "We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of a log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels--specifically, the gap between the probabilities of the best label and the second best label. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs. We demonstrate empirically that the hybrid loss typically performs least as well as--and often better than--both of its constituent losses on a variety of tasks, such as human action recognition. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2014-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2300",
        "title": "Feature and Variable Selection in Classification",
        "authors": [
            "Aaron Karper"
        ],
        "abstract": "The amount of information in the form of features and variables avail- able to machine learning algorithms is ever increasing. This can lead to classifiers that are prone to overfitting in high dimensions, high di- mensional models do not lend themselves to interpretable results, and the CPU and memory resources necessary to run on high-dimensional datasets severly limit the applications of the approaches. Variable and feature selection aim to remedy this by finding a subset of features that in some way captures the information provided best. In this paper we present the general methodology and highlight some specific approaches.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2359",
        "title": "Machine Learner for Automated Reasoning 0.4 and 0.5",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban",
            "Ji\u0159\u00ed Vysko\u010dil"
        ],
        "abstract": "Machine Learner for Automated Reasoning (MaLARea) is a learning and reasoning system for proving in large formal libraries where thousands of theorems are available when attacking a new conjecture, and a large number of related problems and proofs can be used to learn specific theorem-proving knowledge. The last version of the system has by a large margin won the 2013 CASC LTB competition. This paper describes the motivation behind the methods used in MaLARea, discusses the general approach and the issues arising in evaluation of such system, and describes the Mizar@Turing100 and CASC'24 versions of MaLARea.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2871",
        "title": "Planning for Decentralized Control of Multiple Robots Under Uncertainty",
        "authors": [
            "Christopher Amato",
            "George D. Konidaris",
            "Gabriel Cruz",
            "Christopher A. Maynor",
            "Jonathan P. How",
            "Leslie P. Kaelbling"
        ],
        "abstract": "We describe a probabilistic framework for synthesizing control policies for general multi-robot systems, given environment and sensor models and a cost function. Decentralized, partially observable Markov decision processes (Dec-POMDPs) are a general model of decision processes where a team of agents must cooperate to optimize some objective (specified by a shared reward or cost function) in the presence of uncertainty, but where communication limitations mean that the agents cannot share their state, so execution must proceed in a decentralized fashion. While Dec-POMDPs are typically intractable to solve for real-world problems, recent research on the use of macro-actions in Dec-POMDPs has significantly increased the size of problem that can be practically solved as a Dec-POMDP. We describe this general model, and show how, in contrast to most existing methods that are specialized to a particular problem class, it can synthesize control policies that use whatever opportunities for coordination are present in the problem, while balancing off uncertainty in outcomes, sensor information, and information about other agents. We use three variations on a warehouse task to show that a single planner of this type can generate cooperative behavior using task allocation, direct communication, and signaling, as appropriate.\n    ",
        "submission_date": "2014-02-12T00:00:00",
        "last_modified_date": "2014-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2959",
        "title": "Local Optima Networks: A New Model of Combinatorial Fitness Landscapes",
        "authors": [
            "Gabriela Ochoa",
            "S\u00e9bastien Verel",
            "Fabio Daolio",
            "Marco Tomassini"
        ],
        "abstract": "This chapter overviews a recently introduced network-based model of combinatorial landscapes: Local Optima Networks (LON). The model compresses the information given by the whole search space into a smaller mathematical object that is a graph having as vertices the local optima and as edges the possible weighted transitions between them. Two definitions of edges have been proposed: basin-transition and escape-edges, which capture relevant topological features of the underlying search spaces. This network model brings a new set of metrics to characterize the structure of combinatorial landscapes, those associated with the science of complex networks. These metrics are described, and results are presented of local optima network extraction and analysis for two selected combinatorial landscapes: NK landscapes and the quadratic assignment problem. Network features are found to correlate with and even predict the performance of heuristic search algorithms operating on these problems.\n    ",
        "submission_date": "2014-02-12T00:00:00",
        "last_modified_date": "2014-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3044",
        "title": "Finding a Collective Set of Items: From Proportional Multirepresentation to Group Recommendation",
        "authors": [
            "Piotr Skowron",
            "Piotr Faliszewski",
            "Jerome Lang"
        ],
        "abstract": "We consider the following problem: There is a set of items (e.g., movies) and a group of agents (e.g., passengers on a plane); each agent has some intrinsic utility for each of the items. Our goal is to pick a set of $K$ items that maximize the total derived utility of all the agents (i.e., in our example we are to pick $K$ movies that we put on the plane's entertainment system). However, the actual utility that an agent derives from a given item is only a fraction of its intrinsic one, and this fraction depends on how the agent ranks the item among the chosen, available, ones. We provide a formal specification of the model and provide concrete examples and settings where it is applicable. We show that the problem is hard in general, but we show a number of tractability results for its natural special cases.\n    ",
        "submission_date": "2014-02-13T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3096",
        "title": "Relations on FP-Soft Sets Applied to Decision Making Problems",
        "authors": [
            "Irfan Deli",
            "Naim \u00c7a\u011fman"
        ],
        "abstract": "In this work, we first define relations on the fuzzy parametrized soft sets and study their properties. We also give a decision making method based on these relations. In approximate reasoning, relations on the fuzzy parametrized soft sets have shown to be of a primordial importance. Finally, the method is successfully applied to a problems that contain uncertainties.\n    ",
        "submission_date": "2014-02-13T00:00:00",
        "last_modified_date": "2014-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4303",
        "title": "Finding Preference Profiles of Condorcet Dimension $k$ via SAT",
        "authors": [
            "Christian Geist"
        ],
        "abstract": "Condorcet winning sets are a set-valued generalization of the well-known concept of a Condorcet winner. As supersets of Condorcet winning sets are always Condorcet winning sets themselves, an interesting property of preference profiles is the size of the smallest Condorcet winning set they admit. This smallest size is called the Condorcet dimension of a preference profile. Since little is known about profiles that have a certain Condorcet dimension, we show in this paper how the problem of finding a preference profile that has a given Condorcet dimension can be encoded as a satisfiability problem and solved by a SAT solver. Initial results include a minimal example of a preference profile of Condorcet dimension 3, improving previously known examples both in terms of the number of agents as well as alternatives. Due to the high complexity of such problems it remains open whether a preference profile of Condorcet dimension 4 exists.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4306",
        "title": "Student-t Processes as Alternatives to Gaussian Processes",
        "authors": [
            "Amar Shah",
            "Andrew Gordon Wilson",
            "Zoubin Ghahramani"
        ],
        "abstract": "We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4455",
        "title": "Symbiosis of Search and Heuristics for Random 3-SAT",
        "authors": [
            "Sid Mijnders",
            "Boris de Wilde",
            "Marijn Heule"
        ],
        "abstract": "When combined properly, search techniques can reveal the full potential of sophisticated branching heuristics. We demonstrate this observation on the well-known class of random 3-SAT formulae. First, a new branching heuristic is presented, which generalizes existing work on this class. Much smaller search trees can be constructed by using this heuristic. Second, we introduce a variant of discrepancy search, called ALDS. Theoretical and practical evidence support that ALDS traverses the search tree in a near-optimal order when combined with the new heuristic. Both techniques, search and heuristic, have been implemented in the look-ahead solver march. The SAT 2009 competition results show that march is by far the strongest complete solver on random k-SAT formulae.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4465",
        "title": "Concurrent Cube-and-Conquer",
        "authors": [
            "Peter van der Tak",
            "Marijn J.H. Heule",
            "Armin Biere"
        ],
        "abstract": "Recent work introduced the cube-and-conquer technique to solve hard SAT instances. It partitions the search space into cubes using a lookahead solver. Each cube is tackled by a conflict-driven clause learning (CDCL) solver. Crucial for strong performance is the cutoff heuristic that decides when to switch from lookahead to CDCL. Yet, this offline heuristic is far from ideal. In this paper, we present a novel hybrid solver that applies the cube and conquer steps simultaneously. A lookahead and a CDCL solver work together on each cube, while communication is restricted to synchronization. Our concurrent cube-and-conquer solver can solve many instances faster than pure lookahead, pure CDCL and offline cube-and-conquer, and can abort early in favor of a pure CDCL search if an instance is not suitable for cube-and-conquer techniques.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4542",
        "title": "Unsupervised Ranking of Multi-Attribute Objects Based on Principal Curves",
        "authors": [
            "Chun-Guo Li",
            "Xing Mei",
            "Bao-Gang Hu"
        ],
        "abstract": "Unsupervised ranking faces one critical challenge in evaluation applications, that is, no ground truth is available. When PageRank and its variants show a good solution in related subjects, they are applicable only for ranking from link-structure data. In this work, we focus on unsupervised ranking from multi-attribute data which is also common in evaluation tasks. To overcome the challenge, we propose five essential meta-rules for the design and assessment of unsupervised ranking approaches: scale and translation invariance, strict monotonicity, linear/nonlinear capacities, smoothness, and explicitness of parameter size. These meta-rules are regarded as high level knowledge for unsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a ranking principal curve (RPC) model, which learns a one-dimensional manifold function to perform unsupervised ranking tasks on multi-attribute observations. Furthermore, the RPC is modeled to be a cubic B\u00e9zier curve with control points restricted in the interior of a hypercube, thereby complying with all the five meta-rules to infer a reasonable ranking list. With control points as the model parameters, one is able to understand the learned manifold and to interpret the ranking list semantically. Numerical experiments of the presented RPC model are conducted on two open datasets of different ranking applications. In comparison with the state-of-the-art approaches, the new model is able to show more reasonable ranking lists.\n    ",
        "submission_date": "2014-02-19T00:00:00",
        "last_modified_date": "2014-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4699",
        "title": "A Powerful Genetic Algorithm for Traveling Salesman Problem",
        "authors": [
            "Shujia Liu"
        ],
        "abstract": "This paper presents a powerful genetic algorithm(GA) to solve the traveling salesman problem (TSP). To construct a powerful GA, I use edge swapping(ES) with a local search procedure to determine good combinations of building blocks of parent solutions for generating even better offspring solutions. Experimental results on well studied TSP benchmarks demonstrate that the proposed GA is competitive in finding very high quality solutions on instances with up to 16,862 cities.\n    ",
        "submission_date": "2014-02-19T00:00:00",
        "last_modified_date": "2014-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4741",
        "title": "A normative account of defeasible and probabilistic inference",
        "authors": [
            "Julio Lemos"
        ],
        "abstract": "In this paper, we provide more evidence for the contention that logical consequence should be understood in normative terms. Hartry Field and John MacFarlane covered the classical case. We extend their work, examining what it means for an agent to be obliged to infer a conclusion when faced with uncertain information or reasoning within a non-monotonic, defeasible, logical framework (which allows e. g. for inference to be drawn from premises considered true unless evidence to the contrary is presented).\n    ",
        "submission_date": "2014-02-19T00:00:00",
        "last_modified_date": "2014-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4834",
        "title": "The Application of Imperialist Competitive Algorithm for Fuzzy Random Portfolio Selection Problem",
        "authors": [
            "Mir Ehsan Hesam Sadati",
            "Jamshid Bagherzadeh Mohasefi"
        ],
        "abstract": "This paper presents an implementation of the Imperialist Competitive Algorithm (ICA) for solving the fuzzy random portfolio selection problem where the asset returns are represented by fuzzy random variables. Portfolio Optimization is an important research field in modern finance. By using the necessity-based model, fuzzy random variables reformulate to the linear programming and ICA will be designed to find the optimum solution. To show the efficiency of the proposed method, a numerical example illustrates the whole idea on implementation of ICA for fuzzy random portfolio selection problem.\n    ",
        "submission_date": "2014-02-19T00:00:00",
        "last_modified_date": "2014-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5045",
        "title": "Expressing social attitudes in virtual agents for social training games",
        "authors": [
            "Nicolas Sabouret",
            "Haza\u00ebl Jones",
            "Magalie Ochs",
            "Mathieu Chollet",
            "Catherine Pelachaud"
        ],
        "abstract": "The use of virtual agents in social coaching has increased rapidly in the last decade. In order to train the user in different situations than can occur in real life, the virtual agent should be able to express different social attitudes. In this paper, we propose a model of social attitudes that enables a virtual agent to reason on the appropriate social attitude to express during the interaction with a user given the course of the interaction, but also the emotions, mood and personality of the agent. Moreover, the model enables the virtual agent to display its social attitude through its non-verbal behaviour. The proposed model has been developed in the context of job interview simulation. The methodology used to develop such a model combined a theoretical and an empirical approach. Indeed, the model is based both on the literature in Human and Social Sciences on social attitudes but also on the analysis of an audiovisual corpus of job interviews and on post-hoc interviews with the recruiters on their expressed attitudes during the job interview.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5205",
        "title": "A Survey on Dynamic Job Scheduling in Grid Environment Based on Heuristic Algorithms",
        "authors": [
            "D. Thilagavathi",
            "Antony Selvadoss Thanamani"
        ],
        "abstract": "Computational Grids are a new trend in distributed computing systems. They allow the sharing of geographically distributed resources in an efficient way, extending the boundaries of what we perceive as distributed computing. Various sciences can benefit from the use of grids to solve CPU-intensive problems, creating potential benefits to the entire society. Job scheduling is an integrated part of parallel and distributed computing. It allows selecting correct match of resource for a particular job and thus increases the job throughput and utilization of resources. Job should be scheduled in an automatic way to make the system more reliable, accessible and less sensitive to subsystem failures. This paper provides a survey on various heuristic algorithms, used for scheduling in grid.\n    ",
        "submission_date": "2014-02-21T00:00:00",
        "last_modified_date": "2014-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5830",
        "title": "A hybrid swarm-based algorithm for single-objective optimization problems involving high-cost analyses",
        "authors": [
            "Enrico Ampellio",
            "Luca Vassio"
        ],
        "abstract": "In many technical fields, single-objective optimization procedures in continuous domains involve expensive numerical simulations. In this context, an improvement of the Artificial Bee Colony (ABC) algorithm, called the Artificial super-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to provide fast convergence speed, high solution accuracy and robust performance over a wide range of problems. It implements enhancements of the ABC structure and hybridizations with interpolation strategies. The latter are inspired by the quadratic trust region approach for local investigation and by an efficient global optimizer for separable problems. Each modification and their combined effects are studied with appropriate metrics on a numerical benchmark, which is also used for comparing AsBeC with some effective ABC variants and other derivative-free algorithms. In addition, the presented algorithm is validated on two recent benchmarks adopted for competitions in international conferences. Results show remarkable competitiveness and robustness for AsBeC.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5878",
        "title": "Friend Inspector: A Serious Game to Enhance Privacy Awareness in Social Networks",
        "authors": [
            "Alexandra Cetto",
            "Michael Netter",
            "G\u00fcnther Pernul",
            "Christian Richthammer",
            "Moritz Riesner",
            "Christian Roth",
            "Johannes S\u00e4nger"
        ],
        "abstract": "Currently, many users of Social Network Sites are insufficiently aware of who can see their shared personal items. Nonetheless, most approaches focus on enhancing privacy in Social Networks through improved privacy settings, neglecting the fact that privacy awareness is a prerequisite for privacy control. Social Network users first need to know about privacy issues before being able to make adjustments. In this paper, we introduce Friend Inspector, a serious game that allows its users to playfully increase their privacy awareness on Facebook. Since its launch, Friend Inspector has attracted a significant number of visitors, emphasising the need for better tools to understand privacy settings on Social Networks.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5886",
        "title": "Near Optimal Bayesian Active Learning for Decision Making",
        "authors": [
            "Shervin Javdani",
            "Yuxin Chen",
            "Amin Karbasi",
            "Andreas Krause",
            "J. Andrew Bagnell",
            "Siddhartha Srinivasa"
        ],
        "abstract": "How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible.\n",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5988",
        "title": "Incremental Learning of Event Definitions with Inductive Logic Programming",
        "authors": [
            "Nikos Katzouris",
            "Alexander Artikis",
            "George Paliouras"
        ],
        "abstract": "Event recognition systems rely on properly engineered knowledge bases of event definitions to infer occurrences of events in time. The manual development of such knowledge is a tedious and error-prone task, thus event-based applications may benefit from automated knowledge construction techniques, such as Inductive Logic Programming (ILP), which combines machine learning with the declarative and formal semantics of First-Order Logic. However, learning temporal logical formalisms, which are typically utilized by logic-based Event Recognition systems is a challenging task, which most ILP systems cannot fully undertake. In addition, event-based data is usually massive and collected at different times and under various circumstances. Ideally, systems that learn from temporal data should be able to operate in an incremental mode, that is, revise prior constructed knowledge in the face of new evidence. Most ILP systems are batch learners, in the sense that in order to account for new evidence they have no alternative but to forget past knowledge and learn from scratch. Given the increased inherent complexity of ILP and the volumes of real-life temporal data, this results to algorithms that scale poorly. In this work we present an incremental method for learning and revising event-based knowledge, in the form of Event Calculus programs. The proposed algorithm relies on abductive-inductive learning and comprises a scalable clause refinement methodology, based on a compressive summarization of clause coverage in a stream of examples. We present an empirical evaluation of our approach on real and synthetic data from activity recognition and city transport applications.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5991",
        "title": "A predictive analytics approach to reducing avoidable hospital readmission",
        "authors": [
            "Issac Shams",
            "Saeede Ajorlou",
            "Kai Yang"
        ],
        "abstract": "Hospital readmission has become a critical metric of quality and cost of healthcare. Medicare anticipates that nearly $17 billion is paid out on the 20% of patients who are readmitted within 30 days of discharge. Although several interventions such as transition care management and discharge reengineering have been practiced in recent years, the effectiveness and sustainability depends on how well they can identify and target patients at high risk of rehospitalization. Based on the literature, most current risk prediction models fail to reach an acceptable accuracy level; none of them considers patient's history of readmission and impacts of patient attribute changes over time; and they often do not discriminate between planned and unnecessary readmissions. Tackling such drawbacks, we develop a new readmission metric based on administrative data that can identify potentially avoidable readmissions from all other types of readmission. We further propose a tree based classification method to estimate the predicted probability of readmission that can directly incorporate patient's history of readmission and risk factors changes over time. The proposed methods are validated with 2011-12 Veterans Health Administration data from inpatients hospitalized for heart failure, acute myocardial infarction, pneumonia, or chronic obstructive pulmonary disease in the State of Michigan. Results shows improved discrimination power compared to the literature (c-statistics>80%) and good calibration.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6077",
        "title": "Inductive Logic Boosting",
        "authors": [
            "Wang-Zhou Dai",
            "Zhi-Hua Zhou"
        ],
        "abstract": "Recent years have seen a surge of interest in Probabilistic Logic Programming (PLP) and Statistical Relational Learning (SRL) models that combine logic with probabilities. Structure learning of these systems is an intersection area of Inductive Logic Programming (ILP) and statistical learning (SL). However, ILP cannot deal with probabilities, SL cannot model relational hypothesis. The biggest challenge of integrating these two machine learning frameworks is how to estimate the probability of a logic clause only from the observation of grounded logic atoms. Many current methods models a joint probability by representing clause as graphical model and literals as vertices in it. This model is still too complicate and only can be approximate by pseudo-likelihood. We propose Inductive Logic Boosting framework to transform the relational dataset into a feature-based dataset, induces logic rules by boosting Problog Rule Trees and relaxes the independence constraint of pseudo-likelihood. Experimental evaluation on benchmark datasets demonstrates that the AUC-PR and AUC-ROC value of ILP learned rules are higher than current state-of-the-art SRL methods.\n    ",
        "submission_date": "2014-02-25T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6109",
        "title": "The Complexity of Repairing, Adjusting, and Aggregating of Extensions in Abstract Argumentation",
        "authors": [
            "Eun Jung Kim",
            "Sebastian Ordyniak",
            "Stefan Szeider"
        ],
        "abstract": "We study the computational complexity of problems that arise in abstract argumentation in the context of dynamic argumentation, minimal change, and aggregation. In particular, we consider the following problems where always an argumentation framework F and a small positive integer k are given.\n",
        "submission_date": "2014-02-25T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6208",
        "title": "The Anatomy of a Modular System for Media Content Analysis",
        "authors": [
            "Ilias Flaounas",
            "Thomas Lansdall-Welfare",
            "Panagiota Antonakaki",
            "Nello Cristianini"
        ],
        "abstract": "Intelligent systems for the annotation of media content are increasingly being used for the automation of parts of social science research. In this domain the problem of integrating various Artificial Intelligence (AI) algorithms into a single intelligent system arises spontaneously. As part of our ongoing effort in automating media content analysis for the social sciences, we have built a modular system by combining multiple AI modules into a flexible framework in which they can cooperate in complex tasks. Our system combines data gathering, machine translation, topic classification, extraction and annotation of entities and social networks, as well as many other tasks that have been perfected over the past years of AI research. Over the last few years, it has allowed us to realise a series of scientific studies over a vast range of applications including comparative studies between news outlets and media content in different countries, modelling of user preferences, and monitoring public mood. The framework is flexible and allows the design and implementation of modular agents, where simple modules cooperate in the annotation of a large dataset without central coordination.\n    ",
        "submission_date": "2014-02-25T00:00:00",
        "last_modified_date": "2018-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6485",
        "title": "Solving MaxSAT and #SAT on structured CNF formulas",
        "authors": [
            "Sigve Hortemo S\u00e6ther",
            "Jan Arne Telle",
            "Martin Vatshelle"
        ],
        "abstract": "In this paper we propose a structural parameter of CNF formulas and use it to identify instances of weighted MaxSAT and #SAT that can be solved in polynomial time. Given a CNF formula we say that a set of clauses is precisely satisfiable if there is some complete assignment satisfying these clauses only. Let the ps-value of the formula be the number of precisely satisfiable sets of clauses. Applying the notion of branch decompositions to CNF formulas and using ps-value as cut function, we define the ps-width of a formula. For a formula given with a decomposition of polynomial ps-width we show dynamic programming algorithms solving weighted MaxSAT and #SAT in polynomial time. Combining with results of 'Belmonte and Vatshelle, Graph classes with structured neighborhoods and algorithmic applications, Theor. Comput. Sci. 511: 54-65 (2013)' we get polynomial-time algorithms solving weighted MaxSAT and #SAT for some classes of structured CNF formulas. For example, we get $O(m^2(m + n)s)$ algorithms for formulas $F$ of $m$ clauses and $n$ variables and size $s$, if $F$ has a linear ordering of the variables and clauses such that for any variable $x$ occurring in clause $C$, if $x$ appears before $C$ then any variable between them also occurs in $C$, and if $C$ appears before $x$ then $x$ occurs also in any clause between them. Note that the class of incidence graphs of such formulas do not have bounded clique-width.\n    ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6556",
        "title": "Evolutionary solving of the debts' clearing problem",
        "authors": [
            "Csaba Patcas",
            "Attila Bartha"
        ],
        "abstract": "The debts' clearing problem is about clearing all the debts in a group of n entities (persons, companies etc.) using a minimal number of money transaction operations. The problem is known to be NP-hard in the strong sense. As for many intractable problems, techniques from the field of artificial intelligence are useful in finding solutions close to optimum for large inputs. An evolutionary algorithm for solving the debts' clearing problem is proposed.\n    ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6763",
        "title": "Linear Programming for Large-Scale Markov Decision Problems",
        "authors": [
            "Yasin Abbasi-Yadkori",
            "Peter L. Bartlett",
            "Alan Malek"
        ],
        "abstract": "We consider the problem of controlling a Markov decision process (MDP) with a large state space, so as to minimize average cost. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a low-dimensional family of policies. We use the dual linear programming formulation of the MDP average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a neighborhood of a low-dimensional subset of the set of stationary distributions (defined in terms of state-action features) as the comparison class. We propose two techniques, one based on stochastic convex optimization, and one based on constraint sampling. In both cases, we give bounds that show that the performance of our algorithms approaches the best achievable by any policy in the comparison class. Most importantly, these results depend on the size of the comparison class, but not on the size of the state space. Preliminary experiments show the effectiveness of the proposed algorithms in a queuing application.\n    ",
        "submission_date": "2014-02-27T00:00:00",
        "last_modified_date": "2014-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6785",
        "title": "Synthesis of Parametric Programs using Genetic Programming and Model Checking",
        "authors": [
            "Gal Katz",
            "Doron Peled"
        ],
        "abstract": "Formal methods apply algorithms based on mathematical principles to enhance the reliability of systems. It would only be natural to try to progress from verification, model checking or testing a system against its formal specification into constructing it automatically. Classical algorithmic synthesis theory provides interesting algorithms but also alarming high complexity and undecidability results. The use of genetic programming, in combination with model checking and testing, provides a powerful heuristic to synthesize programs. The method is not completely automatic, as it is fine tuned by a user that sets up the specification and parameters. It also does not guarantee to always succeed and converge towards a solution that satisfies all the required properties. However, we applied it successfully on quite nontrivial examples and managed to find solutions to hard programming challenges, as well as to improve and to correct code. We describe here several versions of our method for synthesizing sequential and concurrent systems.\n    ",
        "submission_date": "2014-02-27T00:00:00",
        "last_modified_date": "2014-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.7122",
        "title": "Nested Regular Path Queries in Description Logics",
        "authors": [
            "Meghyn Bienvenu",
            "Diego Calvanese",
            "Magdalena Ortiz",
            "Mantas Simkus"
        ],
        "abstract": "Two-way regular path queries (2RPQs) have received increased attention recently due to their ability to relate pairs of objects by flexibly navigating graph-structured data. They are present in property paths in SPARQL 1.1, the new standard RDF query language, and in the XML query language XPath. In line with XPath, we consider the extension of 2RPQs with nesting, which allows one to require that objects along a path satisfy complex conditions, in turn expressed through (nested) 2RPQs. We study the computational complexity of answering nested 2RPQs and conjunctions thereof (CN2RPQs) in the presence of domain knowledge expressed in description logics (DLs). We establish tight complexity bounds in data and combined complexity for a variety of DLs, ranging from lightweight DLs (DL-Lite, EL) up to highly expressive ones. Interestingly, we are able to show that adding nesting to (C)2RPQs does not affect worst-case data complexity of query answering for any of the considered DLs. However, in the case of lightweight DLs, adding nesting to 2RPQs leads to a surprising jump in combined complexity, from P-complete to Exp-complete.\n    ",
        "submission_date": "2014-02-28T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0222",
        "title": "Beyond Q-Resolution and Prenex Form: A Proof System for Quantified Constraint Satisfaction",
        "authors": [
            "Hubie Chen"
        ],
        "abstract": " We consider the quantified constraint satisfaction problem (QCSP) which is to decide, given a structure and a first-order sentence (not assumed here to be in prenex form) built from conjunction and quantification, whether or not the sentence is true on the structure. We present a proof system for certifying the falsity of QCSP instances and develop its basic theory; for instance, we provide an algorithmic interpretation of its behavior. Our proof system places the established Q-resolution proof system in a broader context, and also allows us to derive QCSP tractability results. \n    ",
        "submission_date": "2014-03-02T00:00:00",
        "last_modified_date": "2014-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0461",
        "title": "Timed Soft Concurrent Constraint Programs: An Interleaved and a Parallel Approach",
        "authors": [
            "Stefano Bistarelli",
            "Maurizio Gabbrielli",
            "Maria Chiara Meo",
            "Francesco Santini"
        ],
        "abstract": "We propose a timed and soft extension of Concurrent Constraint Programming. The time extension is based on the hypothesis of bounded asynchrony: the computation takes a bounded period of time and is measured by a discrete global clock. Action prefixing is then considered as the syntactic marker which distinguishes a time instant from the next one. Supported by soft constraints instead of crisp ones, tell and ask agents are now equipped with a preference (or consistency) threshold which is used to determine their success or suspension. In the paper we provide a language to describe the agents behavior, together with its operational and denotational semantics, for which we also prove the compositionality and correctness properties. After presenting a semantics using maximal parallelism of actions, we also describe a version for their interleaving on a single processor (with maximal parallelism for time elapsing). Coordinating agents that need to take decisions both on preference values and time events may benefit from this language. To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1353",
        "title": "Collaborative Representation for Classification, Sparse or Non-sparse?",
        "authors": [
            "Yang Wu",
            "Vansteenberge Jarich",
            "Masayuki Mukunoki",
            "Michihiko Minoh"
        ],
        "abstract": "Sparse representation based classification (SRC) has been proved to be a simple, effective and robust solution to face recognition. As it gets popular, doubts on the necessity of enforcing sparsity starts coming up, and primary experimental results showed that simply changing the $l_1$-norm based regularization to the computationally much more efficient $l_2$-norm based non-sparse version would lead to a similar or even better performance. However, that's not always the case. Given a new classification task, it's still unclear which regularization strategy (i.e., making the coefficients sparse or non-sparse) is a better choice without trying both for comparison. In this paper, we present as far as we know the first study on solving this issue, based on plenty of diverse classification experiments. We propose a scoring function for pre-selecting the regularization strategy using only the dataset size, the feature dimensionality and a discrimination score derived from a given feature representation. Moreover, we show that when dictionary learning is taking into account, non-sparse representation has a more significant superiority to sparse representation. This work is expected to enrich our understanding of sparse/non-sparse collaborative representation for classification and motivate further research activities.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1523",
        "title": "A Novel Method for Comparative Analysis of DNA Sequences by Ramanujan-Fourier Transform",
        "authors": [
            "Changchuan Yin",
            "Xuemeng E. Yin",
            "Jiasong Wang"
        ],
        "abstract": "Alignment-free sequence analysis approaches provide important alternatives over multiple sequence alignment (MSA) in biological sequence analysis because alignment-free approaches have low computation complexity and are not dependent on high level of sequence identity, however, most of the existing alignment-free methods do not employ true full information content of sequences and thus can not accurately reveal similarities and differences among DNA sequences. We present a novel alignment-free computational method for sequence analysis based on Ramanujan-Fourier transform (RFT), in which complete information of DNA sequences is retained. We represent DNA sequences as four binary indicator sequences and apply RFT on the indicator sequences to convert them into frequency domain. The Euclidean distance of the complete RFT coefficients of DNA sequences are used as similarity measure. To address the different lengths in Euclidean space of RFT coefficients, we pad zeros to short DNA binary sequences so that the binary sequences equal the longest length in the comparison sequence data. Thus, the DNA sequences are compared in the same dimensional frequency space without information loss. We demonstrate the usefulness of the proposed method by presenting experimental results on hierarchical clustering of genes and genomes. The proposed method opens a new channel to biological sequence analysis, classification, and structural module identification.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1891",
        "title": "Counterfactual Estimation and Optimization of Click Metrics for Search Engines",
        "authors": [
            "Lihong Li",
            "Shunbao Chen",
            "Jim Kleban",
            "Ankur Gupta"
        ],
        "abstract": "Optimizing an interactive system against a predefined online metric is particularly challenging, when the metric is computed from user feedback such as clicks and payments. The key challenge is the counterfactual nature: in the case of Web search, any change to a component of the search engine may result in a different search result page for the same query, but we normally cannot infer reliably from search log how users would react to the new result page. Consequently, it appears impossible to accurately estimate online metrics that depend on user feedback, unless the new engine is run to serve users and compared with a baseline in an A/B test. This approach, while valid and successful, is unfortunately expensive and time-consuming. In this paper, we propose to address this problem using causal inference techniques, under the contextual-bandit framework. This approach effectively allows one to run (potentially infinitely) many A/B tests offline from search log, making it possible to estimate and optimize online metrics quickly and inexpensively. Focusing on an important component in a commercial search engine, we show how these ideas can be instantiated and applied, and obtain very promising results that suggest the wide applicability of these techniques.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1893",
        "title": "Becoming More Robust to Label Noise with Classifier Diversity",
        "authors": [
            "Michael R. Smith",
            "Tony Martinez"
        ],
        "abstract": "It is widely known in the machine learning community that class noise can be (and often is) detrimental to inducing a model of the data. Many current approaches use a single, often biased, measurement to determine if an instance is noisy. A biased measure may work well on certain data sets, but it can also be less effective on a broader set of data sets. In this paper, we present noise identification using classifier diversity (NICD) -- a method for deriving a less biased noise measurement and integrating it into the learning process. To lessen the bias of the noise measure, NICD selects a diverse set of classifiers (based on their predictions of novel instances) to determine which instances are noisy. We examine NICD as a technique for filtering, instance weighting, and selecting the base classifiers of a voting ensemble. We compare NICD with several other noise handling techniques that do not consider classifier diversity on a set of 54 data sets and 5 learning algorithms. NICD significantly increases the classification accuracy over the other considered approaches and is effective across a broad set of data sets and learning algorithms.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2150",
        "title": "Constraint-based Causal Discovery from Multiple Interventions over Overlapping Variable Sets",
        "authors": [
            "Sofia Triantafillou",
            "Ioannis Tsamardinos"
        ],
        "abstract": "Scientific practice typically involves repeatedly studying a system, each time trying to unravel a different perspective. In each study, the scientist may take measurements under different experimental conditions (interventions, manipulations, perturbations) and measure different sets of quantities (variables). The result is a collection of heterogeneous data sets coming from different data distributions. In this work, we present algorithm COmbINE, which accepts a collection of data sets over overlapping variable sets under different experimental conditions; COmbINE then outputs a summary of all causal models indicating the invariant and variant structural characteristics of all models that simultaneously fit all of the input data sets. COmbINE converts estimated dependencies and independencies in the data into path constraints on the data-generating causal model and encodes them as a SAT instance. The algorithm is sound and complete in the sample limit. To account for conflicting constraints arising from statistical errors, we introduce a general method for sorting constraints in order of confidence, computed as a function of their corresponding p-values. In our empirical evaluation, COmbINE outperforms in terms of efficiency the only pre-existing similar algorithm; the latter additionally admits feedback cycles, but does not admit conflicting constraints which hinders the applicability on real data. As a proof-of-concept, COmbINE is employed to co-analyze 4 real, mass-cytometry data sets measuring phosphorylated protein concentrations of overlapping protein sets under 3 different interventions.\n    ",
        "submission_date": "2014-03-10T00:00:00",
        "last_modified_date": "2014-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2194",
        "title": "Querying Geometric Figures Using a Controlled Language, Ontological Graphs and Dependency Lattices",
        "authors": [
            "Yannis Haralambous",
            "Pedro Quaresma"
        ],
        "abstract": "Dynamic geometry systems (DGS) have become basic tools in many areas of geometry as, for example, in education. Geometry Automated Theorem Provers (GATP) are an active area of research and are considered as being basic tools in future enhanced educational software as well as in a next generation of mechanized mathematics assistants. Recently emerged Web repositories of geometric knowledge, like TGTP and Intergeo, are an attempt to make the already vast data set of geometric knowledge widely available. Considering the large amount of geometric information already available, we face the need of a query mechanism for descriptions of geometric constructions.\n",
        "submission_date": "2014-03-10T00:00:00",
        "last_modified_date": "2014-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4023",
        "title": "Simulation leagues: Analysis of competition formats",
        "authors": [
            "David Budden",
            "Peter Wang",
            "Oliver Obst",
            "Mikhail Prokopenko"
        ],
        "abstract": "The selection of an appropriate competition format is critical for both the success and credibility of any competition, both real and simulated. In this paper, the automated parallelism offered by the RoboCupSoccer 2D simulation league is leveraged to conduct a 28,000 game round-robin between the top 8 teams from RoboCup 2012 and 2013. A proposed new competition format is found to reduce variation from the resultant statistically significant team performance rankings by 75% and 67%, when compared to the actual competition results from RoboCup 2012 and 2013 respectively. These results are statistically validated by generating 10,000 random tournaments for each of the three considered formats and comparing the respective distributions of ranking discrepancy.\n    ",
        "submission_date": "2014-03-17T00:00:00",
        "last_modified_date": "2014-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4682",
        "title": "Structured Sparse Method for Hyperspectral Unmixing",
        "authors": [
            "Feiyun Zhu",
            "Ying Wang",
            "Shiming Xiang",
            "Bin Fan",
            "Chunhong Pan"
        ],
        "abstract": "Hyperspectral Unmixing (HU) has received increasing attention in the past decades due to its ability of unveiling information latent in hyperspectral data. Unfortunately, most existing methods fail to take advantage of the spatial information in data. To overcome this limitation, we propose a Structured Sparse regularized Nonnegative Matrix Factorization (SS-NMF) method from the following two aspects. First, we incorporate a graph Laplacian to encode the manifold structures embedded in the hyperspectral data space. In this way, the highly similar neighboring pixels can be grouped together. Second, the lasso penalty is employed in SS-NMF for the fact that pixels in the same manifold structure are sparsely mixed by a common set of relevant bases. These two factors act as a new structured sparse constraint. With this constraint, our method can learn a compact space, where highly similar pixels are grouped to share correlated sparse representations. Experiments on real hyperspectral data sets with different noise levels demonstrate that our method outperforms the state-of-the-art methods significantly.\n    ",
        "submission_date": "2014-03-19T00:00:00",
        "last_modified_date": "2014-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5029",
        "title": "Network-based Isoform Quantification with RNA-Seq Data for Cancer Transcriptome Analysis",
        "authors": [
            "Wei Zhang",
            "Jae-Woong Chang",
            "Lilong Lin",
            "Kay Minn",
            "Baolin Wu",
            "Jeremy Chien",
            "Jeongsik Yong",
            "Hui Zheng",
            "Rui Kuang"
        ],
        "abstract": "High-throughput mRNA sequencing (RNA-Seq) is widely used for transcript quantification of gene isoforms. Since RNA-Seq data alone is often not sufficient to accurately identify the read origins from the isoforms for quantification, we propose to explore protein domain-domain interactions as prior knowledge for integrative analysis with RNA-seq data. We introduce a Network-based method for RNA-Seq-based Transcript Quantification (Net-RSTQ) to integrate protein domain-domain interaction network with short read alignments for transcript abundance estimation. Based on our observation that the abundances of the neighboring isoforms by domain-domain interactions in the network are positively correlated, Net-RSTQ models the expression of the neighboring transcripts as Dirichlet priors on the likelihood of the observed read alignments against the transcripts in one gene. The transcript abundances of all the genes are then jointly estimated with alternating optimization of multiple EM problems. In simulation Net-RSTQ effectively improved isoform transcript quantifications when isoform co-expressions correlate with their interactions. qRT-PCR results on 25 multi-isoform genes in a stem cell line, an ovarian cancer cell line, and a breast cancer cell line also showed that Net-RSTQ estimated more consistent isoform proportions with RNA-Seq data. In the experiments on the RNA-Seq data in The Cancer Genome Atlas (TCGA), the transcript abundances estimated by Net-RSTQ are more informative for patient sample classification of ovarian cancer, breast cancer and lung cancer. All experimental results collectively support that Net-RSTQ is a promising approach for isoform quantification.\n    ",
        "submission_date": "2014-03-20T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5045",
        "title": "Matroid Bandits: Fast Combinatorial Optimization with Learning",
        "authors": [
            "Branislav Kveton",
            "Zheng Wen",
            "Azin Ashkan",
            "Hoda Eydgahi",
            "Brian Eriksson"
        ],
        "abstract": "A matroid is a notion of independence in combinatorial optimization which is closely related to computational efficiency. In particular, it is well known that the maximum of a constrained modular function can be found greedily if and only if the constraints are associated with a matroid. In this paper, we bring together the ideas of bandits and matroids, and propose a new class of combinatorial bandits, matroid bandits. The objective in these problems is to learn how to maximize a modular function on a matroid. This function is stochastic and initially unknown. We propose a practical algorithm for solving our problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds, gap-dependent and gap-free, on its regret. Both bounds are sublinear in time and at most linear in all other quantities of interest. The gap-dependent upper bound is tight and we prove a matching lower bound on a partition matroid bandit. Finally, we evaluate our method on three real-world problems and show that it is practical.\n    ",
        "submission_date": "2014-03-20T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6508",
        "title": "Multi-agent Inverse Reinforcement Learning for Two-person Zero-sum Games",
        "authors": [
            "Xiaomin Lin",
            "Peter A. Beling",
            "Randy Cogill"
        ],
        "abstract": "The focus of this paper is a Bayesian framework for solving a class of problems termed multi-agent inverse reinforcement learning (MIRL). Compared to the well-known inverse reinforcement learning (IRL) problem, MIRL is formalized in the context of stochastic games, which generalize Markov decision processes to game theoretic scenarios. We establish a theoretical foundation for competitive two-agent zero-sum MIRL problems and propose a Bayesian solution approach in which the generative model is based on an assumption that the two agents follow a minimax bi-policy. Numerical results are presented comparing the Bayesian MIRL method with two existing methods in the context of an abstract soccer game. Investigation centers on relationships between the extent of prior information and the quality of learned rewards. Results suggest that covariance structure is more important than mean value in reward priors.\n    ",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2019-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6512",
        "title": "Non-characterizability of belief revision: an application of finite model theory",
        "authors": [
            "Gyorgy Turan",
            "Jon Yaggie"
        ],
        "abstract": "A formal framework is given for the characterizability of a class of belief revision operators, defined using minimization over a class of partial preorders, by postulates. It is shown that for partial orders characterizability implies a definability property of the class of partial orders in monadic second-order logic. Based on a non-definability result for a class of partial orders, an example is given of a non-characterizable class of revision operators. This appears to be the first non-characterizability result in belief revision.\n    ",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2014-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7308",
        "title": "Data Generators for Learning Systems Based on RBF Networks",
        "authors": [
            "Marko Robnik-\u0160ikonja"
        ],
        "abstract": "There are plenty of problems where the data available is scarce and expensive. We propose a generator of semi-artificial data with similar properties to the original data which enables development and testing of different data mining algorithms and optimization of their parameters. The generated data allow a large scale experimentation and simulations without danger of overfitting. The proposed generator is based on RBF networks, which learn sets of Gaussian kernels. These Gaussian kernels can be used in a generative mode to generate new data from the same distributions. To assess quality of the generated data we evaluated the statistical properties of the generated data, structural similarity and predictive similarity using supervised and unsupervised learning techniques. To determine usability of the proposed generator we conducted a large scale evaluation using 51 UCI data sets. The results show a considerable similarity between the original and generated data and indicate that the method can be useful in several development and simulation scenarios. We analyze possible improvements in classification performance by adding different amounts of generated data to the training set, performance on high dimensional data sets, and conditions when the proposed approach is successful.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2020-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.8118",
        "title": "E-Generalization Using Grammars",
        "authors": [
            "Jochen Burghardt"
        ],
        "abstract": "We extend the notion of anti-unification to cover equational theories and present a method based on regular tree grammars to compute a finite representation of E-generalization sets. We present a framework to combine Inductive Logic Programming and E-generalization that includes an extension of Plotkin's lgg theorem to the equational case. We demonstrate the potential power of E-generalization by three example applications: computation of suggestions for auxiliary lemmas in equational inductive proofs, computation of construction laws for given term sequences, and learning of screen editor command sequences.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0837",
        "title": "Reasoning about Knowledge and Strategies: Epistemic Strategy Logic",
        "authors": [
            "Francesco Belardinelli"
        ],
        "abstract": "In this paper we introduce Epistemic Strategy Logic (ESL), an extension of Strategy Logic with modal operators for individual knowledge. This enhanced framework allows us to represent explicitly and to reason about the knowledge agents have of their own and other agents' strategies. We provide a semantics to ESL in terms of epistemic concurrent game models, and consider the corresponding model checking problem. We show that the complexity of model checking ESL is not worse than (non-epistemic) Strategy Logic\n    ",
        "submission_date": "2014-04-03T00:00:00",
        "last_modified_date": "2014-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0841",
        "title": "A Resolution Prover for Coalition Logic",
        "authors": [
            "Cl\u00e1udia Nalon",
            "Lan Zhang",
            "Clare Dixon",
            "Ullrich Hustadt"
        ],
        "abstract": "We present a prototype tool for automated reasoning for Coalition Logic, a non-normal modal logic that can be used for reasoning about cooperative agency. The theorem prover CLProver is based on recent work on a resolution-based calculus for Coalition Logic that operates on coalition problems, a normal form for Coalition Logic. We provide an overview of coalition problems and of the resolution-based calculus for Coalition Logic. We then give details of the implementation of CLProver and present the results for a comparison with an existing tableau-based solver.\n    ",
        "submission_date": "2014-04-03T00:00:00",
        "last_modified_date": "2014-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0854",
        "title": "Enabling Automatic Certification of Online Auctions",
        "authors": [
            "Wei Bai",
            "Emmanuel M. Tadjouddine",
            "Yu Guo"
        ],
        "abstract": "We consider the problem of building up trust in a network of online auctions by software agents. This requires agents to have a deeper understanding of auction mechanisms and be able to verify desirable properties of a given mechanism. We have shown how these mechanisms can be formalised as semantic web services in OWL-S, a good enough expressive machine-readable formalism enabling software agents, to discover, invoke, and execute a web service. We have also used abstract interpretation to translate the auction's specifications from OWL-S, based on description logic, to COQ, based on typed lambda calculus, in order to enable automatic verification of desirable properties of the auction by the software agents. For this language translation, we have discussed the syntactic transformation as well as the semantics connections between both concrete and abstract domains.  This work contributes to the implementation of the vision of agent-mediated e-commerce systems.\n    ",
        "submission_date": "2014-04-03T00:00:00",
        "last_modified_date": "2014-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0904",
        "title": "On a correlational clustering of integers",
        "authors": [
            "L. Aszal\u00f3s",
            "L. Hajdu",
            "A. Peth\u0151"
        ],
        "abstract": "Correlation clustering is a concept of machine learning. The ultimate goal of such a clustering is to find a partition with minimal conflicts. In this paper we investigate a correlation clustering of integers, based upon the greatest common divisor.\n    ",
        "submission_date": "2014-04-03T00:00:00",
        "last_modified_date": "2014-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0953",
        "title": "Implementing Anti-Unification Modulo Equational Theory",
        "authors": [
            "Jochen Burghardt",
            "Birgit Heinz"
        ],
        "abstract": "We present an implementation of E-anti-unification as defined in Heinz (1995), where tree-grammar descriptions of equivalence classes of terms are used to compute generalizations modulo equational theories. We discuss several improvements, including an efficient implementation of variable-restricted E-anti-unification from Heinz (1995), and give some runtime figures about them. We present applications in various areas, including lemma generation in equational inductive proofs, intelligence tests, diverging Knuth-Bendix completion, strengthening of induction hypotheses, and theory formation about finite algebras.\n    ",
        "submission_date": "2014-04-01T00:00:00",
        "last_modified_date": "2014-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2458",
        "title": "r-Extreme Signalling for Congestion Control",
        "authors": [
            "Jakub Marecek",
            "Robert Shorten",
            "Jia Yuan Yu"
        ],
        "abstract": "In many \"smart city\" applications, congestion arises in part due to the nature of signals received by individuals from a central authority. In the model of Marecek et al. [",
        "submission_date": "2014-04-09T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2644",
        "title": "A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse Learning",
        "authors": [
            "Aur\u00e9lien Bellet",
            "Yingyu Liang",
            "Alireza Bagheri Garakani",
            "Maria-Florina Balcan",
            "Fei Sha"
        ],
        "abstract": "Learning sparse combinations is a frequent theme in machine learning. In this paper, we study its associated optimization problem in the distributed setting where the elements to be combined are not centrally located but spread over a network. We address the key challenges of balancing communication costs and optimization errors. To this end, we propose a distributed Frank-Wolfe (dFW) algorithm. We obtain theoretical guarantees on the optimization error $\\epsilon$ and communication cost that do not depend on the total number of combining elements. We further show that the communication cost of dFW is optimal by deriving a lower-bound on the communication cost required to construct an $\\epsilon$-approximate solution. We validate our theoretical analysis with empirical studies on synthetic and real-world data, which demonstrate that dFW outperforms both baselines and competing methods. We also study the performance of dFW when the conditions of our analysis are relaxed, and show that dFW is fairly robust.\n    ",
        "submission_date": "2014-04-09T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3708",
        "title": "Inferring Social Status and Rich Club Effects in Enterprise Communication Networks",
        "authors": [
            "Yuxiao Dong",
            "Jie Tang",
            "Nitesh Chawla",
            "Tiancheng Lou",
            "Yang Yang",
            "Bai Wang"
        ],
        "abstract": "Social status, defined as the relative rank or position that an individual holds in a social hierarchy, is known to be among the most important motivating forces in social behaviors. In this paper, we consider the notion of status from the perspective of a position or title held by a person in an enterprise. We study the intersection of social status and social networks in an enterprise. We study whether enterprise communication logs can help reveal how social interactions and individual status manifest themselves in social networks. To that end, we use two enterprise datasets with three communication channels --- voice call, short message, and email --- to demonstrate the social-behavioral differences among individuals with different status. We have several interesting findings and based on these findings we also develop a model to predict social status. On the individual level, high-status individuals are more likely to be spanned as structural holes by linking to people in parts of the enterprise networks that are otherwise not well connected to one another. On the community level, the principle of homophily, social balance and clique theory generally indicate a \"rich club\" maintained by high-status individuals, in the sense that this community is much more connected, balanced and dense. Our model can predict social status of individuals with 93% accuracy.\n    ",
        "submission_date": "2014-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3862",
        "title": "Optimizing the CVaR via Sampling",
        "authors": [
            "Aviv Tamar",
            "Yonatan Glassner",
            "Shie Mannor"
        ],
        "abstract": "Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the CVaR gradient, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risk-sensitive controller for the game of Tetris.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4105",
        "title": "Sparse Compositional Metric Learning",
        "authors": [
            "Yuan Shi",
            "Aur\u00e9lien Bellet",
            "Fei Sha"
        ],
        "abstract": "We propose a new approach for metric learning by framing it as learning a sparse combination of locally discriminative metrics that are inexpensive to generate from the training data. This flexible framework allows us to naturally derive formulations for global, multi-task and local metric learning. The resulting algorithms have several advantages over existing methods in the literature: a much smaller number of parameters to be estimated and a principled way to generalize learned metrics to new testing data points. To analyze the approach theoretically, we derive a generalization bound that justifies the sparse combination. Empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. The results are consistent with our theoretical findings and demonstrate the superiority of our approach in terms of classification performance and scalability.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4304",
        "title": "Automated Classification of Airborne Laser Scanning Point Clouds",
        "authors": [
            "Christoph Waldhauser",
            "Ronald Hochreiter",
            "Johannes Otepka",
            "Norbert Pfeifer",
            "Sajid Ghuffar",
            "Karolina Korzeniowska",
            "Gerald Wagner"
        ],
        "abstract": "Making sense of the physical world has always been at the core of mapping. Up until recently, this has always dependent on using the human eye. Using airborne lasers, it has become possible to quickly \"see\" more of the world in many more dimensions. The resulting enormous point clouds serve as data sources for applications far beyond the original mapping purposes ranging from flooding protection and forestry to threat mitigation. In order to process these large quantities of data, novel methods are required. In this contribution, we develop models to automatically classify ground cover and soil types. Using the logic of machine learning, we critically review the advantages of supervised and unsupervised methods. Focusing on decision trees, we improve accuracy by including beam vector components and using a genetic algorithm. We find that our approach delivers consistently high quality classifications, surpassing classical methods.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4388",
        "title": "Partially Observed, Multi-objective Markov Games",
        "authors": [
            "Yanling Chang",
            "Alan L. Erera",
            "Chelsea C. White III"
        ],
        "abstract": "The intent of this research is to generate a set of non-dominated policies from which one of two agents (the leader) can select a most preferred policy to control a dynamic system that is also affected by the control decisions of the other agent (the follower). The problem is described by an infinite horizon, partially observed Markov game (POMG). At each decision epoch, each agent knows: its past and present states, its past actions, and noise corrupted observations of the other agent's past and present states. The actions of each agent are determined at each decision epoch based on these data. The leader considers multiple objectives in selecting its policy. The follower considers a single objective in selecting its policy with complete knowledge of and in response to the policy selected by the leader. This leader-follower assumption allows the POMG to be transformed into a specially structured, partially observed Markov decision process (POMDP). This POMDP is used to determine the follower's best response policy. A multi-objective genetic algorithm (MOGA) is used to create the next generation of leader policies based on the fitness measures of each leader policy in the current generation. Computing a fitness measure for a leader policy requires a value determination calculation, given the leader policy and the follower's best response policy. The policies from which the leader can select a most preferred policy are the non-dominated policies of the final generation of leader policies created by the MOGA. An example is presented that illustrates how these results can be used to support a manager of a liquid egg production process (the leader) in selecting a sequence of actions to best control this process over time, given that there is an attacker (the follower) who seeks to contaminate the liquid egg production process with a chemical or biological toxin.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4502",
        "title": "A Complete Solver for Constraint Games",
        "authors": [
            "Thi-Van-Anh Nguyen",
            "Arnaud Lallouet"
        ],
        "abstract": "Game Theory studies situations in which multiple agents having conflicting objectives have to reach a collective decision. The question of a compact representation language for agents utility function is of crucial importance since the classical representation of a $n$-players game is given by a $n$-dimensional matrix of exponential size for each player. In this paper we use the framework of Constraint Games in which CSP are used to represent utilities. Constraint Programming --including global constraints-- allows to easily give a compact and elegant model to many useful games. Constraint Games come in two flavors: Constraint Satisfaction Games and Constraint Optimization Games, the first one using satisfaction to define boolean utilities. In addition to multimatrix games, it is also possible to model more complex games where hard constraints forbid certain situations. In this paper we study complete search techniques and show that our solver using the compact representation of Constraint Games is faster than the classical game solver Gambit by one to two orders of magnitude.\n    ",
        "submission_date": "2014-04-17T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5214",
        "title": "Graph Kernels via Functional Embedding",
        "authors": [
            "Anshumali Shrivastava",
            "Ping Li"
        ],
        "abstract": "We propose a representation of graph as a functional object derived from the power iteration of the underlying adjacency matrix. The proposed functional representation is a graph invariant, i.e., the functional remains unchanged under any reordering of the vertices. This property eliminates the difficulty of handling exponentially many isomorphic forms. Bhattacharyya kernel constructed between these functionals significantly outperforms the state-of-the-art graph kernels on 3 out of the 4 standard benchmark graph classification datasets, demonstrating the superiority of our approach. The proposed methodology is simple and runs in time linear in the number of edges, which makes our kernel more efficient and scalable compared to many widely adopted graph kernels with running time cubic in the number of vertices.\n    ",
        "submission_date": "2014-04-21T00:00:00",
        "last_modified_date": "2014-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5528",
        "title": "Hybrid Genetic Algorithm for Cloud Computing Applications",
        "authors": [
            "Saeed Javanmardi",
            "Mohammad Shojafar",
            "Danilo Amendola",
            "Nicola Cordeschi",
            "Hongbo Liu",
            "Ajith Abraham"
        ],
        "abstract": "In this paper with the aid of genetic algorithm and fuzzy theory, we present a hybrid job scheduling approach, which considers the load balancing of the system and reduces total execution time and execution cost. We try to modify the standard Genetic algorithm and to reduce the iteration of creating population with the aid of fuzzy theory. The main goal of this research is to assign the jobs to the resources with considering the VM MIPS and length of jobs. The new algorithm assigns the jobs to the resources with considering the job length and resources capacities. We evaluate the performance of our approach with some famous cloud scheduling models. The results of the experiments show the efficiency of the proposed approach in term of execution time, execution cost and average Degree of Imbalance (DI).\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5711",
        "title": "Modeling multi-stage decision optimization problems",
        "authors": [
            "Ronald Hochreiter"
        ],
        "abstract": "Multi-stage optimization under uncertainty techniques can be used to solve long-term management problems. Although many optimization modeling language extensions as well as computational environments have been proposed, the acceptance of this technique is generally low, due to the inherent complexity of the modeling and solution process. In this paper a simplification to annotate multi-stage decision problems under uncertainty is presented - this simplification contrasts with the common approach to create an extension on top of an existing optimization modeling language. This leads to the definition of meta models, which can be instanced in various programming languages. An example using the statistical computing language R is shown.\n    ",
        "submission_date": "2014-04-23T00:00:00",
        "last_modified_date": "2014-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6071",
        "title": "Rough Clustering Based Unsupervised Image Change Detection",
        "authors": [
            "Chandranath Adak"
        ],
        "abstract": "This paper introduces an unsupervised technique to detect the changed region of multitemporal images on a same reference plane with the help of rough clustering. The proposed technique is a soft-computing approach, based on the concept of rough set with rough clustering and Pawlak's accuracy. It is less noisy and avoids pre-deterministic knowledge about the distribution of the changed and unchanged regions. To show the effectiveness, the proposed technique is compared with some other approaches.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6075",
        "title": "Unsupervised Text Extraction from G-Maps",
        "authors": [
            "Chandranath Adak"
        ],
        "abstract": "This paper represents an text extraction method from Google maps, GIS maps/images. Due to an unsupervised approach there is no requirement of any prior knowledge or training set about the textual and non-textual parts. Fuzzy CMeans clustering technique is used for image segmentation and Prewitt method is used to detect the edges. Connected component analysis and gridding technique enhance the correctness of the results. The proposed method reaches 98.5% accuracy level on the basis of experimental data sets.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6699",
        "title": "An Argumentation-Based Framework to Address the Attribution Problem in Cyber-Warfare",
        "authors": [
            "Paulo Shakarian",
            "Gerardo I. Simari",
            "Geoffrey Moores",
            "Simon Parsons",
            "Marcelo A. Falappa"
        ],
        "abstract": "Attributing a cyber-operation through the use of multiple pieces of technical evidence (i.e., malware reverse-engineering and source tracking) and conventional intelligence sources (i.e., human or signals intelligence) is a difficult problem not only due to the effort required to obtain evidence, but the ease with which an adversary can plant false evidence. In this paper, we introduce a formal reasoning system called the InCA (Intelligent Cyber Attribution) framework that is designed to aid an analyst in the attribution of a cyber-operation even when the available information is conflicting and/or uncertain. Our approach combines argumentation-based reasoning, logic programming, and probabilistic models to not only attribute an operation but also explain to the analyst why the system reaches its conclusions.\n    ",
        "submission_date": "2014-04-27T00:00:00",
        "last_modified_date": "2014-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0501",
        "title": "Exchangeable Variable Models",
        "authors": [
            "Mathias Niepert",
            "Pedro Domingos"
        ],
        "abstract": "A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zero-one loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. Extensive experiments show that EVMs outperform state of the art classifiers such as SVMs and probabilistic models which are solely based on independence assumptions.\n    ",
        "submission_date": "2014-05-02T00:00:00",
        "last_modified_date": "2014-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0647",
        "title": "Feature Selection On Boolean Symbolic Objects",
        "authors": [
            "Djamal Ziani"
        ],
        "abstract": "With the boom in IT technology, the data sets used in application are more and more larger and are described by a huge number of attributes, therefore, the feature selection become an important discipline in Knowledge discovery and data mining, allowing the experts to select the most relevant features to improve the quality of their studies and to reduce the time processing of their algorithm. In addition to that, the data used by the applications become richer. They are now represented by a set of complex and structured objects, instead of simple numerical matrixes. The purpose of our algorithm is to do feature selection on rich data, called Boolean Symbolic Objects (BSOs). These objects are described by multivalued features. The BSOs are considered as higher level units which can model complex data, such as cluster of individuals, aggregated data or taxonomies. In this paper we will introduce a new feature selection criterion for BSOs, and we will explain how we improved its complexity.\n    ",
        "submission_date": "2014-05-04T00:00:00",
        "last_modified_date": "2014-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0921",
        "title": "Gabor Filter and Rough Clustering Based Edge Detection",
        "authors": [
            "Chandranath Adak"
        ],
        "abstract": "This paper introduces an efficient edge detection method based on Gabor filter and rough clustering. The input image is smoothed by Gabor function, and the concept of rough clustering is used to focus on edge detection with soft computational approach. Hysteresis thresholding is used to get the actual output, i.e. edges of the input image. To show the effectiveness, the proposed technique is compared with some other edge detection methods.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1513",
        "title": "A Mathematical Theory of Learning",
        "authors": [
            "Ibrahim Alabdulmohsin"
        ],
        "abstract": "In this paper, a mathematical theory of learning is proposed that has many parallels with information theory. We consider Vapnik's General Setting of Learning in which the learning process is defined to be the act of selecting a hypothesis in response to a given training set. Such hypothesis can, for example, be a decision boundary in classification, a set of centroids in clustering, or a set of frequent item-sets in association rule mining. Depending on the hypothesis space and how the final hypothesis is selected, we show that a learning process can be assigned a numeric score, called learning capacity, which is analogous to Shannon's channel capacity and satisfies similar interesting properties as well such as the data-processing inequality and the information-cannot-hurt inequality. In addition, learning capacity provides the tightest possible bound on the difference between true risk and empirical risk of the learning process for all loss functions that are parametrized by the chosen hypothesis. It is also shown that the notion of learning capacity equivalently quantifies how sensitive the choice of the final hypothesis is to a small perturbation in the training set. Consequently, algorithmic stability is both necessary and sufficient for generalization. While the theory does not rely on concentration inequalities, we finally show that analogs to classical results in learning theory using the Probably Approximately Correct (PAC) model can be immediately deduced using this theory, and conclude with information-theoretic bounds to learning capacity.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2014-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1734",
        "title": "Logic and Constraint Logic Programming for Distributed Constraint Optimization",
        "authors": [
            "Tiep Le",
            "Enrico Pontelli",
            "Tran Cao Son",
            "William Yeoh"
        ],
        "abstract": "The field of Distributed Constraint Optimization Problems (DCOPs) has gained momentum, thanks to its suitability in capturing complex problems (e.g., multi-agent coordination and resource allocation problems) that are naturally distributed and cannot be realistically addressed in a centralized manner. The state of the art in solving DCOPs relies on the use of ad-hoc infrastructures and ad-hoc constraint solving procedures. This paper investigates an infrastructure for solving DCOPs that is completely built on logic programming technologies. In particular, the paper explores the use of a general constraint solver (a constraint logic programming system in this context) to handle the agent-level constraint solving. The preliminary experiments show that logic programming provides benefits over a state-of-the-art DCOP system, in terms of performance and scalability, opening the doors to the use of more advanced technology (e.g., search strategies and complex constraints) for solving DCOPs.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1833",
        "title": "FO(C): A Knowledge Representation Language of Causality",
        "authors": [
            "Bart Bogaerts",
            "Joost Vennekens",
            "Marc Denecker",
            "Jan Van den Bussche"
        ],
        "abstract": "Cause-effect relations are an important part of human knowledge. In real life, humans often reason about complex causes linked to complex effects. By comparison, existing formalisms for representing knowledge about causal relations are quite limited in the kind of specifications of causes and effects they allow. In this paper, we present the new language C-Log, which offers a significantly more expressive representation of effects, including such features as the creation of new objects. We show how C-Log integrates with first-order logic, resulting in the language FO(C). We also compare FO(C) with several related languages and paradigms, including inductive definitions, disjunctive logic programming, business rules and extensions of Datalog.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1864",
        "title": "Dialogues for proof search",
        "authors": [
            "Jesse Alama"
        ],
        "abstract": "Dialogue games are a two-player semantics for a variety of logics, including intuitionistic and classical logic. Dialogues can be viewed as a kind of analytic calculus not unlike tableaux. Can dialogue games be an effective foundation for proof search in intuitionistic logic (both first-order and propositional)? We announce Kuno, an automated theorem prover for intuitionistic first-order logic based on dialogue games.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1958",
        "title": "A Self-Adaptive Network Protection System",
        "authors": [
            "Mohamed Hassan"
        ],
        "abstract": "In this treatise we aim to build a hybrid network automated (self-adaptive) security threats discovery and prevention system; by using unconventional techniques and methods, including fuzzy logic and biological inspired algorithms under the context of soft computing.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2015-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2642",
        "title": "An Abductive Framework for Horn Knowledge Base Dynamics",
        "authors": [
            "Radhakrishnan Delhibabu"
        ],
        "abstract": "The dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information. We introduced the Horn knowledge base dynamics to deal with two important points: first, to handle belief states that need not be deductively closed; and the second point is the ability to declare certain parts of the belief as immutable. In this paper, we address another, radically new approach to this problem. This approach is very close to the Hansson's dyadic representation of belief. Here, we consider the immutable part as defining a new logical system. By a logical system, we mean that it defines its own consequence relation and closure operator. Based on this, we provide an abductive framework for Horn knowledge base dynamics.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2798",
        "title": "Two-Stage Metric Learning",
        "authors": [
            "Jun Wang",
            "Ke Sun",
            "Fei Sha",
            "Stephane Marchand-Maillet",
            "Alexandros Kalousis"
        ],
        "abstract": "In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric with unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2874",
        "title": "A Study of Entanglement in a Categorical Framework of Natural Language",
        "authors": [
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "In both quantum mechanics and corpus linguistics based on vector spaces, the notion of entanglement provides a means for the various subsystems to communicate with each other. In this paper we examine a number of implementations of the categorical framework of Coecke, Sadrzadeh and Clark (2010) for natural language, from an entanglement perspective. Specifically, our goal is to better understand in what way the level of entanglement of the relational tensors (or the lack of it) affects the compositional structures in practical situations. Our findings reveal that a number of proposals for verb construction lead to almost separable tensors, a fact that considerably simplifies the interactions between the words. We examine the ramifications of this fact, and we show that the use of Frobenius algebras mitigates the potential problems to a great extent. Finally, we briefly examine a machine learning method that creates verb tensors exhibiting a sufficient level of entanglement.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3229",
        "title": "Rate of Convergence and Error Bounds for LSTD($\u03bb$)",
        "authors": [
            "Manel Tagorti",
            "Bruno Scherrer"
        ],
        "abstract": "We consider LSTD($\\lambda$), the least-squares temporal-difference algorithm with eligibility traces algorithm proposed by Boyan (2002). It computes a linear approximation of the value function of a fixed policy in a large Markov Decision Process. Under a $\\beta$-mixing assumption, we derive, for any value of $\\lambda \\in (0,1)$, a high-probability estimate of the rate of convergence of this algorithm to its limit. We deduce a high-probability bound on the error of this algorithm, that extends (and slightly improves) that derived by Lazaric et al. (2012) in the specific case where $\\lambda=0$. In particular, our analysis sheds some light on the choice of $\\lambda$ with respect to the quality of the chosen linear space and the number of samples, that complies with simulations.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3559",
        "title": "Credal Model Averaging for classification: representing prior ignorance and expert opinions",
        "authors": [
            "Giorgio Corani",
            "Andrea Mignatti"
        ],
        "abstract": "Bayesian model averaging (BMA) is the state of the art approach for overcoming model uncertainty. Yet, especially on small data sets, the results yielded by BMA might be sensitive to the prior over the models. Credal Model Averaging (CMA) addresses this problem by substituting the single prior over the models by a set of priors (credal set). Such approach solves the problem of how to choose the prior over the models and automates sensitivity analysis. We discuss various CMA algorithms for building an ensemble of logistic regressors characterized by different sets of covariates. We show how CMA can be appropriately tuned to the case in which one is prior-ignorant and to the case in which instead domain knowledge is available. CMA detects prior-dependent instances, namely instances in which a different class is more probable depending on the prior over the models. On such instances CMA suspends the judgment, returning multiple classes. We thoroughly compare different BMA and CMA variants on a real case study, predicting presence of Alpine marmot burrows in an Alpine valley. We find that BMA is almost a random guesser on the instances recognized as prior-dependent by CMA.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3727",
        "title": "Student Dropout Risk Assessment in Undergraduate Course at Residential University",
        "authors": [
            "Sweta Rai"
        ],
        "abstract": "Student dropout prediction is an indispensable for numerous intelligent systems to measure the education system and success rate of any university as well as throughout the university in the world. Therefore, it becomes essential to develop efficient methods for prediction of the students at risk of dropping out, enabling the adoption of proactive process to minimize the situation. Thus, this research work propose a prototype machine learning tool which can automatically recognize whether the student will continue their study or drop their study using classification technique based on decision tree and extract hidden information from large data about what factors are responsible for dropout student. Further the contribution of factors responsible for dropout risk was studied using discriminant analysis and to extract interesting correlations, frequent patterns, associations or casual structures among significant datasets, Association rule mining was applied. In this study, the descriptive statistics analysis was carried out to measure the quality of data using SPSS 20.0 statistical software and application of decision tree and association rule were carried out by using WEKA data mining tool.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3792",
        "title": "Minimum Model Semantics for Extensional Higher-order Logic Programming with Negation",
        "authors": [
            "Angelos Charalambidis",
            "Zolt\u00e1n \u00c9sik",
            "Panos Rondogiannis"
        ],
        "abstract": "Extensional higher-order logic programming has been introduced as a generalization of classical logic programming. An important characteristic of this paradigm is that it preserves all the well-known properties of traditional logic programming. In this paper we consider the semantics of negation in the context of the new paradigm. Using some recent results from non-monotonic fixed-point theory, we demonstrate that every higher-order logic program with negation has a unique minimum infinite-valued model. In this way we obtain the first purely model-theoretic semantics for negation in extensional higher-order logic programming. Using our approach, we resolve an old paradox that was introduced by W. W. Wadge in order to demonstrate the semantic difficulties of higher-order logic programming.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3939",
        "title": "A Novel Method for Developing Robotics via Artificial Intelligence and Internet of Things",
        "authors": [
            "Aadhityan A"
        ],
        "abstract": "This paper describe about a new methodology for developing and improving the robotics field via artificial intelligence and internet of things. Now a day, we can say Artificial Intelligence take the world into robotics. Almost all industries use robots for lot of works. They are use co-operative robots to make different kind of works. But there was some problem to make robot for multi tasks. So there was a necessary new methodology to made multi tasking robots. It will be done only by artificial intelligence and internet of things.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4008",
        "title": "CDF-Intervals: A Reliable Framework to Reason about Data with Uncertainty",
        "authors": [
            "Aya Saad"
        ],
        "abstract": "This research introduces a new constraint domain for reasoning about data with uncertainty. It extends convex modeling with the notion of p-box to gain additional quantifiable information on the data whereabouts. Unlike existing approaches, the p-box envelops an unknown probability instead of approximating its representation. The p-box bounds are uniform cumulative distribution functions (cdf) in order to employ linear computations in the probabilistic domain. The reasoning by means of p-box cdf-intervals is an interval computation which is exerted on the real domain then it is projected onto the cdf domain. This operation conveys additional knowledge represented by the obtained probabilistic bounds. Empirical evaluation shows that, with minimal overhead, the output solution set realizes a full enclosure of the data along with tighter bounds on its probabilistic distributions.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4053",
        "title": "Distributed Representations of Sentences and Documents",
        "authors": [
            "Quoc V. Le",
            "Tomas Mikolov"
        ],
        "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4392",
        "title": "That's sick dude!: Automatic identification of word sense change across different timescales",
        "authors": [
            "Sunny Mitra",
            "Ritwik Mitra",
            "Martin Riedl",
            "Chris Biemann",
            "Animesh Mukherjee",
            "Pawan Goyal"
        ],
        "abstract": "In this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books. We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points. Subsequently, we compare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a particular sense has died. We conduct a thorough evaluation of the proposed methodology both manually as well as through comparison with WordNet. Manual evaluation indicates that the algorithm could correctly identify 60.4% birth cases from a set of 48 randomly picked samples and 57% split/join cases from a set of 21 randomly picked samples. Remarkably, in 44% cases the birth of a novel sense is attested by WordNet, while in 46% cases and 43% cases split and join are respectively confirmed by WordNet. Our approach can be applied for lexicography, as well as for applications like word sense disambiguation or semantic search.\n    ",
        "submission_date": "2014-05-17T00:00:00",
        "last_modified_date": "2014-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4917",
        "title": "An Algebraic Hardness Criterion for Surjective Constraint Satisfaction",
        "authors": [
            "Hubie Chen"
        ],
        "abstract": "The constraint satisfaction problem (CSP) on a relational structure B is to decide, given a set of constraints on variables where the relations come from B, whether or not there is a assignment to the variables satisfying all of the constraints; the surjective CSP is the variant where one decides the existence of a surjective satisfying assignment onto the universe of B. We present an algebraic condition on the polymorphism clone of B and prove that it is sufficient for the hardness of the surjective CSP on a finite structure B, in the sense that this problem admits a reduction from a certain fixed-structure CSP. To our knowledge, this is the first result that allows one to use algebraic information from a relational structure B to infer information on the complexity hardness of surjective constraint satisfaction on B. A corollary of our result is that, on any finite non-trivial structure having only essentially unary polymorphisms, surjective constraint satisfaction is NP-complete.\n    ",
        "submission_date": "2014-05-19T00:00:00",
        "last_modified_date": "2014-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5156",
        "title": "Gaussian Approximation of Collective Graphical Models",
        "authors": [
            "Li-Ping Liu",
            "Daniel Sheldon",
            "Thomas G. Dietterich"
        ],
        "abstract": "The Collective Graphical Model (CGM) models a population of independent and identically distributed individuals when only collective statistics (i.e., counts of individuals) are observed. Exact inference in CGMs is intractable, and previous work has explored Markov Chain Monte Carlo (MCMC) and MAP approximations for learning and inference. This paper studies Gaussian approximations to the CGM. As the population grows large, we show that the CGM distribution converges to a multivariate Gaussian distribution (GCGM) that maintains the conditional independence properties of the original CGM. If the observations are exact marginals of the CGM or marginals that are corrupted by Gaussian noise, inference in the GCGM approximation can be computed efficiently in closed form. If the observations follow a different noise model (e.g., Poisson), then expectation propagation provides efficient and accurate approximate inference. The accuracy and speed of GCGM inference is compared to the MCMC and MAP methods on a simulated bird migration problem. The GCGM matches or exceeds the accuracy of the MAP method while being significantly faster.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5208",
        "title": "A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing",
        "authors": [
            "Alexander M. Rush",
            "Michael Collins"
        ],
        "abstract": "Dual decomposition, and more generally Lagrangian relaxation, is a classical method for combinatorial optimization; it has recently been applied to several inference problems in natural language processing (NLP). This tutorial gives an overview of the technique. We describe example algorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the NLP literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5345",
        "title": "HATP: An HTN Planner for Robotics",
        "authors": [
            "Rapha\u00ebl Lallement",
            "Lavindra de Silva",
            "Rachid Alami"
        ],
        "abstract": "Hierarchical Task Network (HTN) planning is a popular approach that cuts down on the classical planning search space by relying on a given hierarchical library of domain control knowledge. This provides an intuitive methodology for specifying high-level instructions on how robots and agents should perform tasks, while also giving the planner enough flexibility to choose the lower-level steps and their ordering. In this paper we present the HATP (Hierarchical Agent-based Task Planner) planning framework which extends the traditional HTN planning domain representation and semantics by making them more suitable for roboticists, and treating agents as \"first class\" entities in the language. The former is achieved by allowing \"social rules\" to be defined which specify what behaviour is acceptable/unacceptable by the agents/robots in the domain, and interleaving planning with geometric reasoning in order to validate online -with respect to a detailed geometric 3D world- the human/robot actions currently being pursued by HATP.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5498",
        "title": "A Comparison of Monte Carlo Tree Search and Mathematical Optimization for Large Scale Dynamic Resource Allocation",
        "authors": [
            "Dimitris Bertsimas",
            "J. Daniel Griffith",
            "Vishal Gupta",
            "Mykel J. Kochenderfer",
            "Velibor V. Mi\u0161i\u0107",
            "Robert Moss"
        ],
        "abstract": "Dynamic resource allocation (DRA) problems are an important class of dynamic stochastic optimization problems that arise in a variety of important real-world applications. DRA problems are notoriously difficult to solve to optimality since they frequently combine stochastic elements with intractably large state and action spaces. Although the artificial intelligence and operations research communities have independently proposed two successful frameworks for solving dynamic stochastic optimization problems---Monte Carlo tree search (MCTS) and mathematical optimization (MO), respectively---the relative merits of these two approaches are not well understood. In this paper, we adapt both MCTS and MO to a problem inspired by tactical wildfire and management and undertake an extensive computational study comparing the two methods on large scale instances in terms of both the state and the action spaces. We show that both methods are able to greatly improve on a baseline, problem-specific heuristic. On smaller instances, the MCTS and MO approaches perform comparably, but the MO approach outperforms MCTS as the size of the problem increases for a fixed computational budget.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6043",
        "title": "Understanding model counting for $\u03b2$-acyclic CNF-formulas",
        "authors": [
            "Johann Brault-Baron",
            "Florent Capelli",
            "Stefan Mengel"
        ],
        "abstract": "We extend the knowledge about so-called structural restrictions of $\\mathrm{\\#SAT}$ by giving a polynomial time algorithm for $\\beta$-acyclic $\\mathrm{\\#SAT}$. In contrast to previous algorithms in the area, our algorithm does not proceed by dynamic programming but works along an elimination order, solving a weighted version of constraint satisfaction. Moreover, we give evidence that this deviation from more standard algorithm is not a coincidence, but that there is likely no dynamic programming algorithm of the usual style for $\\beta$-acyclic $\\mathrm{\\#SAT}$.\n    ",
        "submission_date": "2014-05-23T00:00:00",
        "last_modified_date": "2014-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6164",
        "title": "Generating Natural Language Descriptions from OWL Ontologies: the NaturalOWL System",
        "authors": [
            "Ion Androutsopoulos",
            "Gerasimos Lampouras",
            "Dimitrios Galanis"
        ],
        "abstract": "We present NaturalOWL, a natural language generation system that produces texts describing individuals or classes of OWL ontologies. Unlike simpler OWL verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. With a system like NaturalOWL, one can publish information in OWL on the Web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. We discuss the processing stages of NaturalOWL, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. We also present trials showing that when the domain-dependent llinguistic resources are available, NaturalOWL produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6341",
        "title": "Efficient Model Learning for Human-Robot Collaborative Tasks",
        "authors": [
            "Stefanos Nikolaidis",
            "Keren Gu",
            "Ramya Ramakrishnan",
            "Julie Shah"
        ],
        "abstract": "We present a framework for learning human user models from joint-action demonstrations that enables the robot to compute a robust policy for a collaborative task with a human. The learning takes place completely automatically, without any human intervention. First, we describe the clustering of demonstrated action sequences into different human types using an unsupervised learning algorithm. These demonstrated sequences are also used by the robot to learn a reward function that is representative for each type, through the employment of an inverse reinforcement learning algorithm. The learned model is then used as part of a Mixed Observability Markov Decision Process formulation, wherein the human type is a partially observable variable. With this framework, we can infer, either offline or online, the human type of a new user that was not included in the training set, and can compute a policy for the robot that will be aligned to the preference of this new user and will be robust to deviations of the human actions from prior demonstrations. Finally we validate the approach using data collected in human subject experiments, and conduct proof-of-concept demonstrations in which a person performs a collaborative task with a small industrial robot.\n    ",
        "submission_date": "2014-05-24T00:00:00",
        "last_modified_date": "2014-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7253",
        "title": "Conformant Planning as a Case Study of Incremental QBF Solving",
        "authors": [
            "Uwe Egly",
            "Martin Kronegger",
            "Florian Lonsing",
            "Andreas Pfandler"
        ],
        "abstract": "We consider planning with uncertainty in the initial state as a case study of incremental quantified Boolean formula (QBF) solving. We report on experiments with a workflow to incrementally encode a planning instance into a sequence of QBFs. To solve this sequence of incrementally constructed QBFs, we use our general-purpose incremental QBF solver DepQBF. Since the generated QBFs have many clauses and variables in common, our approach avoids redundancy both in the encoding phase and in the solving phase. Experimental results show that incremental QBF solving outperforms non-incremental QBF solving. Our results are the first empirical study of incremental QBF solving in the context of planning and motivate its use in other application domains.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7714",
        "title": "The Computational Impact of Partial Votes on Strategic Voting",
        "authors": [
            "Nina Narodytska",
            "Toby Walsh"
        ],
        "abstract": "In many real world elections, agents are not required to rank all candidates. We study three of the most common methods used to modify voting rules to deal with such partial votes. These methods modify scoring rules (like the Borda count), elimination style rules (like single transferable vote) and rules based on the tournament graph (like Copeland) respectively. We argue that with an elimination style voting rule like single transferable vote, partial voting does not change the situations where strategic voting is possible. However, with scoring rules and rules based on the tournament graph, partial voting can increase the situations where strategic voting is possible. As a consequence, the computational complexity of computing a strategic vote can change. For example, with Borda count, the complexity of computing a strategic vote can decrease or stay the same depending on how we score partial votes.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7716",
        "title": "Experimental Demonstration of Array-level Learning with Phase Change Synaptic Devices",
        "authors": [
            "S. Burc Eryilmaz",
            "Duygu Kuzum",
            "Rakesh G. D. Jeyasingh",
            "SangBum Kim",
            "Matthew BrightSky",
            "Chung Lam",
            "H.-S. Philip Wong"
        ],
        "abstract": "The computational performance of the biological brain has long attracted significant interest and has led to inspirations in operating principles, algorithms, and architectures for computing and signal processing. In this work, we focus on hardware implementation of brain-like learning in a brain-inspired architecture. We demonstrate, in hardware, that 2-D crossbar arrays of phase change synaptic devices can achieve associative learning and perform pattern recognition. Device and array-level studies using an experimental 10x10 array of phase change synaptic devices have shown that pattern recognition is robust against synaptic resistance variations and large variations can be tolerated by increasing the number of training iterations. Our measurements show that increase in initial variation from 9 % to 60 % causes required training iterations to increase from 1 to 11.\n    ",
        "submission_date": "2014-05-29T00:00:00",
        "last_modified_date": "2014-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7752",
        "title": "Learning to Act Greedily: Polymatroid Semi-Bandits",
        "authors": [
            "Branislav Kveton",
            "Zheng Wen",
            "Azin Ashkan",
            "Michal Valko"
        ],
        "abstract": "Many important optimization problems, such as the minimum spanning tree and minimum-cost flow, can be solved optimally by a greedy method. In this work, we study a learning variant of these problems, where the model of the problem is unknown and has to be learned by interacting repeatedly with the environment in the bandit setting. We formalize our learning problem quite generally, as learning how to maximize an unknown modular function on a known polymatroid. We propose a computationally efficient algorithm for solving our problem and bound its expected cumulative regret. Our gap-dependent upper bound is tight up to a constant and our gap-free upper bound is tight up to polylogarithmic factors. Finally, we evaluate our method on three problems and demonstrate that it is practical.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7868",
        "title": "A Vague Improved Markov Model Approach for Web Page Prediction",
        "authors": [
            "Priya Bajaj",
            "Supriya Raheja"
        ],
        "abstract": "Today most of the information in all areas is available over the web. It increases the web utilization as well as attracts the interest of researchers to improve the effectiveness of web access and web utilization. As the number of web clients gets increased, the bandwidth sharing is performed that decreases the web access efficiency. Web page prefetching improves the effectiveness of web access by availing the next required web page before the user demand. It is an intelligent predictive mining that analyze the user web access history and predict the next page. In this work, vague improved markov model is presented to perform the prediction. In this work, vague rules are suggested to perform the pruning at different levels of markov model. Once the prediction table is generated, the association mining will be implemented to identify the most effective next page. In this paper, an integrated model is suggested to improve the prediction accuracy and effectiveness.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7869",
        "title": "Integrating Vague Association Mining with Markov Model",
        "authors": [
            "Priya Bajaj",
            "Supriya Raheja"
        ],
        "abstract": "The increasing demand of world wide web raises the need of predicting the user's web page ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7908",
        "title": "Semantic Composition and Decomposition: From Recognition to Generation",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text. Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word. We take a distributional approach to semantics, in which a word is represented by a context vector. Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem. For simplicity, we focus on noun-modifier bigrams and noun unigrams. A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram (\"red salmon\"), generate a noun unigram that is synonymous with the given bigram (\"sockeye\"). A test for semantic decomposition is, given a context vector for a noun unigram (\"snifter\"), generate a noun-modifier bigram that is synonymous with the given unigram (\"brandy glass\"). With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram. We generate ranked lists of potential solutions in two passes. A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list. We evaluate the candidate solutions by comparing them to WordNet synonym sets. For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time. For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0079",
        "title": "Bridging the gap between Legal Practitioners and Knowledge Engineers using semi-formal KR",
        "authors": [
            "Shashishekar Ramakrishna",
            "Adrian Paschke"
        ],
        "abstract": "The use of Structured English as a computation independent knowledge representation format for non-technical users in business rules representation has been proposed in OMGs Semantics and Business Vocabulary Representation (SBVR). In the legal domain we face a similar problem. Formal representation languages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2 ontologies etc.) support the technical knowledge engineer and the automated reasoning. But, they can be hardly used directly by the legal domain experts who do not have a computer science background. In this paper we adapt the SBVR Structured English approach for the legal domain and implement a proof-of-concept, called KR4IPLaw, which enables legal domain experts to represent their knowledge in Structured English in a computational independent and hence, for them, more usable way. The benefit of this approach is that the underlying pre-defined semantics of the Structured English approach makes transformations into formal languages such as OASIS LegalRuleML and OWL2 ontologies possible. We exemplify our approach in the domain of patent law.\n    ",
        "submission_date": "2014-05-31T00:00:00",
        "last_modified_date": "2014-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0175",
        "title": "Evolutionary Search in the Space of Rules for Creation of New Two-Player Board Games",
        "authors": [
            "Zahid Halim"
        ],
        "abstract": "Games have always been a popular test bed for artificial intelligence techniques. Game developers are always in constant search for techniques that can automatically create computer games minimizing the developer's task. In this work we present an evolutionary strategy based solution towards the automatic generation of two player board games. To guide the evolutionary process towards games, which are entertaining, we propose a set of metrics. These metrics are based upon different theories of entertainment in computer games. This work also compares the entertainment value of the evolved games with the existing popular board based games. Further to verify the entertainment value of the evolved games with the entertainment value of the human user a human user survey is conducted. In addition to the user survey we check the learnability of the evolved games using an artificial neural network based controller. The proposed metrics and the evolutionary process can be employed for generating new and entertaining board games, provided an initial search space is given to the evolutionary algorithm.\n    ",
        "submission_date": "2014-06-01T00:00:00",
        "last_modified_date": "2014-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0303",
        "title": "A Superposition Calculus for Abductive Reasoning",
        "authors": [
            "Mnacho Echenim",
            "Nicolas Peltier"
        ],
        "abstract": "We present a modification of the superposition calculus that is meant to generate consequences of sets of first-order axioms. This approach is proven to be sound and deductive-complete in the presence of redundancy elimination rules, provided the considered consequences are built on a given finite set of ground terms, represented by constant symbols. In contrast to other approaches, most existing results about the termination of the superposition calculus can be carried over to our procedure. This ensures in particular that the calculus is terminating for many theories of interest to the SMT community.\n    ",
        "submission_date": "2014-06-02T00:00:00",
        "last_modified_date": "2014-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0893",
        "title": "Query Answering over Contextualized RDF/OWL Knowledge with Forall-Existential Bridge Rules: Attaining Decidability using Acyclicity (full version)",
        "authors": [
            "Mathew Joseph",
            "Gabriel Kuper",
            "Luciano Serafini"
        ],
        "abstract": "The recent outburst of context-dependent knowledge on the Semantic Web (SW) has led to the realization of the importance of the quads in the SW community. Quads, which extend a standard RDF triple, by adding a new parameter of the `context' of an RDF triple, thus informs a reasoner to distinguish between the knowledge in various contexts. Although this distinction separates the triples in an RDF graph into various contexts, and allows the reasoning to be decoupled across various contexts, bridge rules need to be provided for inter-operating the knowledge across these contexts. We call a set of quads together with the bridge rules, a quad-system. In this paper, we discuss the problem of query answering over quad-systems with expressive forall-existential bridge rules. It turns out the query answering over quad-systems is undecidable, in general. We derive a decidable class of quad-systems, namely context-acyclic quad-systems, for which query answering can be done using forward chaining. Tight bounds for data and combined complexity of query entailment has been established for the derived class.\n    ",
        "submission_date": "2014-06-03T00:00:00",
        "last_modified_date": "2014-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1222",
        "title": "Discovering Structure in High-Dimensional Data Through Correlation Explanation",
        "authors": [
            "Greg Ver Steeg",
            "Aram Galstyan"
        ],
        "abstract": "We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1234",
        "title": "A Geometric Method to Obtain the Generation Probability of a Sentence",
        "authors": [
            "Chen Lijiang"
        ],
        "abstract": "\"How to generate a sentence\" is the most critical and difficult problem in all the natural language processing technologies. In this paper, we present a new approach to explain the generation process of a sentence from the perspective of mathematics. Our method is based on the premise that in our brain a sentence is a part of a word network which is formed by many word nodes. Experiments show that the probability of the entire sentence can be obtained by the probabilities of single words and the probabilities of the co-occurrence of word pairs, which indicate that human use the synthesis method to generate a sentence.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1404",
        "title": "On the satisfiability problem for SPARQL patterns",
        "authors": [
            "Xiaowang Zhang",
            "Jan Van den Bussche",
            "Fran\u00e7ois Picalausa"
        ],
        "abstract": "The satisfiability problem for SPARQL patterns is undecidable in general, since the expressive power of SPARQL 1.0 is comparable with that of the relational algebra. The goal of this paper is to delineate the boundary of decidability of satisfiability in terms of the constraints allowed in filter conditions. The classes of constraints considered are bound-constraints, negated bound-constraints, equalities, nonequalities, constant-equalities, and constant-nonequalities. The main result of the paper can be summarized by saying that, as soon as inconsistent filter conditions can be formed, satisfiability is undecidable. The key insight in each case is to find a way to emulate the set difference operation. Undecidability can then be obtained from a known undecidability result for the algebra of binary relations with union, composition, and set difference. When no inconsistent filter conditions can be formed, satisfiability is efficiently decidable by simple checks on bound variables and on the use of literals. The paper also points out that satisfiability for the so-called `well-designed' patterns can be decided by a check on bound variables and a check for inconsistent filter conditions.\n    ",
        "submission_date": "2014-06-05T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1509",
        "title": "Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in the Othello League",
        "authors": [
            "Wojciech Ja\u015bkowski"
        ],
        "abstract": "N-tuple networks have been successfully used as position evaluation functions for board games such as Othello or Connect Four. The effectiveness of such networks depends on their architecture, which is determined by the placement of constituent n-tuples, sequences of board locations, providing input to the network. The most popular method of placing n-tuples consists in randomly generating a small number of long, snake-shaped board location sequences. In comparison, we show that learning n-tuple networks is significantly more effective if they involve a large number of systematically placed, short, straight n-tuples. Moreover, we demonstrate that in order to obtain the best performance and the steepest learning curve for Othello it is enough to use n-tuples of size just 2, yielding a network consisting of only 288 weights. The best such network evolved in this study has been evaluated in the online Othello League, obtaining the performance of nearly 96% --- more than any other player to date.\n    ",
        "submission_date": "2014-06-05T00:00:00",
        "last_modified_date": "2014-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1928",
        "title": "An exact and two heuristic strategies for truthful bidding in combinatorial transport auctions",
        "authors": [
            "Tobias Buer"
        ],
        "abstract": "To support a freight carrier in a combinatorial transport auction, we proposes an exact and two heuristic strategies for bidding on subsets of requests. The exact bidding strategy is based on the concept of elementary request combinations. We show that it is sufficient and necessary for a carrier to bid on each elementary request combination in order to guarantee the same result as bidding on each element of the powerset of the set of tendered requests. Both heuristic bidding strategies identify promising request combinations. For this, pairwise synergies based on saving values as well as the capacitated p-median problem are used. The bidding strategies are evaluated by a computational study that simulates an auction. It is based on 174 benchmark instances and therefore easily extendable by other researchers. On average, the two heuristic strategies achieve 91 percent and 81 percent of the available sales potential while generating 36 and only 4 percent of the bundle bids of the exact strategy. Therefore, the proposed bidding strategies help a carrier to increase her chance to win and at the same time reduce the computational burden to participate in a combinatorial transport auction.\n    ",
        "submission_date": "2014-06-07T00:00:00",
        "last_modified_date": "2014-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1969",
        "title": "A Semantic Enhanced Model for effective Spatial Information Retrieval",
        "authors": [
            "Adeyinka K. Akanbi",
            "Olusanya Y. Agunbiade",
            "Sadiq Kuti",
            "Olumuyiwa J. Dehinbo"
        ],
        "abstract": "A lot of information on the web is geographically referenced. Discovering and retrieving this geographic information to satisfy various users needs across both open and distributed Spatial Data Infrastructures (SDI) poses eminent research challenges. However, this is mostly caused by semantic heterogeneity in users query and lack of semantic referencing of the Geographic Information (GI) metadata. To addressing these challenges, this paper discusses ontology based semantic enhanced model, which explicitly represents GI metadata, and provides linked RDF instances of each entity. The system focuses on semantic search, ontology, and efficient spatial information retrieval. In particular, an integrated model that uses specific domain information extraction to improve the searching and retrieval of ranked spatial search results.\n    ",
        "submission_date": "2014-06-08T00:00:00",
        "last_modified_date": "2014-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2161",
        "title": "Tableaux for Dynamic Logic of Propositional Assignments",
        "authors": [
            "Tiago de Lima",
            "Andreas Herzig"
        ],
        "abstract": "The Dynamic Logic for Propositional Assignments (DL-PA) has recently been studied as an alternative to Propositional Dynamic Logic (PDL). In DL-PA, the abstract atomic programs of PDL are replaced by assignments of propositional variables to truth values. This makes DL-PA enjoy some interesting meta-logical properties that PDL does not, such as eliminability of the Kleene star, compactness and interpolation. We define and analytic tableaux calculus for DL-PA and show that it matches the known complexity results.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2234",
        "title": "Fault-Tolerant, but Paradoxical Path-Finding in Physical and Conceptual Systems",
        "authors": [
            "Bryan Knowles",
            "Mustafa Atici"
        ],
        "abstract": "We report our initial investigations into reliability and path-finding based models and propose future areas of interest. Inspired by broken sidewalks during on-campus construction projects, we develop two models for navigating this \"unreliable network.\" These are based on a concept of \"accumulating risk\" backward from the destination, and both operate on directed acyclic graphs with a probability of failure associated with each edge. The first serves to introduce and has faults addressed by the second, more conservative model. Next, we show a paradox when these models are used to construct polynomials on conceptual networks, such as design processes and software development life cycles. When the risk of a network increases uniformly, the most reliable path changes from wider and longer to shorter and narrower. If we let professional inexperience--such as with entry level cooks and software developers--represent probability of edge failure, does this change in path imply that the novice should follow instructions with fewer \"back-up\" plans, yet those with alternative routes should be followed by the expert?\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2464",
        "title": "Music and Vocal Separation Using Multi-Band Modulation Based Features",
        "authors": [
            "Sunil Kumar Kopparapu",
            "Meghna Pandharipande",
            "G Sita"
        ],
        "abstract": "The potential use of non-linear speech features has not been investigated for music analysis although other commonly used speech features like Mel Frequency Ceptral Coefficients (MFCC) and pitch have been used extensively. In this paper, we assume an audio signal to be a sum of modulated sinusoidal and then use the energy separation algorithm to decompose the audio into amplitude and frequency modulation components using the non-linear Teager-Kaiser energy operator. We first identify the distribution of these non-linear features for music only and voice only segments in the audio signal in different Mel spaced frequency bands and show that they have the ability to discriminate. The proposed method based on Kullback-Leibler divergence measure is evaluated using a set of Indian classical songs from three different artists. Experimental results show that the discrimination ability is evident in certain low and mid frequency bands (200 - 1500 Hz).\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2538",
        "title": "FrameNet CNL: a Knowledge Representation and Information Extraction Language",
        "authors": [
            "Guntis Barzdins"
        ],
        "abstract": "The paper presents a FrameNet-based information extraction and knowledge representation framework, called FrameNet-CNL. The framework is used on natural language documents and represents the extracted knowledge in a tailor-made Frame-ontology from which unambiguous FrameNet-CNL paraphrase text can be generated automatically in multiple languages. This approach brings together the fields of information extraction and CNL, because a source text can be considered belonging to FrameNet-CNL, if information extraction parser produces the correct knowledge representation as a result. We describe a state-of-the-art information extraction parser used by a national news agency and speculate that FrameNet-CNL eventually could shape the natural language subset used for writing the newswire articles.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2602",
        "title": "Graph Approximation and Clustering on a Budget",
        "authors": [
            "Ethan Fetaya",
            "Ohad Shamir",
            "Shimon Ullman"
        ],
        "abstract": "We consider the problem of learning from a similarity matrix (such as spectral clustering and lowd imensional embedding), when computing pairwise similarities are costly, and only a limited number of entries can be observed. We provide a theoretical analysis using standard notions of graph approximation, significantly generalizing previous results (which focused on spectral clustering with two clusters). We also propose a new algorithmic approach based on adaptive sampling, which experimentally matches or improves on previous methods, while being considerably more general and computationally cheaper.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2616",
        "title": "PlanIt: A Crowdsourcing Approach for Learning to Plan Paths from Large Scale Preference Feedback",
        "authors": [
            "Ashesh Jain",
            "Debarghya Das",
            "Jayesh K Gupta",
            "Ashutosh Saxena"
        ],
        "abstract": "We consider the problem of learning user preferences over robot trajectories for environments rich in objects and humans. This is challenging because the criterion defining a good trajectory varies with users, tasks and interactions in the environment. We represent trajectory preferences using a cost function that the robot learns and uses it to generate good trajectories in new environments. We design a crowdsourcing system - PlanIt, where non-expert users label segments of the robot's trajectory. PlanIt allows us to collect a large amount of user feedback, and using the weak and noisy labels from PlanIt we learn the parameters of our model. We test our approach on 122 different environments for robotic navigation and manipulation tasks. Our extensive experiments show that the learned cost function generates preferred trajectories in human environments. Our crowdsourcing system is publicly available for the visualization of the learned costs and for providing preference feedback: \\url{",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2623",
        "title": "Maximum Likelihood-based Online Adaptation of Hyper-parameters in CMA-ES",
        "authors": [
            "Ilya Loshchilov",
            "Marc Schoenauer",
            "Mich\u00e8le Sebag",
            "Nikolaus Hansen"
        ],
        "abstract": "The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is widely accepted as a robust derivative-free continuous optimization algorithm for non-linear and non-convex optimization problems. CMA-ES is well known to be almost parameterless, meaning that only one hyper-parameter, the population size, is proposed to be tuned by the user. In this paper, we propose a principled approach called self-CMA-ES to achieve the online adaptation of CMA-ES hyper-parameters in order to improve its overall performance. Experimental results show that for larger-than-default population size, the default settings of hyper-parameters of CMA-ES are far from being optimal, and that self-CMA-ES allows for dynamically approaching optimal settings.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3185",
        "title": "Generic construction of scale-invariantly coarse grained memory",
        "authors": [
            "Karthik H. Shankar"
        ],
        "abstract": "Encoding temporal information from the recent past as spatially distributed activations is essential in order for the entire recent past to be simultaneously accessible. Any biological or synthetic agent that relies on the past to predict/plan the future, would be endowed with such a spatially distributed temporal memory. Simplistically, we would expect that resource limitations would demand the memory system to store only the most useful information for future prediction. For natural signals in real world which show scale free temporal fluctuations, the predictive information encoded in memory is maximal if the past information is scale invariantly coarse grained. Here we examine the general mechanism to construct a scale invariantly coarse grained memory system. Remarkably, the generic construction is equivalent to encoding the linear combinations of Laplace transform of the past information and their approximated inverses. This reveals a fundamental construction constraint on memory networks that attempt to maximize predictive information storage relevant to the natural world.\n    ",
        "submission_date": "2014-06-12T00:00:00",
        "last_modified_date": "2015-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3843",
        "title": "Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models",
        "authors": [
            "Yichuan Zhang",
            "Charles Sutton"
        ],
        "abstract": "Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.\n    ",
        "submission_date": "2014-06-15T00:00:00",
        "last_modified_date": "2014-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4041",
        "title": "CCCP Algorithms to Minimize the Bethe free energy of 3-SAT Problem",
        "authors": [
            "Yusupjan Habibulla"
        ],
        "abstract": "The k-sat problem is a prototypical constraint satisfaction problem. There are many algorithms to study k-sat problem, BP algorithm is famous one of them. But BP algorithm does not converge when $\\alpha$(constraint density)is bigger than some threshold value. In this paper we use CCCP (Concave Convex Procedure) algorithm to study 3-sat problem and we get better results than BP algorithm that CCCP algorithm still converges when BP algorithm does not converge. Our work almost builds on recent results by Yuille \\cite{Yuille2002} who apply the CCCP algorithm to Bethe and Kikuchi free energies and obtained two algorithms on 2D and 3D spin glasses. Our implementation of CCCP algorithm on 3-sat problem is some different from his implementation and we have some different views about CCCP algorithm's some properties. Some difference of these maybe because of CCCP algorithm have different properties and implementation process on different problem and some others of these are related to the CCCP algorithm itself. Our work indicates that CCCP algorithm has more learning and inference applications.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4110",
        "title": "Acyclicity Notions for Existential Rules and Their Application to Query Answering in Ontologies",
        "authors": [
            "Bernardo Cuenca Grau",
            "Ian Horrocks",
            "Markus Kr\u00f6tzsch",
            "Clemens Kupke",
            "Despoina Magka",
            "Boris Motik",
            "Zhe Wang"
        ],
        "abstract": "Answering conjunctive queries (CQs) over a set of facts extended with existential rules is a prominent problem in knowledge representation and databases. This problem can be solved using the chase algorithm, which extends the given set of facts with fresh facts in order to satisfy the rules. If the chase terminates, then CQs can be evaluated directly in the resulting set of facts. The chase, however, does not terminate necessarily, and checking whether the chase terminates on a given set of rules and facts is undecidable. Numerous acyclicity notions were proposed as sufficient conditions for chase termination. In this paper, we present two new acyclicity notions called model-faithful acyclicity (MFA) and model-summarising acyclicity (MSA). Furthermore, we investigate the landscape of the known acyclicity notions and establish a complete taxonomy of all notions known to us. Finally, we show that MFA and MSA generalise most of these notions.\nExistential rules are closely related to the Horn fragments of the OWL 2 ontology language; furthermore, several prominent OWL 2 reasoners implement CQ answering by using the chase to materialise all relevant facts. In order to avoid termination problems, many of these systems handle only the OWL 2 RL profile of OWL 2; furthermore, some systems go beyond OWL 2 RL, but without any termination guarantees. In this paper we also investigate whether various acyclicity notions can provide a principled and practical solution to these problems. On the theoretical side, we show that query answering for acyclic ontologies is of lower complexity than for general ontologies. On the practical side, we show that many of the commonly used OWL 2 ontologies are MSA, and that the number of facts obtained by materialisation is not too large. Our results thus suggest that principled development of materialisation-based OWL 2 reasoners is practically feasible.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4447",
        "title": "Automatic Fado Music Classification",
        "authors": [
            "Pedro Gir\u00e3o Antunes",
            "David Martins de Matos",
            "Ricardo Ribeiro",
            "Isabel Trancoso"
        ],
        "abstract": "In late 2011, Fado was elevated to the oral and intangible heritage of humanity by UNESCO. This study aims to develop a tool for automatic detection of Fado music based on the audio signal. To do this, frequency spectrum-related characteristics were captured form the audio signal: in addition to the Mel Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the signal was further analysed in two frequency ranges, providing additional information. Tests were run both in a 10-fold cross-validation setup (97.6% accuracy), and in a traditional train/test setup (95.8% accuracy). The good results reflect the fact that Fado is a very distinctive musical style.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4710",
        "title": "Typed Hilbert Epsilon Operators and the Semantics of Determiner Phrases (Invited Lecture)",
        "authors": [
            "Christian Retor\u00e9"
        ],
        "abstract": "The semantics of determiner phrases, be they definite de- scriptions, indefinite descriptions or quantified noun phrases, is often as- sumed to be a fully solved question: common nouns are properties, and determiners are generalised quantifiers that apply to two predicates: the property corresponding to the common noun and the one corresponding to the verb phrase. We first present a criticism of this standard view. Firstly, the semantics of determiners does not follow the syntactical structure of the sentence. Secondly the standard interpretation of the indefinite article cannot ac- count for nominal sentences. Thirdly, the standard view misses the linguis- tic asymmetry between the two properties of a generalised quantifier. In the sequel, we propose a treatment of determiners and quantifiers as Hilbert terms in a richly typed system that we initially developed for lexical semantics, using a many sorted logic for semantical representations. We present this semantical framework called the Montagovian generative lexicon and show how these terms better match the syntactical structure and avoid the aforementioned problems of the standard approach. Hilbert terms rather differ from choice functions in that there is one polymorphic operator and not one operator per formula. They also open an intriguing connection between the logic for meaning assembly, the typed lambda calculus handling compositionality and the many-sorted logic for semantical representations. Furthermore epsilon terms naturally introduce type-judgements and confirm the claim that type judgment are a form of presupposition.\n    ",
        "submission_date": "2014-06-18T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5301",
        "title": "Low-Autocorrelation Binary Sequences: On Improved Merit Factors and Runtime Predictions to Achieve Them",
        "authors": [
            "Borko Bo\u0161kovi\u0107",
            "Franc Brglez",
            "Janez Brest"
        ],
        "abstract": "The search for binary sequences with a high figure of merit, known as the low autocorrelation binary sequence ($labs$}) problem, represents a formidable computational challenge. To mitigate the computational constraints of the problem, we consider solvers that accept odd values of sequence length $L$ and return solutions for skew-symmetric binary sequences only -- with the consequence that not all best solutions under this constraint will be optimal for each $L$. In order to improve both, the search for best merit factor $and$ the asymptotic runtime performance, we instrumented three stochastic solvers, the first two are state-of-the-art solvers that rely on variants of memetic and tabu search ($lssMAts$ and $lssRRts$), the third solver ($lssOrel$) organizes the search as a sequence of independent contiguous self-avoiding walk segments. By adapting a rigorous statistical methodology to performance testing of all three combinatorial solvers, experiments show that the solver with the best asymptotic average-case performance, $lssOrel\\_8 = 0.000032*1.1504^L$, has the best chance of finding solutions that improve, as $L$ increases, figures of merit reported to date. The same methodology can be applied to engineering new $labs$ solvers that may return merit factors even closer to the conjectured asymptotic value of 12.3248.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5311",
        "title": "Towards A Deeper Geometric, Analytic and Algorithmic Understanding of Margins",
        "authors": [
            "Aaditya Ramdas",
            "Javier Pe\u00f1a"
        ],
        "abstract": "Given a matrix $A$, a linear feasibility problem (of which linear classification is a special case) aims to find a solution to a primal problem $w: A^Tw > \\textbf{0}$ or a certificate for the dual problem which is a probability distribution $p: Ap = \\textbf{0}$. Inspired by the continued importance of \"large-margin classifiers\" in machine learning, this paper studies a condition measure of $A$ called its \\textit{margin} that determines the difficulty of both the above problems. To aid geometrical intuition, we first establish new characterizations of the margin in terms of relevant balls, cones and hulls. Our second contribution is analytical, where we present generalizations of Gordan's theorem, and variants of Hoffman's theorems, both using margins. We end by proving some new results on a classical iterative scheme, the Perceptron, whose convergence rates famously depends on the margin. Our results are relevant for a deeper understanding of margin-based learning and proving convergence rates of iterative schemes, apart from providing a unifying perspective on this vast topic.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2016-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5370",
        "title": "Spectral Ranking using Seriation",
        "authors": [
            "Fajwel Fogel",
            "Alexandre d'Aspremont",
            "Milan Vojnovic"
        ],
        "abstract": "We describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than classical scoring methods. Finally, we bound the ranking error when only a random subset of the comparions are observed. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5614",
        "title": "PAC-Bayes Analysis of Multi-view Learning",
        "authors": [
            "Shiliang Sun",
            "John Shawe-Taylor",
            "Liang Mao"
        ],
        "abstract": "This paper presents eight PAC-Bayes bounds to analyze the generalization performance of multi-view classifiers. These bounds adopt data dependent Gaussian priors which emphasize classifiers with high view agreements. The center of the prior for the first two bounds is the origin, while the center of the prior for the third and fourth bounds is given by a data dependent vector. An important technique to obtain these bounds is two derived logarithmic determinant inequalities whose difference lies in whether the dimensionality of data is involved. The centers of the fifth and sixth bounds are calculated on a separate subset of the training set. The last two bounds use unlabeled data to represent view agreements and are thus applicable to semi-supervised multi-view learning. We evaluate all the presented multi-view PAC-Bayes bounds on benchmark data and compare them with previous single-view PAC-Bayes bounds. The usefulness and performance of the multi-view bounds are discussed.\n    ",
        "submission_date": "2014-06-21T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6046",
        "title": "Hybrid Epidemics - A Case Study on Computer Worm Conficker",
        "authors": [
            "Changwang Zhang",
            "Shi Zhou",
            "Benjamin M. Chain"
        ],
        "abstract": "Conficker is a computer worm that erupted on the Internet in 2008. It is unique in combining three different spreading strategies: local probing, neighbourhood probing, and global probing. We propose a mathematical model that combines three modes of spreading, local, neighbourhood and global to capture the worm's spreading behaviour. The parameters of the model are inferred directly from network data obtained during the first day of the Conifcker epidemic. The model is then used to explore the trade-off between spreading modes in determining the worm's effectiveness. Our results show that the Conficker epidemic is an example of a critically hybrid epidemic, in which the different modes of spreading in isolation do not lead to successful epidemics. Such hybrid spreading strategies may be used beneficially to provide the most effective strategies for promulgating information across a large population. When used maliciously, however, they can present a dangerous challenge to current internet security protocols.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6605",
        "title": "SROIQsigma is decidable",
        "authors": [
            "Jon Ha\u00ebl Brenas",
            "Rachid Echahed",
            "Martin Strecker"
        ],
        "abstract": "We consider a dynamic extension of the description logic $\\mathcal{SROIQ}$. This means that interpretations could evolve thanks to some actions such as addition and/or deletion of an element (respectively, a pair of elements) of a concept (respectively, of a role). The obtained logic is called $\\mathcal{SROIQ}$ with explicit substitutions and is written $\\mathcal{SROIQ^\\sigma}$. Substitution is not treated as meta-operation that is carried out immediately, but the operation of substitution may be delayed, so that sub-formulae of $\\mathcal{SROIQ}^\\sigma$ are of the form $\\Phi\\sigma$, where $\\Phi$ is a $\\mathcal{SROIQ}$ formula and $\\sigma$ is a substitution which encodes changes of concepts and roles. In this paper, we particularly prove that the satisfiability problem of $\\mathcal{SROIQ}^\\sigma$ is decidable.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7282",
        "title": "An interacting replica approach applied to the traveling salesman problem",
        "authors": [
            "Bo Sun",
            "Blake Leonard",
            "Peter Ronhovde",
            "Zohar Nussinov"
        ],
        "abstract": "We present a physics inspired heuristic method for solving combinatorial optimization problems. Our approach is specifically motivated by the desire to avoid trapping in metastable local minima- a common occurrence in hard problems with multiple extrema. Our method involves (i) coupling otherwise independent simulations of a system (\"replicas\") via geometrical distances as well as (ii) probabilistic inference applied to the solutions found by individual replicas. The {\\it ensemble} of replicas evolves as to maximize the inter-replica correlation while simultaneously minimize the local intra-replica cost function (e.g., the total path length in the Traveling Salesman Problem within each replica). We demonstrate how our method improves the performance of rudimentary local optimization schemes long applied to the NP hard Traveling Salesman Problem. In particular, we apply our method to the well-known \"$k$-opt\" algorithm and examine two particular cases- $k=2$ and $k=3$. With the aid of geometrical coupling alone, we are able to determine for the optimum tour length on systems up to $280$ cities (an order of magnitude larger than the largest systems typically solved by the bare $k=3$ opt). The probabilistic replica-based inference approach improves $k-opt$ even further and determines the optimal solution of a problem with $318$ cities and find tours whose total length is close to that of the optimal solutions for other systems with a larger number of cities.\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7288",
        "title": "Linearized and Single-Pass Belief Propagation",
        "authors": [
            "Wolfgang Gatterbauer",
            "Stephan G\u00fcnnemann",
            "Danai Koutra",
            "Christos Faloutsos"
        ],
        "abstract": "How can we tell when accounts are fake or real in a social network? And how can we tell which accounts belong to liberal, conservative or centrist users? Often, we can answer such questions and label nodes in a network based on the labels of their neighbors and appropriate assumptions of homophily (\"birds of a feather flock together\") or heterophily (\"opposites attract\"). One of the most widely used methods for this kind of inference is Belief Propagation (BP) which iteratively propagates the information from a few nodes with explicit labels throughout a network until convergence. One main problem with BP, however, is that there are no known exact guarantees of convergence in graphs with loops.\n",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7443",
        "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits",
        "authors": [
            "Zheng Wen",
            "Branislav Kveton",
            "Azin Ashkan"
        ],
        "abstract": "A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines.\n    ",
        "submission_date": "2014-06-28T00:00:00",
        "last_modified_date": "2017-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7473",
        "title": "An Efficient Hybrid CS and K-Means Algorithm for the Capacitated PMedian Problem",
        "authors": [
            "Hassan Gholami Mazinan",
            "Gholam Reza Ahmadi",
            "Erfan Khaji"
        ],
        "abstract": "Capacitated p-median problem (CPMP) is an important variation of facility location problem in which p capacitated medians are economically selected to serve a set of demand vertices so that the total assigned demand to each of the candidate medians must not exceed its capacity. This paper surveys and analyses the combination of Cuckoo Search and K-Means algorithms to solve the CPMP. In order to check for quality and validity of the suggestive method, we compared the final solution produced over the two test problems of Osman and Christofides, each of which including 10 sample tests. According to the results, the suggested meta-heuristic algorithm shows superiority over the rest known algorithms in this field as all the best known solutions in the first problem set, and several sample sets in the second problem set have been improved within reasonable periods of time.\n    ",
        "submission_date": "2014-06-29T00:00:00",
        "last_modified_date": "2014-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7648",
        "title": "Bayesian Network Constraint-Based Structure Learning Algorithms: Parallel and Optimised Implementations in the bnlearn R Package",
        "authors": [
            "Marco Scutari"
        ],
        "abstract": "It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios.\n",
        "submission_date": "2014-06-30T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0481",
        "title": "Semantic Integration & Single-Site Opening of Multiple Governmental Data Sources",
        "authors": [
            "Konstantinos Kotis",
            "Iraklis Athanasakis",
            "George Vouros"
        ],
        "abstract": "In many cases, government data is still \"locked\" in several \"data silos\", even within the boundaries of a single (inter-)national public organization with disparate and distributed organizational units and departments spread across multiple sites. Opening data and enabling its unified querying from a single site in an efficient and effective way is a semantic application integration and open government data challenge. This paper describes how NARA is using Semantic Web technology to implement an application integration approach within the boundaries of its organization via opening and querying multiple governmental data sources from a single site. The generic approach proposed, namely S3-AI, provides support to answering unified, ontology-mediated, federated queries to data produced and exploited by disparate applications, while these are being located in different organizational sites. S3-AI preserves ownership, autonomy and independency of applications and data. The paper extensively demonstrates S3-AI, using the D2RQ and Fuseki technologies, for addressing the needs of a governmental \"IT helpdesk support\" case.\n    ",
        "submission_date": "2014-07-02T00:00:00",
        "last_modified_date": "2014-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0787",
        "title": "Decision-theoretic approaches to non-knowledge in economics",
        "authors": [
            "Ekaterina Svetlova",
            "Henk van Elst"
        ],
        "abstract": "We review two strands of conceptual approaches to the formal representation of a decision maker's non-knowledge at the initial stage of a static one-person, one-shot decision problem in economic theory. One focuses on representations of non-knowledge in terms of probability measures over sets of mutually exclusive and exhaustive consequence-relevant states of Nature, the other deals with unawareness of potentially important events by means of sets of states that are less complete than the full set of consequence-relevant states of Nature. We supplement our review with a brief discussion of unresolved matters in both approaches.\n    ",
        "submission_date": "2014-07-03T00:00:00",
        "last_modified_date": "2014-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1339",
        "title": "Inverse Graphics with Probabilistic CAD Models",
        "authors": [
            "Tejas D. Kulkarni",
            "Vikash K. Mansinghka",
            "Pushmeet Kohli",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "Recently, multiple formulations of vision problems as probabilistic inversions of generative models based on computer graphics have been proposed. However, applications to 3D perception from natural images have focused on low-dimensional latent scenes, due to challenges in both modeling and inference. Accounting for the enormous variability in 3D object shape and 2D appearance via realistic generative models seems intractable, as does inverting even simple versions of the many-to-many computations that link 3D scenes to 2D images. This paper proposes and evaluates an approach that addresses key aspects of both these challenges. We show that it is possible to solve challenging, real-world 3D vision problems by approximate inference in generative models for images based on rendering the outputs of probabilistic CAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3D meshes corresponding to plausible objects and apply affine transformations to place them in a scene. Image likelihoods are based on similarity in a feature space based on standard mid-level image representations from the vision literature. Our inference algorithm integrates single-site and locally blocked Metropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminative data-driven proposals learned from training data generated from our models. We apply this approach to 3D human pose estimation and object shape reconstruction from single images, achieving quantitative and qualitative performance improvements over state-of-the-art baselines.\n    ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1933",
        "title": "Lexpresso: a Controlled Natural Language",
        "authors": [
            "Adam Saulwick"
        ],
        "abstract": "This paper presents an overview of `Lexpresso', a Controlled Natural Language developed at the Defence Science & Technology Organisation as a bidirectional natural language interface to a high-level information fusion system. The paper describes Lexpresso's main features including lexical coverage, expressiveness and range of linguistic syntactic and semantic structures. It also touches on its tight integration with a formal semantic formalism and tentatively classifies it against the PENS system.\n    ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2014-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2002",
        "title": "Discovering Beaten Paths in Collaborative Ontology-Engineering Projects using Markov Chains",
        "authors": [
            "Simon Walk",
            "Philipp Singer",
            "Markus Strohmaier",
            "Tania Tudorache",
            "Mark A. Musen",
            "Natalya F. Noy"
        ],
        "abstract": "Biomedical taxonomies, thesauri and ontologies in the form of the International Classification of Diseases (ICD) as a taxonomy or the National Cancer Institute Thesaurus as an OWL-based ontology, play a critical role in acquiring, representing and processing information about human health. With increasing adoption and relevance, biomedical ontologies have also significantly increased in size. For example, the 11th revision of the ICD, which is currently under active development by the WHO contains nearly 50,000 classes representing a vast variety of different diseases and causes of death. This evolution in terms of size was accompanied by an evolution in the way ontologies are engineered. Because no single individual has the expertise to develop such large-scale ontologies, ontology-engineering projects have evolved from small-scale efforts involving just a few domain experts to large-scale projects that require effective collaboration between dozens or even hundreds of experts, practitioners and other stakeholders. Understanding how these stakeholders collaborate will enable us to improve editing environments that support such collaborations. We uncover how large ontology-engineering projects, such as the ICD in its 11th revision, unfold by analyzing usage logs of five different biomedical ontology-engineering projects of varying sizes and scopes using Markov chains. We discover intriguing interaction patterns (e.g., which properties users subsequently change) that suggest that large collaborative ontology-engineering projects are governed by a few general principles that determine and drive development. From our analysis, we identify commonalities and differences between different projects that have implications for project managers, ontology editors, developers and contributors working on collaborative ontology-engineering projects and tools in the biomedical domain.\n    ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2483",
        "title": "Counting Markov Blanket Structures",
        "authors": [
            "Shyam Visweswaran",
            "Gregory F. Cooper"
        ],
        "abstract": "Learning Markov blanket (MB) structures has proven useful in performing feature selection, learning Bayesian networks (BNs), and discovering causal relationships. We present a formula for efficiently determining the number of MB structures given a target variable and a set of other variables. As expected, the number of MB structures grows exponentially. However, we show quantitatively that there are many fewer MB structures that contain the target variable than there are BN structures that contain it. In particular, the ratio of BN structures to MB structures appears to increase exponentially in the number of variables.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2676",
        "title": "A New Optimal Stepsize For Approximate Dynamic Programming",
        "authors": [
            "Ilya O. Ryzhov",
            "Peter I. Frazier",
            "Warren B. Powell"
        ],
        "abstract": "Approximate dynamic programming (ADP) has proven itself in a wide range of applications spanning large-scale transportation problems, health care, revenue management, and energy systems. The design of effective ADP algorithms has many dimensions, but one crucial factor is the stepsize rule used to update a value function approximation. Many operations research applications are computationally intensive, and it is important to obtain good results quickly. Furthermore, the most popular stepsize formulas use tunable parameters and can produce very poor results if tuned improperly. We derive a new stepsize rule that optimizes the prediction error in order to improve the short-term performance of an ADP algorithm. With only one, relatively insensitive tunable parameter, the new rule adapts to the level of noise in the problem and produces faster convergence in numerical experiments.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2776",
        "title": "What you need to know about the state-of-the-art computational models of object-vision: A tour through the models",
        "authors": [
            "Seyed-Mahdi Khaligh-Razavi"
        ],
        "abstract": "Models of object vision have been of great interest in computer vision and visual neuroscience. During the last decades, several models have been developed to extract visual features from images for object recognition tasks. Some of these were inspired by the hierarchical structure of primate visual system, and some others were engineered models. The models are varied in several aspects: models that are trained by supervision, models trained without supervision, and models (e.g. feature extractors) that are fully hard-wired and do not need training. Some of the models come with a deep hierarchical structure consisting of several layers, and some others are shallow and come with only one or two layers of processing. More recently, new models have been developed that are not hand-tuned but trained using millions of images, through which they learn how to extract informative task-related features. Here I will survey all these different models and provide the reader with an intuitive, as well as a more detailed, understanding of the underlying computations in each of the models.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2845",
        "title": "XML Matchers: approaches and challenges",
        "authors": [
            "Santa Agreste",
            "Pasquale De Meo",
            "Emilio Ferrara",
            "Domenico Ursino"
        ],
        "abstract": "Schema Matching, i.e. the process of discovering semantic correspondences between concepts adopted in different data source schemas, has been a key topic in Database and Artificial Intelligence research areas for many years. In the past, it was largely investigated especially for classical database models (e.g., E/R schemas, relational databases, etc.). However, in the latest years, the widespread adoption of XML in the most disparate application fields pushed a growing number of researchers to design XML-specific Schema Matching approaches, called XML Matchers, aiming at finding semantic matchings between concepts defined in DTDs and XSDs. XML Matchers do not just take well-known techniques originally designed for other data models and apply them on DTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical structure of a DTD/XSD) to improve the performance of the Schema Matching process. The design of XML Matchers is currently a well-established research area. The main goal of this paper is to provide a detailed description and classification of XML Matchers. We first describe to what extent the specificities of DTDs/XSDs impact on the Schema Matching task. Then we introduce a template, called XML Matcher Template, that describes the main components of an XML Matcher, their role and behavior. We illustrate how each of these components has been implemented in some popular XML Matchers. We consider our XML Matcher Template as the baseline for objectively comparing approaches that, at first glance, might appear as unrelated. The introduction of this template can be useful in the design of future XML Matchers. Finally, we analyze commercial tools implementing XML Matchers and introduce two challenging issues strictly related to this topic, namely XML source clustering and uncertainty management in XML Matchers.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2987",
        "title": "FAME: Face Association through Model Evolution",
        "authors": [
            "Eren Golge",
            "Pinar Duygulu"
        ],
        "abstract": "We attack the problem of learning face models for public faces from weakly-labelled images collected from web through querying a name. The data is very noisy even after face detection, with several irrelevant faces corresponding to other people. We propose a novel method, Face Association through Model Evolution (FAME), that is able to prune the data in an iterative way, for the face models associated to a name to evolve. The idea is based on capturing discriminativeness and representativeness of each instance and eliminating the outliers. The final models are used to classify faces on novel datasets with possibly different characteristics. On benchmark datasets, our results are comparable to or better than state-of-the-art studies for the task of face identification.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3247",
        "title": "Computational Aspects of Multi-Winner Approval Voting",
        "authors": [
            "Haris Aziz",
            "Serge Gaspers",
            "Joachim Gudmundsson",
            "Simon Mackenzie",
            "Nicholas Mattei",
            "Toby Walsh"
        ],
        "abstract": "We study computational aspects of three prominent voting rules that use approval ballots to elect multiple winners. These rules are satisfaction approval voting, proportional approval voting, and reweighted approval voting. We first show that computing the winner for proportional approval voting is NP-hard, closing a long standing open problem. As none of the rules are strategyproof, even for dichotomous preferences, we study various strategic aspects of the rules. In particular, we examine the computational complexity of computing a best response for both a single agent and a group of agents. In many settings, we show that it is NP-hard for an agent or agents to compute how best to vote given a fixed set of approval ballots from the other agents.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3501",
        "title": "Robots that can adapt like animals",
        "authors": [
            "Antoine Cully",
            "Jeff Clune",
            "Danesh Tarapore",
            "Jean-Baptiste Mouret"
        ],
        "abstract": "As robots leave the controlled environments of factories to autonomously function in more complex, natural environments, they will have to respond to the inevitable fact that they will become damaged. However, while animals can quickly adapt to a wide variety of injuries, current robots cannot \"think outside the box\" to find a compensatory behavior when damaged: they are limited to their pre-specified self-sensing abilities, can diagnose only anticipated failure modes, and require a pre-programmed contingency plan for every type of potential damage, an impracticality for complex robots. Here we introduce an intelligent trial and error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans. Before deployment, a robot exploits a novel algorithm to create a detailed map of the space of high-performing behaviors: This map represents the robot's intuitions about what behaviors it can perform and their value. If the robot is damaged, it uses these intuitions to guide a trial-and-error learning algorithm that conducts intelligent experiments to rapidly discover a compensatory behavior that works in spite of the damage. Experiments reveal successful adaptations for a legged robot injured in five different ways, including damaged, broken, and missing legs, and for a robotic arm with joints broken in 14 different ways. This new technique will enable more robust, effective, autonomous robots, and suggests principles that animals may use to adapt to injury.\n    ",
        "submission_date": "2014-07-13T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4832",
        "title": "Collaborative Filtering Ensemble for Personalized Name Recommendation",
        "authors": [
            "Bernat Coma-Puig",
            "Ernesto Diaz-Aviles",
            "Wolfgang Nejdl"
        ],
        "abstract": "Out of thousands of names to choose from, picking the right one for your child is a daunting task. In this work, our objective is to help parents making an informed decision while choosing a name for their baby. We follow a recommender system approach and combine, in an ensemble, the individual rankings produced by simple collaborative filtering algorithms in order to produce a personalized list of names that meets the individual parents' taste. Our experiments were conducted using real-world data collected from the query logs of 'nameling' (",
        "submission_date": "2014-07-16T00:00:00",
        "last_modified_date": "2014-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5358",
        "title": "Practical Kernel-Based Reinforcement Learning",
        "authors": [
            "Andr\u00e9 M. S. Barreto",
            "Doina Precup",
            "Joelle Pineau"
        ],
        "abstract": "Kernel-based reinforcement learning (KBRL) stands out among reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which is statistically consistent and converges to a unique solution. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation that takes into account both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also illustrate the potential of our algorithm in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data.\n    ",
        "submission_date": "2014-07-21T00:00:00",
        "last_modified_date": "2014-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5397",
        "title": "Are There Good Mistakes? A Theoretical Analysis of CEGIS",
        "authors": [
            "Susmit Jha",
            "Sanjit A. Seshia"
        ],
        "abstract": "Counterexample-guided inductive synthesis CEGIS is used to synthesize programs from a candidate space of programs. The technique is guaranteed to terminate and synthesize the correct program if the space of candidate programs is finite. But the technique may or may not terminate with the correct program if the candidate space of programs is infinite. In this paper, we perform a theoretical analysis of counterexample-guided inductive synthesis technique. We investigate whether the set of candidate spaces for which the correct program can be synthesized using CEGIS depends on the counterexamples used in inductive synthesis, that is, whether there are good mistakes which would increase the synthesis power. We investigate whether the use of minimal counterexamples instead of arbitrary counterexamples expands the set of candidate spaces of programs for which inductive synthesis can successfully synthesize a correct program. We consider two kinds of  counterexamples:  minimal counterexamples and  history bounded counterexamples. The history bounded counterexample used in any iteration of CEGIS is bounded by the examples used in previous iterations of inductive synthesis. We examine the relative change in power of inductive synthesis in both cases. We show that the synthesis technique using minimal counterexamples MinCEGIS has the same synthesis power as CEGIS but the synthesis technique using history bounded counterexamples HCEGIS has different power than that of CEGIS, but none dominates the other.\n    ",
        "submission_date": "2014-07-21T00:00:00",
        "last_modified_date": "2014-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6513",
        "title": "Convolutional Neural Associative Memories: Massive Capacity with Noise Tolerance",
        "authors": [
            "Amin Karbasi",
            "Amir Hesam Salavati",
            "Amin Shokrollahi"
        ],
        "abstract": "The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions using a network of neurons. An ideal network should have the ability to 1) learn a set of patterns as they arrive, 2) retrieve the correct patterns from noisy queries, and 3) maximize the pattern retrieval capacity while maintaining the reliability in responding to queries. The majority of work on neural associative memories has focused on designing networks capable of memorizing any set of randomly chosen patterns at the expense of limiting the retrieval capacity. In this paper, we show that if we target memorizing only those patterns that have inherent redundancy (i.e., belong to a subspace), we can obtain all the aforementioned properties. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we propose framework based on a convolutional neural network along with an iterative algorithm that learns the redundancy among the patterns. The resulting network has a retrieval capacity that is exponential in the size of the network. Moreover, the asymptotic error correction performance of our network is linear in the size of the patterns. We then ex- tend our approach to deal with patterns lie approximately in a subspace. This extension allows us to memorize datasets containing natural patterns (e.g., images). Finally, we report experimental results on both synthetic and real datasets to support our claims.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7211",
        "title": "An evolutionary solver for linear integer programming",
        "authors": [
            "Jo\u00e3o Pedro Pedroso"
        ],
        "abstract": "In this paper we introduce an evolutionary algorithm for the solution of linear integer programs. The strategy is based on the separation of the variables into the integer subset and the continuous subset; the integer variables are fixed by the evolutionary system, and the continuous ones are determined in function of them, by a linear program solver.\n",
        "submission_date": "2014-07-27T00:00:00",
        "last_modified_date": "2014-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7417",
        "title": "'Almost Sure' Chaotic Properties of Machine Learning Methods",
        "authors": [
            "Nabarun Mondal",
            "Partha P. Ghosh"
        ],
        "abstract": "It has been demonstrated earlier that universal computation is 'almost surely' chaotic. Machine learning is a form of computational fixed point iteration, iterating over the computable function space. We showcase some properties of this iteration, and establish in general that the iteration is 'almost surely' of chaotic nature. This theory explains the observation in the counter intuitive properties of deep learning methods. This paper demonstrates that these properties are going to be universal to any learning method.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8033",
        "title": "Characterization of graphs for protein structure modeling and recognition of solubility",
        "authors": [
            "Lorenzo Livi",
            "Alessandro Giuliani",
            "Alireza Sadeghian"
        ],
        "abstract": "This paper deals with the relations among structural, topological, and chemical properties of the ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8134",
        "title": "People are Strange when you're a Stranger: Impact and Influence of Bots on Social Networks",
        "authors": [
            "Luca Maria Aiello",
            "Martina Deplano",
            "Rossano Schifanella",
            "Giancarlo Ruffo"
        ],
        "abstract": "Bots are, for many Web and social media users, the source of many dangerous attacks or the carrier of unwanted messages, such as spam. Nevertheless, crawlers and software agents are a precious tool for analysts, and they are continuously executed to collect data or to test distributed applications. However, no one knows which is the real potential of a bot whose purpose is to control a community, to manipulate consensus, or to influence user behavior. It is commonly believed that the better an agent simulates human behavior in a social network, the more it can succeed to generate an impact in that community. We contribute to shed light on this issue through an online social experiment aimed to study to what extent a bot with no trust, no profile, and no aims to reproduce human behavior, can become popular and influential in a social media. Results show that a basic social probing activity can be used to acquire social relevance on the network and that the so-acquired popularity can be effectively leveraged to drive users in their social connectivity choices. We also register that our bot activity unveiled hidden social polarization patterns in the community and triggered an emotional response of individuals that brings to light subtle privacy hazards perceived by the user base.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8161",
        "title": "Market Making with Decreasing Utility for Information",
        "authors": [
            "Miroslav Dud\u00edk",
            "Rafael Frongillo",
            "Jennifer Wortman Vaughan"
        ],
        "abstract": "We study information elicitation in cost-function-based combinatorial prediction markets when the market maker's utility for information decreases over time. In the sudden revelation setting, it is known that some piece of information will be revealed to traders, and the market maker wishes to prevent guaranteed profits for trading on the sure information. In the gradual decrease setting, the market maker's utility for (partial) information decreases continuously over time. We design adaptive cost functions for both settings which: (1) preserve the information previously gathered in the market; (2) eliminate (or diminish) rewards to traders for the publicly revealed information; (3) leave the reward structure unaffected for other information; and (4) maintain the market maker's worst-case loss. Our constructions utilize mixed Bregman divergence, which matches our notion of utility for information.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0016",
        "title": "Architecture of a Web-based Predictive Editor for Controlled Natural Language Processing",
        "authors": [
            "Stephen Guy",
            "Rolf Schwitter"
        ],
        "abstract": "In this paper, we describe the architecture of a web-based predictive text editor being developed for the controlled natural language PENG$^{ASP)$. This controlled language can be used to write non-monotonic specifications that have the same expressive power as Answer Set Programs. In order to support the writing process of these specifications, the predictive text editor communicates asynchronously with the controlled natural language processor that generates lookahead categories and additional auxiliary information for the author of a specification text. The text editor can display multiple sets of lookahead categories simultaneously for different possible sentence completions, anaphoric expressions, and supports the addition of new content words to the lexicon.\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0204",
        "title": "Functional Principal Component Analysis and Randomized Sparse Clustering Algorithm for Medical Image Analysis",
        "authors": [
            "Nan Lin",
            "Junhai Jiang",
            "Shicheng Guo",
            "Momiao Xiong"
        ],
        "abstract": "Due to advances in sensors, growing large and complex medical image data have the ability to visualize the pathological change in the cellular or even the molecular level or anatomical changes in tissues and organs. As a consequence, the medical images have the potential to enhance diagnosis of disease, prediction of clinical outcomes, characterization of disease progression, management of health care and development of treatments, but also pose great methodological and computational challenges for representation and selection of features in image cluster analysis. To address these challenges, we first extend one dimensional functional principal component analysis to the two dimensional functional principle component analyses (2DFPCA) to fully capture space variation of image signals. Image signals contain a large number of redundant and irrelevant features which provide no additional or no useful information for cluster analysis. Widely used methods for removing redundant and irrelevant features are sparse clustering algorithms using a lasso-type penalty to select the features. However, the accuracy of clustering using a lasso-type penalty depends on how to select penalty parameters and a threshold for selecting features. In practice, they are difficult to determine. Recently, randomized algorithms have received a great deal of attention in big data analysis. This paper presents a randomized algorithm for accurate feature selection in image cluster analysis. The proposed method is applied to ovarian and kidney cancer histology image data from the TCGA database. The results demonstrate that the randomized feature selection method coupled with functional principal component analysis substantially outperforms the current sparse clustering algorithms in image cluster analysis.\n    ",
        "submission_date": "2014-08-01T00:00:00",
        "last_modified_date": "2014-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0703",
        "title": "Computational Analysis of Perfect-Information Position Auctions",
        "authors": [
            "David R.M Thompson",
            "Kevin Leyton-Brown"
        ],
        "abstract": "After experimentation with other designs, the major search engines converged on the weighted, generalized second-price auction (wGSP) for selling keyword advertisements. Notably, this convergence occurred before position auctions were well understood (or, indeed, widely studied) theoretically. While much progress has been made since, theoretical analysis is still not able to settle the question of why search engines found wGSP preferable to other position auctions. We approach this question in a new way, adopting a new analytical paradigm we dub \"computational mechanism analysis.\" By sampling position auction games from a given distribution, encoding them in a computationally efficient representation language, computing their Nash equilibria, and then calculating economic quantities of interest, we can quantitatively answer questions that theoretical methods have not. We considered seven widely studied valuation models from the literature and three position auction variants (generalized first price, unweighted generalized second price, and wGSP). We found that wGSP consistently showed the best ads of any position auction, measured both by social welfare and by relevance (expected number of clicks). Even in models where wGSP was already known to have bad worse-case efficiency, we found that it almost always performed well on average. In contrast, we found that revenue was extremely variable across auction mechanisms, and was highly sensitive to equilibrium selection, the preference model, and the valuation distribution.\n    ",
        "submission_date": "2014-08-04T00:00:00",
        "last_modified_date": "2014-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1377",
        "title": "The Mathematical Abstraction Theory, The Fundamentals for Knowledge Representation and Self-Evolving Autonomous Problem Solving Systems",
        "authors": [
            "Seppo Ilari Tirri"
        ],
        "abstract": "The intention of the present study is to establish the mathematical fundamentals for automated problem solving essentially targeted for robotics by approaching the task universal algebraically introducing knowledge as realizations of generalized free algebra based nets, graphs with gluing forms connecting in- and out-edges to nodes. Nets are caused to undergo transformations in conceptual level by type wise differentiated intervening net rewriting systems dispersing problems to abstract parts, matching being determined by substitution relations. Achieved sets of conceptual nets constitute congruent classes. New results are obtained within construction of problem solving systems where solution algorithms are derived parallel with other candidates applied to the same net classes. By applying parallel transducer paths consisting of net rewriting systems to net classes congruent quotient algebras are established and the manifested class rewriting comprises all solution candidates whenever produced nets are in anticipated languages liable to acceptance of net automata. Furthermore new solutions will be added to the set of already known ones thus expanding the solving power in the forthcoming. Moreover special attention is set on universal abstraction, thereof generation by net block homomorphism, consequently multiple order solving systems and the overall decidability of the set of the solutions. By overlapping presentation of nets new abstraction relation among nets is formulated alongside with consequent alphabetical net block renetting system proportional to normal forms of renetting systems regarding the operational power. A new structure in self-evolving problem solving is established via saturation by groups of equivalence relations and iterative closures of generated quotient transducer algebras over the whole evolution.\n    ",
        "submission_date": "2014-08-06T00:00:00",
        "last_modified_date": "2014-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1600",
        "title": "Change Impact Analysis Based Regression Testing of Web Services",
        "authors": [
            "Animesh Chaturvedi"
        ],
        "abstract": "Reducing the effort required to make changes in web services is one of the primary goals in web service projects maintenance and evolution. Normally, functional and non-functional testing of a web service is performed by testing the operations specified in its WSDL. The regression testing is performed by identifying the changes made thereafter to the web service code and the WSDL. In this thesis, we present a tool-supported approach to perform efficient regression testing of web services. By representing a web service as a directed graph of WSDL elements, we identify and gathers the changed portions of the graph and use this information to reduce regression testing efforts. Specifically, we identify, categorize, and capture the web service testing needs in two different ways, namely, Operationalized Regression Testing of Web Service (ORTWS) and Parameterized Regression Testing of Web Service (PRTWS). Both of the approach can be combined to reduce the regression testing efforts in the web service project. The proposed approach is prototyped as a tool, named as Automatic Web Service Change Management (AWSCM), which helps in selecting the relevant test cases to construct reduced test suite from the old test suite. We present few case studies on different web service projects to demonstrate the applicability of the proposed tool. The reduction in the effort for regression testing of web service is also estimated.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1993",
        "title": "An Evasion and Counter-Evasion Study in Malicious Websites Detection",
        "authors": [
            "Li Xu",
            "Zhenxin Zhan",
            "Shouhuai Xu",
            "Keyin Ye"
        ],
        "abstract": "Malicious websites are a major cyber attack vector, and effective detection of them is an important cyber defense task. The main defense paradigm in this regard is that the defender uses some kind of machine learning algorithms to train a detection model, which is then used to classify websites in question. Unlike other settings, the following issue is inherent to the problem of malicious websites detection: the attacker essentially has access to the same data that the defender uses to train its detection models. This 'symmetry' can be exploited by the attacker, at least in principle, to evade the defender's detection models. In this paper, we present a framework for characterizing the evasion and counter-evasion interactions between the attacker and the defender, where the attacker attempts to evade the defender's detection models by taking advantage of this symmetry. Within this framework, we show that an adaptive attacker can make malicious websites evade powerful detection models, but proactive training can be an effective counter-evasion defense mechanism. The framework is geared toward the popular detection model of decision tree, but can be adapted to accommodate other classifiers.\n    ",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2014-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2045",
        "title": "Efficient Clustering with Limited Distance Information",
        "authors": [
            "Konstantin Voevodski",
            "Maria-Florina Balcan",
            "Heiko Roglin",
            "Shang-Hua Teng",
            "Yu Xia"
        ],
        "abstract": "Given a point set S and an unknown metric d on S, we study the problem of efficiently partitioning S into k clusters while querying few distances between the points. In our model we assume that we have access to one versus all queries that given a point s 2 S return the distances between s and all other points. We show that given a natural assumption about the structure of the instance, we can efficiently find an accurate clustering using only O(k) distance queries. We use our algorithm to cluster proteins by sequence similarity. This setting nicely fits our model because we can use a fast sequence database search program to query a sequence against an entire dataset. We conduct an empirical study that shows that even though we query a small fraction of the distances between the points, we produce clusterings that are close to a desired clustering given by manual classification.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2196",
        "title": "Exponentiated Gradient Exploration for Active Learning",
        "authors": [
            "Djallel Bouneffouf"
        ],
        "abstract": "Active learning strategies respond to the costly labelling task in a supervised classification by selecting the most useful unlabelled examples in training a predictive model. Many conventional active learning algorithms focus on refining the decision boundary, rather than exploring new regions that can be more informative. In this setting, we propose a sequential algorithm named EG-Active that can improve any Active learning algorithm by an optimal random exploration. Experimental results show a statistically significant and appreciable improvement in the performance of our new approach over the existing active feedback methods.\n    ",
        "submission_date": "2014-08-10T00:00:00",
        "last_modified_date": "2014-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2287",
        "title": "In principle determination of generic priors",
        "authors": [
            "Cael L. Hasse"
        ],
        "abstract": "Probability theory as extended logic is completed such that essentially any probability may be determined. This is done by considering propositional logic (as opposed to predicate logic) as syntactically suffcient and imposing a symmetry from propositional logic. It is shown how the notions of `possibility' and `property' may be suffciently represented in propositional logic such that 1) the principle of indifference drops out and becomes essentially combinatoric in nature and 2) one may appropriately represent assumptions where one assumes there is a space of possibilities but does not assume the size of the space.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2014-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2380",
        "title": "Video Face Editing Using Temporal-Spatial-Smooth Warping",
        "authors": [
            "Xiaoyan Li",
            "Dacheng Tao"
        ],
        "abstract": "Editing faces in videos is a popular yet challenging aspect of computer vision and graphics, which encompasses several applications including facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. Simply applying image-based warping algorithms to video-based face editing produces temporal incoherence in the synthesized videos because it is impossible to consistently localize facial features in two frames representing two different faces in two different videos (or even two consecutive frames representing the same face in one video). Therefore, high performance face editing usually requires significant manual manipulation. In this paper we propose a novel temporal-spatial-smooth warping (TSSW) algorithm to effectively exploit the temporal information in two consecutive frames, as well as the spatial smoothness within each frame. TSSW precisely estimates two control lattices in the horizontal and vertical directions respectively from the corresponding control lattices in the previous frame, by minimizing a novel energy function that unifies a data-driven term, a smoothness term, and feature point constraints. Corresponding warping surfaces then precisely map source frames to the target frames. Experimental testing on facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation demonstrates that the proposed approaches can effectively preserve spatial smoothness and temporal coherence in editing facial geometry, skin detail, identity, and expression, which outperform the existing face editing methods. In particular, TSSW is robust to subtly inaccurate localization of feature points and is a vast improvement over image-based warping methods.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2014-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2447",
        "title": "Fuzzy inequational logic",
        "authors": [
            "Vilem Vychodil"
        ],
        "abstract": "We present a logic for reasoning about graded inequalities which generalizes the ordinary inequational logic used in universal algebra. The logic deals with atomic predicate formulas of the form of inequalities between terms and formalizes their semantic entailment and provability in graded setting which allows to draw partially true conclusions from partially true assumptions. We follow the Pavelka approach and define general degrees of semantic entailment and provability using complete residuated lattices as structures of truth degrees. We prove the logic is Pavelka-style complete. Furthermore, we present a logic for reasoning about graded if-then rules which is obtained as particular case of the general result.\n    ",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2466",
        "title": "Controlled Natural Language Processing as Answer Set Programming: an Experiment",
        "authors": [
            "Rolf Schwitter"
        ],
        "abstract": "Most controlled natural languages (CNLs) are processed with the help of a pipeline architecture that relies on different software components. We investigate in this paper in an experimental way how well answer set programming (ASP) is suited as a unifying framework for parsing a CNL, deriving a formal representation for the resulting syntax trees, and for reasoning with that representation. We start from a list of input tokens in ASP notation and show how this input can be transformed into a syntax tree using an ASP grammar and then into reified ASP rules in form of a set of facts. These facts are then processed by an ASP meta-interpreter that allows us to infer new knowledge.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2014-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2467",
        "title": "Matrix Completion under Interval Uncertainty",
        "authors": [
            "Jakub Marecek",
            "Peter Richtarik",
            "Martin Takac"
        ],
        "abstract": "Matrix completion under interval uncertainty can be cast as matrix completion with element-wise box constraints. We present an efficient alternating-direction parallel coordinate-descent method for the problem. We show that the method outperforms any other known method on a benchmark in image in-painting in terms of signal-to-noise ratio, and that it provides high-quality solutions for an instance of collaborative filtering with 100,198,805 recommendations within 5 minutes.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3100",
        "title": "A Bayesian Probability Calculus for Density Matrices",
        "authors": [
            "Manfred K. Warmuth",
            "Dima Kuzmin"
        ],
        "abstract": "One of the main concepts in quantum physics is a density matrix, which is a symmetric positive definite matrix of trace one. Finite probability distributions are a special case where the density matrix is restricted to be diagonal. Density matrices are mixtures of dyads, where a dyad has the form uu' for any any unit column vector u. These unit vectors are the elementary events of the generalized probability space. Perhaps the simplest case to see that something unusual is going on is the case of uniform density matrix, i.e. 1/n times identity. This matrix assigns probability 1/n to every unit vector, but of course there are infinitely many of them. The new normalization rule thus says that sum of probabilities over any orthonormal basis of directions is one. We develop a probability calculus based on these more general distributions that includes definitions of joints, conditionals and formulas that relate these, i.e. analogs of the theorem of total probability, various Bayes rules for the calculation of posterior density matrices, etc. The resulting calculus parallels the familiar 'classical' probability calculus and always retains the latter as a special case when all matrices are diagonal.\n",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3873",
        "title": "Classifying sequences by the optimized dissimilarity space embedding approach: a case study on the solubility analysis of the E. coli proteome",
        "authors": [
            "Lorenzo Livi",
            "Antonello Rizzi",
            "Alireza Sadeghian"
        ],
        "abstract": "We evaluate a version of the recently-proposed classification system named Optimized Dissimilarity Space Embedding (ODSE) that operates in the input space of sequences of generic objects. The ODSE system has been originally presented as a classification system for patterns represented as labeled graphs. However, since ODSE is founded on the dissimilarity space representation of the input data, the classifier can be easily adapted to any input domain where it is possible to define a meaningful dissimilarity measure. Here we demonstrate the effectiveness of the ODSE classifier for sequences by considering an application dealing with the recognition of the solubility degree of the Escherichia coli proteome. Solubility, or analogously aggregation propensity, is an important property of protein molecules, which is intimately related to the mechanisms underlying the chemico-physical process of folding. Each protein of our dataset is initially associated with a solubility degree and it is represented as a sequence of symbols, denoting the 20 amino acid residues. The herein obtained computational results, which we stress that have been achieved with no context-dependent tuning of the ODSE system, confirm the validity and generality of the ODSE-based approach for structured data classification.\n    ",
        "submission_date": "2014-08-17T00:00:00",
        "last_modified_date": "2015-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3934",
        "title": "On Detecting Messaging Abuse in Short Text Messages using Linguistic and Behavioral patterns",
        "authors": [
            "Alejandro Mosquera",
            "Lamine Aouad",
            "Slawomir Grzonkowski",
            "Dylan Morss"
        ],
        "abstract": "The use of short text messages in social media and instant messaging has become a popular communication channel during the last years. This rising popularity has caused an increment in messaging threats such as spam, phishing or malware as well as other threats. The processing of these short text message threats could pose additional challenges such as the presence of lexical variants, SMS-like contractions or advanced obfuscations which can degrade the performance of traditional filtering solutions. By using a real-world SMS data set from a large telecommunications operator from the US and a social media corpus, in this paper we analyze the effectiveness of machine learning filters based on linguistic and behavioral patterns in order to detect short text spam and abusive users in the network. We have also explored different ways to deal with short text message challenges such as tokenization and entity detection by using text normalization and substring clustering techniques. The obtained results show the validity of the proposed solution by enhancing baseline approaches.\n    ",
        "submission_date": "2014-08-18T00:00:00",
        "last_modified_date": "2014-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4628",
        "title": "Incremental Cardinality Constraints for MaxSAT",
        "authors": [
            "Ruben Martins",
            "Saurabh Joshi",
            "Vasco Manquinho",
            "Ines Lynce"
        ],
        "abstract": "Maximum Satisfiability (MaxSAT) is an optimization variant of the Boolean Satisfiability (SAT) problem. In general, MaxSAT algorithms perform a succession of SAT solver calls to reach an optimum solution making extensive use of cardinality constraints. Many of these algorithms are non-incremental in nature, i.e. at each iteration the formula is rebuilt and no knowledge is reused from one iteration to another. In this paper, we exploit the knowledge acquired across iterations using novel schemes to use cardinality constraints in an incremental fashion. We integrate these schemes with several MaxSAT algorithms. Our experimental results show a significant performance boost for these algo- rithms as compared to their non-incremental counterparts. These results suggest that incremental cardinality constraints could be beneficial for other constraint solving domains.\n    ",
        "submission_date": "2014-08-20T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4901",
        "title": "A Study of Proxies for Shapley Allocations of Transport Costs",
        "authors": [
            "Haris Aziz",
            "Casey Cahan",
            "Charles Gretton",
            "Phillip Kilby",
            "Nicholas Mattei",
            "Toby Walsh"
        ],
        "abstract": "We propose and evaluate a number of solutions to the problem of calculating the cost to serve each location in a single-vehicle transport setting. Such cost to serve analysis has application both strategically and operationally in transportation. The problem is formally given by the traveling salesperson game (TSG), a cooperative total utility game in which agents correspond to locations in a traveling salesperson problem (TSP). The cost to serve a location is an allocated portion of the cost of an optimal tour. The Shapley value is one of the most important normative division schemes in cooperative games, giving a principled and fair allocation both for the TSG and more generally. We consider a number of direct and sampling-based procedures for calculating the Shapley value, and present the first proof that approximating the Shapley value of the TSG within a constant factor is NP-hard. Treating the Shapley value as an ideal baseline allocation, we then develop six proxies for that value which are relatively easy to compute. We perform an experimental evaluation using Synthetic Euclidean games as well as games derived from real-world tours calculated for fast-moving consumer goods scenarios. Our experiments show that several computationally tractable allocation techniques correspond to good proxies for the Shapley value.\n    ",
        "submission_date": "2014-08-21T00:00:00",
        "last_modified_date": "2014-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5246",
        "title": "Improving the Interpretability of Support Vector Machines-based Fuzzy Rules",
        "authors": [
            "Duc-Hien Nguyen",
            "Manh-Thanh Le"
        ],
        "abstract": "Support vector machines (SVMs) and fuzzy rule systems are functionally equivalent under some conditions. Therefore, the learning algorithms developed in the field of support vector machines can be used to adapt the parameters of fuzzy systems. Extracting fuzzy models from support vector machines has the inherent advantage that the model does not need to determine the number of rules in advance. However, after the support vector machine learning, the complexity is usually high, and interpretability is also impaired. This paper not only proposes a complete framework for extracting interpretable SVM-based fuzzy modeling, but also provides optimization issues of the models. Simulations examples are given to embody the idea of this paper.\n    ",
        "submission_date": "2014-08-22T00:00:00",
        "last_modified_date": "2014-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5492",
        "title": "Towards Decision Support Technology Platform for Modular Systems",
        "authors": [
            "Mark Sh. Levin"
        ],
        "abstract": "The survey methodological paper addresses a glance to a general decision support platform technology for modular systems (modular/composite alterantives/solutions) in various applied domains. The decision support platform consists of seven basic combinatorial engineering frameworks (system synthesis, system modeling, evaluation, detection of bottleneck, improvement/extension, multistage design, combinatorial evolution and forecasting). The decision support platform is based on decision support procedures (e.g., multicriteria selection/sorting, clustering), combinatorial optimization problems (e.g., knapsack, multiple choice problem, clique, assignment/allocation, covering, spanning trees), and their combinations. The following is described: (1) general scheme of the decision support platform technology; (2) brief descriptions of modular (composite) systems (or composite alternatives); (3) trends in moving from chocie/selection of alternatives to processing of composite alternatives which correspond to hierarchical modular products/systems; (4) scheme of resource requirements (i.e., human, information-computer); and (5) basic combinatorial engineering frameworks and their applications in various domains.\n    ",
        "submission_date": "2014-08-23T00:00:00",
        "last_modified_date": "2014-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6350",
        "title": "Definition and properties to assess multi-agent environments as social intelligence tests",
        "authors": [
            "Javier Insa-Cabrera",
            "Jos\u00e9 Hern\u00e1ndez-Orallo"
        ],
        "abstract": "Social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour. The amalgamation of these traits is then used to configure the intuitive notion of social intelligence. Instead, in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents, and we assess and derive tests from it. This definition makes several dependencies explicit: (1) the definition depends on the choice (and weight) of environments and agents, (2) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams, (3) the definition mostly depends on the abilities of other agents, and (4) the actual difference between social intelligence and general intelligence (or other abilities) depends on these choices. As a result, we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour (such as action and reward dependency and anticipation on competitive/cooperative behaviours) are met as well as some other more instrumental properties (such as secernment, boundedness, symmetry, validity, reliability, efficiency), which are convenient to convert the definition into a practical test. From the definition and the formalised properties, we take a look at several representative multi-agent environments, tests and games to see whether they meet these properties.\n    ",
        "submission_date": "2014-08-27T00:00:00",
        "last_modified_date": "2014-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0302",
        "title": "Team Behavior in Interactive Dynamic Influence Diagrams with Applications to Ad Hoc Teams",
        "authors": [
            "Muthukumaran Chandrasekaran",
            "Prashant Doshi",
            "Yifeng Zeng",
            "Yingke Chen"
        ],
        "abstract": "Planning for ad hoc teamwork is challenging because it involves agents collaborating without any prior coordination or communication. The focus is on principled methods for a single agent to cooperate with others. This motivates investigating the ad hoc teamwork problem in the context of individual decision making frameworks. However, individual decision making in multiagent settings faces the task of having to reason about other agents' actions, which in turn involves reasoning about others. An established approximation that operationalizes this approach is to bound the infinite nesting from below by introducing level 0 models. We show that a consequence of the finitely-nested modeling is that we may not obtain optimal team solutions in cooperative settings. We address this limitation by including models at level 0 whose solutions involve learning. We demonstrate that the learning integrated into planning in the context of interactive dynamic influence diagrams facilitates optimal team behavior, and is applicable to ad hoc teamwork.\n    ",
        "submission_date": "2014-09-01T00:00:00",
        "last_modified_date": "2014-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0791",
        "title": "Feature Selection in Conditional Random Fields for Map Matching of GPS Trajectories",
        "authors": [
            "Jian Yang",
            "Liqiu Meng"
        ],
        "abstract": "Map matching of the GPS trajectory serves the purpose of recovering the original route on a road network from a sequence of noisy GPS observations. It is a fundamental technique to many Location Based Services. However, map matching of a low sampling rate on urban road network is still a challenging task. In this paper, the characteristics of Conditional Random Fields with regard to inducing many contextual features and feature selection are explored for the map matching of the GPS trajectories at a low sampling rate. Experiments on a taxi trajectory dataset show that our method may achieve competitive results along with the success of reducing model complexity for computation-limited applications.\n    ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0813",
        "title": "Friendly Artificial Intelligence: the Physics Challenge",
        "authors": [
            "Max Tegmark"
        ],
        "abstract": "Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent \"Friendly AI\", designed to safeguard humanity and its values. I argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is \"meaning\" in a particle arrangement? What is \"life\"? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.\n    ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0824",
        "title": "Deontic modality based on preference",
        "authors": [
            "Daniel Osherson",
            "Scott Weinstein"
        ],
        "abstract": "Deontic modalities are here defined in terms of the preference relation explored in our previous work (Osherson and Weinstein, 2012). Some consequences of the system are discussed.\n    ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0925",
        "title": "Bypassing Captcha By Machine A Proof For Passing The Turing Test",
        "authors": [
            "Ahmad B. A. Hassanat"
        ],
        "abstract": "For the last ten years, CAPTCHAs have been widely used by websites to prevent their data being automatically updated by machines. By supposedly allowing only humans to do so, CAPTCHAs take advantage of the reverse Turing test (TT), knowing that humans are more intelligent than machines. Generally, CAPTCHAs have defeated machines, but things are changing rapidly as technology improves. Hence, advanced research into optical character recognition (OCR) is overtaking attempts to strengthen CAPTCHAs against machine-based attacks. This paper investigates the immunity of CAPTCHA, which was built on the failure of the TT. We show that some CAPTCHAs are easily broken using a simple OCR machine built for the purpose of this study. By reviewing other techniques, we show that even more difficult CAPTCHAs can be broken using advanced OCR machines. Current advances in OCR should enable machines to pass the TT in the image recognition domain, which is exactly where machines are seeking to overcome CAPTCHAs. We enhance traditional CAPTCHAs by employing not only characters, but also natural language and multiple objects within the same CAPTCHA. The proposed CAPTCHAs might be able to hold out against machines, at least until the advent of a machine that passes the TT completely.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1455",
        "title": "Unsynthesizable Cores - Minimal Explanations for Unsynthesizable High-Level Robot Behaviors",
        "authors": [
            "Vasumathi Raman",
            "Hadas Kress-Gazit"
        ],
        "abstract": "With the increasing ubiquity of multi-capable, general-purpose robots arises the need for enabling non-expert users to command these robots to perform complex high-level tasks. To this end, high-level robot control has seen the application of formal methods to automatically synthesize correct-by-construction controllers from user-defined specifications; synthesis fails if and only if there exists no controller that achieves the specified behavior. Recent work has also addressed the challenge of providing easy-to-understand feedback to users when a specification fails to yield a corresponding controller. Existing techniques provide feedback on portions of the specification that cause the failure, but do so at a coarse granularity. This work presents techniques for refining this feedback, extracting minimal explanations of unsynthesizability.\n    ",
        "submission_date": "2014-09-04T00:00:00",
        "last_modified_date": "2014-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2287",
        "title": "Variational Inference for Uncertainty on the Inputs of Gaussian Process Models",
        "authors": [
            "Andreas C. Damianou",
            "Michalis K. Titsias",
            "Neil D. Lawrence"
        ],
        "abstract": "The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximizing an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain inputs and semi-supervised Gaussian processes. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.\n    ",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2014-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2399",
        "title": "Prioritized Planning Algorithms for Trajectory Coordination of Multiple Mobile Robots",
        "authors": [
            "Michal \u010c\u00e1p",
            "Peter Nov\u00e1k",
            "Alexander Kleiner",
            "Martin Seleck\u00fd"
        ],
        "abstract": "An important capability of autonomous multi-robot systems is to prevent collision among the individual robots. One approach to this problem is to plan conflict-free trajectories and let each of the robots follow its pre-planned trajectory. A widely used practical method for multi-robot trajectory planning is prioritized planning, which has been shown to be effective in practice, but is in general incomplete. Formal analysis of instances that are provably solvable by prioritized planning is still missing. Moreover, prioritized planning is a centralized algorithm, which may be in many situations undesirable.\n",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2014-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2897",
        "title": "Co-adaptation in a Handwriting Recognition System",
        "authors": [
            "Sunsern Cheamanunkul",
            "Yoav Freund"
        ],
        "abstract": "Handwriting is a natural and versatile method for human-computer interaction, especially on small mobile devices such as smart phones. However, as handwriting varies significantly from person to person, it is difficult to design handwriting recognizers that perform well for all users. A natural solution is to use machine learning to adapt the recognizer to the user. One complicating factor is that, as the computer adapts to the user, the user also adapts to the computer and probably changes their handwriting. This paper investigates the dynamics of co-adaptation, a process in which both the computer and the user are adapting their behaviors in order to improve the speed and accuracy of the communication through handwriting. We devised an information-theoretic framework for quantifying the efficiency of a handwriting system where the system includes both the user and the computer. Using this framework, we analyzed data collected from an adaptive handwriting recognition system and characterized the impact of machine adaptation and of human adaptation. We found that both machine adaptation and human adaptation have significant impact on the input rate and must be considered together in order to improve the efficiency of the system as a whole.\n    ",
        "submission_date": "2014-09-09T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3717",
        "title": "Probabilistic Selection in AgentSpeak(L)",
        "authors": [
            "Francisco Coelho",
            "Vitor Nogueira"
        ],
        "abstract": "Agent programming is mostly a symbolic discipline and, as such, draws little benefits from probabilistic areas as machine learning and graphical models. However, the greatest objective of agent research is the achievement of autonomy in dynamical and complex environments --- a goal that implies embracing uncertainty and therefore the entailed representations, algorithms and techniques. This paper proposes an innovative and conflict free two layer approach to agent programming that uses already established methods and tools from both symbolic and probabilistic artificial intelligence. Moreover, this framework is illustrated by means of a widely used agent programming example, GoldMiners.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3836",
        "title": "Hardness of parameter estimation in graphical models",
        "authors": [
            "Guy Bresler",
            "David Gamarnik",
            "Devavrat Shah"
        ],
        "abstract": "We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan (2008)) but no proof was known.\n",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4936",
        "title": "Ensembles of Random Sphere Cover Classifiers",
        "authors": [
            "Anthony Bagnall",
            "Reda Younsi"
        ],
        "abstract": "We propose and evaluate alternative ensemble schemes for a new instance based learning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fuses instances into spheres, then bases classification on distance to spheres rather than distance to instances. The randomised nature of RSC makes it ideal for use in ensembles. We propose two ensemble methods tailored to the RSC classifier; $\\alpha \\beta$RSE, an ensemble based on instance resampling and $\\alpha$RSSE, a subspace ensemble. We compare $\\alpha \\beta$RSE and $\\alpha$RSSE to tree based ensembles on a set of UCI datasets and demonstrates that RSC ensembles perform significantly better than some of these ensembles, and not significantly worse than the others. We demonstrate via a case study on six gene expression data sets that $\\alpha$RSSE can outperform other subspace ensemble methods on high dimensional data when used in conjunction with an attribute filter. Finally, we perform a set of Bias/Variance decomposition experiments to analyse the source of improvement in comparison to a base classifier.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5326",
        "title": "Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX): Comparing multi-electrode recordings from simulated and biological mammalian cortical tissue",
        "authors": [
            "Richard J. Tomsett",
            "Matt Ainsworth",
            "Alexander Thiele",
            "Mehdi Sanayei",
            "Xing Chen",
            "Alwin Gieselmann",
            "Miles A. Whittington",
            "Mark O. Cunningham",
            "Marcus Kaiser"
        ],
        "abstract": "Local field potentials (LFPs) sampled with extracellular electrodes are frequently used as a measure of population neuronal activity. However, relating such measurements to underlying neuronal behaviour and connectivity is non-trivial. To help study this link, we developed the Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX). We first identified a reduced neuron model that retained the spatial and frequency filtering characteristics of extracellular potentials from neocortical neurons. We then developed VERTEX as an easy-to-use Matlab tool for simulating LFPs from large populations (>100 000 neurons). A VERTEX-based simulation successfully reproduced features of the LFPs from an in vitro multi-electrode array recording of macaque neocortical tissue. Our model, with virtual electrodes placed anywhere in 3D, allows direct comparisons with the in vitro recording setup. We envisage that VERTEX will stimulate experimentalists, clinicians, and computational neuroscientists to use models to understand the mechanisms underlying measured brain dynamics in health and disease.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5758",
        "title": "Effects of Coupling in Human-Virtual Agent Body Interaction",
        "authors": [
            "Elisabetta Bevacqua",
            "Sankovic Igor",
            "Maatalaoui Ayoub",
            "A. N\u00e9d\u00e9lec",
            "Pierre De Loor"
        ],
        "abstract": "This paper presents a study of the dynamic coupling between a user and a virtual character during body interaction. Coupling is directly linked with other dimensions, such as co-presence, engagement, and believability, and was measured in an experiment that allowed users to describe their subjective feelings about those dimensions of interest. The experiment was based on a theatrical game involving the imitation of slow upper-body movements and the proposal of new movements by the user and virtual agent. The agent's behaviour varied in autonomy: the agent could limit itself to imitating the user's movements only, initiate new movements, or combine both behaviours. After the game, each participant completed a questionnaire regarding their engagement in the interaction, their subjective feeling about the co-presence of the agent, etc. Based on four main dimensions of interest, we tested several hypotheses against our experimental results, which are discussed here.\n    ",
        "submission_date": "2014-09-19T00:00:00",
        "last_modified_date": "2014-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6041",
        "title": "Domain Adaptive Neural Networks for Object Recognition",
        "authors": [
            "Muhammad Ghifary",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang"
        ],
        "abstract": "We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks.\n    ",
        "submission_date": "2014-09-21T00:00:00",
        "last_modified_date": "2014-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7291",
        "title": "Optimizing Hybrid Spreading in Metapopulations",
        "authors": [
            "Changwang Zhang",
            "Shi Zhou",
            "Joel C. Miller",
            "Ingemar J. Cox",
            "Benjamin M. Chain"
        ],
        "abstract": "Epidemic spreading phenomena are ubiquitous in nature and society. Examples include the spreading of diseases, information, and computer viruses. Epidemics can spread by local spreading, where infected nodes can only infect a limited set of direct target nodes and global spreading, where an infected node can infect every other node. In reality, many epidemics spread using a hybrid mixture of both types of spreading. In this study we develop a theoretical framework for studying hybrid epidemics, and examine the optimum balance between spreading mechanisms in terms of achieving the maximum outbreak size. We show the existence of critically hybrid epidemics where neither spreading mechanism alone can cause a noticeable spread but a combination of the two spreading mechanisms would produce an enormous outbreak. Our results provide new strategies for maximising beneficial epidemics and estimating the worst outcome of damaging hybrid epidemics.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7403",
        "title": "Optimal high-level descriptions of dynamical systems",
        "authors": [
            "David H. Wolpert",
            "Joshua A. Grochow",
            "Eric Libby",
            "Simon DeDeo"
        ],
        "abstract": "To analyze high-dimensional systems, many fields in science and engineering rely on high-level descriptions, sometimes called \"macrostates,\" \"coarse-grainings,\" or \"effective theories\". Examples of such descriptions include the thermodynamic properties of a large collection of point particles undergoing reversible dynamics, the variables in a macroeconomic model describing the individuals that participate in an economy, and the summary state of a cell composed of a large set of biochemical networks.\n",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7580",
        "title": "Gradient-based Taxis Algorithms for Network Robotics",
        "authors": [
            "Christian Blum",
            "Verena V. Hafner"
        ],
        "abstract": "Finding the physical location of a specific network node is a prototypical task for navigation inside a wireless network. In this paper, we consider in depth the implications of wireless communication as a measurement input of gradient-based taxis algorithms. We discuss how gradients can be measured and determine the errors of this estimation. We then introduce a gradient-based taxis algorithm as an example of a family of gradient-based, convergent algorithms and discuss its convergence in the context of network robotics. We also conduct an exemplary experiment to show how to overcome some of the specific problems related to network robotics. Finally, we show how to adapt this framework to more complex objectives.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2014-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7985",
        "title": "The Utility of Text: The Case of Amicus Briefs and the Supreme Court",
        "authors": [
            "Yanchuan Sim",
            "Bryan Routledge",
            "Noah A. Smith"
        ],
        "abstract": "We explore the idea that authoring a piece of text is an act of maximizing one's expected utility. To make this idea concrete, we consider the societally important decisions of the Supreme Court of the United States. Extensive past work in quantitative political science provides a framework for empirically modeling the decisions of justices and how they relate to text. We incorporate into such a model texts authored by amici curiae (\"friends of the court\" separate from the litigants) who seek to weigh in on the decision, then explicitly model their goals in a random utility model. We demonstrate the benefits of this approach in improved vote prediction and the ability to perform counterfactual analysis.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8484",
        "title": "An agent-driven semantical identifier using radial basis neural networks and reinforcement learning",
        "authors": [
            "Christian Napoli",
            "Giuseppe Pappalardo",
            "Emiliano Tramontana"
        ],
        "abstract": "Due to the huge availability of documents in digital form, and the deception possibility raise bound to the essence of digital documents and the way they are spread, the authorship attribution problem has constantly increased its relevance. Nowadays, authorship attribution,for both information retrieval and analysis, has gained great importance in the context of security, trust and copyright preservation. This work proposes an innovative multi-agent driven machine learning technique that has been developed for authorship attribution. By means of a preprocessing for word-grouping and time-period related analysis of the common lexicon, we determine a bias reference level for the recurrence frequency of the words within analysed texts, and then train a Radial Basis Neural Networks (RBPNN)-based classifier to identify the correct author. The main advantage of the proposed approach lies in the generality of the semantic analysis, which can be applied to different contexts and lexical domains, without requiring any modification. Moreover, the proposed system is able to incorporate an external input, meant to tune the classifier, and then self-adjust by means of continuous learning reinforcement.\n    ",
        "submission_date": "2014-09-30T00:00:00",
        "last_modified_date": "2014-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8498",
        "title": "Non-myopic learning in repeated stochastic games",
        "authors": [
            "Jacob W. Crandall"
        ],
        "abstract": "In repeated stochastic games (RSGs), an agent must quickly adapt to the behavior of previously unknown associates, who may themselves be learning. This machine-learning problem is particularly challenging due, in part, to the presence of multiple (even infinite) equilibria and inherently large strategy spaces. In this paper, we introduce a method to reduce the strategy space of two-player general-sum RSGs to a handful of expert strategies. This process, called Mega, effectually reduces an RSG to a bandit problem. We show that the resulting strategy space preserves several important properties of the original RSG, thus enabling a learner to produce robust strategies within a reasonably small number of interactions. To better establish strengths and weaknesses of this approach, we empirically evaluate the resulting learning system against other algorithms in three different RSGs.\n    ",
        "submission_date": "2014-09-30T00:00:00",
        "last_modified_date": "2018-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0083",
        "title": "Integrating active sensing into reactive synthesis with temporal logic constraints under partial observations",
        "authors": [
            "Jie Fu",
            "Ufuk Topcu"
        ],
        "abstract": "We introduce the notion of online reactive planning with sensing actions for systems with temporal logic constraints in partially observable and dynamic environments. With incomplete information on the dynamic environment, reactive controller synthesis amounts to solving a two-player game with partial observations, which has impractically computational complexity. To alleviate the high computational burden, online replanning via sensing actions avoids solving the strategy in the reactive system under partial observations. Instead, we only solve for a strategy that ensures a given temporal logic specification can be satisfied had the system have complete observations of its environment. Such a strategy is then transformed into one which makes control decisions based on the observed sequence of states (of the interacting system and its environment). When the system encounters a belief---a set including all possible hypotheses the system has for the current state---for which the observation-based strategy is undefined, a sequence of sensing actions are triggered, chosen by an active sensing strategy, to reduce the uncertainty in the system's belief. We show that by alternating between the observation-based strategy and the active sensing strategy, under a mild technical assumption of the set of sensors in the system, the given temporal logic specification can be satisfied with probability 1.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0413",
        "title": "Risk Dynamics in Trade Networks",
        "authors": [
            "Rafael M. Frongillo",
            "Mark D. Reid"
        ],
        "abstract": "We introduce a new framework to model interactions among agents which seek to trade to minimize their risk with respect to some future outcome. We quantify this risk using the concept of risk measures from finance, and introduce a class of trade dynamics which allow agents to trade contracts contingent upon the future outcome. We then show that these trade dynamics exactly correspond to a variant of randomized coordinate descent. By extending the analysis of these coordinate descent methods to account for our more organic setting, we are able to show convergence rates for very general trade dynamics, showing that the market or network converges to a unique steady state. Applying these results to prediction markets, we expand on recent results by adding convergence rates and general aggregation properties. Finally, we illustrate the generality of our framework by applying it to agent interactions on a scale-free network.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0471",
        "title": "PinView: Implicit Feedback in Content-Based Image Retrieval",
        "authors": [
            "Zakria Hussain",
            "Arto Klami",
            "Jussi Kujala",
            "Alex P. Leung",
            "Kitsuchart Pasupa",
            "Peter Auer",
            "Samuel Kaski",
            "Jorma Laaksonen",
            "John Shawe-Taylor"
        ],
        "abstract": "This paper describes PinView, a content-based image retrieval system that exploits implicit relevance feedback collected during a search session. PinView contains several novel methods to infer the intent of the user. From relevance feedback, such as eye movements or pointer clicks, and visual features of images, PinView learns a similarity metric between images which depends on the current interests of the user. It then retrieves images with a specialized online learning algorithm that balances the tradeoff between exploring new images and exploiting the already inferred interests of the user. We have integrated PinView to the content-based image retrieval system PicSOM, which enables applying PinView to real-world image databases. With the new algorithms PinView outperforms the original PicSOM, and in online experiments with real users the combination of implicit and explicit feedback gives the best results.\n    ",
        "submission_date": "2014-10-02T00:00:00",
        "last_modified_date": "2014-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0547",
        "title": "Design Mining Interacting Wind Turbines",
        "authors": [
            "Richard J. Preen",
            "Larry Bull"
        ],
        "abstract": "An initial study of surrogate-assisted evolutionary algorithms used to design vertical-axis wind turbines wherein candidate prototypes are evaluated under fan generated wind conditions after being physically instantiated by a 3D printer has recently been presented. Unlike other approaches, such as computational fluid dynamics simulations, no mathematical formulations were used and no model assumptions were made. This paper extends that work by exploring alternative surrogate modelling and evolutionary techniques. The accuracy of various modelling algorithms used to estimate the fitness of evaluated individuals from the initial experiments is compared. The effect of temporally windowing surrogate model training samples is explored. A surrogate-assisted approach based on an enhanced local search is introduced; and alternative coevolution collaboration schemes are examined.\n    ",
        "submission_date": "2014-10-02T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0736",
        "title": "HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition",
        "authors": [
            "Zhicheng Yan",
            "Hao Zhang",
            "Robinson Piramuthu",
            "Vignesh Jagadeesh",
            "Dennis DeCoste",
            "Wei Di",
            "Yizhou Yu"
        ],
        "abstract": "In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively.\n    ",
        "submission_date": "2014-10-03T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0949",
        "title": "Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits",
        "authors": [
            "Branislav Kveton",
            "Zheng Wen",
            "Azin Ashkan",
            "Csaba Szepesvari"
        ],
        "abstract": "A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we close the problem of computationally and sample efficient learning in stochastic combinatorial semi-bandits. In particular, we analyze a UCB-like algorithm for solving the problem, which is known to be computationally efficient; and prove $O(K L (1 / \\Delta) \\log n)$ and $O(\\sqrt{K L n \\log n})$ upper bounds on its $n$-step regret, where $L$ is the number of ground items, $K$ is the maximum number of chosen items, and $\\Delta$ is the gap between the expected returns of the optimal and best suboptimal solutions. The gap-dependent bound is tight up to a constant factor and the gap-free bound is tight up to a polylogarithmic factor.\n    ",
        "submission_date": "2014-10-03T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1068",
        "title": "Gamma Processes, Stick-Breaking, and Variational Inference",
        "authors": [
            "Anirban Roychowdhury",
            "Brian Kulis"
        ],
        "abstract": "While most Bayesian nonparametric models in machine learning have focused on the Dirichlet process, the beta process, or their variants, the gamma process has recently emerged as a useful nonparametric prior in its own right. Current inference schemes for models involving the gamma process are restricted to MCMC-based methods, which limits their scalability. In this paper, we present a variational inference framework for models involving gamma process priors. Our approach is based on a novel stick-breaking constructive definition of the gamma process. We prove correctness of this stick-breaking process by using the characterization of the gamma process as a completely random measure (CRM), and we explicitly derive the rate measure of our construction using Poisson process machinery. We also derive error bounds on the truncation of the infinite process required for variational inference, similar to the truncation analyses for other nonparametric models based on the Dirichlet and beta processes. Our representation is then used to derive a variational inference algorithm for a particular Bayesian nonparametric latent structure formulation known as the infinite Gamma-Poisson model, where the latent variables are drawn from a gamma process prior with Poisson likelihoods. Finally, we present results for our algorithms on nonnegative matrix factorization tasks on document corpora, and show that we compare favorably to both sampling-based techniques and variational approaches based on beta-Bernoulli priors.\n    ",
        "submission_date": "2014-10-04T00:00:00",
        "last_modified_date": "2014-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1141",
        "title": "On the Computational Efficiency of Training Neural Networks",
        "authors": [
            "Roi Livni",
            "Shai Shalev-Shwartz",
            "Ohad Shamir"
        ],
        "abstract": "It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.\n    ",
        "submission_date": "2014-10-05T00:00:00",
        "last_modified_date": "2014-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1462",
        "title": "Top Rank Optimization in Linear Time",
        "authors": [
            "Nan Li",
            "Rong Jin",
            "Zhi-Hua Zhou"
        ],
        "abstract": "Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the ranking loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.\n    ",
        "submission_date": "2014-10-06T00:00:00",
        "last_modified_date": "2014-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3726",
        "title": "Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding",
        "authors": [
            "Chern Hong Lim",
            "Anhar Risnumawan",
            "Chee Seng Chan"
        ],
        "abstract": "Ambiguity or uncertainty is a pervasive element of many real world decision making processes. Variation in decisions is a norm in this situation when the same problem is posed to different subjects. Psychological and metaphysical research had proven that decision making by human is subjective. It is influenced by many factors such as experience, age, background, etc. Scene understanding is one of the computer vision problems that fall into this category. Conventional methods relax this problem by assuming scene images are mutually exclusive; and therefore, focus on developing different approaches to perform the binary classification tasks. In this paper, we show that scene images are non-mutually exclusive, and propose the Fuzzy Qualitative Rank Classifier (FQRC) to tackle the aforementioned problems. The proposed FQRC provides a ranking interpretation instead of binary decision. Evaluations in term of qualitative and quantitative using large numbers and challenging public scene datasets have shown the effectiveness of our proposed method in modeling the non-mutually exclusive scene images.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4604",
        "title": "Domain-Independent Optimistic Initialization for Reinforcement Learning",
        "authors": [
            "Marlos C. Machado",
            "Sriram Srinivasan",
            "Michael Bowling"
        ],
        "abstract": "In Reinforcement Learning (RL), it is common to use optimistic initialization of value functions to encourage exploration. However, such an approach generally depends on the domain, viz., the scale of the rewards must be known, and the feature representation must have a constant norm. We present a simple approach that performs optimistic initialization with less dependence on the domain.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4615",
        "title": "Learning to Execute",
        "authors": [
            "Wojciech Zaremba",
            "Ilya Sutskever"
        ],
        "abstract": "Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99% accuracy.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5476",
        "title": "Certified Connection Tableaux Proofs for HOL Light and TPTP",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban",
            "Jiri Vyskocil"
        ],
        "abstract": "In the recent years, the Metis prover based on ordered paramodulation and model elimination has replaced the earlier built-in methods for general-purpose proof automation in HOL4 and Isabelle/HOL. In the annual CASC competition, the leanCoP system based on connection tableaux has however performed better than Metis. In this paper we show how the leanCoP's core algorithm can be implemented inside HOLLight. leanCoP's flagship feature, namely its minimalistic core, results in a very simple proof system. This plays a crucial role in extending the MESON proof reconstruction mechanism to connection tableaux proofs, providing an implementation of leanCoP that certifies its proofs. We discuss the differences between our direct implementation using an explicit Prolog stack, to the continuation passing implementation of MESON present in HOLLight and compare their performance on all core HOLLight goals. The resulting prover can be also used as a general purpose TPTP prover. We compare its performance against the resolution based Metis on TPTP and other interesting datasets.\n    ",
        "submission_date": "2014-10-20T00:00:00",
        "last_modified_date": "2014-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5557",
        "title": "Where do goals come from? A Generic Approach to Autonomous Goal-System Development",
        "authors": [
            "Matthias Rolf",
            "Minoru Asada"
        ],
        "abstract": "Goals express agents' intentions and allow them to organize their behavior based on low-dimensional abstractions of high-dimensional world states. How can agents develop such goals autonomously? This paper proposes a detailed conceptual and computational account to this longstanding problem. We argue to consider goals as high-level abstractions of lower-level intention mechanisms such as rewards and values, and point out that goals need to be considered alongside with a detection of the own actions' effects. We propose Latent Goal Analysis as a computational learning formulation thereof, and show constructively that any reward or value function can by explained by goals and such self-detection as latent mechanisms. We first show that learned goals provide a highly effective dimensionality reduction in a practical reinforcement learning problem. Then, we investigate a developmental scenario in which entirely task-unspecific rewards induced by visual saliency lead to self and goal representations that constitute goal-directed reaching.\n    ",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2014-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5614",
        "title": "The Tomaco Hybrid Matching Framework for SAWSDL Semantic Web Services",
        "authors": [
            "Thanos G. Stavropoulos",
            "Stelios Andreadis",
            "Nick Bassiliades",
            "Dimitris Vrakas",
            "Ioannis Vlahavas"
        ],
        "abstract": "This work aims to resolve issues related to Web Service retrieval, also known as Service Selection, Discovery or essentially Matching, in two directions. Firstly, a novel matching algorithm for SAWSDL is introduced. The algorithm is hybrid in nature, combining novel and known concepts, such as a logic-based strategy and syntactic text-similarity measures on semantic annotations and textual descriptions. A plugin for the S3 contest environment was developed, in order to position Tomaco amongst state-of-the-art in an objective, reproducible manner. Evaluation showed that Tomaco ranks high amongst state of the art, especially for early recall levels. Secondly, this work introduces the Tomaco web application, which aims to accelerate the wide-spread adoption of Semantic Web Service technologies and algorithms while targeting the lack of user-friendly applications in this field. Tomaco integrates a variety of configurable matching algorithms proposed in this paper. It, finally, allows discovery of both existing and user-contributed service collections and ontologies, serving also as a service registry.\n    ",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2014-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5792",
        "title": "Generalized Compression Dictionary Distance as Universal Similarity Measure",
        "authors": [
            "Andrey Bogomolov",
            "Bruno Lepri",
            "Fabio Pianesi"
        ],
        "abstract": "We present a new similarity measure based on information theoretic measures which is superior than Normalized Compression Distance for clustering problems and inherits the useful properties of conditional Kolmogorov complexity. We show that Normalized Compression Dictionary Size and Normalized Compression Dictionary Entropy are computationally more efficient, as the need to perform the compression itself is eliminated. Also they scale linearly with exponential vector size growth and are content independent. We show that normalized compression dictionary distance is compressor independent, if limited to lossless compressors, which gives space for optimizations and implementation speed improvement for real-time and big data applications. The introduced measure is applicable for machine learning tasks of parameter-free unsupervised clustering, supervised learning such as classification and regression, feature selection, and is applicable for big data problems with order of magnitude speed increase.\n    ",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2014-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6414",
        "title": "A Parallel and Efficient Algorithm for Learning to Match",
        "authors": [
            "Jingbo Shang",
            "Tianqi Chen",
            "Hang Li",
            "Zhengdong Lu",
            "Yong Yu"
        ],
        "abstract": "Many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains, including collaborative filtering, link prediction, image tagging, and web search. Machine learning techniques, referred to as learning-to-match in this paper, have been successfully applied to the problems. Among them, a class of state-of-the-art methods, named feature-based matrix factorization, formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model. Unfortunately, making those algorithms scale to real world problems is challenging, and simple parallelization strategies fail due to the complex cross talking patterns between sub-tasks. In this paper, we tackle this challenge with a novel parallel and efficient algorithm for feature-based matrix factorization. Our algorithm, based on coordinate descent, can easily handle hundreds of millions of instances and features on a single machine. The key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters, with guaranteed convergence on minimizing the original objective function. Experimental results demonstrate that the proposed method is effective on a wide range of matching problems, with efficiency significantly improved upon the baselines while accuracy retained unchanged.\n    ",
        "submission_date": "2014-10-22T00:00:00",
        "last_modified_date": "2014-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7265",
        "title": "An Unsupervised Ensemble-based Markov Random Field Approach to Microscope Cell Image Segmentation",
        "authors": [
            "Balint Antal",
            "Bence Remenyik",
            "Andras Hajdu"
        ],
        "abstract": "In this paper, we propose an approach to the unsupervised segmentation of images using Markov Random Field. The proposed approach is based on the idea of Bit Plane Slicing. We use the planes as initial labellings for an ensemble of segmentations. With pixelwise voting, a robust segmentation approach can be achieved, which we demonstrate on microscope cell images. We tested our approach on a publicly available database, where it proven to be competitive with other methods and manual segmentation.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7452",
        "title": "Consensus Message Passing for Layered Graphical Models",
        "authors": [
            "Varun Jampani",
            "S. M. Ali Eslami",
            "Daniel Tarlow",
            "Pushmeet Kohli",
            "John Winn"
        ],
        "abstract": "Generative models provide a powerful framework for probabilistic reasoning. However, in many domains their use has been hampered by the practical difficulties of inference. This is particularly the case in computer vision, where models of the imaging process tend to be large, loopy and layered. For this reason bottom-up conditional models have traditionally dominated in such domains. We find that widely-used, general-purpose message passing inference algorithms such as Expectation Propagation (EP) and Variational Message Passing (VMP) fail on the simplest of vision models. With these models in mind, we introduce a modification to message passing that learns to exploit their layered structure by passing 'consensus' messages that guide inference towards good solutions. Experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results, not only when compared to standard EP and VMP, but also when compared to competitive bottom-up conditional models.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2015-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7690",
        "title": "Trend Filtering on Graphs",
        "authors": [
            "Yu-Xiang Wang",
            "James Sharpnack",
            "Alex Smola",
            "Ryan J. Tibshirani"
        ],
        "abstract": "We introduce a family of adaptive estimators on graphs, based on penalizing the $\\ell_1$ norm of discrete graph differences. This generalizes the idea of trend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual $\\ell_2$-based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through examples and theory.\n    ",
        "submission_date": "2014-10-28T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7827",
        "title": "Generalized Product of Experts for Automatic and Principled Fusion of Gaussian Process Predictions",
        "authors": [
            "Yanshuai Cao",
            "David J. Fleet"
        ],
        "abstract": "In this work, we propose a generalized product of experts (gPoE) framework for combining the predictions of multiple probabilistic models. We identify four desirable properties that are important for scalability, expressiveness and robustness, when learning and inferring with a combination of multiple models. Through analysis and experiments, we show that gPoE of Gaussian processes (GP) have these qualities, while no other existing combination schemes satisfy all of them at the same time. The resulting GP-gPoE is highly scalable as individual GP experts can be independently learned in parallel; very expressive as the way experts are combined depends on the input rather than fixed; the combined prediction is still a valid probabilistic model with natural interpretation; and finally robust to unreliable predictions from individual experts.\n    ",
        "submission_date": "2014-10-28T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7851",
        "title": "Efficient optimisation of structures using tabu search",
        "authors": [
            "Andy M. Connor",
            "Keith A. Seffen",
            "Geoffrey T. Parks",
            "P. John Clarkson"
        ],
        "abstract": "This paper presents a novel approach to the optimisation of structures using a Tabu search (TS) method. TS is a metaheuristic which is used to guide local search methods towards a globally optimal solution by using flexible memory cycles of differing time spans. Results are presented for the well established ten bar truss problem and compared to results published in the literature. In the first example a truss is optimised to minimise mass and the results compared to results obtained using an alternative TS implementation. In the second example, the problem has multiple objectives that are compounded into a single objective function value using game theory. In general the results demonstrate that the TS method is capable of solving structural optimisation problems at least as efficiently as other numerical optimisation approaches.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7942",
        "title": "Using synchronous Boolean networks to model several phenomena of collective behavior",
        "authors": [
            "Stepan Kochemazov",
            "Alexander Semenov"
        ],
        "abstract": "In this paper, we propose an approach for modeling and analysis of a number of phenomena of collective behavior. By collectives we mean multi-agent systems that transition from one state to another at discrete moments of time. The behavior of a member of a collective (agent) is called conforming if the opinion of this agent at current time moment conforms to the opinion of some other agents at the previous time moment. We presume that at each moment of time every agent makes a decision by choosing from the set {0,1} (where 1-decision corresponds to action and 0-decision corresponds to inaction). In our approach we model collective behavior with synchronous Boolean networks. We presume that in a network there can be agents that act at every moment of time. Such agents are called instigators. Also there can be agents that never act. Such agents are called loyalists. Agents that are neither instigators nor loyalists are called simple agents. We study two combinatorial problems. The first problem is to find a disposition of instigators that in several time moments transforms a network from a state where a majority of simple agents are inactive to a state with a majority of active agents. The second problem is to find a disposition of loyalists that returns the network to a state with a majority of inactive agents. Similar problems are studied for networks in which simple agents demonstrate the contrary to conforming behavior that we call anticonforming. We obtained several theoretical results regarding the behavior of collectives of agents with conforming or anticonforming behavior. In computational experiments we solved the described problems for randomly generated networks with several hundred vertices. We reduced corresponding combinatorial problems to the Boolean satisfiability problem (SAT) and used modern SAT solvers to solve the instances obtained.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8326",
        "title": "Towards Learning Object Affordance Priors from Technical Texts",
        "authors": [
            "Nicholas H. Kirk"
        ],
        "abstract": "Everyday activities performed by artificial assistants can potentially be executed naively and dangerously given their lack of common sense knowledge. This paper presents conceptual work towards obtaining prior knowledge on the usual modality (passive or active) of any given entity, and their affordance estimates, by extracting high-confidence ability modality semantic relations (X can Y relationship) from non-figurative texts, by analyzing co-occurrence of grammatical instances of subjects and verbs, and verbs and objects. The discussion includes an outline of the concept, potential and limitations, and possible feature and learning framework adoption.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8498",
        "title": "Training for Fast Sequential Prediction Using Dynamic Feature Selection",
        "authors": [
            "Emma Strubell",
            "Luke Vilnis",
            "Andrew McCallum"
        ],
        "abstract": "We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. We present experiments in left-to-right part-of-speech tagging on WSJ, demonstrating that we can preserve accuracy above 97% with over a five-fold reduction in run-time.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8577",
        "title": "An Ensemble-based System for Microaneurysm Detection and Diabetic Retinopathy Grading",
        "authors": [
            "Balint Antal",
            "Andras Hajdu"
        ],
        "abstract": "Reliable microaneurysm detection in digital fundus images is still an open issue in medical image processing. We propose an ensemble-based framework to improve microaneurysm detection. Unlike the well-known approach of considering the output of multiple classifiers, we propose a combination of internal components of microaneurysm detectors, namely preprocessing methods and candidate extractors. We have evaluated our approach for microaneurysm detection in an online competition, where this algorithm is currently ranked as first and also on two other databases. Since microaneurysm detection is decisive in diabetic retinopathy grading, we also tested the proposed method for this task on the publicly available Messidor database, where a promising AUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classification based on the presence or absence of the microaneurysms.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8620",
        "title": "A Comparison of learning algorithms on the Arcade Learning Environment",
        "authors": [
            "Aaron Defazio",
            "Thore Graepel"
        ],
        "abstract": "Reinforcement learning agents have traditionally been evaluated on small toy problems. With advances in computing power and the advent of the Arcade Learning Environment, it is now possible to evaluate algorithms on diverse and difficult problems within a consistent framework. We discuss some challenges posed by the arcade learning environment which do not manifest in simpler environments. We then provide a comparison of model-free, linear learning algorithms on this challenging problem set.\n    ",
        "submission_date": "2014-10-31T00:00:00",
        "last_modified_date": "2014-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0264",
        "title": "On the read-once property of branching programs and CNFs of bounded treewidth",
        "authors": [
            "Igor Razgon"
        ],
        "abstract": "In this paper we prove a space lower bound of $n^{\\Omega(k)}$ for non-deterministic (syntactic) read-once branching programs ({\\sc nrobp}s) on functions expressible as {\\sc cnf}s with treewidth at most $k$ of their primal graphs. This lower bound rules out the possibility of fixed-parameter space complexity of {\\sc nrobp}s parameterized by $k$.\n",
        "submission_date": "2014-11-02T00:00:00",
        "last_modified_date": "2015-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0541",
        "title": "Distributed Submodular Maximization",
        "authors": [
            "Baharan Mirzasoleiman",
            "Amin Karbasi",
            "Rik Sarkar",
            "Andreas Krause"
        ],
        "abstract": "Many large-scale machine learning problems--clustering, non-parametric learning, kernel machines, etc.--require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol GreeDi, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show that under certain natural conditions, performance close to the centralized approach can be achieved. We begin with monotone submodular maximization subject to a cardinality constraint, and then extend this approach to obtain approximation guarantees for (not necessarily monotone) submodular maximization subject to more general constraints including matroid or knapsack constraints. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar based clustering on tens of millions of examples using Hadoop.\n    ",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0659",
        "title": "Approximate Counting in SMT and Value Estimation for Probabilistic Programs",
        "authors": [
            "Dmitry Chistikov",
            "Rayna Dimitrova",
            "Rupak Majumdar"
        ],
        "abstract": "#SMT, or model counting for logical theories, is a well-known hard problem that generalizes such tasks as counting the number of satisfying assignments to a Boolean formula and computing the volume of a polytope. In the realm of satisfiability modulo theories (SMT) there is a growing need for model counting solvers, coming from several application domains (quantitative information flow, static analysis of probabilistic programs). In this paper, we show a reduction from an approximate version of #SMT to SMT.\n",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2015-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0895",
        "title": "Tied Probabilistic Linear Discriminant Analysis for Speech Recognition",
        "authors": [
            "Liang Lu",
            "Steve Renals"
        ],
        "abstract": "Acoustic models using probabilistic linear discriminant analysis (PLDA) capture the correlations within feature vectors using subspaces which do not vastly expand the model. This allows high dimensional and correlated feature spaces to be used, without requiring the estimation of multiple high dimension covariance matrices. In this letter we extend the recently presented PLDA mixture model for speech recognition through a tied PLDA approach, which is better able to control the model size to avoid overfitting. We carried out experiments using the Switchboard corpus, with both mel frequency cepstral coefficient features and bottleneck feature derived from a deep neural network. Reductions in word error rate were obtained by using tied PLDA, compared with the PLDA mixture model, subspace Gaussian mixture models, and deep neural networks.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1170",
        "title": "An Intelligent Personal Robot Assistant",
        "authors": [
            "Ong Sing Goh",
            "Lance Fung"
        ],
        "abstract": "Recent development in developing humanoid robot poses new challenges to human-machine interaction communication. A major challenge is to develop robots that can behave like and interact with human in the most natural way possible. This paper proposes a system to develop a robot that can receive command, and talk to people in natural language. In addition, the robot can also be \"trained\" to become an expert in sepcific areas to provide expert advice to human-beings. Most important of all, the robot can display emotions through facial expression, speech and gesture so that the interaction process will become more comprehensive and compelling.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1752",
        "title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets",
        "authors": [
            "Adarsh Prasad",
            "Stefanie Jegelka",
            "Dhruv Batra"
        ],
        "abstract": "To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1784",
        "title": "Conditional Generative Adversarial Nets",
        "authors": [
            "Mehdi Mirza",
            "Simon Osindero"
        ],
        "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1953",
        "title": "Hardware and Software manual for Evolution of Oil Droplets in a Chemo-Robotic Platform",
        "authors": [
            "Juan Manuel Parrilla Gutierrez",
            "Trevor Hinkley",
            "James Taylor",
            "Kliment Yanev",
            "Leroy Cronin"
        ],
        "abstract": "This manual outlines a fully automated liquid handling robot to enable physically-embodied evolution within a chemical oil-droplet system. The robot is based upon the REPRAP3D printer system and makes the droplets by mixing chemicals and then placing them in a petri dish after which they are recorded using a camera and the behaviour of the droplets analysed using image recognition software. This manual accompanies the open access publication published in Nature Communications DOI: ",
        "submission_date": "2014-11-07T00:00:00",
        "last_modified_date": "2014-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2186",
        "title": "Estimating Fire Weather Indices via Semantic Reasoning over Wireless Sensor Network Data Streams",
        "authors": [
            "Lianli Gao",
            "Michael Bruenig",
            "Jane Hunter"
        ],
        "abstract": "Wildfires are frequent, devastating events in Australia that regularly cause significant loss of life and widespread property damage. Fire weather indices are a widely-adopted method for measuring fire danger and they play a significant role in issuing bushfire warnings and in anticipating demand for bushfire management resources. Existing systems that calculate fire weather indices are limited due to low spatial and temporal resolution. Localized wireless sensor networks, on the other hand, gather continuous sensor data measuring variables such as air temperature, relative humidity, rainfall and wind speed at high resolutions. However, using wireless sensor networks to estimate fire weather indices is a challenge due to data quality issues, lack of standard data formats and lack of agreement on thresholds and methods for calculating fire weather indices. Within the scope of this paper, we propose a standardized approach to calculating Fire Weather Indices (a.k.a. fire danger ratings) and overcome a number of the challenges by applying Semantic Web Technologies to the processing of data streams from a wireless sensor network deployed in the Springbrook region of South East Queensland. This paper describes the underlying ontologies, the semantic reasoning and the Semantic Fire Weather Index (SFWI) system that we have developed to enable domain experts to specify and adapt rules for calculating Fire Weather Indices. We also describe the Web-based mapping interface that we have developed, that enables users to improve their understanding of how fire weather indices vary over time within a particular ",
        "submission_date": "2014-11-09T00:00:00",
        "last_modified_date": "2014-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2328",
        "title": "Modeling Word Relatedness in Latent Dirichlet Allocation",
        "authors": [
            "Xun Wang"
        ],
        "abstract": "Standard LDA model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. To address this problem, in this paper, we propose a model called Word Related Latent Dirichlet Allocation (WR-LDA) by incorporating word correlation into LDA topic models. This leads to new capabilities that standard LDA model does not have such as estimating infrequently occurring words or multi-language topic modeling. Experimental results demonstrate the effectiveness of our model compared with standard LDA.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2374",
        "title": "Similarity Learning for High-Dimensional Sparse Data",
        "authors": [
            "Kuan Liu",
            "Aur\u00e9lien Bellet",
            "Fei Sha"
        ],
        "abstract": "A good measure of similarity between data points is crucial to many tasks in machine learning. Similarity and metric learning methods learn such measures automatically from data, but they do not scale well respect to the dimensionality of the data. In this paper, we propose a method that can learn efficiently similarity measure from high-dimensional sparse data. The core idea is to parameterize the similarity measure as a convex combination of rank-one matrices with specific sparsity structures. The parameters are then optimized with an approximate Frank-Wolfe procedure to maximally satisfy relative similarity constraints on the training data. Our algorithm greedily incorporates one pair of features at a time into the similarity measure, providing an efficient way to control the number of active features and thus reduce overfitting. It enjoys very appealing convergence guarantees and its time and memory complexity depends on the sparsity of the data instead of the dimension of the feature space. Our experiments on real-world high-dimensional datasets demonstrate its potential for classification, dimensionality reduction and data exploration.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2019-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2636",
        "title": "Bounding the Probability of Causation in Mediation Analysis",
        "authors": [
            "A. P. Dawid",
            "R. Murtas",
            "M. Musio"
        ],
        "abstract": "Given empirical evidence for the dependence of an outcome variable on an exposure variable, we can typically only provide bounds for the \"probability of causation\" in the case of an individual who has developed the outcome after being exposed. We show how these bounds can be adapted or improved if further information becomes available. In addition to reviewing existing work on this topic, we provide a new analysis for the case where a mediating variable can be observed. In particular we show how the probability of causation can be bounded when there is no direct effect and no confounding.\n",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2679",
        "title": "Inferring User Preferences by Probabilistic Logical Reasoning over Social Networks",
        "authors": [
            "Jiwei Li",
            "Alan Ritter",
            "Dan Jurafsky"
        ],
        "abstract": "We propose a framework for inferring the latent attitudes or preferences of users by performing probabilistic first-order logical reasoning over the social network graph. Our method answers questions about Twitter users like {\\em Does this user like sushi?} or {\\em Is this user a New York Knicks fan?} by building a probabilistic model that reasons over user attributes (the user's location or gender) and the social network (the user's friends and spouse), via inferences like homophily (I am more likely to like sushi if spouse or friends like sushi, I am more likely to like the Knicks if I live in New York). The algorithm uses distant supervision, semi-supervised data harvesting and vector space models to extract user attributes (e.g. spouse, education, location) and preferences (likes and dislikes) from text. The extracted propositions are then fed into a probabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft Logic). Our experiments show that probabilistic logical reasoning significantly improves the performance on attribute and relation extraction, and also achieves an F-score of 0.791 at predicting a users likes or dislikes, significantly better than two strong baselines.\n    ",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2014-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2842",
        "title": "Logical Limitations to Machine Ethics with Consequences to Lethal Autonomous Weapons",
        "authors": [
            "Matthias Englert",
            "Sandra Siebert",
            "Martin Ziegler"
        ],
        "abstract": "Lethal Autonomous Weapons promise to revolutionize warfare -- and raise a multitude of ethical and legal questions. It has thus been suggested to program values and principles of conduct (such as the Geneva Conventions) into the machines' control, thereby rendering them both physically and morally superior to human combatants.\n",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2014-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3622",
        "title": "Handling owl:sameAs via Rewriting",
        "authors": [
            "Boris Motik",
            "Yavor Nenov",
            "Robert Piro",
            "Ian Horrocks"
        ],
        "abstract": "Rewriting is widely used to optimise owl:sameAs reasoning in materialisation based OWL 2 RL systems. We investigate issues related to both the correctness and efficiency of rewriting, and present an algorithm that guarantees correctness, improves efficiency, and can be effectively parallelised. Our evaluation shows that our approach can reduce reasoning times on practical data sets by orders of magnitude.\n    ",
        "submission_date": "2014-11-13T00:00:00",
        "last_modified_date": "2014-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3675",
        "title": "Scalable Link Prediction in Dynamic Networks via Non-Negative Matrix Factorization",
        "authors": [
            "Linhong Zhu",
            "Dong Guo",
            "Junming Yin",
            "Greg Ver Steeg",
            "Aram Galstyan"
        ],
        "abstract": "We propose a scalable temporal latent space model for link prediction in dynamic social networks, where the goal is to predict links over time based on a sequence of previous graph snapshots. The model assumes that each user lies in an unobserved latent space and interactions are more likely to form between similar users in the latent space representation. In addition, the model allows each user to gradually move its position in the latent space as the network structure evolves over time. We present a global optimization algorithm to effectively infer the temporal latent space, with a quadratic convergence rate. Two alternative optimization algorithms with local and incremental updates are also proposed, allowing the model to scale to larger networks without compromising prediction accuracy. Empirically, we demonstrate that our model, when evaluated on a number of real-world dynamic networks, significantly outperforms existing approaches for temporal link prediction in terms of both scalability and predictive power.\n    ",
        "submission_date": "2014-11-13T00:00:00",
        "last_modified_date": "2016-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3895",
        "title": "Learning Fuzzy Controllers in Mobile Robotics with Embedded Preprocessing",
        "authors": [
            "I. Rodr\u00edguez-Fdez",
            "M. Mucientes",
            "A. Bugar\u00edn"
        ],
        "abstract": "The automatic design of controllers for mobile robots usually requires two stages. In the first stage,sensorial data are preprocessed or transformed into high level and meaningful values of variables whichare usually defined from expert knowledge. In the second stage, a machine learning technique is applied toobtain a controller that maps these high level variables to the control commands that are actually sent tothe robot. This paper describes an algorithm that is able to embed the preprocessing stage into the learningstage in order to get controllers directly starting from sensorial raw data with no expert knowledgeinvolved. Due to the high dimensionality of the sensorial data, this approach uses Quantified Fuzzy Rules(QFRs), that are able to transform low-level input variables into high-level input variables, reducingthe dimensionality through summarization. The proposed learning algorithm, called Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic programming. IQFRL is able to learn rules with differentstructures, and can manage linguistic variables with multiple granularities. The algorithm has been testedwith the implementation of the wall-following behavior both in several realistic simulated environmentswith different complexity and on a Pioneer 3-AT robot in two real environments. Results have beencompared with several well-known learning algorithms combined with different data preprocessingtechniques, showing that IQFRL exhibits a better and statistically significant performance. Moreover,three real world applications for which IQFRL plays a central role are also presented: path and objecttracking with static and moving obstacles avoidance.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4000",
        "title": "How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets",
        "authors": [
            "Zhiyun Lu",
            "Avner May",
            "Kuan Liu",
            "Alireza Bagheri Garakani",
            "Dong Guo",
            "Aur\u00e9lien Bellet",
            "Linxi Fan",
            "Michael Collins",
            "Brian Kingsbury",
            "Michael Picheny",
            "Fei Sha"
        ],
        "abstract": "The computational complexity of kernel methods has often been a major barrier for applying them to large-scale learning problems. We argue that this barrier can be effectively overcome. In particular, we develop methods to scale up kernel models to successfully tackle large-scale learning problems that are so far only approachable by deep learning architectures. Based on the seminal work by Rahimi and Recht on approximating kernel functions with features derived from random projections, we advance the state-of-the-art by proposing methods that can efficiently train models with hundreds of millions of parameters, and learn optimal representations from multiple kernels. We conduct extensive empirical studies on problems from image recognition and automatic speech recognition, and show that the performance of our kernel models matches that of well-engineered deep neural nets (DNNs). To the best of our knowledge, this is the first time that a direct comparison between these two methods on large-scale problems is reported. Our kernel methods have several appealing properties: training with convex optimization, cost for training a single model comparable to DNNs, and significantly reduced total cost due to fewer hyperparameters to tune for model selection. Our contrastive study between these two very different but equally competitive models sheds light on fundamental questions such as how to learn good representations.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4109",
        "title": "Resolution of Difficult Pronouns Using the ROSS Method",
        "authors": [
            "Glenn R. Hofford"
        ],
        "abstract": "A new natural language understanding method for disambiguation of difficult pronouns is described. Difficult pronouns are those pronouns for which a level of world or domain knowledge is needed in order to perform anaphoral or other types of resolution. Resolution of difficult pronouns may in some cases require a prior step involving the application of inference to a situation that is represented by the natural language text. A general method is described: it performs entity resolution and pronoun resolution. An extension to the general pronoun resolution method performs inference as an embedded commonsense reasoning method. The general method and the embedded method utilize features of the ROSS representational scheme; in particular the methods use ROSS ontology classes and the ROSS situation model. The overall method is a working solution that solves the following Winograd schemas: a) trophy and suitcase, b) person lifts person, c) person pays detective, and d) councilmen and demonstrators.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4342",
        "title": "Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations",
        "authors": [
            "Kirthevasan Kandasamy",
            "Akshay Krishnamurthy",
            "Barnabas Poczos",
            "Larry Wasserman",
            "James M. Robins"
        ],
        "abstract": "We propose and analyze estimators for statistical functionals of one or more distributions under nonparametric assumptions. Our estimators are based on the theory of influence functions, which appear in the semiparametric statistics literature. We show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties. We apply this framework to derive estimators for several popular information theoretic quantities, and via empirical evaluation, show the advantage of this approach over existing estimators.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4379",
        "title": "FGPGA: An Efficient Genetic Approach for Producing Feasible Graph Partitions",
        "authors": [
            "Md. Lisul Islam",
            "Novia Nurain",
            "Swakkhar Shatabda",
            "M Sohel Rahman"
        ],
        "abstract": "Graph partitioning, a well studied problem of parallel computing has many applications in diversified fields such as distributed computing, social network analysis, data mining and many other domains. In this paper, we introduce FGPGA, an efficient genetic approach for producing feasible graph partitions. Our method takes into account the heterogeneity and capacity constraints of the partitions to ensure balanced partitioning. Such approach has various applications in mobile cloud computing that include feasible deployment of software applications on the more resourceful infrastructure in the cloud instead of mobile hand set. Our proposed approach is light weight and hence suitable for use in cloud architecture. We ensure feasibility of the partitions generated by not allowing over-sized partitions to be generated during the initialization and search. Our proposed method tested on standard benchmark datasets significantly outperforms the state-of-the-art methods in terms of quality of partitions and feasibility of the solutions.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4618",
        "title": "Relations World: A Possibilistic Graphical Model",
        "authors": [
            "Christopher J.C. Burges",
            "Erin Renshaw",
            "Andrzej Pastusiak"
        ],
        "abstract": "We explore the idea of using a \"possibilistic graphical model\" as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user's family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4702",
        "title": "Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal Memory in Light of Frontal Cortical Function",
        "authors": [
            "Michael R. Ferrier"
        ],
        "abstract": "A wide range of evidence points toward the existence of a common algorithm underlying the processing of information throughout the cerebral cortex. Several hypothesized features of this cortical algorithm are reviewed, including sparse distributed representation, Bayesian inference, hierarchical organization composed of alternating template matching and pooling layers, temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is a family of learning algorithms and corresponding theories of cortical function that embodies these principles. HTM has previously been applied mainly to perceptual tasks typical of posterior cortex. In order to evaluate HTM as a candidate model of cortical function, it is necessary also to investigate its compatibility with the requirements of frontal cortical function. To this end, a variety of models of frontal cortical function are reviewed and integrated, to arrive at the hypothesis that frontal functions including attention, working memory and action selection depend largely upon the same basic algorithms as do posterior functions, with the notable additions of a mechanism for the active maintenance of representations and of multiple cortico-striato-thalamo-cortical loops that allow communication between regions of frontal cortex to be gated in an adaptive manner. Computational models of this system are reviewed. Finally, there is a discussion of how HTM can contribute to the understanding of frontal cortical function, and of what the requirements of frontal cortical function mean for the future development of HTM.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5224",
        "title": "Can we build a conscious machine?",
        "authors": [
            "Dorian Aur"
        ],
        "abstract": "The underlying physiological mechanisms of generating conscious states are still unknown. To make progress on the problem of consciousness, we will need to experimentally design a system that evolves in a similar way our brains do. Recent experimental data show that the multiscale nature of the evolving human brain can be implemented by reprogramming human cells. A hybrid system can be designed to include an evolving brain equipped with digital computers that maintain homeostasis and provide the right amount of nutrients and oxygen for the brain growth. Shaping the structure of the evolving brain will be progressively achieved by controlling spatial organization of various types of cells. Following a specific program, the evolving brain can be trained using substitutional reality to learn and experience live scenes. We already know from neuroelectrodynamics that meaningful information in the brain is electrically (wirelessly) read out and written fast in neurons and synapses at the molecular (protein) level during the generation of action potentials and synaptic activities. Since with training, meaningful information accumulates and is electrically integrated in the brain, one can predict, that this gradual process of training will trigger a tipping point for conscious experience to emerge in the hybrid system.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5268",
        "title": "Sparse distributed localized gradient fused features of objects",
        "authors": [
            "Swathikiran Sudhakarana",
            "Alex Pappachen James"
        ],
        "abstract": "The sparse, hierarchical, and modular processing of natural signals is related to the ability of humans to recognize objects with high accuracy. In this study, we report a sparse feature processing and encoding method, which improved the recognition performance of an automated object recognition system. Randomly distributed localized gradient enhanced features were selected before employing aggregate functions for representation, where we used a modular and hierarchical approach to detect the object features. These object features were combined with a minimum distance classifier, thereby obtaining object recognition system accuracies of 93% using the Amsterdam library of object images (ALOI) database, 92% using the Columbia object image library (COIL)-100 database, and 69% using the PASCAL visual object challenge 2007 database. The object recognition performance was shown to be robust to variations in noise, object scaling, and object shifts. Finally, a comparison with eight existing object recognition methods indicated that our new method improved the recognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10% with the PASCAL visual object challenge 2007 database.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5328",
        "title": "ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image Collections",
        "authors": [
            "Bolei Zhou",
            "Vignesh Jagadeesh",
            "Robinson Piramuthu"
        ],
        "abstract": "Discovering visual knowledge from weakly labeled data is crucial to scale up computer vision recognition system, since it is expensive to obtain fully labeled data for a large number of concept categories. In this paper, we propose ConceptLearner, which is a scalable approach to discover visual concepts from weakly labeled image collections. Thousands of visual concept detectors are learned automatically, without human in the loop for additional annotation. We show that these learned detectors could be applied to recognize concepts at image-level and to detect concepts at image region-level accurately. Under domain-specific supervision, we further evaluate the learned concepts for scene recognition on SUN database and for object detection on Pascal VOC 2007. ConceptLearner shows promising performance compared to fully supervised and weakly supervised methods.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5654",
        "title": "Learning a Recurrent Visual Representation for Image Caption Generation",
        "authors": [
            "Xinlei Chen",
            "C. Lawrence Zitnick"
        ],
        "abstract": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over $19.8\\%$ of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5878",
        "title": "Salient Object Detection: A Survey",
        "authors": [
            "Ali Borji",
            "Ming-Ming Cheng",
            "Qibin Hou",
            "Huaizu Jiang",
            "Jia Li"
        ],
        "abstract": "Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. While many models have been proposed and several applications have emerged, a deep understanding of achievements and issues remains lacking. We aim to provide a comprehensive review of recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics for salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance, and suggest future research directions.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2019-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6279",
        "title": "Logics of Temporal-Epistemic Actions",
        "authors": [
            "Bryan Renne",
            "Joshua Sack",
            "Audrey Yap"
        ],
        "abstract": "We present Dynamic Epistemic Temporal Logic, a framework for reasoning about operations on multi-agent Kripke models that contain a designated temporal relation. These operations are natural extensions of the well-known \"action models\" from Dynamic Epistemic Logic. Our \"temporal action models\" may be used to define a number of informational actions that can modify the \"objective\" temporal structure of a model along with the agents' basic and higher-order knowledge and beliefs about this structure, including their beliefs about the time. In essence, this approach provides one way to extend the domain of action model-style operations from atemporal Kripke models to temporal Kripke models in a manner that allows actions to control the flow of time. We present a number of examples to illustrate the subtleties involved in interpreting the effects of our extended action models on temporal Kripke models. We also study preservation of important epistemic-temporal properties of temporal Kripke models under temporal action model-induced operations, provide complete axiomatizations for two theories of temporal action models, and connect our approach with previous work on time in Dynamic Epistemic Logic.\n    ",
        "submission_date": "2014-11-23T00:00:00",
        "last_modified_date": "2014-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6307",
        "title": "Diversifying Sparsity Using Variational Determinantal Point Processes",
        "authors": [
            "Nematollah Kayhan Batmanghelich",
            "Gerald Quon",
            "Alex Kulesza",
            "Manolis Kellis",
            "Polina Golland",
            "Luke Bornn"
        ],
        "abstract": "We propose a novel diverse feature selection method based on determinantal point processes (DPPs). Our model enables one to flexibly define diversity based on the covariance of features (similar to orthogonal matching pursuit) or alternatively based on side information. We introduce our approach in the context of Bayesian sparse regression, employing a DPP as a variational approximation to the true spike and slab posterior distribution. We subsequently show how this variational DPP approximation generalizes and extends mean-field approximation, and can be learned efficiently by exploiting the fast sampling properties of DPPs. Our motivating application comes from bioinformatics, where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network. We also explore an application in spatial statistics. In both cases, we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods, without compromising accuracy.\n    ",
        "submission_date": "2014-11-23T00:00:00",
        "last_modified_date": "2014-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6314",
        "title": "On the High-dimensional Power of Linear-time Kernel Two-Sample Testing under Mean-difference Alternatives",
        "authors": [
            "Aaditya Ramdas",
            "Sashank J. Reddi",
            "Barnabas Poczos",
            "Aarti Singh",
            "Larry Wasserman"
        ],
        "abstract": "Nonparametric two sample testing deals with the question of consistently deciding if two distributions are different, given samples from both, without making any parametric assumptions about the form of the distributions. The current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ (\\textit{general} alternatives), and those which are designed to specifically test easier alternatives, like a difference in means (\\textit{mean-shift} alternatives).\n",
        "submission_date": "2014-11-23T00:00:00",
        "last_modified_date": "2014-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6721",
        "title": "Detecting fraudulent activity in a cloud using privacy-friendly data aggregates",
        "authors": [
            "Marc Solanas",
            "Julio Hernandez-Castro",
            "Debojyoti Dutta"
        ],
        "abstract": "More users and companies make use of cloud services every day. They all expect a perfect performance and any issue to remain transparent to them. This last statement is very challenging to perform. A user's activities in our cloud can affect the overall performance of our servers, having an impact on other resources. We can consider these kind of activities as fraudulent. They can be either illegal activities, such as launching a DDoS attack or just activities which are undesired by the cloud provider, such as Bitcoin mining, which uses substantial power, reduces the life of the hardware and can possibly slow down other user's activities. This article discusses a method to detect such activities by using non-intrusive, privacy-friendly data: billing data. We use OpenStack as an example with data provided by Telemetry, the component in charge of measuring resource usage for billing purposes. Results will be shown proving the efficiency of this method and ways to improve it will be provided as well as its advantages and disadvantages.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6768",
        "title": "Hypotheses of neural code and the information model of the neuron-detector",
        "authors": [
            "Yuri Parzhin"
        ],
        "abstract": "This paper deals with the problem of neural code solving. On the basis of the formulated hypotheses the information model of a neuron-detector is suggested, the detector being one of the basic elements of an artificial neural network (ANN). The paper subjects the connectionist paradigm of ANN building to criticism and suggests a new presentation paradigm for ANN building and neuroelements (NE) learning. The adequacy of the suggested model is proved by the fact that is does not contradict the modern propositions of neuropsychology and neurophysiology.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7014",
        "title": "Efficient Algorithms for Bayesian Network Parameter Learning from Incomplete Data",
        "authors": [
            "Guy Van den Broeck",
            "Karthika Mohan",
            "Arthur Choi",
            "Judea Pearl"
        ],
        "abstract": "We propose an efficient family of algorithms to learn the parameters of a Bayesian network from incomplete data. In contrast to textbook approaches such as EM and the gradient method, our approach is non-iterative, yields closed form parameter estimates, and eliminates the need for inference in a Bayesian network. Our approach provides consistent parameter estimates for missing data problems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approach is orders of magnitude faster than EM (as our approach requires no inference). Given sufficient data, we learn parameters that can be orders of magnitude more accurate.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7806",
        "title": "Two Gaussian Approaches to Black-Box Optomization",
        "authors": [
            "Luk\u00e1\u0161 Bajer",
            "Martin Hole\u0148a"
        ],
        "abstract": "Outline of several strategies for using Gaussian processes as surrogate models for the covariance matrix adaptation evolution strategy (CMA-ES).\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7920",
        "title": "Probability Theory without Bayes' Rule",
        "authors": [
            "Samuel G. Rodriques"
        ],
        "abstract": "Within the Kolmogorov theory of probability, Bayes' rule allows one to perform statistical inference by relating conditional probabilities to unconditional probabilities. As we show here, however, there is a continuous set of alternative inference rules that yield the same results, and that may have computational or practical advantages for certain problems. We formulate generalized axioms for probability theory, according to which the reverse conditional probability distribution P(B|A) is not specified by the forward conditional probability distribution P(A|B) and the marginals P(A) and P(B). Thus, in order to perform statistical inference, one must specify an additional \"inference axiom,\" which relates P(B|A) to P(A|B), P(A), and P(B). We show that when Bayes' rule is chosen as the inference axiom, the axioms are equivalent to the classical Kolmogorov axioms. We then derive consistency conditions on the inference axiom, and thereby characterize the set of all possible rules for inference. The set of \"first-order\" inference axioms, defined as the set of axioms in which P(B|A) depends on the first power of P(A|B), is found to be a 1-simplex, with Bayes' rule at one of the extreme points. The other extreme point, the \"inversion rule,\" is studied in depth.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0320",
        "title": "Canonical Logic Programs are Succinctly Incomparable with Propositional Formulas",
        "authors": [
            "Yuping Shen",
            "Xishun Zhao"
        ],
        "abstract": "\\emph{Canonical (logic) programs} (CP) refer to normal logic programs augmented with connective $not\\ not$. In this paper we address the question of whether CP are \\emph{succinctly incomparable} with \\emph{propositional formulas} (PF). Our main result shows that the PARITY problem, which can be polynomially represented in PF but \\emph{only} has exponential representations in CP. In other words, PARITY \\emph{separates} PF from CP. Simply speaking, this means that exponential size blowup is generally inevitable when translating a set of formulas in PF into an equivalent program in CP (without introducing new variables). Furthermore, since it has been shown by Lifschitz and Razborov that there is also a problem that separates CP from PF (assuming $\\mathsf{P}\\nsubseteq \\mathsf{NC^1/poly}$), it follows that CP and PF are indeed succinctly incomparable. From the view of the theory of computation, the above result may also be considered as the separation of two \\emph{models of computation}, i.e., we identify a language in $\\mathsf{NC^1/poly}$ which is not in the set of languages computable by polynomial size CP programs.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2015-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0439",
        "title": "Fuzzy human motion analysis: A review",
        "authors": [
            "Chern Hong Lim",
            "Ekta Vats",
            "Chee Seng Chan"
        ],
        "abstract": "Human Motion Analysis (HMA) is currently one of the most popularly active research domains as such significant research interests are motivated by a number of real world applications such as video surveillance, sports analysis, healthcare monitoring and so on. However, most of these real world applications face high levels of uncertainties that can affect the operations of such applications. Hence, the fuzzy set theory has been applied and showed great success in the recent past. In this paper, we aim at reviewing the fuzzy set oriented approaches for HMA, individuating how the fuzzy set may improve the HMA, envisaging and delineating the future perspectives. To the best of our knowledge, there is not found a single survey in the current literature that has discussed and reviewed fuzzy approaches towards the HMA. For ease of understanding, we conceptually classify the human motion into three broad levels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1042",
        "title": "A CSP implementation of the bigraph embedding problem",
        "authors": [
            "Marino Miculan",
            "Marco Peressotti"
        ],
        "abstract": "A crucial problem for many results and tools about bigraphs and bigraphical reactive systems is bigraph embedding. An embedding is more informative than a bigraph matching, since it keeps track of the correspondence between the various components of the redex (guest) within the agent (host). In this paper, we present an algorithm for computing embeddings based on a reduction to a constraint satisfaction problem. This algorithm, that we prove to be sound and complete, has been successfully implemented in LibBig, a library for manipulating bigraphical reactive systems. This library can be used for implementing a wide range of tools, and it can be adapted to various extensions of bigraphs.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2019-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1069",
        "title": "Approximate Lifted Inference with Probabilistic Databases",
        "authors": [
            "Wolfgang Gatterbauer",
            "Dan Suciu"
        ],
        "abstract": "This paper proposes a new approach for approximate evaluation of #P-hard queries with probabilistic databases. In our approach, every query is evaluated entirely in the database engine by evaluating a fixed number of query plans, each providing an upper bound on the true probability, then taking their minimum. We provide an algorithm that takes into account important schema information to enumerate only the minimal necessary plans among all possible plans. Importantly, this algorithm is a strict generalization of all known results of PTIME self-join-free conjunctive queries: A query is safe if and only if our algorithm returns one single plan. We also apply three relational query optimization techniques to evaluate all minimal safe plans very fast. We give a detailed experimental evaluation of our approach and, in the process, provide a new way of thinking about the value of probabilistic methods over non-probabilistic methods for ranking query answers.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1138",
        "title": "Highly comparative fetal heart rate analysis",
        "authors": [
            "B. D. Fulcher",
            "A. E. Georgieva",
            "C. W. G. Redman",
            "Nick S. Jones"
        ],
        "abstract": "A database of fetal heart rate (FHR) time series measured from 7221 patients during labor is analyzed with the aim of learning the types of features of these recordings that are informative of low cord pH. Our 'highly comparative' analysis involves extracting over 9000 time-series analysis features from each FHR time series, including measures of autocorrelation, entropy, distribution, and various model fits. This diverse collection of features was developed in previous work, and is publicly available. We describe five features that most accurately classify a balanced training set of 59 'low pH' and 59 'normal pH' FHR recordings. We then describe five of the features with the strongest linear correlation to cord pH across the full dataset of FHR time series. The features identified in this work may be used as part of a system for guiding intervention during labor in future. This work successfully demonstrates the utility of comparing across a large, interdisciplinary literature on time-series analysis to automatically contribute new scientific results for specific biomedical signal processing challenges.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1505",
        "title": "Symmetric Weighted First-Order Model Counting",
        "authors": [
            "Paul Beame",
            "Guy Van den Broeck",
            "Eric Gribkoff",
            "Dan Suciu"
        ],
        "abstract": "The FO Model Counting problem (FOMC) is the following: given a sentence $\\Phi$ in FO and a number $n$, compute the number of models of $\\Phi$ over a domain of size $n$; the Weighted variant (WFOMC) generalizes the problem by associating a weight to each tuple and defining the weight of a model to be the product of weights of its tuples. In this paper we study the complexity of the symmetric WFOMC, where all tuples of a given relation have the same weight. Our motivation comes from an important application, inference in Knowledge Bases with soft constraints, like Markov Logic Networks, but the problem is also of independent theoretical interest. We study both the data complexity, and the combined complexity of FOMC and WFOMC. For the data complexity we prove the existence of an FO$^{3}$ formula for which FOMC is #P$_1$-complete, and the existence of a Conjunctive Query for which WFOMC is #P$_1$-complete. We also prove that all $\\gamma$-acyclic queries have polynomial time data complexity. For the combined complexity, we prove that, for every fragment FO$^{k}$, $k\\geq 2$, the combined complexity of FOMC (or WFOMC) is #P-complete.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1862",
        "title": "Knowledge, Justification, and Adequate Reasons",
        "authors": [
            "Paul \u00c9gr\u00e9",
            "Paul Marty",
            "Bryan Renne"
        ],
        "abstract": "Is knowledge definable as justified true belief (\"JTB\")? We argue that one can legitimately answer positively or negatively, depending on whether or not one's true belief is justified by what we call adequate reasons. To facilitate our argument we introduce a simple propositional logic of reason-based belief, and give an axiomatic characterization of the notion of adequacy for reasons. We show that this logic is sufficiently flexible to accommodate various useful features, including quantification over reasons. We use our framework to contrast two notions of JTB: one internalist, the other externalist. We argue that Gettier cases essentially challenge the internalist notion but not the externalist one. Our approach commits us to a form of infallibilism about knowledge, but it also leaves us with a puzzle, namely whether knowledge involves the possession of only adequate reasons, or leaves room for some inadequate reasons. We favor the latter position, which reflects a milder and more realistic version of infallibilism.\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2021-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1897",
        "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
        "authors": [
            "Anh Nguyen",
            "Jason Yosinski",
            "Jeff Clune"
        ],
        "abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call \"fooling images\" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2122",
        "title": "Non-Verbal Communication Analysis in Victim-Offender Mediations",
        "authors": [
            "V\u00edctor Ponce-L\u00f3pez",
            "Sergio Escalera",
            "Marc P\u00e9rez",
            "Oriol Jan\u00e9s",
            "Xavier Bar\u00f3"
        ],
        "abstract": "  In this paper we present a non-invasive ambient intelligence framework for the semi-automatic analysis of non-verbal communication applied to the restorative justice field. In particular, we propose the use of computer vision and social signal processing technologies in real scenarios of Victim-Offender Mediations, applying feature extraction techniques to multi-modal audio-RGB-depth data. We compute a set of behavioral indicators that define communicative cues from the fields of psychology and observational methodology. We test our methodology on data captured in real world Victim-Offender Mediation sessions in Catalonia in collaboration with the regional government. We define the ground truth based on expert opinions when annotating the observed social responses. Using different state-of-the-art binary classification approaches, our system achieves recognition accuracies of 86% when predicting satisfaction, and 79% when predicting both agreement and receptivity. Applying a regression strategy, we obtain a mean deviation for the predictions between 0.5 and 0.7 in the range [1-5] for the computed social signals.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2015-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2186",
        "title": "Using Artificial Neural Network Techniques for Prediction of Electric Energy Consumption",
        "authors": [
            "Hasan M. H. Owda",
            "Babatunji Omoniwa",
            "Ahmad R. Shahid",
            "Sheikh Ziauddin"
        ],
        "abstract": "Due to imprecision and uncertainties in predicting real world problems, artificial neural network (ANN) techniques have become increasingly useful for modeling and optimization. This paper presents an artificial neural network approach for forecasting electric energy consumption. For effective planning and operation of power systems, optimal forecasting tools are needed for energy operators to maximize profit and also to provide maximum satisfaction to energy consumers. Monthly data for electric energy consumed in the Gaza strip was collected from year 1994 to 2013. Data was trained and the proposed model was validated using 2-Fold and K-Fold cross validation techniques. The model has been tested with actual energy consumption data and yields satisfactory performance.\n    ",
        "submission_date": "2014-12-06T00:00:00",
        "last_modified_date": "2014-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2221",
        "title": "Declarative Statistical Modeling with Datalog",
        "authors": [
            "Vince Barany",
            "Balder ten Cate",
            "Benny Kimelfeld",
            "Dan Olteanu",
            "Zografoula Vagena"
        ],
        "abstract": "Formalisms for specifying statistical models, such as probabilistic-programming languages, typically consist of two components: a specification of a stochastic process (the prior), and a specification of observations that restrict the probability space to a conditional subspace (the posterior). Use cases of such formalisms include the development of algorithms in machine learning and artificial intelligence. We propose and investigate a declarative framework for specifying statistical models on top of a database, through an appropriate extension of Datalog. By virtue of extending Datalog, our framework offers a natural integration with the database, and has a robust declarative semantics. Our Datalog extension provides convenient mechanisms to include numerical probability functions; in particular, conclusions of rules may contain values drawn from such functions. The semantics of a program is a probability distribution over the possible outcomes of the input database with respect to the program; these outcomes are minimal solutions with respect to a related program with existentially quantified variables in conclusions. Observations are naturally incorporated by means of integrity constraints over the extensional and intensional relations. We focus on programs that use discrete numerical distributions, but even then the space of possible outcomes may be uncountable (as a solution can be infinite). We define a probability measure over possible outcomes by applying the known concept of cylinder sets to a probabilistic chase procedure. We show that the resulting semantics is robust under different chases. We also identify conditions guaranteeing that all possible outcomes are finite (and then the probability space is discrete). We argue that the framework we propose retains the purely declarative nature of Datalog, and allows for natural specifications of statistical models.\n    ",
        "submission_date": "2014-12-06T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2309",
        "title": "Visual Causal Feature Learning",
        "authors": [
            "Krzysztof Chalupka",
            "Pietro Perona",
            "Frederick Eberhardt"
        ],
        "abstract": "We provide a rigorous definition of the visual cause of a behavior that is broadly applicable to the visually driven behavior in humans, animals, neurons, robots and other perceiving systems. Our framework generalizes standard accounts of causal learning to settings in which the causal variables need to be constructed from micro-variables. We prove the Causal Coarsening Theorem, which allows us to gain causal knowledge from observational data with minimal experimental effort. The theorem provides a connection to standard inference techniques in machine learning that identify features of an image that correlate with, but may not cause, the target behavior. Finally, we propose an active learning scheme to learn a manipulator function that performs optimal manipulations on the image to automatically identify the visual cause of a target behavior. We illustrate our inference and learning algorithms in experiments based on both synthetic and real data.\n    ",
        "submission_date": "2014-12-07T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2689",
        "title": "A New Approach of Learning Hierarchy Construction Based on Fuzzy Logic",
        "authors": [
            "Ali Aajli",
            "Karim Afdel"
        ],
        "abstract": "In recent years, adaptive learning systems rely increasingly on learning hierarchy to customize the educational logic developed in their courses. Most approaches do not consider that the relationships of prerequisites between the skills are fuzzy relationships. In this article, we describe a new approach of a practical application of fuzzy logic techniques to the construction of learning hierarchies. For this, we use a learning hierarchy predefined by one or more experts of a specific field. However, the relationships of prerequisites between the skills in the learning hierarchy are not definitive and they are fuzzy relationships. Indeed, we measure relevance degree of all relationships existing in this learning hierarchy and we try to answer to the following question: Is the relationships of prerequisites predefined in initial learning hierarchy are correctly established or not?\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2812",
        "title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error Minimization Framework",
        "authors": [
            "Ivan Titov",
            "Ehsan Khoddam"
        ],
        "abstract": "We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3056",
        "title": "Phishing Detection in IMs using Domain Ontology and CBA - An innovative Rule Generation Approach",
        "authors": [
            "Mohammad S. Qaseem",
            "A. Govardhan"
        ],
        "abstract": "User ignorance towards the use of communication services like Instant Messengers, emails, websites, social networks etc. is becoming the biggest advantage for phishers. It is required to create technical awareness in users by educating them to create a phishing detection application which would generate phishing alerts for the user so that phishing messages are not ignored. The lack of basic security features to detect and prevent phishing has had a profound effect on the IM clients, as they lose their faith in e-banking and e-commerce transactions, which will have a disastrous impact on the corporate and banking sectors and businesses which rely heavily on the internet. Very little research contributions were available in for phishing detection in Instant messengers. A context based, dynamic and intelligent phishing detection methodology in IMs is proposed, to analyze and detect phishing in Instant Messages with relevance to domain ontology (OBIE) and utilizes the Classification based on Association (CBA) for generating phishing rules and alerting the victims. A PDS Monitoring system algorithm is used to identify the phishing activity during exchange of messages in IMs, with high ratio of precision and recall. The results have shown improvement by the increased percentage of precision and recall when compared to the existing methods.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3078",
        "title": "Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process Regression",
        "authors": [
            "Jun Wei Ng",
            "Marc Peter Deisenroth"
        ],
        "abstract": "We propose a practical and scalable Gaussian process model for large-scale nonlinear probabilistic regression. Our mixture-of-experts model is conceptually simple and hierarchically recombines computations for an overall approximation of a full Gaussian process. Closed-form and distributed computations allow for efficient and massive parallelisation while keeping the memory consumption small. Given sufficient computing resources, our model can handle arbitrarily large data sets, without explicit sparse approximations. We provide strong experimental evidence that our model can be applied to large data sets of sizes far beyond millions. Hence, our model has the potential to lay the foundation for general large-scale Gaussian process research.\n    ",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3131",
        "title": "A tool for implementation of a domain model based on fuzzy relationships",
        "authors": [
            "Ali Aajli",
            "Karim Afdel"
        ],
        "abstract": "The domain model is one of the important components used by adaptive learning systems to automatically generate customized courses for the learners. In this paper our contribution is to propose a new tool for implementation of a domain model based on fuzzy relationships among concepts. This tool allows the experts and teachers to find the best parameters in order to adapt the learners's differences.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3633",
        "title": "Logic of temporal attribute implications",
        "authors": [
            "Jan Triska",
            "Vilem Vychodil"
        ],
        "abstract": "We study logic for reasoning with if-then formulas describing dependencies between attributes of objects which are observed in consecutive points in time. We introduce semantic entailment of the formulas, show its fixed-point characterization, investigate closure properties of model classes, present an axiomatization and prove its completeness, and investigate alternative axiomatizations and normalized proofs. We investigate decidability and complexity issues of the logic and prove that the entailment problem is NP-hard and belongs to EXPSPACE. We show that by restricting to predictive formulas, the entailment problem is decidable in pseudo-linear time.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3714",
        "title": "Feature Weight Tuning for Recursive Neural Networks",
        "authors": [
            "Jiwei Li"
        ],
        "abstract": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \"weight tuning\" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3773",
        "title": "Distinguishing cause from effect using observational data: methods and benchmarks",
        "authors": [
            "Joris M. Mooij",
            "Jonas Peters",
            "Dominik Janzing",
            "Jakob Zscheischler",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X, Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: Additive Noise Methods (ANM) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 100 different cause-effect pairs selected from 37 datasets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the \"ground truth\" causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the additive-noise method originally proposed by Hoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of 0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4736",
        "title": "On the Inductive Bias of Dropout",
        "authors": [
            "David P. Helmbold",
            "Philip M. Long"
        ],
        "abstract": "Dropout is a simple but effective technique for learning in neural networks and other settings. A sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer pioneered by Wager, ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2015-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5090",
        "title": "Belief as Willingness to Bet",
        "authors": [
            "Jan van Eijck",
            "Bryan Renne"
        ],
        "abstract": "We investigate modal logics of high probability having two unary modal operators: an operator $K$ expressing probabilistic certainty and an operator $B$ expressing probability exceeding a fixed rational threshold $c\\geq\\frac 12$. Identifying knowledge with the former and belief with the latter, we may think of $c$ as the agent's betting threshold, which leads to the motto \"belief is willingness to bet.\" The logic $\\mathsf{KB.5}$ for $c=\\frac 12$ has an $\\mathsf{S5}$ $K$ modality along with a sub-normal $B$ modality that extends the minimal modal logic $\\mathsf{EMND45}$ by way of four schemes relating $K$ and $B$, one of which is a complex scheme arising out of a theorem due to Scott. Lenzen was the first to use Scott's theorem to show that a version of this logic is sound and complete for the probability interpretation. We reformulate Lenzen's results and present them here in a modern and accessible form. In addition, we introduce a new epistemic neighborhood semantics that will be more familiar to modern modal logicians. Using Scott's theorem, we provide the Lenzen-derivative properties that must be imposed on finite epistemic neighborhood models so as to guarantee the existence of a probability measure respecting the neighborhood function in the appropriate way for threshold $c=\\frac 12$. This yields a link between probabilistic and modal neighborhood semantics that we hope will be of use in future work on modal logics of qualitative probability. We leave open the question of which properties must be imposed on finite epistemic neighborhood models so as to guarantee existence of an appropriate probability measure for thresholds $c\\neq\\frac 12$.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5207",
        "title": "Are We Ready for Driver-less Vehicles? Security vs. Privacy- A Social Perspective",
        "authors": [
            "Anish Acharya"
        ],
        "abstract": "At this moment Autonomous cars are probably the biggest and most talked about technology in the Robotics Research Community. In spite of great technological advances over past few years a full edged autonomous car is still far from reality. This article talks about the existing system and discusses the possibility of a Computer Vision enabled driving being superior than the LiDar based system. A detailed overview of privacy violations that might arise from autonomous driving has been discussed in detail both from a technical as well as legal perspective. It has been proved through evidence and arguments that efficient and accurate estimation and efficient solution of the constraint satisfaction problem addressed in the case of autonomous cars are negatively correlated with the preserving the privacy of the user. It is a very difficult trade-off since both are very important aspects and has to be taken into account. The fact that one cannot compromise with the safety issues of the car makes it inevitable to run into serious privacy concerns that might have adverse social and political effects.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5244",
        "title": "Learning unbiased features",
        "authors": [
            "Yujia Li",
            "Kevin Swersky",
            "Richard Zemel"
        ],
        "abstract": "A key element in transfer learning is representation learning; if representations can be developed that expose the relevant factors underlying the data, then new tasks and domains can be learned readily based on mappings of these salient factors. We propose that an important aim for these representations are to be unbiased. Different forms of representation learning can be derived from alternative definitions of unwanted bias, e.g., bias to particular tasks, domains, or irrelevant underlying data dimensions. One very useful approach to estimating the amount of bias in a representation comes from maximum mean discrepancy (MMD) [5], a measure of distance between probability distributions. We are not the first to suggest that MMD can be a useful criterion in developing representations that apply across multiple domains or tasks [1]. However, in this paper we describe a number of novel applications of this criterion that we have devised, all based on the idea of developing unbiased representations. These formulations include: a standard domain adaptation framework; a method of learning invariant representations; an approach based on noise-insensitive autoencoders; and a novel form of generative model.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5513",
        "title": "Towards a constructive multilayer perceptron for regression task using non-parametric clustering. A case study of Photo-Z redshift reconstruction",
        "authors": [
            "Cyrine Arouri",
            "Engelbert Mephu Nguifo",
            "Sabeur Aridhi",
            "C\u00e9cile Roucelle",
            "Gaelle Bonnet-Loosli",
            "Norbert Tsopz\u00e9"
        ],
        "abstract": "The choice of architecture of artificial neuron network (ANN) is still a challenging task that users face every time. It greatly affects the accuracy of the built network. In fact there is no optimal method that is applicable to various implementations at the same time. In this paper we propose a method to construct ANN based on clustering, that resolves the problems of random and ad hoc approaches for multilayer ANN architecture. Our method can be applied to regression problems. Experimental results obtained with different datasets, reveals the efficiency of our method.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6153",
        "title": "Intelligent Indoor Mobile Robot Navigation Using Stereo Vision",
        "authors": [
            "Arjun B. Krishnan",
            "Jayaram Kollipara"
        ],
        "abstract": "Majority of the existing robot navigation systems, which facilitate the use of laser range finders, sonar sensors or artificial landmarks, has the ability to locate itself in an unknown environment and then build a map of the corresponding environment. Stereo vision, while still being a rapidly developing technique in the field of autonomous mobile robots, are currently less preferable due to its high implementation cost. This paper aims at describing an experimental approach for the building of a stereo vision system that helps the robots to avoid obstacles and navigate through indoor environments and at the same time remaining very much cost effective. This paper discusses the fusion techniques of stereo vision and ultrasound sensors which helps in the successful navigation through different types of complex environments. The data from the sensor enables the robot to create the two dimensional topological map of unknown environments and stereo vision systems models the three dimension model of the same environment.\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2014-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6177",
        "title": "Example Selection For Dictionary Learning",
        "authors": [
            "Tomoki Tsuchida",
            "Garrison W. Cottrell"
        ],
        "abstract": "In unsupervised learning, an unbiased uniform sampling strategy is typically used, in order that the learned features faithfully encode the statistical structure of the training data. In this work, we explore whether active example selection strategies - algorithms that select which examples to use, based on the current estimate of the features - can accelerate learning. Specifically, we investigate effects of heuristic and saliency-inspired selection algorithms on the dictionary learning task with sparse activations. We show that some selection algorithms do improve the speed of learning, and we speculate on why they might work.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6285",
        "title": "From dependency to causality: a machine learning approach",
        "authors": [
            "Gianluca Bontempi",
            "Maxime Flauder"
        ],
        "abstract": "The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with $n>2$ variables. The approach relies on the asymmetry of some conditional (in)dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for $n>2$ variate distributions.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6451",
        "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer",
        "authors": [
            "Mark Wernsdorfer",
            "Ute Schmid"
        ],
        "abstract": "Methods of deep machine learning enable to to reuse low-level representations efficiently for generating more abstract high-level representations. Originally, deep learning has been applied passively (e.g., for classification purposes). Recently, it has been extended to estimate the value of actions for autonomous agents within the framework of reinforcement learning (RL). Explicit models of the environment can be learned to augment such a value function. Although \"flat\" connectionist methods have already been used for model-based RL, up to now, only model-free variants of RL have been equipped with methods from deep learning. We propose a variant of deep model-based RL that enables an agent to learn arbitrarily abstract hierarchical representations of its environment. In this paper, we present research on how such hierarchical representations can be grounded in sensorimotor interaction between an agent and its environment.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6464",
        "title": "Simplified firefly algorithm for 2D image key-points search",
        "authors": [
            "Christian Napoli",
            "Giuseppe Pappalardo",
            "Emiliano Tramontana",
            "Zbigniew Marsza\u0142ek",
            "Dawid Po\u0142ap",
            "Marcin Wo\u017aniak"
        ],
        "abstract": "In order to identify an object, human eyes firstly search the field of view for points or areas which have particular properties. These properties are used to recognise an image or an object. Then this process could be taken as a model to develop computer algorithms for images identification. This paper proposes the idea of applying the simplified firefly algorithm to search for key-areas in 2D images. For a set of input test images the proposed version of firefly algorithm has been examined. Research results are presented and discussed to show the efficiency of this evolutionary computation method.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6614",
        "title": "In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning",
        "authors": [
            "Behnam Neyshabur",
            "Ryota Tomioka",
            "Nathan Srebro"
        ],
        "abstract": "We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6749",
        "title": "SENNS: Sparse Extraction Neural NetworkS for Feature Extraction",
        "authors": [
            "Abdulrahman Oladipupo Ibraheem"
        ],
        "abstract": "By drawing on ideas from optimisation theory, artificial neural networks (ANN), graph embeddings and sparse representations, I develop a novel technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at addressing the feature extraction problem. The proposed method uses (preferably deep) ANNs for projecting input attribute vectors to an output space wherein pairwise distances are maximized for vectors belonging to different classes, but minimized for those belonging to the same class, while simultaneously enforcing sparsity on the ANN outputs. The vectors that result from the projection can then be used as features in any classifier of choice. Mathematically, I formulate the proposed method as the minimisation of an objective function which can be interpreted, in the ANN output space, as a negative factor of the sum of the squares of the pair-wise distances between output vectors belonging to different classes, added to a positive factor of the sum of squares of the pair-wise distances between output vectors belonging to the same classes, plus sparsity and weight decay terms. To derive an algorithm for minimizing the objective function via gradient descent, I use the multi-variate version of the chain rule to obtain the partial derivatives of the function with respect to ANN weights and biases, and find that each of the required partial derivatives can be expressed as a sum of six terms. As it turns out, four of those six terms can be computed using the standard back propagation algorithm; the fifth can be computed via a slight modification of the standard backpropagation algorithm; while the sixth one can be computed via simple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora of digits and letters, the CMU PIE database of faces, the MNIST digits database, and other standard machine learning databases.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2014-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7386",
        "title": "A web-based tool to Analyze Semantic Similarity Networks",
        "authors": [
            "Mario Cannataro",
            "Pietro Hiram Guzzi",
            "Marianna Milano",
            "Pierangelo Veltri"
        ],
        "abstract": "In computational biology, biological entities such as genes or proteins are usually annotated with terms extracted from Gene Ontology (GO). The functional similarity among terms of an ontology is evaluated by using Semantic Similarity Measures (SSM). More recently, the extensive application of SSMs yielded to the Semantic Similarity Networks (SSNs). SSNs are edge-weighted graphs where the nodes are concepts (e.g. proteins) and each edge has an associated weight that represents the semantic similarity among related pairs of nodes. The analysis of SSNs may reveal biologically meaningful knowledge. For these aims, the need for the introduction of tool able to manage and analyze SSN arises. Consequently we developed SSN-Analyzer a web based tool able to build and preprocess SSN. As proof of concept we demonstrate that community detection algorithms applied to filtered (thresholded) networks, have better performances in terms of biological relevance of the results, with respect to the use of raw unfiltered networks.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2014-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7927",
        "title": "Polyphonic Music Generation by Modeling Temporal Dependencies Using a RNN-DBN",
        "authors": [
            "Kratarth Goel",
            "Raunaq Vohra",
            "J.K. Sahoo"
        ],
        "abstract": "In this paper, we propose a generic technique to model temporal dependencies and sequences using a combination of a recurrent neural network and a Deep Belief Network. Our technique, RNN-DBN, is an amalgamation of the memory state of the RNN that allows it to provide temporal information and a multi-layer DBN that helps in high level representation of the data. This makes RNN-DBNs ideal for sequence generation. Further, the use of a DBN in conjunction with the RNN makes this model capable of significantly more complex data representation than an RBM. We apply this technique to the task of polyphonic music generation.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8520",
        "title": "Understanding and Designing Complex Systems: Response to \"A framework for optimal high-level descriptions in science and engineering---preliminary report\"",
        "authors": [
            "James P. Crutchfield",
            "Ryan G. James",
            "Sarah Marzen",
            "Dowman P. Varn"
        ],
        "abstract": "We recount recent history behind building compact models of nonlinear, complex processes and identifying their relevant macroscopic patterns or \"macrostates\". We give a synopsis of computational mechanics, predictive rate-distortion theory, and the role of information measures in monitoring model complexity and predictive performance. Computational mechanics provides a method to extract the optimal minimal predictive model for a given process. Rate-distortion theory provides methods for systematically approximating such models. We end by commenting on future prospects for developing a general framework that automatically discovers optimal compact models. As a response to the manuscript cited in the title above, this brief commentary corrects potentially misleading claims about its state space compression method and places it in a broader historical setting.\n    ",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    }
]