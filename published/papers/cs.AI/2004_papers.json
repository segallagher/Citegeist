[
    {
        "url": "https://arxiv.org/abs/cs/0401009",
        "title": "Unifying Computing and Cognition: The SP Theory and its Applications",
        "authors": [
            "J Gerard Wolff"
        ],
        "abstract": "  This book develops the conjecture that all kinds of information processing in computers and in brains may usefully be understood as \"information compression by multiple alignment, unification and search\". This \"SP theory\", which has been under development since 1987, provides a unified view of such things as the workings of a universal Turing machine, the nature of 'knowledge', the interpretation and production of natural language, pattern recognition and best-match information retrieval, several kinds of probabilistic reasoning, planning and problem solving, unsupervised learning, and a range of concepts in mathematics and logic. The theory also provides a basis for the design of an 'SP' computer with several potential advantages compared with traditional digital computers.\n    ",
        "submission_date": "2004-01-13T00:00:00",
        "last_modified_date": "2004-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402013",
        "title": "Corollaries on the fixpoint completion: studying the stable semantics by means of the Clark completion",
        "authors": [
            "Pascal Hitzler"
        ],
        "abstract": "  The fixpoint completion fix(P) of a normal logic program P is a program transformation such that the stable models of P are exactly the models of the Clark completion of fix(P). This is well-known and was studied by Dung and Kanchanasut (1989). The correspondence, however, goes much further: The Gelfond-Lifschitz operator of P coincides with the immediate consequence operator of fix(P), as shown by Wendt (2002), and even carries over to standard operators used for characterizing the well-founded and the Kripke-Kleene semantics. We will apply this knowledge to the study of the stable semantics, and this will allow us to almost effortlessly derive new results concerning fixed-point and metric-based semantics, and neural-symbolic integration.\n    ",
        "submission_date": "2004-02-09T00:00:00",
        "last_modified_date": "2004-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402019",
        "title": "The Munich Rent Advisor: A Success for Logic Programming on the Internet",
        "authors": [
            "Thom Fruehwirth",
            "Slim Abdennadher"
        ],
        "abstract": "  Most cities in Germany regularly publish a booklet called the {\\em Mietspiegel}. It basically contains a verbal description of an expert system. It allows the calculation of the estimated fair rent for a flat. By hand, one may need a weekend to do so. With our computerized version, the {\\em Munich Rent Advisor}, the user just fills in a form in a few minutes and the rent is calculated immediately. We also extended the functionality and applicability of the {\\em Mietspiegel} so that the user need not answer all questions on the form. The key to computing with partial information using high-level programming was to use constraint logic programming. We rely on the internet, and more specifically the World Wide Web, to provide this service to a broad user group. More than ten thousand people have used our service in the last three years. This article describes the experiences in implementing and using the {\\em Munich Rent Advisor}. Our results suggests that logic programming with constraints can be an important ingredient in intelligent internet systems.\n    ",
        "submission_date": "2004-02-10T00:00:00",
        "last_modified_date": "2004-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402033",
        "title": "Recycling Computed Answers in Rewrite Systems for Abduction",
        "authors": [
            "Fangzhen Lin",
            "Jia-Huai You"
        ],
        "abstract": "  In rule-based systems, goal-oriented computations correspond naturally to the possible ways that an observation may be explained. In some applications, we need to compute explanations for a series of observations with the same domain. The question whether previously computed answers can be recycled arises. A yes answer could result in substantial savings of repeated computations. For systems based on classic logic, the answer is YES. For nonmonotonic systems however, one tends to believe that the answer should be NO, since recycling is a form of adding information. In this paper, we show that computed answers can always be recycled, in a nontrivial way, for the class of rewrite procedures that we proposed earlier for logic programs with negation. We present some experimental results on an encoding of the logistics domain.\n    ",
        "submission_date": "2004-02-16T00:00:00",
        "last_modified_date": "2004-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402035",
        "title": "Memory As A Monadic Control Construct In Problem-Solving",
        "authors": [
            "Jean-Marie Chauvet"
        ],
        "abstract": "  Recent advances in programming languages study and design have established a standard way of grounding computational systems representation in category theory. These formal results led to a better understanding of issues of control and side-effects in functional and imperative languages. This framework can be successfully applied to the investigation of the performance of Artificial Intelligence (AI) inference and cognitive systems. In this paper, we delineate a categorical formalisation of memory as a control structure driving performance in inference systems. Abstracting away control mechanisms from three widely used representations of memory in cognitive systems (scripts, production rules and clusters) we explain how categorical triples capture the interaction between learning and problem-solving.\n    ",
        "submission_date": "2004-02-16T00:00:00",
        "last_modified_date": "2004-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402057",
        "title": "Integrating Defeasible Argumentation and Machine Learning Techniques",
        "authors": [
            "Sergio Alejandro Gomez",
            "Carlos Ivan Ches\u00f1evar"
        ],
        "abstract": "  The field of machine learning (ML) is concerned with the question of how to construct algorithms that automatically improve with experience. In recent years many successful ML applications have been developed, such as datamining programs, information-filtering systems, etc. Although ML algorithms allow the detection and extraction of interesting patterns of data for several kinds of problems, most of these algorithms are based on quantitative reasoning, as they rely on training data in order to infer so-called target functions.\n",
        "submission_date": "2004-02-25T00:00:00",
        "last_modified_date": "2004-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403001",
        "title": "Evolving a Stigmergic Self-Organized Data-Mining",
        "authors": [
            "Vitorino Ramos",
            "Ajith Abraham"
        ],
        "abstract": "  Self-organizing complex systems typically are comprised of a large number of frequently similar components or events. Through their process, a pattern at the global-level of a system emerges solely from numerous interactions among the lower-level components of the system. Moreover, the rules specifying interactions among the system's components are executed using only local information, without reference to the global pattern, which, as in many real-world problems is not easily accessible or possible to be found. Stigmergy, a kind of indirect communication and learning by the environment found in social insects is a well know example of self-organization, providing not only vital clues in order to understand how the components can interact to produce a complex pattern, as can pinpoint simple biological non-linear rules and methods to achieve improved artificial intelligent adaptive categorization systems, critical for Data-Mining. On the present work it is our intention to show that a new type of Data-Mining can be designed based on Stigmergic paradigms, taking profit of several natural features of this phenomenon. By hybridizing bio-inspired Swarm Intelligence with Evolutionary Computation we seek for an entire distributed, adaptive, collective and cooperative self-organized Data-Mining. As a real-world, real-time test bed for our proposal, World-Wide-Web Mining will be used. Having that purpose in mind, Web usage Data was collected from the Monash University's Web site (Australia), with over 7 million hits every week. Results are compared to other recent systems, showing that the system presented is by far promising.\n    ",
        "submission_date": "2004-02-28T00:00:00",
        "last_modified_date": "2004-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403002",
        "title": "Epistemic Foundation of Stable Model Semantics",
        "authors": [
            "Y. Loyer",
            "U. Straccia"
        ],
        "abstract": "  Stable model semantics has become a very popular approach for the management of negation in logic programming. This approach relies mainly on the closed world assumption to complete the available knowledge and its formulation has its basis in the so-called Gelfond-Lifschitz transformation.\n",
        "submission_date": "2004-03-02T00:00:00",
        "last_modified_date": "2005-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403006",
        "title": "The role of behavior modifiers in representation development",
        "authors": [
            "Carlos R. de la Mora B.",
            "Carlos Gershenson",
            "Angelica Garcia-Vega"
        ],
        "abstract": "  We address the problem of the development of representations and their relationship to the environment. We study a software agent which develops in a network a representation of its simple environment which captures and integrates the relationships between agent and environment through a closure mechanism. The inclusion of a variable behavior modifier allows better representation development. This can be confirmed with an internal description of the closure mechanism, and with an external description of the properties of the representation network.\n    ",
        "submission_date": "2004-03-05T00:00:00",
        "last_modified_date": "2004-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403009",
        "title": "Demolishing Searle's Chinese Room",
        "authors": [
            "Wolfram Schmied"
        ],
        "abstract": "  Searle's Chinese Room argument is refuted by showing that he has actually given two different versions of the room, which fail for different reasons. Hence, Searle does not achieve his stated goal of showing ``that a system could have input and output capabilities that duplicated those of a native Chinese speaker and still not understand Chinese''.\n    ",
        "submission_date": "2004-03-08T00:00:00",
        "last_modified_date": "2004-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403031",
        "title": "Concept of E-machine: How does a \"dynamical\" brain learn to process \"symbolic\" information? Part I",
        "authors": [
            "Victor Eliashberg"
        ],
        "abstract": "  The human brain has many remarkable information processing characteristics that deeply puzzle scientists and engineers. Among the most important and the most intriguing of these characteristics are the brain's broad universality as a learning system and its mysterious ability to dynamically change (reconfigure) its behavior depending on a combinatorial number of different contexts.\n",
        "submission_date": "2004-03-19T00:00:00",
        "last_modified_date": "2004-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403032",
        "title": "Where Fail-Safe Default Logics Fail",
        "authors": [
            "Paolo Liberatore"
        ],
        "abstract": "  Reiter's original definition of default logic allows for the application of a default that contradicts a previously applied one. We call failure this condition. The possibility of generating failures has been in the past considered as a semantical problem, and variants have been proposed to solve it. We show that it is instead a computational feature that is needed to encode some domains into default logic.\n    ",
        "submission_date": "2004-03-19T00:00:00",
        "last_modified_date": "2004-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404011",
        "title": "Parametric external predicates for the DLV System",
        "authors": [
            "G. Ianni",
            "F. Calimeri",
            "A. Pietramala",
            "M.C. Santoro"
        ],
        "abstract": "  This document describes syntax, semantics and implementation guidelines in order to enrich the DLV system with the possibility to make external C function calls. This feature is realized by the introduction of parametric external predicates, whose extension is not specified through a logic program but implicitly computed through external code.\n    ",
        "submission_date": "2004-04-05T00:00:00",
        "last_modified_date": "2004-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404012",
        "title": "Toward the Implementation of Functions in the DLV System (Preliminary Technical Report)",
        "authors": [
            "Francesco Calimeri",
            "Nicola Leone"
        ],
        "abstract": "  This document describes the functions as they are treated in the DLV system. We give first the language, then specify the main implementation issues.\n    ",
        "submission_date": "2004-04-05T00:00:00",
        "last_modified_date": "2004-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404030",
        "title": "XML framework for concept description and knowledge representation",
        "authors": [
            "Andreas de Vries"
        ],
        "abstract": "  An XML framework for concept description is given, based upon the fact that the tree structure of XML implies the logical structure of concepts as defined by attributional calculus. Especially, the attribute-value representation is implementable in the XML framework. Since the attribute-value representation is an important way to represent knowledge in AI, the framework offers a further and simpler way than the powerful RDF technology.\n    ",
        "submission_date": "2004-04-14T00:00:00",
        "last_modified_date": "2004-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404051",
        "title": "Knowledge And The Action Description Language A",
        "authors": [
            "Jorge Lobo",
            "Gisela Mendez",
            "Stuart R. Taylor"
        ],
        "abstract": "  We introduce Ak, an extension of the action description language A (Gelfond and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing actions to increase an agent's knowledge of the world and non-deterministic actions to remove knowledge. We include complex plans involving conditionals and loops in our query language for hypothetical reasoning. We also present a translation of Ak domain descriptions into epistemic logic programs.\n    ",
        "submission_date": "2004-04-24T00:00:00",
        "last_modified_date": "2004-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405002",
        "title": "Splitting an operator: Algebraic modularity results for logics with fixpoint semantics",
        "authors": [
            "Joost Vennekens",
            "David Gilis",
            "Marc Denecker"
        ],
        "abstract": "  It is well known that, under certain conditions, it is possible to split logic programs under stable model semantics, i.e. to divide such a program into a number of different \"levels\", such that the models of the entire program can be constructed by incrementally constructing models for each level. Similar results exist for other non-monotonic formalisms, such as auto-epistemic logic and default logic. In this work, we present a general, algebraicsplitting theory for logics with a fixpoint semantics. Together with the framework of approximation theory, a general fixpoint theory for arbitrary operators, this gives us a uniform and powerful way of deriving splitting results for each logic with a fixpoint semantics. We demonstrate the usefulness of these results, by generalizing existing results for logic programming, auto-epistemic logic and default logic.\n    ",
        "submission_date": "2004-05-03T00:00:00",
        "last_modified_date": "2006-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405004",
        "title": "Quantum Computers",
        "authors": [
            "Archil Avaliani"
        ],
        "abstract": "  This research paper gives an overview of quantum computers - description of their operation, differences between quantum and silicon computers, major construction problems of a quantum computer and many other basic aspects. No special scientific knowledge is necessary for the reader.\n    ",
        "submission_date": "2004-05-03T00:00:00",
        "last_modified_date": "2004-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405007",
        "title": "\"In vivo\" spam filtering: A challenge problem for data mining",
        "authors": [
            "Tom Fawcett"
        ],
        "abstract": "  Spam, also known as Unsolicited Commercial Email (UCE), is the bane of email communication. Many data mining researchers have addressed the problem of detecting spam, generally by treating it as a static text classification problem. True in vivo spam filtering has characteristics that make it a rich and challenging domain for data mining. Indeed, real-world datasets with these characteristics are typically difficult to acquire and to share. This paper demonstrates some of these characteristics and argues that researchers should pursue in vivo spam filtering as an accessible domain for investigating them.\n    ",
        "submission_date": "2004-05-04T00:00:00",
        "last_modified_date": "2004-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405008",
        "title": "A Comparative Study of Fuzzy Classification Methods on Breast Cancer Data",
        "authors": [
            "Ravi Jain",
            "Ajith Abraham"
        ],
        "abstract": "  In this paper, we examine the performance of four fuzzy rule generation methods on Wisconsin breast cancer data. The first method generates fuzzy if then rules using the mean and the standard deviation of attribute values. The second approach generates fuzzy if then rules using the histogram of attributes values. The third procedure generates fuzzy if then rules with certainty of each attribute into homogeneous fuzzy sets. In the fourth approach, only overlapping areas are partitioned. The first two approaches generate a single fuzzy if then rule for each class by specifying the membership function of each antecedent fuzzy set using the information about attribute values of training patterns. The other two approaches are based on fuzzy grids with homogeneous fuzzy partitions of each attribute. The performance of each approach is evaluated on breast cancer data sets. Simulation results show that the Modified grid approach has a high classification rate of 99.73 %.\n    ",
        "submission_date": "2004-05-04T00:00:00",
        "last_modified_date": "2004-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405009",
        "title": "Intelligent Systems: Architectures and Perspectives",
        "authors": [
            "Ajith Abraham"
        ],
        "abstract": "  The integration of different learning and adaptation techniques to overcome individual limitations and to achieve synergetic effects through the hybridization or fusion of these techniques has, in recent years, contributed to a large number of new intelligent system designs. Computational intelligence is an innovative framework for constructing intelligent hybrid architectures involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic Reasoning (PR) and derivative free optimization techniques such as Evolutionary Computation (EC). Most of these hybridization approaches, however, follow an ad hoc design methodology, justified by success in certain application domains. Due to the lack of a common framework it often remains difficult to compare the various hybrid systems conceptually and to evaluate their performance comparatively. This chapter introduces the different generic architectures for integrating intelligent systems. The designing aspects and perspectives of different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC systems are presented. Some conclusions are also provided towards the end.\n    ",
        "submission_date": "2004-05-04T00:00:00",
        "last_modified_date": "2004-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405010",
        "title": "A Neuro-Fuzzy Approach for Modelling Electricity Demand in Victoria",
        "authors": [
            "Ajith Abraham",
            "Baikunth Nath"
        ],
        "abstract": "  Neuro-fuzzy systems have attracted growing interest of researchers in various scientific and engineering areas due to the increasing need of intelligent systems. This paper evaluates the use of two popular soft computing techniques and conventional statistical approach based on Box--Jenkins autoregressive integrated moving average (ARIMA) model to predict electricity demand in the State of Victoria, Australia. The soft computing methods considered are an evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN) trained using scaled conjugate gradient algorithm (CGA) and backpropagation (BP) algorithm. The forecast accuracy is compared with the forecasts used by Victorian Power Exchange (VPX) and the actual energy demand. To evaluate, we considered load demand patterns for 10 consecutive months taken every 30 min for training the different prediction models. Test results show that the neuro-fuzzy system performed better than neural networks, ARIMA model and the VPX forecasts.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405011",
        "title": "Neuro Fuzzy Systems: Sate-of-the-Art Modeling Techniques",
        "authors": [
            "Ajith Abraham"
        ],
        "abstract": "  Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS) have attracted the growing interest of researchers in various scientific and engineering areas due to the growing need of adaptive intelligent systems to solve the real world problems. ANN learns from scratch by adjusting the interconnections between layers. FIS is a popular computing framework based on the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The advantages of a combination of ANN and FIS are obvious. There are several approaches to integrate ANN and FIS and very often it depends on the application. We broadly classify the integration of ANN and FIS into three categories namely concurrent model, cooperative model and fully fused model. This paper starts with a discussion of the features of each model and generalize the advantages and deficiencies of each model. We further focus the review on the different types of fused neuro-fuzzy systems and citing the advantages and disadvantages of each model.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405012",
        "title": "Is Neural Network a Reliable Forecaster on Earth? A MARS Query!",
        "authors": [
            "Ajith Abraham",
            "Dan Steinberg"
        ],
        "abstract": "  Long-term rainfall prediction is a challenging task especially in the modern world where we are facing the major environmental problem of global warming. In general, climate and rainfall are highly non-linear phenomena in nature exhibiting what is known as the butterfly effect. While some regions of the world are noticing a systematic decrease in annual rainfall, others notice increases in flooding and severe storms. The global nature of this phenomenon is very complicated and requires sophisticated computer modeling and simulation to predict accurately. In this paper, we report a performance analysis for Multivariate Adaptive Regression Splines (MARS)and artificial neural networks for one month ahead prediction of rainfall. To evaluate the prediction efficiency, we made use of 87 years of rainfall data in Kerala state, the southern part of the Indian peninsula situated at latitude -longitude pairs (8o29'N - 76o57' E). We used an artificial neural network trained using the scaled conjugate gradient algorithm. The neural network and MARS were trained with 40 years of rainfall data. For performance evaluation, network predicted outputs were compared with the actual rainfall data. Simulation results reveal that MARS is a good forecasting tool and performed better than the considered neural network.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405013",
        "title": "DCT Based Texture Classification Using Soft Computing Approach",
        "authors": [
            "Golam Sorwar",
            "Ajith Abraham"
        ],
        "abstract": "  Classification of texture pattern is one of the most important problems in pattern recognition. In this paper, we present a classification method based on the Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works on gray level image, the color scheme of each image is transformed into gray levels. For classifying the images using DCT we used two popular soft computing techniques namely neurocomputing and neuro-fuzzy computing. We used a feedforward neural network trained using the backpropagation learning and an evolving fuzzy neural network to classify the textures. The soft computing models were trained using 80% of the texture data and remaining was used for testing and validation purposes. A performance comparison was made among the soft computing models for the texture classification problem. We also analyzed the effects of prolonged training of neural networks. It is observed that the proposed neuro-fuzzy model performed better than neural network.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405014",
        "title": "Estimating Genome Reversal Distance by Genetic Algorithm",
        "authors": [
            "Andy AuYeung",
            "Ajith Abraham"
        ],
        "abstract": "  Sorting by reversals is an important problem in inferring the evolutionary relationship between two genomes. The problem of sorting unsigned permutation has been proven to be NP-hard. The best guaranteed error bounded is the 3/2- approximation algorithm. However, the problem of sorting signed permutation can be solved easily. Fast algorithms have been developed both for finding the sorting sequence and finding the reversal distance of signed permutation. In this paper, we present a way to view the problem of sorting unsigned permutation as signed permutation. And the problem can then be seen as searching an optimal signed permutation in all n2 corresponding signed permutations. We use genetic algorithm to conduct the search. Our experimental result shows that the proposed method outperform the 3/2-approximation algorithm.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405016",
        "title": "Intrusion Detection Systems Using Adaptive Regression Splines",
        "authors": [
            "Srinivas Mukkamala",
            "Andrew H. Sung",
            "Ajith Abraham",
            "Vitorino Ramos"
        ],
        "abstract": "  Past few years have witnessed a growing recognition of intelligent techniques for the construction of efficient and reliable intrusion detection systems. Due to increasing incidents of cyber attacks, building effective intrusion detection systems (IDS) are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. In this paper, we report a performance analysis between Multivariate Adaptive Regression Splines (MARS), neural networks and support vector machines. The MARS procedure builds flexible regression models by fitting separate splines to distinct intervals of the predictor variables. A brief comparison of different neural network learning algorithms is also given.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405017",
        "title": "Data Mining Approach for Analyzing Call Center Performance",
        "authors": [
            "Marcin Paprzycki",
            "Ajith Abraham",
            "Ruiyuan Guo"
        ],
        "abstract": "  The aim of our research was to apply well-known data mining techniques (such as linear neural networks, multi-layered perceptrons, probabilistic neural networks, classification and regression trees, support vector machines and finally a hybrid decision tree neural network approach) to the problem of predicting the quality of service in call centers; based on the performance data actually collected in a call center of a large insurance company. Our aim was two-fold. First, to compare the performance of models built using the above-mentioned techniques and, second, to analyze the characteristics of the input sensitivity in order to better understand the relationship between the perform-ance evaluation process and the actual performance and in this way help improve the performance of call centers. In this paper we summarize our findings.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405018",
        "title": "Modeling Chaotic Behavior of Stock Indices Using Intelligent Paradigms",
        "authors": [
            "Ajith Abraham",
            "Ninan Sajith Philip",
            "P. Saratchandran"
        ],
        "abstract": "  The use of intelligent systems for stock market predictions has been widely established. In this paper, we investigate how the seemingly chaotic behavior of stock markets could be well represented using several connectionist paradigms and soft computing techniques. To demonstrate the different techniques, we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&P CNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4 year's NIFTY index values. This paper investigates the development of a reliable and efficient technique to model the seemingly chaotic behavior of stock markets. We considered an artificial neural network trained using Levenberg-Marquardt algorithm, Support Vector Machine (SVM), Takagi-Sugeno neuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper briefly explains how the different connectionist paradigms could be formulated using different learning methods and then investigates whether they can provide the required level of performance, which are sufficiently good and robust so as to provide a reliable forecast model for stock market indices. Experiment results reveal that all the connectionist paradigms considered could represent the stock indices behavior very accurately.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405019",
        "title": "Hybrid Fuzzy-Linear Programming Approach for Multi Criteria Decision Making Problems",
        "authors": [
            "Sonja Petrovic-Lazarevic",
            "Ajith Abraham"
        ],
        "abstract": "  The purpose of this paper is to point to the usefulness of applying a linear mathematical formulation of fuzzy multiple criteria objective decision methods in organising business activities. In this respect fuzzy parameters of linear programming are modelled by preference-based membership functions. This paper begins with an introduction and some related research followed by some fundamentals of fuzzy set theory and technical concepts of fuzzy multiple objective decision models. Further a real case study of a manufacturing plant and the implementation of the proposed technique is presented. Empirical results clearly show the superiority of the fuzzy technique in optimising individual objective functions when compared to non-fuzzy approach. Furthermore, for the problem considered, the optimal solution helps to infer that by incorporating fuzziness in a linear programming model either in constraints, or both in objective functions and constraints, provides a similar (or even better) level of satisfaction for obtained results compared to non-fuzzy linear programming.\n    ",
        "submission_date": "2004-05-05T00:00:00",
        "last_modified_date": "2004-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405024",
        "title": "Meta-Learning Evolutionary Artificial Neural Networks",
        "authors": [
            "Ajith Abraham"
        ],
        "abstract": "  In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial Neural Network), an automatic computational framework for the adaptive optimization of artificial neural networks wherein the neural network architecture, activation function, connection weights; learning algorithm and its parameters are adapted according to the problem. We explored the performance of MLEANN and conventionally designed artificial neural networks for function approximation problems. To evaluate the comparative performance, we used three different well-known chaotic time series. We also present the state of the art popular neural network learning algorithms and some experimentation results related to convergence speed and generalization performance. We explored the performance of backpropagation algorithm; conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt algorithm for the three chaotic time series. Performances of the different learning algorithms were evaluated when the activation functions and architecture were changed. We further present the theoretical background, algorithm, design strategy and further demonstrate how effective and inevitable is the proposed MLEANN framework to design a neural network, which is smaller, faster and with a better generalization performance.\n    ",
        "submission_date": "2004-05-06T00:00:00",
        "last_modified_date": "2004-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405025",
        "title": "The Largest Compatible Subset Problem for Phylogenetic Data",
        "authors": [
            "Andy Auyeung",
            "Ajith Abraham"
        ],
        "abstract": "  The phylogenetic tree construction is to infer the evolutionary relationship between species from the experimental data. However, the experimental data are often imperfect and conflicting each others. Therefore, it is important to extract the motif from the imperfect data. The largest compatible subset problem is that, given a set of experimental data, we want to discard the minimum such that the remaining is compatible. The largest compatible subset problem can be viewed as the vertex cover problem in the graph theory that has been proven to be NP-hard. In this paper, we propose a hybrid Evolutionary Computing (EC) method for this problem. The proposed method combines the EC approach and the algorithmic approach for special structured graphs. As a result, the complexity of the problem is dramatically reduced. Experiments were performed on randomly generated graphs with different edge densities. The vertex covers produced by the proposed method were then compared to the vertex covers produced by a 2-approximation algorithm. The experimental results showed that the proposed method consistently outperformed a classical 2- approximation algorithm. Furthermore, a significant improvement was found when the graph density was small.\n    ",
        "submission_date": "2004-05-06T00:00:00",
        "last_modified_date": "2004-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405026",
        "title": "A Concurrent Fuzzy-Neural Network Approach for Decision Support Systems",
        "authors": [
            "Cong Tran",
            "Ajith Abraham",
            "Lakhmi Jain"
        ],
        "abstract": "  Decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved. The past few years have witnessed a growing recognition of Soft Computing technologies that underlie the conception, design and utilization of intelligent systems. Several works have been done where engineers and scientists have applied intelligent techniques and heuristics to obtain optimal decisions from imprecise information. In this paper, we present a concurrent fuzzy-neural network approach combining unsupervised and supervised learning techniques to develop the Tactical Air Combat Decision Support System (TACDSS). Experiment results clearly demonstrate the efficiency of the proposed technique.\n    ",
        "submission_date": "2004-05-06T00:00:00",
        "last_modified_date": "2004-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405027",
        "title": "Evolution of a Subsumption Architecture Neurocontroller",
        "authors": [
            "Julian Togelius"
        ],
        "abstract": "  An approach to robotics called layered evolution and merging features from the subsumption architecture into evolutionary robotics is presented, and its advantages are discussed. This approach is used to construct a layered controller for a simulated robot that learns which light source to approach in an environment with obstacles. The evolvability and performance of layered evolution on this task is compared to (standard) monolithic evolution, incremental and modularised evolution. To corroborate the hypothesis that a layered controller performs at least as well as an integrated one, the evolved layers are merged back into a single network. On the grounds of the test results, it is argued that layered evolution provides a superior approach for many tasks, and it is suggested that this approach may be the key to scaling up evolutionary robotics.\n    ",
        "submission_date": "2004-05-06T00:00:00",
        "last_modified_date": "2004-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405028",
        "title": "Analysis of Hybrid Soft and Hard Computing Techniques for Forex Monitoring Systems",
        "authors": [
            "Ajith Abraham"
        ],
        "abstract": "  In a universe with a single currency, there would be no foreign exchange market, no foreign exchange rates, and no foreign exchange. Over the past twenty-five years, the way the market has performed those tasks has changed enormously. The need for intelligent monitoring systems has become a necessity to keep track of the complex forex market. The vast currency market is a foreign concept to the average individual. However, once it is broken down into simple terms, the average individual can begin to understand the foreign exchange market and use it as a financial instrument for future investing. In this paper, we attempt to compare the performance of hybrid soft computing and hard computing techniques to predict the average monthly forex rates one month ahead. The soft computing models considered are a neural network trained by the scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive Regression Splines (MARS), Classification and Regression Trees (CART) and a hybrid CART-MARS technique. We considered the exchange rates of Australian dollar with respect to US dollar, Singapore dollar, New Zealand dollar, Japanese yen and United Kingdom pounds. The models were trained using 70% of the data and remaining was used for testing and validation purposes. It is observed that the proposed hybrid models could predict the forex rates more accurately than all the techniques when applied individually. Empirical results also reveal that the hybrid hard computing approach also improved some of our previous work using a neuro-fuzzy approach.\n    ",
        "submission_date": "2004-05-07T00:00:00",
        "last_modified_date": "2004-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405030",
        "title": "Business Intelligence from Web Usage Mining",
        "authors": [
            "Ajith Abraham"
        ],
        "abstract": "  The rapid e-commerce growth has made both business community and customers face a new situation. Due to intense competition on one hand and the customer's option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management. Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, creating adaptive Web sites, business and support services, personalization, network traffic flow analysis and so on. In this paper, we present the important concepts of Web usage mining and its various practical applications. We further present a novel approach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture of a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy inference system to analyze the Web site visitor trends. A hybrid evolutionary fuzzy clustering algorithm is proposed in this paper to optimally segregate similar user interests. The clustered data is then used to analyze the trends using a Takagi-Sugeno fuzzy inference system learned using a combination of evolutionary algorithm and neural network learning. Proposed approach is compared with self-organizing maps (to discover patterns) and several function approximation techniques like neural networks, linear genetic programming and Takagi-Sugeno fuzzy inference system (to analyze the clusters). The results are graphically illustrated and the practical significance is discussed in detail. Empirical results clearly show that the proposed Web usage-mining framework is efficient.\n    ",
        "submission_date": "2004-05-06T00:00:00",
        "last_modified_date": "2004-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405031",
        "title": "Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic Approach for Tactical Air Combat Decision Support System",
        "authors": [
            "Cong Tran",
            "Lakhmi Jain",
            "Ajith Abraham"
        ],
        "abstract": "  Normally a decision support system is build to solve problem where multi-criteria decisions are involved. The knowledge base is the vital part of the decision support containing the information or data that is used in decision-making process. This is the field where engineers and scientists have applied several intelligent techniques and heuristics to obtain optimal decisions from imprecise information. In this paper, we present a hybrid neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference system for the Tactical Air Combat Decision Support System (TACDSS). Some simulation results demonstrating the difference of the learning techniques and are also provided.\n    ",
        "submission_date": "2004-05-06T00:00:00",
        "last_modified_date": "2004-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405032",
        "title": "EvoNF: A Framework for Optimization of Fuzzy Inference Systems Using Neural Network Learning and Evolutionary Computation",
        "authors": [
            "Ajith Abraham"
        ],
        "abstract": "  Several adaptation techniques have been investigated to optimize fuzzy inference systems. Neural network learning algorithms have been used to determine the parameters of fuzzy inference system. Such models are often called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model there is no guarantee that the neural network learning algorithm converges and the tuning of fuzzy inference system will be successful. Success of evolutionary search procedures for optimization of fuzzy inference system is well proven and established in many application areas. In this paper, we will explore how the optimization of fuzzy inference systems could be further improved using a meta-heuristic approach combining neural network learning and evolutionary computation. The proposed technique could be considered as a methodology to integrate neural networks, fuzzy inference systems and evolutionary search procedures. We present the theoretical frameworks and some experimental results to demonstrate the efficiency of the proposed technique.\n    ",
        "submission_date": "2004-05-07T00:00:00",
        "last_modified_date": "2004-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405033",
        "title": "Optimization of Evolutionary Neural Networks Using Hybrid Learning Algorithms",
        "authors": [
            "Ajith Abraham"
        ],
        "abstract": "  Evolutionary artificial neural networks (EANNs) refer to a special class of artificial neural networks (ANNs) in which evolution is another fundamental form of adaptation in addition to learning. Evolutionary algorithms are used to adapt the connection weights, network architecture and learning algorithms according to the problem environment. Even though evolutionary algorithms are well known as efficient global search algorithms, very often they miss the best local solutions in the complex solution space. In this paper, we propose a hybrid meta-heuristic learning approach combining evolutionary learning and local search methods (using 1st and 2nd order error information) to improve the learning and faster convergence obtained using a direct evolutionary approach. The proposed technique is tested on three different chaotic time series and the test results are compared with some popular neuro-fuzzy systems and a recently developed cutting angle method of global optimization. Empirical results reveal that the proposed technique is efficient in spite of the computational complexity.\n    ",
        "submission_date": "2004-05-07T00:00:00",
        "last_modified_date": "2004-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405038",
        "title": "Deductive Algorithmic Knowledge",
        "authors": [
            "Riccardo Pucella"
        ],
        "abstract": "  The framework of algorithmic knowledge assumes that agents use algorithms to compute the facts they explicitly know. In many cases of interest, a deductive system, rather than a particular algorithm, captures the formal reasoning used by the agents to compute what they explicitly know. We introduce a logic for reasoning about both implicit and explicit knowledge with the latter defined with respect to a deductive system formalizing a logical theory for agents. The highly structured nature of deductive systems leads to very natural axiomatizations of the resulting logic when interpreted over any fixed deductive system. The decision problem for the logic, in the presence of a single agent, is NP-complete in general, no harder than propositional logic. It remains NP-complete when we fix a deductive system that is decidable in nondeterministic polynomial time. These results extend in a straightforward way to multiple agents.\n    ",
        "submission_date": "2004-05-11T00:00:00",
        "last_modified_date": "2006-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405049",
        "title": "Export Behaviour Modeling Using EvoNF Approach",
        "authors": [
            "Ron Edwards",
            "Ajith Abraham",
            "Sonja Petrovic-Lazarevic"
        ],
        "abstract": "  The academic literature suggests that the extent of exporting by multinational corporation subsidiaries (MCS) depends on their product manufactured, resources, tax protection, customers and markets, involvement strategy, financial independence and suppliers' relationship with a multinational corporation (MNC). The aim of this paper is to model the complex export pattern behaviour using a Takagi-Sugeno fuzzy inference system in order to determine the actual volume of MCS export output (sales exported). The proposed fuzzy inference system is optimised by using neural network learning and evolutionary computation. Empirical results clearly show that the proposed approach could model the export behaviour reasonable well compared to a direct neural network approach.\n    ",
        "submission_date": "2004-05-16T00:00:00",
        "last_modified_date": "2004-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405050",
        "title": "Traffic Accident Analysis Using Decision Trees and Neural Networks",
        "authors": [
            "Miao M. Chong",
            "Ajith Abraham",
            "Marcin Paprzycki"
        ],
        "abstract": "  The costs of fatalities and injuries due to traffic accident have a great impact on society. This paper presents our research to model the severity of injury resulting from traffic accidents using artificial neural networks and decision trees. We have applied them to an actual data set obtained from the National Automotive Sampling System (NASS) General Estimates System (GES). Experiment results reveal that in all the cases the decision tree outperforms the neural network. Our research analysis also shows that the three most important factors in fatal injury are: driver's seat belt usage, light condition of the roadway, and driver's alcohol usage.\n    ",
        "submission_date": "2004-05-16T00:00:00",
        "last_modified_date": "2004-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405051",
        "title": "Short Term Load Forecasting Models in Czech Republic Using Soft Computing Paradigms",
        "authors": [
            "Muhammad Riaz Khan",
            "Ajith Abraham"
        ],
        "abstract": "  This paper presents a comparative study of six soft computing models namely multilayer perceptron networks, Elman recurrent neural network, radial basis function network, Hopfield model, fuzzy inference system and hybrid fuzzy neural network for the hourly electricity demand forecast of Czech Republic. The soft computing models were trained and tested using the actual hourly load data for seven years. A comparison of the proposed techniques is presented for predicting 2 day ahead demands for electricity. Simulation results indicate that hybrid fuzzy neural network and radial basis function networks are the best candidates for the analysis and forecasting of electricity demand.\n    ",
        "submission_date": "2004-05-16T00:00:00",
        "last_modified_date": "2004-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405052",
        "title": "Decision Support Systems Using Intelligent Paradigms",
        "authors": [
            "Cong Tran",
            "Ajith Abraham",
            "Lakhmi Jain"
        ],
        "abstract": "  Decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved. The past few years have witnessed a growing recognition of Soft Computing (SC) technologies that underlie the conception, design and utilization of intelligent systems. In this paper, we present different SC paradigms involving an artificial neural network trained using the scaled conjugate gradient algorithm, two different fuzzy inference methods optimised using neural network learning/evolutionary algorithms and regression trees for developing intelligent decision support systems. We demonstrate the efficiency of the different algorithms by developing a decision support system for a Tactical Air Combat Environment (TACE). Some empirical comparisons between the different algorithms are also provided.\n    ",
        "submission_date": "2004-05-16T00:00:00",
        "last_modified_date": "2004-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405071",
        "title": "Regression with respect to sensing actions and partial states",
        "authors": [
            "Le-Chi Tuan",
            "Chitta Baral",
            "Tran Cao Son"
        ],
        "abstract": "  In this paper, we present a state-based regression function for planning domains where an agent does not have complete information and may have sensing actions. We consider binary domains and employ the 0-approximation [Son & Baral 2001] to define the regression function. In binary domains, the use of 0-approximation means using 3-valued states. Although planning using this approach is incomplete with respect to the full semantics, we adopt it to have a lower complexity. We prove the soundness and completeness of our regression formulation with respect to the definition of progression. More specifically, we show that (i) a plan obtained through regression for a planning problem is indeed a progression solution of that planning problem, and that (ii) for each plan found through progression, using regression one obtains that plan or an equivalent one. We then develop a conditional planner that utilizes our regression function. We prove the soundness and completeness of our planning algorithm and present experimental results with respect to several well known planning problems in the literature.\n    ",
        "submission_date": "2004-05-21T00:00:00",
        "last_modified_date": "2004-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405090",
        "title": "Propositional Defeasible Logic has Linear Complexity",
        "authors": [
            "Michael J. Maher"
        ],
        "abstract": "  Defeasible logic is a rule-based nonmonotonic logic, with both strict and defeasible rules, and a priority relation on rules. We show that inference in the propositional form of the logic can be performed in linear time. This contrasts markedly with most other propositional nonmonotonic logics, in which inference is intractable.\n    ",
        "submission_date": "2004-05-24T00:00:00",
        "last_modified_date": "2004-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405098",
        "title": "A Logic for Reasoning about Evidence",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "abstract": "  We introduce a logic for reasoning about evidence that essentially views evidence as a function from prior beliefs (before making an observation) to posterior beliefs (after making the observation). We provide a sound and complete axiomatization for the logic, and consider the complexity of the decision problem. Although the reasoning in the logic is mainly propositional, we allow variables representing numbers and quantification over them. This expressive power seems necessary to capture important properties of evidence.\n    ",
        "submission_date": "2004-05-26T00:00:00",
        "last_modified_date": "2006-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405106",
        "title": "Pruning Search Space in Defeasible Argumentation",
        "authors": [
            "Carlos Iv\u00e1n Ches\u00f1evar",
            "Guillermo Ricardo Simari",
            "Alejandro Javier Garc\u00eda"
        ],
        "abstract": "  Defeasible argumentation has experienced a considerable growth in AI in the last decade. Theoretical results have been combined with development of practical applications in AI & Law, Case-Based Reasoning and various knowledge-based systems. However, the dialectical process associated with inference is computationally expensive. This paper focuses on speeding up this inference process by pruning the involved search space. Our approach is twofold. On one hand, we identify distinguished literals for computing defeat. On the other hand, we restrict ourselves to a subset of all possible conflicting arguments by introducing dialectical constraints.\n    ",
        "submission_date": "2004-05-27T00:00:00",
        "last_modified_date": "2004-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405107",
        "title": "A Framework for Combining Defeasible Argumentation with Labeled Deduction",
        "authors": [
            "Carlos Iv\u00e1n Ches\u00f1evar",
            "Guillermo Ricardo Simari"
        ],
        "abstract": "  In the last years, there has been an increasing demand of a variety of logical systems, prompted mostly by applications of logic in AI and other related areas. Labeled Deductive Systems (LDS) were developed as a flexible methodology to formalize such a kind of complex logical systems. Defeasible argumentation has proven to be a successful approach to formalizing commonsense reasoning, encompassing many other alternative formalisms for defeasible reasoning. Argument-based frameworks share some common notions (such as the concept of argument, defeater, etc.) along with a number of particular features which make it difficult to compare them with each other from a logical viewpoint. This paper introduces LDSar, a LDS for defeasible argumentation in which many important issues concerning defeasible argumentation are captured within a unified logical framework. We also discuss some logical properties and extensions that emerge from the proposed framework.\n    ",
        "submission_date": "2004-05-27T00:00:00",
        "last_modified_date": "2004-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405113",
        "title": "A proposal to design expert system for the calculations in the domain of QFT",
        "authors": [
            "Andrea Severe"
        ],
        "abstract": "  Main purposes of the paper are followings: 1) To show examples of the calculations in domain of QFT via ``derivative rules'' of an expert system; 2) To consider advantages and disadvantage that technology of the calculations; 3) To reflect about how one would develop new physical theories, what knowledge would be useful in their investigations and how this problem can be connected with designing an expert system.\n    ",
        "submission_date": "2004-05-31T00:00:00",
        "last_modified_date": "2004-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0406025",
        "title": "Directional Consistency for Continuous Numerical Constraints",
        "authors": [
            "Frederic Goualard",
            "Laurent Granvilliers"
        ],
        "abstract": "  Bounds consistency is usually enforced on continuous constraints by first decomposing them into binary and ternary primitives. This decomposition has long been shown to drastically slow down the computation of solutions. To tackle this, Benhamou et al. have introduced an algorithm that avoids formally decomposing constraints. Its better efficiency compared to the former method has already been experimentally demonstrated. It is shown here that their algorithm implements a strategy to enforce on a continuous constraint a consistency akin to Directional Bounds Consistency as introduced by Dechter and Pearl for discrete problems. The algorithm is analyzed in this framework, and compared with algorithms that enforce bounds consistency. These theoretical results are eventually contrasted with new experimental results on standard benchmarks from the interval constraint community.\n    ",
        "submission_date": "2004-06-16T00:00:00",
        "last_modified_date": "2004-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0406038",
        "title": "A New Approach to Draw Detection by Move Repetition in Computer Chess Programming",
        "authors": [
            "Vladan Vuckovic",
            "Djordje Vidanovic"
        ],
        "abstract": "  We will try to tackle both the theoretical and practical aspects of a very important problem in chess programming as stated in the title of this article - the issue of draw detection by move repetition. The standard approach that has so far been employed in most chess programs is based on utilising positional matrices in original and compressed format as well as on the implementation of the so-called bitboard format.\n",
        "submission_date": "2004-06-21T00:00:00",
        "last_modified_date": "2004-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0406055",
        "title": "Web Services: A Process Algebra Approach",
        "authors": [
            "Andrea Ferrara"
        ],
        "abstract": "  It is now well-admitted that formal methods are helpful for many issues raised in the Web service area. In this paper we present a framework for the design and verification of WSs using process algebras and their tools. We define a two-way mapping between abstract specifications written using these calculi and executable Web services written in BPEL4WS. Several choices are available: design and correct errors in BPEL4WS, using process algebra verification tools, or design and correct in process algebra and automatically obtaining the corresponding BPEL4WS code. The approaches can be combined. Process algebra are not useful only for temporal logic verification: we remark the use of simulation/bisimulation both for verification and for the hierarchical refinement design method. It is worth noting that our approach allows the use of any process algebra depending on the needs of the user at different levels (expressiveness, existence of reasoning tools, user expertise).\n    ",
        "submission_date": "2004-06-28T00:00:00",
        "last_modified_date": "2004-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407008",
        "title": "Autogenic Training With Natural Language Processing Modules: A Recent Tool For Certain Neuro Cognitive Studies",
        "authors": [
            "S. Ravichandran",
            "M.N. Karthik"
        ],
        "abstract": "  Learning to respond to voice-text input involves the subject's ability in understanding the phonetic and text based contents and his/her ability to communicate based on his/her experience. The neuro-cognitive facility of the subject has to support two important domains in order to make the learning process complete. In many cases, though the understanding is complete, the response is partial. This is one valid reason why we need to support the information from the subject with scalable techniques such as Natural Language Processing (NLP) for abstraction of the contents from the output. This paper explores the feasibility of using NLP modules interlaced with Neural Networks to perform the required task in autogenic training related to medical applications.\n    ",
        "submission_date": "2004-07-02T00:00:00",
        "last_modified_date": "2004-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407009",
        "title": "Search Using N-gram Technique Based Statistical Analysis for Knowledge Extraction in Case Based Reasoning Systems",
        "authors": [
            "M. N. Karthik",
            "Moshe Davis"
        ],
        "abstract": "  Searching techniques for Case Based Reasoning systems involve extensive methods of elimination. In this paper, we look at a new method of arriving at the right solution by performing a series of transformations upon the data. These involve N-gram based comparison and deduction of the input data with the case data, using Morphemes and Phonemes as the deciding parameters. A similar technique for eliminating possible errors using a noise removal function is performed. The error tracking and elimination is performed through a statistical analysis of obtained data, where the entire data set is analyzed as sub-categories of various etymological derivatives. A probability analysis for the closest match is then performed, which yields the final expression. This final expression is referred to the Case Base. The output is redirected through an Expert System based on best possible match. The threshold for the match is customizable, and could be set by the Knowledge-Architect.\n    ",
        "submission_date": "2004-07-02T00:00:00",
        "last_modified_date": "2004-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407016",
        "title": "Learning for Adaptive Real-time Search",
        "authors": [
            "Vadim Bulitko"
        ],
        "abstract": "  Real-time heuristic search is a popular model of acting and learning in intelligent autonomous agents. Learning real-time search agents improve their performance over time by acquiring and refining a value function guiding the application of their actions. As computing the perfect value function is typically intractable, a heuristic approximation is acquired instead. Most studies of learning in real-time search (and reinforcement learning) assume that a simple value-function-greedy policy is used to select actions. This is in contrast to practice, where high-performance is usually attained by interleaving planning and acting via a lookahead search of a non-trivial depth. In this paper, we take a step toward bridging this gap and propose a novel algorithm that (i) learns a heuristic function to be used specifically with a lookahead-based policy, (ii) selects the lookahead depth adaptively in each state, (iii) gives the user control over the trade-off between exploration and exploitation. We extensively evaluate the algorithm in the sliding tile puzzle testbed comparing it to the classical LRTA* and the more recent weighted LRTA*, bounded LRTA*, and FALCONS. Improvements of 5 to 30 folds in convergence speed are observed.\n    ",
        "submission_date": "2004-07-06T00:00:00",
        "last_modified_date": "2004-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407034",
        "title": "On the Complexity of Case-Based Planning",
        "authors": [
            "Paolo Liberatore"
        ],
        "abstract": "  We analyze the computational complexity of problems related to case-based planning: planning when a plan for a similar instance is known, and planning from a library of plans. We prove that planning from a single case has the same complexity than generative planning (i.e., planning \"from scratch\"); using an extended definition of cases, complexity is reduced if the domain stored in the case is similar to the one to search plans for. Planning from a library of cases is shown to have the same complexity. In both cases, the complexity of planning remains, in the worst case, PSPACE-complete.\n    ",
        "submission_date": "2004-07-15T00:00:00",
        "last_modified_date": "2004-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407037",
        "title": "Generalized Evolutionary Algorithm based on Tsallis Statistics",
        "authors": [
            "Ambedkar Dukkipati",
            "M. Narasimha Murty",
            "Shalabh Bhatnagar"
        ],
        "abstract": "  Generalized evolutionary algorithm based on Tsallis canonical distribution is proposed. The algorithm uses Tsallis generalized canonical distribution to weigh the configurations for `selection' instead of Gibbs-Boltzmann distribution. Our simulation results show that for an appropriate choice of non-extensive index that is offered by Tsallis statistics, evolutionary algorithms based on this generalization outperform algorithms based on Gibbs-Boltzmann distribution.\n    ",
        "submission_date": "2004-07-16T00:00:00",
        "last_modified_date": "2004-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407040",
        "title": "Decomposition Based Search - A theoretical and experimental evaluation",
        "authors": [
            "W.J. van Hoeve",
            "M. Milano"
        ],
        "abstract": "  In this paper we present and evaluate a search strategy called Decomposition Based Search (DBS) which is based on two steps: subproblem generation and subproblem solution. The generation of subproblems is done through value ranking and domain splitting. Subdomains are explored so as to generate, according to the heuristic chosen, promising subproblems first.\n",
        "submission_date": "2004-07-16T00:00:00",
        "last_modified_date": "2004-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407042",
        "title": "Postponing Branching Decisions",
        "authors": [
            "Willem Jan van Hoeve",
            "Michela Milano"
        ],
        "abstract": "  Solution techniques for Constraint Satisfaction and Optimisation Problems often make use of backtrack search methods, exploiting variable and value ordering heuristics. In this paper, we propose and analyse a very simple method to apply in case the value ordering heuristic produces ties: postponing the branching decision. To this end, we group together values in a tie, branch on this sub-domain, and defer the decision among them to lower levels of the search tree. We show theoretically and experimentally that this simple modification can dramatically improve the efficiency of the search strategy. Although in practise similar methods may have been applied already, to our knowledge, no empirical or theoretical study has been proposed in the literature to identify when and to what extent this strategy should be used.\n    ",
        "submission_date": "2004-07-16T00:00:00",
        "last_modified_date": "2004-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407044",
        "title": "Reduced cost-based ranking for generating promising subproblems",
        "authors": [
            "M. Milano",
            "W.J. van Hoeve"
        ],
        "abstract": "  In this paper, we propose an effective search procedure that interleaves two steps: subproblem generation and subproblem solution. We mainly focus on the first part. It consists of a variable domain value ranking based on reduced costs. Exploiting the ranking, we generate, in a Limited Discrepancy Search tree, the most promising subproblems first. An interesting result is that reduced costs provide a very precise ranking that allows to almost always find the optimal solution in the first generated subproblem, even if its dimension is significantly smaller than that of the original problem. Concerning the proof of optimality, we exploit a way to increase the lower bound for subproblems at higher discrepancies. We show experimental results on the TSP and its time constrained variant to show the effectiveness of the proposed approach, but the technique could be generalized for other problems.\n    ",
        "submission_date": "2004-07-16T00:00:00",
        "last_modified_date": "2004-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408010",
        "title": "A Simple Proportional Conflict Redistribution Rule",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "abstract": "  One proposes a first alternative rule of combination to WAO (Weighted Average Operator) proposed recently by Josang, Daniel and Vannoorenberghe, called Proportional Conflict Redistribution rule (denoted PCR1). PCR1 and WAO are particular cases of WO (the Weighted Operator) because the conflicting mass is redistributed with respect to some weighting factors. In this first PCR rule, the proportionalization is done for each non-empty set with respect to the non-zero sum of its corresponding mass matrix - instead of its mass column average as in WAO, but the results are the same as Ph. Smets has pointed out. Also, we extend WAO (which herein gives no solution) for the degenerate case when all column sums of all non-empty sets are zero, and then the conflicting mass is transferred to the non-empty disjunctive form of all non-empty sets together; but if this disjunctive form happens to be empty, then one considers an open world (i.e. the frame of discernment might contain new hypotheses) and thus all conflicting mass is transferred to the empty set. In addition to WAO, we propose a general formula for PCR1 (WAO for non-degenerate cases).\n    ",
        "submission_date": "2004-08-03T00:00:00",
        "last_modified_date": "2004-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408021",
        "title": "An Algorithm for Quasi-Associative and Quasi-Markovian Rules of Combination in Information Fusion",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "abstract": "  In this paper one proposes a simple algorithm of combining the fusion rules, those rules which first use the conjunctive rule and then the transfer of conflicting mass to the non-empty sets, in such a way that they gain the property of associativity and fulfill the Markovian requirement for dynamic fusion. Also, a new rule, SDL-improved, is presented.\n    ",
        "submission_date": "2004-08-08T00:00:00",
        "last_modified_date": "2004-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408023",
        "title": "On Global Warming (Softening Global Constraints)",
        "authors": [
            "Willem Jan van Hoeve",
            "Gilles Pesant",
            "Louis-Martin Rousseau"
        ],
        "abstract": "  We describe soft versions of the global cardinality constraint and the regular constraint, with efficient filtering algorithms maintaining domain consistency. For both constraints, the softening is achieved by augmenting the underlying graph. The softened constraints can be used to extend the meta-constraint framework for over-constrained problems proposed by Petit, Regin and Bessiere.\n    ",
        "submission_date": "2004-08-09T00:00:00",
        "last_modified_date": "2004-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408044",
        "title": "FLUX: A Logic Programming Method for Reasoning Agents",
        "authors": [
            "Michael Thielscher"
        ],
        "abstract": "  FLUX is a programming method for the design of agents that reason logically about their actions and sensor information in the presence of incomplete knowledge. The core of FLUX is a system of Constraint Handling Rules, which enables agents to maintain an internal model of their environment by which they control their own behavior. The general action representation formalism of the fluent calculus provides the formal semantics for the constraint solver. FLUX exhibits excellent computational behavior due to both a carefully restricted expressiveness and the inference paradigm of progression.\n    ",
        "submission_date": "2004-08-19T00:00:00",
        "last_modified_date": "2004-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408055",
        "title": "Cauchy Annealing Schedule: An Annealing Schedule for Boltzmann Selection Scheme in Evolutionary Algorithms",
        "authors": [
            "Ambedkar Dukkipati",
            "M. Narasimha Murty",
            "Shalabh Bhatnagar"
        ],
        "abstract": "  Boltzmann selection is an important selection mechanism in evolutionary algorithms as it has theoretical properties which help in theoretical analysis. However, Boltzmann selection is not used in practice because a good annealing schedule for the `inverse temperature' parameter is lacking. In this paper we propose a Cauchy annealing schedule for Boltzmann selection scheme based on a hypothesis that selection-strength should increase as evolutionary process goes on and distance between two selection strengths should decrease for the process to converge. To formalize these aspects, we develop formalism for selection mechanisms using fitness distributions and give an appropriate measure for selection-strength. In this paper, we prove an important result, by which we derive an annealing schedule called Cauchy annealing schedule. We demonstrate the novelty of proposed annealing schedule using simulations in the framework of genetic algorithms.\n    ",
        "submission_date": "2004-08-24T00:00:00",
        "last_modified_date": "2004-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408064",
        "title": "Proportional Conflict Redistribution Rules for Information Fusion",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "abstract": "  In this paper we propose five versions of a Proportional Conflict Redistribution rule (PCR) for information fusion together with several examples. From PCR1 to PCR2, PCR3, PCR4, PCR5 one increases the complexity of the rules and also the exactitude of the redistribution of conflicting masses. PCR1 restricted from the hyper-power set to the power set and without degenerate cases gives the same result as the Weighted Average Operator (WAO) proposed recently by J\u00f8sang, Daniel and Vannoorenberghe but does not satisfy the neutrality property of vacuous belief assignment. That's why improved PCR rules are proposed in this paper. PCR4 is an improvement of minC and Dempster's rules. The PCR rules redistribute the conflicting mass, after the conjunctive rule has been applied, proportionally with some functions depending on the masses assigned to their corresponding columns in the mass matrix. There are infinitely many ways these functions (weighting factors) can be chosen depending on the complexity one wants to deal with in specific applications and fusion systems. Any fusion combination rule is at some degree ad-hoc.\n    ",
        "submission_date": "2004-08-28T00:00:00",
        "last_modified_date": "2005-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408069",
        "title": "The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challenge for Artificial Intelligence",
        "authors": [
            "Sebastian Bader",
            "Pascal Hitzler",
            "Steffen Hoelldobler"
        ],
        "abstract": "  Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.\n    ",
        "submission_date": "2004-08-31T00:00:00",
        "last_modified_date": "2004-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0409002",
        "title": "Default reasoning over domains and concept hierarchies",
        "authors": [
            "Pascal Hitzler"
        ],
        "abstract": "  W.C. Rounds and G.-Q. Zhang (2001) have proposed to study a form of disjunctive logic programming generalized to algebraic domains. This system allows reasoning with information which is hierarchically structured and forms a (suitable) domain. We extend this framework to include reasoning with default negation, giving rise to a new nonmonotonic reasoning framework on hierarchical knowledge which encompasses answer set programming with extended disjunctive logic programs. We also show that the hierarchically structured knowledge on which programming in this paradigm can be done, arises very naturally from formal concept analysis. Together, we obtain a default reasoning paradigm for conceptual knowledge which is in accordance with mainstream developments in nonmonotonic reasoning.\n    ",
        "submission_date": "2004-09-01T00:00:00",
        "last_modified_date": "2004-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0409003",
        "title": "ScheduleNanny: Using GPS to Learn the User's Significant Locations, Travel Times and Schedule",
        "authors": [
            "Parth Bhawalkar",
            "Victor Bigio",
            "Adam Davis",
            "Karthik Narayanaswami",
            "Femi Olumoko"
        ],
        "abstract": "  As computing technology becomes more pervasive, personal devices such as the PDA, cell-phone, and notebook should use context to determine how to act. Location is one form of context that can be used in many ways. We present a multiple-device system that collects and clusters GPS data into significant locations. These locations are then used to determine travel times and a probabilistic model of the user's schedule, which is used to intelligently alert the user. We evaluate our system and suggest how it should be integrated with a variety of applications.\n    ",
        "submission_date": "2004-09-02T00:00:00",
        "last_modified_date": "2004-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0409007",
        "title": "The Generalized Pignistic Transformation",
        "authors": [
            "Jean Dezert",
            "Florentin Smarandache",
            "Milan Daniel"
        ],
        "abstract": "  This paper presents in detail the generalized pignistic transformation (GPT) succinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a tool for decision process. The GPT allows to provide a subjective probability measure from any generalized basic belief assignment given by any corpus of evidence. We mainly focus our presentation on the 3D case and provide the complete result obtained by the GPT and its validation drawn from the probability theory.\n    ",
        "submission_date": "2004-09-06T00:00:00",
        "last_modified_date": "2004-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0409019",
        "title": "Outlier Detection by Logic Programming",
        "authors": [
            "Fabrizio Angiulli",
            "Gianluigi Greco",
            "Luigi Palopoli"
        ],
        "abstract": "  The development of effective knowledge discovery techniques has become in the recent few years a very active research area due to the important impact it has in several relevant application areas. One interesting task thereof is that of singling out anomalous individuals from a given population, e.g., to detect rare events in time-series analysis settings, or to identify objects whose behavior is deviant w.r.t. a codified standard set of \"social\" rules. Such exceptional individuals are usually referred to as outliers in the literature.\n",
        "submission_date": "2004-09-09T00:00:00",
        "last_modified_date": "2005-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0409040",
        "title": "Unification of Fusion Theories",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "  Since no fusion theory neither rule fully satisfy all needed applications, the author proposes a Unification of Fusion Theories and a combination of fusion rules in solving problems/applications. For each particular application, one selects the most appropriate model, rule(s), and algorithm of implementation. We are working in the unification of the fusion theories and rules, which looks like a cooking recipe, better we'd say like a logical chart for a computer programmer, but we don't see another method to comprise/unify all things. The unification scenario presented herein, which is now in an incipient form, should periodically be updated incorporating new discoveries from the fusion and engineering research.\n    ",
        "submission_date": "2004-09-23T00:00:00",
        "last_modified_date": "2004-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0409045",
        "title": "Augmenting ALC(D) (atemporal) roles and (aspatial) concrete domain with temporal roles and a spatial concrete domain -first results",
        "authors": [
            "Amar Isli"
        ],
        "abstract": "  We consider the well-known family ALC(D) of description logics with a concrete domain, and provide first results on a framework obtained by augmenting ALC(D) atemporal roles and aspatial concrete domain with temporal roles and a spatial concrete domain.\n    ",
        "submission_date": "2004-09-24T00:00:00",
        "last_modified_date": "2004-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0409046",
        "title": "A TCSP-like decidable constraint language generalising existing cardinal direction relations",
        "authors": [
            "Amar Isli"
        ],
        "abstract": "  We define a quantitative constraint language subsuming two calculi well-known in QSR (Qualitative Spatial Reasoning): Frank's cone-shaped and projection-based calculi of cardinal direction relations. We show how to solve a CSP (Constraint Satisfaction Problem) expressed in the language.\n    ",
        "submission_date": "2004-09-24T00:00:00",
        "last_modified_date": "2004-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0409047",
        "title": "An ALC(D)-based combination of temporal constraints and spatial constraints suitable for continuous (spatial) change",
        "authors": [
            "Amar Isli"
        ],
        "abstract": "  We present a family of spatio-temporal theories suitable for continuous spatial change in general, and for continuous motion of spatial scenes in particular. The family is obtained by spatio-temporalising the well-known ALC(D) family of Description Logics (DLs) with a concrete domain D, as follows, where TCSPs denotes \"Temporal Constraint Satisfaction Problems\", a well-known constraint-based framework:\n",
        "submission_date": "2004-09-24T00:00:00",
        "last_modified_date": "2004-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410004",
        "title": "Applying Policy Iteration for Training Recurrent Neural Networks",
        "authors": [
            "I. Szita",
            "A. Lorincz"
        ],
        "abstract": "  Recurrent neural networks are often used for learning time-series data. Based on a few assumptions we model this learning task as a minimization problem of a nonlinear least-squares cost function. The special structure of the cost function allows us to build a connection to reinforcement learning. We exploit this connection and derive a convergent, policy iteration-based algorithm. Furthermore, we argue that RNN training can be fit naturally into the reinforcement learning framework.\n    ",
        "submission_date": "2004-10-02T00:00:00",
        "last_modified_date": "2004-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410014",
        "title": "Normal forms for Answer Sets Programming",
        "authors": [
            "Stefania Costantini",
            "Alessandro Provetti"
        ],
        "abstract": "  Normal forms for logic programs under stable/answer set semantics are introduced. We argue that these forms can simplify the study of program properties, mainly consistency. The first normal form, called the {\\em kernel} of the program, is useful for studying existence and number of answer sets. A kernel program is composed of the atoms which are undefined in the Well-founded semantics, which are those that directly affect the existence of answer sets. The body of rules is composed of negative literals only. Thus, the kernel form tends to be significantly more compact than other formulations. Also, it is possible to check consistency of kernel programs in terms of colorings of the Extended Dependency Graph program representation which we previously developed. The second normal form is called {\\em 3-kernel.} A 3-kernel program is composed of the atoms which are undefined in the Well-founded semantics. Rules in 3-kernel programs have at most two conditions, and each rule either belongs to a cycle, or defines a connection between cycles. 3-kernel programs may have positive conditions. The 3-kernel normal form is very useful for the static analysis of program consistency, i.e., the syntactic characterization of existence of answer sets. This result can be obtained thanks to a novel graph-like representation of programs, called Cycle Graph which presented in the companion article \\cite{Cos04b}.\n    ",
        "submission_date": "2004-10-06T00:00:00",
        "last_modified_date": "2004-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410033",
        "title": "An In-Depth Look at Information Fusion Rules & the Unification of Fusion Theories",
        "authors": [
            "Florentin Smarandache"
        ],
        "abstract": "  This paper may look like a glossary of the fusion rules and we also introduce new ones presenting their formulas and examples: Conjunctive, Disjunctive, Exclusive Disjunctive, Mixed Conjunctive-Disjunctive rules, Conditional rule, Dempster's, Yager's, Smets' TBM rule, Dubois-Prade's, Dezert-Smarandache classical and hybrid rules, Murphy's average rule, Inagaki-Lefevre-Colot-Vannoorenberghe Unified Combination rules [and, as particular cases: Iganaki's parameterized rule, Weighting Average Operator, minC (M. Daniel), and newly Proportional Conflict Redistribution rules (Smarandache-Dezert) among which PCR5 is the most exact way of redistribution of the conflicting mass to non-empty sets following the path of the conjunctive rule], Zhang's Center Combination rule, Convolutive x-Averaging, Consensus Operator (Josang), Cautious Rule (Smets), ?-junctions rules (Smets), etc. and three new T-norm & T-conorm rules adjusted from fuzzy and neutrosophic sets to information fusion (Tchamova-Smarandache). Introducing the degree of union and degree of inclusion with respect to the cardinal of sets not with the fuzzy set point of view, besides that of intersection, many fusion rules can be improved. There are corner cases where each rule might have difficulties working or may not get an expected result.\n    ",
        "submission_date": "2004-10-14T00:00:00",
        "last_modified_date": "2004-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410049",
        "title": "Intransitivity and Vagueness",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "  There are many examples in the literature that suggest that indistinguishability is intransitive, despite the fact that the indistinguishability relation is typically taken to be an equivalence relation (and thus transitive). It is shown that if the uncertainty perception and the question of when an agent reports that two things are indistinguishable are both carefully modeled, the problems disappear, and indistinguishability can indeed be taken to be an equivalence relation. Moreover, this model also suggests a logic of vagueness that seems to solve many of the problems related to vagueness discussed in the philosophical literature. In particular, it is shown here how the logic can handle the sorites paradox.\n    ",
        "submission_date": "2004-10-19T00:00:00",
        "last_modified_date": "2004-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410050",
        "title": "Sleeping Beauty Reconsidered: Conditioning and Reflection in Asynchronous Systems",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "abstract": "  A careful analysis of conditioning in the Sleeping Beauty problem is done, using the formal model for reasoning about knowledge and probability developed by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as revealing problems with conditioning in the presence of imperfect recall, the analysis done here reveals that the problems are not so much due to imperfect recall as to asynchrony. The implications of this analysis for van Fraassen's Reflection Principle and Savage's Sure-Thing Principle are considered.\n    ",
        "submission_date": "2004-10-19T00:00:00",
        "last_modified_date": "2004-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411015",
        "title": "Bounded Input Bounded Predefined Control Bounded Output",
        "authors": [
            "Ziny Flikop"
        ],
        "abstract": "  The paper is an attempt to generalize a methodology, which is similar to the bounded-input bounded-output method currently widely used for the system stability studies. The presented earlier methodology allows decomposition of input space into bounded subspaces and defining for each subspace its bounding surface. It also defines a corresponding predefined control, which maps any point of a bounded input into a desired bounded output subspace. This methodology was improved by providing a mechanism for the fast defining a bounded surface. This paper presents enhanced bounded-input bounded-predefined-control bounded-output approach, which provides adaptability feature to the control and allows transferring of a controlled system along a suboptimal trajectory.\n    ",
        "submission_date": "2004-11-08T00:00:00",
        "last_modified_date": "2004-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411016",
        "title": "Intelligent search strategies based on adaptive Constraint Handling Rules",
        "authors": [
            "Armin Wolf"
        ],
        "abstract": "  The most advanced implementation of adaptive constraint processing with Constraint Handling Rules (CHR) allows the application of intelligent search strategies to solve Constraint Satisfaction Problems (CSP). This presentation compares an improved version of conflict-directed backjumping and two variants of dynamic backtracking with respect to chronological backtracking on some of the AIM instances which are a benchmark set of random 3-SAT problems. A CHR implementation of a Boolean constraint solver combined with these different search strategies in Java is thus being compared with a CHR implementation of the same Boolean constraint solver combined with chronological backtracking in SICStus Prolog. This comparison shows that the addition of ``intelligence'' to the search process may reduce the number of search steps dramatically. Furthermore, the runtime of their Java implementations is in most cases faster than the implementations of chronological backtracking. More specifically, conflict-directed backjumping is even faster than the SICStus Prolog implementation of chronological backtracking, although our Java implementation of CHR lacks the optimisations made in the SICStus Prolog system. To appear in Theory and Practice of Logic Programming (TPLP).\n    ",
        "submission_date": "2004-11-08T00:00:00",
        "last_modified_date": "2004-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411034",
        "title": "Generating Conditional Probabilities for Bayesian Networks: Easing the Knowledge Acquisition Problem",
        "authors": [
            "Balaram Das"
        ],
        "abstract": "  The number of probability distributions required to populate a conditional probability table (CPT) in a Bayesian network, grows exponentially with the number of parent-nodes associated with that table. If the table is to be populated through knowledge elicited from a domain expert then the sheer magnitude of the task forms a considerable cognitive barrier. In this paper we devise an algorithm to populate the CPT while easing the extent of knowledge acquisition. The input to the algorithm consists of a set of weights that quantify the relative strengths of the influences of the parent-nodes on the child-node, and a set of probability distributions the number of which grows only linearly with the number of associated parent-nodes. These are elicited from the domain expert. The set of probabilities are obtained by taking into consideration the heuristics that experts use while arriving at probabilistic estimations. The algorithm is used to populate the CPT by computing appropriate weighted sums of the elicited distributions. We invoke the methods of information geometry to demonstrate how these weighted sums capture the expert's judgemental strategy.\n    ",
        "submission_date": "2004-11-12T00:00:00",
        "last_modified_date": "2008-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411071",
        "title": "Comparing Multi-Target Trackers on Different Force Unit Levels",
        "authors": [
            "Hedvig Sidenbladh",
            "Pontus Svenson",
            "Johan Schubert"
        ],
        "abstract": "  Consider the problem of tracking a set of moving targets. Apart from the tracking result, it is often important to know where the tracking fails, either to steer sensors to that part of the state-space, or to inform a human operator about the status and quality of the obtained information. An intuitive quality measure is the correlation between two tracking results based on uncorrelated observations. In the case of Bayesian trackers such a correlation measure could be the Kullback-Leibler difference.\n",
        "submission_date": "2004-11-19T00:00:00",
        "last_modified_date": "2004-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411072",
        "title": "Extremal optimization for sensor report pre-processing",
        "authors": [
            "Pontus Svenson"
        ],
        "abstract": "  We describe the recently introduced extremal optimization algorithm and apply it to target detection and association problems arising in pre-processing for multi-target tracking.\n",
        "submission_date": "2004-11-19T00:00:00",
        "last_modified_date": "2004-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412002",
        "title": "Ranking Pages by Topology and Popularity within Web Sites",
        "authors": [
            "Jose Borges",
            "Mark Levene"
        ],
        "abstract": "  We compare two link analysis ranking methods of web pages in a site. The first, called Site Rank, is an adaptation of PageRank to the granularity of a web site and the second, called Popularity Rank, is based on the frequencies of user clicks on the outlinks in a page that are captured by navigation sessions of users through the web site. We ran experiments on artificially created web sites of different sizes and on two real data sets, employing the relative entropy to compare the distributions of the two ranking methods. For the real data sets we also employ a nonparametric measure, called Spearman's footrule, which we use to compare the top-ten web pages ranked by the two methods. Our main result is that the distributions of the Popularity Rank and Site Rank are surprisingly close to each other, implying that the topology of a web site is very instrumental in guiding users through the site. Thus, in practice, the Site Rank provides a reasonable first order approximation of the aggregate behaviour of users within a web site given by the Popularity Rank.\n    ",
        "submission_date": "2004-12-01T00:00:00",
        "last_modified_date": "2004-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412021",
        "title": "Finite Domain Bounds Consistency Revisited",
        "authors": [
            "Chiu Wo Choi",
            "Warwick Harvey",
            "Jimmy Ho-Man Lee",
            "Peter J. Stuckey"
        ],
        "abstract": "  A widely adopted approach to solving constraint satisfaction problems combines systematic tree search with constraint propagation for pruning the search space. Constraint propagation is performed by propagators implementing a certain notion of consistency. Bounds consistency is the method of choice for building propagators for arithmetic constraints and several global constraints in the finite integer domain. However, there has been some confusion in the definition of bounds consistency. In this paper we clarify the differences and similarities among the three commonly used notions of bounds consistency.\n    ",
        "submission_date": "2004-12-06T00:00:00",
        "last_modified_date": "2004-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412066",
        "title": "From Feature Extraction to Classification: A multidisciplinary Approach applied to Portuguese Granites",
        "authors": [
            "Vitorino Ramos",
            "Pedro Pina",
            "Fernando Muge"
        ],
        "abstract": "  The purpose of this paper is to present a complete methodology based on a multidisciplinary approach, that goes from the extraction of features till the classification of a set of different portuguese granites. The set of tools to extract the features that characterise polished surfaces of the granites is mainly based on mathematical morphology. The classification methodology is based on a genetic algorithm capable of search the input feature space used by the nearest neighbour rule classifier. Results show that is adequate to perform feature reduction and simultaneous improve the recognition rate. Moreover, the present methodology represents a robust strategy to understand the proper nature of the images treated, and their discriminant features. KEYWORDS: Portuguese grey granites, feature extraction, mathematical morphology, feature reduction, genetic algorithms, nearest neighbour rule classifiers (k-NNR).\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412069",
        "title": "Swarming around Shellfish Larvae",
        "authors": [
            "Vitorino Ramos",
            "Jonathan Campbell",
            "John Slater",
            "John Gillespie",
            "Ivan F. Bendezu",
            "Fionn Murtagh"
        ],
        "abstract": "  The collection of wild larvae seed as a source of raw material is a major sub industry of shellfish aquaculture. To predict when, where and in what quantities wild seed will be available, it is necessary to track the appearance and growth of planktonic larvae. One of the most difficult groups to identify, particularly at the species level are the Bivalvia. This difficulty arises from the fact that fundamentally all bivalve larvae have a similar shape and colour. Identification based on gross morphological appearance is limited by the time-consuming nature of the microscopic examination and by the limited availability of expertise in this field. Molecular and immunological methods are also being studied. We describe the application of computational pattern recognition methods to the automated identification and size analysis of scallop larvae. For identification, the shape features used are binary invariant moments; that is, the features are invariant to shift (position within the image), scale (induced either by growth or differential image magnification) and rotation. Images of a sample of scallop and non-scallop larvae covering a range of maturities have been analysed. In order to overcome the automatic identification, as well as to allow the system to receive new unknown samples at any moment, a self-organized and unsupervised ant-like clustering algorithm based on Swarm Intelligence is proposed, followed by simple k-NNR nearest neighbour classification on the final map. Results achieve a full recognition rate of 100% under several situations (k =1 or 3).\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412070",
        "title": "Less is More - Genetic Optimisation of Nearest Neighbour Classifiers",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "abstract": "  The present paper deals with optimisation of Nearest Neighbour rule Classifiers via Genetic Algorithms. The methodology consists on implement a Genetic Algorithm capable of search the input feature space used by the NNR classifier. Results show that is adequate to perform feature reduction and simultaneous improve the Recognition Rate. Some practical examples prove that is possible to Recognise Portuguese Granites in 100%, with only 3 morphological features (from an original set of 117 features), which is well suited for real time applications. Moreover, the present method represents a robust strategy to understand the proper nature of the images treated, and their discriminant features. KEYWORDS: Feature Reduction, Genetic Algorithms, Nearest Neighbour Rule Classifiers (k-NNR).\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412071",
        "title": "Web Usage Mining Using Artificial Ant Colony Clustering and Genetic Programming",
        "authors": [
            "Ajith Abraham",
            "Vitorino Ramos"
        ],
        "abstract": "  The rapid e-commerce growth has made both business community and customers face a new situation. Due to intense competition on one hand and the customer's option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management. Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, creating adaptive Web sites, business and support services, personalization, network traffic flow analysis and so on. The study of ant colonies behavior and their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences, because it provides models of distributed adaptive organization, which are useful to solve difficult optimization, classification, and distributed control problems, among others. In this paper, we propose an ant clustering algorithm to discover Web usage patterns (data clusters) and a linear genetic programming approach to analyze the visitor trends. Empirical results clearly shows that ant colony clustering performs well when compared to a self-organizing map (for clustering Web usage patterns) even though the performance accuracy is not that efficient when comparared to evolutionary-fuzzy clustering (i-miner) approach. KEYWORDS: Web Usage Mining, Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Linear Genetic Programming.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412072",
        "title": "Swarms on Continuous Data",
        "authors": [
            "Vitorino Ramos",
            "Ajith Abraham"
        ],
        "abstract": "  While being it extremely important, many Exploratory Data Analysis (EDA) systems have the inhability to perform classification and visualization in a continuous basis or to self-organize new data-items into the older ones (evenmore into new labels if necessary), which can be crucial in KDD - Knowledge Discovery, Retrieval and Data Mining Systems (interactive and online forms of Web Applications are just one example). This disadvantge is also present in more recent approaches using Self-Organizing Maps. On the present work, and exploiting past sucesses in recently proposed Stigmergic Ant Systems a robust online classifier is presented, which produces class decisions on a continuous stream data, allowing for continuous mappings. Results show that increasingly better results are achieved, as demonstraded by other authors in different areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Exploratory Data Analysis, Image Retrieval, Continuous Classification.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412075",
        "title": "Self-Organized Stigmergic Document Maps: Environment as a Mechanism for Context Learning",
        "authors": [
            "Vitorino Ramos",
            "Juan J. Merelo"
        ],
        "abstract": "  Social insect societies and more specifically ant colonies, are distributed systems that, in spite of the simplicity of their individuals, present a highly structured social organization. As a result of this organization, ant colonies can accomplish complex tasks that in some cases exceed the individual capabilities of a single ant. The study of ant colonies behavior and of their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences, because it provides models of distributed adaptive organization which are useful to solve difficult optimization, classification, and distributed control problems, among others. In the present work we overview some models derived from the observation of real ants, emphasizing the role played by stigmergy as distributed communication paradigm, and we present a novel strategy to tackle unsupervised clustering as well as data retrieval problems. The present ant clustering system (ACLUSTER) avoids not only short-term memory based strategies, as well as the use of several artificial ant types (using different speeds), present in some recent approaches. Moreover and according to our knowledge, this is also the first application of ant systems into textual document clustering. KEYWORDS: Swarm Intelligence, Ant Systems, Unsupervised Clustering, Data Retrieval, Data Mining, Distributed Computing, Document Maps, Textual Document Clustering.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412076",
        "title": "Clustering Techniques for Marbles Classification",
        "authors": [
            "J.R. Caldas-Pinto",
            "Pedro Pina",
            "Vitorino Ramos",
            "Mario Ramalho"
        ],
        "abstract": "  Automatic marbles classification based on their visual appearance is an important industrial issue. However, there is no definitive solution to the problem mainly due to the presence of randomly distributed high number of different colours and its subjective evaluation by the human expert. In this paper we present a study of segmentation techniques, we evaluate they overall performance using a training set and standard quality measures and finally we apply different clustering techniques to automatically classify the marbles. KEYWORDS: Segmentation, Clustering, Quadtrees, Learning Vector Quantization (LVQ), Simulated Annealing (SA).\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412077",
        "title": "On the Implicit and on the Artificial - Morphogenesis and Emergent Aesthetics in Autonomous Collective Systems",
        "authors": [
            "Vitorino Ramos"
        ],
        "abstract": "  Imagine a \"machine\" where there is no pre-commitment to any particular representational scheme: the desired behaviour is distributed and roughly specified simultaneously among many parts, but there is minimal specification of the mechanism required to generate that behaviour, i.e. the global behaviour evolves from the many relations of multiple simple behaviours. A machine that lives to and from/with Synergy. An artificial super-organism that avoids specific constraints and emerges within multiple low-level implicit bio-inspired mechanisms. KEYWORDS: Complex Science, ArtSBots Project, Swarm Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm Paintings, Robot Paintings, Non-Human Art, Painting Emergence and Cooperation, Art and Complexity, ArtBots: The Robot Talent Show.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412079",
        "title": "The MC2 Project [Machines of Collective Conscience]: A possible walk, up to Life-like Complexity and Behaviour, from bottom, basic and simple bio-inspired heuristics - a walk, up into the morphogenesis of information",
        "authors": [
            "Vitorino Ramos"
        ],
        "abstract": "  Synergy (from the Greek word synergos), broadly defined, refers to combined or co-operative effects produced by two or more elements (parts or individuals). The definition is often associated with the holistic conviction quote that \"the whole is greater than the sum of its parts\" (Aristotle, in Metaphysics), or the whole cannot exceed the sum of the energies invested in each of its parts (e.g. first law of thermodynamics) even if it is more accurate to say that the functional effects produced by wholes are different from what the parts can produce alone. Synergy is a ubiquitous phenomena in nature and human societies alike. One well know example is provided by the emergence of self-organization in social insects, via direct or indirect interactions. The latter types are more subtle and defined as stigmergy to explain task coordination and regulation in the context of nest reconstruction in termites. An example, could be provided by two individuals, who interact indirectly when one of them modifies the environment and the other responds to the new environment at a later time. In other words, stigmergy could be defined as a particular case of environmental or spatial synergy. The system is purely holistic, and their properties are intrinsically emergent and autocatalytic. On the present work we present a \"machine\" where there is no precommitment to any particular representational scheme: the desired behaviour is distributed and roughly specified simultaneously among many parts, but there is minimal specification of the mechanism required to generate that behaviour, i.e. the global behaviour evolves from the many relations of multiple simple behaviours.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412080",
        "title": "The Biological Concept of Neoteny in Evolutionary Colour Image Segmentation - Simple Experiments in Simple Non-Memetic Genetic Algorithms",
        "authors": [
            "Vitorino Ramos"
        ],
        "abstract": "  Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of existing organisms may give rise, under certain circumstances, to wholly new organisms. Although the present work does not pretend to model or simulate the biological details of such a concept in any way, these ideas were incorporated by a rather simple abstract computational strategy, in order to allow (if possible) for faster convergence into simple non-memetic Genetic Algorithms, i.e. without using local improvement procedures (e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic Algorithm was used for colour image segmentation purposes by using K-mean unsupervised clustering methods, namely for guiding the evolutionary algorithm in his search for finding the optimal or sub-optimal data partition. Average results suggest that the use of neotonic strategies by employing juvenile genotypes into the later generations and the use of linear-dynamic mutation rates instead of constant, can increase fitness values by 58% comparing to classical Genetic Algorithms, independently from the starting population characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412081",
        "title": "Artificial Neoteny in Evolutionary Image Segmentation",
        "authors": [
            "Vitorino Ramos"
        ],
        "abstract": "  Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of existing organisms may give rise, under certain circumstances, to wholly new organisms. Although the present work does not pretend to model or simulate the biological details of such a concept in any way, these ideas were incorporated by a rather simple abstract computational strategy, in order to allow (if possible) for faster convergence into simple non-memetic Genetic Algorithms, i.e. without using local improvement procedures (e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic Algorithm was used for colour image segmentation purposes by using K-mean unsupervised clustering methods, namely for guiding the evolutionary algorithm in his search for finding the optimal or sub-optimal data partition. Average results suggest that the use of neotonic strategies by employing juvenile genotypes into the later generations and the use of linear-dynamic mutation rates instead of constant, can increase fitness values by 58% comparing to classical Genetic Algorithms, independently from the starting population characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412083",
        "title": "Line and Word Matching in Old Documents",
        "authors": [
            "A. Marcolino",
            "Vitorino Ramos",
            "Mario Ramalho",
            "J.R. Caldas Pinto"
        ],
        "abstract": "  This paper is concerned with the problem of establishing an index based on word matching. It is assumed that the book was digitised as better as possible and some pre-processing techniques were already applied as line orientation correction and some noise removal. However two main factor are responsible for being not possible to apply ordinary optical character recognition techniques (OCR): the presence of antique fonts and the degraded state of many characters due to unrecoverable original time degradation. In this paper we make a short introduction to word segmentation that involves finding the lines that characterise a word. After we discuss different approaches for word matching and how they can be combined to obtain an ordered list for candidate words for the matching. This discussion will be illustrated by examples.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412084",
        "title": "Map Segmentation by Colour Cube Genetic K-Mean Clustering",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "abstract": "  Segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. In this work, a method is described for evolving adaptive procedures for these problems. In many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be mapped into topological domains. We formulate the segmentation problem upon such images as an optimisation problem and adopt evolutionary strategy of Genetic Algorithms for the clustering of small regions in colour feature space. The present approach uses k-Means unsupervised clustering methods into Genetic Algorithms, namely for guiding this last Evolutionary Algorithm in his search for finding the optimal or sub-optimal data partition, task that as we know, requires a non-trivial search because of its NP-complete nature. To solve this task, the appropriate genetic coding is also discussed, since this is a key aspect in the implementation. Our purpose is to demonstrate the efficiency of Genetic Algorithms to automatic and unsupervised texture segmentation. Some examples in Colour Maps are presented and overall results discussed. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412086",
        "title": "Artificial Ant Colonies in Digital Image Habitats - A Mass Behaviour Effect Study on Pattern Recognition",
        "authors": [
            "Vitorino Ramos",
            "Filipe Almeida"
        ],
        "abstract": "  Some recent studies have pointed that, the self-organization of neurons into brain-like structures, and the self-organization of ants into a swarm are similar in many respects. If possible to implement, these features could lead to important developments in pattern recognition systems, where perceptive capabilities can emerge and evolve from the interaction of many simple local rules. The principle of the method is inspired by the work of Chialvo and Millonas who developed the first numerical simulation in which swarm cognitive map formation could be explained. From this point, an extended model is presented in order to deal with digital image habitats, in which artificial ants could be able to react to the environment and perceive it. Evolution of pheromone fields point that artificial ant colonies could react and adapt appropriately to any type of digital habitat. KEYWORDS: Swarm Intelligence, Self-Organization, Stigmergy, Artificial Ant Systems, Pattern Recognition and Perception, Image Segmentation, Gestalt Perception Theory, Distributed Computation.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412087",
        "title": "Image Colour Segmentation by Genetic Algorithms",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "abstract": "  Segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. In this work, a method is described for evolving adaptive procedures for these problems. In many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be mapped into topological domains. We formulate the segmentation problem upon such images as an optimisation problem and adopt evolutionary strategy of Genetic Algorithms for the clustering of small regions in colour feature space. The present approach uses k-Means unsupervised clustering methods into Genetic Algorithms, namely for guiding this last Evolutionary Algorithm in his search for finding the optimal or sub-optimal data partition, task that as we know, requires a non-trivial search because of its intrinsic NP-complete nature. To solve this task, the appropriate genetic coding is also discussed, since this is a key aspect in the implementation. Our purpose is to demonstrate the efficiency of Genetic Algorithms to automatic and unsupervised texture segmentation. Some examples in Colour Maps, Ornamental Stones and in Human Skin Mark segmentation are presented and overall results discussed. KEYWORDS: Genetic Algorithms, Colour Image Segmentation, Classification, Clustering.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412091",
        "title": "The Combination of Paradoxical, Uncertain, and Imprecise Sources of Information based on DSmT and Neutro-Fuzzy Inference",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "abstract": "  The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been, and still remains today, of primal importance for the development of reliable modern information systems involving artificial reasoning. In this chapter, we present a survey of our recent theory of plausible and paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT) in the literature, developed for dealing with imprecise, uncertain and paradoxical sources of information. We focus our presentation here rather on the foundations of DSmT, and on the two important new rules of combination, than on browsing specific applications of DSmT available in literature. Several simple examples are given throughout the presentation to show the efficiency and the generality of this new approach. The last part of this chapter concerns the presentation of the neutrosophic logic, the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and neutrosophic logic are useful tools in decision making after fusioning the information using the DSm hybrid rule of combination of masses.\n    ",
        "submission_date": "2004-12-19T00:00:00",
        "last_modified_date": "2004-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412105",
        "title": "On the existence of stable models of non-stratified logic programs",
        "authors": [
            "Stefania Costantini"
        ],
        "abstract": "  This paper introduces a fundamental result, which is relevant for Answer Set programming, and planning. For the first time since the definition of the stable model semantics, the class of logic programs for which a stable model exists is given a syntactic characterization. This condition may have a practical importance both for defining new algorithms for checking consistency and computing answer sets, and for improving the existing systems. The approach of this paper is to introduce a new canonical form (to which any logic program can be reduced to), to focus the attention on cyclic dependencies. The technical result is then given in terms of programs in canonical form (canonical programs), without loss of generality. The result is based on identifying the cycles contained in the program, showing that stable models of the overall program are composed of stable models of suitable sub-programs, corresponding to the cycles, and on defining the Cycle Graph. Each vertex of this graph corresponds to one cycle, and each edge corresponds to onehandle, which is a literal containing an atom that, occurring in both cycles, actually determines a connection between them. In fact, the truth value of the handle in the cycle where it appears as the head of a rule, influences the truth value of the atoms of the cycle(s) where it occurs in the body. We can therefore introduce the concept of a handle path, connecting different cycles. If for every odd cycle we can find a handle path with certain properties, then the existence of stable model is guaranteed.\n    ",
        "submission_date": "2004-12-23T00:00:00",
        "last_modified_date": "2004-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cond-mat/0408190",
        "title": "From spin glasses to hard satisfiable formulas",
        "authors": [
            "Haixia Jia",
            "Cristopher Moore",
            "Bart Selman"
        ],
        "abstract": "We introduce a highly structured family of hard satisfiable 3-SAT formulas corresponding to an ordered spin-glass model from statistical physics. This model has provably \"glassy\" behavior; that is, it has many local optima with large energy barriers between them, so that local search algorithms get stuck and have difficulty finding the true ground state, i.e., the unique satisfying assignment. We test the hardness of our formulas with two Davis-Putnam solvers, Satz and zChaff, the recently introduced Survey Propagation (SP), and two local search algorithms, Walksat and Record-to-Record Travel (RRT). We compare our formulas to random 3-XOR-SAT formulas and to two other generators of hard satisfiable instances, the minimum disagreement parity formulas of Crawford et al., and Hirsch's hgen. For the complete solvers the running time of our formulas grows exponentially in sqrt(n), and exceeds that of random 3-XOR-SAT formulas for small problem sizes. SP is unable to solve our formulas with as few as 25 variables. For Walksat, our formulas appear to be harder than any other known generator of satisfiable instances. Finally, our formulas can be solved efficiently by RRT but only if the parameter d is tuned to the height of the barriers between local minima, and we use this parameter to measure the barrier heights in random 3-XOR-SAT formulas as well.\n    ",
        "submission_date": "2004-08-09T00:00:00",
        "last_modified_date": "2012-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0401004",
        "title": "Cyborg Systems as Platforms for Computer-Vision Algorithm-Development for Astrobiology",
        "authors": [
            "Patrick C. McGuire",
            "J.A. Rodriguez-Manfredi",
            "E. Sebastian-Martinez",
            "J. Gomez-Elvira",
            "E. Diaz-Martinez",
            "J. Ormo",
            "K. Neuffer",
            "A. Giaquinta",
            "F. Camps-Martinez",
            "A. Lepinette-Malvitte",
            "J. Perez-Mercader",
            "H. Ritter",
            "M. Oesker",
            "J. Ontrup",
            "J. Walter"
        ],
        "abstract": "  Employing the allegorical imagery from the film \"The Matrix\", we motivate and discuss our `Cyborg Astrobiologist' research program. In this research program, we are using a wearable computer and video camcorder in order to test and train a computer-vision system to be a field-geologist and field-astrobiologist.\n    ",
        "submission_date": "2004-01-02T00:00:00",
        "last_modified_date": "2004-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402014",
        "title": "Self-Organising Networks for Classification: developing Applications to Science Analysis for Astroparticle Physics",
        "authors": [
            "A. De Angelis",
            "P. Boinee",
            "M. Frailis",
            "E. Milotti"
        ],
        "abstract": "  Physics analysis in astroparticle experiments requires the capability of recognizing new phenomena; in order to establish what is new, it is important to develop tools for automatic classification, able to compare the final result with data from different detectors. A typical example is the problem of Gamma Ray Burst detection, classification, and possible association to known sources: for this task physicists will need in the next years tools to associate data from optical databases, from satellite experiments (EGRET, GLAST), and from Cherenkov telescopes (MAGIC, HESS, CANGAROO, VERITAS).\n    ",
        "submission_date": "2004-02-09T00:00:00",
        "last_modified_date": "2004-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402030",
        "title": "Computational complexity and simulation of rare events of Ising spin glasses",
        "authors": [
            "Martin Pelikan",
            "Jiri Ocenasek",
            "Simon Trebst",
            "Matthias Troyer",
            "Fabien Alet"
        ],
        "abstract": "  We discuss the computational complexity of random 2D Ising spin glasses, which represent an interesting class of constraint satisfaction problems for black box optimization. Two extremal cases are considered: (1) the +/- J spin glass, and (2) the Gaussian spin glass. We also study a smooth transition between these two extremal cases. The computational complexity of all studied spin glass systems is found to be dominated by rare events of extremely hard spin glass samples. We show that complexity of all studied spin glass systems is closely related to Frechet extremal value distribution. In a hybrid algorithm that combines the hierarchical Bayesian optimization algorithm (hBOA) with a deterministic bit-flip hill climber, the number of steps performed by both the global searcher (hBOA) and the local searcher follow Frechet distributions. Nonetheless, unlike in methods based purely on local search, the parameters of these distributions confirm good scalability of hBOA with local search. We further argue that standard performance measures for optimization algorithms--such as the average number of evaluations until convergence--can be misleading. Finally, our results indicate that for highly multimodal constraint satisfaction problems, such as Ising spin glasses, recombination-based search can provide qualitatively better results than mutation-based search.\n    ",
        "submission_date": "2004-02-15T00:00:00",
        "last_modified_date": "2004-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402031",
        "title": "Parameter-less hierarchical BOA",
        "authors": [
            "Martin Pelikan",
            "Tz-Kai Lin"
        ],
        "abstract": "  The parameter-less hierarchical Bayesian optimization algorithm (hBOA) enables the use of hBOA without the need for tuning parameters for solving each problem instance. There are three crucial parameters in hBOA: (1) the selection pressure, (2) the window size for restricted tournaments, and (3) the population size. Although both the selection pressure and the window size influence hBOA performance, performance should remain low-order polynomial with standard choices of these two parameters. However, there is no standard population size that would work for all problems of interest and the population size must thus be eliminated in a different way. To eliminate the population size, the parameter-less hBOA adopts the population-sizing technique of the parameter-less genetic algorithm. Based on the existing theory, the parameter-less hBOA should be able to solve nearly decomposable and hierarchical problems in quadratic or subquadratic number of function evaluations without the need for setting any parameters whatsoever. A number of experiments are presented to verify scalability of the parameter-less hBOA.\n    ",
        "submission_date": "2004-02-15T00:00:00",
        "last_modified_date": "2004-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402032",
        "title": "Fitness inheritance in the Bayesian optimization algorithm",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry"
        ],
        "abstract": "  This paper describes how fitness inheritance can be used to estimate fitness for a proportion of newly sampled candidate solutions in the Bayesian optimization algorithm (BOA). The goal of estimating fitness for some candidate solutions is to reduce the number of fitness evaluations for problems where fitness evaluation is expensive. Bayesian networks used in BOA to model promising solutions and generate the new ones are extended to allow not only for modeling and sampling candidate solutions, but also for estimating their fitness. The results indicate that fitness inheritance is a promising concept in BOA, because population-sizing requirements for building appropriate models of promising solutions lead to good fitness estimates even if only a small proportion of candidate solutions is evaluated using the actual fitness function. This can lead to a reduction of the number of actual fitness evaluations by a factor of 30 or more.\n    ",
        "submission_date": "2004-02-15T00:00:00",
        "last_modified_date": "2004-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0402053",
        "title": "The Complexity of Modified Instances",
        "authors": [
            "Paolo Liberatore"
        ],
        "abstract": "  In this paper we study the complexity of solving a problem when a solution of a similar instance is known. This problem is relevant whenever instances may change from time to time, and known solutions may not remain valid after the change. We consider two scenarios: in the first one, what is known is only a solution of the problem before the change; in the second case, we assume that some additional information, found during the search for this solution, is also known. In the first setting, the techniques from the theory of NP-completeness suffice to show complexity results. In the second case, negative results can only be proved using the techniques of compilability, and are often related to the size of considered changes.\n    ",
        "submission_date": "2004-02-23T00:00:00",
        "last_modified_date": "2004-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403016",
        "title": "A Comparative Study of Arithmetic Constraints on Integer Intervals",
        "authors": [
            "Krzysztof R. Apt",
            "Peter Zoeteweij"
        ],
        "abstract": "  We propose here a number of approaches to implement constraint propagation for arithmetic constraints on integer intervals. To this end we introduce integer interval arithmetic. Each approach is explained using appropriate proof rules that reduce the variable domains. We compare these approaches using a set of benchmarks.\n    ",
        "submission_date": "2004-03-12T00:00:00",
        "last_modified_date": "2004-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403025",
        "title": "Distribution of Mutual Information from Complete and Incomplete Data",
        "authors": [
            "Marcus Hutter",
            "Marco Zaffalon"
        ],
        "abstract": "  Mutual information is widely used, in a descriptive way, to measure the stochastic dependence of categorical random variables. In order to address questions such as the reliability of the descriptive value, one must consider sample-to-population inferential approaches. This paper deals with the posterior distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean, and analytical approximations for the variance, skewness and kurtosis are derived. These approximations have a guaranteed accuracy level of the order O(1/n^3), where n is the sample size. Leading order approximations for the mean and the variance are derived in the case of incomplete samples. The derived analytical expressions allow the distribution of mutual information to be approximated reliably and quickly. In fact, the derived expressions can be computed with the same order of complexity needed for descriptive mutual information. This makes the distribution of mutual information become a concrete alternative to descriptive mutual information in many applications which would benefit from moving to the inductive side. Some of these prospective applications are discussed, and one of them, namely feature selection, is shown to perform significantly better when inductive mutual information is used.\n    ",
        "submission_date": "2004-03-15T00:00:00",
        "last_modified_date": "2004-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403038",
        "title": "Tournament versus Fitness Uniform Selection",
        "authors": [
            "Shane Legg",
            "Marcus Hutter",
            "Akshat Kumar"
        ],
        "abstract": "  In evolutionary algorithms a critical parameter that must be tuned is that of selection pressure. If it is set too low then the rate of convergence towards the optimum is likely to be slow. Alternatively if the selection pressure is set too high the system is likely to become stuck in a local optimum due to a loss of diversity in the population. The recent Fitness Uniform Selection Scheme (FUSS) is a conceptually simple but somewhat radical approach to addressing this problem - rather than biasing the selection towards higher fitness, FUSS biases selection towards sparsely populated fitness levels. In this paper we compare the relative performance of FUSS with the well known tournament selection scheme on a range of problems.\n    ",
        "submission_date": "2004-03-23T00:00:00",
        "last_modified_date": "2004-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0403039",
        "title": "A Flexible Rule Compiler for Speech Synthesis",
        "authors": [
            "Wojciech Skut",
            "Stefan Ulrich",
            "Kathrine Hammervold"
        ],
        "abstract": "  We present a flexible rule compiler developed for a text-to-speech (TTS) system. The compiler converts a set of rules into a finite-state transducer (FST). The input and output of the FST are subject to parameterization, so that the system can be applied to strings and sequences of feature-structures. The resulting transducer is guaranteed to realize a function (as opposed to a relation), and therefore can be implemented as a deterministic device (either a deterministic FST or a bimachine).\n    ",
        "submission_date": "2004-03-23T00:00:00",
        "last_modified_date": "2004-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404018",
        "title": "NLML--a Markup Language to Describe the Unlimited English Grammar",
        "authors": [
            "Jiyou Jia"
        ],
        "abstract": "  In this paper we present NLML (Natural Language Markup Language), a markup language to describe the syntactic and semantic structure of any grammatically correct English expression. At first the related works are analyzed to demonstrate the necessity of the NLML: simple form, easy management and direct storage. Then the description of the English grammar with NLML is introduced in details in three levels: sentences (with different complexities, voices, moods, and tenses), clause (relative clause and noun clause) and phrase (noun phrase, verb phrase, prepositional phrase, adjective phrase, adverb phrase and predicate phrase). At last the application fields of the NLML in NLP are shown with two typical examples: NLOJM (Natural Language Object Modal in Java) and NLDB (Natural Language Database).\n    ",
        "submission_date": "2004-04-07T00:00:00",
        "last_modified_date": "2004-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404024",
        "title": "Computability Logic: a formal theory of interaction",
        "authors": [
            "Giorgi Japaridze"
        ],
        "abstract": "  Computability logic is a formal theory of (interactive) computability in the same sense as classical logic is a formal theory of truth. This approach was initiated very recently in \"Introduction to computability logic\" (Annals of Pure and Applied Logic 123 (2003), pp.1-99). The present paper reintroduces computability logic in a more compact and less technical way. It is written in a semitutorial style with a general computer science, logic or mathematics audience in mind. An Internet source on the subject is available at ",
        "submission_date": "2004-04-09T00:00:00",
        "last_modified_date": "2004-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404032",
        "title": "When Do Differences Matter? On-Line Feature Extraction Through Cognitive Economy",
        "authors": [
            "David J. Finton"
        ],
        "abstract": "  For an intelligent agent to be truly autonomous, it must be able to adapt its representation to the requirements of its task as it interacts with the world. Most current approaches to on-line feature extraction are ad hoc; in contrast, this paper presents an algorithm that bases judgments of state compatibility and state-space abstraction on principled criteria derived from the psychological principle of cognitive economy. The algorithm incorporates an active form of Q-learning, and partitions continuous state-spaces by merging and splitting Voronoi regions. The experiments illustrate a new methodology for testing and comparing representations by means of learning curves. Results from the puck-on-a-hill task demonstrate the algorithm's ability to learn effective representations, superior to those produced by some other, well-known, methods.\n    ",
        "submission_date": "2004-04-15T00:00:00",
        "last_modified_date": "2004-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404038",
        "title": "2-Sat Sub-Clauses and the Hypernodal Structure of the 3-Sat Problem",
        "authors": [
            "D. B. Powell"
        ],
        "abstract": "  Like simpler graphs, nested (hypernodal) graphs consist of two components: a set of nodes and a set of edges, where each edge connects a pair of nodes. In the hypernodal graph model, however, a node may contain other graphs, so that a node may be contained in a graph that it contains. The inherently recursive structure of the hypernodal graph model aptly characterizes both the structure and dynamic of the 3-sat problem, a broadly applicable, though intractable, computer science problem. In this paper I first discuss the structure of the 3-sat problem, analyzing the relation of 3-sat to 2-sat, a related, though tractable problem. I then discuss sub-clauses and sub-clause thresholds and the transformation of sub-clauses into implication graphs, demonstrating how combinations of implication graphs are equivalent to hypernodal graphs. I conclude with a brief discussion of the use of hypernodal graphs to model the 3-sat problem, illustrating how hypernodal graphs model both the conditions for satisfiability and the process by which particular 3-sat assignments either succeed or fail.\n    ",
        "submission_date": "2004-04-20T00:00:00",
        "last_modified_date": "2004-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404045",
        "title": "Speculation on graph computation architectures and computing via synchronization",
        "authors": [
            "Bayle Shanks"
        ],
        "abstract": "  A speculative overview of a future topic of research. The paper is a collection of ideas concerning two related areas:\n",
        "submission_date": "2004-04-22T00:00:00",
        "last_modified_date": "2004-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404049",
        "title": "Exploiting Cross-Document Relations for Multi-document Evolving Summarization",
        "authors": [
            "Stergos D. Afantenos",
            "Irene Doura",
            "Eleni Kapellou",
            "Vangelis Karkaletsis"
        ],
        "abstract": "  This paper presents a methodology for summarization from multiple documents which are about a specific topic. It is based on the specification and identification of the cross-document relations that occur among textual elements within those documents. Our methodology involves the specification of the topic-specific entities, the messages conveyed for the specific entities by certain textual elements and the specification of the relations that can hold among these messages. The above resources are necessary for setting up a specific topic for our query-based summarization approach which uses these resources to identify the query-specific messages within the documents and the query-specific relations that connect these messages across documents.\n    ",
        "submission_date": "2004-04-23T00:00:00",
        "last_modified_date": "2004-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0404057",
        "title": "Convergence of Discrete MDL for Sequential Prediction",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "abstract": "  We study the properties of the Minimum Description Length principle for sequence prediction, considering a two-part MDL estimator which is chosen from a countable class of models. This applies in particular to the important case of universal sequence prediction, where the model class corresponds to all algorithms for some fixed universal Turing machine (this correspondence is by enumerable semimeasures, hence the resulting models are stochastic). We prove convergence theorems similar to Solomonoff's theorem of universal induction, which also holds for general Bayes mixtures. The bound characterizing the convergence speed for MDL predictions is exponentially larger as compared to Bayes mixtures. We observe that there are at least three different ways of using MDL for prediction. One of these has worse prediction properties, for which predictions only converge if the MDL estimator stabilizes. We establish sufficient conditions for this to occur. Finally, some immediate consequences for complexity relations and randomness criteria are proven.\n    ",
        "submission_date": "2004-04-28T00:00:00",
        "last_modified_date": "2004-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0405043",
        "title": "Prediction with Expert Advice by Following the Perturbed Leader for General Weights",
        "authors": [
            "Marcus Hutter",
            "Jan Poland"
        ],
        "abstract": "  When applying aggregating strategies to Prediction with Expert Advice, the learning rate must be adaptively tuned. The natural choice of sqrt(complexity/current loss) renders the analysis of Weighted Majority derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative \"Follow the Perturbed Leader\" (FPL) algorithm from Kalai (2003} (based on Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are (to our knowledge) new.\n    ",
        "submission_date": "2004-05-12T00:00:00",
        "last_modified_date": "2004-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0406032",
        "title": "A Dynamic Clustering-Based Markov Model for Web Usage Mining",
        "authors": [
            "Jos\u00e9 Borges",
            "Mark Levene"
        ],
        "abstract": "  Markov models have been widely utilized for modelling user web navigation behaviour. In this work we propose a dynamic clustering-based method to increase a Markov model's accuracy in representing a collection of user web navigation sessions. The method makes use of the state cloning concept to duplicate states in a way that separates in-links whose corresponding second-order probabilities diverge. In addition, the new method incorporates a clustering technique which determines an effcient way to assign in-links with similar second-order probabilities to the same clone. We report on experiments conducted with both real and random data and we provide a comparison with the N-gram Markov concept. The results show that the number of additional states induced by the dynamic clustering method can be controlled through a threshold parameter, and suggest that the method's performance is linear time in the size of the model.\n    ",
        "submission_date": "2004-06-17T00:00:00",
        "last_modified_date": "2004-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0406047",
        "title": "Self-organizing neural networks in classification and image recognition",
        "authors": [
            "G.A. Ososkov",
            "S.G. Dmitrievskiy",
            "A.V. Stadnik"
        ],
        "abstract": "  Self-organizing neural networks are used for brick finding in OPERA experiment. Self-organizing neural networks and wavelet analysis used for recognition and extraction of car numbers from images.\n    ",
        "submission_date": "2004-06-24T00:00:00",
        "last_modified_date": "2004-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0406056",
        "title": "P=NP",
        "authors": [
            "Selmer Bringsjord",
            "Joshua Taylor"
        ],
        "abstract": "  We claim to resolve the P=?NP problem via a formal argument for P=NP.\n    ",
        "submission_date": "2004-06-28T00:00:00",
        "last_modified_date": "2004-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407021",
        "title": "Multi-agent coordination using nearest neighbor rules: revisiting the Vicsek model",
        "authors": [
            "Sanjiang Li",
            "Huaiqing Wang"
        ],
        "abstract": "  Recently, Jadbabaie, Lin, and Morse (IEEE TAC, 48(6)2003:988-1001) offered a mathematical analysis of the discrete time model of groups of mobile autonomous agents raised by Vicsek et al. in 1995. In their paper, Jadbabaie et al. showed that all agents shall move in the same heading, provided that these agents are periodically linked together. This paper sharpens this result by showing that coordination will be reached under a very weak condition that requires all agents are finally linked together. This condition is also strictly weaker than the one Jadbabaie et al. desired.\n    ",
        "submission_date": "2004-07-09T00:00:00",
        "last_modified_date": "2004-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407039",
        "title": "On the Convergence Speed of MDL Predictions for Bernoulli Sequences",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "abstract": "  We consider the Minimum Description Length principle for online sequence prediction. If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is bounded, implying convergence with probability one, and (b) it additionally specifies a `rate of convergence'. Generally, for MDL only exponential loss bounds hold, as opposed to the linear bounds for a Bayes mixture. We show that this is even the case if the model class contains only Bernoulli distributions. We derive a new upper bound on the prediction error for countable Bernoulli classes. This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes. The results apply to many Machine Learning tasks including classification and hypothesis testing. We provide arguments that our theorems generalize to countable classes of i.i.d. models.\n    ",
        "submission_date": "2004-07-16T00:00:00",
        "last_modified_date": "2004-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407047",
        "title": "Channel-Independent and Sensor-Independent Stimulus Representations",
        "authors": [
            "David N. Levin"
        ],
        "abstract": "  This paper shows how a machine, which observes stimuli through an uncharacterized, uncalibrated channel and sensor, can glean machine-independent information (i.e., channel- and sensor-independent information) about the stimuli. First, we demonstrate that a machine defines a specific coordinate system on the stimulus state space, with the nature of that coordinate system depending on the device's channel and sensor. Thus, machines with different channels and sensors \"see\" the same stimulus trajectory through state space, but in different machine-specific coordinate systems. For a large variety of physical stimuli, statistical properties of that trajectory endow the stimulus configuration space with differential geometric structure (a metric and parallel transfer procedure), which can then be used to represent relative stimulus configurations in a coordinate-system-independent manner (and, therefore, in a channel- and sensor-independent manner). The resulting description is an \"inner\" property of the stimulus time series in the sense that it does not depend on extrinsic factors like the observer's choice of a coordinate system in which the stimulus is viewed (i.e., the observer's choice of channel and sensor). This methodology is illustrated with analytic examples and with a numerically simulated experiment. In an intelligent sensory device, this kind of representation \"engine\" could function as a \"front-end\" that passes channel/sensor-independent stimulus representations to a pattern recognition module. After a pattern recognizer has been trained in one of these devices, it could be used without change in other devices having different channels and sensors.\n    ",
        "submission_date": "2004-07-19T00:00:00",
        "last_modified_date": "2005-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407049",
        "title": "Preferred Answer Sets for Ordered Logic Programs",
        "authors": [
            "Davy Van Nieuwenborgh",
            "Dirk Vermeir"
        ],
        "abstract": "  We extend answer set semantics to deal with inconsistent programs (containing classical negation), by finding a ``best'' answer set. Within the context of inconsistent programs, it is natural to have a partial order on rules, representing a preference for satisfying certain rules, possibly at the cost of violating less important ones. We show that such a rule order induces a natural order on extended answer sets, the minimal elements of which we call preferred answer sets. We characterize the expressiveness of the resulting semantics and show that it can simulate negation as failure, disjunction and some other formalisms such as logic programs with ordered disjunction. The approach is shown to be useful in several application areas, e.g. repairing database, where minimal repairs correspond to preferred answer sets.\n",
        "submission_date": "2004-07-19T00:00:00",
        "last_modified_date": "2004-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407054",
        "title": "From truth to computability I",
        "authors": [
            "Giorgi Japaridze"
        ],
        "abstract": "  The recently initiated approach called computability logic is a formal theory of interactive computation. See a comprehensive online source on the subject at ",
        "submission_date": "2004-07-21T00:00:00",
        "last_modified_date": "2005-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407057",
        "title": "Universal Convergence of Semimeasures on Individual Random Sequences",
        "authors": [
            "Marcus Hutter",
            "Andrej Muchnik"
        ],
        "abstract": "  Solomonoff's central result on induction is that the posterior of a universal semimeasure M converges rapidly and with probability 1 to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal sequence predictor in case of unknown mu. Despite some nearby results and proofs in the literature, the stronger result of convergence for all (Martin-Loef) random sequences remained open. Such a convergence result would be particularly interesting and natural, since randomness can be defined in terms of M itself. We show that there are universal semimeasures M which do not converge for all random sequences, i.e. we give a partial negative answer to the open problem. We also provide a positive answer for some non-universal semimeasures. We define the incomputable measure D as a mixture over all computable measures and the enumerable semimeasure W as a mixture over all enumerable nearly-measures. We show that W converges to D and D to mu on all random sequences. The Hellinger distance measuring closeness of two distributions plays a central role.\n    ",
        "submission_date": "2004-07-23T00:00:00",
        "last_modified_date": "2004-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0407064",
        "title": "A Sequent Calculus and a Theorem Prover for Standard Conditional Logics",
        "authors": [
            "Nicola Olivetti",
            "Gian Luca Pozzato",
            "Camilla Schwind"
        ],
        "abstract": "  In this paper we present a cut-free sequent calculus, called SeqS, for some standard conditional logics, namely CK, CK+ID, CK+MP and CK+MP+ID. The calculus uses labels and transition formulas and can be used to prove decidability and space complexity bounds for the respective logics. We also present CondLean, a theorem prover for these logics implementing SeqS calculi written in SICStus Prolog.\n    ",
        "submission_date": "2004-07-29T00:00:00",
        "last_modified_date": "2004-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408037",
        "title": "Multi-dimensional Type Theory: Rules, Categories, and Combinators for Syntax and Semantics",
        "authors": [
            "J\u00f8rgen Villadsen"
        ],
        "abstract": "  We investigate the possibility of modelling the syntax and semantics of natural language by constraints, or rules, imposed by the multi-dimensional type theory Nabla. The only multiplicity we explicitly consider is two, namely one dimension for the syntax and one dimension for the semantics, but the general perspective is important. For example, issues of pragmatics could be handled as additional dimensions.\n",
        "submission_date": "2004-08-15T00:00:00",
        "last_modified_date": "2004-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408056",
        "title": "A CHR-based Implementation of Known Arc-Consistency",
        "authors": [
            "Marco Alberti",
            "Marco Gavanelli",
            "Evelina Lamma",
            "Paola Mello",
            "Michela Milano"
        ],
        "abstract": "  In classical CLP(FD) systems, domains of variables are completely known at the beginning of the constraint propagation process. However, in systems interacting with an external environment, acquiring the whole domains of variables before the beginning of constraint propagation may cause waste of computation time, or even obsolescence of the acquired data at the time of use.\n",
        "submission_date": "2004-08-24T00:00:00",
        "last_modified_date": "2004-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0408057",
        "title": "The role of robust semantic analysis in spoken language dialogue systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "abstract": "  In this paper we summarized a framework for designing grammar-based procedure for the automatic extraction of the semantic content from spoken queries. Starting with a case study and following an approach which combines the notions of fuzziness and robustness in sentence parsing, we showed we built practical domain-dependent rules which can be applied whenever it is possible to superimpose a sentence-level semantic structure to a text without relying on a previous deep syntactical analysis. This kind of procedure can be also profitably used as a pre-processing tool in order to cut out part of the sentence which have been recognized to have no relevance in the understanding process. In the case of particular dialogue applications where there is no need to build a complex semantic structure (e.g. word spotting or excerpting) the presented methodology may represent an efficient alternative solution to a sequential composition of deep linguistic analysis modules. Even if the query generation problem may not seem a critical application it should be held in mind that the sentence processing must be done on-line. Having this kind of constraints we cannot design our system without caring for efficiency and thus provide an immediate response. Another critical issue is related to whole robustness of the system. In our case study we tried to make experiences on how it is possible to deal with an unreliable and noisy input without asking the user for any repetition or clarification. This may correspond to a similar problem one may have when processing text coming from informal writing such as e-mails, news and in many cases Web pages where it is often the case to have irrelevant surrounding information.\n    ",
        "submission_date": "2004-08-25T00:00:00",
        "last_modified_date": "2004-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410015",
        "title": "L1 regularization is better than L2 for learning and predicting chaotic systems",
        "authors": [
            "Z. Szabo",
            "A. Lorincz"
        ],
        "abstract": "  Emergent behaviors are in the focus of recent research interest. It is then of considerable importance to investigate what optimizations suit the learning and prediction of chaotic systems, the putative candidates for emergence. We have compared L1 and L2 regularizations on predicting chaotic time series using linear recurrent neural networks. The internal representation and the weights of the networks were optimized in a unifying framework. Computational tests on different problems indicate considerable advantages for the L1 regularization: It had considerably better learning time and better interpolating capabilities. We shall argue that optimization viewed as a maximum likelihood estimation justifies our results, because L1 regularization fits heavy-tailed distributions -- an apparently general feature of emergent systems -- better.\n    ",
        "submission_date": "2004-10-07T00:00:00",
        "last_modified_date": "2004-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410058",
        "title": "Robust Dialogue Understanding in HERALD",
        "authors": [
            "Vincenzo Pallotta",
            "Afzal Ballim"
        ],
        "abstract": "  We tackle the problem of robust dialogue processing from the perspective of language engineering. We propose an agent-oriented architecture that allows us a flexible way of composing robust processors. Our approach is based on Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP agent model can be enriched with special features and components that allow us to deal with classical problems of dialogue understanding.\n    ",
        "submission_date": "2004-10-22T00:00:00",
        "last_modified_date": "2004-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410060",
        "title": "Semantic filtering by inference on domain knowledge in spoken dialogue systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "abstract": "  General natural dialogue processing requires large amounts of domain knowledge as well as linguistic knowledge in order to ensure acceptable coverage and understanding. There are several ways of integrating lexical resources (e.g. dictionaries, thesauri) and knowledge bases or ontologies at different levels of dialogue processing. We concentrate in this paper on how to exploit domain knowledge for filtering interpretation hypotheses generated by a robust semantic parser. We use domain knowledge to semantically constrain the hypothesis space. Moreover, adding an inference mechanism allows us to complete the interpretation when information is not explicitly available. Further, we discuss briefly how this can be generalized towards a predictive natural interactive system.\n    ",
        "submission_date": "2004-10-23T00:00:00",
        "last_modified_date": "2004-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410068",
        "title": "Analyzing and Improving Performance of a Class of Anomaly-based Intrusion Detectors",
        "authors": [
            "Zhuowei Li",
            "Amitabha Das"
        ],
        "abstract": "  Anomaly-based intrusion detection (AID) techniques are useful for detecting novel intrusions into computing resources. One of the most successful AID detectors proposed to date is stide, which is based on analysis of system call sequences. In this paper, we present a detailed formal framework to analyze, understand and improve the performance of stide and similar AID techniques. Several important properties of stide-like detectors are established through formal proofs, and validated by carefully conducted experiments using test datasets. Finally, the framework is utilized to design two applications to improve the cost and performance of stide-like detectors which are based on sequence analysis. The first application reduces the cost of developing AID detectors by identifying the critical sections in the training dataset, and the second application identifies the intrusion context in the intrusive dataset, that helps to fine-tune the detectors. Such fine-tuning in turn helps to improve detection rate and reduce false alarm rate, thereby increasing the effectiveness and efficiency of the intrusion detectors.\n    ",
        "submission_date": "2004-10-26T00:00:00",
        "last_modified_date": "2004-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0410071",
        "title": "The Cyborg Astrobiologist: First Field Experience",
        "authors": [
            "Patrick C. McGuire",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Jose Antonio Rodriguez-Manfredi",
            "Javier Gomez-Elvira",
            "Helge Ritter",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "abstract": "  We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist and field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: first, detection and accounting for shadows caused by 3D jagged edges in the outcrop; second, reincorporation of more sophisticated texture-analysis algorithms into the system; third, creation of hardware and software capabilities to control the camera's zoom lens in an intelligent manner; and fourth, development of algorithms for interpretation of complex geological scenery. Nonetheless, despite these technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.\n    ",
        "submission_date": "2004-10-27T00:00:00",
        "last_modified_date": "2004-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411008",
        "title": "Intuitionistic computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "abstract": "  Computability logic (CL) is a systematic formal theory of computational tasks and resources, which, in a sense, can be seen as a semantics-based alternative to (the syntactically introduced) linear logic. With its expressive and flexible language, where formulas represent computational problems and \"truth\" is understood as algorithmic solvability, CL potentially offers a comprehensive logical basis for constructive applied theories and computing systems inherently requiring constructive and computationally meaningful underlying logics.\n",
        "submission_date": "2004-11-04T00:00:00",
        "last_modified_date": "2006-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411018",
        "title": "Artificial Intelligence and Systems Theory: Applied to Cooperative Robots",
        "authors": [
            "Pedro U. Lima",
            "Luis M. M. Custodio"
        ],
        "abstract": "  This paper describes an approach to the design of a population of cooperative robots based on concepts borrowed from Systems Theory and Artificial Intelligence. The research has been developed under the SocRob project, carried out by the Intelligent Systems Laboratory at the Institute for Systems and Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the project stands both for \"Society of Robots\" and \"Soccer Robots\", the case study where we are testing our population of robots. Designing soccer robots is a very challenging problem, where the robots must act not only to shoot a ball towards the goal, but also to detect and avoid static (walls, stopped robots) and dynamic (moving robots) obstacles. Furthermore, they must cooperate to defeat an opposing team. Our past and current research in soccer robotics includes cooperative sensor fusion for world modeling, object recognition and tracking, robot navigation, multi-robot distributed task planning and coordination, including cooperative reinforcement learning in cooperative and adversarial environments, and behavior-based architectures for real time task execution of cooperating robot teams.\n    ",
        "submission_date": "2004-11-08T00:00:00",
        "last_modified_date": "2004-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411022",
        "title": "Topological Navigation of Simulated Robots using Occupancy Grid",
        "authors": [
            "Richard Szabo"
        ],
        "abstract": "  Formerly I presented a metric navigation method in the Webots mobile robot simulator. The navigating Khepera-like robot builds an occupancy grid of the environment and explores the square-shaped room around with a value iteration algorithm. Now I created a topological navigation procedure based on the occupancy grid process. The extension by a skeletonization algorithm results a graph of important places and the connecting routes among them. I also show the significant time profit gained during the process.\n    ",
        "submission_date": "2004-11-08T00:00:00",
        "last_modified_date": "2004-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411025",
        "title": "Bionic Humans Using EAP as Artificial Muscles Reality and Challenges",
        "authors": [
            "Yoseph Bar-Cohen"
        ],
        "abstract": "  For many years, the idea of a human with bionic muscles immediately conjures up science fiction images of a TV series superhuman character that was implanted with bionic muscles and portrayed with strength and speed far superior to any normal human. As fantastic as this idea may seem, recent developments in electroactive polymers (EAP) may one day make such bionics possible. Polymers that exhibit large displacement in response to stimulation that is other than electrical signal were known for many years. Initially, EAP received relatively little attention due to their limited actuation capability. However, in the recent years, the view of the EAP materials has changed due to the introduction of effective new materials that significantly surpassed the capability of the widely used piezoelectric polymer, PVDF. As this technology continues to evolve, novel mechanisms that are biologically inspired are expected to emerge. EAP materials can potentially provide actuation with lifelike response and more flexible configurations. While further improvements in performance and robustness are still needed, there already have been several reported successes. In recognition of the need for cooperation in this multidisciplinary field, the author initiated and organized a series of international forums that are leading to a growing number of research and development projects and to great advances in the field. In 1999, he challenged the worldwide science and engineering community of EAP experts to develop a robotic arm that is actuated by artificial muscles to win a wrestling match against a human opponent. In this paper, the field of EAP as artificial muscles will be reviewed covering the state of the art, the challenges and the vision for the progress in future years.\n    ",
        "submission_date": "2004-11-08T00:00:00",
        "last_modified_date": "2004-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411035",
        "title": "A FP-Tree Based Approach for Mining All Strongly Correlated Pairs without Candidate Generation",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "abstract": "  Given a user-specified minimum correlation threshold and a transaction database, the problem of mining all-strong correlated pairs is to find all item pairs with Pearson's correlation coefficients above the threshold . Despite the use of upper bound based pruning technique in the Taper algorithm [1], when the number of items and transactions are very large, candidate pair generation and test is still costly. To avoid the costly test of a large number of candidate pairs, in this paper, we propose an efficient algorithm, called Tcp, based on the well-known FP-tree data structure, for mining the complete set of all-strong correlated item pairs. Our experimental results on both synthetic and real world datasets show that, Tcp's performance is significantly better than that of the previously developed Taper algorithm over practical ranges of correlation threshold specifications.\n    ",
        "submission_date": "2004-11-12T00:00:00",
        "last_modified_date": "2004-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0411099",
        "title": "A Note on the PAC Bayesian Theorem",
        "authors": [
            "Andreas Maurer"
        ],
        "abstract": "  We prove general exponential moment inequalities for averages of [0,1]-valued iid random variables and use them to tighten the PAC Bayesian Theorem. The logarithmic dependence on the sample count in the enumerator of the PAC Bayesian bound is halved.\n    ",
        "submission_date": "2004-11-30T00:00:00",
        "last_modified_date": "2004-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412018",
        "title": "Modeling Complex Higher Order Patterns",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "abstract": "  The goal of this paper is to show that generalizing the notion of frequent patterns can be useful in extending association analysis to more complex higher order patterns. To that end, we describe a general framework for modeling a complex pattern based on evaluating the interestingness of its sub-patterns. A key goal of any framework is to allow people to more easily express, explore, and communicate ideas, and hence, we illustrate how our framework can be used to describe a variety of commonly used patterns, such as frequent patterns, frequent closed patterns, indirect association patterns, hub patterns and authority patterns. To further illustrate the usefulness of the framework, we also present two new kinds of patterns that derived from the framework: clique pattern and bi-clique pattern and illustrate their practical use.\n    ",
        "submission_date": "2004-12-04T00:00:00",
        "last_modified_date": "2004-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412019",
        "title": "A Link Clustering Based Approach for Clustering Categorical Data",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "abstract": "  Categorical data clustering (CDC) and link clustering (LC) have been considered as separate research and application areas. The main focus of this paper is to investigate the commonalities between these two problems and the uses of these commonalities for the creation of new clustering algorithms for categorical data based on cross-fertilization between the two disjoint research fields. More precisely, we formally transform the CDC problem into an LC problem, and apply LC approach for clustering categorical data. Experimental results on real datasets show that LC based clustering method is competitive with existing CDC algorithms with respect to clustering accuracy.\n    ",
        "submission_date": "2004-12-04T00:00:00",
        "last_modified_date": "2004-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412023",
        "title": "Multidimensional data classification with artificial neural networks",
        "authors": [
            "P. Boinee",
            "F. Barbarino",
            "A. De Angelis"
        ],
        "abstract": "  Multi-dimensional data classification is an important and challenging problem in many astro-particle experiments. Neural networks have proved to be versatile and robust in multi-dimensional data classification. In this article we shall study the classification of gamma from the hadrons for the MAGIC Experiment. Two neural networks have been used for the classification task. One is Multi-Layer Perceptron based on supervised learning and other is Self-Organising Map (SOM), which is based on unsupervised learning technique. The results have been shown and the possible ways of combining these networks have been proposed to yield better and faster classification results.\n    ",
        "submission_date": "2004-12-06T00:00:00",
        "last_modified_date": "2004-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412026",
        "title": "Removing Propagation Redundant Constraints in Redundant Modeling",
        "authors": [
            "Chiu Wo Choi",
            "Jimmy Ho-Man Lee",
            "Peter J. Stuckey"
        ],
        "abstract": "  A widely adopted approach to solving constraint satisfaction problems combines systematic tree search with various degrees of constraint propagation for pruning the search space. One common technique to improve the execution efficiency is to add redundant constraints, which are constraints logically implied by others in the problem model. However, some redundant constraints are propagation redundant and hence do not contribute additional propagation information to the constraint solver. Redundant constraints arise naturally in the process of redundant modeling where two models of the same problem are connected and combined through channeling constraints. In this paper, we give general theorems for proving propagation redundancy of one constraint with respect to channeling constraints and constraints in the other model. We illustrate, on problems from CSPlib (",
        "submission_date": "2004-12-07T00:00:00",
        "last_modified_date": "2004-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412041",
        "title": "An Efficient and Flexible Engine for Computing Fixed Points",
        "authors": [
            "Hai-Feng Guo",
            "Gopal Gupta"
        ],
        "abstract": "  An efficient and flexible engine for computing fixed points is critical for many practical applications. In this paper, we firstly present a goal-directed fixed point computation strategy in the logic programming paradigm. The strategy adopts a tabled resolution (or memorized resolution) to mimic the efficient semi-naive bottom-up computation. Its main idea is to dynamically identify and record those clauses that will lead to recursive variant calls, and then repetitively apply those alternatives incrementally until the fixed point is reached. Secondly, there are many situations in which a fixed point contains a large number or even infinite number of solutions. In these cases, a fixed point computation engine may not be efficient enough or feasible at all. We present a mode-declaration scheme which provides the capabilities to reduce a fixed point from a big solution set to a preferred small one, or from an infeasible infinite set to a finite one. The mode declaration scheme can be characterized as a meta-level operation over the original fixed point. We show the correctness of the mode declaration scheme. Thirdly, the mode-declaration scheme provides a new declarative method for dynamic programming, which is typically used for solving optimization problems. There is no need to define the value of an optimal solution recursively, instead, defining a general solution suffices. The optimal value as well as its corresponding concrete solution can be derived implicitly and automatically using a mode-directed fixed point computation engine. Finally, this fixed point computation engine has been successfully implemented in a commercial Prolog system. Experimental results are shown to indicate that the mode declaration improves both time and space performances in solving dynamic programming problems.\n    ",
        "submission_date": "2004-12-09T00:00:00",
        "last_modified_date": "2005-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412049",
        "title": "Neural Networks in Mobile Robot Motion",
        "authors": [
            "Danica Janglova"
        ],
        "abstract": "  This paper deals with a path planning and intelligent control of an autonomous robot which should move safely in partially structured environment. This environment may involve any number of obstacles of arbitrary shape and size; some of them are allowed to move. We describe our approach to solving the motion-planning problem in mobile robot control using neural networks-based technique. Our method of the construction of a collision-free path for moving robot among obstacles is based on two neural networks. The first neural network is used to determine the \"free\" space using ultrasound range finder data. The second neural network \"finds\" a safe direction for the next robot section of the path in the workspace while avoiding the nearest obstacles. Simulation examples of generated path with proposed techniques will be presented.\n    ",
        "submission_date": "2004-12-11T00:00:00",
        "last_modified_date": "2004-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412058",
        "title": "Clustering Categorical Data Streams",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng",
            "Joshua Zhexue Huang"
        ],
        "abstract": "  The data stream model has been defined for new classes of applications involving massive data being generated at a fast pace. Web click stream analysis and detection of network intrusions are two examples. Cluster analysis on data streams becomes more difficult, because the data objects in a data stream must be accessed in order and can be read only once or few times with limited resources. Recently, a few clustering algorithms have been developed for analyzing numeric data streams. However, to our knowledge to date, no algorithm exists for clustering categorical data streams. In this paper, we propose an efficient clustering algorithm for analyzing categorical data streams. It has been proved that the proposed algorithm uses small memory footprints. We provide empirical analysis on the performance of the algorithm in clustering both synthetic and real data streams\n    ",
        "submission_date": "2004-12-13T00:00:00",
        "last_modified_date": "2004-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412059",
        "title": "Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience",
        "authors": [
            "Ross W. Gayler"
        ],
        "abstract": "  Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.\n    ",
        "submission_date": "2004-12-13T00:00:00",
        "last_modified_date": "2004-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412068",
        "title": "ANTIDS: Self-Organized Ant-based Clustering Model for Intrusion Detection System",
        "authors": [
            "Vitorino Ramos",
            "Ajith Abraham"
        ],
        "abstract": "  Security of computers and the networks that connect them is increasingly becoming of great significance. Computer security is defined as the protection of computing systems against threats to confidentiality, integrity, and availability. There are two types of intruders: the external intruders who are unauthorized users of the machines they attack, and internal intruders, who have permission to access the system with some restrictions. Due to the fact that it is more and more improbable to a system administrator to recognize and manually intervene to stop an attack, there is an increasing recognition that ID systems should have a lot to earn on following its basic principles on the behavior of complex natural systems, namely in what refers to self-organization, allowing for a real distributed and collective perception of this phenomena. With that aim in mind, the present work presents a self-organized ant colony based intrusion detection system (ANTIDS) to detect intrusions in a network infrastructure. The performance is compared among conventional soft computing paradigms like Decision Trees, Support Vector Machines and Linear Genetic Programming to model fast, online and efficient intrusion detection systems.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412073",
        "title": "Self-Organizing the Abstract: Canvas as a Swarm Habitat for Collective Memory, Perception and Cooperative Distributed Creativity",
        "authors": [
            "Vitorino Ramos"
        ],
        "abstract": "  Past experiences under the designation of \"Swarm Paintings\" conducted in 2001, not only confirmed the possibility of realizing an artificial art (thus non-human), as introduced into the process the questioning of creative migration, specifically from the computer monitors to the canvas via a robotic harm. In more recent self-organized based research we seek to develop and profound the initial ideas by using a swarm of autonomous robots (ARTsBOT project 2002-03), that \"live\" avoiding the purpose of being merely a simple perpetrator of order streams coming from an external computer, but instead, that actually co-evolve within the canvas space, acting (that is, laying ink) according to simple inner threshold stimulus response functions, reacting simultaneously to the chromatic stimulus present in the canvas environment done by the passage of their team-mates, as well as by the distributed feedback, affecting their future collective behaviour. In parallel, and in what respects to certain types of collective systems, we seek to confirm, in a physically embedded way, that the emergence of order (even as a concept) seems to be found at a lower level of complexity, based on simple and basic interchange of information, and on the local dynamic of parts, who, by self-organizing mechanisms tend to form an lived whole, innovative and adapting, allowing for emergent open-ended creative and distributed production. KEYWORDS: ArtSBots Project, Swarm Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm Paintings, Robot Paintings, Non-Human Art, Painting Emergence and Cooperation, Art and Complexity, ArtBots: The Robot Talent Show.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412088",
        "title": "On Image Filtering, Noise and Morphological Size Intensity Diagrams",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "abstract": "  In the absence of a pure noise-free image it is hard to define what noise is, in any original noisy image, and as a consequence also where it is, and in what amount. In fact, the definition of noise depends largely on our own aim in the whole image analysis process, and (perhaps more important) in our self-perception of noise. For instance, when we perceive noise as disconnected and small it is normal to use MM-ASF filters to treat it. There is two evidences of this. First, in many instances there is no ideal and pure noise-free image to compare our filtering process (nothing but our self-perception of its pure image); second, and related with this first point, MM transformations that we chose are only based on our self - and perhaps - fuzzy notion. The present proposal combines the results of two MM filtering transformations (FT1, FT2) and makes use of some measures and quantitative relations on their Size/Intensity Diagrams to find the most appropriate noise removal process. Results can also be used for finding the most appropriate stop criteria, and the right sequence of MM operators combination on Alternating Sequential Filters (ASF), if these measures are applied, for instance, on a Genetic Algorithm's target function.\n    ",
        "submission_date": "2004-12-17T00:00:00",
        "last_modified_date": "2004-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0412098",
        "title": "The Google Similarity Distance",
        "authors": [
            "Rudi Cilibrasi",
            "Paul M. B. Vitanyi"
        ],
        "abstract": "  Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers the equivalent of `society' is `database,' and the equivalent of `use' is `way to search the database.' We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts we use the world-wide-web as database, and Google as search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the world-wide-web using Google page counts. The world-wide-web is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies, and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87% with the expert crafted WordNet categories.\n    ",
        "submission_date": "2004-12-21T00:00:00",
        "last_modified_date": "2007-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/math/0408146",
        "title": "Learning a Machine for the Decision in a Partially Observable Markov Universe",
        "authors": [
            "Frederic Dambreville"
        ],
        "abstract": "  In this paper, we are interested in optimal decisions in a partially observable Markov universe. Our viewpoint departs from the dynamic programming viewpoint: we are directly approximating an optimal strategic tree depending on the observation. This approximation is made by means of a parameterized probabilistic law. In this paper, a particular family of hidden Markov models, with input and output, is considered as a learning framework. A method for optimizing the parameters of these HMMs is proposed and applied. This optimization method is based on the cross-entropic principle.\n    ",
        "submission_date": "2004-08-11T00:00:00",
        "last_modified_date": "2004-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/nlin/0404004",
        "title": "Protocol Requirements for Self-organizing Artifacts: Towards an Ambient Intelligence",
        "authors": [
            "Carlos Gershenson",
            "Francis Heylighen"
        ],
        "abstract": "  We discuss which properties common-use artifacts should have to collaborate without human intervention. We conceive how devices, such as mobile phones, PDAs, and home appliances, could be seamlessly integrated to provide an \"ambient intelligence\" that responds to the user's desires without requiring explicit programming or commands. While the hardware and software technology to build such systems already exists, as yet there is no standard protocol that can learn new meanings. We propose the first steps in the development of such a protocol, which would need to be adaptive, extensible, and open to the community, while promoting self-organization. We argue that devices, interacting through \"game-like\" moves, can learn to agree about how to communicate, with whom to cooperate, and how to delegate and coordinate specialized tasks. Thus, they may evolve a distributed cognition or collective intelligence capable of tackling complex tasks.\n    ",
        "submission_date": "2004-04-01T00:00:00",
        "last_modified_date": "2004-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/nlin/0407032",
        "title": "Application of Artificial Neural Network in Jitter Analysis of Dispersion-Managed Communication System",
        "authors": [
            "F.P. Zen",
            "B.E. Gunara",
            "W. Hidayat",
            "Z.A. Thalib",
            "H. Zainuddin",
            "J. Aminuddin"
        ],
        "abstract": "  Artificial Neural Network (ANN) is used as numerical methode in solving modified Nonlinear Schroedinger (NLS) equation with Dispersion Managed System (DMS) for jitter analysis. We take the optical axis z and the time t as input, and then some relevant values such as the change of position and the center frequency of the pulse, and further the mean square time of incoming pulse which are needed for jitter analysis. It shows that ANN yields numerical solutions which are adaptive with respect to the numerical errors and also verifies the previous numerical results using conventional numerical method. Our result indicates that DMS can minimize the timing jitter induced by some amplifiers.\n    ",
        "submission_date": "2004-07-14T00:00:00",
        "last_modified_date": "2004-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/nlin/0411066",
        "title": "Self-Organizing Traffic Lights",
        "authors": [
            "Carlos Gershenson"
        ],
        "abstract": "  Steering traffic in cities is a very complex task, since improving efficiency involves the coordination of many actors. Traditional approaches attempt to optimize traffic lights for a particular density and configuration of traffic. The disadvantage of this lies in the fact that traffic densities and configurations change constantly. Traffic seems to be an adaptation problem rather than an optimization problem. We propose a simple and feasible alternative, in which traffic lights self-organize to improve traffic flow. We use a multi-agent simulation to study three self-organizing methods, which are able to outperform traditional rigid and adaptive methods. Using simple rules and no direct communication, traffic lights are able to self-organize and adapt to changing traffic conditions, reducing waiting times, number of stopped cars, and increasing average speeds.\n    ",
        "submission_date": "2004-11-30T00:00:00",
        "last_modified_date": "2005-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/q-bio/0403022",
        "title": "Intelligent encoding and economical communication in the visual stream",
        "authors": [
            "Andras Lorincz"
        ],
        "abstract": "  The theory of computational complexity is used to underpin a recent model of neocortical sensory processing. We argue that encoding into reconstruction networks is appealing for communicating agents using Hebbian learning and working on hard combinatorial problems, which are easy to verify. Computational definition of the concept of intelligence is provided. Simulations illustrate the idea.\n    ",
        "submission_date": "2004-03-16T00:00:00",
        "last_modified_date": "2004-03-16T00:00:00"
    }
]