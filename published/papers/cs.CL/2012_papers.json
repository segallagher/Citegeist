[
    {
        "url": "https://arxiv.org/abs/1201.1192",
        "title": "Formalization of semantic network of image constructions in electronic content",
        "authors": [
            "Oleg Bisikalo",
            "Irina Kravchuk"
        ],
        "abstract": "A formal theory based on a binary operator of directional associative relation is constructed in the article and an understanding of an associative normal form of image constructions is introduced. A model of a commutative semigroup, which provides a presentation of a sentence as three components of an interrogative linguistic image construction, is considered.\n    ",
        "submission_date": "2012-01-05T00:00:00",
        "last_modified_date": "2012-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.1652",
        "title": "Toward a Motor Theory of Sign Language Perception",
        "authors": [
            "Sylvie Gibet",
            "Pierre-Fran\u00e7ois Marteau",
            "Kyle Duarte"
        ],
        "abstract": "Researches on signed languages still strongly dissociate lin- guistic issues related on phonological and phonetic aspects, and gesture studies for recognition and synthesis purposes. This paper focuses on the imbrication of motion and meaning for the analysis, synthesis and evaluation of sign language gestures. We discuss the relevance and interest of a motor theory of perception in sign language communication. According to this theory, we consider that linguistic knowledge is mapped on sensory-motor processes, and propose a methodology based on the principle of a synthesis-by-analysis approach, guided by an evaluation process that aims to validate some hypothesis and concepts of this theory. Examples from existing studies illustrate the di erent concepts and provide avenues for future work.\n    ",
        "submission_date": "2012-01-08T00:00:00",
        "last_modified_date": "2012-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2010",
        "title": "Recognizing Bangla Grammar using Predictive Parser",
        "authors": [
            "K. M. Azharul Hasan",
            "Al-Mahmud",
            "Amit Mondal",
            "Amit Saha"
        ],
        "abstract": "We describe a Context Free Grammar (CFG) for Bangla language and hence we propose a Bangla parser based on the grammar. Our approach is very much general to apply in Bangla Sentences and the method is well accepted for parsing a language of a grammar. The proposed parser is a predictive parser and we construct the parse table for recognizing Bangla grammar. Using the parse table we recognize syntactical mistakes of Bangla sentences when there is no entry for a terminal in the parse table. If a natural language can be successfully parsed then grammar checking from this language becomes possible. The proposed scheme is based on Top down parsing method and we have avoided the left recursion of the CFG using the idea of left factoring.\n    ",
        "submission_date": "2012-01-10T00:00:00",
        "last_modified_date": "2012-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.4733",
        "title": "Du TAL au TIL",
        "authors": [
            "Michael Zock",
            "Guy Lapalme"
        ],
        "abstract": "Historically two types of NLP have been investigated: fully automated processing of language by machines (NLP) and autonomous processing of natural language by people, i.e. the human brain (psycholinguistics). We believe that there is room and need for another kind, INLP: interactive natural language processing. This intermediate approach starts from peoples' needs, trying to bridge the gap between their actual knowledge and a given goal. Given the fact that peoples' knowledge is variable and often incomplete, the aim is to build bridges linking a given knowledge state to a given goal. We present some examples, trying to show that this goal is worth pursuing, achievable and at a reasonable cost.\n    ",
        "submission_date": "2012-01-20T00:00:00",
        "last_modified_date": "2012-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.6224",
        "title": "Wikipedia Arborification and Stratified Explicit Semantic Analysis",
        "authors": [
            "Yannis Haralambous",
            "Vitaly Klyuev"
        ],
        "abstract": "[This is the translation of paper \"Arborification de Wikip\u00e9dia et analyse s\u00e9mantique explicite stratifi\u00e9e\" submitted to TALN 2012.]\n",
        "submission_date": "2012-01-30T00:00:00",
        "last_modified_date": "2012-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0116",
        "title": "Inference and Plausible Reasoning in a Natural Language Understanding System Based on Object-Oriented Semantics",
        "authors": [
            "Yuriy Ostapov"
        ],
        "abstract": "Algorithms of inference in a computer system oriented to input and semantic processing of text information are presented. Such inference is necessary for logical questions when the direct comparison of objects from a question and database can not give a result. The following classes of problems are considered: a check of hypotheses for persons and non-typical actions, the determination of persons and circumstances for non-typical actions, planning actions, the determination of event cause and state of persons. To form an answer both deduction and plausible reasoning are used. As a knowledge domain under consideration is social behavior of persons, plausible reasoning is based on laws of social psychology. Proposed algorithms of inference and plausible reasoning can be realized in computer systems closely connected with text processing (criminology, operation of business, medicine, document systems).\n    ",
        "submission_date": "2012-02-01T00:00:00",
        "last_modified_date": "2012-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.1054",
        "title": "Considering a resource-light approach to learning verb valencies",
        "authors": [
            "Alex Rudnick"
        ],
        "abstract": "Here we describe work on learning the subcategories of verbs in a morphologically rich language using only minimal linguistic resources. Our goal is to learn verb subcategorizations for Quechua, an under-resourced morphologically rich language, from an unannotated corpus. We compare results from applying this approach to an unannotated Arabic corpus with those achieved by processing the same text in treebank form. The original plan was to use only a morphological analyzer and an unannotated corpus, but experiments suggest that this approach by itself will not be effective for learning the combinatorial potential of Arabic verbs in general. The lower bound on resources for acquiring this information is somewhat higher, apparently requiring a a part-of-speech tagger and chunker for most languages, and a morphological disambiguater for Arabic.\n    ",
        "submission_date": "2012-02-06T00:00:00",
        "last_modified_date": "2012-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.1568",
        "title": "Beyond Sentiment: The Manifold of Human Emotions",
        "authors": [
            "Seungyeon Kim",
            "Fuxin Li",
            "Guy Lebanon",
            "Irfan Essa"
        ],
        "abstract": "Sentiment analysis predicts the presence of positive or negative emotions in a text document. In this paper we consider higher dimensional extensions of the sentiment concept, which represent a richer set of human emotions. Our approach goes beyond previous work in that our model contains a continuous manifold rather than a finite set of human emotions. We investigate the resulting model, compare it to psychological observations, and explore its predictive capabilities. Besides obtaining significant improvements over a baseline without manifold, we are also able to visualize different notions of positive sentiment in different domains.\n    ",
        "submission_date": "2012-02-08T00:00:00",
        "last_modified_date": "2013-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6266",
        "title": "Realisation d'un systeme de reconnaissance automatique de la parole arabe base sur CMU Sphinx",
        "authors": [
            "Ali Sadiqui",
            "Noureddine Chenfour"
        ],
        "abstract": "This paper presents the continuation of the work completed by Satori and all. [SCH07] by the realization of an automatic speech recognition system (ASR) for Arabic language based SPHINX 4 system. The previous work was limited to the recognition of the first ten digits, whereas the present work is a remarkable projection consisting in continuous Arabic speech recognition with a rate of recognition of surroundings 96%.\n    ",
        "submission_date": "2012-02-28T00:00:00",
        "last_modified_date": "2012-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.6583",
        "title": "A Lexical Analysis Tool with Ambiguity Support",
        "authors": [
            "Luis Quesada",
            "Fernando Berzal",
            "Francisco J. Cortijo"
        ],
        "abstract": "Lexical ambiguities naturally arise in languages. We present Lamb, a lexical analyzer that produces a lexical analysis graph describing all the possible sequences of tokens that can be found within the input string. Parsers can process such lexical analysis graphs and discard any sequence of tokens that does not produce a valid syntactic sentence, therefore performing, together with Lamb, a context-sensitive lexical analysis in lexically-ambiguous language specifications.\n    ",
        "submission_date": "2012-02-29T00:00:00",
        "last_modified_date": "2012-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0145",
        "title": "The Horse Raced Past: Gardenpath Processing in Dynamical Systems",
        "authors": [
            "Peter beim Graben"
        ],
        "abstract": "I pinpoint an interesting similarity between a recent account to rational parsing and the treatment of sequential decisions problems in a dynamical systems approach. I argue that expectation-driven search heuristics aiming at fast computation resembles a high-risk decision strategy in favor of large transition velocities. Hale's rational parser, combining generalized left-corner parsing with informed $\\mathrm{A}^*$ search to resolve processing conflicts, explains gardenpath effects in natural sentence processing by misleading estimates of future processing costs that are to be minimized. On the other hand, minimizing the duration of cognitive computations in time-continuous dynamical systems can be described by combining vector space representations of cognitive states by means of filler/role decompositions and subsequent tensor product representations with the paradigm of stable heteroclinic sequences. Maximizing transition velocities according to a high-risk decision strategy could account for a fast race even between states that are apparently remote in representation space.\n    ",
        "submission_date": "2012-03-01T00:00:00",
        "last_modified_date": "2012-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0504",
        "title": "Modelling Social Structures and Hierarchies in Language Evolution",
        "authors": [
            "Martin Bachwerk",
            "Carl Vogel"
        ],
        "abstract": "Language evolution might have preferred certain prior social configurations over others. Experiments conducted with models of different social structures (varying subgroup interactions and the role of a dominant interlocutor) suggest that having isolated agent groups rather than an interconnected agent is more advantageous for the emergence of a social communication system. Distinctive groups that are closely connected by communication yield systems less like natural language than fully isolated groups inhabiting the same world. Furthermore, the addition of a dominant male who is asymmetrically favoured as a hearer, and equally likely to be a speaker has no positive influence on the disjoint groups.\n    ",
        "submission_date": "2012-03-02T00:00:00",
        "last_modified_date": "2012-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.0512",
        "title": "Establishing linguistic conventions in task-oriented primeval dialogue",
        "authors": [
            "Martin Bachwerk",
            "Carl Vogel"
        ],
        "abstract": "In this paper, we claim that language is likely to have emerged as a mechanism for coordinating the solution of complex tasks. To confirm this thesis, computer simulations are performed based on the coordination task presented by Garrod & Anderson (1987). The role of success in task-oriented dialogue is analytically evaluated with the help of performance measurements and a thorough lexical analysis of the emergent communication system. Simulation results confirm a strong effect of success mattering on both reliability and dispersion of linguistic conventions.\n    ",
        "submission_date": "2012-03-02T00:00:00",
        "last_modified_date": "2012-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.1685",
        "title": "Statistical Function Tagging and Grammatical Relations of Myanmar Sentences",
        "authors": [
            "Win Win Thant",
            "Tin Myat Htwe",
            "Ni Lar Thein"
        ],
        "abstract": "This paper describes a context free grammar (CFG) based grammatical relations for Myanmar sentences which combine corpus-based function tagging system. Part of the challenge of statistical function tagging for Myanmar sentences comes from the fact that Myanmar has free-phrase-order and a complex morphological system. Function tagging is a pre-processing step to show grammatical relations of Myanmar sentences. In the task of function tagging, which tags the function of Myanmar sentences with correct segmentation, POS (part-of-speech) tagging and chunking information, we use Naive Bayesian theory to disambiguate the possible function tags of a word. We apply context free grammar (CFG) to find out the grammatical relations of the function tags. We also create a functional annotated tagged corpus for Myanmar and propose the grammar rules for Myanmar sentences. Experiments show that our analysis achieves a good result with simple sentences and complex sentences.\n    ",
        "submission_date": "2012-03-08T00:00:00",
        "last_modified_date": "2012-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.1858",
        "title": "Distributional Measures of Semantic Distance: A Survey",
        "authors": [
            "Saif M. Mohammad",
            "Graeme Hirst"
        ],
        "abstract": "The ability to mimic human notions of semantic distance has widespread applications. Some measures rely only on raw text (distributional measures) and some rely on knowledge sources such as WordNet. Although extensive studies have been performed to compare WordNet-based measures with human judgment, the use of distributional measures as proxies to estimate semantic distance has received little attention. Even though they have traditionally performed poorly when compared to WordNet-based measures, they lay claim to certain uniquely attractive features, such as their applicability in resource-poor languages and their ability to mimic both semantic similarity and semantic relatedness. Therefore, this paper presents a detailed study of distributional measures. Particular attention is paid to flesh out the strengths and limitations of both WordNet-based and distributional measures, and how distributional measures of distance can be brought more in line with human notions of semantic distance. We conclude with a brief discussion of recent work on hybrid measures.\n    ",
        "submission_date": "2012-03-08T00:00:00",
        "last_modified_date": "2012-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.1889",
        "title": "Distributional Measures as Proxies for Semantic Relatedness",
        "authors": [
            "Saif M Mohammad",
            "Graeme Hirst"
        ],
        "abstract": "The automatic ranking of word pairs as per their semantic relatedness and ability to mimic human notions of semantic relatedness has widespread applications. Measures that rely on raw data (distributional measures) and those that use knowledge-rich ontologies both exist. Although extensive studies have been performed to compare ontological measures with human judgment, the distributional measures have primarily been evaluated by indirect means. This paper is a detailed study of some of the major distributional measures; it lists their respective merits and limitations. New measures that overcome these drawbacks, that are more in line with the human notions of semantic relatedness, are suggested. The paper concludes with an exhaustive comparison of the distributional and ontology-based measures. Along the way, significant research problems are identified. Work on these problems may lead to a better understanding of how semantic relatedness is to be measured.\n    ",
        "submission_date": "2012-03-08T00:00:00",
        "last_modified_date": "2012-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.2293",
        "title": "Categories of Emotion names in Web retrieved texts",
        "authors": [
            "Sergey Petrov",
            "Jose F. Fontanari",
            "Leonid I. Perlovsky"
        ],
        "abstract": "The categorization of emotion names, i.e., the grouping of emotion words that have similar emotional connotations together, is a key tool of Social Psychology used to explore people's knowledge about emotions. Without exception, the studies following that research line were based on the gauging of the perceived similarity between emotion names by the participants of the experiments. Here we propose and examine a new approach to study the categories of emotion names - the similarities between target emotion names are obtained by comparing the contexts in which they appear in texts retrieved from the World Wide Web. This comparison does not account for any explicit semantic information; it simply counts the number of common words or lexical items used in the contexts. This procedure allows us to write the entries of the similarity matrix as dot products in a linear vector space of contexts. The properties of this matrix were then explored using Multidimensional Scaling Analysis and Hierarchical Clustering. Our main findings, namely, the underlying dimension of the emotion space and the categories of emotion names, were consistent with those based on people's judgments of emotion names similarities.\n    ",
        "submission_date": "2012-03-11T00:00:00",
        "last_modified_date": "2012-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.2299",
        "title": "A Cross-cultural Corpus of Annotated Verbal and Nonverbal Behaviors in Receptionist Encounters",
        "authors": [
            "Maxim Makatchev",
            "Reid Simmons",
            "Majd Sakr"
        ],
        "abstract": "We present the first annotated corpus of nonverbal behaviors in receptionist interactions, and the first nonverbal corpus (excluding the original video and audio data) of service encounters freely available online. Native speakers of American English and Arabic participated in a naturalistic role play at reception desks of university buildings in Doha, Qatar and Pittsburgh, USA. Their manually annotated nonverbal behaviors include gaze direction, hand and head gestures, torso positions, and facial expressions. We discuss possible uses of the corpus and envision it to become a useful tool for the human-robot interaction community.\n    ",
        "submission_date": "2012-03-11T00:00:00",
        "last_modified_date": "2012-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.2498",
        "title": "Fault detection system for Arabic language",
        "authors": [
            "Riadh Bouslimi",
            "Houda Amraoui"
        ],
        "abstract": "The study of natural language, especially Arabic, and mechanisms for the implementation of automatic processing is a fascinating field of study, with various potential applications. The importance of tools for natural language processing is materialized by the need to have applications that can effectively treat the vast mass of information available nowadays on electronic forms. Among these tools, mainly driven by the necessity of a fast writing in alignment to the actual daily life speed, our interest is on the writing auditors. The morphological and syntactic properties of Arabic make it a difficult language to master, and explain the lack in the processing tools for that language. Among these properties, we can mention: the complex structure of the Arabic word, the agglutinative nature, lack of vocalization, the segmentation of the text, the linguistic richness, etc.\n    ",
        "submission_date": "2012-03-08T00:00:00",
        "last_modified_date": "2013-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3023",
        "title": "Toward an example-based machine translation from written text to ASL using virtual agent animation",
        "authors": [
            "Mehrez Boulares",
            "Mohamed Jemni"
        ],
        "abstract": "Modern computational linguistic software cannot produce important aspects of sign language translation. Using some researches we deduce that the majority of automatic sign language translation systems ignore many aspects when they generate animation; therefore the interpretation lost the truth information meaning. Our goals are: to translate written text from any language to ASL animation; to model maximum raw information using machine learning and computational techniques; and to produce a more adapted and expressive form to natural looking and understandable ASL animations. Our methods include linguistic annotation of initial text and semantic orientation to generate the facial expression. We use the genetic algorithms coupled to learning/recognized systems to produce the most natural form. To detect emotion we are based on fuzzy logic to produce the degree of interpolation between facial expressions. Roughly, we present a new expressive language Text Adapted Sign Modeling Language TASML that describes all maximum aspects related to a natural sign language interpretation. This paper is organized as follow: the next section is devoted to present the comprehension effect of using Space/Time/SVO form in ASL animation based on experimentation. In section 3, we describe our technical considerations. We present the general approach we adopted to develop our tool in section 4. Finally, we give some perspectives and future works.\n    ",
        "submission_date": "2012-03-14T00:00:00",
        "last_modified_date": "2012-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3584",
        "title": "An Accurate Arabic Root-Based Lemmatizer for Information Retrieval Purposes",
        "authors": [
            "Tarek El-Shishtawy",
            "Fatma El-Ghannam"
        ],
        "abstract": "In spite of its robust syntax, semantic cohesion, and less ambiguity, lemma level analysis and generation does not yet focused in Arabic NLP literatures. In the current research, we propose the first non-statistical accurate Arabic lemmatizer algorithm that is suitable for information retrieval (IR) systems. The proposed lemmatizer makes use of different Arabic language knowledge resources to generate accurate lemma form and its relevant features that support IR purposes. As a POS tagger, the experimental results show that, the proposed algorithm achieves a maximum accuracy of 94.8%. For first seen documents, an accuracy of 89.15% is achieved, compared to 76.7% of up to date Stanford accurate Arabic model, for the same, dataset.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4176",
        "title": "SignsWorld; Deeping Into the Silence World and Hearing Its Signs (State of the Art)",
        "authors": [
            "A.M. Riad",
            "Hamdy K.Elmonier",
            "Samaa. M. Shohieb",
            "A.S. Asem"
        ],
        "abstract": "Automatic speech processing systems are employed more and more often in real environments. Although the underlying speech technology is mostly language independent, differences between languages with respect to their structure and grammar have substantial effect on the recognition systems performance. In this paper, we present a review of the latest developments in the sign language recognition research in general and in the Arabic sign language (ArSL) in specific. This paper also presents a general framework for improving the deaf community communication with the hearing people that is called SignsWorld. The overall goal of the SignsWorld project is to develop a vision-based technology for recognizing and translating continuous Arabic sign language ArSL.\n    ",
        "submission_date": "2012-03-10T00:00:00",
        "last_modified_date": "2012-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4605",
        "title": "Arabic Keyphrase Extraction using Linguistic knowledge and Machine Learning Techniques",
        "authors": [
            "Tarek El-shishtawy",
            "Abdulwahab Al-sammak"
        ],
        "abstract": "In this paper, a supervised learning technique for extracting keyphrases of Arabic documents is presented. The extractor is supplied with linguistic knowledge to enhance its efficiency instead of relying only on statistical information such as term frequency and distance. During analysis, an annotated Arabic corpus is used to extract the required lexical features of the document words. The knowledge also includes syntactic rules based on part of speech tags and allowed word sequences to extract the candidate keyphrases. In this work, the abstract form of Arabic words is used instead of its stem form to represent the candidate terms. The Abstract form hides most of the inflections found in Arabic words. The paper introduces new features of keyphrases based on linguistic knowledge, to capture titles and subtitles of a document. A simple ANOVA test is used to evaluate the validity of selected features. Then, the learning model is built using the LDA - Linear Discriminant Analysis - and training documents. Although, the presented system is trained using documents in the IT domain, experiments carried out show that it has a significantly better performance than the existing Arabic extractor systems, where precision and recall values reach double their corresponding values in the other systems especially for lengthy and non-scientific articles.\n    ",
        "submission_date": "2012-03-20T00:00:00",
        "last_modified_date": "2012-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4933",
        "title": "Reduplicated MWE (RMWE) helps in improving the CRF based Manipuri POS Tagger",
        "authors": [
            "Kishorjit Nongmeikapam",
            "Lairenlakpam Nonglenjaoba",
            "Yumnam Nirmal",
            "Sivaji Bandyopadhyay"
        ],
        "abstract": "This paper gives a detail overview about the modified features selection in CRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging. Selection of features is so important in CRF that the better are the features then the better are the outputs. This work is an attempt or an experiment to make the previous work more efficient. Multiple new features are tried to run the CRF and again tried with the Reduplicated Multiword Expression (RMWE) as another feature. The CRF run with RMWE because Manipuri is rich of RMWE and identification of RMWE becomes one of the necessities to bring up the result of POS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15% and F-measure of 75.60%. With the identification of RMWE and considering it as a feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and F-measure of 77.14%.\n    ",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5051",
        "title": "Analysing Temporally Annotated Corpora with CAVaT",
        "authors": [
            "Leon Derczynski",
            "Robert Gaizauskas"
        ],
        "abstract": "We present CAVaT, a tool that performs Corpus Analysis and Validation for TimeML. CAVaT is an open source, modular checking utility for statistical analysis of features specific to temporally-annotated natural language corpora. It provides reporting, highlights salient links between a variety of general and time-specific linguistic features, and also validates a temporal annotation to ensure that it is logically consistent and sufficiently annotated. Uniquely, CAVaT provides analysis specific to TimeML-annotated temporal information. TimeML is a standard for annotating temporal information in natural language text. In this paper, we present the reporting part of CAVaT, and then its error-checking ability, including the workings of several novel TimeML document verification methods. This is followed by the execution of some example tasks using the tool to show relations between times, events, signals and links. We also demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been detected with CAVaT.\n    ",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5055",
        "title": "Using Signals to Improve Automatic Classification of Temporal Relations",
        "authors": [
            "Leon Derczynski",
            "Robert Gaizauskas"
        ],
        "abstract": "Temporal information conveyed by language describes how the world around us changes through time. Events, durations and times are all temporal elements that can be viewed as intervals. These intervals are sometimes temporally related in text. Automatically determining the nature of such relations is a complex and unsolved problem. Some words can act as \"signals\" which suggest a temporal ordering between intervals. In this paper, we use these signal words to improve the accuracy of a recent approach to classification of temporal links.\n    ",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5060",
        "title": "USFD2: Annotating Temporal Expresions and TLINKs for TempEval-2",
        "authors": [
            "Leon Derczynski",
            "Robert Gaizauskas"
        ],
        "abstract": "We describe the University of Sheffield system used in the TempEval-2 challenge, USFD2. The challenge requires the automatic identification of temporal entities and relations in text. USFD2 identifies and anchors temporal expressions, and also attempts two of the four temporal relation assignment tasks. A rule-based system picks out and anchors temporal expressions, and a maximum entropy classifier assigns temporal link labels, based on features that include descriptions of associated temporal signal words. USFD2 identified temporal expressions successfully, and correctly classified their type in 90% of cases. Determining the relation between an event and time expression in the same sentence was performed at 63% accuracy, the second highest score in this part of the challenge.\n    ",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5062",
        "title": "An Annotation Scheme for Reichenbach's Verbal Tense Structure",
        "authors": [
            "Leon Derczynski",
            "Robert Gaizauskas"
        ],
        "abstract": "In this paper we present RTMML, a markup language for the tenses of verbs and temporal relations between verbs. There is a richness to tense in language that is not fully captured by existing temporal annotation schemata. Following Reichenbach we present an analysis of tense in terms of abstract time points, with the aim of supporting automated processing of tense and temporal relations in language. This allows for precise reasoning about tense in documents, and the deduction of temporal relations between the times and verbal events in a discourse. We define the syntax of RTMML, and demonstrate the markup in a range of situations.\n    ",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5066",
        "title": "A Corpus-based Study of Temporal Signals",
        "authors": [
            "Leon Derczynski",
            "Robert Gaizauskas"
        ],
        "abstract": "Automatic temporal ordering of events described in discourse has been of great interest in recent years. Event orderings are conveyed in text via va rious linguistic mechanisms including the use of expressions such as \"before\", \"after\" or \"during\" that explicitly assert a temporal relation -- temporal signals. In this paper, we investigate the role of temporal signals in temporal relation extraction and provide a quantitative analysis of these expres sions in the TimeBank annotated corpus.\n    ",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5073",
        "title": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding",
        "authors": [
            "Amev Burman",
            "Arun Jayapal",
            "Sathish Kannan",
            "Madhu Kavilikatta",
            "Ayman Alhelbawy",
            "Leon Derczynski",
            "Robert Gaizauskas"
        ],
        "abstract": "This paper describes the University of Sheffield's entry in the 2011 TAC KBP entity linking and slot filling tasks. We chose to participate in the monolingual entity linking task, the monolingual slot filling task and the temporal slot filling tasks. We set out to build a framework for experimentation with knowledge base population. This framework was created, and applied to multiple KBP tasks. We demonstrated that our proposed framework is effective and suitable for collaborative development efforts, as well as useful in a teaching environment. Finally we present results that, while very modest, provide improvements an order of magnitude greater than our 2010 attempt.\n    ",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5076",
        "title": "Massively Increasing TIMEX3 Resources: A Transduction Approach",
        "authors": [
            "Leon Derczynski",
            "H\u00e9ctor Llorens",
            "Estela Saquete"
        ],
        "abstract": "Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. Gold standard temporally-annotated resources are limited in size, which makes research using them difficult. Standards have also evolved over the past decade, so not all temporally annotated data is in the same format. We vastly increase available human-annotated temporal expression resources by converting older format resources to TimeML/TIMEX3. This task is difficult due to differing annotation methods. We present a robust conversion tool and a new, large temporal expression resource. Using this, we evaluate our conversion process by using it as training data for an existing TimeML annotation tool, achieving a 0.87 F1 measure -- better than any system in the TempEval-2 timex recognition exercise.\n    ",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5084",
        "title": "A Data Driven Approach to Query Expansion in Question Answering",
        "authors": [
            "Leon Derczynski",
            "Jun Wang",
            "Robert Gaizauskas",
            "Mark A. Greenwood"
        ],
        "abstract": "Automated answering of natural language questions is an interesting and useful problem to solve. Question answering (QA) systems often perform information retrieval at an initial stage. Information retrieval (IR) performance, provided by engines such as Lucene, places a bound on overall system performance. For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions.\n",
        "submission_date": "2012-03-22T00:00:00",
        "last_modified_date": "2012-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5255",
        "title": "Post-Editing Error Correction Algorithm for Speech Recognition using Bing Spelling Suggestion",
        "authors": [
            "Youssef Bassil",
            "Mohammad Alwani"
        ],
        "abstract": "ASR short for Automatic Speech Recognition is the process of converting a spoken speech into text that can be manipulated by a computer. Although ASR has several applications, it is still erroneous and imprecise especially if used in a harsh surrounding wherein the input speech is of low quality. This paper proposes a post-editing ASR error correction method and algorithm based on Bing's online spelling suggestion. In this approach, the ASR recognized output text is spell-checked using Bing's spelling suggestion technology to detect and correct misrecognized words. More specifically, the proposed algorithm breaks down the ASR output text into several word-tokens that are submitted as search queries to Bing search engine. A returned spelling suggestion implies that a query is misspelled; and thus it is replaced by the suggested correction; otherwise, no correction is performed and the algorithm continues with the next token until all tokens get validated. Experiments carried out on various speeches in different languages indicated a successful decrease in the number of ASR errors and an improvement in the overall error correction rate. Future research can improve upon the proposed algorithm so much so that it can be parallelized to take advantage of multiprocessor computers.\n    ",
        "submission_date": "2012-03-23T00:00:00",
        "last_modified_date": "2012-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5262",
        "title": "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset",
        "authors": [
            "Youssef Bassil",
            "Paul Semaan"
        ],
        "abstract": "At the present time, computers are employed to solve complex tasks and problems ranging from simple calculations to intensive digital image processing and intricate algorithmic optimization problems to computationally-demanding weather forecasting problems. ASR short for Automatic Speech Recognition is yet another type of computational problem whose purpose is to recognize human spoken speech and convert it into text that can be processed by a computer. Despite that ASR has many versatile and pervasive real-world applications,it is still relatively erroneous and not perfectly solved as it is prone to produce spelling errors in the recognized text, especially if the ASR system is operating in a noisy environment, its vocabulary size is limited, and its input speech is of bad or low quality. This paper proposes a post-editing ASR error correction method based on MicrosoftN-Gram dataset for detecting and correcting spelling errors generated by ASR systems. The proposed method comprises an error detection algorithm for detecting word errors; a candidate corrections generation algorithm for generating correction suggestions for the detected word errors; and a context-sensitive error correction algorithm for selecting the best candidate for correction. The virtue of using the Microsoft N-Gram dataset is that it contains real-world data and word sequences extracted from the web which canmimica comprehensive dictionary of words having a large and all-inclusive vocabulary. Experiments conducted on numerous speeches, performed by different speakers, showed a remarkable reduction in ASR errors. Future research can improve upon the proposed algorithm so much so that it can be parallelized to take advantage of multiprocessor and distributed systems.\n    ",
        "submission_date": "2012-03-23T00:00:00",
        "last_modified_date": "2012-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5502",
        "title": "Exploring Text Virality in Social Networks",
        "authors": [
            "Marco Guerini",
            "Carlo Strapparava",
            "Gozde Ozbal"
        ],
        "abstract": "This paper aims to shed some light on the concept of virality - especially in social networks - and to provide new insights on its structure. We argue that: (a) virality is a phenomenon strictly connected to the nature of the content being spread, rather than to the influencers who spread it, (b) virality is a phenomenon with many facets, i.e. under this generic term several different effects of persuasive communication are comprised and they only partially overlap. To give ground to our claims, we provide initial experiments in a machine learning framework to show how various aspects of virality can be independently predicted according to content features.\n    ",
        "submission_date": "2012-03-25T00:00:00",
        "last_modified_date": "2012-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.6136",
        "title": "Tree Transducers, Machine Translation, and Cross-Language Divergences",
        "authors": [
            "Alex Rudnick"
        ],
        "abstract": "Tree transducers are formal automata that transform trees into other trees. Many varieties of tree transducers have been explored in the automata theory literature, and more recently, in the machine translation literature. In this paper I review T and xT transducers, situate them among related formalisms, and show how they can be used to implement rules for machine translation systems that cover all of the cross-language structural divergences described in Bonnie Dorr's influential article on the topic. I also present an implementation of xT transduction, suitable and convenient for experimenting with translation rules.\n    ",
        "submission_date": "2012-03-28T00:00:00",
        "last_modified_date": "2012-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.6360",
        "title": "You had me at hello: How phrasing affects memorability",
        "authors": [
            "Cristian Danescu-Niculescu-Mizil",
            "Justin Cheng",
            "Jon Kleinberg",
            "Lillian Lee"
        ],
        "abstract": "Understanding the ways in which information achieves widespread public awareness is a research question of significant interest. We consider whether, and how, the way in which the information is phrased --- the choice of words and sentence structure --- can affect this process. To this end, we develop an analysis framework and build a corpus of movie quotes, annotated with memorability information, in which we are able to control for both the speaker and the setting of the quotes. We find that there are significant differences between memorable and non-memorable quotes in several key dimensions, even after controlling for situational and contextual factors. One is lexical distinctiveness: in aggregate, memorable quotes use less common word choices, but at the same time are built upon a scaffolding of common syntactic patterns. Another is that memorable quotes tend to be more general in ways that make them easy to apply in new contexts --- that is, more portable. We also show how the concept of \"memorable language\" can be extended across domains.\n    ",
        "submission_date": "2012-03-28T00:00:00",
        "last_modified_date": "2012-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.6845",
        "title": "Information Retrieval Systems Adapted to the Biomedical Domain",
        "authors": [
            "M\u00f3nica Marrero",
            "Sonia S\u00e1nchez-Cuadrado",
            "Juli\u00e1n Urbano",
            "Jorge Morato",
            "Jos\u00e9-Antonio Moreiro"
        ],
        "abstract": "The terminology used in Biomedicine shows lexical peculiarities that have required the elaboration of terminological resources and information retrieval systems with specific functionalities. The main characteristics are the high rates of synonymy and homonymy, due to phenomena such as the proliferation of polysemic acronyms and their interaction with common language. Information retrieval systems in the biomedical domain use techniques oriented to the treatment of these lexical peculiarities. In this paper we review some of the techniques used in this domain, such as the application of Natural Language Processing (BioNLP), the incorporation of lexical-semantic resources, and the application of Named Entity Recognition (BioNER). Finally, we present the evaluation methods adopted to assess the suitability of these techniques for retrieving biomedical resources.\n    ",
        "submission_date": "2012-03-30T00:00:00",
        "last_modified_date": "2012-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0140",
        "title": "Roget's Thesaurus as a Lexical Resource for Natural Language Processing",
        "authors": [
            "Mario Jarmasz"
        ],
        "abstract": "WordNet proved that it is possible to construct a large-scale electronic lexical database on the principles of lexical semantics. It has been accepted and used extensively by computational linguists ever since it was released. Inspired by WordNet's success, we propose as an alternative a similar resource, based on the 1987 Penguin edition of Roget's Thesaurus of English Words and Phrases.\n",
        "submission_date": "2012-03-31T00:00:00",
        "last_modified_date": "2012-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0184",
        "title": "Parallel Spell-Checking Algorithm Based on Yahoo! N-Grams Dataset",
        "authors": [
            "Youssef Bassil"
        ],
        "abstract": "Spell-checking is the process of detecting and sometimes providing suggestions for incorrectly spelled words in a text. Basically, the larger the dictionary of a spell-checker is, the higher is the error detection rate; otherwise, misspellings would pass undetected. Unfortunately, traditional dictionaries suffer from out-of-vocabulary and data sparseness problems as they do not encompass large vocabulary of words indispensable to cover proper names, domain-specific terms, technical jargons, special acronyms, and terminologies. As a result, spell-checkers will incur low error detection and correction rate and will fail to flag all errors in the text. This paper proposes a new parallel shared-memory spell-checking algorithm that uses rich real-world word statistics from Yahoo! N-Grams Dataset to correct non-word and real-word errors in computer text. Essentially, the proposed algorithm can be divided into three sub-algorithms that run in a parallel fashion: The error detection algorithm that detects misspellings, the candidates generation algorithm that generates correction suggestions, and the error correction algorithm that performs contextual error correction. Experiments conducted on a set of text articles containing misspellings, showed a remarkable spelling error correction rate that resulted in a radical reduction of both non-word and real-word errors in electronic text. In a further study, the proposed algorithm is to be optimized for message-passing systems so as to become more flexible and less costly to scale over distributed machines.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0188",
        "title": "OCR Context-Sensitive Error Correction Based on Google Web 1T 5-Gram Data Set",
        "authors": [
            "Youssef Bassil",
            "Mohammad Alwani"
        ],
        "abstract": "Since the dawn of the computing era, information has been represented digitally so that it can be processed by electronic computers. Paper books and documents were abundant and widely being published at that time; and hence, there was a need to convert them into digital format. OCR, short for Optical Character Recognition was conceived to translate paper-based books into digital e-books. Regrettably, OCR systems are still erroneous and inaccurate as they produce misspellings in the recognized text, especially when the source document is of low printing quality. This paper proposes a post-processing OCR context-sensitive error correction method for detecting and correcting non-word and real-word OCR errors. The cornerstone of this proposed approach is the use of Google Web 1T 5-gram data set as a dictionary of words to spell-check OCR text. The Google data set incorporates a very large vocabulary and word statistics entirely reaped from the Internet, making it a reliable source to perform dictionary-based error correction. The core of the proposed solution is a combination of three algorithms: The error detection, candidate spellings generator, and error correction algorithms, which all exploit information extracted from Google Web 1T 5-gram data set. Experiments conducted on scanned images written in different languages showed a substantial improvement in the OCR error correction rate. As future developments, the proposed algorithm is to be parallelised so as to support parallel and distributed computing architectures.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0191",
        "title": "OCR Post-Processing Error Correction Algorithm using Google Online Spelling Suggestion",
        "authors": [
            "Youssef Bassil",
            "Mohammad Alwani"
        ],
        "abstract": "With the advent of digital optical scanners, a lot of paper-based books, textbooks, magazines, articles, and documents are being transformed into an electronic version that can be manipulated by a computer. For this purpose, OCR, short for Optical Character Recognition was developed to translate scanned graphical text into editable computer text. Unfortunately, OCR is still imperfect as it occasionally mis-recognizes letters and falsely identifies scanned text, leading to misspellings and linguistics errors in the OCR output text. This paper proposes a post-processing context-based error correction algorithm for detecting and correcting OCR non-word and real-word errors. The proposed algorithm is based on Google's online spelling suggestion which harnesses an internal database containing a huge collection of terms and word sequences gathered from all over the web, convenient to suggest possible replacements for words that have been misspelled during the OCR process. Experiments carried out revealed a significant improvement in OCR error correction rate. Future research can improve upon the proposed algorithm so much so that it can be parallelized and executed over multiprocessing platforms.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0245",
        "title": "Roget's Thesaurus and Semantic Similarity",
        "authors": [
            "Mario Jarmasz",
            "Stan Szpakowicz"
        ],
        "abstract": "We have implemented a system that measures semantic similarity using a computerized 1987 Roget's Thesaurus, and evaluated it by performing a few typical tests. We compare the results of these tests with those produced by WordNet-based similarity measures. One of the benchmarks is Miller and Charles' list of 30 noun pairs to which human judges had assigned similarity measures. We correlate these measures with those computed by several NLP systems. The 30 pairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have also studied. Our Roget's-based system gets correlations of .878 for the smaller and .818 for the larger list of noun pairs; this is quite close to the .885 that Resnik obtained when he employed humans to replicate the Miller and Charles experiment. We further evaluate our measure by using Roget's and WordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the correct synonym must be selected amongst a group of four words. Our system gets 78.75%, 82.00% and 74.33% of the questions respectively.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0255",
        "title": "Keyphrase Extraction : Enhancing Lists",
        "authors": [
            "Mario Jarmasz",
            "Caroline Barri\u00e8re"
        ],
        "abstract": "This paper proposes some modest improvements to Extractor, a state-of-the-art keyphrase extraction system, by using a terabyte-sized corpus to estimate the informativeness and semantic similarity of keyphrases. We present two techniques to improve the organization and remove outliers of lists of keyphrases. The first is a simple ordering according to their occurrences in the corpus; the second is clustering according to semantic similarity. Evaluation issues are discussed. We present a novel technique of comparing extracted keyphrases to a gold standard which relies on semantic similarity rather than string matching or an evaluation involving human judges.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0257",
        "title": "Not As Easy As It Seems: Automating the Construction of Lexical Chains Using Roget's Thesaurus",
        "authors": [
            "Mario Jarmasz",
            "Stan Szpakowicz"
        ],
        "abstract": "Morris and Hirst present a method of linking significant words that are about the same topic. The resulting lexical chains are a means of identifying cohesive regions in a text, with applications in many natural language processing tasks, including text summarization. The first lexical chains were constructed manually using Roget's International Thesaurus. Morris and Hirst wrote that automation would be straightforward given an electronic thesaurus. All applications so far have used WordNet to produce lexical chains, perhaps because adequate electronic versions of Roget's were not available until recently. We discuss the building of lexical chains using an electronic version of Roget's Thesaurus. We implement a variant of the original algorithm, and explain the necessary design decisions. We include a comparison with other implementations.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.0258",
        "title": "Roget's Thesaurus: a Lexical Resource to Treasure",
        "authors": [
            "Mario Jarmasz",
            "Stan Szpakowicz"
        ],
        "abstract": "This paper presents the steps involved in creating an electronic lexical knowledge base from the 1987 Penguin edition of Roget's Thesaurus. Semantic relations are labelled with the help of WordNet. The two resources are compared in a qualitative and quantitative manner. Differences in the organization of the lexical material are discussed, as well as the possibility of merging both resources.\n    ",
        "submission_date": "2012-04-01T00:00:00",
        "last_modified_date": "2012-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2765",
        "title": "A practical approach to language complexity: a Wikipedia case study",
        "authors": [
            "Taha Yasseri",
            "Andr\u00e1s Kornai",
            "J\u00e1nos Kert\u00e9sz"
        ],
        "abstract": "In this paper we present statistical analysis of English texts from Wikipedia. We try to address the issue of language complexity empirically by comparing the simple English Wikipedia (Simple) to comparable samples of the main English Wikipedia (Main). Simple is supposed to use a more simplified language with a limited vocabulary, and editors are explicitly requested to follow this guideline, yet in practice the vocabulary richness of both samples are at the same level. Detailed analysis of longer units (n-grams of words and part of speech tags) shows that the language of Simple is less complex than that of Main primarily due to the use of shorter sentences, as opposed to drastically simplified syntax or vocabulary. Comparing the two language varieties by the Gunning readability index supports this conclusion. We also report on the topical dependence of language complexity, e.g. that the language is more advanced in conceptual articles compared to person-based (biographical) and object-based articles. Finally, we investigate the relation between conflict and language complexity by analyzing the content of the talk pages associated to controversial and peacefully developing articles, concluding that controversy has the effect of reducing language complexity.\n    ",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2847",
        "title": "Segmentation Similarity and Agreement",
        "authors": [
            "Chris Fournier",
            "Diana Inkpen"
        ],
        "abstract": "We propose a new segmentation evaluation metric, called segmentation similarity (S), that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmentation. We show that S is configurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance.\n    ",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3800",
        "title": "Indus script corpora, archaeo-metallurgy and Meluhha (Mleccha)",
        "authors": [
            "Srinivasan Kalyanaraman"
        ],
        "abstract": "Jules Bloch's work on formation of the Marathi language has to be expanded further to provide for a study of evolution and formation of Indian languages in the Indian language union (sprachbund). The paper analyses the stages in the evolution of early writing systems which began with the evolution of counting in the ancient Near East. A stage anterior to the stage of syllabic representation of sounds of a language, is identified. Unique geometric shapes required for tokens to categorize objects became too large to handle to abstract hundreds of categories of goods and metallurgical processes during the production of bronze-age goods. About 3500 BCE, Indus script as a writing system was developed to use hieroglyphs to represent the 'spoken words' identifying each of the goods and processes. A rebus method of representing similar sounding words of the lingua franca of the artisans was used in Indus script. This method is recognized and consistently applied for the lingua franca of the Indian sprachbund. That the ancient languages of India, constituted a sprachbund (or language union) is now recognized by many linguists. The sprachbund area is proximate to the area where most of the Indus script inscriptions were discovered, as documented in the corpora. That hundreds of Indian hieroglyphs continued to be used in metallurgy is evidenced by their use on early punch-marked coins. This explains the combined use of syllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs on Rampurva copper bolt, and Sohgaura copper plate from about 6th century ",
        "submission_date": "2012-04-17T00:00:00",
        "last_modified_date": "2012-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5316",
        "title": "ILexicOn: toward an ECD-compliant interlingual lexical ontology described with semantic web formalisms",
        "authors": [
            "Maxime Lefran\u00e7ois",
            "Fabien Gandon"
        ],
        "abstract": "We are interested in bridging the world of natural language and the world of the semantic web in particular to support natural multilingual access to the web of data. In this paper we introduce a new type of lexical ontology called interlingual lexical ontology (ILexicOn), which uses semantic web formalisms to make each interlingual lexical unit class (ILUc) support the projection of its semantic decomposition on itself. After a short overview of existing lexical ontologies, we briefly introduce the semantic web formalisms we use. We then present the three layered architecture of our approach: i) the interlingual lexical meta-ontology (ILexiMOn); ii) the ILexicOn where ILUcs are formally defined; iii) the data layer. We illustrate our approach with a standalone ILexicOn, and introduce and explain a concise human-readable notation to represent ILexicOns. Finally, we show how semantic web formalisms enable the projection of a semantic decomposition on the decomposed ILUc.\n    ",
        "submission_date": "2012-04-24T00:00:00",
        "last_modified_date": "2012-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5369",
        "title": "Ecological Evaluation of Persuasive Messages Using Google AdWords",
        "authors": [
            "Marco Guerini",
            "Carlo Strapparava",
            "Oliviero Stock"
        ],
        "abstract": "In recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for NLP tasks. In particular, evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks. In this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost. The central idea is exploiting existing commercial tools for advertising on the web, such as Google AdWords, to measure message impact in an ecological setting. The paper includes a description of the approach, tips for how to use AdWords for scientific research, and results of pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach.\n    ",
        "submission_date": "2012-04-24T00:00:00",
        "last_modified_date": "2012-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5852",
        "title": "Context-sensitive Spelling Correction Using Google Web 1T 5-Gram Information",
        "authors": [
            "Youssef Bassil",
            "Mohammad Alwani"
        ],
        "abstract": "In computing, spell checking is the process of detecting and sometimes providing spelling suggestions for incorrectly spelled words in a text. Basically, a spell checker is a computer program that uses a dictionary of words to perform spell checking. The bigger the dictionary is, the higher is the error detection rate. The fact that spell checkers are based on regular dictionaries, they suffer from data sparseness problem as they cannot capture large vocabulary of words including proper names, domain-specific terms, technical jargons, special acronyms, and terminologies. As a result, they exhibit low error detection rate and often fail to catch major errors in the text. This paper proposes a new context-sensitive spelling correction method for detecting and correcting non-word and real-word errors in digital text documents. The approach hinges around data statistics from Google Web 1T 5-gram data set which consists of a big volume of n-gram word sequences, extracted from the World Wide Web. Fundamentally, the proposed method comprises an error detector that detects misspellings, a candidate spellings generator based on a character 2-gram model that generates correction suggestions, and an error corrector that performs contextual error correction. Experiments conducted on a set of text documents from different domains and containing misspellings, showed an outstanding spelling error correction rate and a drastic reduction of both non-word and real-word errors. In a further study, the proposed algorithm is to be parallelized so as to lower the computational cost of the error detection and correction processes.\n    ",
        "submission_date": "2012-04-26T00:00:00",
        "last_modified_date": "2012-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6364",
        "title": "A Corpus-based Evaluation of a Domain-specific Text to Knowledge Mapping Prototype",
        "authors": [
            "Rushdi Shams",
            "Adel Elsayed",
            "Quazi Mah-Zereen Akter"
        ],
        "abstract": "The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM) Prototype. The prototype is domain-specific, the purpose of which is to map instructional text onto a knowledge domain. The context of the knowledge domain is DC electrical circuit. During development, the prototype has been tested with a limited data set from the domain. The prototype reached a stage where it needs to be evaluated with a representative linguistic data set called corpus. A corpus is a collection of text drawn from typical sources which can be used as a test data set to evaluate NLP systems. As there is no available corpus for the domain, we developed and annotated a representative corpus. The evaluation of the prototype considers two of its major components- lexical components and knowledge model. Evaluation on lexical components enriches the lexical resources of the prototype like vocabulary and grammar structures. This leads the prototype to parse a reasonable amount of sentences in the corpus. While dealing with the lexicon was straight forward, the identification and extraction of appropriate semantic relations was much more involved. It was necessary, therefore, to manually develop a conceptual structure for the domain to formulate a domain-specific framework of semantic relations. The framework of semantic relationsthat has resulted from this study consisted of 55 relations, out of which 42 have inverse relations. We also conducted rhetorical analysis on the corpus to prove its representativeness in conveying semantic. Finally, we conducted a topical and discourse analysis on the corpus to analyze the coverage of discourse by the prototype.\n    ",
        "submission_date": "2012-04-28T00:00:00",
        "last_modified_date": "2012-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.0627",
        "title": "Rule-weighted and terminal-weighted context-free grammars have identical expressivity",
        "authors": [
            "Yann Ponty"
        ],
        "abstract": "Two formalisms, both based on context-free grammars, have recently been proposed as a basis for a non-uniform random generation of combinatorial objects. The former, introduced by Denise et al, associates weights with letters, while the latter, recently explored by Weinberg et al in the context of random generation, associates weights to transitions. In this short note, we use a simple modification of the Greibach Normal Form transformation algorithm, due to Blum and Koch, to show the equivalent expressivities, in term of their induced distributions, of these two formalisms.\n    ",
        "submission_date": "2012-05-03T00:00:00",
        "last_modified_date": "2012-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1564",
        "title": "Characterizing Ranked Chinese Syllable-to-Character Mapping Spectrum: A Bridge Between the Spoken and Written Chinese Language",
        "authors": [
            "Wentian Li"
        ],
        "abstract": "One important aspect of the relationship between spoken and written Chinese is the ranked syllable-to-character mapping spectrum, which is the ranked list of syllables by the number of characters that map to the syllable. Previously, this spectrum is analyzed for more than 400 syllables without distinguishing the four intonations. In the current study, the spectrum with 1280 toned syllables is analyzed by logarithmic function, Beta rank function, and piecewise logarithmic function. Out of the three fitting functions, the two-piece logarithmic function fits the data the best, both by the smallest sum of squared errors (SSE) and by the lowest Akaike information criterion (AIC) value. The Beta rank function is the close second. By sampling from a Poisson distribution whose parameter value is chosen from the observed data, we empirically estimate the $p$-value for testing the two-piece-logarithmic-function being better than the Beta rank function hypothesis, to be 0.16. For practical purposes, the piecewise logarithmic function and the Beta rank function can be considered a tie.\n    ",
        "submission_date": "2012-05-08T00:00:00",
        "last_modified_date": "2012-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1603",
        "title": "Parsing of Myanmar sentences with function tagging",
        "authors": [
            "Win Win Thant",
            "Tin Myat Htwe",
            "Ni Lar Thein"
        ],
        "abstract": "This paper describes the use of Naive Bayes to address the task of assigning function tags and context free grammar (CFG) to parse Myanmar sentences. Part of the challenge of statistical function tagging for Myanmar sentences comes from the fact that Myanmar has free-phrase-order and a complex morphological system. Function tagging is a pre-processing step for parsing. In the task of function tagging, we use the functional annotated corpus and tag Myanmar sentences with correct segmentation, POS (part-of-speech) tagging and chunking information. We propose Myanmar grammar rules and apply context free grammar (CFG) to find out the parse tree of function tagged Myanmar sentences. Experiments show that our analysis achieves a good result with parsing of simple sentences and three types of complex sentences.\n    ",
        "submission_date": "2012-05-08T00:00:00",
        "last_modified_date": "2012-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1639",
        "title": "Spectral Analysis of Projection Histogram for Enhancing Close matching character Recognition in Malayalam",
        "authors": [
            "Sajilal Divakaran"
        ],
        "abstract": "The success rates of Optical Character Recognition (OCR) systems for printed Malayalam documents is quite impressive with the state of the art accuracy levels in the range of 85-95% for various. However for real applications, further enhancement of this accuracy levels are required. One of the bottle necks in further enhancement of the accuracy is identified as close-matching characters. In this paper, we delineate the close matching characters in Malayalam and report the development of a specialised classifier for these close-matching characters. The output of a state of the art of OCR is taken and characters falling into the close-matching character set is further fed into this specialised classifier for enhancing the accuracy. The classifier is based on support vector machine algorithm and uses feature vectors derived out of spectral coefficients of projection histogram signals of close-matching characters.\n    ",
        "submission_date": "2012-05-08T00:00:00",
        "last_modified_date": "2012-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.2657",
        "title": "Multilingual Topic Models for Unaligned Text",
        "authors": [
            "Jordan Boyd-Graber",
            "David Blei"
        ],
        "abstract": "We develop the multilingual topic model for unaligned text (MuTo), a probabilistic model of text that is designed to analyze corpora composed of documents in two languages. From these documents, MuTo uses stochastic EM to simultaneously discover both a matching between the languages and multilingual latent topics. We demonstrate that MuTo is able to find shared topics on real-world multilingual corpora, successfully pairing related documents across languages. MuTo provides a new framework for creating multilingual topic models without needing carefully curated parallel corpora and allows applications built using the topic model formalism to be applied to a much wider class of corpora.\n    ",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3183",
        "title": "A Model-Driven Probabilistic Parser Generator",
        "authors": [
            "Luis Quesada",
            "Fernando Berzal",
            "Francisco J. Cortijo"
        ],
        "abstract": "Existing probabilistic scanners and parsers impose hard constraints on the way lexical and syntactic ambiguities can be resolved. Furthermore, traditional grammar-based parsing tools are limited in the mechanisms they allow for taking context into account. In this paper, we propose a model-driven tool that allows for statistical language models with arbitrary probability estimators. Our work on model-driven probabilistic parsing is built on top of ModelCC, a model-based parser generator, and enables the probabilistic interpretation and resolution of anaphoric, cataphoric, and recursive references in the disambiguation of abstract syntax graphs. In order to prove the expression power of ModelCC, we describe the design of a general-purpose natural language parser.\n    ",
        "submission_date": "2012-05-14T00:00:00",
        "last_modified_date": "2012-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.3316",
        "title": "Arabic Language Learning Assisted by Computer, based on Automatic Speech Recognition",
        "authors": [
            "Naim Terbeh",
            "Mounir Zrigui"
        ],
        "abstract": "This work consists of creating a system of the Computer Assisted Language Learning (CALL) based on a system of Automatic Speech Recognition (ASR) for the Arabic language using the tool CMU Sphinx3 [1], based on the approach of HMM. To this work, we have constructed a corpus of six hours of speech recordings with a number of nine speakers. we find in the robustness to noise a grounds for the choice of the HMM approach [2]. the results achieved are encouraging since our corpus is made by only nine speakers, but they are always reasons that open the door for other improvement works.\n    ",
        "submission_date": "2012-05-15T00:00:00",
        "last_modified_date": "2012-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.4298",
        "title": "Task-specific Word-Clustering for Part-of-Speech Tagging",
        "authors": [
            "Yoav Goldberg"
        ],
        "abstract": "While the use of cluster features became ubiquitous in core NLP tasks, most cluster features in NLP are based on distributional similarity. We propose a new type of clustering criteria, specific to the task of part-of-speech tagging. Instead of distributional similarity, these clusters are based on the beha vior of a baseline tagger when applied to a large corpus. These cluster features provide similar gains in accuracy to those achieved by distributional-similarity derived clusters. Using both types of cluster features together further improve tagging accuracies. We show that the method is effective for both the in-domain and out-of-domain scenarios for English, and for French, German and Italian. The effect is larger for out-of-domain text.\n    ",
        "submission_date": "2012-05-19T00:00:00",
        "last_modified_date": "2012-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.4387",
        "title": "Precision-biased Parsing and High-Quality Parse Selection",
        "authors": [
            "Yoav Goldberg",
            "Michael Elhadad"
        ],
        "abstract": "We introduce precision-biased parsing: a parsing task which favors precision over recall by allowing the parser to abstain from decisions deemed uncertain. We focus on dependency-parsing and present an ensemble method which is capable of assigning parents to 84% of the text tokens while being over 96% accurate on these tokens. We use the precision-biased parsing task to solve the related high-quality parse-selection task: finding a subset of high-quality (accurate) trees in a large collection of parsed text. We present a method for choosing over a third of the input trees while keeping unlabeled dependency parsing accuracy of 97% on these trees. We also present a method which is not based on an ensemble but rather on directly predicting the risk associated with individual parser decisions. In addition to its efficiency, this method demonstrates that a parsing system can provide reasonable estimates of confidence in its predictions without relying on ensembles or aggregate corpus counts.\n    ",
        "submission_date": "2012-05-20T00:00:00",
        "last_modified_date": "2012-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.5407",
        "title": "FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely Lexical Substitutes Based on an N-gram Language Model",
        "authors": [
            "Deniz Yuret"
        ],
        "abstract": "Lexical substitutes have found use in areas such as paraphrasing, text simplification, machine translation, word sense disambiguation, and part of speech induction. However the computational complexity of accurately identifying the most likely substitutes for a word has made large scale experiments difficult. In this paper I introduce a new search algorithm, FASTSUBS, that is guaranteed to find the K most likely lexical substitutes for a given word in a sentence based on an n-gram language model. The computation is sub-linear in both K and the vocabulary size V. An implementation of the algorithm and a dataset with the top 100 substitutes of each token in the WSJ section of the Penn Treebank are available at ",
        "submission_date": "2012-05-24T00:00:00",
        "last_modified_date": "2012-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.6832",
        "title": "Syst\u00e8me d'aide \u00e0 l'acc\u00e8s lexical : trouver le mot qu'on a sur le bout de la langue",
        "authors": [
            "Ga\u00eblle Lortal",
            "Brigitte Grau",
            "Michael Zock"
        ],
        "abstract": "The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues and insights concerning the organisation of the mental lexicon (meaning, number of syllables, relation with other words, etc.). This paper describes a tool based on psycho-linguistic observations concerning the TOT phenomenon. We've built it to enable a speaker/writer to find the word he is looking for, word he may know, but which he is unable to access in time. We try to simulate the TOT phenomenon by creating a situation where the system knows the target word, yet is unable to access it. In order to find the target word we make use of the paradigmatic and syntagmatic associations stored in the linguistic databases. Our experiment allows the following conclusion: a tool like SVETLAN, capable to structure (automatically) a dictionary by domains can be used sucessfully to help the speaker/writer to find the word he is looking for, if it is combined with a database rich in terms of paradigmatic links like EuroWordNet.\n    ",
        "submission_date": "2012-01-20T00:00:00",
        "last_modified_date": "2012-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0042",
        "title": "Language Acquisition in Computers",
        "authors": [
            "Megan Belzner",
            "Sean Colin-Ellerin",
            "Jorge H. Roman"
        ],
        "abstract": "This project explores the nature of language acquisition in computers, guided by techniques similar to those used in children. While existing natural language processing methods are limited in scope and understanding, our system aims to gain an understanding of language from first principles and hence minimal initial input. The first portion of our system was implemented in Java and is focused on understanding the morphology of language using bigrams. We use frequency distributions and differences between them to define and distinguish languages. English and French texts were analyzed to determine a difference threshold of 55 before the texts are considered to be in different languages, and this threshold was verified using Spanish texts. The second portion of our system focuses on gaining an understanding of the syntax of a language using a recursive method. The program uses one of two possible methods to analyze given sentences based on either sentence patterns or surrounding words. Both methods have been implemented in C++. The program is able to understand the structure of simple sentences and learn new words. In addition, we have provided some suggestions regarding future work and potential extensions of the existing program.\n    ",
        "submission_date": "2012-05-31T00:00:00",
        "last_modified_date": "2012-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0377",
        "title": "Automated Word Puzzle Generation via Topic Dictionaries",
        "authors": [
            "Balazs Pinter",
            "Gyula Voros",
            "Zoltan Szabo",
            "Andras Lorincz"
        ],
        "abstract": "We propose a general method for automated word puzzle generation. Contrary to previous approaches in this novel field, the presented method does not rely on highly structured datasets obtained with serious human annotation effort: it only needs an unstructured and unannotated corpus (i.e., document collection) as input. The method builds upon two additional pillars: (i) a topic model, which induces a topic dictionary from the input corpus (examples include e.g., latent semantic analysis, group-structured dictionaries or latent Dirichlet allocation), and (ii) a semantic similarity measure of word pairs. Our method can (i) generate automatically a large number of proper word puzzles of different types, including the odd one out, choose the related word and separate the topics puzzle. (ii) It can easily create domain-specific puzzles by replacing the corpus component. (iii) It is also capable of automatically generating puzzles with parameterizable levels of difficulty suitable for, e.g., beginners or intermediate learners.\n    ",
        "submission_date": "2012-06-02T00:00:00",
        "last_modified_date": "2012-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.0381",
        "title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser Approach",
        "authors": [
            "Md. Nawab Yousuf Ali",
            "Shamim Ripon",
            "Shaikh Muhammad Allayear"
        ],
        "abstract": "Universal Networking Language (UNL) is a declarative formal language that is used to represent semantic data extracted from natural language texts. This paper presents a novel approach to converting Bangla natural language text into UNL using a method known as Predicate Preserving Parser (PPP) technique. PPP performs morphological, syntactic and semantic, and lexical analysis of text synchronously. This analysis produces a semantic-net like structure represented using UNL. We demonstrate how Bangla texts are analyzed following the PPP technique to produce UNL documents which can then be translated into any other suitable natural language facilitating the opportunity to develop a universal language translation method via UNL.\n    ",
        "submission_date": "2012-06-02T00:00:00",
        "last_modified_date": "2012-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1066",
        "title": "Hedge detection as a lens on framing in the GMO debates: A position paper",
        "authors": [
            "Eunsol Choi",
            "Chenhao Tan",
            "Lillian Lee",
            "Cristian Danescu-Niculescu-Mizil",
            "Jennifer Spindel"
        ],
        "abstract": "Understanding the ways in which participants in public discussions frame their arguments is important in understanding how public opinion is formed. In this paper, we adopt the position that it is time for more computationally-oriented research on problems involving framing. In the interests of furthering that goal, we propose the following specific, interesting and, we believe, relatively accessible question: In the controversy regarding the use of genetically-modified organisms (GMOs) in agriculture, do pro- and anti-GMO articles differ in whether they choose to adopt a \"scientific\" tone?\n",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2012-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.2009",
        "title": "Developing a model for a text database indexed pedagogically for teaching the Arabic language",
        "authors": [
            "Asma Boudhief",
            "Mohsen Maraoui",
            "Mounir Zrigui"
        ],
        "abstract": "In this memory we made the design of an indexing model for Arabic language and adapting standards for describing learning resources used (the LOM and their application profiles) with learning conditions such as levels education of students, their levels of understanding...the pedagogical context with taking into account the repre-sentative elements of the text, text's length,...in particular, we highlight the specificity of the Arabic language which is a complex language, characterized by its flexion, its voyellation and its agglutination.\n    ",
        "submission_date": "2012-06-10T00:00:00",
        "last_modified_date": "2012-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.2010",
        "title": "Temporal expression normalisation in natural language texts",
        "authors": [
            "Michele Filannino"
        ],
        "abstract": "Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. In this report, I describe a novel rule-based architecture, built on top of a pre-existing system, which is able to normalise temporal expressions detected in English texts. Gold standard temporally-annotated resources are limited in size and this makes research difficult. The proposed system outperforms the state-of-the-art systems with respect to TempEval-2 Shared Task (value attribute) and achieves substantially better results with respect to the pre-existing system on top of which it has been developed. I will also introduce a new free corpus consisting of 2822 unique annotated temporal expressions. Both the corpus and the system are freely available on-line.\n    ",
        "submission_date": "2012-06-10T00:00:00",
        "last_modified_date": "2012-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4522",
        "title": "BADREX: In situ expansion and coreference of biomedical abbreviations using dynamic regular expressions",
        "authors": [
            "Phil Gooch"
        ],
        "abstract": "BADREX uses dynamically generated regular expressions to annotate term definition-term abbreviation pairs, and corefers unpaired acronyms and abbreviations back to their initial definition in the text. Against the Medstract corpus BADREX achieves precision and recall of 98% and 97%, and against a much larger corpus, 90% and 85%, respectively. BADREX yields improved performance over previous approaches, requires no training data and allows runtime customisation of its input parameters. BADREX is freely available from ",
        "submission_date": "2012-06-20T00:00:00",
        "last_modified_date": "2012-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5333",
        "title": "TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations",
        "authors": [
            "Naushad UzZaman",
            "Hector Llorens",
            "James Allen",
            "Leon Derczynski",
            "Marc Verhagen",
            "James Pustejovsky"
        ],
        "abstract": "We describe the TempEval-3 task which is currently in preparation for the SemEval-2013 evaluation exercise. The aim of TempEval is to advance research on temporal information processing. TempEval-3 follows on from previous TempEval events, incorporating: a three-part task structure covering event, temporal expression and temporal relation extraction; a larger dataset; and single overall task quality scores.\n    ",
        "submission_date": "2012-06-22T00:00:00",
        "last_modified_date": "2014-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5384",
        "title": "Keyphrase Based Arabic Summarizer (KPAS)",
        "authors": [
            "Tarek El-Shishtawy",
            "Fatma El-Ghannam"
        ],
        "abstract": "This paper describes a computationally inexpensive and efficient generic summarization algorithm for Arabic texts. The algorithm belongs to extractive summarization family, which reduces the problem into representative sentences identification and extraction sub-problems. Important keyphrases of the document to be summarized are identified employing combinations of statistical and linguistic features. The sentence extraction algorithm exploits keyphrases as the primary attributes to rank a sentence. The present experimental work, demonstrates different techniques for achieving various summarization goals including: informative richness, coverage of both main and auxiliary topics, and keeping redundancy to a minimum. A scoring scheme is then adopted that balances between these summarization goals. To evaluate the resulted Arabic summaries with well-established systems, aligned English/Arabic texts are used through the experiments.\n    ",
        "submission_date": "2012-06-23T00:00:00",
        "last_modified_date": "2012-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6403",
        "title": "Two Step CCA: A new spectral method for estimating vector models of words",
        "authors": [
            "Paramveer Dhillon",
            "Jordan Rodu",
            "Dean Foster",
            "Lyle Ungar"
        ],
        "abstract": "Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank \"dictionary\" by an eigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6423",
        "title": "A Joint Model of Language and Perception for Grounded Attribute Learning",
        "authors": [
            "Cynthia Matuszek",
            "Nicholas FitzGerald",
            "Luke Zettlemoyer",
            "Liefeng Bo",
            "Dieter Fox"
        ],
        "abstract": "As robots become more ubiquitous and capable, it becomes ever more important to enable untrained users to easily interact with them. Recently, this has led to study of the language grounding problem, where the goal is to extract representations of the meanings of natural language tied to perception and actuation in the physical world. In this paper, we present an approach for joint learning of language and perception models for grounded attribute induction. Our perception model includes attribute classifiers, for example to detect object color and shape, and the language model is based on a probabilistic categorial grammar that enables the construction of rich, compositional meaning representations. The approach is evaluated on the task of interpreting sentences that describe sets of objects in a physical workspace. We demonstrate accurate task performance and effective latent-variable concept induction in physical grounded scenes.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6426",
        "title": "A Fast and Simple Algorithm for Training Neural Probabilistic Language Models",
        "authors": [
            "Andriy Mnih",
            "Yee Whye Teh"
        ],
        "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.\n",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6481",
        "title": "Cross Language Text Classification via Subspace Co-Regularized Multi-View Learning",
        "authors": [
            "Yuhong Guo",
            "Min Xiao"
        ],
        "abstract": "In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods.\n    ",
        "submission_date": "2012-06-27T00:00:00",
        "last_modified_date": "2012-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.6735",
        "title": "Elimination of Spurious Ambiguity in Transition-Based Dependency Parsing",
        "authors": [
            "Shay B. Cohen",
            "Carlos G\u00f3mez-Rodr\u00edguez",
            "Giorgio Satta"
        ],
        "abstract": "We present a novel technique to remove spurious ambiguity from transition systems for dependency parsing. Our technique chooses a canonical sequence of transition operations (computation) for a given dependency tree. Our technique can be applied to a large class of bottom-up transition systems, including for instance Nivre (2004) and Attardi (2006).\n    ",
        "submission_date": "2012-06-28T00:00:00",
        "last_modified_date": "2012-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0245",
        "title": "Adversarial Evaluation for Models of Natural Language",
        "authors": [
            "Noah A. Smith"
        ],
        "abstract": "We now have a rich and growing set of modeling tools and algorithms for inducing linguistic structure from text that is less than fully annotated. In this paper, we discuss some of the weaknesses of our current methodology. We present a new abstract framework for evaluating natural language processing (NLP) models in general and unsupervised NLP models in particular. The central idea is to make explicit certain adversarial roles among researchers, so that the different roles in an evaluation are more clearly defined and performers of all roles are offered ways to make measurable contributions to the larger goal. Adopting this approach may help to characterize model successes and failures by encouraging earlier consideration of error analysis. The framework can be instantiated in a variety of ways, simulating some familiar intrinsic and extrinsic evaluations as well as some new evaluations.\n    ",
        "submission_date": "2012-07-01T00:00:00",
        "last_modified_date": "2012-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0396",
        "title": "Applying Deep Belief Networks to Word Sense Disambiguation",
        "authors": [
            "Peratham Wiriyathammabhum",
            "Boonserm Kijsirikul",
            "Hiroya Takamura",
            "Manabu Okumura"
        ],
        "abstract": "In this paper, we applied a novel learning algorithm, namely, Deep Belief Networks (DBN) to word sense disambiguation (WSD). DBN is a probabilistic generative model composed of multiple layers of hidden units. DBN uses Restricted Boltzmann Machine (RBM) to greedily train layer by layer as a pretraining. Then, a separate fine tuning step is employed to improve the discriminative power. We compared DBN with various state-of-the-art supervised learning algorithms in WSD such as Support Vector Machine (SVM), Maximum Entropy model (MaxEnt), Naive Bayes classifier (NB) and Kernel Principal Component Analysis (KPCA). We used all words in the given paragraph, surrounding context words and part-of-speech of surrounding words as our knowledge sources. We conducted our experiment on the SENSEVAL-2 data set. We observed that DBN outperformed all other learning algorithms.\n    ",
        "submission_date": "2012-07-02T00:00:00",
        "last_modified_date": "2012-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1420",
        "title": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars",
        "authors": [
            "Luke S. Zettlemoyer",
            "Michael Collins"
        ],
        "abstract": "This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.\n    ",
        "submission_date": "2012-07-04T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.1847",
        "title": "Finding Structure in Text, Genome and Other Symbolic Sequences",
        "authors": [
            "Ted Dunning"
        ],
        "abstract": "The statistical methods derived and described in this thesis provide new ways to elucidate the structural properties of text and other symbolic sequences. Generically, these methods allow detection of a difference in the frequency of a single feature, the detection of a difference between the frequencies of an ensemble of features and the attribution of the source of a text. These three abstract tasks suffice to solve problems in a wide variety of settings. Furthermore, the techniques described in this thesis can be extended to provide a wide range of additional tests beyond the ones described here.\n",
        "submission_date": "2012-07-08T00:00:00",
        "last_modified_date": "2012-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2265",
        "title": "Challenges for Distributional Compositional Semantics",
        "authors": [
            "Daoud Clarke"
        ],
        "abstract": "This paper summarises the current state-of-the art in the study of compositionality in distributional semantics, and major challenges for this area. We single out generalised quantifiers and intensional semantics as areas on which to focus attention for the development of the theory. Once suitable theories have been developed, algorithms will be needed to apply the theory to tasks. Evaluation is a major problem; we single out application to recognising textual entailment and machine translation for this purpose.\n    ",
        "submission_date": "2012-07-10T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2334",
        "title": "Distinct word length frequencies: distributions and symbol entropies",
        "authors": [
            "Reginald D. Smith"
        ],
        "abstract": "The distribution of frequency counts of distinct words by length in a language's vocabulary will be analyzed using two methods. The first, will look at the empirical distributions of several languages and derive a distribution that reasonably explains the number of distinct words as a function of length. We will be able to derive the frequency count, mean word length, and variance of word length based on the marginal probability of letters and spaces. The second, based on information theory, will demonstrate that the conditional entropies can also be used to estimate the frequency of distinct words of a given length in a language. In addition, it will be shown how these techniques can also be applied to estimate higher order entropies using vocabulary word length.\n    ",
        "submission_date": "2012-07-10T00:00:00",
        "last_modified_date": "2012-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.2714",
        "title": "Clustering based approach extracting collocations",
        "authors": [
            "Mohamed Achraf Ben Mohamed",
            "Mounir Zrigui",
            "Mohsen Maraoui"
        ],
        "abstract": "The following study presents a collocation extraction approach based on clustering technique. This study uses a combination of several classical measures which cover all aspects of a given corpus then it suggests separating bigrams found in the corpus in several disjoint groups according to the probability of presence of collocations. This will allow excluding groups where the presence of collocations is very unlikely and thus reducing in a meaningful way the search space.\n    ",
        "submission_date": "2012-07-11T00:00:00",
        "last_modified_date": "2012-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3932",
        "title": "Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units",
        "authors": [
            "Kishorjit Nongmeikapam",
            "Vidya Raj RK",
            "Oinam Imocha Singh",
            "Sivaji Bandyopadhyay"
        ],
        "abstract": "The work of automatic segmentation of a Manipuri language (or Meiteilon) word into syllabic units is demonstrated in this paper. This language is a scheduled Indian language of Tibeto-Burman origin, which is also a very highly agglutinative language. This language usages two script: a Bengali script and Meitei Mayek (Script). The present work is based on the second script. An algorithm is designed so as to identify mainly the syllables of Manipuri origin word. The result of the algorithm shows a Recall of 74.77, Precision of 91.21 and F-Score of 82.18 which is a reasonable score with the first attempt of such kind for this language.\n    ",
        "submission_date": "2012-07-17T00:00:00",
        "last_modified_date": "2012-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4307",
        "title": "Frame Interpretation and Validation in a Open Domain Dialogue System",
        "authors": [
            "Artur Ventura",
            "Nuno Diegues",
            "David Martins de Matos"
        ],
        "abstract": "Our goal in this paper is to establish a means for a dialogue platform to be able to cope with open domains considering the possible interaction between the embodied agent and humans. To this end we present an algorithm capable of processing natural language utterances and validate them against knowledge structures of an intelligent agent's mind. Our algorithm leverages dialogue techniques in order to solve ambiguities and acquire knowledge about unknown entities.\n    ",
        "submission_date": "2012-07-18T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.4625",
        "title": "Appropriate Nouns with Obligatory Modifiers",
        "authors": [
            "E. Laporte"
        ],
        "abstract": "The notion of appropriate sequence as introduced by Z. Harris provides a powerful syntactic way of analysing the detailed meaning of various sentences, including ambiguous ones. In an adjectival sentence like 'The leather was yellow', the introduction of an appropriate noun, here 'colour', specifies which quality the adjective describes. In some other adjectival sentences with an appropriate noun, that noun plays the same part as 'colour' and seems to be relevant to the description of the adjective. These appropriate nouns can usually be used in elementary sentences like 'The leather had some colour', but in many cases they have a more or less obligatory modifier. For example, you can hardly mention that an object has a colour without qualifying that colour at all. About 300 French nouns are appropriate in at least one adjectival sentence and have an obligatory modifier. They enter in a number of sentence structures related by several syntactic transformations. The appropriateness of the noun and the fact that the modifier is obligatory are reflected in these transformations. The description of these syntactic phenomena provides a basis for a classification of these nouns. It also concerns the lexical properties of thousands of predicative adjectives, and in particular the relations between the sentence without the noun : 'The leather was yellow' and the adjectival sentence with the noun : 'The colour of the leather was yellow'.\n    ",
        "submission_date": "2012-07-19T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5328",
        "title": "A prototype for projecting HPSG syntactic lexica towards LMF",
        "authors": [
            "Kais Haddar",
            "H\u00e9la Fehri",
            "Laurent Romary"
        ],
        "abstract": "The comparative evaluation of Arabic HPSG grammar lexica requires a deep study of their linguistic coverage. The complexity of this task results mainly from the heterogeneity of the descriptive components within those lexica (underlying linguistic resources and different data categories, for example). It is therefore essential to define more homogeneous representations, which in turn will enable us to compare them and eventually merge them. In this context, we present a method for comparing HPSG lexica based on a rule system. This method is implemented within a prototype for the projection from Arabic HPSG to a normalised pivot language compliant with LMF (ISO 24613 - Lexical Markup Framework) and serialised using a TEI (Text Encoding Initiative) based representation. The design of this system is based on an initial study of the HPSG formalism looking at its adequacy for the representation of Arabic, and from this, we identify the appropriate feature structures corresponding to each Arabic lexical category and their possible LMF counterparts.\n    ",
        "submission_date": "2012-07-23T00:00:00",
        "last_modified_date": "2012-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.5409",
        "title": "FST Based Morphological Analyzer for Hindi Language",
        "authors": [
            "Deepak Kumar",
            "Manjeet Singh",
            "Seema Shukla"
        ],
        "abstract": "Hindi being a highly inflectional language, FST (Finite State Transducer) based approach is most efficient for developing a morphological analyzer for this language. The work presented in this paper uses the SFST (Stuttgart Finite State Transducer) tool for generating the FST. A lexicon of root words is created. Rules are then added for generating inflectional and derivational words from these root words. The Morph Analyzer developed was used in a Part Of Speech (POS) Tagger based on Stanford POS Tagger. The system was first trained using a manually tagged corpus and MAXENT (Maximum Entropy) approach of Stanford POS tagger was then used for tagging input sentences. The morphological analyzer gives approximately 97% correct results. POS tagger gives an accuracy of approximately 87% for the sentences that have the words known to the trained model file, and 80% accuracy for the sentences that have the words unknown to the trained model file.\n    ",
        "submission_date": "2012-07-23T00:00:00",
        "last_modified_date": "2012-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.0200",
        "title": "Adaptation of pedagogical resources description standard (LOM) with the specificity of Arabic language",
        "authors": [
            "Asma Boudhief",
            "Mohsen Maraoui",
            "Mounir Zrigui"
        ],
        "abstract": "In this article we focus firstly on the principle of pedagogical indexing and characteristics of Arabic language and secondly on the possibility of adapting the standard for describing learning resources used (the LOM and its Application Profiles) with learning conditions such as the educational levels of students and their levels of understanding,... the educational context with taking into account the representative elements of text, text length, ... in particular, we put in relief the specificity of the Arabic language which is a complex language, characterized by its flexion, its voyellation and agglutination.\n    ",
        "submission_date": "2012-08-01T00:00:00",
        "last_modified_date": "2012-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.2777",
        "title": "A Method for Selecting Noun Sense using Co-occurrence Relation in English-Korean Translation",
        "authors": [
            "Hyonil Kim",
            "Changil Choe"
        ],
        "abstract": "The sense analysis is still critical problem in machine translation system, especially such as English-Korean translation which the syntactical different between source and target languages is very great. We suggest a method for selecting the noun sense using contextual feature in English-Korean Translation.\n    ",
        "submission_date": "2012-08-14T00:00:00",
        "last_modified_date": "2012-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3001",
        "title": "More than Word Frequencies: Authorship Attribution via Natural Frequency Zoned Word Distribution Analysis",
        "authors": [
            "Zhili Chen",
            "Liusheng Huang",
            "Wei Yang",
            "Peng Meng",
            "Haibo Miao"
        ],
        "abstract": "With such increasing popularity and availability of digital text data, authorships of digital texts can not be taken for granted due to the ease of copying and parsing. This paper presents a new text style analysis called natural frequency zoned word distribution analysis (NFZ-WDA), and then a basic authorship attribution scheme and an open authorship attribution scheme for digital texts based on the analysis. NFZ-WDA is based on the observation that all authors leave distinct intrinsic word usage traces on texts written by them and these intrinsic styles can be identified and employed to analyze the authorship. The intrinsic word usage styles can be estimated through the analysis of word distribution within a text, which is more than normal word frequency analysis and can be expressed as: which groups of words are used in the text; how frequently does each group of words occur; how are the occurrences of each group of words distributed in the text. Next, the basic authorship attribution scheme and the open authorship attribution scheme provide solutions for both closed and open authorship attribution problems. Through analysis and extensive experimental studies, this paper demonstrates the efficiency of the proposed method for authorship attribution.\n    ",
        "submission_date": "2012-08-15T00:00:00",
        "last_modified_date": "2012-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.4079",
        "title": "Recent Technological Advances in Natural Language Processing and Artificial Intelligence",
        "authors": [
            "Nishal Pradeepkumar Shah"
        ],
        "abstract": "A recent advance in computer technology has permitted scientists to implement and test algorithms that were known from quite some time (or not) but which were computationally expensive. Two such projects are IBM's Jeopardy as a part of its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methods implement natural language processing (another goal of AI scientists) and try to answer questions as asked by the user. Though the goal of the two projects is similar, both of them have a different procedure at it's core. In the following sections, the mechanism and history of IBM's Jeopardy and Wolfram alpha has been explained followed by the implications of these projects in realizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipe of taking the above projects to a new level is also explained.\n    ",
        "submission_date": "2012-08-20T00:00:00",
        "last_modified_date": "2012-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.4503",
        "title": "Introduction of the weight edition errors in the Levenshtein distance",
        "authors": [
            "Gueddah Hicham"
        ],
        "abstract": "In this paper, we present a new approach dedicated to correcting the spelling errors of the Arabic language. This approach corrects typographical errors like inserting, deleting, and permutation. Our method is inspired from the Levenshtein algorithm, and allows a finer and better scheduling than Levenshtein. The results obtained are very satisfactory and encouraging, which shows the interest of our new approach.\n    ",
        "submission_date": "2012-08-22T00:00:00",
        "last_modified_date": "2012-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.6109",
        "title": "Average word length dynamics as indicator of cultural changes in society",
        "authors": [
            "Vladimir V. Bochkarev",
            "Anna V. Shevlyakova",
            "Valery D. Solovyev"
        ],
        "abstract": "Dynamics of average length of words in Russian and English is analysed in the article. Words belonging to the diachronic text corpus Google Books Ngram and dated back to the last two centuries are studied. It was found out that average word length slightly increased in the 19th century, and then it was growing rapidly most of the 20th century and started decreasing over the period from the end of the 20th - to the beginning of the 21th century. Words which contributed mostly to increase or decrease of word average length were identified. At that, content words and functional words are analysed separately. Long content words contribute mostly to word average length of word. As it was shown, these words reflect the main tendencies of social development and thus, are used frequently. Change of frequency of personal pronouns also contributes significantly to change of average word length. The other parameters connected with average length of word were also analysed.\n    ",
        "submission_date": "2012-08-30T00:00:00",
        "last_modified_date": "2012-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.6268",
        "title": "Authorship Identification in Bengali Literature: a Comparative Analysis",
        "authors": [
            "Tanmoy Chakraborty"
        ],
        "abstract": "Stylometry is the study of the unique linguistic styles and writing behaviors of individuals. It belongs to the core task of text categorization like authorship identification, plagiarism detection etc. Though reasonable number of studies have been conducted in English language, no major work has been done so far in Bengali. In this work, We will present a demonstration of authorship identification of the documents written in Bengali. We adopt a set of fine-grained stylistic features for the analysis of the text and use them to develop two different models: statistical similarity model consisting of three measures and their combination, and machine learning model with Decision Tree, Neural Network and SVM. Experimental results show that SVM outperforms other state-of-the-art methods after 10-fold cross validations. We also validate the relative importance of each stylistic feature to show that some of them remain consistently significant in every model used in this experiment.\n    ",
        "submission_date": "2012-08-30T00:00:00",
        "last_modified_date": "2013-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.0249",
        "title": "Robopinion: Opinion Mining Framework Inspired by Autonomous Robot Navigation",
        "authors": [
            "M. A. El-Dosuky",
            "M. Z. Rashad",
            "T. T. Hamza",
            "A. H. EL-Bassiouny"
        ],
        "abstract": "Data association methods are used by autonomous robots to find matches between the current landmarks and the new set of observed features. We seek a framework for opinion mining to benefit from advancements in autonomous robot navigation in both research and development\n    ",
        "submission_date": "2012-09-03T00:00:00",
        "last_modified_date": "2012-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.1300",
        "title": "Input Scheme for Hindi Using Phonetic Mapping",
        "authors": [
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Written Communication on Computers requires knowledge of writing text for the desired language using Computer. Mostly people do not use any other language besides English. This creates a barrier. To resolve this issue we have developed a scheme to input text in Hindi using phonetic mapping scheme. Using this scheme we generate intermediate code strings and match them with pronunciations of input text. Our system show significant success over other input systems available.\n    ",
        "submission_date": "2012-08-19T00:00:00",
        "last_modified_date": "2012-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.1301",
        "title": "Evaluation of Computational Grammar Formalisms for Indian Languages",
        "authors": [
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Natural Language Parsing has been the most prominent research area since the genesis of Natural Language Processing. Probabilistic Parsers are being developed to make the process of parser development much easier, accurate and fast. In Indian context, identification of which Computational Grammar Formalism is to be used is still a question which needs to be answered. In this paper we focus on this problem and try to analyze different formalisms for Indian languages.\n    ",
        "submission_date": "2012-08-19T00:00:00",
        "last_modified_date": "2012-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2400",
        "title": "Identification of Fertile Translations in Medical Comparable Corpora: a Morpho-Compositional Approach",
        "authors": [
            "Estelle Delpech",
            "B\u00e9atrice Daille",
            "Emmanuel Morin",
            "Claire Lemaire"
        ],
        "abstract": "This paper defines a method for lexicon in the biomedical domain from comparable corpora. The method is based on compositional translation and exploits morpheme-level translation equivalences. It can generate translations for a large variety of morphologically constructed words and can also generate 'fertile' translations. We show that fertile translations increase the overall quality of the extracted lexicon for English to French translation.\n    ",
        "submission_date": "2012-09-11T00:00:00",
        "last_modified_date": "2012-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4471",
        "title": "Stemmer for Serbian language",
        "authors": [
            "Nikola Milo\u0161evi\u0107"
        ],
        "abstract": "In linguistic morphology and information retrieval, stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form; generally a written word form. In this work is presented suffix stripping stemmer for Serbian language, one of the highly inflectional languages.\n    ",
        "submission_date": "2012-09-20T00:00:00",
        "last_modified_date": "2012-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.6238",
        "title": "Natural Language Processing - A Survey",
        "authors": [
            "Kevin Mote"
        ],
        "abstract": "The utility and power of Natural Language Processing (NLP) seems destined to change our technological society in profound and fundamental ways. However there are, to date, few accessible descriptions of the science of NLP that have been written for a popular audience, or even for an audience of intelligent, but uninitiated scientists. This paper aims to provide just such an overview. In short, the objective of this article is to describe the purpose, procedures and practical applications of NLP in a clear, balanced, and readable way. We will examine the most recent literature describing the methods and processes of NLP, analyze some of the challenges that researchers are faced with, and briefly survey some of the current and future applications of this science to IT research in general.\n    ",
        "submission_date": "2012-09-25T00:00:00",
        "last_modified_date": "2012-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0252",
        "title": "A Linguistic Model for Terminology Extraction based Conditional Random Fields",
        "authors": [
            "Fethi Fkih",
            "Mohamed Nazih Omri",
            "Imen Toumia"
        ],
        "abstract": "In this paper, we show the possibility of using a linear Conditional Random Fields (CRF) for terminology extraction from a specialized text corpus.\n    ",
        "submission_date": "2012-09-30T00:00:00",
        "last_modified_date": "2014-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0852",
        "title": "Detecting multiword phrases in mathematical text corpora",
        "authors": [
            "Winfried G\u00f6dert"
        ],
        "abstract": "We present an approach for detecting multiword phrases in mathematical text corpora. The method used is based on characteristic features of mathematical terminology. It makes use of a software tool named Lingo which allows to identify words by means of previously defined dictionaries for specific word classes as adjectives, personal names or nouns. The detection of multiword groups is done algorithmically. Possible advantages of the method for indexing and information retrieval and conclusions for applying dictionary-based methods of automatic indexing instead of stemming procedures are discussed.\n    ",
        "submission_date": "2012-10-02T00:00:00",
        "last_modified_date": "2012-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3634",
        "title": "Quick Summary",
        "authors": [
            "Robert Wahlstedt"
        ],
        "abstract": "Quick Summary is an innovate implementation of an automatic document summarizer that inputs a document in the English language and evaluates each sentence. The scanner or evaluator determines criteria based on its grammatical structure and place in the paragraph. The program then asks the user to specify the number of sentences the person wishes to highlight. For example should the user ask to have three of the most important sentences, it would highlight the first and most important sentence in green. Commonly this is the sentence containing the conclusion. Then Quick Summary finds the second most important sentence usually called a satellite and highlights it in yellow. This is usually the topic sentence. Then the program finds the third most important sentence and highlights it in red. The implementations of this technology are useful in a society of information overload when a person typically receives 42 emails a day (Microsoft). The paper also is a candid look at difficulty that machine learning has in textural translating. However, it speaks on how to overcome the obstacles that historically prevented progress. This paper proposes mathematical meta-data criteria that justify the place of importance of a sentence. Just as tools for the study of relational symmetry in bio-informatics, this tool seeks to classify words with greater clarity. \"Survey Finds Workers Average Only Three Productive Days per Week.\" Microsoft News Center. Microsoft. Web. 31 Mar. 2012.\n    ",
        "submission_date": "2012-10-12T00:00:00",
        "last_modified_date": "2012-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3729",
        "title": "Inference of Fine-grained Attributes of Bengali Corpus for Stylometry Detection",
        "authors": [
            "Tanmoy Chakraborty",
            "Sivaji Bandyopadhyay"
        ],
        "abstract": "Stylometry, the science of inferring characteristics of the author from the characteristics of documents written by that author, is a problem with a long history and belongs to the core task of Text categorization that involves authorship identification, plagiarism detection, forensic investigation, computer security, copyright and estate disputes etc. In this work, we present a strategy for stylometry detection of documents written in Bengali. We adopt a set of fine-grained attribute features with a set of lexical markers for the analysis of the text and use three semi-supervised measures for making decisions. Finally, a majority voting approach has been taken for final classification. The system is fully automatic and language-independent. Evaluation results of our attempt for Bengali author's stylometry detection show reasonably promising accuracy in comparison to the baseline model.\n    ",
        "submission_date": "2012-10-13T00:00:00",
        "last_modified_date": "2012-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3865",
        "title": "Opinion Mining for Relating Subjective Expressions and Annual Earnings in US Financial Statements",
        "authors": [
            "Chien-Liang Chen",
            "Chao-Lin Liu",
            "Yuan-Chen Chang",
            "Hsiang-Ping Tsai"
        ],
        "abstract": "Financial statements contain quantitative information and manager's subjective evaluation of firm's financial status. Using information released in U.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for quality financial decisions. To extract such opinioned statements from the reports, we built tagging models based on the conditional random field (CRF) techniques, considering a variety of combinations of linguistic factors including morphology, orthography, predicate-argument structure, syntax, and simple semantics. Our results show that the CRF models are reasonably effective to find opinion holders in experiments when we adopted the popular MPQA corpus for training and testing. The contribution of our paper is to identify opinion patterns in multiword expressions (MWEs) forms rather than in single word forms.\n",
        "submission_date": "2012-10-15T00:00:00",
        "last_modified_date": "2012-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3926",
        "title": "Learning Attitudes and Attributes from Multi-Aspect Reviews",
        "authors": [
            "Julian McAuley",
            "Jure Leskovec",
            "Dan Jurafsky"
        ],
        "abstract": "The majority of online reviews consist of plain-text feedback together with a single numeric score. However, there are multiple dimensions to products and opinions, and understanding the `aspects' that contribute to users' ratings may help us to better understand their individual preferences. For example, a user's impression of an audiobook presumably depends on aspects such as the story and the narrator, and knowing their opinions on these aspects may help us to recommend better products. In this paper, we build models for rating systems in which such dimensions are explicit, in the sense that users leave separate ratings for each aspect of a product. By introducing new corpora consisting of five million reviews, rated with between three and six aspects, we evaluate our models on three prediction tasks: First, we use our model to uncover which parts of a review discuss which of the rated aspects. Second, we use our model to summarize reviews, which for us means finding the sentences that best explain a user's rating. Finally, since aspect ratings are optional in many of the datasets we consider, we use our model to recover those ratings that are missing from a user's evaluation. Our model matches state-of-the-art approaches on existing small-scale datasets, while scaling to the real-world datasets we introduce. Moreover, our model is able to `disentangle' content and sentiment words: we automatically learn content words that are indicative of a particular aspect as well as the aspect-specific sentiment words that are indicative of a particular rating.\n    ",
        "submission_date": "2012-10-15T00:00:00",
        "last_modified_date": "2012-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4567",
        "title": "Gender identity and lexical variation in social media",
        "authors": [
            "David Bamman",
            "Jacob Eisenstein",
            "Tyler Schnoebelen"
        ],
        "abstract": "We present a study of the relationship between gender, linguistic style, and social networks, using a novel corpus of 14,000 Twitter users. Prior quantitative work on gender often treats this social variable as a female/male binary; we argue for a more nuanced approach. By clustering Twitter users, we find a natural decomposition of the dataset into various styles and topical interests. Many clusters have strong gender orientations, but their use of linguistic resources sometimes directly conflicts with the population-level language statistics. We view these clusters as a more accurate reflection of the multifaceted nature of gendered language styles. Previous corpus-based work has also had little to say about individuals whose linguistic styles defy population-level gender patterns. To identify such individuals, we train a statistical classifier, and measure the classifier confidence for each individual in the dataset. Examining individuals whose language does not match the classifier's model for their gender, we find that they have social networks that include significantly fewer same-gender social connections and that, in general, social network homophily is correlated with the use of same-gender language markers. Pairing computational methods and social theory thus offers a new perspective on how gender emerges as individuals position themselves relative to audiences, topics, and mainstream gender norms.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4854",
        "title": "Semantic Understanding of Professional Soccer Commentaries",
        "authors": [
            "Hannaneh Hajishirzi",
            "Mohammad Rastegari",
            "Ali Farhadi",
            "Jessica K. Hodgins"
        ],
        "abstract": "This paper presents a novel approach to the problem of semantic parsing via learning the correspondences between complex sentences and rich sets of events. Our main intuition is that correct correspondences tend to occur more frequently. Our model benefits from a discriminative notion of similarity to learn the correspondence between sentence and an event and a ranking machinery that scores the popularity of each correspondence. Our method can discover a group of events (called macro-events) that best describes a sentence. We evaluate our method on our novel dataset of professional soccer commentaries. The empirical results show that our method significantly outperforms the state-of-theart.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5268",
        "title": "Diffusion of Lexical Change in Social Media",
        "authors": [
            "Jacob Eisenstein",
            "Brendan O'Connor",
            "Noah A. Smith",
            "Eric P. Xing"
        ],
        "abstract": "Computer-mediated communication is driving fundamental changes in the nature of written language. We investigate these changes by statistical analysis of a dataset comprising 107 million Twitter messages (authored by 2.7 million unique user accounts). Using a latent vector autoregressive model to aggregate across thousands of words, we identify high-level patterns in diffusion of linguistic change over the United States. Our model is robust to unpredictable changes in Twitter's sampling rate, and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors. The results of this analysis offer support for prior arguments that focus on geographical proximity and population size. However, demographic similarity -- especially with regard to race -- plays an even more central role, as cities with similar racial demographics are far more likely to share linguistic influence. Rather than moving towards a single unified \"netspeak\" dialect, language evolution in computer-mediated communication reproduces existing fault lines in spoken American English.\n    ",
        "submission_date": "2012-10-18T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5321",
        "title": "The origin of Mayan languages from Formosan language group of Austronesian",
        "authors": [
            "Koji Ohnishi"
        ],
        "abstract": "Basic body-part names (BBPNs) were defined as body-part names in Swadesh basic 200 words. Non-Mayan cognates of Mayan (MY) BBPNs were extensively searched for, by comparing with non-MY vocabulary, including ca.1300 basic words of 82 AN languages listed by Tryon (1985), etc. Thus found cognates (CGs) in non-MY are listed in Table 1, as classified by language groups to which most similar cognates (MSCs) of MY BBPNs belong. CGs of MY are classified to 23 mutually unrelated CG-items, of which 17.5 CG-items have their MSCs in Austronesian (AN), giving its closest similarity score (CSS), CSS(AN) = 17.5, which consists of 10.33 MSCs in Formosan, 1.83 MSCs in Western Malayo-Polynesian (",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5486",
        "title": "A Lightweight Stemmer for Gujarati",
        "authors": [
            "Juhi Ameta",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Gujarati is a resource poor language with almost no language processing tools being available. In this paper we have shown an implementation of a rule based stemmer of Gujarati. We have shown the creation of rules for stemming and the richness in morphology that Gujarati possesses. We have also evaluated our results by verifying it with a human expert.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5517",
        "title": "Design of English-Hindi Translation Memory for Efficient Translation",
        "authors": [
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Developing parallel corpora is an important and a difficult activity for Machine Translation. This requires manual annotation by Human Translators. Translating same text again is a useless activity. There are tools available to implement this for European Languages, but no such tool is available for Indian Languages. In this paper we present a tool for Indian Languages which not only provides automatic translations of the previously available translation but also provides multiple translations, in cases where a sentence has multiple translations, in ranked list of suggestive translations for a sentence. Moreover this tool also lets translators have global and local saving options of their work, so that they may share it with others, which further lightens the task.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5581",
        "title": "Hidden Trends in 90 Years of Harvard Business Review",
        "authors": [
            "Chia-Chi Tsai",
            "Chao-Lin Liu",
            "Wei-Jie Huang",
            "Man-Kwan Shan"
        ],
        "abstract": "In this paper, we demonstrate and discuss results of our mining the abstracts of the publications in Harvard Business Review between 1922 and 2012. Techniques for computing n-grams, collocations, basic sentiment analysis, and named-entity recognition were employed to uncover trends hidden in the abstracts. We present findings about international relationships, sentiment in HBR's abstracts, important international companies, influential technological inventions, renown researchers in management theories, US presidents via chronological analyses.\n    ",
        "submission_date": "2012-10-20T00:00:00",
        "last_modified_date": "2012-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5751",
        "title": "Extraction of domain-specific bilingual lexicon from comparable corpora: compositional translation and ranking",
        "authors": [
            "Estelle Delpech",
            "B\u00e9atrice Daille",
            "Emmanuel Morin",
            "Claire Lemaire"
        ],
        "abstract": "This paper proposes a method for extracting translations of morphologically constructed terms from comparable corpora. The method is based on compositional translation and exploits translation equivalences at the morpheme-level, which allows for the generation of \"fertile\" translations (translation pairs in which the target term has more words than the source term). Ranking methods relying on corpus-based and translation-based features are used to select the best candidate translation. We obtain an average precision of 91% on the Top1 candidate translation. The method was tested on two language pairs (English-French and English-German) and with a small specialized comparable corpora (400k words per language).\n    ",
        "submission_date": "2012-10-21T00:00:00",
        "last_modified_date": "2012-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5898",
        "title": "Some Chances and Challenges in Applying Language Technologies to Historical Studies in Chinese",
        "authors": [
            "Chao-Lin Liu",
            "Guantao Jin",
            "Qingfeng Liu",
            "Wei-Yun Chiu",
            "Yih-Soong Yu"
        ],
        "abstract": "We report applications of language technology to analyzing historical documents in the Database for the Study of Modern Chinese Thoughts and Literature (DSMCTL). We studied two historical issues with the reported techniques: the conceptualization of \"huaren\" (Chinese people) and the attempt to institute constitutional monarchy in the late Qing dynasty. We also discuss research challenges for supporting sophisticated issues using our experience with DSMCTL, the Database of Government Officials of the Republic of China, and the Dream of the Red Chamber. Advanced techniques and tools for lexical, syntactic, semantic, and pragmatic processing of language information, along with more thorough data collection, are needed to strengthen the collaboration between historians and computer scientists.\n    ",
        "submission_date": "2012-10-22T00:00:00",
        "last_modified_date": "2012-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.5965",
        "title": "Classification Analysis Of Authorship Fiction Texts in The Space Of Semantic Fields",
        "authors": [
            "Bohdan Pavlyshenko"
        ],
        "abstract": "The use of naive Bayesian classifier (NB) and the classifier by the k nearest neighbors (kNN) in classification semantic analysis of authors' texts of English fiction has been analysed. The authors' works are considered in the vector space the basis of which is formed by the frequency characteristics of semantic fields of nouns and verbs. Highly precise classification of authors' texts in the vector space of semantic fields indicates about the presence of particular spheres of author's idiolect in this space which characterizes the individual author's style.\n    ",
        "submission_date": "2012-10-22T00:00:00",
        "last_modified_date": "2012-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7282",
        "title": "The Hangulphabet: A Descriptive Alphabet",
        "authors": [
            "Robert Bishop",
            "Ruggero Micheletto"
        ],
        "abstract": "This paper describes the Hangulphabet, a new writing system that should prove useful in a number of contexts. Using the Hangulphabet, a user can instantly see voicing, manner and place of articulation of any phoneme found in human language. The Hangulphabet places consonant graphemes on a grid with the x-axis representing the place of articulation and the y-axis representing manner of articulation. Each individual grapheme contains radicals from both axes where the points intersect. The top radical represents manner of articulation where the bottom represents place of articulation. A horizontal line running through the middle of the bottom radical represents voicing. For vowels, place of articulation is located on a grid that represents the position of the tongue in the mouth. This grid is similar to that of the IPA vowel chart (International Phonetic Association, 1999). The difference with the Hangulphabet being the trapezoid representing the vocal apparatus is on a slight tilt. Place of articulation for a vowel is represented by a breakout figure from the grid. This system can be used as an alternative to the International Phonetic Alphabet (IPA) or as a complement to it. Beginning students of linguistics may find it particularly useful. A Hangulphabet font has been created to facilitate switching between the Hangulphabet and the IPA.\n    ",
        "submission_date": "2012-10-27T00:00:00",
        "last_modified_date": "2012-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7917",
        "title": "The Model of Semantic Concepts Lattice For Data Mining Of Microblogs",
        "authors": [
            "Bohdan Pavlyshenko"
        ],
        "abstract": "The model of semantic concept lattice for data mining of microblogs has been proposed in this work. It is shown that the use of this model is effective for the semantic relations analysis and for the detection of associative rules of key words.\n    ",
        "submission_date": "2012-10-30T00:00:00",
        "last_modified_date": "2012-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8436",
        "title": "Optimal size, freshness and time-frame for voice search vocabulary",
        "authors": [
            "Maryam Kamvar",
            "Ciprian Chelba"
        ],
        "abstract": "In this paper, we investigate how to optimize the vocabulary for a voice search language model. The metric we optimize over is the out-of-vocabulary (OoV) rate since it is a strong indicator of user experience. In a departure from the usual way of measuring OoV rates, web search logs allow us to compute the per-session OoV rate and thus estimate the percentage of users that experience a given OoV rate. Under very conservative text normalization, we find that a voice search vocabulary consisting of 2 to 2.5 million words extracted from 1 week of search query data will result in an aggregate OoV rate of 1%; at that size, the same OoV rate will also be experienced by 90% of users. The number of words included in the vocabulary is a stable indicator of the OoV rate. Altering the freshness of the vocabulary or the duration of the time window over which the training data is gathered does not significantly change the OoV rate. Surprisingly, a significantly larger vocabulary (approximately 10 million words) is required to guarantee OoV rates below 1% for 95% of the users.\n    ",
        "submission_date": "2012-10-31T00:00:00",
        "last_modified_date": "2012-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.8440",
        "title": "Large Scale Language Modeling in Automatic Speech Recognition",
        "authors": [
            "Ciprian Chelba",
            "Dan Bikel",
            "Maria Shugrina",
            "Patrick Nguyen",
            "Shankar Kumar"
        ],
        "abstract": "Large language models have been proven quite beneficial for a variety of automatic speech recognition tasks in Google. We summarize results on Voice Search and a few YouTube speech transcription tasks to highlight the impact that one can expect from increasing both the amount of training data, and the size of the language model estimated from such data. Depending on the task, availability and amount of training data used, language model size and amount of work and care put into integrating them in the lattice rescoring step we observe reductions in word error rate between 6% and 10% relative, for systems on a wide range of operating points between 17% and 52% word error rate.\n    ",
        "submission_date": "2012-10-31T00:00:00",
        "last_modified_date": "2012-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0074",
        "title": "Transition-Based Dependency Parsing With Pluggable Classifiers",
        "authors": [
            "Alex Rudnick"
        ],
        "abstract": "In principle, the design of transition-based dependency parsers makes it possible to experiment with any general-purpose classifier without other changes to the parsing algorithm. In practice, however, it often takes substantial software engineering to bridge between the different representations used by two software packages. Here we present extensions to MaltParser that allow the drop-in use of any classifier conforming to the interface of the Weka machine learning package, a wrapper for the TiMBL memory-based learner to this interface, and experiments on multilingual dependency parsing with a variety of classifiers. While earlier work had suggested that memory-based learners might be a good choice for low-resource parsing scenarios, we cannot support that hypothesis in this work. We observed that support-vector machines give better parsing performance than the memory-based learner, regardless of the size of the training set.\n    ",
        "submission_date": "2012-11-01T00:00:00",
        "last_modified_date": "2012-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0418",
        "title": "Verbalizing Ontologies in Controlled Baltic Languages",
        "authors": [
            "Normunds Gr\u016bz\u012btis",
            "Gunta Ne\u0161pore",
            "Baiba Saul\u012bte"
        ],
        "abstract": "Controlled natural languages (mostly English-based) recently have emerged as seemingly informal supplementary means for OWL ontology authoring, if compared to the formal notations that are used by professional knowledge engineers. In this paper we present by examples controlled Latvian language that has been designed to be compliant with the state of the art Attempto Controlled English. We also discuss relation with controlled Lithuanian language that is being designed in parallel.\n    ",
        "submission_date": "2012-11-02T00:00:00",
        "last_modified_date": "2012-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.0498",
        "title": "Detecting English Writing Styles For Non-native Speakers",
        "authors": [
            "Rami Al-Rfou'"
        ],
        "abstract": "Analyzing writing styles of non-native speakers is a challenging task. In this paper, we analyze the comments written in the discussion pages of the English Wikipedia. Using learning algorithms, we are able to detect native speakers' writing style with an accuracy of 74%. Given the diversity of the English Wikipedia users and the large number of languages they speak, we measure the similarities among their native languages by comparing the influence they have on their English writing style. Our results show that languages known to have the same origin and development path have similar footprint on their speakers' English writing style. To enable further studies, the dataset we extracted from Wikipedia will be made available publicly.\n    ",
        "submission_date": "2012-11-02T00:00:00",
        "last_modified_date": "2012-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2290",
        "title": "Dating Texts without Explicit Temporal Cues",
        "authors": [
            "Abhimanu Kumar",
            "Jason Baldridge",
            "Matthew Lease",
            "Joydeep Ghosh"
        ],
        "abstract": "This paper tackles temporal resolution of documents, such as determining when a document is about or when it was written, based only on its text. We apply techniques from information retrieval that predict dates via language models over a discretized timeline. Unlike most previous works, we rely {\\it solely} on temporal cues implicit in the text. We consider both document-likelihood and divergence based techniques and several smoothing methods for both of them. Our best model predicts the mid-point of individuals' lives with a median of 22 and mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present day. We also show that this approach works well when training on such biographies and predicting dates both for non-biographical Wikipedia pages about specific years (500 B.C. to 2010 A.D.) and for publication dates of short stories (1798 to 2008). Together, our work shows that, even in absence of temporal extraction resources, it is possible to achieve remarkable temporal locality across a diverse set of texts.\n    ",
        "submission_date": "2012-11-10T00:00:00",
        "last_modified_date": "2012-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.2741",
        "title": "A Hindi Speech Actuated Computer Interface for Web Search",
        "authors": [
            "Kamlesh Sharma",
            "S. V. A. V. Prasad",
            "T. V. Prasad"
        ],
        "abstract": "Aiming at increasing system simplicity and flexibility, an audio evoked based system was developed by integrating simplified headphone and user-friendly software design. This paper describes a Hindi Speech Actuated Computer Interface for Web search (HSACIWS), which accepts spoken queries in Hindi language and provides the search result on the screen. This system recognizes spoken queries by large vocabulary continuous speech recognition (LVCSR), retrieves relevant document by text retrieval, and provides the search result on the Web by the integration of the Web and the voice systems. The LVCSR in this system showed enough performance levels for speech with acoustic and language models derived from a query corpus with target contents.\n    ",
        "submission_date": "2012-11-12T00:00:00",
        "last_modified_date": "2012-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.3643",
        "title": "A Principled Approach to Grammars for Controlled Natural Languages and Predictive Editors",
        "authors": [
            "Tobias Kuhn"
        ],
        "abstract": "Controlled natural languages (CNL) with a direct mapping to formal logic have been proposed to improve the usability of knowledge representation systems, query interfaces, and formal specifications. Predictive editors are a popular approach to solve the problem that CNLs are easy to read but hard to write. Such predictive editors need to be able to \"look ahead\" in order to show all possible continuations of a given unfinished sentence. Such lookahead features, however, are difficult to implement in a satisfying way with existing grammar frameworks, especially if the CNL supports complex nonlocal structures such as anaphoric references. Here, methods and algorithms are presented for a new grammar notation called Codeco, which is specifically designed for controlled natural languages and predictive editors. A parsing approach for Codeco based on an extended chart parsing algorithm is presented. A large subset of Attempto Controlled English (ACE) has been represented in Codeco. Evaluation of this grammar and the parser implementation shows that the approach is practical, adequate and efficient.\n    ",
        "submission_date": "2012-11-15T00:00:00",
        "last_modified_date": "2012-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4161",
        "title": "Semantic Polarity of Adjectival Predicates in Online Reviews",
        "authors": [
            "Ae-Lim Ahn",
            "\u00c9ric Laporte",
            "Jee-Sun Nam"
        ],
        "abstract": "Web users produce more and more documents expressing opinions. Because these have become important resources for customers and manufacturers, many have focused on them. Opinions are often expressed through adjectives with positive or negative semantic values. In extracting information from users' opinion in online reviews, exact recognition of the semantic polarity of adjectives is one of the most important requirements. Since adjectives have different semantic orientations according to contexts, it is not satisfying to extract opinion information without considering the semantic and lexical relations between the adjectives and the feature nouns appropriate to a given domain. In this paper, we present a classification of adjectives by polarity, and we analyze adjectives that are undetermined in the absence of contexts. Our research should be useful for accurately predicting semantic orientations of opinion sentences, and should be taken into account before relying on an automatic methods.\n    ",
        "submission_date": "2012-11-17T00:00:00",
        "last_modified_date": "2012-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4488",
        "title": "A Rule-Based Approach For Aligning Japanese-Spanish Sentences From A Comparable Corpora",
        "authors": [
            "Jessica C. Ram\u00edrez",
            "Yuji Matsumoto"
        ],
        "abstract": "The performance of a Statistical Machine Translation System (SMT) system is proportionally directed to the quality and length of the parallel corpus it uses. However for some pair of languages there is a considerable lack of them. The long term goal is to construct a Japanese-Spanish parallel corpus to be used for SMT, whereas, there are a lack of useful Japanese-Spanish parallel Corpus. To address this problem, In this study we proposed a method for extracting Japanese-Spanish Parallel Sentences from Wikipedia using POS tagging and Rule-Based approach. The main focus of this approach is the syntactic features of both languages. Human evaluation was performed over a sample and shows promising results, in comparison with the baseline.\n    ",
        "submission_date": "2012-11-19T00:00:00",
        "last_modified_date": "2012-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6887",
        "title": "Automating rule generation for grammar checkers",
        "authors": [
            "Marcin Mi\u0142kowski"
        ],
        "abstract": "In this paper, I describe several approaches to automatic or semi-automatic development of symbolic rules for grammar checkers from the information contained in corpora. The rules obtained this way are an important addition to manually-created rules that seem to dominate in rule-based checkers. However, the manual process of creation of rules is costly, time-consuming and error-prone. It seems therefore advisable to use machine-learning algorithms to create the rules automatically or semi-automatically. The results obtained seem to corroborate my initial hypothesis that symbolic machine learning algorithms can be useful for acquiring new rules for grammar checking. It turns out, however, that for practical uses, error corpora cannot be the sole source of information used in grammar checking. I suggest therefore that only by using different approaches, grammar-checkers, or more generally, computer-aided proofreading tools, will be able to cover most frequent and severe mistakes and avoid false alarms that seem to distract users.\n    ",
        "submission_date": "2012-11-29T00:00:00",
        "last_modified_date": "2012-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0927",
        "title": "Two Algorithms for Finding $k$ Shortest Paths of a Weighted Pushdown Automaton",
        "authors": [
            "Ke Wu",
            "Philip Resnik"
        ],
        "abstract": "We introduce efficient algorithms for finding the $k$ shortest paths of a weighted pushdown automaton (WPDA), a compact representation of a weighted set of strings with potential applications in parsing and machine translation. Both of our algorithms are derived from the same weighted deductive logic description of the execution of a WPDA using different search strategies. Experimental results show our Algorithm 2 adds very little overhead vs. the single shortest path algorithm, even with a large $k$.\n    ",
        "submission_date": "2012-12-05T00:00:00",
        "last_modified_date": "2013-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1192",
        "title": "Using external sources of bilingual information for on-the-fly word alignment",
        "authors": [
            "Miquel Espl\u00e0-Gomis",
            "Felipe S\u00e1nchez-Mart\u00ednez",
            "Mikel L. Forcada"
        ],
        "abstract": "In this paper we present a new and simple language-independent method for word-alignment based on the use of external sources of bilingual information such as machine translation systems. We show that the few parameters of the aligner can be trained on a very small corpus, which leads to results comparable to those obtained by the state-of-the-art tool GIZA++ in terms of precision. Regarding other metrics, such as alignment error rate or F-measure, the parametric aligner, when trained on a very small gold-standard (450 pairs of sentences), provides results comparable to those produced by GIZA++ when trained on an in-domain corpus of around 10,000 pairs of sentences. Furthermore, the results obtained indicate that the training is domain-independent, which enables the use of the trained aligner 'on the fly' on any new pair of sentences.\n    ",
        "submission_date": "2012-12-05T00:00:00",
        "last_modified_date": "2012-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1478",
        "title": "The Clustering of Author's Texts of English Fiction in the Vector Space of Semantic Fields",
        "authors": [
            "Bohdan Pavlyshenko"
        ],
        "abstract": "The clustering of text documents in the vector space of semantic fields and in the semantic space with orthogonal basis has been analysed. It is shown that using the vector space model with the basis of semantic fields is effective in the cluster analysis algorithms of author's texts in English fiction. The analysis of the author's texts distribution in cluster structure showed the presence of the areas of semantic space that represent the author's ideolects of individual authors. SVD factorization of the semantic fields matrix makes it possible to reduce significantly the dimension of the semantic space in the cluster analysis of author's texts.\n    ",
        "submission_date": "2012-12-06T00:00:00",
        "last_modified_date": "2012-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2006",
        "title": "A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization",
        "authors": [
            "Jiwei Li",
            "Sujian Li"
        ],
        "abstract": "Both supervised learning methods and LDA based topic model have been successfully applied in the field of query focused multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experiments on TAC2008 and TAC2009 demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2012-12-10T00:00:00",
        "last_modified_date": "2013-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2036",
        "title": "Query-focused Multi-document Summarization: Combining a Novel Topic Model with Graph-based Semi-supervised Learning",
        "authors": [
            "Jiwei Li",
            "Sujian Li"
        ],
        "abstract": "Graph-based semi-supervised learning has proven to be an effective approach for query-focused multi-document summarization. The problem of previous semi-supervised learning is that sentences are ranked without considering the higher level information beyond sentence level. Researches on general summarization illustrated that the addition of topic level can effectively improve the summary quality. Inspired by previous researches, we propose a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach. At the same time, we propose a novel topic model which makes full use of the dependence between sentences and words. Experimental results on DUC and TAC data sets demonstrate the effectiveness of our proposed approach.\n    ",
        "submission_date": "2012-12-10T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2390",
        "title": "On the complexity of learning a language: An improvement of Block's algorithm",
        "authors": [
            "Eric Werner"
        ],
        "abstract": "Language learning is thought to be a highly complex process. One of the hurdles in learning a language is to learn the rules of syntax of the language. Rules of syntax are often ordered in that before one rule can applied one must apply another. It has been thought that to learn the order of n rules one must go through all n! permutations. Thus to learn the order of 27 rules would require 27! steps or 1.08889x10^{28} steps. This number is much greater than the number of seconds since the beginning of the universe! In an insightful analysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with the assumption of transitivity this vast number of learning steps reduces to a mere 377 steps. We present a mathematical analysis of the complexity of Block's algorithm. The algorithm has a complexity of order n^2 given n rules. In addition, we improve Block's results exponentially, by introducing an algorithm that has complexity of order less than n log n.\n    ",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2676",
        "title": "Mining the Web for the Voice of the Herd to Track Stock Market Bubbles",
        "authors": [
            "Aaron Gerow",
            "Mark Keane"
        ],
        "abstract": "We show that power-law analyses of financial commentaries from newspaper web-sites can be used to identify stock market bubbles, supplementing traditional volatility analyses. Using a four-year corpus of 17,713 online, finance-related articles (10M+ words) from the Financial Times, the New York Times, and the BBC, we show that week-to-week changes in power-law distributions reflect market movements of the Dow Jones Industrial Average (DJI), the FTSE-100, and the NIKKEI-225. Notably, the statistical regularities in language track the 2007 stock market bubble, showing emerging structure in the language of commentators, as progressively greater agreement arose in their positive perceptions of the market. Furthermore, during the bubble period, a marked divergence in positive language occurs as revealed by a Kullback-Leibler analysis.\n    ",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3138",
        "title": "Identifying Metaphor Hierarchies in a Corpus Analysis of Finance Articles",
        "authors": [
            "Aaron Georw",
            "Mark Keane"
        ],
        "abstract": "Using a corpus of over 17,000 financial news reports (involving over 10M words), we perform an analysis of the argument-distributions of the UP- and DOWN-verbs used to describe movements of indices, stocks, and shares. Using measures of the overlap in the argument distributions of these verbs and k-means clustering of their distributions, we advance evidence for the proposal that the metaphors referred to by these verbs are organised into hierarchical structures of superordinate and subordinate groups.\n    ",
        "submission_date": "2012-12-13T00:00:00",
        "last_modified_date": "2012-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3139",
        "title": "Identifying Metaphoric Antonyms in a Corpus Analysis of Finance Articles",
        "authors": [
            "Aaron Gerow",
            "Mark Keane"
        ],
        "abstract": "Using a corpus of 17,000+ financial news reports (involving over 10M words), we perform an analysis of the argument-distributions of the UP and DOWN verbs used to describe movements of indices, stocks and shares. In Study 1 participants identified antonyms of these verbs in a free-response task and a matching task from which the most commonly identified antonyms were compiled. In Study 2, we determined whether the argument-distributions for the verbs in these antonym-pairs were sufficiently similar to predict the most frequently-identified antonym. Cosine similarity correlates moderately with the proportions of antonym-pairs identified by people (r = 0.31). More impressively, 87% of the time the most frequently-identified antonym is either the first- or second-most similar pair in the set of alternatives. The implications of these results for distributional approaches to determining metaphoric knowledge are discussed.\n    ",
        "submission_date": "2012-12-13T00:00:00",
        "last_modified_date": "2013-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3162",
        "title": "Diachronic Variation in Grammatical Relations",
        "authors": [
            "Aaron Gerow",
            "Khurshid Ahmad"
        ],
        "abstract": "We present a method of finding and analyzing shifts in grammatical relations found in diachronic corpora. Inspired by the econometric technique of measuring return and volatility instead of relative frequencies, we propose them as a way to better characterize changes in grammatical patterns like nominalization, modification and comparison. To exemplify the use of these techniques, we examine a corpus of NIPS papers and report trends which manifest at the token, part-of-speech and grammatical levels. Building up from frequency observations to a second-order analysis, we show that shifts in frequencies overlook deeper trends in language, even when part-of-speech information is included. Examining token, POS and grammatical levels of variation enables a summary view of diachronic text as a whole. We conclude with a discussion about how these methods can inform intuitions about specialist domains as well as changes in language use as a whole.\n    ",
        "submission_date": "2012-12-13T00:00:00",
        "last_modified_date": "2012-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3228",
        "title": "Language Without Words: A Pointillist Model for Natural Language Processing",
        "authors": [
            "Peiyou Song",
            "Anhei Shu",
            "David Phipps",
            "Dan Wallach",
            "Mohit Tiwari",
            "Jedidiah Crandall",
            "George Luger"
        ],
        "abstract": "This paper explores two separate questions: Can we perform natural language processing tasks without a lexicon?; and, Should we? Existing natural language processing techniques are either based on words as units or use units such as grams only for basic classification tasks. How close can a machine come to reasoning about the meanings of words and phrases in a corpus without using any lexicon, based only on grams?\n",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3493",
        "title": "Sentence Compression in Spanish driven by Discourse Segmentation and Language Models",
        "authors": [
            "Alejandro Molina",
            "Juan-Manuel Torres-Moreno",
            "Iria da Cunha",
            "Eric SanJuan",
            "Gerardo Sierra"
        ],
        "abstract": "Previous works demonstrated that Automatic Text Summarization (ATS) by sentences extraction may be improved using sentence compression. In this work we present a sentence compressions approach guided by level-sentence discourse segmentation and probabilistic language models (LM). The results presented here show that the proposed solution is able to generate coherent summaries with grammatical compressed sentences. The approach is simple enough to be transposed into other languages.\n    ",
        "submission_date": "2012-12-14T00:00:00",
        "last_modified_date": "2012-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3634",
        "title": "A comparative study of root-based and stem-based approaches for measuring the similarity between arabic words for arabic text mining applications",
        "authors": [
            "Hanane Froud",
            "Abdelmonaim Lachkar",
            "Said Alaoui Ouatik"
        ],
        "abstract": "Representation of semantic information contained in the words is needed for any Arabic Text Mining applications. More precisely, the purpose is to better take into account the semantic dependencies between words expressed by the co-occurrence frequencies of these words. There have been many proposals to compute similarities between words based on their distributions in contexts. In this paper, we compare and contrast the effect of two preprocessing techniques applied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming) approaches for measuring the similarity between Arabic words with the well known abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of distance functions and similarity measures, such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient. The obtained results show that, on the one hand, the variety of the corpus produces more accurate results; on the other hand, the Stem-based approach outperformed the Root-based one because this latter affects the words meanings.\n    ",
        "submission_date": "2012-12-14T00:00:00",
        "last_modified_date": "2012-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.4315",
        "title": "Assessing Sentiment Strength in Words Prior Polarities",
        "authors": [
            "Lorenzo Gatti",
            "Marco Guerini"
        ],
        "abstract": "Many approaches to sentiment analysis rely on lexica where words are tagged with their prior polarity - i.e. if a word out of context evokes something positive or something negative. In particular, broad-coverage resources like SentiWordNet provide polarities for (almost) every word. Since words can have multiple senses, we address the problem of how to compute the prior polarity of a word starting from the polarity of each sense and returning its polarity strength as an index between -1 and 1. We compare 14 such formulae that appear in the literature, and assess which one best approximates the human judgement of prior polarities, with both regression and classification models.\n    ",
        "submission_date": "2012-12-18T00:00:00",
        "last_modified_date": "2012-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.4674",
        "title": "Natural Language Understanding Based on Semantic Relations between Sentences",
        "authors": [
            "Hyeok Kong"
        ],
        "abstract": "In this paper, we define event expression over sentences of natural language and semantic relations between events. Based on this definition, we formally consider text understanding process having events as basic unit.\n    ",
        "submission_date": "2012-12-19T00:00:00",
        "last_modified_date": "2012-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2073",
        "title": "Pbm: A new dataset for blog mining",
        "authors": [
            "Mehwish Aziz",
            "Muhammad Rafi"
        ],
        "abstract": "Text mining is becoming vital as Web 2.0 offers collaborative content creation and sharing. Now Researchers have growing interest in text mining methods for discovering knowledge. Text mining researchers come from variety of areas like: Natural Language Processing, Computational Linguistic, Machine Learning, and Statistics. A typical text mining application involves preprocessing of text, stemming and lemmatization, tagging and annotation, deriving knowledge patterns, evaluating and interpreting the results. There are numerous approaches for performing text mining tasks, like: clustering, categorization, sentimental analysis, and summarization. There is a growing need to standardize the evaluation of these tasks. One major component of establishing standardization is to provide standard datasets for these tasks. Although there are various standard datasets available for traditional text mining tasks, but there are very few and expensive datasets for blog-mining task. Blogs, a new genre in web 2.0 is a digital diary of web user, which has chronological entries and contains a lot of useful knowledge, thus offers a lot of challenges and opportunities for text mining. In this paper, we report a new indigenous dataset for Pakistani Political Blogosphere. The paper describes the process of data collection, organization, and standardization. We have used this dataset for carrying out various text mining tasks for blogosphere, like: blog-search, political sentiments analysis and tracking, identification of influential blogger, and clustering of the blog-posts. We wish to offer this dataset free for others who aspire to pursue further in this domain.\n    ",
        "submission_date": "2012-01-10T00:00:00",
        "last_modified_date": "2012-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2240",
        "title": "Bengali text summarization by sentence extraction",
        "authors": [
            "Kamal Sarkar"
        ],
        "abstract": "Text summarization is a process to produce an abstract or a summary by selecting significant portion of the information from one or more texts. In an automatic text summarization process, a text is given to the computer and the computer returns a shorter less redundant extract or abstract of the original text(s). Many techniques have been developed for summarizing English text(s). But, a very few attempts have been made for Bengali text summarization. This paper presents a method for Bengali text summarization which extracts important sentences from a Bengali document to produce a summary.\n    ",
        "submission_date": "2012-01-11T00:00:00",
        "last_modified_date": "2012-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.2719",
        "title": "Ultrametric Model of Mind, II: Application to Text Content Analysis",
        "authors": [
            "Fionn Murtagh"
        ],
        "abstract": "In a companion paper, Murtagh (2012), we discussed how Matte Blanco's work linked the unrepressed unconscious (in the human) to symmetric logic and thought processes. We showed how ultrametric topology provides a most useful representational and computational framework for this. Now we look at the extent to which we can find ultrametricity in text. We use coherent and meaningful collections of nearly 1000 texts to show how we can measure inherent ultrametricity. On the basis of our findings we hypothesize that inherent ultrametricty is a basis for further exploring unconscious thought processes.\n    ",
        "submission_date": "2012-01-13T00:00:00",
        "last_modified_date": "2012-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5477",
        "title": "Entropy-growth-based model of emotionally charged online dialogues",
        "authors": [
            "Julian Sienkiewicz",
            "Marcin Skowron",
            "Georgios Paltoglou",
            "Janusz A. Holyst"
        ],
        "abstract": "We analyze emotionally annotated massive data from IRC (Internet Relay Chat) and model the dialogues between its participants by assuming that the driving force for the discussion is the entropy growth of emotional probability distribution. This process is claimed to be correlated to the emergence of the power-law distribution of the discussion lengths observed in the dialogues. We perform numerical simulations based on the noticed phenomenon obtaining a good agreement with the real data. Finally, we propose a method to artificially prolong the duration of the discussion that relies on the entropy of emotional probability distribution.\n    ",
        "submission_date": "2012-01-26T00:00:00",
        "last_modified_date": "2012-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1201.5484",
        "title": "Statistical analysis of emotions and opinions at Digg website",
        "authors": [
            "Piotr Pohorecki",
            "Julian Sienkiewicz",
            "Marija Mitrovic",
            "Georgios Paltoglou",
            "Janusz A. Holyst"
        ],
        "abstract": "We performed statistical analysis on data from the ",
        "submission_date": "2012-01-26T00:00:00",
        "last_modified_date": "2012-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.0617",
        "title": "Classification of Flames in Computer Mediated Communications",
        "authors": [
            "Nitin",
            "Ankush Bansal",
            "Siddhartha Mahadev Sharma",
            "Kapil Kumar",
            "Anuj Aggarwal",
            "Sheenu Goyal",
            "Kanika Choudhary",
            "Kunal Chawla",
            "Kunal Jain",
            "Manav Bhasin"
        ],
        "abstract": "Computer Mediated Communication (CMC) has brought about a revolution in the way the world communicates with each other. With the increasing number of people, interacting through the internet and the rise of new platforms and technologies has brought together the people from different social, cultural and geographical backgrounds to present their thoughts, ideas and opinions on topics of their interest. CMC has, in some cases, gave users more freedom to express themselves as compared to Face-to-face communication. This has also led to rise in the use of hostile and aggressive language and terminologies uninhibitedly. Since such use of language is detrimental to the discussion process and affects the audience and individuals negatively, efforts are being taken to control them. The research sees the need to understand the concept of flaming and hence attempts to classify them in order to give a better understanding of it. The classification is done on the basis of type of flame content being presented and the Style in which they are presented.\n    ",
        "submission_date": "2012-02-03T00:00:00",
        "last_modified_date": "2012-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.2518",
        "title": "Segmenting DNA sequence into `words'",
        "authors": [
            "Wang Liang"
        ],
        "abstract": "This paper presents a novel method to segment/decode DNA sequences based on n-grams statistical language model. Firstly, we find the length of most DNA 'words' is 12 to 15 bps by analyzing the genomes of 12 model species. Then we design an unsupervised probability based approach to segment the DNA sequences. The benchmark of segmenting method is also proposed.\n    ",
        "submission_date": "2012-02-12T00:00:00",
        "last_modified_date": "2013-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.3752",
        "title": "Multidimensional counting grids: Inferring word order from disordered bags of words",
        "authors": [
            "Nebojsa Jojic",
            "Alessandro Perina"
        ],
        "abstract": "Models of bags of words typically assume topic mixing so that the words in a single bag come from a limited number of topics. We show here that many sets of bag of words exhibit a very different pattern of variation than the patterns that are efficiently captured by topic mixing. In many cases, from one bag of words to the next, the words disappear and new ones appear as if the theme slowly and smoothly shifted across documents (providing that the documents are somehow ordered). Examples of latent structure that describe such ordering are easily imagined. For example, the advancement of the date of the news stories is reflected in a smooth change over the theme of the day as certain evolving news stories fall out of favor and new events create new stories. Overlaps among the stories of consecutive days can be modeled by using windows over linearly arranged tight distributions over words. We show here that such strategy can be extended to multiple dimensions and cases where the ordering of data is not readily obvious. We demonstrate that this way of modeling covariation in word occurrences outperforms standard topic models in classification and prediction tasks in applications in biology, text modeling and computer vision.\n    ",
        "submission_date": "2012-02-14T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.4837",
        "title": "The GF Mathematics Library",
        "authors": [
            "Jordi Saludes",
            "Sebastian Xamb\u00f3"
        ],
        "abstract": "This paper is devoted to present the Mathematics Grammar Library, a system for multilingual mathematical text processing. We explain the context in which it originated, its current design and functionality and the current development goals.  We also present two prototype services and comment on possible future applications in the area of artificial mathematics assistants.\n    ",
        "submission_date": "2012-02-22T00:00:00",
        "last_modified_date": "2012-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1202.5913",
        "title": "Fly out-smarts man",
        "authors": [
            "Ruedi Stoop",
            "Patrick N\u00fcesch",
            "Ralph Lukas Stoop",
            "Leonid Bunimovich"
        ],
        "abstract": "Precopulatory courtship is a high-cost, non-well understood animal world mystery. Drosophila's (=D.'s) precopulatory courtship not only shows marked structural similarities with mammalian courtship, but also with human spoken language. This suggests the study of purpose, modalities and in particular of the power of this language and to compare it to human language. Following a mathematical symbolic dynamics approach, we translate courtship videos of D.'s body language into a formal language. This approach made it possible to show that D. may use its body language to express individual information - information that may be important for evolutionary optimization, on top of the sexual group membership. Here, we use Chomsky's hierarchical language classification to characterize the power of D.'s body language, and then compare it with the power of languages spoken by humans. We find that from a formal language point of view, D.'s body language is at least as powerful as the languages spoken by humans. From this we conclude that human intellect cannot be the direct consequence of the formal grammar complexity of human language.\n    ",
        "submission_date": "2012-02-27T00:00:00",
        "last_modified_date": "2012-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.1743",
        "title": "Variable types for meaning assembly: a logical syntax for generic noun phrases introduced by most",
        "authors": [
            "Christian Retor\u00e9"
        ],
        "abstract": "This paper proposes a way to compute the meanings associated with sentences with generic noun phrases corresponding to the generalized quantifier most. We call these generics specimens and they resemble stereotypes or prototypes in lexical semantics. The meanings are viewed as logical formulae that can thereafter be interpreted in your favourite models. To do so, we depart significantly from the dominant Fregean view with a single untyped universe. Indeed, our proposal adopts type theory with some hints from Hilbert \\epsilon-calculus (Hilbert, 1922; Avigad and Zach, 2008) and from medieval philosophy, see e.g. de Libera (1993, 1996). Our type theoretic analysis bears some resemblance with ongoing work in lexical semantics (Asher 2011; Bassac et al. 2010; Moot, Pr\u00e9vot and Retor\u00e9 2011). Our model also applies to classical examples involving a class, or a generic element of this class, which is not uttered but provided by the context. An outcome of this study is that, in the minimalism-contextualism debate, see Conrad (2011), if one adopts a type theoretical view, terms encode the purely semantic meaning component while their typing is pragmatically determined.\n    ",
        "submission_date": "2012-03-08T00:00:00",
        "last_modified_date": "2012-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3227",
        "title": "Generalisation of language and knowledge models for corpus analysis",
        "authors": [
            "Anton Loss"
        ],
        "abstract": "This paper takes new look on language and knowledge modelling for corpus linguistics. Using ideas of Chaitin, a line of argument is made against language/knowledge separation in Natural Language Processing. A simplistic model, that generalises approaches to language and knowledge, is proposed. One of hypothetical consequences of this model is Strong AI.\n    ",
        "submission_date": "2012-03-14T00:00:00",
        "last_modified_date": "2012-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3511",
        "title": "Inference by Minimizing Size, Divergence, or their Sum",
        "authors": [
            "Sebastian Riedel",
            "David A. Smith",
            "Andrew McCallum"
        ],
        "abstract": "We speed up marginal inference by ignoring factors that do not significantly contribute to overall accuracy. In order to pick a suitable subset of factors to ignore, we propose three schemes: minimizing the number of model factors under a bound on the KL divergence between pruned and full models; minimizing the KL divergence under a bound on factor count; and minimizing the weighted sum of KL divergence and factor count. All three problems are solved using an approximation of the KL divergence than can be calculated in terms of marginals computed on a simple seed graph. Applied to synthetic image denoising and to three different types of NLP parsing models, this technique performs marginal inference up to 11 times faster than loopy BP, with graph sizes reduced up to 98%-at comparable error in marginals and parsing accuracy. We also show that minimizing the weighted sum of divergence and size is substantially faster than minimizing either of the other objectives based on the approximation to divergence presented here.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.3586",
        "title": "Automated Text Summarization Base on Lexicales Chain and graph Using of WordNet and Wikipedia Knowledge Base",
        "authors": [
            "Mohsen Pourvali",
            "Mohammad Saniee Abadeh"
        ],
        "abstract": "The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. Nowadays, document summarization plays an important role in information retrieval. With a large volume of documents, presenting the user with a summary of each document greatly facilitates the task of finding the desired documents. Document summarization is a process of automatically creating a compressed version of a given document that provides useful information to users, and multi-document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic. The lexical cohesion structure of the text can be exploited to determine the importance of a sentence/phrase. Lexical chains are useful tools to analyze the lexical cohesion structure in a text .In this paper we consider the effect of the use of lexical cohesion features in Summarization, And presenting a algorithm base on the knowledge base. Ours algorithm at first find the correct sense of any word, Then constructs the lexical chains, remove Lexical chains that less score than other, detects topics roughly from lexical chains, segments the text with respect to the topics and selects the most important sentences. The experimental results on an open benchmark datasets from DUC01 and DUC02 show that our proposed approach can improve the performance compared to sate-of-the-art summarization approaches.\n    ",
        "submission_date": "2012-03-15T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.4238",
        "title": "Do Linguistic Style and Readability of Scientific Abstracts affect their Virality?",
        "authors": [
            "Marco Guerini",
            "Alberto Pepe",
            "Bruno Lepri"
        ],
        "abstract": "Reactions to textual content posted in an online social network show different dynamics depending on the linguistic style and readability of the submitted content. Do similar dynamics exist for responses to scientific articles? Our intuition, supported by previous research, suggests that the success of a scientific article depends on its content, rather than on its linguistic style. In this article, we examine a corpus of scientific abstracts and three forms of associated reactions: article downloads, citations, and bookmarks. Through a class-based psycholinguistic analysis and readability indices tests, we show that certain stylistic and readability features of abstracts clearly concur in determining the success and viral capability of a scientific article.\n    ",
        "submission_date": "2012-03-19T00:00:00",
        "last_modified_date": "2012-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.5188",
        "title": "Semi-Automatically Extracting FAQs to Improve Accessibility of Software Development Knowledge",
        "authors": [
            "Stefan Hen\u00df",
            "Martin Monperrus",
            "Mira Mezini"
        ],
        "abstract": "Frequently asked questions (FAQs) are a popular way to document software development knowledge. As creating such documents is expensive, this paper presents an approach for automatically extracting FAQs from sources of software development discussion, such as mailing lists and Internet forums, by combining techniques of text mining and natural language processing. We apply the approach to popular mailing lists and carry out a survey among software developers to show that it is able to extract high-quality FAQs that may be further improved by experts.\n    ",
        "submission_date": "2012-03-23T00:00:00",
        "last_modified_date": "2012-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1203.6339",
        "title": "Intelligent Interface Architectures for Folksonomy Driven Structure Network",
        "authors": [
            "Massimiliano Dal Mas"
        ],
        "abstract": "The folksonomy is the result of free personal information or assignment of tags to an object (determined by the URI) in order to find them. The practice of tagging is done in a collective environment. Folksonomies are self constructed, based on co-occurrence of definitions, rather than a hierarchical structure of the data. The downside of this was that a few sites and applications are able to successfully exploit the sharing of bookmarks. The need for tools that are able to resolve the ambiguity of the definitions is becoming urgent as the need of simple instruments for their visualization, editing and exploitation in web applications still hinders their diffusion and wide adoption. An intelligent interactive interface design for folksonomies should consider the contextual design and inquiry based on a concurrent interaction for a perceptual user interfaces. To represent folksonomies a new concept structure called \"Folksodriven\" is used in this paper. While it is presented the Folksodriven Structure Network (FSN) to resolve the ambiguity of definitions of folksonomy tags suggestions for the user. On this base a Human-Computer Interactive (HCI) systems is developed for the visualization, navigation, updating and maintenance of folksonomies Knowledge Bases - the FSN - through the web. System functionalities as well as its internal architecture will be introduced.\n    ",
        "submission_date": "2012-03-28T00:00:00",
        "last_modified_date": "2012-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.1615",
        "title": "Discrimination between Arabic and Latin from bilingual documents",
        "authors": [
            "Sofiene Haboubi",
            "Samia Maddouri",
            "Hamid Amiri"
        ],
        "abstract": "2011 International Conference on Communications, Computing and Control Applications (CCCA)\n    ",
        "submission_date": "2012-04-07T00:00:00",
        "last_modified_date": "2012-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2523",
        "title": "Concept Modeling with Superwords",
        "authors": [
            "Khalid El-Arini",
            "Emily B. Fox",
            "Carlos Guestrin"
        ],
        "abstract": "In information retrieval, a fundamental goal is to transform a document into concepts that are representative of its content. The term \"representative\" is in itself challenging to define, and various tasks require different granularities of concepts. In this paper, we aim to model concepts that are sparse over the vocabulary, and that flexibly adapt their content based on other relevant semantic information such as textual structure or associated image features. We explore a Bayesian nonparametric model based on nested beta processes that allows for inferring an unknown number of strictly sparse concepts. The resulting model provides an inherently different representation of concepts than a standard LDA (or HDP) based topic model, and allows for direct incorporation of semantic features. We demonstrate the utility of this representation on multilingual blog data and the Congressional Record.\n    ",
        "submission_date": "2012-04-11T00:00:00",
        "last_modified_date": "2012-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.2804",
        "title": "Estimating the Prevalence of Deception in Online Review Communities",
        "authors": [
            "Myle Ott",
            "Claire Cardie",
            "Jeff Hancock"
        ],
        "abstract": "Consumers' purchase decisions are increasingly influenced by user-generated online reviews. Accordingly, there has been growing concern about the potential for posting \"deceptive opinion spam\" -- fictitious reviews that have been deliberately written to sound authentic, to deceive the reader. But while this practice has received considerable public attention and concern, relatively little is known about the actual prevalence, or rate, of deception in online review communities, and less still about the factors that influence it.\n",
        "submission_date": "2012-04-12T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3198",
        "title": "The failure of the law of brevity in two New World primates. Statistical caveats",
        "authors": [
            "Ramon Ferrer-i-Cancho",
            "Antoni Hern\u00e1ndez-Fern\u00e1ndez"
        ],
        "abstract": "Parallels of Zipf's law of brevity, the tendency of more frequent words to be shorter, have been found in bottlenose dolphins and Formosan macaques. Although these findings suggest that behavioral repertoires are shaped by a general principle of compression, common marmosets and golden-backed uakaris do not exhibit the law. However, we argue that the law may be impossible or difficult to detect statistically in a given species if the repertoire is too small, a problem that could be affecting golden backed uakaris, and show that the law is present in a subset of the repertoire of common marmosets. We suggest that the visibility of the law will depend on the subset of the repertoire under consideration or the repertoire size.\n    ",
        "submission_date": "2012-04-14T00:00:00",
        "last_modified_date": "2012-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3458",
        "title": "The logic of quantum mechanics - Take II",
        "authors": [
            "Bob Coecke"
        ],
        "abstract": "We put forward a new take on the logic of quantum mechanics, following Schroedinger's point of view that it is composition which makes quantum theory what it is, rather than its particular propositional structure due to the existence of superpositions, as proposed by Birkhoff and von Neumann. This gives rise to an intrinsically quantitative kind of logic, which truly deserves the name `logic' in that it also models meaning in natural language, the latter being the origin of logic, that it supports automation, the most prominent practical use of logic, and that it supports probabilistic inference.\n    ",
        "submission_date": "2012-04-16T00:00:00",
        "last_modified_date": "2012-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3498",
        "title": "A Computational Analysis of Collective Discourse",
        "authors": [
            "Vahed Qazvinian",
            "Dragomir R. Radev"
        ],
        "abstract": "This paper is focused on the computational analysis of collective discourse, a collective behavior seen in non-expert content contributions in online social media. We collect and analyze a wide range of real-world collective discourse datasets from movie user reviews to microblogs and news headlines to scientific citations. We show that all these datasets exhibit diversity of perspective, a property seen in other collective systems and a criterion in wise crowds. Our experiments also confirm that the network of different perspective co-occurrences exhibits the small-world property with high clustering of different perspectives. Finally, we show that non-expert contributions in collective discourse can be used to answer simple questions that are otherwise hard to answer.\n    ",
        "submission_date": "2012-04-16T00:00:00",
        "last_modified_date": "2012-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.3731",
        "title": "Towards Real-Time Summarization of Scheduled Events from Twitter Streams",
        "authors": [
            "Arkaitz Zubiaga",
            "Damiano Spina",
            "Enrique Amig\u00f3",
            "Julio Gonzalo"
        ],
        "abstract": "This paper explores the real-time summarization of scheduled events such as soccer games from torrential flows of Twitter streams. We propose and evaluate an approach that substantially shrinks the stream of tweets in real-time, and consists of two steps: (i) sub-event detection, which determines if something new has occurred, and (ii) tweet selection, which picks a representative tweet to describe each sub-event. We compare the summaries generated in three languages for all the soccer games in \"Copa America 2011\" to reference live reports offered by Yahoo! Sports journalists. We show that simple text analysis methods which do not involve external knowledge lead to summaries that cover 84% of the sub-events on average, and 100% of key types of sub-events (such as goals in soccer). Our approach should be straightforwardly applicable to other kinds of scheduled events such as other sports, award ceremonies, keynote talks, TV shows, etc.\n    ",
        "submission_date": "2012-04-17T00:00:00",
        "last_modified_date": "2012-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4346",
        "title": "Your Two Weeks of Fame and Your Grandmother's",
        "authors": [
            "James Cook",
            "Atish Das Sarma",
            "Alex Fabrikant",
            "Andrew Tomkins"
        ],
        "abstract": "Did celebrity last longer in 1929, 1992 or 2009? We investigate the phenomenon of fame by mining a collection of news articles that spans the twentieth century, and also perform a side study on a collection of blog posts from the last 10 years. By analyzing mentions of personal names, we measure each person's time in the spotlight, using two simple metrics that evaluate, roughly, the duration of a single news story about a person, and the overall duration of public interest in a person. We watched the distribution evolve from 1895 to 2010, expecting to find significantly shortening fame durations, per the much popularly bemoaned shortening of society's attention spans and quickening of media's news cycles. Instead, we conclusively demonstrate that, through many decades of rapid technological and societal change, through the appearance of Twitter, communication satellites, and the Internet, fame durations did not decrease, neither for the typical case nor for the extremely famous, with the last statistically significant fame duration decreases coming in the early 20th century, perhaps from the spread of telegraphy and telephony. Furthermore, while median fame durations stayed persistently constant, for the most famous of the famous, as measured by either volume or duration of media attention, fame durations have actually trended gently upward since the 1940s, with statistically significant increases on 40-year timescales. Similar studies have been done with much shorter timescales specifically in the context of information spreading on Twitter and similar social networking sites. To the best of our knowledge, this is the first massive scale study of this nature that spans over a century of archived data, thereby allowing us to track changes across decades.\n    ",
        "submission_date": "2012-04-19T00:00:00",
        "last_modified_date": "2012-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.4914",
        "title": "Quantum Interference in Cognition: Structural Aspects of the Brain",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "We identify the presence of typically quantum effects, namely 'superposition' and 'interference', in what happens when human concepts are combined, and provide a quantum model in complex Hilbert space that represents faithfully experimental data measuring the situation of combining concepts. Our model shows how 'interference of concepts' explains the effects of underextension and overextension when two concepts combine to the disjunction of these two concepts. This result supports our earlier hypothesis that human thought has a superposed two-layered structure, one layer consisting of 'classical logical thought' and a superposed layer consisting of 'quantum conceptual thought'. Possible connections with recent findings of a 'grid-structure' for the brain are analyzed, and influences on the mind/brain relation, and consequences on applied disciplines, such as artificial intelligence and quantum computation, are considered.\n    ",
        "submission_date": "2012-04-22T00:00:00",
        "last_modified_date": "2012-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.5345",
        "title": "Time-dependent wave selection for information processing in excitable media",
        "authors": [
            "William M. Stevens",
            "Andrew Adamatzky",
            "Ishrat Jahan",
            "Ben de Lacy Costello"
        ],
        "abstract": "We demonstrate an improved technique for implementing logic circuits in light-sensitive chemical excitable media. The technique makes use of the constant-speed propagation of waves along defined channels in an excitable medium based on the Belousov-Zhabotinsky reaction, along with the mutual annihilation of colliding waves. What distinguishes this work from previous work in this area is that regions where channels meet at a junction can periodically alternate between permitting the propagation of waves and blocking them. These valve-like areas are used to select waves based on the length of time that it takes waves to propagate from one valve to another. In an experimental implementation, the channels which make up the circuit layout are projected by a digital projector connected to a computer. Excitable channels are projected as dark areas, unexcitable regions as light areas. Valves alternate between dark and light: every valve has the same period and phase, with a 50% duty cycle. This scheme can be used to make logic gates based on combinations of OR and AND-NOT operations, with few geometrical constraints. Because there are few geometrical constraints, compact circuits can be implemented. Experimental results from an implementation of a 4-bit input, 2-bit output integer square root circuit are given. This is the most complex logic circuit that has been implemented in BZ excitable media to date.\n    ",
        "submission_date": "2012-04-24T00:00:00",
        "last_modified_date": "2012-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6362",
        "title": "A Corpus-based Evaluation of Lexical Components of a Domainspecific Text to Knowledge Mapping Prototype",
        "authors": [
            "Rushdi Shams",
            "Adel Elsayed"
        ],
        "abstract": "The aim of this paper is to evaluate the lexical components of a Text to Knowledge Mapping (TKM) prototype. The prototype is domain-specific, the purpose of which is to map instructional text onto a knowledge domain. The context of the knowledge domain of the prototype is physics, specifically DC electrical circuits. During development, the prototype has been tested with a limited data set from the domain. The prototype now reached a stage where it needs to be evaluated with a representative linguistic data set called corpus. A corpus is a collection of text drawn from typical sources which can be used as a test data set to evaluate NLP systems. As there is no available corpus for the domain, we developed a representative corpus and annotated it with linguistic information. The evaluation of the prototype considers one of its two main components- lexical knowledge base. With the corpus, the evaluation enriches the lexical knowledge resources like vocabulary and grammar structure. This leads the prototype to parse a reasonable amount of sentences in the corpus.\n    ",
        "submission_date": "2012-04-28T00:00:00",
        "last_modified_date": "2012-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1204.6441",
        "title": "\"I Wanted to Predict Elections with Twitter and all I got was this Lousy Paper\" -- A Balanced Survey on Election Prediction using Twitter Data",
        "authors": [
            "Daniel Gayo-Avello"
        ],
        "abstract": "Predicting X from Twitter is a popular fad within the Twitter research subculture. It seems both appealing and relatively easy. Among such kind of studies, electoral prediction is maybe the most attractive, and at this moment there is a growing body of literature on such a topic. This is not only an interesting research problem but, above all, it is extremely difficult. However, most of the authors seem to be more interested in claiming positive results than in providing sound and reproducible methods. It is also especially worrisome that many recent papers seem to only acknowledge those studies supporting the idea of Twitter predicting elections, instead of conducting a balanced literature review showing both sides of the matter. After reading many of such papers I have decided to write such a survey myself. Hence, in this paper, every study relevant to the matter of electoral prediction using social media is commented. From this review it can be concluded that the predictive power of Twitter regarding elections has been greatly exaggerated, and that hard research problems still lie ahead.\n    ",
        "submission_date": "2012-04-28T00:00:00",
        "last_modified_date": "2012-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1794",
        "title": "A Novel Method For Speech Segmentation Based On Speakers' Characteristics",
        "authors": [
            "Behrouz Abdolali",
            "Hossein Sameti"
        ],
        "abstract": "Speech Segmentation is the process change point detection for partitioning an input audio stream into regions each of which corresponds to only one audio source or one speaker. One application of this system is in Speaker Diarization systems. There are several methods for speaker segmentation; however, most of the Speaker Diarization Systems use BIC-based Segmentation methods. The main goal of this paper is to propose a new method for speaker segmentation with higher speed than the current methods - e.g. BIC - and acceptable accuracy. Our proposed method is based on the pitch frequency of the speech. The accuracy of this method is similar to the accuracy of common speaker segmentation methods. However, its computation cost is much less than theirs. We show that our method is about 2.4 times faster than the BIC-based method, while the average accuracy of pitch-based method is slightly higher than that of the BIC-based method.\n    ",
        "submission_date": "2012-05-08T00:00:00",
        "last_modified_date": "2012-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.1975",
        "title": "Expressivity of Time-Varying Graphs and the Power of Waiting in Dynamic Networks",
        "authors": [
            "Arnaud Casteigts",
            "Paola Flocchini",
            "Emmanuel Godard",
            "Nicola Santoro",
            "Masafumi Yamashita"
        ],
        "abstract": "In infrastructure-less highly dynamic networks, computing and performing even basic tasks (such as routing and broadcasting) is a very challenging activity due to the fact that connectivity does not necessarily hold, and the network may actually be disconnected at every time instant. Clearly the task of designing protocols for these networks is less difficult if the environment allows waiting (i.e., it provides the nodes with store-carry-forward-like mechanisms such as local buffering) than if waiting is not feasible. No quantitative corroborations of this fact exist (e.g., no answer to the question: how much easier?). In this paper, we consider these qualitative questions about dynamic networks, modeled as time-varying (or evolving) graphs, where edges exist only at some times.\n",
        "submission_date": "2012-05-09T00:00:00",
        "last_modified_date": "2012-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.4324",
        "title": "Universal Properties of Mythological Networks",
        "authors": [
            "P\u00e1draig Mac Carron",
            "Ralph Kenna"
        ],
        "abstract": "As in statistical physics, the concept of universality plays an important, albeit qualitative, role in the field of comparative mythology. Here we apply statistical mechanical tools to analyse the networks underlying three iconic mythological narratives with a view to identifying common and distinguishing quantitative features. Of the three narratives, an Anglo-Saxon and a Greek text are mostly believed by antiquarians to be partly historically based while the third, an Irish epic, is often considered to be fictional. Here we show that network analysis is able to discriminate real from imaginary social networks and place mythological narratives on the spectrum between them. Moreover, the perceived artificiality of the Irish narrative can be traced back to anomalous features associated with six characters. Considering these as amalgams of several entities or proxies, renders the plausibility of the Irish text comparable to the others from a network-theoretic point of view.\n    ",
        "submission_date": "2012-05-19T00:00:00",
        "last_modified_date": "2012-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1205.6396",
        "title": "Effective Listings of Function Stop words for Twitter",
        "authors": [
            "Murphy Choy"
        ],
        "abstract": "Many words in documents recur very frequently but are essentially meaningless as they are used to join words together in a sentence. It is commonly understood that stop words do not contribute to the context or content of textual documents. Due to their high frequency of occurrence, their presence in text mining presents an obstacle to the understanding of the content in the documents. To eliminate the bias effects, most text mining software or approaches make use of stop words list to identify and remove those words. However, the development of such top words list is difficult and inconsistent between textual sources. This problem is further aggravated by sources such as Twitter which are highly repetitive or similar in nature. In this paper, we will be examining the original work using term frequency, inverse document frequency and term adjacency for developing a stop words list for the Twitter data source. We propose a new technique using combinatorial values as an alternative measure to effectively list out stop words.\n    ",
        "submission_date": "2012-05-29T00:00:00",
        "last_modified_date": "2012-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.1069",
        "title": "Concepts and Their Dynamics: A Quantum-Theoretic Modeling of Human Thought",
        "authors": [
            "Diederik Aerts",
            "Liane Gabora",
            "Sandro Sozzo"
        ],
        "abstract": "We analyze different aspects of our quantum modeling approach of human concepts, and more specifically focus on the quantum effects of contextuality, interference, entanglement and emergence, illustrating how each of them makes its appearance in specific situations of the dynamics of human concepts and their combinations. We point out the relation of our approach, which is based on an ontology of a concept as an entity in a state changing under influence of a context, with the main traditional concept theories, i.e. prototype theory, exemplar theory and theory theory. We ponder about the question why quantum theory performs so well in its modeling of human concepts, and shed light on this question by analyzing the role of complex amplitudes, showing how they allow to describe interference in the statistics of measurement outcomes, while in the traditional theories statistics of outcomes originates in classical probability weights, without the possibility of interference. The relevance of complex numbers, the appearance of entanglement, and the role of Fock space in explaining contextual emergence, all as unique features of the quantum modeling, are explicitly revealed in this paper by analyzing human concepts and their dynamics.\n    ",
        "submission_date": "2012-06-05T00:00:00",
        "last_modified_date": "2013-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3254",
        "title": "Latent Topic Models for Hypertext",
        "authors": [
            "Amit Gruber",
            "Michal Rosen-Zvi",
            "Yair Weiss"
        ],
        "abstract": "Latent topic models have been successfully applied as an unsupervised topic discovery technique in large document collections. With the proliferation of hypertext document collection such as the Internet, there has also been great interest in extending these approaches to hypertext [6, 9]. These approaches typically model links in an analogous fashion to how they model words - the document-link co-occurrence matrix is modeled in the same way that the document-word co-occurrence matrix is modeled in standard topic models. In this paper we present a probabilistic generative model for hypertext document collections that explicitly models the generation of links. Specifically, links from a word w to a document d depend directly on how frequent the topic of w is in d, in addition to the in-degree of d. We show how to perform EM learning on this model efficiently. By not modeling links as analogous to words, we end up using far fewer free parameters and obtain better link prediction results.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.3293",
        "title": "Propagation using Chain Event Graphs",
        "authors": [
            "Peter Thwaites",
            "Jim Q. Smith",
            "Robert G. Cowell"
        ],
        "abstract": "A Chain Event Graph (CEG) is a graphial model which designed to embody conditional independencies in problems whose state spaces are highly asymmetric and do not admit a natural product structure. In this paer we present a probability propagation algorithm which uses the topology of the CEG to build a transporter CEG. Intriungly,the transporter CEG is directly analogous to the triangulated Bayesian Network (BN) in the more conventional junction tree propagation algorithms used with BNs. The propagation method uses factorization formulae also analogous to (but different from) the ones using potentials on cliques and separators of the BN. It appears that the methods will be typically more efficient than the BN algorithms when applied to contexts where there is significant asymmetry present.\n    ",
        "submission_date": "2012-06-13T00:00:00",
        "last_modified_date": "2012-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4631",
        "title": "A Poisson convolution model for characterizing topical content with word frequency and exclusivity",
        "authors": [
            "Edoardo M Airoldi",
            "Jonathan M Bischof"
        ],
        "abstract": "An ongoing challenge in the analysis of document collections is how to summarize content in terms of a set of inferred themes that can be interpreted substantively in terms of topics. The current practice of parametrizing the themes in terms of most frequent words limits interpretability by ignoring the differential use of words across topics. We argue that words that are both common and exclusive to a theme are more effective at characterizing topical content. We consider a setting where professional editors have annotated documents to a collection of topic categories, organized into a tree, in which leaf-nodes correspond to the most specific topics. Each document is annotated to multiple categories, at different levels of the tree. We introduce a hierarchical Poisson convolution model to analyze annotated documents in this setting. The model leverages the structure among categories defined by professional editors to infer a clear semantic description for each topic in terms of words that are both frequent and exclusive. We carry out a large randomized experiment on Amazon Turk to demonstrate that topic summaries based on the FREX score are more interpretable than currently established frequency based summaries, and that the proposed model produces more efficient estimates of exclusivity than with currently models. We also develop a parallelized Hamiltonian Monte Carlo sampler that allows the inference to scale to millions of documents.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4637",
        "title": "Learning to Identify Regular Expressions that Describe Email Campaigns",
        "authors": [
            "Paul Prasse",
            "Christoph Sawade",
            "Niels Landwehr",
            "Tobias Scheffer"
        ],
        "abstract": "This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written to identify the language. This is motivated by our goal of automating the task of postmasters of an email service who use regular expressions to describe and blacklist email spam campaigns. Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist. We model this task as a learning problem with structured output spaces and an appropriate loss function, derive a decoder and the resulting optimization problem, and a report on a case study conducted with an email service.\n    ",
        "submission_date": "2012-06-18T00:00:00",
        "last_modified_date": "2012-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.4958",
        "title": "A Pointillism Approach for Natural Language Processing of Social Media",
        "authors": [
            "Peiyou Song",
            "Anhei Shu",
            "Anyu Zhou",
            "Dan Wallach",
            "Jedidiah R. Crandall"
        ],
        "abstract": "The Chinese language poses challenges for natural language processing based on the unit of a word even for formal uses of the Chinese language, social media only makes word segmentation in Chinese even more difficult. In this document we propose a pointillism approach to natural language processing. Rather than words that have individual meanings, the basic unit of a pointillism approach is trigrams of characters. These grams take on meaning in aggregate when they appear together in a way that is correlated over time.\n",
        "submission_date": "2012-06-21T00:00:00",
        "last_modified_date": "2012-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1206.5851",
        "title": "A meta-analysis of state-of-the-art electoral prediction from Twitter data",
        "authors": [
            "Daniel Gayo-Avello"
        ],
        "abstract": "Electoral prediction from Twitter data is an appealing research topic. It seems relatively straightforward and the prevailing view is overly optimistic. This is problematic because while simple approaches are assumed to be good enough, core problems are not addressed. Thus, this paper aims to (1) provide a balanced and critical review of the state of the art; (2) cast light on the presume predictive power of Twitter data; and (3) depict a roadmap to push forward the field. Hence, a scheme to characterize Twitter prediction methods is proposed. It covers every aspect from data collection to performance evaluation, through data processing and vote inference. Using that scheme, prior research is analyzed and organized to explain the main approaches taken up to date but also their weaknesses. This is the first meta-analysis of the whole body of research regarding electoral prediction from Twitter data. It reveals that its presumed predictive power regarding electoral prediction has been rather exaggerated: although social media may provide a glimpse on electoral outcomes current research does not provide strong evidence to support it can replace traditional polls. Finally, future lines of research along with a set of requirements they must fulfill are provided.\n    ",
        "submission_date": "2012-06-25T00:00:00",
        "last_modified_date": "2012-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0052",
        "title": "The Complexity of Learning Principles and Parameters Grammars",
        "authors": [
            "Jacob Andreas"
        ],
        "abstract": "We investigate models for learning the class of context-free and context-sensitive languages (CFLs and CSLs). We begin with a brief discussion of some early hardness results which show that unrestricted language learning is impossible, and unrestricted CFL learning is computationally infeasible; we then briefly survey the literature on algorithms for learning restricted subclasses of the CFLs. Finally, we introduce a new family of subclasses, the principled parametric context-free grammars (and a corresponding family of principled parametric context-sensitive grammars), which roughly model the \"Principles and Parameters\" framework in psycholinguistics. We present three hardness results: first, that the PPCFGs are not efficiently learnable given equivalence and membership oracles, second, that the PPCFGs are not efficiently learnable from positive presentations unless P = NP, and third, that the PPCSGs are not efficiently learnable from positive presentations unless integer factorization is in P.\n    ",
        "submission_date": "2012-06-30T00:00:00",
        "last_modified_date": "2012-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0658",
        "title": "On the origin of long-range correlations in texts",
        "authors": [
            "Eduardo G. Altmann",
            "Giampaolo Cristadoro",
            "Mirko Degli Esposti"
        ],
        "abstract": "The complexity of human interactions with social and natural phenomena is mirrored in the way we describe our experiences through natural language. In order to retain and convey such a high dimensional information, the statistical properties of our linguistic output has to be highly correlated in time. An example are the robust observations, still largely not understood, of correlations on arbitrary long scales in literary texts. In this paper we explain how long-range correlations flow from highly structured linguistic levels down to the building blocks of a text (words, letters, etc..). By combining calculations and data analysis we show that correlations take form of a bursty sequence of events once we approach the semantically relevant topics of the text. The mechanisms we identify are fairly general and can be equally applied to other hierarchical settings.\n    ",
        "submission_date": "2012-07-03T00:00:00",
        "last_modified_date": "2012-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.0742",
        "title": "The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling",
        "authors": [
            "Marc Dymetman",
            "Guillaume Bouchard",
            "Simon Carter"
        ],
        "abstract": "Most current sampling algorithms for high-dimensional distributions are based on MCMC techniques and are approximate in the sense that they are valid only asymptotically. Rejection sampling, on the other hand, produces valid samples, but is unrealistically slow in high-dimension spaces. The OS* algorithm that we propose is a unified approach to exact optimization and sampling, based on incremental refinements of a functional upper bound, which combines ideas of adaptive rejection sampling and of A* optimization search. We show that the choice of the refinement can be done in a way that ensures tractability in high-dimension spaces, and we present first experiments in two different settings: inference in high-order HMMs and in large discrete graphical models.\n    ",
        "submission_date": "2012-07-03T00:00:00",
        "last_modified_date": "2012-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1207.3169",
        "title": "The law of brevity in macaque vocal communication is not an artifact of analyzing mean call durations",
        "authors": [
            "Stuart Semple",
            "Minna J. Hsu",
            "Govindasamy Agoramoorthy",
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Words follow the law of brevity, i.e. more frequent words tend to be shorter. From a statistical point of view, this qualitative definition of the law states that word length and word frequency are negatively correlated. Here the recent finding of patterning consistent with the law of brevity in Formosan macaque vocal communication (Semple et al., 2010) is revisited. It is shown that the negative correlation between mean duration and frequency of use in the vocalizations of Formosan macaques is not an artifact of the use of a mean duration for each call type instead of the customary 'word' length of studies of the law in human language. The key point demonstrated is that the total duration of calls of a particular type increases with the number of calls of that type. The finding of the law of brevity in the vocalizations of these macaques therefore defies a trivial explanation.\n    ",
        "submission_date": "2012-07-13T00:00:00",
        "last_modified_date": "2012-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.2873",
        "title": "Detecting Events and Patterns in Large-Scale User Generated Textual Streams with Statistical Learning Methods",
        "authors": [
            "Vasileios Lampos"
        ],
        "abstract": "A vast amount of textual web streams is influenced by events or phenomena emerging in the real world. The social web forms an excellent modern paradigm, where unstructured user generated content is published on a regular basis and in most occasions is freely distributed. The present Ph.D. Thesis deals with the problem of inferring information - or patterns in general - about events emerging in real life based on the contents of this textual stream. We show that it is possible to extract valuable information about social phenomena, such as an epidemic or even rainfall rates, by automatic analysis of the content published in Social Media, and in particular Twitter, using Statistical Machine Learning methods. An important intermediate task regards the formation and identification of features which characterise a target event; we select and use those textual features in several linear, non-linear and hybrid inference approaches achieving a significantly good performance in terms of the applied loss function. By examining further this rich data set, we also propose methods for extracting various types of mood signals revealing how affective norms - at least within the social web's population - evolve during the day and how significant events emerging in the real world are influencing them. Lastly, we present some preliminary findings showing several spatiotemporal characteristics of this textual information as well as the potential of using it to tackle tasks such as the prediction of voting intentions.\n    ",
        "submission_date": "2012-08-13T00:00:00",
        "last_modified_date": "2012-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3047",
        "title": "Parallelization of Maximum Entropy POS Tagging for Bahasa Indonesia with MapReduce",
        "authors": [
            "Arif Nurwidyantoro",
            "Edi Winarko"
        ],
        "abstract": "In this paper, MapReduce programming model is used to parallelize training and tagging proceess in Maximum Entropy part of speech tagging for Bahasa Indonesia. In training process, MapReduce model is implemented dictionary, tagtoken, and feature creation. In tagging process, MapReduce is implemented to tag lines of document in parallel. The training experiments showed that total training time using MapReduce is faster, but its result reading time inside the process slow down the total training time. The tagging experiments using different number of map and reduce process showed that MapReduce implementation could speedup the tagging process. The fastest tagging result is showed by tagging process using 1,000,000 word corpus and 30 map process.\n    ",
        "submission_date": "2012-08-15T00:00:00",
        "last_modified_date": "2012-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1208.3530",
        "title": "Leveraging Subjective Human Annotation for Clustering Historic Newspaper Articles",
        "authors": [
            "Haimonti Dutta",
            "William Chan",
            "Deepak Shankargouda",
            "Manoj Pooleery",
            "Axinia Radeva",
            "Kyle Rego",
            "Boyi Xie",
            "Rebecca Passonneau",
            "Austin Lee",
            "Barbara Taranto"
        ],
        "abstract": "The New York Public Library is participating in the Chronicling America initiative to develop an online searchable database of historically significant newspaper articles. Microfilm copies of the newspapers are scanned and high resolution Optical Character Recognition (OCR) software is run on them. The text from the OCR provides a wealth of data and opinion for researchers and historians. However, categorization of articles provided by the OCR engine is rudimentary and a large number of the articles are labeled editorial without further grouping. Manually sorting articles into fine-grained categories is time consuming if not impossible given the size of the corpus. This paper studies techniques for automatic categorization of newspaper articles so as to enhance search and retrieval on the archive. We explore unsupervised (e.g. KMeans) and semi-supervised (e.g. constrained clustering) learning algorithms to develop article categorization schemes geared towards the needs of end-users. A pilot study was designed to understand whether there was unanimous agreement amongst patrons regarding how articles can be categorized. It was found that the task was very subjective and consequently automated algorithms that could deal with subjective labels were used. While the small scale pilot study was extremely helpful in designing machine learning algorithms, a much larger system needs to be developed to collect annotations from users of the archive. The \"BODHI\" system currently being developed is a step in that direction, allowing users to correct wrongly scanned OCR and providing keywords and tags for newspaper articles used frequently. On successful implementation of the beta version of this system, we hope that it can be integrated with existing software being developed for the Chronicling America project.\n    ",
        "submission_date": "2012-08-17T00:00:00",
        "last_modified_date": "2012-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.1751",
        "title": "Information content versus word length in random typing",
        "authors": [
            "Ramon Ferrer-i-Cancho",
            "Ferm\u00edn Moscoso del Prado Mart\u00edn"
        ],
        "abstract": "Recently, it has been claimed that a linear relationship between a measure of information content and word length is expected from word length optimization and it has been shown that this linearity is supported by a strong correlation between information content and word length in many languages (Piantadosi et al. 2011, PNAS 108, 3825-3826). Here, we study in detail some connections between this measure and standard information theory. The relationship between the measure and word length is studied for the popular random typing process where a text is constructed by pressing keys at random from a keyboard containing letters and a space behaving as a word delimiter. Although this random process does not optimize word lengths according to information content, it exhibits a linear relationship between information content and word length. The exact slope and intercept are presented for three major variants of the random typing process. A strong correlation between information content and word length can simply arise from the units making a word (e.g., letters) and not necessarily from the interplay between a word and its context as proposed by Piantadosi et al. In itself, the linear relation does not entail the results of any optimization process.\n    ",
        "submission_date": "2012-09-08T00:00:00",
        "last_modified_date": "2012-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2163",
        "title": "Modeling controversies in the press: the case of the abnormal bees' death",
        "authors": [
            "Alexandre Delano\u00eb",
            "Serge Galam"
        ],
        "abstract": "The controversy about the cause(s) of abnormal death of bee colonies in France is investigated through an extensive analysis of the french speaking press. A statistical analysis of textual data is first performed on the lexicon used by journalists to describe the facts and to present associated informations during the period 1998-2010. Three states are identified to explain the phenomenon. The first state asserts a unique cause, the second one focuses on multifactor causes and the third one states the absence of current proof. Assigning each article to one of the three states, we are able to follow the associated opinion dynamics among the journalists over 13 years. Then, we apply the Galam sequential probabilistic model of opinion dynamic to those data. Assuming journalists are either open mind or inflexible about their respective opinions, the results are reproduced precisely provided we account for a series of annual changes in the proportions of respective inflexibles. The results shed a new counter intuitive light on the various pressure supposed to apply on the journalists by either chemical industries or beekeepers and experts or politicians. The obtained dynamics of respective inflexibles shows the possible effect of lobbying, the inertia of the debate and the net advantage gained by the first whistleblowers.\n    ",
        "submission_date": "2012-09-10T00:00:00",
        "last_modified_date": "2012-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2341",
        "title": "Leveraging Sentiment to Compute Word Similarity",
        "authors": [
            "A.R. Balamurali",
            "Subhabrata Mukherjee",
            "Akshat Malu",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "In this paper, we introduce a new WordNet based similarity metric, SenSim, which incorporates sentiment content (i.e., degree of positive or negative sentiment) of the words being compared to measure the similarity between them. The proposed metric is based on the hypothesis that knowing the sentiment is beneficial in measuring the similarity. To verify this hypothesis, we measure and compare the annotator agreement for 2 annotation strategies: 1) sentiment information of a pair of words is considered while annotating and 2) sentiment information of a pair of words is not considered while annotating. Inter-annotator correlation scores show that the agreement is better when the two annotators consider sentiment information while assigning a similarity score to a pair of words. We use this hypothesis to measure the similarity between a pair of words. Specifically, we represent each word as a vector containing sentiment scores of all the content words in the WordNet gloss of the sense of that word. These sentiment scores are derived from a sentiment lexicon. We then measure the cosine similarity between the two vectors. We perform both intrinsic and extrinsic evaluation of SenSim and compare the performance with other widely usedWordNet similarity metrics.\n    ",
        "submission_date": "2012-09-11T00:00:00",
        "last_modified_date": "2012-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2352",
        "title": "Feature Specific Sentiment Analysis for Product Reviews",
        "authors": [
            "Subhabrata Mukherjee",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "In this paper, we present a novel approach to identify feature specific expressions of opinion in product reviews with different features and mixed emotions. The objective is realized by identifying a set of potential features in the review and extracting opinion expressions about those features by exploiting their associations. Capitalizing on the view that more closely associated words come together to express an opinion about a certain feature, dependency parsing is used to identify relations between the opinion expressions. The system learns the set of significant relations to be used by dependency parsing and a threshold parameter which allows us to merge closely associated opinion expressions. The data requirement is minimal as this is a one time learning of the domain independent parameters. The associations are represented in the form of a graph which is partitioned to finally retrieve the opinion expression describing the user specified feature. We show that the system achieves a high accuracy across all domains and performs at par with state-of-the-art systems despite its data limitations.\n    ",
        "submission_date": "2012-09-11T00:00:00",
        "last_modified_date": "2012-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2493",
        "title": "WikiSent : Weakly Supervised Sentiment Analysis Through Extractive Summarization With Wikipedia",
        "authors": [
            "Subhabrata Mukherjee",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "This paper describes a weakly supervised system for sentiment analysis in the movie review domain. The objective is to classify a movie review into a polarity class, positive or negative, based on those sentences bearing opinion on the movie alone. The irrelevant text, not directly related to the reviewer opinion on the movie, is left out of analysis. Wikipedia incorporates the world knowledge of movie-specific features in the system which is used to obtain an extractive summary of the review, consisting of the reviewer's opinions about the specific aspects of the movie. This filters out the concepts which are irrelevant or objective with respect to the given movie. The proposed system, WikiSent, does not require any labeled data for training. The only weak supervision arises out of the usage of resources like WordNet, Part-of-Speech Tagger and Sentiment Lexicons by virtue of their construction. WikiSent achieves a considerable accuracy improvement over the baseline and has a better or comparable accuracy to the existing semi-supervised and unsupervised systems in the domain, on the same dataset. We also perform a general movie review trend analysis using WikiSent to find the trend in movie-making and the public acceptance in terms of movie genre, year of release and polarity.\n    ",
        "submission_date": "2012-09-12T00:00:00",
        "last_modified_date": "2012-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.2495",
        "title": "TwiSent: A Multistage System for Analyzing Sentiment in Twitter",
        "authors": [
            "Subhabrata Mukherjee",
            "Akshat Malu",
            "A.R. Balamurali",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "In this paper, we present TwiSent, a sentiment analysis system for Twitter. Based on the topic searched, TwiSent collects tweets pertaining to it and categorizes them into the different polarity classes positive, negative and objective. However, analyzing micro-blog posts have many inherent challenges compared to the other text genres. Through TwiSent, we address the problems of 1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomalies in the text in the form of incorrect spellings, nonstandard abbreviations, slangs etc., 3) Entity specificity in the context of the topic searched and 4) Pragmatics embedded in text. The system performance is evaluated on manually annotated gold standard data and on an automatically annotated tweet set based on hashtags. It is a common practise to show the efficacy of a supervised system on an automatically annotated dataset. However, we show that such a system achieves lesser classification accurcy when tested on generic twitter dataset. We also show that our system performs much better than an existing system.\n    ",
        "submission_date": "2012-09-12T00:00:00",
        "last_modified_date": "2012-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.3126",
        "title": "Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic Text Summarization",
        "authors": [
            "Juan-Manuel Torres-Moreno"
        ],
        "abstract": "In Automatic Text Summarization, preprocessing is an important phase to reduce the space of textual representation. Classically, stemming and lemmatization have been widely used for normalizing words. However, even using normalization on large texts, the curse of dimensionality can disturb the performance of summarizers. This paper describes a new method for normalization of words to further reduce the space of representation. We propose to reduce each word to its initial letters, as a form of Ultra-stemming. The results show that Ultra-stemming not only preserve the content of summaries produced by this representation, but often the performances of the systems can be dramatically improved. Summaries on trilingual corpora were evaluated automatically with Fresa. Results confirm an increase in the performance, regardless of summarizer system used.\n    ",
        "submission_date": "2012-09-14T00:00:00",
        "last_modified_date": "2012-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1209.4277",
        "title": "Multi-Level Modeling of Quotation Families Morphogenesis",
        "authors": [
            "Elisa Omodei",
            "Thierry Poibeau",
            "Jean-Philippe Cointet"
        ],
        "abstract": "This paper investigates cultural dynamics in social media by examining the proliferation and diversification of clearly-cut pieces of content: quoted texts. In line with the pioneering work of Leskovec et al. and Simmons et al. on memes dynamics we investigate in deep the transformations that quotations published online undergo during their diffusion. We deliberately put aside the structure of the social network as well as the dynamical patterns pertaining to the diffusion process to focus on the way quotations are changed, how often they are modified and how these changes shape more or less diverse families and sub-families of quotations. Following a biological metaphor, we try to understand in which way mutations can transform quotations at different scales and how mutation rates depend on various properties of the quotations.\n    ",
        "submission_date": "2012-09-19T00:00:00",
        "last_modified_date": "2013-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0794",
        "title": "A Semantic Approach for Automatic Structuring and Analysis of Software Process Patterns",
        "authors": [
            "Nahla Jlaiel",
            "Khouloud Madhbouh",
            "Mohamed Ben Ahmed"
        ],
        "abstract": "The main contribution of this paper, is to propose a novel semantic approach based on a Natural Language Processing technique in order to ensure a semantic unification of unstructured process patterns which are expressed not only in different formats but also, in different forms. This approach is implemented using the GATE text engineering framework and then evaluated leading up to high-quality results motivating us to continue in this direction.\n    ",
        "submission_date": "2012-10-02T00:00:00",
        "last_modified_date": "2012-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.0848",
        "title": "Enhancing Twitter Data Analysis with Simple Semantic Filtering: Example in Tracking Influenza-Like Illnesses",
        "authors": [
            "Son Doan",
            "Lucila Ohno-Machado",
            "Nigel Collier"
        ],
        "abstract": "  Systems that exploit publicly available user generated content such as Twitter messages have been successful in tracking seasonal influenza. We developed a novel filtering method for Influenza-Like-Illnesses (ILI)-related messages using 587 million messages from Twitter micro-blogs. We first filtered messages based on syndrome keywords from the BioCaster Ontology, an extant knowledge model of laymen's terms. We then filtered the messages according to semantic features such as negation, hashtags, emoticons, humor and geography. The data covered 36 weeks for the US 2009 influenza season from 30th August 2009 to 8th May 2010. Results showed that our system achieved the highest Pearson correlation coefficient of 98.46% (p-value<2.2e-16), an improvement of 3.98% over the previous state-of-the-art method. The results indicate that simple NLP-based enhancements to existing approaches to mine Twitter data can increase the value of this inexpensive resource.\n    ",
        "submission_date": "2012-10-02T00:00:00",
        "last_modified_date": "2012-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.3312",
        "title": "Artex is AnotheR TEXt summarizer",
        "authors": [
            "Juan-Manuel Torres-Moreno"
        ],
        "abstract": "This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conferences (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that summarizer achieves interesting results.\n    ",
        "submission_date": "2012-10-11T00:00:00",
        "last_modified_date": "2012-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.4871",
        "title": "Learning Mixtures of Submodular Shells with Application to Document Summarization",
        "authors": [
            "Hui Lin",
            "Jeff A. Bilmes"
        ],
        "abstract": "We introduce a method to learn a mixture of submodular \"shells\" in a large-margin setting. A submodular shell is an abstract submodular function that can be instantiated with a ground set and a set of parameters to produce a submodular function. A mixture of such shells can then also be so instantiated to produce a more complex submodular function. What our algorithm learns are the mixture weights over such shells. We provide a risk bound guarantee when learning in a large-margin structured-prediction setting using a projected subgradient method when only approximate submodular optimization is possible (such as with submodular function maximization). We apply this method to the problem of multi-document summarization and produce the best results reported so far on the widely used NIST DUC-05 through DUC-07 document summarization corpora.\n    ",
        "submission_date": "2012-10-16T00:00:00",
        "last_modified_date": "2012-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7137",
        "title": "Alberti's letter counts",
        "authors": [
            "Bernard Ycart"
        ],
        "abstract": "Four centuries before modern statistical linguistics was born, Leon Battista Alberti (1404--1472) compared the frequency of vowels in Latin poems and orations, making the first quantified observation of a stylistic difference ever. Using a corpus of 20 Latin texts (over 5 million letters), Alberti's observations are statistically assessed. Letter counts prove that poets used significantly more a's, e's, and y's, whereas orators used more of the other vowels. The sample sizes needed to justify the assertions are studied, and proved to be within reach for Alberti's scholarship.\n    ",
        "submission_date": "2012-10-26T00:00:00",
        "last_modified_date": "2012-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1210.7599",
        "title": "The automatic creation of concept maps from documents written using morphologically rich languages",
        "authors": [
            "Krunoslav Zubrinic",
            "Damir Kalpic",
            "Mario Milicevic"
        ],
        "abstract": "Concept map is a graphical tool for representing knowledge. They have been used in many different areas, including education, knowledge management, business and intelligence. Constructing of concept maps manually can be a complex task; an unskilled person may encounter difficulties in determining and positioning concepts relevant to the problem area. An application that recommends concept candidates and their position in a concept map can significantly help the user in that situation. This paper gives an overview of different approaches to automatic and semi-automatic creation of concept maps from textual and non-textual sources. The concept map mining process is defined, and one method suitable for the creation of concept maps from unstructured textual sources in highly inflected languages such as the Croatian language is described in detail. Proposed method uses statistical and data mining techniques enriched with linguistic tools. With minor adjustments, that method can also be used for concept map mining from textual sources in other morphologically rich languages.\n    ",
        "submission_date": "2012-10-29T00:00:00",
        "last_modified_date": "2014-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.3402",
        "title": "Genetic Optimization of Keywords Subset in the Classification Analysis of Texts Authorship",
        "authors": [
            "Bohdan Pavlyshenko"
        ],
        "abstract": "The genetic selection of keywords set, the text frequencies of which are considered as attributes in text classification analysis, has been analyzed. The genetic optimization was performed on a set of words, which is the fraction of the frequency dictionary with given frequency limits. The frequency dictionary was formed on the basis of analyzed text array of texts of English fiction. As the fitness function which is minimized by the genetic algorithm, the error of nearest k neighbors classifier was used. The obtained results show high precision and recall of texts classification by authorship categories on the basis of attributes of keywords set which were selected by the genetic algorithm from the frequency dictionary.\n    ",
        "submission_date": "2012-11-14T00:00:00",
        "last_modified_date": "2012-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.4929",
        "title": "Summarizing Reviews with Variable-length Syntactic Patterns and Topic Models",
        "authors": [
            "Trung V. Nguyen",
            "Alice H. Oh"
        ],
        "abstract": "We present a novel summarization framework for reviews of products and services by selecting informative and concise text segments from the reviews. Our method consists of two major steps. First, we identify five frequently occurring variable-length syntactic patterns and use them to extract candidate segments. Then we use the output of a joint generative sentiment topic model to filter out the non-informative segments. We verify the proposed method with quantitative and qualitative experiments. In a quantitative study, our approach outperforms previous methods in producing informative segments and summaries that capture aspects of products and services as expressed in the user-generated pros and cons lists. Our user study with ninety users resonates with this result: individual segments extracted and filtered by our method are rated as more useful by users compared to previous approaches by users.\n    ",
        "submission_date": "2012-11-21T00:00:00",
        "last_modified_date": "2012-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1211.6847",
        "title": "Letter counting: a stem cell for Cryptology, Quantitative Linguistics, and Statistics",
        "authors": [
            "Bernard Ycart"
        ],
        "abstract": "Counting letters in written texts is a very ancient practice. It has accompanied the development of Cryptology, Quantitative Linguistics, and Statistics. In Cryptology, counting frequencies of the different characters in an encrypted message is the basis of the so called frequency analysis method. In Quantitative Linguistics, the proportion of vowels to consonants in different languages was studied long before authorship attribution. In Statistics, the alternation vowel-consonants was the only example that Markov ever gave of his theory of chained events. A short history of letter counting is presented. The three domains, Cryptology, Quantitative Linguistics, and Statistics, are then examined, focusing on the interactions with the other two fields through letter counting. As a conclusion, the eclectism of past centuries scholars, their background in humanities, and their familiarity with cryptograms, are identified as contributing factors to the mutual enrichment process which is described here.\n    ",
        "submission_date": "2012-11-29T00:00:00",
        "last_modified_date": "2012-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0074",
        "title": "Challenges in Kurdish Text Processing",
        "authors": [
            "Kyumars Sheykh Esmaili"
        ],
        "abstract": "Despite having a large number of speakers, the Kurdish language is among the less-resourced languages. In this work we highlight the challenges and problems in providing the required tools and techniques for processing texts written in Kurdish. From a high-level perspective, the main challenges are: the inherent diversity of the language, standardization and segmentation issues, and the lack of language resources.\n    ",
        "submission_date": "2012-12-01T00:00:00",
        "last_modified_date": "2012-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.0229",
        "title": "Simplification and integration in computing and cognition: the SP theory and the multiple alignment concept",
        "authors": [
            "James Gerard Wolff"
        ],
        "abstract": "The main purpose of this article is to describe potential benefits and applications of the SP theory, a unique attempt to simplify and integrate ideas across artificial intelligence, mainstream computing and human cognition, with information compression as a unifying theme. The theory, including a concept of multiple alignment, combines conceptual simplicity with descriptive and explanatory power in several areas including representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, the storage and retrieval of information, planning and problem solving, unsupervised learning, information compression, and human perception and cognition. In the SP machine -- an expression of the SP theory which is currently realised in the form of computer models -- there is potential for an overall simplification of computing systems, including software. As a theory with a broad base of support, the SP theory promises useful insights in many areas and the integration of structures and functions, both within a given area and amongst different areas. There are potential benefits in natural language processing (with potential for the understanding and translation of natural languages), the need for a versatile intelligence in autonomous robots, computer vision, intelligent databases, maintaining multiple versions of documents or web pages, software engineering, criminal investigations, the management of big data and gaining benefits from it, the semantic web, medical diagnosis, the detection of computer viruses, the economical transmission of data, and data fusion. Further development of these ideas would be facilitated by the creation of a high-parallel, web-based, open-source version of the SP machine, with a good user interface. This would provide a means for researchers to explore what can be done with the system and to refine it.\n    ",
        "submission_date": "2012-12-02T00:00:00",
        "last_modified_date": "2012-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1362",
        "title": "Stochastic model for the vocabulary growth in natural languages",
        "authors": [
            "Martin Gerlach",
            "Eduardo G. Altmann"
        ],
        "abstract": "We propose a stochastic model for the number of different words in a given database which incorporates the dependence on the database size and historical changes. The main feature of our model is the existence of two different classes of words: (i) a finite number of core-words which have higher frequency and do not affect the probability of a new word to be used; and (ii) the remaining virtually infinite number of noncore-words which have lower frequency and once used reduce the probability of a new word to be used in the future. Our model relies on a careful analysis of the google-ngram database of books published in the last centuries and its main consequence is the generalization of Zipf's and Heaps' law to two scaling regimes. We confirm that these generalizations yield the best simple description of the data among generic descriptive models and that the two free parameters depend only on the language but not on the database. From the point of view of our model the main change on historical time scales is the composition of the specific words included in the finite list of core-words, which we observe to decay exponentially in time with a rate of approximately 30 words per year for English.\n    ",
        "submission_date": "2012-12-06T00:00:00",
        "last_modified_date": "2013-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1709",
        "title": "Evolution of the most common English words and phrases over the centuries",
        "authors": [
            "Matjaz Perc"
        ],
        "abstract": "By determining which were the most common English words and phrases since the beginning of the 16th century, we obtain a unique large-scale view of the evolution of written text. We find that the most common words and phrases in any given year had a much shorter popularity lifespan in the 16th than they had in the 20th century. By measuring how their usage propagated across the years, we show that for the past two centuries the process has been governed by linear preferential attachment. Along with the steady growth of the English lexicon, this provides an empirical explanation for the ubiquity of the Zipf's law in language statistics and confirms that writing, although undoubtedly an expression of art and skill, is not immune to the same influences of self-organization that are known to regulate processes as diverse as the making of new friends and World Wide Web growth.\n    ",
        "submission_date": "2012-12-07T00:00:00",
        "last_modified_date": "2012-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.1918",
        "title": "Condens\u00e9s de textes par des m\u00e9thodes num\u00e9riques",
        "authors": [
            "Juan-Manuel Torres-Moreno",
            "Patricia Vel\u00e1zquez-Morales",
            "Jean-Guy Meunier"
        ],
        "abstract": "Since information in electronic form is already a standard, and that the variety and the quantity of information become increasingly large, the methods of summarizing or automatic condensation of texts is a critical phase of the analysis of texts. This article describes CORTEX a system based on numerical methods, which allows obtaining a condensation of a text, which is independent of the topic and of the length of the text. The structure of the system enables it to find the abstracts in French or Spanish in very short times.\n    ",
        "submission_date": "2012-12-09T00:00:00",
        "last_modified_date": "2012-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2145",
        "title": "A Scale-Space Theory for Text",
        "authors": [
            "Shuang-Hong Yang"
        ],
        "abstract": "Scale-space theory has been established primarily by the computer vision and signal processing communities as a well-founded and promising framework for multi-scale processing of signals (e.g., images). By embedding an original signal into a family of gradually coarsen signals parameterized with a continuous scale parameter, it provides a formal framework to capture the structure of a signal at different scales in a consistent way. In this paper, we present a scale space theory for text by integrating semantic and spatial filters, and demonstrate how natural language documents can be understood, processed and analyzed at multiple resolutions, and how this scale-space representation can be used to facilitate a variety of NLP and text analysis tasks.\n    ",
        "submission_date": "2012-12-10T00:00:00",
        "last_modified_date": "2012-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2453",
        "title": "Web-Based Question Answering: A Decision-Making Perspective",
        "authors": [
            "David Azari",
            "Eric J. Horvitz",
            "Susan Dumais",
            "Eric Brill"
        ],
        "abstract": "We describe an investigation of the use of probabilistic models and cost-benefit analyses to guide resource-intensive procedures used by a Web-based question answering system. We first provide an overview of research on question-answering systems. Then, we present details on AskMSR, a prototype web-based question answering system. We discuss Bayesian analyses of the quality of answers generated by the system and show how we can endow the system with the ability to make decisions about the number of queries issued to a search engine, given the cost of queries and the expected value of query results in refining an ultimate answer. Finally, we review the results of a set of experiments.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2477",
        "title": "1 Billion Pages = 1 Million Dollars? Mining the Web to Play \"Who Wants to be a Millionaire?\"",
        "authors": [
            "Shyong",
            "K. Lam",
            "David M Pennock",
            "Dan Cosley",
            "Steve Lawrence"
        ],
        "abstract": "We exploit the redundancy and volume of information on the web to build a     computerized player for the ABC TV game show 'Who Wants To Be A Millionaire?'     The player consists of a question-answering module and a decision-making     module. The question-answering module utilizes question transformation     techniques, natural language parsing, multiple information retrieval     algorithms, and multiple search engines; results are combined in the spirit of     ensemble learning using an adaptive weighting scheme. Empirically, the system     correctly answers about 75% of questions from the Millionaire CD-ROM, 3rd     edition - general-interest trivia questions often about popular culture and     common knowledge. The decision-making module chooses from allowable actions in     the game in order to maximize expected risk-adjusted winnings, where the     estimated probability of answering correctly is a function of past performance     and confidence in in correctly answering the current question. When given a six     question head start (i.e., when starting from the $2,000 level), we find that     the system performs about as well on average as humans starting at the     beginning. Our system demonstrates the potential of simple but well-chosen     techniques for mining answers from unstructured information such as the web.\n    ",
        "submission_date": "2012-10-19T00:00:00",
        "last_modified_date": "2012-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.2616",
        "title": "Languages cool as they expand: Allometric scaling and the decreasing need for new words",
        "authors": [
            "Alexander M. Petersen",
            "Joel N. Tenenbaum",
            "Shlomo Havlin",
            "H. Eugene Stanley",
            "Matjaz Perc"
        ],
        "abstract": "We analyze the occurrence frequencies of over 15 million words recorded in millions of books published during the past two centuries in seven different languages. For all languages and chronological subsets of the data we confirm that two scaling regimes characterize the word frequency distributions, with only the more common words obeying the classic Zipf law. Using corpora of unprecedented size, we test the allometric scaling relation between the corpus size and the vocabulary size of growing languages to demonstrate a decreasing marginal need for new words, a feature that is likely related to the underlying correlations between words. We calculate the annual growth fluctuations of word use which has a decreasing trend as the corpus size increases, indicating a slowdown in linguistic evolution following language expansion. This \"cooling pattern\" forms the basis of a third statistical regularity, which unlike the Zipf and the Heaps law, is dynamical in nature.\n    ",
        "submission_date": "2012-12-11T00:00:00",
        "last_modified_date": "2012-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3023",
        "title": "Keyword Extraction for Identifying Social Actors",
        "authors": [
            "Mahyuddin K. M. Nasution",
            "Shahrul Azman Mohd Noah"
        ],
        "abstract": "Identifying the social actor has become one of tasks in Artificial Intelligence, whereby extracting keyword from Web snippets depend on the use of web is steadily gaining ground in this research. We develop therefore an approach based on overlap principle for utilizing a collection of features in web snippets, where use of keyword will eliminate the un-relevant web pages.\n    ",
        "submission_date": "2012-12-13T00:00:00",
        "last_modified_date": "2012-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.3171",
        "title": "Multifractal analysis of sentence lengths in English literary texts",
        "authors": [
            "Iwona Grabska-Gradzi\u0144ska",
            "Andrzej Kulig",
            "Jaros\u0142aw Kwapie\u0144",
            "Pawe\u0142 O\u015bwi\u0119cimka",
            "Stanis\u0142aw Dro\u017cd\u017c"
        ],
        "abstract": "This paper presents analysis of 30 literary texts written in English by different authors. For each text, there were created time series representing length of sentences in words and analyzed its fractal properties using two methods of multifractal analysis: MFDFA and WTMM. Both methods showed that there are texts which can be considered multifractal in this representation but a majority of texts are not multifractal or even not fractal at all. Out of 30 books, only a few have so-correlated lengths of consecutive sentences that the analyzed signals can be interpreted as real multifractals. An interesting direction for future investigations would be identifying what are the specific features which cause certain texts to be multifractal and other to be monofractal or even not fractal at all.\n    ",
        "submission_date": "2012-12-13T00:00:00",
        "last_modified_date": "2012-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.5238",
        "title": "The Twitter of Babel: Mapping World Languages through Microblogging Platforms",
        "authors": [
            "Delia Mocanu",
            "Andrea Baronchelli",
            "Bruno Gon\u00e7alves",
            "Nicola Perra",
            "Alessandro Vespignani"
        ],
        "abstract": "Large scale analysis and statistics of socio-technical systems that just a few short years ago would have required the use of consistent economic and human resources can nowadays be conveniently performed by mining the enormous amount of digital data produced by human activities. Although a characterization of several aspects of our societies is emerging from the data revolution, a number of questions concerning the reliability and the biases inherent to the big data \"proxies\" of social life are still open. Here, we survey worldwide linguistic indicators and trends through the analysis of a large-scale dataset of microblogging posts. We show that available data allow for the study of language geography at scales ranging from country-level aggregation to specific city neighborhoods. The high resolution and coverage of the data allows us to investigate different indicators such as the linguistic homogeneity of different countries, the touristic seasonal patterns within countries and the geographical distribution of different languages in multilingual regions. This work highlights the potential of geolocalized studies of open data sources to improve current analysis and develop indicators for major social phenomena in specific communities.\n    ",
        "submission_date": "2012-12-20T00:00:00",
        "last_modified_date": "2012-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1212.6527",
        "title": "Discovering Basic Emotion Sets via Semantic Clustering on a Twitter Corpus",
        "authors": [
            "Eugene Yuta Bann"
        ],
        "abstract": "A plethora of words are used to describe the spectrum of human emotions, but how many emotions are there really, and how do they interact? Over the past few decades, several theories of emotion have been proposed, each based around the existence of a set of 'basic emotions', and each supported by an extensive variety of research including studies in facial expression, ethology, neurology and physiology. Here we present research based on a theory that people transmit their understanding of emotions through the language they use surrounding emotion keywords. Using a labelled corpus of over 21,000 tweets, six of the basic emotion sets proposed in existing literature were analysed using Latent Semantic Clustering (LSC), evaluating the distinctiveness of the semantic meaning attached to the emotional label. We hypothesise that the more distinct the language is used to express a certain emotion, then the more distinct the perception (including proprioception) of that emotion is, and thus more 'basic'. This allows us to select the dimensions best representing the entire spectrum of emotion. We find that Ekman's set, arguably the most frequently used for classifying emotions, is in fact the most semantically distinct overall. Next, taking all analysed (that is, previously proposed) emotion terms into account, we determine the optimal semantically irreducible basic emotion set using an iterative LSC algorithm. Our newly-derived set (Accepting, Ashamed, Contempt, Interested, Joyful, Pleased, Sleepy, Stressed) generates a 6.1% increase in distinctiveness over Ekman's set (Angry, Disgusted, Joyful, Sad, Scared). We also demonstrate how using LSC data can help visualise emotions. We introduce the concept of an Emotion Profile and briefly analyse compound emotions both visually and mathematically.\n    ",
        "submission_date": "2012-12-28T00:00:00",
        "last_modified_date": "2012-12-28T00:00:00"
    }
]