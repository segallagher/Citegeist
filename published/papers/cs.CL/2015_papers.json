[
    {
        "url": "https://arxiv.org/abs/1501.00841",
        "title": "Chasing the Ghosts of Ibsen: A computational stylistic analysis of drama in translation",
        "authors": [
            "Gerard Lynch",
            "Carl Vogel"
        ],
        "abstract": "Research into the stylistic properties of translations is an issue which has received some attention in computational stylistics. Previous work by Rybicki (2006) on the distinguishing of character idiolects in the work of Polish author Henryk Sienkiewicz and two corresponding English translations using Burrow's Delta method concluded that idiolectal differences could be observed in the source texts and this variation was preserved to a large degree in both translations. This study also found that the two translations were also highly distinguishable from one another. Burrows (2002) examined English translations of Juvenal also using the Delta method, results of this work suggest that some translators are more adept at concealing their own style when translating the works of another author whereas other authors tend to imprint their own style to a greater extent on the work they translate. Our work examines the writing of a single author, Norwegian playwright Henrik Ibsen, and these writings translated into both German and English from Norwegian, in an attempt to investigate the preservation of characterization, defined here as the distinctiveness of textual contributions of characters.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01243",
        "title": "Un r\u00e9sumeur \u00e0 base de graphes, ind\u00e9p\u00e9ndant de la langue",
        "authors": [
            "Juan-Manuel Torres-Moreno",
            "Javier Ramirez",
            "Iria da Cunha"
        ],
        "abstract": "In this paper we present REG, a graph-based approach for study a fundamental problem of Natural Language Processing (NLP): the automatic text summarization. The algorithm maps a document as a graph, then it computes the weight of their sentences. We have applied this approach to summarize documents in three languages.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01254",
        "title": "Unknown Words Analysis in POS tagging of Sinhala Language",
        "authors": [
            "A.J.P.M.P. Jayaweera",
            "N.G.J. Dias"
        ],
        "abstract": "Part of Speech (POS) is a very vital topic in Natural Language Processing (NLP) task in any language, which involves analysing the construction of the language, behaviours and the dynamics of the language, the knowledge that could be utilized in computational linguistics analysis and automation applications. In this context, dealing with unknown words (words do not appear in the lexicon referred as unknown words) is also an important task, since growing NLP systems are used in more and more new applications. One aid of predicting lexical categories of unknown words is the use of syntactical knowledge of the language. The distinction between open class words and closed class words together with syntactical features of the language used in this research to predict lexical categories of unknown words in the tagging process. An experiment is performed to investigate the ability of the approach to parse unknown words using syntactical knowledge without human intervention. This experiment shows that the performance of the tagging process is enhanced when word class distinction is used together with syntactic rules to parse sentences containing unknown words in Sinhala language.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01386",
        "title": "Roman Urdu Opinion Mining System (RUOMiS)",
        "authors": [
            "Misbah Daud",
            "Rafiullah Khan",
            "Mohibullah",
            "Aitazaz Daud"
        ],
        "abstract": "Convincing a customer is always considered as a challenging task in every business. But when it comes to online business, this task becomes even more difficult. Online retailers try everything possible to gain the trust of the customer. One of the solutions is to provide an area for existing users to leave their comments. This service can effectively develop the trust of the customer however normally the customer comments about the product in their native language using Roman script. If there are hundreds of comments this makes difficulty even for the native customers to make a buying decision. This research proposes a system which extracts the comments posted in Roman Urdu, translate them, find their polarity and then gives us the rating of the product. This rating will help the native and non-native customers to make buying decision efficiently from the comments posted in Roman Urdu.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01866",
        "title": "The Hebrew Bible as Data: Laboratory - Sharing - Experiences",
        "authors": [
            "Dirk Roorda"
        ],
        "abstract": "The systematic study of ancient texts including their production, transmission and interpretation is greatly aided by the digital methods that started taking off in the 1970s. But how is that research in turn transmitted to new generations of researchers? We tell a story of Bible and computer across the decades and then point out the current challenges: (1) finding a stable data representation for changing methods of computation; (2) sharing results in inter- and intra-disciplinary ways, for reproducibility and cross-fertilization. We report recent developments in meeting these challenges. The scene is the text database of the Hebrew Bible, constructed by the Eep Talstra Centre for Bible and Computer (ETCBC), which is still growing in detail and sophistication. We show how a subtle mix of computational ingredients enable scholars to research the transmission and interpretation of the Hebrew Bible in new ways: (1) a standard data format, Linguistic Annotation Framework (LAF); (2) the methods of scientific computing, made accessible by (interactive) Python and its associated ecosystem. Additionally, we show how these efforts have culminated in the construction of a new, publicly accessible search engine SHEBANQ, where the text of the Hebrew Bible and its underlying data can be queried in a simple, yet powerful query language MQL, and where those queries can be saved and shared.\n    ",
        "submission_date": "2015-01-08T00:00:00",
        "last_modified_date": "2015-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01894",
        "title": "Quantifying Scripts: Defining metrics of characters for quantitative and descriptive analysis",
        "authors": [
            "Vinodh Rajan"
        ],
        "abstract": "Analysis of scripts plays an important role in paleography and in quantitative linguistics. Especially in the field of digital paleography quantitative features are much needed to differentiate glyphs. We describe an elaborate set of metrics that quantify qualitative information contained in characters and hence indirectly also quantify the scribal features. We broadly divide the metrics into several categories and describe each individual metric with its underlying qualitative significance. The metrics are largely derived from the related area of gesture design and recognition. We also propose several novel metrics. The proposed metrics are soundly grounded on the principles of handwriting production and handwriting analysis. These computed metrics could serve as descriptors for scripts and also be used for comparing and analyzing scripts. We illustrate some quantitative analysis based on the proposed metrics by applying it to the paleographic evolution of the medieval Tamil script from Brahmi. We also outline future work.\n    ",
        "submission_date": "2015-01-08T00:00:00",
        "last_modified_date": "2015-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02527",
        "title": "Autodetection and Classification of Hidden Cultural City Districts from Yelp Reviews",
        "authors": [
            "Harini Suresh",
            "Nicholas Locascio"
        ],
        "abstract": "Topic models are a way to discover underlying themes in an otherwise unstructured collection of documents. In this study, we specifically used the Latent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to classify restaurants based off of their reviews. Furthermore, we hypothesize that within a city, restaurants can be grouped into similar \"clusters\" based on both location and similarity. We used several different clustering methods, including K-means Clustering and a Probabilistic Mixture Model, in order to uncover and classify districts, both well-known and hidden (i.e. cultural areas like Chinatown or hearsay like \"the best street for Italian restaurants\") within a city. We use these models to display and label different clusters on a map. We also introduce a topic similarity heatmap that displays the similarity distribution in a city to a new restaurant.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02598",
        "title": "Combining Language and Vision with a Multimodal Skip-gram Model",
        "authors": [
            "Angeliki Lazaridou",
            "Nghia The Pham",
            "Marco Baroni"
        ],
        "abstract": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02670",
        "title": "Navigating the Semantic Horizon using Relative Neighborhood Graphs",
        "authors": [
            "Amaru Cuba Gyllensten",
            "Magnus Sahlgren"
        ],
        "abstract": "This paper is concerned with nearest neighbor search in distributional semantic models. A normal nearest neighbor search only returns a ranked list of neighbors, with no information about the structure or topology of the local neighborhood. This is a potentially serious shortcoming of the mode of querying a distributional semantic model, since a ranked list of neighbors may conflate several different senses. We argue that the topology of neighborhoods in semantic space provides important information about the different senses of terms, and that such topological structures can be used for word-sense induction. We also argue that the topology of the neighborhoods in semantic space can be used to determine the semantic horizon of a point, which we define as the set of neighbors that have a direct connection to the point. We introduce relative neighborhood graphs as method to uncover the topological properties of neighborhoods in semantic models. We also provide examples of relative neighborhood graphs for three well-known semantic models; the PMI model, the GloVe model, and the skipgram model.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02714",
        "title": "From Visual Attributes to Adjectives through Decompositional Distributional Semantics",
        "authors": [
            "Angeliki Lazaridou",
            "Georgiana Dinu",
            "Adam Liska",
            "Marco Baroni"
        ],
        "abstract": "As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown...) attracting most attention. By building on the recent \"zero-shot learning\" approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as \"visual phrases\", and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it outperforms various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03191",
        "title": "Annotating Cognates and Etymological Origin in Turkic Languages",
        "authors": [
            "Benjamin S. Mericli",
            "Michael Bloodgood"
        ],
        "abstract": "Turkic languages exhibit extensive and diverse etymological relationships among lexical items. These relationships make the Turkic languages promising for exploring automated translation lexicon induction by leveraging cognate and other etymological information. However, due to the extent and diversity of the types of relationships between words, it is not clear how to annotate such information. In this paper, we present a methodology for annotating cognates and etymological origin in Turkic languages. Our method strives to balance the amount of research effort the annotator expends with the utility of the annotations for supporting research on improving automated translation lexicon induction.\n    ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04324",
        "title": "Phrase Based Language Model For Statistical Machine Translation",
        "authors": [
            "Jia Xu",
            "Geliang Chen"
        ],
        "abstract": "We consider phrase based Language Models (LM), which generalize the commonly used word level models. Similar concept on phrase based LMs appears in speech recognition, which is rather specialized and thus less suitable for machine translation (MT). In contrast to the dependency LM, we first introduce the exhaustive phrase-based LMs tailored for MT use. Preliminary experimental results show that our approach outperform word based LMs with the respect to perplexity and translation quality.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04325",
        "title": "Deep Belief Nets for Topic Modeling",
        "authors": [
            "Lars Maaloe",
            "Morten Arngren",
            "Ole Winther"
        ],
        "abstract": "Applying traditional collaborative filtering to digital publishing is challenging because user data is very sparse due to the high volume of documents relative to the number of users. Content based approaches, on the other hand, is attractive because textual content is often very informative. In this paper we describe large-scale content based collaborative filtering for digital publishing. To solve the digital publishing recommender problem we compare two approaches: latent Dirichlet allocation (LDA) and deep belief nets (DBN) that both find low-dimensional latent representations for documents. Efficient retrieval can be carried out in the latent representation. We work both on public benchmarks and digital media content provided by Issuu, an online publishing platform. This article also comes with a newly developed deep belief nets toolbox for topic modeling tailored towards performance evaluation of the DBN model and comparisons to the LDA model.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05203",
        "title": "Phrase Based Language Model for Statistical Machine Translation: Empirical Study",
        "authors": [
            "Geliang Chen"
        ],
        "abstract": "Reordering is a challenge to machine translation (MT) systems. In MT, the widely used approach is to apply word based language model (LM) which considers the constituent units of a sentence as words. In speech recognition (SR), some phrase based LM have been proposed. However, those LMs are not necessarily suitable or optimal for reordering. We propose two phrase based LMs which considers the constituent units of a sentence as phrases. Experiments show that our phrase based LMs outperform the word based LM with the respect of perplexity and n-best list re-ranking.\n    ",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2015-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05396",
        "title": "Deep Multimodal Learning for Audio-Visual Speech Recognition",
        "authors": [
            "Youssef Mroueh",
            "Etienne Marcheret",
            "Vaibhava Goel"
        ],
        "abstract": "In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of $41\\%$ under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of $35.83\\%$ demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of $34.03\\%$.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07005",
        "title": "Survey:Natural Language Parsing For Indian Languages",
        "authors": [
            "Monika T. Makwana",
            "Deepak C. Vegda"
        ],
        "abstract": "Syntactic parsing is a necessary task which is required for NLP applications including machine translation. It is a challenging task to develop a qualitative parser for morphological rich and agglutinative languages. Syntactic analysis is used to understand the grammatical structure of a natural language sentence. It outputs all the grammatical information of each word and its constituent. Also issues related to it help us to understand the language in a more detailed way. This literature survey is groundwork to understand the different parser development for Indian languages and various approaches that are used to develop such tools and techniques. This paper provides a survey of research papers from well known journals and conferences.\n    ",
        "submission_date": "2015-01-28T00:00:00",
        "last_modified_date": "2015-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00512",
        "title": "Scaling Recurrent Neural Network Language Models",
        "authors": [
            "Will Williams",
            "Niranjani Prasad",
            "David Mrva",
            "Tom Ash",
            "Tony Robinson"
        ],
        "abstract": "This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00831",
        "title": "Open System Categorical Quantum Semantics in Natural Language Processing",
        "authors": [
            "Robin Piedeleu",
            "Dimitri Kartsaklis",
            "Bob Coecke",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "Originally inspired by categorical quantum mechanics (Abramsky and Coecke, LiCS'04), the categorical compositional distributional model of natural language meaning of Coecke, Sadrzadeh and Clark provides a conceptually motivated procedure to compute the meaning of a sentence, given its grammatical structure within a Lambek pregroup and a vectorial representation of the meaning of its parts. The predictions of this first model have outperformed that of other models in mainstream empirical language processing tasks on large scale data. Moreover, just like CQM allows for varying the model in which we interpret quantum axioms, one can also vary the model in which we interpret word meaning.\n",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01245",
        "title": "Authorship recognition via fluctuation analysis of network topology and word intermittency",
        "authors": [
            "Diego R. Amancio"
        ],
        "abstract": "Statistical methods have been widely employed in many practical natural language processing applications. More specifically, complex networks concepts and methods from dynamical systems theory have been successfully applied to recognize stylistic patterns in written texts. Despite the large amount of studies devoted to represent texts with physical models, only a few studies have assessed the relevance of attributes derived from the analysis of stylistic fluctuations. Because fluctuations represent a pivotal factor for characterizing a myriad of real systems, this study focused on the analysis of the properties of stylistic fluctuations in texts via topological analysis of complex networks and intermittency measurements. The results showed that different authors display distinct fluctuation patterns. In particular, it was found that it is possible to identify the authorship of books using the intermittency of specific words. Taken together, the results described here suggest that the patterns found in stylistic fluctuations could be used to analyze other related complex systems. Furthermore, the discovery of novel patterns related to textual stylistic fluctuations indicates that these patterns could be useful to improve the state of the art of many stylistic-based natural language processing tasks.\n    ",
        "submission_date": "2015-02-04T00:00:00",
        "last_modified_date": "2015-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01271",
        "title": "INRIASAC: Simple Hypernym Extraction Methods",
        "authors": [
            "Gregory Grefenstette"
        ],
        "abstract": "Given a set of terms from a given domain, how can we structure them into a taxonomy without manual intervention? This is the task 17 of SemEval 2015. Here we present our simple taxonomy structuring techniques which, despite their simplicity, ranked first in this 2015 benchmark. We use large quantities of text (English Wikipedia) and simple heuristics such as term overlap and document and sentence co-occurrence to produce hypernym lists. We describe these techniques and pre-sent an initial evaluation of results.\n    ",
        "submission_date": "2015-02-04T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01446",
        "title": "Beyond Word-based Language Model in Statistical Machine Translation",
        "authors": [
            "Jiajun Zhang",
            "Shujie Liu",
            "Mu Li",
            "Ming Zhou",
            "Chengqing Zong"
        ],
        "abstract": "Language model is one of the most important modules in statistical machine translation and currently the word-based language model dominants this community. However, many translation models (e.g. phrase-based models) generate the target language sentences by rendering and compositing the phrases rather than the words. Thus, it is much more reasonable to model dependency between phrases, but few research work succeed in solving this problem. In this paper, we tackle this problem by designing a novel phrase-based language model which attempts to solve three key sub-problems: 1, how to define a phrase in language model; 2, how to determine the phrase boundary in the large-scale monolingual data in order to enlarge the training set; 3, how to alleviate the data sparsity problem due to the huge vocabulary size of phrases. By carefully handling these issues, the extensive experiments on Chinese-to-English translation show that our phrase-based language model can significantly improve the translation quality by up to +1.47 absolute BLEU score.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01682",
        "title": "Use of Modality and Negation in Semantically-Informed Syntactic MT",
        "authors": [
            "Kathryn Baker",
            "Michael Bloodgood",
            "Bonnie J. Dorr",
            "Chris Callison-Burch",
            "Nathaniel W. Filardo",
            "Christine Piatko",
            "Lori Levin",
            "Scott Miller"
        ],
        "abstract": "This paper describes the resource- and system-building efforts of an eight-week Johns Hopkins University Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation (SIMT). We describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available) MN lexicon, and two automated MN taggers that we built using the annotation scheme and lexicon. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation) and a holder (an experiencer of modality). We describe how our MN lexicon was semi-automatically produced and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set.\n",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01753",
        "title": "Monitoring Term Drift Based on Semantic Consistency in an Evolving Vector Field",
        "authors": [
            "Peter Wittek",
            "S\u00e1ndor Dar\u00e1nyi",
            "Efstratios Kontopoulos",
            "Theodoros Moysiadis",
            "Ioannis Kompatsiaris"
        ],
        "abstract": "Based on the Aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language, we propose a field approach to lexical analysis. Falling back on the distributional hypothesis to statistically model word meaning, we used evolving fields as a metaphor to express time-dependent changes in a vector space model by a combination of random indexing and evolving self-organizing maps (ESOM). To monitor semantic drifts within the observation period, an experiment was carried out on the term space of a collection of 12.8 million Amazon book reviews. For evaluation, the semantic consistency of ESOM term clusters was compared with their respective neighbourhoods in WordNet, and contrasted with distances among term vectors by random indexing. We found that at 0.05 level of significance, the terms in the clusters showed a high level of semantic consistency. Tracking the drift of distributional patterns in the term space across time periods, we found that consistency decreased, but not at a statistically significant level. Our method is highly scalable, with interpretations in philosophy.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02655",
        "title": "An investigation into language complexity of World-of-Warcraft game-external texts",
        "authors": [
            "Simon \u0160uster"
        ],
        "abstract": "We present a language complexity analysis of World of Warcraft (WoW) community texts, which we compare to texts from a general corpus of web English. Results from several complexity types are presented, including lexical diversity, density, readability and syntactic complexity. The language of WoW texts is found to be comparable to the general corpus on some complexity measures, yet more specialized on other measures. Our findings can be used by educators willing to include game-related activities into school curricula.\n    ",
        "submission_date": "2015-02-07T00:00:00",
        "last_modified_date": "2015-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03322",
        "title": "Boost Phrase-level Polarity Labelling with Review-level Sentiment Classification",
        "authors": [
            "Yongfeng Zhang",
            "Min Zhang",
            "Yiqun Liu",
            "Shaoping Ma"
        ],
        "abstract": "Sentiment analysis on user reviews helps to keep track of user reactions towards products, and make advices to users about what to buy. State-of-the-art review-level sentiment classification techniques could give pretty good precisions of above 90%. However, current phrase-level sentiment analysis approaches might only give sentiment polarity labelling precisions of around 70%~80%, which is far from satisfaction and restricts its application in many practical tasks. In this paper, we focus on the problem of phrase-level sentiment polarity labelling and attempt to bridge the gap between phrase-level and review-level sentiment analysis. We investigate the inconsistency between the numerical star ratings and the sentiment orientation of textual user reviews. Although they have long been treated as identical, which serves as a basic assumption in previous work, we find that this assumption is not necessarily true. We further propose to leverage the results of review-level sentiment classification to boost the performance of phrase-level polarity labelling using a novel constrained convex optimization framework. Besides, the framework is capable of integrating various kinds of information sources and heuristics, while giving the global optimal solution due to its convexity. Experimental results on both English and Chinese reviews show that our framework achieves high labelling precisions of up to 89%, which is a significant improvement from current approaches.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2015-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03671",
        "title": "Phrase-based Image Captioning",
        "authors": [
            "R\u00e9mi Lebret",
            "Pedro O. Pinheiro",
            "Ronan Collobert"
        ],
        "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03682",
        "title": "Applying deep learning techniques on medical corpora from the World Wide Web: a prototypical system and evaluation",
        "authors": [
            "Jose Antonio Mi\u00f1arro-Gim\u00e9nez",
            "Oscar Mar\u00edn-Alonso",
            "Matthias Samwald"
        ],
        "abstract": "BACKGROUND: The amount of biomedical literature is rapidly growing and it is becoming increasingly difficult to keep manually curated knowledge bases and ontologies up-to-date. In this study we applied the word2vec deep learning toolkit to medical corpora to test its potential for identifying relationships from unstructured text. We evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid-sized, unstructured medical text corpora available on the web. Properties included relationships to diseases ('may treat') or physiological processes ('has physiological effect'). We compared the relationships identified by word2vec with manually curated information from the National Drug File - Reference Terminology (NDF-RT) ontology as a gold standard. RESULTS: Our results revealed a maximum accuracy of 49.28% which suggests a limited ability of word2vec to capture linguistic regularities on the collected medical corpora compared with other published results. We were able to document the influence of different parameter settings on result accuracy and found and unexpected trade-off between ranking quality and accuracy. Pre-processing corpora to reduce syntactic variability proved to be a good strategy for increasing the utility of the trained vector models. CONCLUSIONS: Word2vec is a very efficient implementation for computing vector representations and for its ability to identify relationships in textual data without any prior domain knowledge. We found that the ranking and retrieved results generated by word2vec were not of sufficient quality for automatic population of knowledge bases and ontologies, but could serve as a starting point for further manual curation.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03752",
        "title": "A new hybrid metric for verifying parallel corpora of Arabic-English",
        "authors": [
            "Saad Alkahtani",
            "Wei Liu",
            "William J. Teahan"
        ],
        "abstract": "This paper discusses a new metric that has been applied to verify the quality in translation between sentence pairs in parallel corpora of Arabic-English. This metric combines two techniques, one based on sentence length and the other based on compression code length. Experiments on sample test parallel Arabic-English corpora indicate the combination of these two techniques improves accuracy of the identification of satisfactory and unsatisfactory sentence pairs compared to sentence length and compression code length alone. The new method proposed in this research is effective at filtering noise and reducing mis-translations resulting in greatly improved quality.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04174",
        "title": "Probabilistic Models for High-Order Projective Dependency Parsing",
        "authors": [
            "Xuezhe Ma",
            "Hai Zhao"
        ],
        "abstract": "This paper presents generalized probabilistic models for high-order projective dependency parsing and an algorithmic framework for learning these statistical models involving dependency trees. Partition functions and marginals for high-order dependency trees can be computed efficiently, by adapting our algorithms which extend the inside-outside algorithm to higher-order cases. To show the effectiveness of our algorithms, we perform experiments on three languages---English, Chinese and Czech, using maximum conditional likelihood estimation for model training and L-BFGS for parameter estimation. Our methods achieve competitive performance for English, and outperform all previously reported dependency parsers for Chinese and Czech.\n    ",
        "submission_date": "2015-02-14T00:00:00",
        "last_modified_date": "2015-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04938",
        "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena",
        "authors": [
            "Arianna Bisazza",
            "Marcello Federico"
        ],
        "abstract": "Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials. To orientate the reader in this vast and complex research area, we present a comprehensive survey of word reordering viewed as a statistical modeling challenge and as a natural language phenomenon. The survey describes in detail how word reordering is modeled within different string-based and tree-based SMT frameworks and as a stand-alone task, including systematic overviews of the literature in advanced reordering modeling. We then question why some approaches are more successful than others in different language pairs. We argue that, besides measuring the amount of reordering, it is important to understand which kinds of reordering occur in a given language pair. To this end, we conduct a qualitative analysis of word reordering phenomena in a diverse sample of language pairs, based on a large collection of linguistic knowledge. Empirical results in the SMT literature are shown to support the hypothesis that a few linguistic facts can be very useful to anticipate the reordering characteristics of a language pair and to select the SMT framework that best suits them.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06161",
        "title": "Using NLP to measure democracy",
        "authors": [
            "Thiago Marzag\u00e3o"
        ],
        "abstract": "This paper uses natural language processing to create the first machine-coded democracy index, which I call Automated Democracy Scores (ADS). The ADS are based on 42 million news articles from 6,043 different sources and cover all independent countries in the 1993-2012 period. Unlike the democracy indices we have today the ADS are replicable and have standard errors small enough to actually distinguish between cases.\n",
        "submission_date": "2015-02-22T00:00:00",
        "last_modified_date": "2015-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06922",
        "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval",
        "authors": [
            "Hamid Palangi",
            "Li Deng",
            "Yelong Shen",
            "Jianfeng Gao",
            "Xiaodong He",
            "Jianshu Chen",
            "Xinying Song",
            "Rabab Ward"
        ],
        "abstract": "This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms it for web document retrieval task.\n    ",
        "submission_date": "2015-02-24T00:00:00",
        "last_modified_date": "2016-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07038",
        "title": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing",
        "authors": [
            "Dominick Ng",
            "Mohit Bansal",
            "James R. Curran"
        ],
        "abstract": "We develop novel first- and second-order features for dependency parsing based on the Google Syntactic Ngrams corpus, a collection of subtree counts of parsed sentences from scanned books. We also extend previous work on surface $n$-gram features from Web1T to the Google Books corpus and from first-order to second-order, comparing and analysing performance over newswire and web treebanks.\n",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07257",
        "title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram",
        "authors": [
            "Sergey Bartunov",
            "Dmitry Kondrashkin",
            "Anton Osokin",
            "Dmitry Vetrov"
        ],
        "abstract": "Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07504",
        "title": "Rational Kernels for Arabic Stemming and Text Classification",
        "authors": [
            "Attia Nehar",
            "Djelloul Ziadi",
            "Hadda Cherroun"
        ],
        "abstract": "In this paper, we address the problems of Arabic Text Classification and stemming using Transducers and Rational Kernels. We introduce a new stemming technique based on the use of Arabic patterns (Pattern Based Stemmer). Patterns are modelled using transducers and stemming is done without depending on any dictionary. Using transducers for stemming, documents are transformed into finite state transducers. This document representation allows us to use and explore rational kernels as a framework for Arabic Text Classification. Stemming experiments are conducted on three word collections and classification experiments are done on the Saudi Press Agency dataset. Results show that our approach, when compared with other approaches, is promising specially in terms of Accuracy, Recall and F1.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07920",
        "title": "Local Translation Prediction with Global Sentence Representation",
        "authors": [
            "Jiajun Zhang"
        ],
        "abstract": "Statistical machine translation models have made great progress in improving the translation quality. However, the existing models predict the target translation with only the source- and target-side local context information. In practice, distinguishing good translations from bad ones does not only depend on the local features, but also rely on the global sentence-level information. In this paper, we explore the source-side global sentence-level features for target-side local translation prediction. We propose a novel bilingually-constrained chunk-based convolutional neural network to learn sentence semantic representations. With the sentence-level feature representation, we further design a feed-forward neural network to better predict translations using both local and global information. The large-scale experiments show that our method can obtain substantial improvements in translation quality over the strong baseline: the hierarchical phrase-based translation model augmented with the neural network joint model.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00030",
        "title": "Parsing as Reduction",
        "authors": [
            "Daniel Fern\u00e1ndez-Gonz\u00e1lez",
            "Andr\u00e9 F. T. Martins"
        ],
        "abstract": "We reduce phrase-representation parsing to dependency parsing. Our reduction is grounded on a new intermediate representation, \"head-ordered dependency trees\", shown to be isomorphic to constituent trees. By encoding order information in the dependency labels, we show that any off-the-shelf, trainable dependency parser can be used to produce constituents. When this parser is non-projective, we can perform discontinuous parsing in a very natural manner. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with strong baselines, such as the Berkeley parser for English and the best single system in the SPMRL-2014 shared task. Results are particularly striking for discontinuous parsing of German, where we surpass the current state of the art by a wide margin.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00075",
        "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
        "authors": [
            "Kai Sheng Tai",
            "Richard Socher",
            "Christopher D. Manning"
        ],
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00095",
        "title": "Task-Oriented Learning of Word Embeddings for Semantic Relation Classification",
        "authors": [
            "Kazuma Hashimoto",
            "Pontus Stenetorp",
            "Makoto Miwa",
            "Yoshimasa Tsuruoka"
        ],
        "abstract": "We present a novel learning method for word embeddings designed for relation classification. Our word embeddings are trained by predicting words between noun pairs using lexical relation-specific features on a large unlabeled corpus. This allows us to explicitly incorporate relation-specific information into the word embeddings. The learned word embeddings are then used to construct feature vectors for a relation classification model. On a well-established semantic relation classification task, our method significantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models that use syntactic information or manually constructed external resources.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00107",
        "title": "Non-linear Learning for Statistical Machine Translation",
        "authors": [
            "Shujian Huang",
            "Huadong Chen",
            "Xinyu Dai",
            "Jiajun Chen"
        ],
        "abstract": "Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the current data. In this paper, we propose a non-linear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear models. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00168",
        "title": "The NLP Engine: A Universal Turing Machine for NLP",
        "authors": [
            "Jiwei Li",
            "Eduard Hovy"
        ],
        "abstract": "It is commonly accepted that machine translation is a more complex task than part of speech tagging. But how much more complex? In this paper we make an attempt to develop a general framework and methodology for computing the informational and/or processing complexity of NLP applications and tasks. We define a universal framework akin to a Turning Machine that attempts to fit (most) NLP tasks into one paradigm. We calculate the complexities of various NLP tasks using measures of Shannon Entropy, and compare `simple' ones such as part of speech tagging to `complex' ones such as machine translation. This paper provides a first, though far from perfect, attempt to quantify NLP tasks under a uniform paradigm. We point out current deficiencies and suggest some avenues for fruitful research.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00339",
        "title": "Variation of word frequencies in Russian literary texts",
        "authors": [
            "Vladislav Kargin"
        ],
        "abstract": "We study the variation of word frequencies in Russian literary texts. Our findings indicate that the standard deviation of a word's frequency across texts depends on its average frequency according to a power law with exponent $0.62,$ showing that the rarer words have a relatively larger degree of frequency volatility (i.e., \"burstiness\").\n",
        "submission_date": "2015-03-01T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00693",
        "title": "Bayesian Optimization of Text Representations",
        "authors": [
            "Dani Yogatama",
            "Noah A. Smith"
        ],
        "abstract": "When applying machine learning to problems in NLP, there are many choices to make about how to represent input texts. These choices can have a big effect on performance, but they are often uninteresting to researchers or practitioners who simply need a module that performs well. We propose an approach to optimizing over this space of choices, formulating the problem as global optimization. We apply a sequential model-based optimization technique and show that our method makes standard linear models competitive with more sophisticated, expensive state-of-the-art methods based on latent variable models or neural networks on various topic classification and sentiment analysis problems. Our approach is a first step towards black-box NLP systems that work with raw text and do not require manual tuning.\n    ",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2015-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00841",
        "title": "Robustly Leveraging Prior Knowledge in Text Classification",
        "authors": [
            "Biao Liu",
            "Minlie Huang"
        ],
        "abstract": "Prior knowledge has been shown very useful to address many natural language processing tasks. Many approaches have been proposed to formalise a variety of knowledge, however, whether the proposed approach is robust or sensitive to the knowledge supplied to the model has rarely been discussed. In this paper, we propose three regularization terms on top of generalized expectation criteria, and conduct extensive experiments to justify the robustness of the proposed methods. Experimental results demonstrate that our proposed methods obtain remarkable improvements and are much more robust than baselines.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01129",
        "title": "Complexity and universality in the long-range order of words",
        "authors": [
            "Marcelo A Montemurro",
            "Dami\u00e1n H Zanette"
        ],
        "abstract": "As is the case of many signals produced by complex systems, language presents a statistical structure that is balanced between order and disorder. Here we review and extend recent results from quantitative characterisations of the degree of order in linguistic sequences that give insights into two relevant aspects of language: the presence of statistical universals in word ordering, and the link between semantic information and the statistical linguistic structure. We first analyse a measure of relative entropy that assesses how much the ordering of words contributes to the overall statistical structure of language. This measure presents an almost constant value close to 3.5 bits/word across several linguistic families. Then, we show that a direct application of information theory leads to an entropy measure that can quantify and extract semantic structures from linguistic samples, even without prior knowledge of the underlying language.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01190",
        "title": "Statistical modality tagging from rule-based annotations and crowdsourcing",
        "authors": [
            "Vinodkumar Prabhakaran",
            "Michael Bloodgood",
            "Mona Diab",
            "Bonnie Dorr",
            "Lori Levin",
            "Christine D. Piatko",
            "Owen Rambow",
            "Benjamin Van Durme"
        ],
        "abstract": "We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01558",
        "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision",
        "authors": [
            "Jonathan Malmaud",
            "Jonathan Huang",
            "Vivek Rathod",
            "Nick Johnston",
            "Andrew Rabinovich",
            "Kevin Murphy"
        ],
        "abstract": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01655",
        "title": "Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation",
        "authors": [
            "Eneko Agirre",
            "Ander Barrena",
            "Aitor Soroa"
        ],
        "abstract": "Hyperlinks and other relations in Wikipedia are a extraordinary resource which is still not fully understood. In this paper we study the different types of links in Wikipedia, and contrast the use of the full graph with respect to just direct links. We apply a well-known random walk algorithm on two tasks, word relatedness and named-entity disambiguation. We show that using the full graph is more effective than just direct links by a large margin, that non-reciprocal links harm performance, and that there is no benefit from categories and infoboxes, with coherent results on both tasks. We set new state-of-the-art figures for systems based on Wikipedia links, comparable to systems exploiting several information sources and/or supervised machine learning. Our approach is open source, with instruction to reproduce results, and amenable to be integrated with complementary text-based methods.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01838",
        "title": "Encoding Source Language with Convolutional Neural Network for Machine Translation",
        "authors": [
            "Fandong Meng",
            "Zhengdong Lu",
            "Mingxuan Wang",
            "Hang Li",
            "Wenbin Jiang",
            "Qun Liu"
        ],
        "abstract": "The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02120",
        "title": "Identifying missing dictionary entries with frequency-conserving context models",
        "authors": [
            "Jake Ryland Williams",
            "Eric M. Clark",
            "James P. Bagrow",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "In an effort to better understand meaning from natural language texts, we explore methods aimed at organizing lexical objects into contexts. A number of these methods for organization fall into a family defined by word ordering. Unlike demographic or spatial partitions of data, these collocation models are of special importance for their universal applicability. While we are interested here in text and have framed our treatment appropriately, our work is potentially applicable to other areas of research (e.g., speech, genomics, and mobility patterns) where one has ordered categorical data, (e.g., sounds, genes, and locations). Our approach focuses on the phrase (whether word or larger) as the primary meaning-bearing lexical unit and object of study. To do so, we employ our previously developed framework for generating word-conserving phrase-frequency data. Upon training our model with the Wiktionary---an extensive, online, collaborative, and open-source dictionary that contains over 100,000 phrasal-definitions---we develop highly effective filters for the identification of meaningful, missing phrase-entries. With our predictions we then engage the editorial community of the Wiktionary and propose short lists of potential missing entries for definition, developing a breakthrough, lexical extraction technique, and expanding our knowledge of the defined English lexicon of phrases.\n    ",
        "submission_date": "2015-03-07T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02335",
        "title": "An Unsupervised Method for Uncovering Morphological Chains",
        "authors": [
            "Karthik Narasimhan",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ],
        "abstract": "Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.\n    ",
        "submission_date": "2015-03-08T00:00:00",
        "last_modified_date": "2015-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02357",
        "title": "Context-Dependent Translation Selection Using Convolutional Neural Network",
        "authors": [
            "Zhaopeng Tu",
            "Baotian Hu",
            "Zhengdong Lu",
            "Hang Li"
        ],
        "abstract": "We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02364",
        "title": "Neural Responding Machine for Short-Text Conversation",
        "authors": [
            "Lifeng Shang",
            "Zhengdong Lu",
            "Hang Li"
        ],
        "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02427",
        "title": "Syntax-based Deep Matching of Short Texts",
        "authors": [
            "Mingxuan Wang",
            "Zhengdong Lu",
            "Hang Li",
            "Qun Liu"
        ],
        "abstract": "Many tasks in natural language processing, ranging from machine translation to question answering, can be reduced to the problem of matching two sentences or more generally two short texts. We propose a new approach to the problem, called Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The approach consists of two components, 1) a mining algorithm to discover patterns for matching two short-texts, defined in the product space of dependency trees, and 2) a deep neural network for matching short texts using the mined patterns, as well as a learning algorithm to build the network having a sparse structure. We test our algorithm on the problem of matching a tweet and a response in social media, a hard matching problem proposed in [Wang et al., 2013], and show that DeepMatch$_{tree}$ can outperform a number of competitor models including one without using dependency trees and one based on word-embedding, all with large margins\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02510",
        "title": "Compositional Distributional Semantics with Long Short Term Memory",
        "authors": [
            "Phong Le",
            "Willem Zuidema"
        ],
        "abstract": "We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the `memory cell') and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03244",
        "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences",
        "authors": [
            "Baotian Hu",
            "Zhengdong Lu",
            "Hang Li",
            "Qingcai Chen"
        ],
        "abstract": "Semantic matching is of central importance to many natural language tasks \\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03512",
        "title": "Is language evolution grinding to a halt? The scaling of lexical turbulence in English fiction suggests it is not",
        "authors": [
            "Eitan Adam Pechenick",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "Of basic interest is the quantification of the long term growth of a language's lexicon as it develops to more completely cover both a culture's communication requirements and knowledge space. Here, we explore the usage dynamics of words in the English language as reflected by the Google Books 2012 English Fiction corpus. We critique an earlier method that found decreasing birth and increasing death rates of words over the second half of the 20th Century, showing death rates to be strongly affected by the imposed time cutoff of the arbitrary present and not increasing dramatically. We provide a robust, principled approach to examining lexical evolution by tracking the volume of word flux across various relative frequency thresholds. We show that while the overall statistical structure of the English language remains stable over time in terms of its raw Zipf distribution, we find evidence of an enduring `lexical turbulence': The flux of words across frequency thresholds from decade to decade scales superlinearly with word rank and exhibits a scaling break we connect to that of Zipf's law. To better understand the changing lexicon, we examine the contributions to the Jensen-Shannon divergence of individual words crossing frequency thresholds. We also find indications that scholarly works about fiction are strongly represented in the 2012 English Fiction corpus, and suggest that a future revision of the corpus should attempt to separate critical works from fiction itself.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2017-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03535",
        "title": "On Using Monolingual Corpora in Neural Machine Translation",
        "authors": [
            "Caglar Gulcehre",
            "Orhan Firat",
            "Kelvin Xu",
            "Kyunghyun Cho",
            "Loic Barrault",
            "Huei-Chi Lin",
            "Fethi Bougares",
            "Holger Schwenk",
            "Yoshua Bengio"
        ],
        "abstract": "Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03989",
        "title": "An implementation of Apertium based Assamese morphological analyzer",
        "authors": [
            "Mirzanur Rahman",
            "Shikhar Kumar Sarma"
        ],
        "abstract": "Morphological Analysis is an important branch of linguistics for any Natural Language Processing Technology. Morphology studies the word structure and formation of word of a language. In current scenario of NLP research, morphological analysis techniques have become more popular day by day. For processing any language, morphology of the word should be first analyzed. Assamese language contains very complex morphological structure. In our work we have used Apertium based Finite-State-Transducers for developing morphological analyzer for Assamese Language with some limited domain and we get 72.7% accuracy\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04881",
        "title": "Long Short-Term Memory Over Tree Structures",
        "authors": [
            "Xiaodan Zhu",
            "Parinaz Sobhani",
            "Hongyu Guo"
        ],
        "abstract": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.\n    ",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05034",
        "title": "$gen$CNN: A Convolutional Architecture for Word Sequence Prediction",
        "authors": [
            "Mingxuan Wang",
            "Zhengdong Lu",
            "Hang Li",
            "Wenbin Jiang",
            "Qun Liu"
        ],
        "abstract": "We propose a novel convolutional architecture, named $gen$CNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and $n$-best re-ranking in machine translation show that $gen$CNN outperforms the state-of-the-arts with big margins.\n    ",
        "submission_date": "2015-03-17T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05123",
        "title": "Prediction Using Note Text: Synthetic Feature Creation with word2vec",
        "authors": [
            "Manuel Amunategui",
            "Tristan Markwell",
            "Yelena Rozenfeld"
        ],
        "abstract": "word2vec affords a simple yet powerful approach of extracting quantitative variables from unstructured textual data. Over half of healthcare data is unstructured and therefore hard to model without involved expertise in data engineering and natural language processing. word2vec can serve as a bridge to quickly gather intelligence from such data sources.\n",
        "submission_date": "2015-03-17T00:00:00",
        "last_modified_date": "2015-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05543",
        "title": "Text Segmentation based on Semantic Word Embeddings",
        "authors": [
            "Alexander A Alemi",
            "Paul Ginsparg"
        ],
        "abstract": "We explore the use of semantic word embeddings in text segmentation algorithms, including the C99 segmentation algorithm and new algorithms inspired by the distributed word vector representation. By developing a general framework for discussing a class of segmentation objectives, we study the effectiveness of greedy versus exact optimization approaches and suggest a new iterative refinement technique for improving the performance of greedy strategies. We compare our results to known benchmarks, using known metrics. We demonstrate state-of-the-art performance for an untrained method with our Content Vector Segmentation (CVS) on the Choi test set. Finally, we apply the segmentation procedure to an in-the-wild dataset consisting of text extracted from scholarly articles in the ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05615",
        "title": "Learning to Search for Dependencies",
        "authors": [
            "Kai-Wei Chang",
            "He He",
            "Hal Daum\u00e9 III",
            "John Langford"
        ],
        "abstract": "We demonstrate that a dependency parser can be built using a credit assignment compiler which removes the burden of worrying about low-level machine learning details from the parser implementation. The result is a simple parser which robustly applies to many languages that provides similar statistical and computational performance with best-to-date transition-based parsing approaches, while avoiding various downsides including randomization, extra feature requirements, and custom learning algorithms.\n    ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05626",
        "title": "Phrase database Approach to structural and semantic disambiguation in English-Korean Machine Translation",
        "authors": [
            "Myong-Chol Pak"
        ],
        "abstract": "In machine translation it is common phenomenon that machine-readable dictionaries and standard parsing rules are not enough to ensure accuracy in parsing and translating English phrases into Korean language, which is revealed in misleading translation results due to consequent structural and semantic ambiguities. This paper aims to suggest a solution to structural and semantic ambiguities due to the idiomaticity and non-grammaticalness of phrases commonly used in English language by applying bilingual phrase database in English-Korean Machine Translation (EKMT). This paper firstly clarifies what the phrase unit in EKMT is based on the definition of the English phrase, secondly clarifies what kind of language unit can be the target of the phrase database for EKMT, thirdly suggests a way to build the phrase database by presenting the format of the phrase database with examples, and finally discusses briefly the method to apply this bilingual phrase database to the EKMT for structural and semantic disambiguation.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05907",
        "title": "Syntagma Lexical Database",
        "authors": [
            "Daniel Christen"
        ],
        "abstract": "This paper discusses the structure of Syntagma's Lexical Database (focused on Italian). The basic database consists in four tables. Table Forms contains word inflections, used by the POS-tagger for the identification of input-words. Forms is related to Lemma. Table Lemma stores all kinds of grammatical features of words, word-level semantic data and restrictions. In the table Meanings meaning-related data are stored: definition, examples, domain, and semantic information. Table Valency contains the argument structure of each meaning, with syntactic and semantic features for each argument. The extended version of SLD contains the links to Syntagma's Semantic Net and to the WordNet synsets of other languages.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06151",
        "title": "On measuring linguistic intelligence",
        "authors": [
            "Maxim Litvak"
        ],
        "abstract": "This work addresses the problem of measuring how many languages a person \"effectively\" speaks given that some of the languages are close to each other. In other words, to assign a meaningful number to her language portfolio. Intuition says that someone who speaks fluently Spanish and Portuguese is linguistically less proficient compared to someone who speaks fluently Spanish and Chinese since it takes more effort for a native Spanish speaker to learn Chinese than Portuguese. As the number of languages grows and their proficiency levels vary, it gets even more complicated to assign a score to a language portfolio. In this article we propose such a measure (\"linguistic quotient\" - LQ) that can account for these effects.\n",
        "submission_date": "2015-03-20T00:00:00",
        "last_modified_date": "2015-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06450",
        "title": "Multilingual Open Relation Extraction Using Cross-lingual Projection",
        "authors": [
            "Manaal Faruqui",
            "Shankar Kumar"
        ],
        "abstract": "Open domain relation extraction systems identify relation and argument phrases in a sentence without relying on any underlying schema. However, current state-of-the-art relation extraction systems are available only for English because of their heavy reliance on linguistic tools such as part-of-speech taggers and dependency parsers. We present a cross-lingual annotation projection method for language independent relation extraction. We evaluate our method on a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in 61 languages from Wikipedia.\n    ",
        "submission_date": "2015-03-22T00:00:00",
        "last_modified_date": "2021-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06733",
        "title": "Yara Parser: A Fast and Accurate Dependency Parser",
        "authors": [
            "Mohammad Sadegh Rasooli",
            "Joel Tetreault"
        ],
        "abstract": "Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06760",
        "title": "Unsupervised POS Induction with Word Embeddings",
        "authors": [
            "Chu-Cheng Lin",
            "Waleed Ammar",
            "Chris Dyer",
            "Lori Levin"
        ],
        "abstract": "Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on \"downstream\" POS induction results.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07283",
        "title": "Morphological Analyzer and Generator for Russian and Ukrainian Languages",
        "authors": [
            "Mikhail Korobov"
        ],
        "abstract": "pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian languages. It uses large efficiently encoded lexi- cons built from OpenCorpora and LanguageTool data. A set of linguistically motivated rules is developed to enable morphological analysis and generation of out-of-vocabulary words observed in real-world documents. For Russian pymorphy2 provides state-of-the-arts morphological analysis quality. The analyzer is implemented in Python programming language with optional C++ extensions. Emphasis is put on ease of use, documentation and extensibility. The package is distributed under a permissive open-source license, encouraging its use in both academic and commercial setting.\n    ",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07294",
        "title": "Using Latent Semantic Analysis to Identify Quality in Use (QU) Indicators from User Reviews",
        "authors": [
            "Wendy Tan Wei Syn",
            "Bong Chih How",
            "Issa Atoum"
        ],
        "abstract": "The paper describes a novel approach to categorize users' reviews according to the three Quality in Use (QU) indicators defined in ISO: effectiveness, efficiency and freedom from risk. With the tremendous amount of reviews published each day, there is a need to automatically summarize user reviews to inform us if any of the software able to meet requirement of a company according to the quality requirements. We implemented the method of Latent Semantic Analysis (LSA) and its subspace to predict QU indicators. We build a reduced dimensionality universal semantic space from Information System journals and Amazon reviews. Next, we projected set of indicators' measurement scales into the universal semantic space and represent them as subspace. In the subspace, we can map similar measurement scales to the unseen reviews and predict the QU indicators. Our preliminary study able to obtain the average of F-measure, 0.3627.\n    ",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07613",
        "title": "Unsupervised authorship attribution",
        "authors": [
            "David Fifield",
            "Torbj\u00f8rn Follan",
            "Emil Lunde"
        ],
        "abstract": "We describe a technique for attributing parts of a written text to a set of unknown authors. Nothing is assumed to be known a priori about the writing styles of potential authors. We use multiple independent clusterings of an input text to identify parts that are similar and dissimilar to one another. We describe algorithms necessary to combine the multiple clusterings into a meaningful output. We show results of the application of the technique on texts having multiple writing styles.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08167",
        "title": "Normalization of Non-Standard Words in Croatian Texts",
        "authors": [
            "Slobodan Beliga",
            "Miran Pobar",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107"
        ],
        "abstract": "This paper presents text normalization which is an integral part of any text-to-speech synthesis system. Text normalization is a set of methods with a task to write non-standard words, like numbers, dates, times, abbreviations, acronyms and the most common symbols, in their full expanded form are presented. The whole taxonomy for classification of non-standard words in Croatian language together with rule-based normalization methods combined with a lookup dictionary are proposed. Achieved token rate for normalization of Croatian texts is 95%, where 80% of expanded words are in correct morphological form.\n    ",
        "submission_date": "2015-03-27T00:00:00",
        "last_modified_date": "2015-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.09144",
        "title": "Towards Using Machine Translation Techniques to Induce Multilingual Lexica of Discourse Markers",
        "authors": [
            "Ant\u00f3nio Lopes",
            "David Martins de Matos",
            "Vera Cabarr\u00e3o",
            "Ricardo Ribeiro",
            "Helena Moniz",
            "Isabel Trancoso",
            "Ana Isabel Mata"
        ],
        "abstract": "Discourse markers are universal linguistic events subject to language variation. Although an extensive literature has already reported language specific traits of these events, little has been said on their cross-language behavior and on building an inventory of multilingual lexica of discourse markers. This work describes new methods and approaches for the description, classification, and annotation of discourse markers in the specific domain of the Europarl corpus. The study of discourse markers in the context of translation is crucial due to the idiomatic nature of these structures. Multilingual lexica together with the functional analysis of such structures are useful tools for the hard task of translating discourse markers into possible equivalents from one language to another. Using Daniel Marcu's validated discourse markers for English, extracted from the Brown Corpus, our purpose is to build multilingual lexica of discourse markers for other languages, based on machine translation techniques. The major assumption in this study is that the usage of a discourse marker is independent of the language, i.e., the rhetorical function of a discourse marker in a sentence in one language is equivalent to the rhetorical function of the same discourse marker in another language.\n    ",
        "submission_date": "2015-03-31T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00548",
        "title": "Learning to Understand Phrases by Embedding the Dictionary",
        "authors": [
            "Felix Hill",
            "Kyunghyun Cho",
            "Anna Korhonen",
            "Yoshua Bengio"
        ],
        "abstract": "Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: \"reverse dictionaries\" that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences.\n    ",
        "submission_date": "2015-04-02T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00923",
        "title": "A Unified Deep Neural Network for Speaker and Language Recognition",
        "authors": [
            "Fred Richardson",
            "Douglas Reynolds",
            "Najim Dehak"
        ],
        "abstract": "Learned feature representations and sub-phoneme posteriors from Deep Neural Networks (DNNs) have been used separately to produce significant performance gains for speaker and language recognition tasks. In this work we show how these gains are possible using a single DNN for both speaker and language recognition. The unified DNN approach is shown to yield substantial performance improvements on the the 2013 Domain Adaptation Challenge speaker recognition task (55% reduction in EER for the out-of-domain condition) and on the NIST 2011 Language Recognition Evaluation (48% reduction in EER for the 30s test condition).\n    ",
        "submission_date": "2015-04-03T00:00:00",
        "last_modified_date": "2015-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01106",
        "title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution",
        "authors": [
            "Lili Mou",
            "Hao Peng",
            "Ge Li",
            "Yan Xu",
            "Lu Zhang",
            "Zhi Jin"
        ],
        "abstract": "This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling. Our models leverage either constituency trees or dependency trees of sentences. The tree-based convolution process extracts sentences' structural features, and these features are aggregated by max pooling. Such architecture allows short propagation paths between the output layer and underlying feature detectors, which enables effective structural feature learning and extraction. We evaluate our models on two tasks: sentiment analysis and question classification. In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.\n    ",
        "submission_date": "2015-04-05T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01182",
        "title": "Bengali to Assamese Statistical Machine Translation using Moses (Corpus Based)",
        "authors": [
            "Nayan Jyoti Kalita",
            "Baharul Islam"
        ],
        "abstract": "Machine dialect interpretation assumes a real part in encouraging man-machine correspondence and in addition men-men correspondence in Natural Language Processing (NLP). Machine Translation (MT) alludes to utilizing machine to change one dialect to an alternate. Statistical Machine Translation is a type of MT consisting of Language Model (LM), Translation Model (TM) and decoder. In this paper, Bengali to Assamese Statistical Machine Translation Model has been created by utilizing Moses. Other translation tools like IRSTLM for Language Model and GIZA-PP-V1.0.7 for Translation model are utilized within this framework which is accessible in Linux situations. The purpose of the LM is to encourage fluent output and the purpose of TM is to encourage similarity between input and output, the decoder increases the probability of translated text in target language. A parallel corpus of 17100 sentences in Bengali and Assamese has been utilized for preparing within this framework. Measurable MT procedures have not so far been generally investigated for Indian dialects. It might be intriguing to discover to what degree these models can help the immense continuous MT deliberations in the nation.\n    ",
        "submission_date": "2015-04-06T00:00:00",
        "last_modified_date": "2015-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01383",
        "title": "QUOTUS: The Structure of Political Media Coverage as Revealed by Quoting Patterns",
        "authors": [
            "Vlad Niculae",
            "Caroline Suen",
            "Justine Zhang",
            "Cristian Danescu-Niculescu-Mizil",
            "Jure Leskovec"
        ],
        "abstract": "Given the extremely large pool of events and stories available, media outlets need to focus on a subset of issues and aspects to convey to their audience. Outlets are often accused of exhibiting a systematic bias in this selection process, with different outlets portraying different versions of reality. However, in the absence of objective measures and empirical evidence, the direction and extent of systematicity remains widely disputed.\n",
        "submission_date": "2015-04-06T00:00:00",
        "last_modified_date": "2015-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01427",
        "title": "A Metric to Classify Style of Spoken Speech",
        "authors": [
            "Sunil Kopparapu",
            "Saurabh Bhatnagar",
            "K. Sahana",
            "Sathyanarayana",
            "Akhilesh Srivastava",
            "P.V.S. Rao"
        ],
        "abstract": "The ability to classify spoken speech based on the style of speaking is an important problem. With the advent of BPO's in recent times, specifically those that cater to a population other than the local population, it has become necessary for BPO's to identify people with certain style of speaking (American, British etc). Today BPO's employ accent analysts to identify people having the required style of speaking. This process while involving human bias, it is becoming increasingly infeasible because of the high attrition rate in the BPO industry. In this paper, we propose a new metric, which robustly and accurately helps classify spoken speech based on the style of speaking. The role of the proposed metric is substantiated by using it to classify real speech data collected from over seventy different people working in a BPO. We compare the performance of the metric against human experts who independently carried out the classification process. Experimental results show that the performance of the system using the novel metric performs better than two different human expert.\n    ",
        "submission_date": "2015-04-06T00:00:00",
        "last_modified_date": "2015-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01496",
        "title": "Voice based self help System: User Experience Vs Accuracy",
        "authors": [
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "In general, self help systems are being increasingly deployed by service based industries because they are capable of delivering better customer service and increasingly the switch is to voice based self help systems because they provide a natural interface for a human to interact with a machine. A speech based self help system ideally needs a speech recognition engine to convert spoken speech to text and in addition a language processing engine to take care of any misrecognitions by the speech recognition engine. Any off-the-shelf speech recognition engine is generally a combination of acoustic processing and speech grammar. While this is the norm, we believe that ideally a speech recognition application should have in addition to a speech recognition engine a separate language processing engine to give the system better performance. In this paper, we discuss ways in which the speech recognition engine and the language processing engine can be combined to give a better user experience.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01683",
        "title": "Jointly Embedding Relations and Mentions for Knowledge Population",
        "authors": [
            "Miao Fan",
            "Kai Cao",
            "Yifan He",
            "Ralph Grishman"
        ],
        "abstract": "This paper contributes a joint embedding model for predicting relations between a pair of entities in the scenario of relation inference. It differs from most stand-alone approaches which separately operate on either knowledge bases or free texts. The proposed model simultaneously learns low-dimensional vector representations for both triplets in knowledge repositories and the mentions of relations in free texts, so that we can leverage the evidence both resources to make more accurate predictions. We use NELL to evaluate the performance of our approach, compared with cutting-edge methods. Results of extensive experiments show that our model achieves significant improvement on relation extraction.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02148",
        "title": "Mining and discovering biographical information in Difangzhi with a language-model-based approach",
        "authors": [
            "Peter K. Bol",
            "Chao-Lin Liu",
            "Hongsu Wang"
        ],
        "abstract": "We present results of expanding the contents of the China Biographical Database by text mining historical local gazetteers, difangzhi. The goal of the database is to see how people are connected together, through kinship, social connections, and the places and offices in which they served. The gazetteers are the single most important collection of names and offices covering the Song through Qing periods. Although we begin with local officials we shall eventually include lists of local examination candidates, people from the locality who served in government, and notable local figures with biographies. The more data we collect the more connections emerge. The value of doing systematic text mining work is that we can identify relevant connections that are either directly informative or can become useful without deep historical research. Academia Sinica is developing a name database for officials in the central governments of the Ming and Qing dynasties.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02150",
        "title": "Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual Entailment in NTCIR RITE Evaluation Tasks",
        "authors": [
            "Wei-Jie Huang",
            "Chao-Lin Liu"
        ],
        "abstract": "We computed linguistic information at the lexical, syntactic, and semantic levels for Recognizing Inference in Text (RITE) tasks for both traditional and simplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing, named-entity recognition, and near synonym recognition were employed, and features like counts of common words, statement lengths, negation words, and antonyms were considered to judge the entailment relationships of two statements, while we explored both heuristics-based functions and machine-learning approaches. The reported systems showed robustness by simultaneously achieving second positions in the binary-classification subtasks for both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted more experiments with the test data of NTCIR-9 RITE, with good results. We also extended our work to search for better configurations of our classifiers and investigated contributions of individual features. This extended work showed interesting results and should encourage further discussion.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02162",
        "title": "Concentric network symmetry grasps authors' styles in word adjacency networks",
        "authors": [
            "Diego R. Amancio",
            "Filipi N. Silva",
            "Luciano da F. Costa"
        ],
        "abstract": "Several characteristics of written texts have been inferred from statistical analysis derived from networked models. Even though many network measurements have been adapted to study textual properties at several levels of complexity, some textual aspects have been disregarded. In this paper, we study the symmetry of word adjacency networks, a well-known representation of text as a graph. A statistical analysis of the symmetry distribution performed in several novels showed that most of the words do not display symmetric patterns of connectivity. More specifically, the merged symmetry displayed a distribution similar to the ubiquitous power-law distribution. Our experiments also revealed that the studied metrics do not correlate with other traditional network measurements, such as the degree or betweenness centrality. The effectiveness of the symmetry measurements was verified in the authorship attribution task. Interestingly, we found that specific authors prefer particular types of symmetric motifs. As a consequence, the authorship of books could be accurately identified in 82.5% of the cases, in a dataset comprising books written by 8 authors. Because the proposed measurements for text analysis are complementary to the traditional approach, they can be used to improve the characterization of text networks, which might be useful for related applications, such as those relying on the identification of topical words and information retrieval.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02490",
        "title": "Leveraging Twitter for Low-Resource Conversational Speech Language Modeling",
        "authors": [
            "Aaron Jaech",
            "Mari Ostendorf"
        ],
        "abstract": "In applications involving conversational speech, data sparsity is a limiting factor in building a better language model. We propose a simple, language-independent method to quickly harvest large amounts of data from Twitter to supplement a smaller training set that is more closely matched to the domain. The techniques lead to a significant reduction in perplexity on four low-resource languages even though the presence on Twitter of these languages is relatively small. We also find that the Twitter text is more useful for learning word classes than the in-domain text and that use of these word classes leads to further reductions in perplexity. Additionally, we introduce a method of using social and textual information to prioritize the download queue during the Twitter crawling. This maximizes the amount of useful data that can be collected, impacting both perplexity and vocabulary coverage.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03659",
        "title": "Temporal ordering of clinical events",
        "authors": [
            "Azad Dehghan"
        ],
        "abstract": "This report describes a minimalistic set of methods engineered to anchor clinical events onto a temporal space. Specifically, we describe methods to extract clinical events (e.g., Problems, Treatments and Tests), temporal expressions (i.e., time, date, duration, and frequency), and temporal links (e.g., Before, After, Overlap) between events and temporal entities. These methods are developed and validated using high quality datasets.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04666",
        "title": "Unsupervised Dependency Parsing: Let's Use Supervised Parsers",
        "authors": [
            "Phong Le",
            "Willem Zuidema"
        ],
        "abstract": "We present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms. Our approach, called `iterated reranking' (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees. Our system achieves 1.8% accuracy higher than the state-of-the-part parser of Spitkovsky et al. (2013) on the WSJ corpus.\n    ",
        "submission_date": "2015-04-18T00:00:00",
        "last_modified_date": "2015-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04716",
        "title": "Gap Analysis of Natural Language Processing Systems with respect to Linguistic Modality",
        "authors": [
            "Vishal Shukla"
        ],
        "abstract": "Modality is one of the important components of grammar in linguistics. It lets speaker to express attitude towards, or give assessment or potentiality of state of affairs. It implies different senses and thus has different perceptions as per the context. This paper presents an account showing the gap in the functionality of the current state of art Natural Language Processing (NLP) systems. The contextual nature of linguistic modality is studied. In this paper, the works and logical approaches employed by Natural Language Processing systems dealing with modality are reviewed. It sees human cognition and intelligence as multi-layered approach that can be implemented by intelligent systems for learning. Lastly, current flow of research going on within this field is talked providing futurology.\n    ",
        "submission_date": "2015-04-18T00:00:00",
        "last_modified_date": "2015-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04751",
        "title": "A Knowledge-poor Pronoun Resolution System for Turkish",
        "authors": [
            "Dilek K\u00fc\u00e7\u00fck",
            "Meltem Turhan Y\u00f6ndem"
        ],
        "abstract": "A pronoun resolution system which requires limited syntactic knowledge to identify the antecedents of personal and reflexive pronouns in Turkish is presented. As in its counterparts for languages like English, Spanish and French, the core of the system is the constraints and preferences determined empirically. In the evaluation phase, it performed considerably better than the baseline algorithm used for comparison. The system is significant for its being the first fully specified knowledge-poor computational framework for pronoun resolution in Turkish where Turkish possesses different structural properties from the languages for which knowledge-poor systems had been developed.\n    ",
        "submission_date": "2015-04-18T00:00:00",
        "last_modified_date": "2015-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04770",
        "title": "Online Inference for Relation Extraction with a Reduced Feature Set",
        "authors": [
            "Maxim Rabinovich",
            "C\u00e9dric Archambeau"
        ],
        "abstract": "Access to web-scale corpora is gradually bringing robust automatic knowledge base creation and extension within reach. To exploit these large unannotated---and extremely difficult to annotate---corpora, unsupervised machine learning methods are required. Probabilistic models of text have recently found some success as such a tool, but scalability remains an obstacle in their application, with standard approaches relying on sampling schemes that are known to be difficult to scale. In this report, we therefore present an empirical assessment of the sublinear time sparse stochastic variational inference (SSVI) scheme applied to RelLDA. We demonstrate that online inference leads to relatively strong qualitative results but also identify some of its pathologies---and those of the model---which will need to be overcome if SSVI is to be used for large-scale relation extraction.\n    ",
        "submission_date": "2015-04-18T00:00:00",
        "last_modified_date": "2015-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05070",
        "title": "Self-Adaptive Hierarchical Sentence Model",
        "authors": [
            "Han Zhao",
            "Zhengdong Lu",
            "Pascal Poupart"
        ],
        "abstract": "The ability to accurately model a sentence at varying stages (e.g., word-phrase-sentence) plays a central role in natural language processing. As an effort towards this goal we propose a self-adaptive hierarchical sentence model (AdaSent). AdaSent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. We design a competitive mechanism (through gating networks) to allow the representations of the same sentence to be engaged in a particular learning task (e.g., classification), therefore effectively mitigating the gradient vanishing problem persistent in other recursive models. Both qualitative and quantitative analysis shows that AdaSent can automatically form and select the representations suitable for the task at hand during training, yielding superior classification performance over competitor models on 5 benchmark data sets.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05319",
        "title": "Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word: The Impact of Word Representation on Sequence Labelling Tasks",
        "authors": [
            "Lizhen Qu",
            "Gabriela Ferraro",
            "Liyuan Zhou",
            "Weiwei Hou",
            "Nathan Schneider",
            "Timothy Baldwin"
        ],
        "abstract": "Word embeddings -- distributed word representations that can be learned from unlabelled data -- have been shown to have high utility in many natural language processing applications. In this paper, we perform an extrinsic evaluation of five popular word embedding methods in the context of four sequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE identification. A particular focus of the paper is analysing the effects of task-based updating of word representations. We show that when using word embeddings as features, as few as several hundred training instances are sufficient to achieve competitive results, and that word embeddings lead to improvements over OOV words and out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05929",
        "title": "A Hierarchical Distance-dependent Bayesian Model for Event Coreference Resolution",
        "authors": [
            "Bishan Yang",
            "Claire Cardie",
            "Peter Frazier"
        ],
        "abstract": "We present a novel hierarchical distance-dependent Bayesian model for event coreference resolution. While existing generative models for event coreference resolution are completely unsupervised, our model allows for the incorporation of pairwise distances between event mentions -- information that is widely used in supervised coreference models to guide the generative clustering processing for better event clustering both within and across documents. We model the distances between event mentions using a feature-rich learnable distance function and encode them as Bayesian priors for nonparametric clustering. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods for both within- and cross-document event coreference resolution.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06078",
        "title": "x.ent: R Package for Entities and Relations Extraction based on Unsupervised Learning and Document Structure",
        "authors": [
            "Nicolas Turenne",
            "Tien Phan"
        ],
        "abstract": "Relation extraction with accurate precision is still a challenge when processing full text databases. We propose an approach based on cooccurrence analysis in each document for which we used document organization to improve accuracy of relation extraction. This approach is implemented in a R package called \\emph{",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06391",
        "title": "On the Stability of Online Language Features: How Much Text do you Need to know a Person?",
        "authors": [
            "Eben M. Haber"
        ],
        "abstract": "In recent years, numerous studies have inferred personality and other traits from people's online writing. While these studies are encouraging, more information is needed in order to use these techniques with confidence. How do linguistic features vary across different online media, and how much text is required to have a representative sample for a person? In this paper, we examine several large sets of online, user-generated text, drawn from Twitter, email, blogs, and online discussion forums. We examine and compare population-wide results for the linguistic measure LIWC, and the inferred traits of Big5 Personality and Basic Human Values. We also empirically measure the stability of these traits across different sized samples for each individual. Our results highlight the importance of tuning models to each online medium, and include guidelines for the minimum amount of text required for a representative result.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06580",
        "title": "Classifying Relations by Ranking with Convolutional Neural Networks",
        "authors": [
            "Cicero Nogueira dos Santos",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06650",
        "title": "Learning Dictionaries for Named Entity Recognition using Minimal Supervision",
        "authors": [
            "Arvind Neelakantan",
            "Michael Collins"
        ],
        "abstract": "This paper describes an approach for automatic construction of dictionaries for Named Entity Recognition (NER) using large amounts of unlabeled data and a few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower dimensional embeddings (representations) for candidate phrases and classify these phrases using a small number of labeled examples. Our method achieves 16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER respectively. We also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06654",
        "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space",
        "authors": [
            "Arvind Neelakantan",
            "Jeevan Shankar",
            "Alexandre Passos",
            "Andrew McCallum"
        ],
        "abstract": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06658",
        "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods",
        "authors": [
            "Arvind Neelakantan",
            "Ming-Wei Chang"
        ],
        "abstract": "Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06662",
        "title": "Compositional Vector Space Models for Knowledge Base Completion",
        "authors": [
            "Arvind Neelakantan",
            "Benjamin Roth",
            "Andrew McCallum"
        ],
        "abstract": "Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z) -> containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recursive neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06665",
        "title": "Using Syntax-Based Machine Translation to Parse English into Abstract Meaning Representation",
        "authors": [
            "Michael Pust",
            "Ulf Hermjakob",
            "Kevin Knight",
            "Daniel Marcu",
            "Jonathan May"
        ],
        "abstract": "We present a parser for Abstract Meaning Representation (AMR). We treat English-to-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser improves upon state-of-the-art results by 7 Smatch points.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07071",
        "title": "Exploring semantically-related concepts from Wikipedia: the case of SeRE",
        "authors": [
            "Daniel Hienert",
            "Dennis Wegener",
            "Siegfried Schomisch"
        ],
        "abstract": "In this paper we present our web application SeRE designed to explore semantically related concepts. Wikipedia and DBpedia are rich data sources to extract related entities for a given topic, like in- and out-links, broader and narrower terms, categorisation information etc. We use the Wikipedia full text body to compute the semantic relatedness for extracted terms, which results in a list of entities that are most relevant for a topic. For any given query, the user interface of SeRE visualizes these related concepts, ordered by semantic relatedness; with snippets from Wikipedia articles that explain the connection between those two entities. In a user study we examine how SeRE can be used to find important entities and their relationships for a given topic and to answer the question of how the classification system can be used for filtering.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07225",
        "title": "Correlational Neural Networks",
        "authors": [
            "Sarath Chandar",
            "Mitesh M. Khapra",
            "Hugo Larochelle",
            "Balaraman Ravindran"
        ],
        "abstract": "Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently. Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE based methods learn a common representation by minimizing the error of reconstructing the two views. Each of these approaches has its own advantages and disadvantages. For example, while CCA based approaches outperform AE based approaches for the task of transfer learning, they are not as scalable as the latter. In this work we propose an AE based approach called Correlational Neural Network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace. Through a series of experiments, we demonstrate that the proposed CorrNet is better than the above mentioned approaches with respect to its ability to learn correlated common representations. Further, we employ CorrNet for several cross language tasks and show that the representations learned using CorrNet perform better than the ones learned using other state of the art approaches.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07295",
        "title": "Document Classification by Inversion of Distributed Language Representations",
        "authors": [
            "Matt Taddy"
        ],
        "abstract": "There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07324",
        "title": "Reader-Aware Multi-Document Summarization via Sparse Coding",
        "authors": [
            "Piji Li",
            "Lidong Bing",
            "Wai Lam",
            "Hang Li",
            "Yi Liao"
        ],
        "abstract": "We propose a new MDS paradigm called reader-aware multi-document summarization (RA-MDS). Specifically, a set of reader comments associated with the news reports are also collected. The generated summaries from the reports for the event should be salient according to not only the reports but also the reader comments. To tackle this RA-MDS problem, we propose a sparse-coding-based method that is able to calculate the salience of the text units by jointly considering news reports and reader comments. Another reader-aware characteristic of our framework is to improve linguistic quality via entity rewriting. The rewriting consideration is jointly assessed together with other summarization requirements under a unified optimization model. To support the generation of compressive summaries via optimization, we explore a finer syntactic unit, namely, noun/verb phrase. In this work, we also generate a data set for conducting RA-MDS. Extensive experiments on this data set and some classical data sets demonstrate the effectiveness of our proposed approach.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07395",
        "title": "Lexical Translation Model Using a Deep Neural Network Architecture",
        "authors": [
            "Thanh-Le Ha",
            "Jan Niehues",
            "Alex Waibel"
        ],
        "abstract": "In this paper we combine the advantages of a model using global source sentence contexts, the Discriminative Word Lexicon, and neural networks. By using deep neural networks instead of the linear maximum entropy model in the Discriminative Word Lexicon models, we are able to leverage dependencies between different source words due to the non-linearity. Furthermore, the models for different target words can share parameters and therefore data sparsity problems are effectively reduced.\n",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07459",
        "title": "CommentWatcher: An Open Source Web-based platform for analyzing discussions on web forums",
        "authors": [
            "Marian-Andrei Rizoiu",
            "Adrien Guille",
            "Julien Velcin"
        ],
        "abstract": "We present CommentWatcher, an open source tool aimed at analyzing discussions on web forums. Constructed as a web platform, CommentWatcher features automatic mass fetching of user posts from forum on multiple sites, extracting topics, visualizing the topics as an expression cloud and exploring their temporal evolution. The underlying social network of users is simultaneously constructed using the citation relations between users and visualized as a graph structure. Our platform addresses the issues of the diversity and dynamics of structures of webpages hosting the forums by implementing a parser architecture that is independent of the HTML structure of webpages. This allows easy on-the-fly adding of new websites. Two types of users are targeted: end users who seek to study the discussed topics and their temporal evolution, and researchers in need of establishing a forum benchmark dataset and comparing the performances of analysis tools.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07678",
        "title": "Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation",
        "authors": [
            "Hongzhao Huang",
            "Larry Heck",
            "Heng Ji"
        ],
        "abstract": "Entity Disambiguation aims to link mentions of ambiguous entities to a knowledge base (e.g., Wikipedia). Modeling topical coherence is crucial for this task based on the assumption that information from the same semantic context tends to belong to the same topic. This paper presents a novel deep semantic relatedness model (DSRM) based on deep neural networks (DNN) and semantic knowledge graphs (KGs) to measure entity semantic relatedness for topical coherence modeling. The DSRM is directly trained on large-scale KGs and it maps heterogeneous types of knowledge of an entity from KGs to numerical feature vectors in a latent space such that the distance between two semantically-related entities is minimized. Compared with the state-of-the-art relatedness approach proposed by (Milne and Witten, 2008a), the DSRM obtains 19.4% and 24.5% reductions in entity disambiguation errors on two publicly available datasets respectively.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08050",
        "title": "Detecting Concept-level Emotion Cause in Microblogging",
        "authors": [
            "Shuangyong Song",
            "Yao Meng"
        ],
        "abstract": "In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead of the mere word-level models, to discover causes of microblogging users' diversified emotions on specific hot event. A modified topic-supervised biterm topic model is utilized in CECM to detect emotion topics' in event-related tweets, and then context-sensitive topical PageRank is utilized to detect meaningful multiword expressions as emotion causes. Experimental results on a dataset from Sina Weibo, one of the largest microblogging websites in China, show CECM can better detect emotion causes than baseline methods.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08102",
        "title": "Detecting and ordering adjectival scalemates",
        "authors": [
            "Emiel van Miltenburg"
        ],
        "abstract": "This paper presents a pattern-based method that can be used to infer adjectival scales, such as <lukewarm, warm, hot>, from a corpus. Specifically, the proposed method uses lexical patterns to automatically identify and order pairs of scalemates, followed by a filtering phase in which unrelated pairs are discarded. For the filtering phase, several different similarity measures are implemented and compared. The model presented in this paper is evaluated using the current standard, along with a novel evaluation set, and shown to be at least as good as the current state-of-the-art.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08183",
        "title": "Texts in, meaning out: neural language models in semantic similarity task for Russian",
        "authors": [
            "Andrey Kutuzov",
            "Igor Andreev"
        ],
        "abstract": "Distributed vector representations for natural language vocabulary get a lot of attention in contemporary computational linguistics. This paper summarizes the experience of applying neural network language models to the task of calculating semantic similarity for Russian. The experiments were performed in the course of Russian Semantic Similarity Evaluation track, where our models took from the 2nd to the 5th position, depending on the task.\n",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08342",
        "title": "Parsing Linear Context-Free Rewriting Systems with Fast Matrix Multiplication",
        "authors": [
            "Shay B. Cohen",
            "Daniel Gildea"
        ],
        "abstract": "We describe a matrix multiplication recognition algorithm for a subset of binary linear context-free rewriting systems (LCFRS) with running time $O(n^{\\omega d})$ where $M(m) = O(m^{\\omega})$ is the running time for $m \\times m$ matrix multiplication and $d$ is the \"contact rank\" of the LCFRS -- the maximal number of combination and non-combination points that appear in the grammar rules. We also show that this algorithm can be used as a subroutine to get a recognition algorithm for general binary LCFRS with running time $O(n^{\\omega d + 1})$. The currently best known $\\omega$ is smaller than $2.38$. Our result provides another proof for the best known result for parsing mildly context sensitive formalisms such as combinatory categorial grammars, head grammars, linear indexed grammars, and tree adjoining grammars, which can be parsed in time $O(n^{4.76})$. It also shows that inversion transduction grammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRS subsumes many other formalisms and types of grammars, for some of which we also improve the asymptotic complexity of parsing.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00138",
        "title": "Compositional Distributional Semantics with Compact Closed Categories and Frobenius Algebras",
        "authors": [
            "Dimitri Kartsaklis"
        ],
        "abstract": "This thesis contributes to ongoing research related to the categorical compositional model for natural language of Coecke, Sadrzadeh and Clark in three ways: Firstly, I propose a concrete instantiation of the abstract framework based on Frobenius algebras (joint work with Sadrzadeh). The theory improves shortcomings of previous proposals, extends the coverage of the language, and is supported by experimental work that improves existing results. The proposed framework describes a new class of compositional models that find intuitive interpretations for a number of linguistic phenomena. Secondly, I propose and evaluate in practice a new compositional methodology which explicitly deals with the different levels of lexical ambiguity (joint work with Pulman). A concrete algorithm is presented, based on the separation of vector disambiguation from composition in an explicit prior step. Extensive experimental work shows that the proposed methodology indeed results in more accurate composite representations for the framework of Coecke et al. in particular and every other class of compositional models in general. As a last contribution, I formalize the explicit treatment of lexical ambiguity in the context of the categorical framework by resorting to categorical quantum mechanics (joint work with Coecke). In the proposed extension, the concept of a distributional vector is replaced with that of a density matrix, which compactly represents a probability distribution over the potential different meanings of the specific word. Composition takes the form of quantum measurements, leading to interesting analogies between quantum physics and linguistics.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00161",
        "title": "Embedding Semantic Relations into Word Representations",
        "authors": [
            "Danushka Bollegala",
            "Takanori Maehara",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "Learning representations for semantic relations is important for various tasks such as analogy detection, relational search, and relation classification. Although there have been several proposals for learning representations for individual words, learning word representations that explicitly capture the semantic relations between words remains under developed. We propose an unsupervised method for learning vector representations for words such that the learnt representations are sensitive to the semantic relations that exist between two words. First, we extract lexical patterns from the co-occurrence contexts of two words in a corpus to represent the semantic relations that exist between those two words. Second, we represent a lexical pattern as the weighted sum of the representations of the words that co-occur with that lexical pattern. Third, we train a binary classifier to detect relationally similar vs. non-similar lexical pattern pairs. The proposed method is unsupervised in the sense that the lexical pattern pairs we use as train data are automatically sampled from a corpus, without requiring any manual intervention. Our proposed method statistically significantly outperforms the current state-of-the-art word representations on three benchmark datasets for proportional analogy detection, demonstrating its ability to accurately capture the semantic relations among words.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00277",
        "title": "Grounded Discovery of Coordinate Term Relationships between Software Entities",
        "authors": [
            "Dana Movshovitz-Attias",
            "William W. Cohen"
        ],
        "abstract": "We present an approach for the detection of coordinate-term relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the \"grounded\" entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00468",
        "title": "VQA: Visual Question Answering",
        "authors": [
            "Aishwarya Agrawal",
            "Jiasen Lu",
            "Stanislaw Antol",
            "Margaret Mitchell",
            "C. Lawrence Zitnick",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01072",
        "title": "Mining Measured Information from Text",
        "authors": [
            "Arun S. Maiya",
            "Dale Visser",
            "Andrew Wan"
        ],
        "abstract": "We present an approach to extract measured information from text (e.g., a 1370 degrees C melting point, a BMI greater than 29.9 kg/m^2 ). Such extractions are critically important across a wide range of domains - especially those involving search and exploration of scientific and technical documents. We first propose a rule-based entity extractor to mine measured quantities (i.e., a numeric value paired with a measurement unit), which supports a vast and comprehensive set of both common and obscure measurement units. Our method is highly robust and can correctly recover valid measured quantities even when significant errors are introduced through the process of converting document formats like PDF to plain text. Next, we describe an approach to extracting the properties being measured (e.g., the property \"pixel pitch\" in the phrase \"a pixel pitch as high as 352 {\\mu}m\"). Finally, we present MQSearch: the realization of a search engine with full support for measured information.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01757",
        "title": "Contextual Analysis for Middle Eastern Languages with Hidden Markov Models",
        "authors": [
            "Kazem Taghva"
        ],
        "abstract": "Displaying a document in Middle Eastern languages requires contextual analysis due to different presentational forms for each character of the alphabet. The words of the document will be formed by the joining of the correct positional glyphs representing corresponding presentational forms of the characters. A set of rules defines the joining of the glyphs. As usual, these rules vary from language to language and are subject to interpretation by the software developers.\n",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01809",
        "title": "Language Models for Image Captioning: The Quirks and What Works",
        "authors": [
            "Jacob Devlin",
            "Hao Cheng",
            "Hao Fang",
            "Saurabh Gupta",
            "Li Deng",
            "Xiaodong He",
            "Geoffrey Zweig",
            "Margaret Mitchell"
        ],
        "abstract": "Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02419",
        "title": "Improved Relation Extraction with Feature-Rich Compositional Embedding Models",
        "authors": [
            "Matthew R. Gormley",
            "Mo Yu",
            "Mark Dredze"
        ],
        "abstract": "Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings. We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) hand-crafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results.\n    ",
        "submission_date": "2015-05-10T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02425",
        "title": "Fast Rhetorical Structure Theory Discourse Parsing",
        "authors": [
            "Michael Heilman",
            "Kenji Sagae"
        ],
        "abstract": "In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing. Most of the recent work on RST parsing has focused on implementing new types of features or learning algorithms in order to improve accuracy, with relatively little focus on efficiency, robustness, or practical use. Also, most implementations are not widely available. Here, we describe an RST segmentation and parsing system that adapts models and feature sets from various previous work, as described below. Its accuracy is near state-of-the-art, and it was developed to be fast, robust, and practical. For example, it can process short documents such as news articles or essays in less than a second.\n    ",
        "submission_date": "2015-05-10T00:00:00",
        "last_modified_date": "2015-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02973",
        "title": "Comparing methods for Twitter Sentiment Analysis",
        "authors": [
            "Evangelos Psomakelis",
            "Konstantinos Tserpes",
            "Dimosthenis Anagnostopoulos",
            "Theodora Varvarigou"
        ],
        "abstract": "This work extends the set of works which deal with the popular problem of sentiment analysis in Twitter. It investigates the most popular document (\"tweet\") representation methods which feed sentiment evaluation mechanisms. In particular, we study the bag-of-words, n-grams and n-gram graphs approaches and for each of them we evaluate the performance of a lexicon-based and 7 learning-based classification algorithms (namely SVM, Na\u00efve Bayesian Networks, Logistic Regression, Multilayer Perceptrons, Best-First Trees, Functional Trees and C4.5) as well as their combinations, using a set of 4451 manually annotated tweets. The results demonstrate the superiority of learning-based methods and in particular of n-gram graphs approaches for predicting the sentiment of tweets. They also show that the combinatory approach has impressive effects on n-grams, raising the confidence up to 83.15% on the 5-Grams, using majority vote and a balanced dataset (equal number of positive, negative and neutral tweets for training). In the n-gram graph cases the improvement was small to none, reaching 94.52% on the 4-gram graphs, using Orthodromic distance and a threshold of 0.001.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03081",
        "title": "Turn Segmentation into Utterances for Arabic Spontaneous Dialogues and Instance Messages",
        "authors": [
            "AbdelRahim A. Elmadany",
            "Sherif M. Abdou",
            "Mervat Gheith"
        ],
        "abstract": "Text segmentation task is an essential processing task for many of Natural Language Processing (NLP) such as text summarization, text translation, dialogue language understanding, among others. Turns segmentation considered the key player in dialogue understanding task for building automatic Human-Computer systems. In this paper, we introduce a novel approach to turn segmentation into utterances for Egyptian spontaneous dialogues and Instance Messages (IM) using Machine Learning (ML) approach as a part of automatic understanding Egyptian spontaneous dialogues and IM task. Due to the lack of Egyptian dialect dialogue corpus the system evaluated by our corpus includes 3001 turns, which are collected, segmented, and annotated manually from Egyptian call-centers. The system achieves F1 scores of 90.74% and accuracy of 95.98%.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03084",
        "title": "A Survey of Arabic Dialogues Understanding for Spontaneous Dialogues and Instant Message",
        "authors": [
            "AbdelRahim A. Elmadany",
            "Sherif M. Abdou",
            "Mervat Gheith"
        ],
        "abstract": "Building dialogues systems interaction has recently gained considerable attention, but most of the resources and systems built so far are tailored to English and other Indo-European languages. The need for designing systems for other languages is increasing such as Arabic language. For this reasons, there are more interest for Arabic dialogue acts classification task because it a key player in Arabic language understanding to building this systems. This paper surveys different techniques for dialogue acts classification for Arabic. We describe the main existing techniques for utterances segmentations and classification, annotation schemas, and test corpora for Arabic dialogues understanding that have introduced in the literature\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03085",
        "title": "Indonesian Social Media Sentiment Analysis With Sarcasm Detection",
        "authors": [
            "Edwin Lunando",
            "Ayu Purwarianti"
        ],
        "abstract": "Sarcasm is considered one of the most difficult problem in sentiment analysis. In our ob-servation on Indonesian social media, for cer-tain topics, people tend to criticize something using sarcasm. Here, we proposed two additional features to detect sarcasm after a common sentiment analysis is conducted. The features are the negativity information and the number of interjection words. We also employed translated SentiWordNet in the sentiment classification. All the classifications were conducted with machine learning algorithms. The experimental results showed that the additional features are quite effective in the sarcasm detection.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03105",
        "title": "Sentiment Analysis For Modern Standard Arabic And Colloquial",
        "authors": [
            "Hossam S. Ibrahim",
            "Sherif M. Abdou",
            "Mervat Gheith"
        ],
        "abstract": "The rise of social media such as blogs and social networks has fueled interest in sentiment analysis. With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations, therefore many are now looking to the field of sentiment analysis. In this paper, we present a feature-based sentence level approach for Arabic sentiment analysis. Our approach is using Arabic idioms/saying phrases lexicon as a key importance for improving the detection of the sentiment polarity in Arabic sentences as well as a number of novels and rich set of linguistically motivated features contextual Intensifiers, contextual Shifter and negation handling), syntactic features for conflicting phrases which enhance the sentiment classification accuracy. Furthermore, we introduce an automatic expandable wide coverage polarity lexicon of Arabic sentiment words. The lexicon is built with gold-standard sentiment words as a seed which is manually collected and annotated and it expands and detects the sentiment orientation automatically of new sentiment words using synset aggregation technique and free online Arabic lexicons and thesauruses. Our data focus on modern standard Arabic (MSA) and Egyptian dialectal Arabic tweets and microblogs (hotel reservation, product reviews, etc.). The experimental results using our resources and techniques with SVM classifier indicate high performance levels, with accuracies of over 95%.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03239",
        "title": "Feature selection using Fisher's ratio technique for automatic speech recognition",
        "authors": [
            "Sarika Hegde",
            "K. K. Achary",
            "Surendra Shetty"
        ],
        "abstract": "Automatic Speech Recognition involves mainly two steps; feature extraction and classification . Mel Frequency Cepstral Coefficient is used as one of the prominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC coefficients is used as the feature vector in the classification step. But the question is whether the same or improved classification accuracy can be achieved by using a subset of 12 MFCC as feature vector. In this paper, Fisher's ratio technique is used for selecting a subset of 12 MFCC coefficients that contribute more in discriminating a pattern. The selected coefficients are used in classification with Hidden Markov Model algorithm. The classification accuracies that we get by using 12 coefficients and by using the selected coefficients are compared.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03783",
        "title": "Rank diversity of languages: Generic behavior in computational linguistics",
        "authors": [
            "Germinal Cocho",
            "Jorge Flores",
            "Carlos Gershenson",
            "Carlos Pineda",
            "Sergio S\u00e1nchez"
        ],
        "abstract": "Statistical studies of languages have focused on the rank-frequency distribution of words. Instead, we introduce here a measure of how word ranks change in time and call this distribution \\emph{rank diversity}. We calculate this diversity for books published in six European languages since 1800, and find that it follows a universal lognormal distribution. Based on the mean and standard deviation associated with the lognormal distribution, we define three different word regimes of languages: \"heads\" consist of words which almost do not change their rank in time, \"bodies\" are words of general use, while \"tails\" are comprised by context-specific words and vary their rank considerably in time. The heads and bodies reflect the size of language cores identified by linguists for basic communication. We propose a Gaussian random walk model which reproduces the rank variation of words in time and thus the diversity. Rank diversity of words can be understood as the result of random variations in rank, where the size of the variation depends on the rank itself. We find that the core size is similar for all languages studied.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03823",
        "title": "Distant Supervision for Entity Linking",
        "authors": [
            "Miao Fan",
            "Qiang Zhou",
            "Thomas Fang Zheng"
        ],
        "abstract": "Entity linking is an indispensable operation of populating knowledge repositories for information extraction. It studies on aligning a textual entity mention to its corresponding disambiguated entry in a knowledge repository. In this paper, we propose a new paradigm named distantly supervised entity linking (DSEL), in the sense that the disambiguated entities that belong to a huge knowledge repository (Freebase) are automatically aligned to the corresponding descriptive webpages (Wiki pages). In this way, a large scale of weakly labeled data can be generated without manual annotation and fed to a classifier for linking more newly discovered entities. Compared with traditional paradigms based on solo knowledge base, DSEL benefits more via jointly leveraging the respective advantages of Freebase and Wikipedia. Specifically, the proposed paradigm facilitates bridging the disambiguated labels (Freebase) of entities and their textual descriptions (Wikipedia) for Web-scale entities. Experiments conducted on a dataset of 140,000 items and 60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze the feature performance and improve the F1-measure to 0.545.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04197",
        "title": "Arabic Inquiry-Answer Dialogue Acts Annotation Schema",
        "authors": [
            "AbdelRahim A. Elmadany",
            "Sherif M. Abdou",
            "Mervat Gheith"
        ],
        "abstract": "We present an annotation schema as part of an effort to create a manually annotated corpus for Arabic dialogue language understanding including spoken dialogue and written \"chat\" dialogue for inquiry-answer domain. The proposed schema handles mainly the request and response acts that occurs frequently in inquiry-answer debate conversations expressing request services, suggests, and offers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04313",
        "title": "A type-theoretical approach to Universal Grammar",
        "authors": [
            "Erkki Luuk"
        ],
        "abstract": "The idea of Universal Grammar (UG) as the hypothetical linguistic structure shared by all human languages harkens back at least to the 13th century. The best known modern elaborations of the idea are due to Chomsky. Following a devastating critique from theoretical, typological and field linguistics, these elaborations, the idea of UG itself and the more general idea of language universals stand untenable and are largely abandoned. The proposal tackles the hypothetical contents of UG using dependent and polymorphic type theory in a framework very different from the Chomskyan ones. We introduce a type logic for a precise, universal and parsimonious representation of natural language morphosyntax and compositional semantics. The logic handles grammatical ambiguity (with polymorphic types), selectional restrictions and diverse kinds of anaphora (with dependent types), and features a partly universal set of morphosyntactic types (by the Curry-Howard isomorphism).\n    ",
        "submission_date": "2015-05-16T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04342",
        "title": "Sifting Robotic from Organic Text: A Natural Language Approach for Detecting Automation on Twitter",
        "authors": [
            "Eric M. Clark",
            "Jake Ryland Williams",
            "Chris A. Jones",
            "Richard A. Galbraith",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "Twitter, a popular social media outlet, has evolved into a vast source of linguistic data, rich with opinion, sentiment, and discussion. Due to the increasing popularity of Twitter, its perceived potential for exerting social influence has led to the rise of a diverse community of automatons, commonly referred to as bots. These inorganic and semi-organic Twitter entities can range from the benevolent (e.g., weather-update bots, help-wanted-alert bots) to the malevolent (e.g., spamming messages, advertisements, or radical opinions). Existing detection algorithms typically leverage meta-data (time between tweets, number of followers, etc.) to identify robotic accounts. Here, we present a powerful classification scheme that exclusively uses the natural language text from organic users to provide a criterion for identifying accounts posting automated messages. Since the classifier operates on text alone, it is flexible and may be applied to any textual data beyond the Twitter-sphere.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04420",
        "title": "CCG Parsing and Multiword Expressions",
        "authors": [
            "Miryam de Lhoneux"
        ],
        "abstract": "This thesis presents a study about the integration of information about Multiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar (CCG). We build on previous work which has shown the benefit of adding information about MWEs to syntactic parsing by implementing a similar pipeline with CCG parsing. More specifically, we collapse MWEs to one token in training and test data in CCGbank, a corpus which contains sentences annotated with CCG derivations. Our collapsing algorithm however can only deal with MWEs when they form a constituent in the data which is one of the limitations of our approach.\n",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04891",
        "title": "Learning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph",
        "authors": [
            "Fei Tian",
            "Bin Gao",
            "Enhong Chen",
            "Tie-Yan Liu"
        ],
        "abstract": "Word embedding, which refers to low-dimensional dense vector representations of natural words, has demonstrated its power in many natural language processing tasks. However, it may suffer from the inaccurate and incomplete information contained in the free text corpus as training data. To tackle this challenge, there have been quite a few works that leverage knowledge graphs as an additional information source to improve the quality of word embedding. Although these works have achieved certain success, they have neglected some important facts about knowledge graphs: (i) many relationships in knowledge graphs are \\emph{many-to-one}, \\emph{one-to-many} or even \\emph{many-to-many}, rather than simply \\emph{one-to-one}; (ii) most head entities and tail entities in knowledge graphs come from very different semantic spaces. To address these issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet models the relationships between head and tail entities after transforming them with different low-rank projection matrices. The low-rank projection can allow non \\emph{one-to-one} relationships between entities, while different projection matrices for head and tail entities allow them to originate in different semantic spaces. The experimental results demonstrate that ProjectNet yields more accurate word embedding than previous works, thus leads to clear improvements in various natural language processing tasks.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05008",
        "title": "Boosting Named Entity Recognition with Neural Character Embeddings",
        "authors": [
            "Cicero Nogueira dos Santos",
            "Victor Guimar\u00e3es"
        ],
        "abstract": "Most state-of-the-art named entity recognition (NER) systems rely on handcrafted features and on the output of other NLP tasks such as part-of-speech (POS) tagging and text chunking. In this work we propose a language-independent NER system that uses automatically learned features only. Our approach is based on the CharWNN deep neural network, which uses word-level and character-level representations (embeddings) to perform sequential classification. We perform an extensive number of experiments using two annotated corpora in two different languages: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in Spanish. Our experimental results shade light on the contribution of neural character embeddings for NER. Moreover, we demonstrate that the same neural network which has been successfully applied to POS tagging can also achieve state-of-the-art results for language-independet NER, using the same hyperparameters, and without any handcrafted features. For the HAREM I corpus, CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score for the total scenario (ten NE classes), and by 7.2 points in the F1 for the selective scenario (five NE classes).\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05253",
        "title": "Knowlege Graph Embedding by Flexible Translation",
        "authors": [
            "Jun Feng",
            "Mantong Zhou",
            "Yu Hao",
            "Minlie Huang",
            "Xiaoyan Zhu"
        ],
        "abstract": "Knowledge graph embedding refers to projecting entities and relations in knowledge graph into continuous vector spaces. State-of-the-art methods, such as TransE, TransH, and TransR build embeddings by treating relation as translation from head entity to tail entity. However, previous models can not deal with reflexive/one-to-many/many-to-one/many-to-many relations properly, or lack of scalability and efficiency. Thus, we propose a novel method, flexible translation, named TransF, to address the above issues. TransF regards relation as translation between head entity vector and tail entity vector with flexible magnitude. To evaluate the proposed model, we conduct link prediction and triple classification on benchmark datasets. Experimental results show that our method remarkably improve the performance compared with several state-of-the-art baselines.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05667",
        "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network",
        "authors": [
            "Chenxi Zhu",
            "Xipeng Qiu",
            "Xinchi Chen",
            "Xuanjing Huang"
        ],
        "abstract": "In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05841",
        "title": "Translation Memory Retrieval Methods",
        "authors": [
            "Michael Bloodgood",
            "Benjamin Strauss"
        ],
        "abstract": "Translation Memory (TM) systems are one of the most widely used translation technologies. An important part of TM systems is the matching algorithm that determines what translations get retrieved from the bank of available translations to assist the human translator. Although detailed accounts of the matching algorithms used in commercial systems can't be found in the literature, it is widely believed that edit distance algorithms are used. This paper investigates and evaluates the use of several matching algorithms, including the edit distance algorithm that is believed to be at the heart of most modern commercial TM systems. This paper presents results showing how well various matching algorithms correlate with human judgments of helpfulness (collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithm based on weighted n-gram precision that can be adjusted for translator length preferences consistently returns translations judged to be most helpful by translators for multiple domains and language pairs.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05899",
        "title": "The IBM 2015 English Conversational Telephone Speech Recognition System",
        "authors": [
            "George Saon",
            "Hong-Kwang J. Kuo",
            "Steven Rennie",
            "Michael Picheny"
        ],
        "abstract": "We describe the latest improvements to the IBM English conversational telephone speech recognition system. Some of the techniques that were found beneficial are: maxout networks with annealed dropout rates; networks with a very large number of outputs trained on 2000 hours of data; joint modeling of partially unfolded recurrent neural networks and convolutional nets by combining the bottleneck and output layers and retraining the resulting model; and lastly, sophisticated language model rescoring with exponential and neural network LMs. These techniques result in an 8.0% word error rate on the Switchboard part of the Hub5-2000 evaluation test set which is 23% relative better than our previous best published result.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06169",
        "title": "Learning Dynamic Feature Selection for Fast Sequential Prediction",
        "authors": [
            "Emma Strubell",
            "Luke Vilnis",
            "Kate Silverstein",
            "Andrew McCallum"
        ],
        "abstract": "We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a five-fold reduction in run-time, and NER F1 above 88 with more than 2x increase in speed.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06228",
        "title": "Keyphrase Based Evaluation of Automatic Text Summarization",
        "authors": [
            "Fatma Elghannam",
            "Tarek El-Shishtawy"
        ],
        "abstract": "The development of methods to deal with the informative contents of the text units in the matching process is a major challenge in automatic summary evaluation systems that use fixed n-gram matching. The limitation causes inaccurate matching between units in a peer and reference summaries. The present study introduces a new Keyphrase based Summary Evaluator KpEval for evaluating automatic summaries. The KpEval relies on the keyphrases since they convey the most important concepts of a text. In the evaluation process, the keyphrases are used in their lemma form as the matching text unit. The system was applied to evaluate different summaries of Arabic multi-document data set presented at TAC2011. The results showed that the new evaluation technique correlates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4, and AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENG MeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667 respectively.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06256",
        "title": "Exposing ambiguities in a relation-extraction gold standard with crowdsourcing",
        "authors": [
            "Tong Shu Li",
            "Benjamin M. Good",
            "Andrew I. Su"
        ],
        "abstract": "Semantic relation extraction is one of the frontiers of biomedical natural language processing research. Gold standards are key tools for advancing this research. It is challenging to generate these standards because of the high cost of expert time and the difficulty in establishing agreement between annotators. We implemented and evaluated a microtask crowdsourcing approach that can produce a gold standard for extracting drug-disease relations. The aggregated crowd judgment agreed with expert annotations from a pre-existing corpus on 43 of 60 sentences tested. The levels of crowd agreement varied in a similar manner to the levels of agreement among the original expert annotators. This work rein-forces the power of crowdsourcing in the process of assembling gold standards for relation extraction. Further, it high-lights the importance of exposing the levels of agreement between human annotators, expert or crowd, in gold standard corpora as these are reproducible signals indicating ambiguities in the data or in the annotation guidelines.\n    ",
        "submission_date": "2015-05-23T00:00:00",
        "last_modified_date": "2015-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06289",
        "title": "Text to 3D Scene Generation with Rich Lexical Grounding",
        "authors": [
            "Angel Chang",
            "Will Monroe",
            "Manolis Savva",
            "Christopher Potts",
            "Christopher D. Manning"
        ],
        "abstract": "The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.\n    ",
        "submission_date": "2015-05-23T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06294",
        "title": "A Frobenius Model of Information Structure in Categorical Compositional Distributional Semantics",
        "authors": [
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "The categorical compositional distributional model of Coecke, Sadrzadeh and Clark provides a linguistically motivated procedure for computing the meaning of a sentence as a function of the distributional meaning of the words therein. The theoretical framework allows for reasoning about compositional aspects of language and offers structural ways of studying the underlying relationships. While the model so far has been applied on the level of syntactic structures, a sentence can bring extra information conveyed in utterances via intonational means. In the current paper we extend the framework in order to accommodate this additional information, using Frobenius algebraic structures canonically induced over the basis of finite-dimensional vector spaces. We detail the theory, provide truth-theoretic and distributional semantics for meanings of intonationally-marked utterances, and present justifications and extensive examples.\n    ",
        "submission_date": "2015-05-23T00:00:00",
        "last_modified_date": "2015-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06427",
        "title": "Deep Speaker Vectors for Semi Text-independent Speaker Verification",
        "authors": [
            "Lantian Li",
            "Dong Wang",
            "Zhiyong Zhang",
            "Thomas Fang Zheng"
        ],
        "abstract": "Recent research shows that deep neural networks (DNNs) can be used to extract deep speaker vectors (d-vectors) that preserve speaker characteristics and can be used in speaker verification. This new method has been tested on text-dependent speaker verification tasks, and improvement was reported when combined with the conventional i-vector method.\n",
        "submission_date": "2015-05-24T00:00:00",
        "last_modified_date": "2015-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06816",
        "title": "Representing Meaning with a Combination of Logical and Distributional Models",
        "authors": [
            "I. Beltagy",
            "Stephen Roller",
            "Pengxiang Cheng",
            "Katrin Erk",
            "Raymond J. Mooney"
        ],
        "abstract": "NLP tasks differ in the semantic information they require, and at this time no single se- mantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. So it has been argued that the two are complementary. We adopt a hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs). In this paper, we focus on the three components of a practical system integrating logical and distributional models: 1) Parsing and task representation is the logic-based part where input problems are represented in probabilistic logic. This is quite different from representing them in standard first-order logic. 2) For knowledge base construction we form weighted inference rules. We integrate and compare distributional information with other sources, notably WordNet and an existing paraphrase collection. In particular, we use our system to evaluate distributional lexical entailment approaches. We use a variant of Robinson resolution to determine the necessary inference rules. More sources can easily be added by mapping them to logical rules; our system learns a resource-specific weight that corrects for scaling differences between resources. 3) In discussing probabilistic inference, we show how to solve the inference problems efficiently. To evaluate our approach, we use the task of textual entailment (RTE), which can utilize the strengths of both logic-based and distributional representations. In particular we focus on the SICK dataset, where we achieve state-of-the-art results.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07184",
        "title": "Unsupervised Cross-Domain Word Representation Learning",
        "authors": [
            "Danushka Bollegala",
            "Takanori Maehara",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "Meaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of \\emph{source}-\\emph{target} domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domain-specific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as \\emph{pivots}. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-the-art domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07302",
        "title": "Unveiling the Political Agenda of the European Parliament Plenary: A Topical Analysis",
        "authors": [
            "Derek Greene",
            "James P. Cross"
        ],
        "abstract": "This study analyzes political interactions in the European Parliament (EP) by considering how the political agenda of the plenary sessions has evolved over time and the manner in which Members of the European Parliament (MEPs) have reacted to external and internal stimuli when making Parliamentary speeches. It does so by considering the context in which speeches are made, and the content of those speeches. To detect latent themes in legislative speeches over time, speech content is analyzed using a new dynamic topic modeling method, based on two layers of matrix factorization. This method is applied to a new corpus of all English language legislative speeches in the EP plenary from the period 1999-2014. Our findings suggest that the political agenda of the EP has evolved significantly over time, is impacted upon by the committee structure of the Parliament, and reacts to exogenous events such as EU Treaty referenda and the emergence of the Euro-crisis have a significant impact on what is being discussed in Parliament.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07599",
        "title": "Overview of the NLPCC 2015 Shared Task: Chinese Word Segmentation and POS Tagging for Micro-blog Texts",
        "authors": [
            "Xipeng Qiu",
            "Peng Qian",
            "Liusong Yin",
            "Shiyu Wu",
            "Xuanjing Huang"
        ],
        "abstract": "In this paper, we give an overview for the shared task at the 4th CCF Conference on Natural Language Processing \\& Chinese Computing (NLPCC 2015): Chinese word segmentation and part-of-speech (POS) tagging for micro-blog texts. Different with the popular used newswire datasets, the dataset of this shared task consists of the relatively informal micro-texts. The shared task has two sub-tasks: (1) individual Chinese word segmentation and (2) joint Chinese word segmentation and POS Tagging. Each subtask has three tracks to distinguish the systems with different resources. We first introduce the dataset and task, then we characterize the different approaches of the participating systems, report the test results, and provide a overview analysis of these results. An online system is available for open registration and evaluation at ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07909",
        "title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding",
        "authors": [
            "Huazheng Wang",
            "Fei Tian",
            "Bin Gao",
            "Jiang Bian",
            "Tie-Yan Liu"
        ],
        "abstract": "Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by artificial intelligence technologies, especially the deep learning technologies that are recently developed and successfully applied in a number of fields. However, we found that the task was quite challenging, and simply applying existing technologies (e.g., word embedding) could not achieve a good performance, mainly due to the multiple senses of words and the complex relations among words. To tackle these challenges, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). Second, we obtain distributed representations of words and relations by leveraging a novel word embedding method that considers the multi-sense nature of words and the relational knowledge among words (or their senses) contained in dictionaries. Third, for each type of questions, we propose a specific solver based on the obtained distributed word representations and relation representations. Experimental results have shown that the proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study. The results indicate that with appropriate uses of the deep learning technologies we might be a further step closer to the human intelligence.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07931",
        "title": "Supervised Fine Tuning for Word Embedding with Integrated Knowledge",
        "authors": [
            "Xuefeng Yang",
            "Kezhi Mao"
        ],
        "abstract": "Learning vector representation for words is an important research field which may benefit many natural language processing tasks. Two limitations exist in nearly all available models, which are the bias caused by the context definition and the lack of knowledge utilization. They are difficult to tackle because these algorithms are essentially unsupervised learning approaches. Inspired by deep learning, the authors propose a supervised framework for learning vector representation of words to provide additional supervised fine tuning after unsupervised learning. The framework is knowledge rich approacher and compatible with any numerical vectors word representation. The authors perform both intrinsic evaluation like attributional and relational similarity prediction and extrinsic evaluations like the sentence completion and sentiment analysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show that the proposed fine tuning framework may significantly improve the quality of the vector representation of words.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08075",
        "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
        "authors": [
            "Chris Dyer",
            "Miguel Ballesteros",
            "Wang Ling",
            "Austin Matthews",
            "Noah A. Smith"
        ],
        "abstract": "We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08149",
        "title": "Modeling meaning: computational interpreting and understanding of natural language fragments",
        "authors": [
            "Michael Kapustin",
            "Pavlo Kapustin"
        ],
        "abstract": "In this introductory article we present the basics of an approach to implementing computational interpreting of natural language aiming to model the meanings of words and phrases. Unlike other approaches, we attempt to define the meanings of text fragments in a composable and computer interpretable way. We discuss models and ideas for detecting different types of semantic incomprehension and choosing the interpretation that makes most sense in a given context. Knowledge representation is designed for handling context-sensitive and uncertain / imprecise knowledge, and for easy accommodation of new information. It stores quantitative information capturing the essence of the concepts, because it is crucial for working with natural language understanding and reasoning. Still, the representation is general enough to allow for new knowledge to be learned, and even generated by the system. The article concludes by discussing some reasoning-related topics: possible approaches to generation of new abstract concepts, and describing situations and concepts in words (e.g. for specifying interpretation difficulties).\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2019-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00037",
        "title": "Using Syntactic Features for Phishing Detection",
        "authors": [
            "Gilchan Park",
            "Julia M. Taylor"
        ],
        "abstract": "This paper reports on the comparison of the subject and object of verbs in their usage between phishing emails and legitimate emails. The purpose of this research is to explore whether the syntactic structures and subjects and objects of verbs can be distinguishable features for phishing detection. To achieve the objective, we have conducted two series of experiments: the syntactic similarity for sentences, and the subject and object of verb comparison. The results of the experiments indicated that both features can be used for some verbs, but more work has to be done for others.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00195",
        "title": "Recurrent Neural Networks with External Memory for Language Understanding",
        "authors": [
            "Baolin Peng",
            "Kaisheng Yao"
        ],
        "abstract": "Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.\n    ",
        "submission_date": "2015-05-31T00:00:00",
        "last_modified_date": "2015-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00196",
        "title": "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion",
        "authors": [
            "Kaisheng Yao",
            "Geoffrey Zweig"
        ],
        "abstract": "Sequence-to-sequence translation methods based on generation with a side-conditioned language model have recently shown promising results in several tasks. In machine translation, models conditioned on source side words have been used to produce target-language text, and in image captioning, models conditioned images have been used to generate caption text. Past work with this approach has focused on large vocabulary tasks, and measured quality in terms of BLEU. In this paper, we explore the applicability of such models to the qualitatively different grapheme-to-phoneme task. Here, the input and output side vocabularies are small, plain n-gram models do well, and credit is only given when the output is exactly correct. We find that the simple side-conditioned generation approach is able to rival the state-of-the-art, and we are able to significantly advance the stat-of-the-art with bi-directional long short-term memory (LSTM) neural networks that use the same alignment information that is used in conventional approaches.\n    ",
        "submission_date": "2015-05-31T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00275",
        "title": "Diversity in Spectral Learning for Natural Language Parsing",
        "authors": [
            "Shashi Narayan",
            "Shay B. Cohen"
        ],
        "abstract": "We describe an approach to create a diverse set of predictions with spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the $F_1$ score of 90.18, and for German we achieve the $F_1$ score of 83.38.\n    ",
        "submission_date": "2015-05-31T00:00:00",
        "last_modified_date": "2015-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00333",
        "title": "Learning to Answer Questions From Image Using Convolutional Neural Network",
        "authors": [
            "Lin Ma",
            "Zhengdong Lu",
            "Hang Li"
        ],
        "abstract": "In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CNN to compose the words of the question, and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA datasets, which are two benchmark datasets for the image QA, with the performances significantly outperforming the state-of-the-art.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00379",
        "title": "Modeling Relation Paths for Representation Learning of Knowledge Bases",
        "authors": [
            "Yankai Lin",
            "Zhiyuan Liu",
            "Huanbo Luan",
            "Maosong Sun",
            "Siwei Rao",
            "Song Liu"
        ],
        "abstract": "Representation learning of knowledge bases (KBs) aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves significant and consistent improvements on knowledge base completion and relation extraction from text.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00406",
        "title": "Monolingually Derived Phrase Scores for Phrase Based SMT Using Neural Networks Vector Representations",
        "authors": [
            "Amir Pouya Aghasadeghi",
            "Mohadeseh Bastan"
        ],
        "abstract": "In this paper, we propose two new features for estimating phrase-based machine translation parameters from mainly monolingual data. Our method is based on two recently introduced neural network vector representation models for words and sentences. It is the first time that these models have been used in an end to end phrase-based machine translation system. Scores obtained from our method can recover more than 80% of BLEU loss caused by removing phrase table probabilities. We also show that our features combined with the phrase table probabilities improve the BLEU score by absolute 0.74 points.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00528",
        "title": "Medical Synonym Extraction with Concept Space Models",
        "authors": [
            "Chang Wang",
            "Liangliang Cao",
            "Bowen Zhou"
        ],
        "abstract": "In this paper, we present a novel approach for medical synonym extraction. We aim to integrate the term embedding with the medical domain knowledge for healthcare applications. One advantage of our method is that it is very scalable. Experiments on a dataset with more than 1M term pairs show that the proposed approach outperforms the baseline approaches by a large margin.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00698",
        "title": "Statistical Machine Translation Features with Multitask Tensor Networks",
        "authors": [
            "Hendra Setiawan",
            "Zhongqiang Huang",
            "Jacob Devlin",
            "Thomas Lamar",
            "Rabih Zbib",
            "Richard Schwartz",
            "John Makhoul"
        ],
        "abstract": "We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various non-local translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese-English translation over a state-of-the-art system that already includes neural network features.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00799",
        "title": "Learning Speech Rate in Speech Recognition",
        "authors": [
            "Xiangyu Zeng",
            "Shi Yin",
            "Dong Wang"
        ],
        "abstract": "A significant performance reduction is often observed in speech recognition when the rate of speech (ROS) is too low or too high. Most of present approaches to addressing the ROS variation focus on the change of speech signals in dynamic properties caused by ROS, and accordingly modify the dynamic model, e.g., the transition probabilities of the hidden Markov model (HMM). However, an abnormal ROS changes not only the dynamic but also the static property of speech signals, and thus can not be compensated for purely by modifying the dynamic model. This paper proposes an ROS learning approach based on deep neural networks (DNN), which involves an ROS feature as the input of the DNN model and so the spectrum distortion caused by ROS can be learned and compensated for. The experimental results show that this approach can deliver better performance for too slow and too fast utterances, demonstrating our conjecture that ROS impacts both the dynamic and the static property of speech. In addition, the proposed approach can be combined with the conventional HMM transition adaptation method, offering additional performance gains.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00839",
        "title": "The Influence of Context on Dialogue Act Recognition",
        "authors": [
            "Eug\u00e9nio Ribeiro",
            "Ricardo Ribeiro",
            "David Martins de Matos"
        ],
        "abstract": "This article presents an analysis of the influence of context information on dialog act recognition. We performed experiments on the widely explored Switchboard corpus, as well as on data annotated according to the recent ISO 24617-2 standard. The latter was obtained from the Tilburg DialogBank and through the mapping of the annotations of a subset of the Let's Go corpus. We used a classification approach based on SVMs, which had proved successful in previous work and allowed us to limit the amount of context information provided. This way, we were able to observe the influence patterns as the amount of context information increased. Our base features consisted of n-grams, punctuation, and wh-words. Context information was obtained from one to five preceding segments and provided either as n-grams or dialog act classifications, with the latter typically leading to better results and more stable influence patterns. In addition to the conclusions about the importance and influence of context information, our experiments on the Switchboard corpus also led to results that advanced the state-of-the-art on the dialog act recognition task on that corpus. Furthermore, the results obtained on data annotated according to the ISO 24617-2 standard define a baseline for future work and contribute for the standardization of experiments in the area.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2017-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01057",
        "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents",
        "authors": [
            "Jiwei Li",
            "Minh-Thang Luong",
            "Dan Jurafsky"
        ],
        "abstract": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization\\footnote{Code for the three models described in this paper can be found at ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01066",
        "title": "Visualizing and Understanding Neural Models in NLP",
        "authors": [
            "Jiwei Li",
            "Xinlei Chen",
            "Eduard Hovy",
            "Dan Jurafsky"
        ],
        "abstract": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01070",
        "title": "Do Multi-Sense Embeddings Improve Natural Language Understanding?",
        "authors": [
            "Jiwei Li",
            "Dan Jurafsky"
        ],
        "abstract": "Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while `multi-sense' methods have been proposed and tested on artificial word-similarity tasks, we don't know if they improve real natural language understanding tasks. In this paper we introduce a multi-sense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language understanding.\n",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01094",
        "title": "Traversing Knowledge Graphs in Vector Space",
        "authors": [
            "Kelvin Guu",
            "John Miller",
            "Percy Liang"
        ],
        "abstract": "Path queries on a knowledge graph can be used to answer compositional questions such as \"What languages are spoken by people living in Lisbon?\". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \"compositional\" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01171",
        "title": "A Hybrid Model for Enhancing Lexical Statistical Machine Translation (SMT)",
        "authors": [
            "Ahmed G. M. ElSayed",
            "Ahmed S. Salama",
            "Alaa El-Din M. El-Ghazali"
        ],
        "abstract": "The interest in statistical machine translation systems increases currently due to political and social events in the world. A proposed Statistical Machine Translation (SMT) based model that can be used to translate a sentence from the source Language (English) to the target language (Arabic) automatically through efficiently incorporating different statistical and Natural Language Processing (NLP) models such as language model, alignment model, phrase based model, reordering model, and translation model. These models are combined to enhance the performance of statistical machine translation (SMT). Many implementation tools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, and BLEU. Based on the implementation, evaluation of this model, and comparing the generated translation with other implemented machine translation systems like Google Translate, it was proved that this proposed model has enhanced the results of the statistical machine translation, and forms a reliable and efficient model in this field of research.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01192",
        "title": "Personalizing Universal Recurrent Neural Network Language Model with User Characteristic Features by Social Network Crowdsouring",
        "authors": [
            "Bo-Hsiang Tseng",
            "Hung-Yi Lee",
            "Lin-Shan Lee"
        ],
        "abstract": "With the popularity of mobile devices, personalized speech recognizer becomes more realizable today and highly attractive. Each mobile device is primarily used by a single user, so it's possible to have a personalized recognizer well matching to the characteristics of individual user. Although acoustic model personalization has been investigated for decades, much less work have been reported on personalizing language model, probably because of the difficulties in collecting enough personalized corpora. Previous work used the corpora collected from social networks to solve the problem, but constructing a personalized model for each user is troublesome. In this paper, we propose a universal recurrent neural network language model with user characteristic features, so all users share the same model, except each with different user characteristic features. These user characteristic features can be obtained by crowdsouring over social networks, which include huge quantity of texts posted by users with known friend relationships, who may share some subject topics and wording patterns. The preliminary experiments on Facebook corpus showed that this proposed approach not only drastically reduced the model perplexity, but offered very good improvement in recognition accuracy in n-best rescoring tests. This approach also mitigated the data sparseness problem for personalized language models.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01273",
        "title": "Summarization of Films and Documentaries Based on Subtitles and Scripts",
        "authors": [
            "Marta Apar\u00edcio",
            "Paulo Figueiredo",
            "Francisco Raposo",
            "David Martins de Matos",
            "Ricardo Ribeiro",
            "Lu\u00eds Marujo"
        ],
        "abstract": "We assess the performance of generic text summarization algorithms applied to films and documentaries, using the well-known behavior of summarization of news articles as reference. We use three datasets: (i) news articles, (ii) film scripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics are used for comparing generated summaries against news abstracts, plot summaries, and synopses. We show that the best performing algorithms are LSA, for news articles and documentaries, and LexRank and Support Sets, for films. Despite the different nature of films and documentaries, their relative behavior is in accordance with that obtained for news articles.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01597",
        "title": "Abstractive Multi-Document Summarization via Phrase Selection and Merging",
        "authors": [
            "Lidong Bing",
            "Piji Li",
            "Yi Liao",
            "Wai Lam",
            "Weiwei Guo",
            "Rebecca J. Passonneau"
        ],
        "abstract": "We propose an abstraction-based multi-document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of-the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01906",
        "title": "Idioms-Proverbs Lexicon for Modern Standard Arabic and Colloquial Sentiment Analysis",
        "authors": [
            "Hossam S. Ibrahim",
            "Sherif M. Abdou",
            "Mervat Gheith"
        ],
        "abstract": "Although, the fair amount of works in sentiment analysis (SA) and opinion mining (OM) systems in the last decade and with respect to the performance of these systems, but it still not desired performance, especially for morphologically-Rich Language (MRL) such as Arabic, due to the complexities and challenges exist in the nature of the languages itself. One of these challenges is the detection of idioms or proverbs phrases within the writer text or comment. An idiom or proverb is a form of speech or an expression that is peculiar to itself. Grammatically, it cannot be understood from the individual meanings of its elements and can yield different sentiment when treats as separate words. Consequently, In order to facilitate the task of detection and classification of lexical phrases for automated SA systems, this paper presents AIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic (MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentence level with semantic orientation (positive or negative). The efforts of manually building and annotating the lexicon are reported. Moreover, we build a classifier that extracts idioms and proverbs, phrases from text using n-gram and similarity measure methods. Finally, several experiments were carried out on various data, including Arabic tweets and Arabic microblogs (hotel reservation, product reviews, and TV program comments) from publicly available Arabic online reviews websites (social media, blogs, forums, e-commerce web sites) to evaluate the coverage and accuracy of AIPSeLEX.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01914",
        "title": "Content Translation: Computer-assisted translation tool for Wikipedia articles",
        "authors": [
            "Niklas Laxstr\u00f6m",
            "Pau Giner",
            "Santhosh Thottingal"
        ],
        "abstract": "The quality and quantity of articles in each Wikipedia language varies greatly. Translating from another Wikipedia is a natural way to add more content, but the translation process is not properly supported in the software used by Wikipedia. Past computer-assisted translation tools built for Wikipedia are not commonly used. We created a tool that adapts to the specific needs of an open community and to the kind of content in Wikipedia. Qualitative and quantitative data indicates that the new tool helps users translate articles easier and faster.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02004",
        "title": "Sparse Overcomplete Word Vector Representations",
        "authors": [
            "Manaal Faruqui",
            "Yulia Tsvetkov",
            "Dani Yogatama",
            "Chris Dyer",
            "Noah Smith"
        ],
        "abstract": "Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02275",
        "title": "Confounds and Consequences in Geotagged Twitter Data",
        "authors": [
            "Umashanthi Pavalanathan",
            "Jacob Eisenstein"
        ],
        "abstract": "Twitter is often used in quantitative studies that identify geographically-preferred topics, writing styles, and entities. These studies rely on either GPS coordinates attached to individual messages, or on the user-supplied location field in each profile. In this paper, we compare these data acquisition techniques and quantify the biases that they introduce; we also measure their effects on linguistic analysis and text-based geolocation. GPS-tagging and self-reported locations yield measurably different corpora, and these linguistic differences are partially attributable to differences in dataset composition by age and gender. Using a latent variable model to induce age and gender, we show how these demographic variables interact with geography to affect language use. We also show that the accuracy of text-based geolocation varies with population demographics, giving the best results for men above the age of 40.\n    ",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2015-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02306",
        "title": "SQUINKY! A Corpus of Sentence-level Formality, Informativeness, and Implicature",
        "authors": [
            "Shibamouli Lahiri"
        ],
        "abstract": "We introduce a corpus of 7,032 sentences rated by human annotators for formality, informativeness, and implicature on a 1-7 scale. The corpus was annotated using Amazon Mechanical Turk. Reliability in the obtained judgments was examined by comparing mean ratings across two MTurk experiments, and correlation with pilot annotations (on sentence formality) conducted in a more controlled setting. Despite the subjectivity and inherent difficulty of the annotation task, correlations between mean ratings were quite encouraging, especially on formality and informativeness. We further explored correlation between the three linguistic variables, genre-wise variation of ratings and correlations within genres, compatibility with automatic stylistic scoring, and sentential make-up of a document in terms of style. To date, our corpus is the largest sentence-level annotated corpus released for formality, informativeness, and implicature.\n    ",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02327",
        "title": "A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for Unsupervised Discovery of Linguistic Units and Generation of High Quality Features",
        "authors": [
            "Cheng-Tao Chung",
            "Cheng-Yu Tsai",
            "Hsiang-Hung Lu",
            "Yuan-ming Liou",
            "Yen-Chen Wu",
            "Yen-Ju Lu",
            "Hung-yi Lee",
            "Lin-shan Lee"
        ],
        "abstract": "This paper summarizes the work done by the authors for the Zero Resource Speech Challenge organized in the technical program of Interspeech 2015. The goal of the challenge is to discover linguistic units directly from unlabeled speech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work automatically discovers multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics of the given corpus and the language behind thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target DNN (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are used as feedback for the MAT and the MDNN itself. We call this iterative system the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) which generates high quality features for track 1 of the challenge and acoustic tokens for track 2 of the challenge.\n    ",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2015-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02338",
        "title": "Modeling Order in Neural Word Embeddings at Scale",
        "authors": [
            "Andrew Trask",
            "David Gilmore",
            "Matthew Russell"
        ],
        "abstract": "Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02739",
        "title": "Connotation Frames: A Data-Driven Investigation",
        "authors": [
            "Hannah Rashkin",
            "Sameer Singh",
            "Yejin Choi"
        ],
        "abstract": "Through a particular choice of a predicate (e.g., \"x violated y\"), a writer can subtly connote a range of implied sentiments and presupposed facts about the entities x and y: (1) writer's perspective: projecting x as an \"antagonist\"and y as a \"victim\", (2) entities' perspective: y probably dislikes x, (3) effect: something bad happened to y, (4) value: y is something valuable, and (5) mental state: y is distressed by the event. We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations. First, we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments. We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations. Empirical results confirm that connotation frames can be induced from various data sources that reflect how people use language and give rise to the connotative meanings. We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2016-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02761",
        "title": "WordRank: Learning Word Embeddings via Robust Ranking",
        "authors": [
            "Shihao Ji",
            "Hyokun Yun",
            "Pinar Yanardag",
            "Shin Matsushima",
            "S. V. N. Vishwanathan"
        ],
        "abstract": "Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02816",
        "title": "Leveraging Textual Features for Best Answer Prediction in Community-based Question Answering",
        "authors": [
            "George Gkotsis",
            "Maria Liakata",
            "Carlos Pedrinaci",
            "John Domingue"
        ],
        "abstract": "This paper addresses the problem of determining the best answer in Community-based Question Answering (CQA) websites by focussing on the content. In particular, we present a system, ACQUA [",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02922",
        "title": "An Ensemble method for Content Selection for Data-to-text Systems",
        "authors": [
            "Dimitra Gkatzia",
            "Helen Hastie"
        ],
        "abstract": "We present a novel approach for automatic report generation from time-series data, in the context of student feedback generation. Our proposed methodology treats content selection as a multi-label classification (MLC) problem, which takes as input time-series data (students' learning data) and outputs a summary of these data (feedback). Unlike previous work, this method considers all data simultaneously using ensembles of classifiers, and therefore, it achieves higher accuracy and F- score compared to meaningful baselines.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03139",
        "title": "Robust Subgraph Generation Improves Abstract Meaning Representation Parsing",
        "authors": [
            "Keenon Werling",
            "Gabor Angeli",
            "Christopher Manning"
        ],
        "abstract": "The Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classifier. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 and LDC2014T12 datasets.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03229",
        "title": "A cognitive neural architecture able to learn and communicate through natural language",
        "authors": [
            "Bruno Golosio",
            "Angelo Cangelosi",
            "Olesya Gamotina",
            "Giovanni Luca Masala"
        ],
        "abstract": "Communicative interactions involve a kind of procedural knowledge that is used by the human brain for processing verbal and nonverbal inputs and for language production. Although considerable work has been done on modeling human language abilities, it has been difficult to bring them together to a comprehensive tabula rasa system compatible with current knowledge of how verbal information is processed in the brain. This work presents a cognitive system, entirely based on a large-scale neural architecture, which was developed to shed light on the procedural knowledge involved in language elaboration. The main component of this system is the central executive, which is a supervising system that coordinates the other components of the working memory. In our model, the central executive is a neural network that takes as input the neural activation states of the short-term memory and yields as output mental actions, which control the flow of information among the working memory components through neural gating mechanisms. The proposed system is capable of learning to communicate through natural language starting from tabula rasa, without any a priori knowledge of the structure of phrases, meaning of words, role of the different classes of words, only by interacting with a human through a text-based interface, using an open-ended incremental learning process. It is able to learn nouns, verbs, adjectives, pronouns and other word classes, and to use them in expressive language. The model was validated on a corpus of 1587 input sentences, based on literature on early language assessment, at the level of about 4-years old child, and produced 521 output sentences, expressing a broad range of language processing functionalities.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03257",
        "title": "Combining Temporal Information and Topic Modeling for Cross-Document Event Ordering",
        "authors": [
            "Borja Navarro-Colorado",
            "Estela Saquete"
        ],
        "abstract": "Building unified timelines from a collection of written news articles requires cross-document event coreference resolution and temporal relation extraction. In this paper we present an approach event coreference resolution according to: a) similar temporal information, and b) similar semantic arguments. Temporal information is detected using an automatic temporal information system (TIPSem), while semantic information is represented by means of LDA Topic Modeling. The evaluation of our approach shows that it obtains the highest Micro-average F-score results in the SemEval2015 Task 4: TimeLine: Cross-Document Event Ordering (25.36\\% for TrackB, 23.15\\% for SubtrackB), with an improvement of up to 6\\% in comparison to the other systems. However, our experiment also showed some draw-backs in the Topic Modeling approach that degrades performance of the system.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03340",
        "title": "Teaching Machines to Read and Comprehend",
        "authors": [
            "Karl Moritz Hermann",
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Edward Grefenstette",
            "Lasse Espeholt",
            "Will Kay",
            "Mustafa Suleyman",
            "Phil Blunsom"
        ],
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03487",
        "title": "From Paraphrase Database to Compositional Paraphrase Model and Back",
        "authors": [
            "John Wieting",
            "Mohit Bansal",
            "Kevin Gimpel",
            "Karen Livescu",
            "Dan Roth"
        ],
        "abstract": "The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list of phrase pairs with (heuristic) confidence estimates. However, it is still unclear how it can best be used, due to the heuristic nature of the confidences and its necessarily incomplete coverage. We propose models to leverage the phrase pairs from the PPDB to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB's internal scores while simultaneously improving its coverage. They allow for learning phrase embeddings as well as improved word embeddings. Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03694",
        "title": "Learning language through pictures",
        "authors": [
            "Grzegorz Chrupa\u0142a",
            "\u00c1kos K\u00e1d\u00e1r",
            "Afra Alishahi"
        ],
        "abstract": "We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.\n    ",
        "submission_date": "2015-06-11T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03775",
        "title": "Entity-Specific Sentiment Classification of Yahoo News Comments",
        "authors": [
            "Prakhar Biyani",
            "Cornelia Caragea",
            "Narayan Bhamidipati"
        ],
        "abstract": "Sentiment classification is widely used for product reviews and in online social media such as forums, Twitter, and blogs. However, the problem of classifying the sentiment of user comments on news sites has not been addressed yet. News sites cover a wide range of domains including politics, sports, technology, and entertainment, in contrast to other online social sites such as forums and review sites, which are specific to a particular domain. A user associated with a news site is likely to post comments on diverse topics (e.g., politics, smartphones, and sports) or diverse entities (e.g., Obama, iPhone, or Google). Classifying the sentiment of users tied to various entities may help obtain a holistic view of their personality, which could be useful in applications such as online advertising, content personalization, and political campaign planning. In this paper, we formulate the problem of entity-specific sentiment classification of comments posted on news articles in Yahoo News and propose novel features that are specific to news comments. Experimental results show that our models outperform state-of-the-art baselines.\n    ",
        "submission_date": "2015-06-11T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04089",
        "title": "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences",
        "authors": [
            "Hongyuan Mei",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence \"regions\" salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or task-specific annotations (e.g., seed lexicons). It is therefore generalizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04228",
        "title": "A Publicly Available Cross-Platform Lemmatizer for Bulgarian",
        "authors": [
            "Grigor Iliev",
            "Nadezhda Borisova",
            "Elena Karashtranova",
            "Dafina Kostadinova"
        ],
        "abstract": "Our dictionary-based lemmatizer for the Bulgarian language presented here is distributed as free software, publicly available to download and use under the GPL v3 license. The presented software is written entirely in Java and is distributed as a GATE plugin. To our best knowledge, at the time of writing this article, there are not any other free lemmatization tools specifically targeting the Bulgarian language. The presented lemmatizer is a work in progress and currently yields an accuracy of about 95% in comparison to the manually annotated corpus BulTreeBank-Morph, which contains 273933 tokens.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2015-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04229",
        "title": "Evaluation of the Accuracy of the BGLemmatizer",
        "authors": [
            "Elena Karashtranova",
            "Grigor Iliev",
            "Nadezhda Borisova",
            "Yana Chankova",
            "Irena Atanasova"
        ],
        "abstract": "This paper reveals the results of an analysis of the accuracy of developed software for automatic lemmatization for the Bulgarian language. This lemmatization software is written entirely in Java and is distributed as a GATE plugin. Certain statistical methods are used to define the accuracy of this software. The results of the analysis show 95% lemmatization accuracy.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2015-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04334",
        "title": "A Bayesian Model for Generative Transition-based Dependency Parsing",
        "authors": [
            "Jan Buys",
            "Phil Blunsom"
        ],
        "abstract": "We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2015-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04365",
        "title": "Leveraging Word Embeddings for Spoken Document Summarization",
        "authors": [
            "Kuan-Yu Chen",
            "Shih-Hung Liu",
            "Hsin-Min Wang",
            "Berlin Chen",
            "Hsin-Hsi Chen"
        ],
        "abstract": "Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04488",
        "title": "Distilling Word Embeddings: An Encoding Approach",
        "authors": [
            "Lili Mou",
            "Ran Jia",
            "Yan Xu",
            "Ge Li",
            "Lu Zhang",
            "Zhi Jin"
        ],
        "abstract": "Distilling knowledge from a well-trained cumbersome network to a small one has recently become a new research topic, as lightweight neural networks with high performance are particularly in need in various resource-restricted systems. This paper addresses the problem of distilling word embeddings for NLP tasks. We propose an encoding approach to distill task-specific knowledge from a set of high-dimensional embeddings, which can reduce model complexity by a large margin as well as retain high accuracy, showing a good compromise between efficiency and performance. Experiments in two tasks reveal the phenomenon that distilling knowledge from cumbersome embeddings is better than directly training neural networks with small embeddings.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2016-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04744",
        "title": "Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game",
        "authors": [
            "Vlad Niculae",
            "Srijan Kumar",
            "Jordan Boyd-Graber",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Interpersonal relations are fickle, with close friendships often dissolving into enmity. In this work, we explore linguistic cues that presage such transitions by studying dyadic interactions in an online strategy game where players form alliances and break those alliances through betrayal. We characterize friendships that are unlikely to last and examine temporal patterns that foretell betrayal.\n",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04803",
        "title": "Exploiting Text and Network Context for Geolocation of Social Media Users",
        "authors": [
            "Afshin Rahimi",
            "Duy Vu",
            "Trevor Cohn",
            "Timothy Baldwin"
        ],
        "abstract": "Research on automatically geolocating social media users has conventionally been based on the text content of posts from a given user or the social network of the user, with very little crossover between the two, and no bench-marking of the two approaches over compara- ble datasets. We bring the two threads of research together in first proposing a text-based method based on adaptive grids, followed by a hybrid network- and text-based method. Evaluating over three Twitter datasets, we show that the empirical difference between text- and network-based methods is not great, and that hybridisation of the two is superior to the component methods, especially in contexts where the user graph is not well connected. We achieve state-of-the-art results on all three datasets.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04828",
        "title": "Significance of the levels of spectral valleys with application to front/back distinction of vowel sounds",
        "authors": [
            "T. V. Ananthapadmanabha",
            "A. G. Ramakrishnan",
            "Shubham Sharma"
        ],
        "abstract": "An objective critical distance (OCD) has been defined as that spacing between adjacent formants, when the level of the valley between them reaches the mean spectral level. The measured OCD lies in the same range (viz., 3-3.5 bark) as the critical distance determined by subjective experiments for similar experimental conditions. The level of spectral valley serves a purpose similar to that of the spacing between the formants with an added advantage that it can be measured from the spectral envelope without an explicit knowledge of formant frequencies. Based on the relative spacing of formant frequencies, the level of the spectral valley, VI (between F1 and F2) is much higher than the level of VII (spectral valley between F2 and F3) for back vowels and vice-versa for front vowels. Classification of vowels into front/back distinction with the difference (VI-VII) as an acoustic feature, tested using TIMIT, NTIMIT, Tamil and Kannada language databases gives, on the average, an accuracy of about 95%, which is comparable to the accuracy (90.6%) obtained using a neural network classifier trained and tested using MFCC as the feature vector for TIMIT database. The acoustic feature (VI-VII) has also been tested for its robustness on the TIMIT database for additive white and babble noise and an accuracy of about 95% has been obtained for SNRs down to 25 dB for both types of noise.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04834",
        "title": "Tree-structured composition in neural networks without tree-structured architectures",
        "authors": [
            "Samuel R. Bowman",
            "Christopher D. Manning",
            "Christopher Potts"
        ],
        "abstract": "Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find an LSTM-based sequence model can indeed learn to exploit the underlying tree structure. However, its performance consistently lags behind that of tree models, even on large training sets, suggesting that tree-structured models are more effective at exploiting recursive structure.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04891",
        "title": "Author Identification using Multi-headed Recurrent Neural Networks",
        "authors": [
            "Douglas Bagnall"
        ],
        "abstract": "Recurrent neural networks (RNNs) are very good at modelling the flow of text, but typically need to be trained on a far larger corpus than is available for the PAN 2015 Author Identification task. This paper describes a novel approach where the output layer of a character-level RNN language model is split into several independent predictive sub-models, each representing an author, while the recurrent layer is shared by all. This allows the recurrent layer to model the language as a whole without over-fitting, while the outputs select aspects of the underlying model that reflect their author's style. The method proves competitive, ranking first in two of the four languages.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04897",
        "title": "Parsing Natural Language Sentences by Semi-supervised Methods",
        "authors": [
            "Rudolf Rosa"
        ],
        "abstract": "We present our work on semi-supervised parsing of natural language sentences, focusing on multi-source crosslingual transfer of delexicalized dependency parsers. We first evaluate the influence of treebank annotation styles on parsing performance, focusing on adposition attachment style. Then, we present KLcpos3, an empirical language similarity measure, designed and tuned for source parser weighting in multi-source delexicalized parser transfer. And finally, we introduce a novel resource combination method, based on interpolation of trained parser models.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04940",
        "title": "Recognize Foreign Low-Frequency Words with Similar Pairs",
        "authors": [
            "Xi Ma",
            "Xiaoxi Wang",
            "Dong Wang",
            "Zhiyong Zhang"
        ],
        "abstract": "Low-frequency words place a major challenge for automatic speech recognition (ASR). The probabilities of these words, which are often important name entities, are generally under-estimated by the language model (LM) due to their limited occurrences in the training data. Recently, we proposed a word-pair approach to deal with the problem, which borrows information of frequent words to enhance the probabilities of low-frequency words. This paper presents an extension to the word-pair method by involving multiple `predicting words' to produce better estimation for low-frequency words. We also employ this approach to deal with out-of-language words in the task of multi-lingual speech recognition.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05012",
        "title": "Emotion Analysis of Songs Based on Lyrical and Audio Features",
        "authors": [
            "Adit Jamdar",
            "Jessica Abraham",
            "Karishma Khanna",
            "Rahul Dubey"
        ],
        "abstract": "In this paper, a method is proposed to detect the emotion of a song based on its lyrical and audio features. Lyrical features are generated by segmentation of lyrics during the process of data extraction. ANEW and WordNet knowledge is then incorporated to compute Valence and Arousal values. In addition to this, linguistic association rules are applied to ensure that the issue of ambiguity is properly addressed. Audio features are used to supplement the lyrical ones and include attributes like energy, tempo, and danceability. These features are extracted from The Echo Nest, a widely used music intelligence platform. Construction of training and test sets is done on the basis of social tags extracted from the ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05230",
        "title": "Non-distributional Word Vector Representations",
        "authors": [
            "Manaal Faruqui",
            "Chris Dyer"
        ],
        "abstract": "Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05402",
        "title": "Editorial for the First Workshop on Mining Scientific Papers: Computational Linguistics and Bibliometrics",
        "authors": [
            "Iana Atanassova",
            "Marc Bertin",
            "Philipp Mayr"
        ],
        "abstract": "The workshop \"Mining Scientific Papers: Computational Linguistics and Bibliometrics\" (CLBib 2015), co-located with the 15th International Society of Scientometrics and Informetrics Conference (ISSI 2015), brought together researchers in Bibliometrics and Computational Linguistics in order to study the ways Bibliometrics can benefit from large-scale text analytics and sense mining of scientific papers, thus exploring the interdisciplinarity of Bibliometrics and Natural Language Processing (NLP). The goals of the workshop were to answer questions like: How can we enhance author network analysis and Bibliometrics using data obtained by text analytics? What insights can NLP provide on the structure of scientific writing, on citation networks, and on in-text citation analysis? This workshop is the first step to foster the reflection on the interdisciplinarity and the benefits that the two disciplines Bibliometrics and Natural Language Processing can drive from it.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05561",
        "title": "Comparing and evaluating extended Lambek calculi",
        "authors": [
            "Richard Moot"
        ],
        "abstract": "Lambeks Syntactic Calculus, commonly referred to as the Lambek calculus, was innovative in many ways, notably as a precursor of linear logic. But it also showed that we could treat our grammatical framework as a logic (as opposed to a logical theory). However, though it was successful in giving at least a basic treatment of many linguistic phenomena, it was also clear that a slightly more expressive logical calculus was needed for many other cases. Therefore, many extensions and variants of the Lambek calculus have been proposed, since the eighties and up until the present day. As a result, there is now a large class of calculi, each with its own empirical successes and theoretical results, but also each with its own logical primitives. This raises the question: how do we compare and evaluate these different logical formalisms? To answer this question, I present two unifying frameworks for these extended Lambek calculi. Both are proof net calculi with graph contraction criteria. The first calculus is a very general system: you specify the structure of your sequents and it gives you the connectives and contractions which correspond to it. The calculus can be extended with structural rules, which translate directly into graph rewrite rules. The second calculus is first-order (multiplicative intuitionistic) linear logic, which turns out to have several other, independently proposed extensions of the Lambek calculus as fragments. I will illustrate the use of each calculus in building bridges between analyses proposed in different frameworks, in highlighting differences and in helping to identify problems.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05676",
        "title": "Pragmatic Side Effects",
        "authors": [
            "Jiri Marsik",
            "Maxime Amblard"
        ],
        "abstract": "In the quest to give a formal compositional semantics to natural languages, semanticists have started turning their attention to phenomena that have been also considered as parts of pragmatics (e.g., discourse anaphora and presupposition projection). To account for these phenomena, the very kinds of meanings assigned to words and phrases are often revisited. To be more specific, in the prevalent paradigm of modeling natural language denotations using the simply-typed lambda calculus (higher-order logic) this means revisiting the types of denotations assigned to individual parts of speech. However, the lambda calculus also serves as a fundamental theory of computation, and in the study of computation, similar type shifts have been employed to give a meaning to side effects. Side effects in programming languages correspond to actions that go beyond the lexical scope of an expression (a thrown exception might propagate throughout a program, a variable modified at one point might later be read at an another) or even beyond the scope of the program itself (a program might interact with the outside world by e.g., printing documents, making sounds, operating robotic limbs...).\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05702",
        "title": "Comparing the writing style of real and artificial papers",
        "authors": [
            "Diego R. Amancio"
        ],
        "abstract": "Recent years have witnessed the increase of competition in science. While promoting the quality of research in many cases, an intense competition among scientists can also trigger unethical scientific behaviors. To increase the total number of published papers, some authors even resort to software tools that are able to produce grammatical, but meaningless scientific manuscripts. Because automatically generated papers can be misunderstood as real papers, it becomes of paramount importance to develop means to identify these scientific frauds. In this paper, I devise a methodology to distinguish real manuscripts from those generated with SCIGen, an automatic paper generator. Upon modeling texts as complex networks (CN), it was possible to discriminate real from fake papers with at least 89\\% of accuracy. A systematic analysis of features relevance revealed that the accessibility and betweenness were useful in particular cases, even though the relevance depended upon the dataset. The successful application of the methods described here show, as a proof of principle, that network features can be used to identify scientific gibberish papers. In addition, the CN-based approach can be combined in a straightforward fashion with traditional statistical language processing methods to improve the performance in identifying artificially generated papers.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05703",
        "title": "\"The Sum of Its Parts\": Joint Learning of Word and Phrase Representations with Autoencoders",
        "authors": [
            "R\u00e9mi Lebret",
            "Ronan Collobert"
        ],
        "abstract": "Recently, there has been a lot of effort to represent words in continuous vector spaces. Those representations have been shown to capture both semantic and syntactic information about words. However, distributed representations of phrases remain a challenge. We introduce a novel model that jointly learns word vector representations and their summation. Word representations are learnt using the word co-occurrence statistical information. To embed sequences of words (i.e. phrases) with different sizes into a common semantic space, we propose to average word vector representations. In contrast with previous methods which reported a posteriori some compositionality aspects by simple summation, we simultaneously train words to sum, while keeping the maximum information from the original vectors. We evaluate the quality of the word representations on several classical word evaluation tasks, and we introduce a novel task to evaluate the quality of the phrase representations. While our distributed representations compete with other methods of learning word representations on word evaluations, we show that they give better performance on the phrase evaluation. Such representations of phrases could be interesting for many tasks in natural language processing.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05865",
        "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset",
        "authors": [
            "Baotian Hu",
            "Qingcai Chen",
            "Fangze Zhu"
        ],
        "abstract": "Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public {",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2016-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05869",
        "title": "A Neural Conversational Model",
        "authors": [
            "Oriol Vinyals",
            "Quoc Le"
        ],
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06158",
        "title": "Structured Training for Neural Network Transition-Based Parsing",
        "authors": [
            "David Weiss",
            "Chris Alberti",
            "Michael Collins",
            "Slav Petrov"
        ],
        "abstract": "We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06418",
        "title": "Extreme Extraction: Only One Hour per Relation",
        "authors": [
            "Raphael Hoffmann",
            "Luke Zettlemoyer",
            "Daniel S. Weld"
        ],
        "abstract": "Information Extraction (IE) aims to automatically generate a large knowledge base from natural language text, but progress remains slow. Supervised learning requires copious human annotation, while unsupervised and weakly supervised approaches do not deliver competitive accuracy. As a result, most fielded applications of IE, as well as the leading TAC-KBP systems, rely on significant amounts of manual engineering. Even \"Extreme\" methods, such as those reported in Freedman et al. 2011, require about 10 hours of expert labor per relation.\n",
        "submission_date": "2015-06-21T00:00:00",
        "last_modified_date": "2015-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06442",
        "title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning",
        "authors": [
            "Fandong Meng",
            "Zhengdong Lu",
            "Zhaopeng Tu",
            "Hang Li",
            "Qun Liu"
        ],
        "abstract": "We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequenceto-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improving upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06490",
        "title": "Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering",
        "authors": [
            "Xiaoqiang Zhou",
            "Baotian Hu",
            "Qingcai Chen",
            "Buzhou Tang",
            "Xiaolong Wang"
        ],
        "abstract": "In this paper, the answer selection problem in community question answering (CQA) is regarded as an answer sequence labeling task, and a novel approach is proposed based on the recurrent architecture for this problem. Our approach applies convolution neural networks (CNNs) to learning the joint representation of question-answer pair firstly, and then uses the joint representation as input of the long short-term memory (LSTM) to learn the answer sequence of a question for labeling the matching quality of each answer. Experiments conducted on the SemEval 2015 CQA dataset shows the effectiveness of our approach.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06534",
        "title": "Distributional Sentence Entailment Using Density Matrices",
        "authors": [
            "Esma Balkir",
            "Mehrnoosh Sadrzadeh",
            "Bob Coecke"
        ],
        "abstract": "Categorical compositional distributional model of Coecke et al. (2010) suggests a way to combine grammatical composition of the formal, type logical models with the corpus based, empirical word representations of distributional semantics. This paper contributes to the project by expanding the model to also capture entailment relations. This is achieved by extending the representations of words from points in meaning space to density operators, which are probability distributions on the subspaces of the space. A symmetric measure of similarity and an asymmetric measure of entailment is defined, where lexical entailment is measured using von Neumann entropy, the quantum variant of Kullback-Leibler divergence. Lexical entailment, combined with the composition map on word representations, provides a method to obtain entailment relations on the level of sentences. Truth theoretic and corpus-based examples are provided.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06714",
        "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses",
        "authors": [
            "Alessandro Sordoni",
            "Michel Galley",
            "Michael Auli",
            "Chris Brockett",
            "Yangfeng Ji",
            "Margaret Mitchell",
            "Jian-Yun Nie",
            "Jianfeng Gao",
            "Bill Dolan"
        ],
        "abstract": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06726",
        "title": "Skip-Thought Vectors",
        "authors": [
            "Ryan Kiros",
            "Yukun Zhu",
            "Ruslan Salakhutdinov",
            "Richard S. Zemel",
            "Antonio Torralba",
            "Raquel Urtasun",
            "Sanja Fidler"
        ],
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06833",
        "title": "A Survey of Current Datasets for Vision and Language Research",
        "authors": [
            "Francis Ferraro",
            "Nasrin Mostafazadeh",
            "Ting-Hao",
            "Huang",
            "Lucy Vanderwende",
            "Jacob Devlin",
            "Michel Galley",
            "Margaret Mitchell"
        ],
        "abstract": "Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06863",
        "title": "deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets",
        "authors": [
            "Michel Galley",
            "Chris Brockett",
            "Alessandro Sordoni",
            "Yangfeng Ji",
            "Michael Auli",
            "Chris Quirk",
            "Margaret Mitchell",
            "Jianfeng Gao",
            "Bill Dolan"
        ],
        "abstract": "We introduce Discriminative BLEU (deltaBLEU), a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs. Reference strings are scored for quality by human raters on a scale of [-1, +1] to weight multi-reference BLEU. In tasks involving generation of conversational responses, deltaBLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman's rho and Kendall's tau.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06904",
        "title": "New Approach to translation of Isolated Units in English-Korean Machine Translation",
        "authors": [
            "Kim Song Jon",
            "An Hae Gum"
        ],
        "abstract": "It is the most effective way for quick translation of tremendous amount of explosively increasing science and technique information material to develop a practicable machine translation system and introduce it into translation practice. This essay treats problems arising from translation of isolated units on the basis of the practical materials and experiments obtained in the development and introduction of English-Korean machine translation system. In other words, this essay considers establishment of information for isolated units and their Korean equivalents and word order.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07190",
        "title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks",
        "authors": [
            "Nikola Mrk\u0161i\u0107",
            "Diarmuid \u00d3 S\u00e9aghdha",
            "Blaise Thomson",
            "Milica Ga\u0161i\u0107",
            "Pei-Hao Su",
            "David Vandyke",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "Dialog state tracking is a key component of many modern dialog systems, most of which are designed with a single, well-defined domain in mind. This paper shows that dialog data drawn from different dialog domains can be used to train a general belief tracking model which can operate across all of these domains, exhibiting superior performance to each of the domain-specific models. We propose a training procedure which uses out-of-domain data to initialise belief tracking models for entirely new domains. This procedure leads to improvements in belief tracking performance regardless of the amount of in-domain data available for training the model.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07285",
        "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
        "authors": [
            "Ankit Kumar",
            "Ozan Irsoy",
            "Peter Ondruska",
            "Mohit Iyyer",
            "James Bradbury",
            "Ishaan Gulrajani",
            "Victor Zhong",
            "Romain Paulus",
            "Richard Socher"
        ],
        "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2016-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07503",
        "title": "Attention-Based Models for Speech Recognition",
        "authors": [
            "Jan Chorowski",
            "Dzmitry Bahdanau",
            "Dmitriy Serdyuk",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ],
        "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07650",
        "title": "Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling",
        "authors": [
            "Kun Xu",
            "Yansong Feng",
            "Songfang Huang",
            "Dongyan Zhao"
        ],
        "abstract": "Syntactic features play an essential role in identifying relationship in a sentence. Previous neural network models often suffer from irrelevant information introduced when subjects and objects are in a long distance. In this paper, we propose to learn more robust relation representations from the shortest dependency path through a convolution neural network. We further propose a straightforward negative sampling strategy to improve the assignment of subjects and objects. Experimental results show that our method outperforms the state-of-the-art methods on the SemEval-2010 Task 8 dataset.\n    ",
        "submission_date": "2015-06-25T00:00:00",
        "last_modified_date": "2015-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08052",
        "title": "Automagically encoding Adverse Drug Reactions in MedDRA",
        "authors": [
            "Carlo Combi",
            "Riccardo Lora",
            "Ugo Moretti",
            "Marco Pagliarini",
            "Margherita Zorzi"
        ],
        "abstract": "Pharmacovigilance is the field of science devoted to the collection, analysis and prevention of Adverse Drug Reactions (ADRs). Efficient strategies for the extraction of information about ADRs from free text resources are essential to support the work of experts, employed in the crucial task of detecting and classifying unexpected pathologies possibly related to drug assumptions. Narrative ADR descriptions may be collected in several way, e.g. by monitoring social networks or through the so called spontaneous reporting, the main method pharmacovigilance adopts in order to identify ADRs. The encoding of free-text ADR descriptions according to MedDRA standard terminology is central for report analysis. It is a complex work, which has to be manually implemented by the pharmacovigilance experts. The manual encoding is expensive (in terms of time). Moreover, a problem about the accuracy of the encoding may occur, since the number of reports is growing up day by day. In this paper, we propose MagiCoder, an efficient Natural Language Processing algorithm able to automatically derive MedDRA terminologies from free-text ADR descriptions. MagiCoder is part of VigiWork, a web application for online ADR reporting and analysis. From a practical view-point, MagiCoder radically reduces the revision time of ADR reports: the pharmacologist has simply to revise and validate the automatic solution versus the hard task of choosing solutions in the 70k terms of MedDRA. This improvement of the expert work efficiency has a meaningful impact on the quality of data analysis. Moreover, our procedure is general purpose. We developed MagiCoder for the Italian pharmacovigilance language, but preliminarily analyses show that it is robust to language and dictionary changes.\n    ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2017-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08126",
        "title": "Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest",
        "authors": [
            "Dragomir Radev",
            "Amanda Stent",
            "Joel Tetreault",
            "Aasish Pappu",
            "Aikaterini Iliakopoulou",
            "Agustin Chanfreau",
            "Paloma de Juan",
            "Jordi Vallmitjana",
            "Alejandro Jaimes",
            "Rahul Jha",
            "Bob Mankoff"
        ],
        "abstract": "The New Yorker publishes a weekly captionless cartoon. More than 5,000 readers submit captions for it. The editors select three of them and ask the readers to pick the funniest one. We describe an experiment that compares a dozen automatic methods for selecting the funniest caption. We show that negative sentiment, human-centeredness, and lexical centrality most strongly match the funniest captions, followed by positive sentiment. These results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal (vision+text) systems. As part of this work, a large set of cartoons and captions is being made available to the community.\n    ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2015-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08259",
        "title": "Twitter User Geolocation Using a Unified Text and Network Prediction Model",
        "authors": [
            "Afshin Rahimi",
            "Trevor Cohn",
            "Timothy Baldwin"
        ],
        "abstract": "We propose a label propagation approach to geolocation prediction based on Modified Adsorption, with two enhancements:(1) the removal of \"celebrity\" nodes to increase location homophily and boost tractability, and (2) he incorporation of text-based geolocation priors for test users. Experiments over three Twitter benchmark datasets achieve state-of-the-art results, and demonstrate the effectiveness of the enhancements.\n    ",
        "submission_date": "2015-06-27T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08349",
        "title": "Improved Deep Speaker Feature Learning for Text-Dependent Speaker Recognition",
        "authors": [
            "Lantian Li",
            "Yiye Lin",
            "Zhiyong Zhang",
            "Dong Wang"
        ],
        "abstract": "A deep learning approach has been proposed recently to derive speaker identifies (d-vector) by a deep neural network (DNN). This approach has been applied to text-dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i-vector approach. Although promising, the existing d-vector implementation still can not compete with the i-vector baseline. This paper presents two improvements for the deep learning approach: a phonedependent DNN structure to normalize phone variation, and a new scoring approach based on dynamic time warping (DTW). Experiments on a text-dependent speaker recognition task demonstrated that the proposed methods can provide considerable performance improvement over the existing d-vector implementation.\n    ",
        "submission_date": "2015-06-28T00:00:00",
        "last_modified_date": "2015-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08422",
        "title": "Topic2Vec: Learning Distributed Representations of Topics",
        "authors": [
            "Li-Qiang Niu",
            "Xin-Yu Dai"
        ],
        "abstract": "Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.\n    ",
        "submission_date": "2015-06-28T00:00:00",
        "last_modified_date": "2015-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08454",
        "title": "WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Visual Information Extraction",
        "authors": [
            "Vijil Chenthamarakshan",
            "Prasad M Desphande",
            "Raghu Krishnapuram",
            "Ramakrishna Varadarajan",
            "Knut Stolze"
        ],
        "abstract": "The visual layout of a webpage can provide valuable clues for certain types of Information Extraction (IE) tasks. In traditional rule based IE frameworks, these layout cues are mapped to rules that operate on the HTML source of the webpages. In contrast, we have developed a framework in which the rules can be specified directly at the layout level. This has many advantages, since the higher level of abstraction leads to simpler extraction rules that are largely independent of the source code of the page, and, therefore, more robust. It can also enable specification of new types of rules that are not otherwise possible. To the best of our knowledge, there is no general framework that allows declarative specification of information extraction rules based on spatial layout. Our framework is complementary to traditional text based rules framework and allows a seamless combination of spatial layout based rules with traditional text based rules. We describe the algebra that enables such a system and its efficient implementation using standard relational and text indexing features of a relational database. We demonstrate the simplicity and efficiency of this system for a task involving the extraction of software system requirements from software product pages.\n    ",
        "submission_date": "2015-06-28T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08663",
        "title": "Linguistics and some aspects of its underlying dynamics",
        "authors": [
            "Massimo Piattelli-Palmarini",
            "Giuseppe Vitiello"
        ],
        "abstract": "In recent years, central components of a new approach to linguistics, the Minimalist Program (MP) have come closer to physics. Features of the Minimalist Program, such as the unconstrained nature of recursive Merge, the operation of the Labeling Algorithm that only operates at the interface of Narrow Syntax with the Conceptual-Intentional and the Sensory-Motor interfaces, the difference between pronounced and un-pronounced copies of elements in a sentence and the build-up of the Fibonacci sequence in the syntactic derivation of sentence structures, are directly accessible to representation in terms of algebraic formalism. Although in our scheme linguistic structures are classical ones, we find that an interesting and productive isomorphism can be established between the MP structure, algebraic structures and many-body field theory opening new avenues of inquiry on the dynamics underlying some central aspects of linguistics.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08909",
        "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
        "authors": [
            "Ryan Lowe",
            "Nissan Pow",
            "Iulian Serban",
            "Joelle Pineau"
        ],
        "abstract": "This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08941",
        "title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
        "authors": [
            "Karthik Narasimhan",
            "Tejas Kulkarni",
            "Regina Barzilay"
        ],
        "abstract": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09107",
        "title": "A complex network approach to stylometry",
        "authors": [
            "Diego R. Amancio"
        ],
        "abstract": "Statistical methods have been widely employed to study the fundamental properties of language. In recent years, methods from complex and dynamical systems proved useful to create several language models. Despite the large amount of studies devoted to represent texts with physical models, only a limited number of studies have shown how the properties of the underlying physical systems can be employed to improve the performance of natural language processing tasks. In this paper, I address this problem by devising complex networks methods that are able to improve the performance of current statistical methods. Using a fuzzy classification strategy, I show that the topological properties extracted from texts complement the traditional textual description. In several cases, the performance obtained with hybrid approaches outperformed the results obtained when only traditional or networked methods were used. Because the proposed model is generic, the framework devised here could be straightforwardly used to study similar textual applications where the topology plays a pivotal role in the description of the interacting agents.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00133",
        "title": "Prior Polarity Lexical Resources for the Italian Language",
        "authors": [
            "Valeria Borz\u00ec",
            "Simone Faro",
            "Arianna Pavone",
            "Sabrina Sansone"
        ],
        "abstract": "In this paper we present SABRINA (Sentiment Analysis: a Broad Resource for Italian Natural language Applications) a manually annotated prior polarity lexical resource for Italian natural language applications in the field of opinion mining and sentiment induction. The resource consists in two different sets, an Italian dictionary of more than 277.000 words tagged with their prior polarity value, and a set of polarity modifiers, containing more than 200 words, which can be used in combination with non neutral terms of the dictionary in order to induce the sentiment of Italian compound terms. To the best of our knowledge this is the first prior polarity manually annotated resource which has been developed for the Italian natural language.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00209",
        "title": "Dimensionality on Summarization",
        "authors": [
            "Hai Zhuge"
        ],
        "abstract": "Summarization is one of the key features of human intelligence. It plays an important role in understanding and representation. With rapid and continual expansion of texts, pictures and videos in cyberspace, automatic summarization becomes more and more desirable. Text summarization has been studied for over half century, but it is still hard to automatically generate a satisfied summary. Traditional methods process texts empirically and neglect the fundamental characteristics and principles of language use and understanding. This paper summarizes previous text summarization approaches in a multi-dimensional classification space, introduces a multi-dimensional methodology for research and development, unveils the basic characteristics and principles of language use and understanding, investigates some fundamental mechanisms of summarization, studies the dimensions and forms of representations, and proposes a multi-dimensional evaluation mechanisms. Investigation extends to the incorporation of pictures into summary and to the summarization of videos, graphs and pictures, and then reaches a general summarization framework.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00639",
        "title": "Simple, Fast Semantic Parsing with a Tensor Kernel",
        "authors": [
            "Daoud Clarke"
        ],
        "abstract": "We describe a simple approach to semantic parsing based on a tensor product kernel. We extract two feature vectors: one for the query and one for each candidate logical form. We then train a classifier using the tensor product of the two vectors. Using very simple features for both, our system achieves an average F1 score of 40.1% on the WebQuestions dataset. This is comparable to more complex systems but is simpler to implement and runs faster.\n    ",
        "submission_date": "2015-07-02T00:00:00",
        "last_modified_date": "2015-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00955",
        "title": "Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination",
        "authors": [
            "Olga Kolchyna",
            "Tharsis T. P. Souza",
            "Philip Treleaven",
            "Tomaso Aste"
        ],
        "abstract": "This paper covers the two approaches for sentiment analysis: i) lexicon based method; ii) machine learning method. We describe several techniques to implement these approaches and discuss how they can be adopted for sentiment classification of Twitter messages. We present a comparative study of different lexicon combinations and show that enhancing sentiment lexicons with emoticons, abbreviations and social-media slang expressions increases the accuracy of lexicon-based classification for Twitter. We discuss the importance of feature generation and feature selection processes for machine learning sentiment classification. To quantify the performance of the main sentiment analysis methods over Twitter we run these algorithms on a benchmark Twitter dataset from the SemEval-2013 competition, task 2-B. The results show that machine learning method based on SVM and Naive Bayes classifiers outperforms the lexicon method. We present a new ensemble method that uses a lexicon based sentiment score as input feature for the machine learning approach. The combined method proved to produce more precise classifications. We also show that employing a cost-sensitive classifier for highly unbalanced datasets yields an improvement of sentiment classification performance up to 7%.\n    ",
        "submission_date": "2015-07-03T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01127",
        "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes",
        "authors": [
            "Sascha Rothe",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We present \\textit{AutoExtend}, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks.\n    ",
        "submission_date": "2015-07-04T00:00:00",
        "last_modified_date": "2015-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01193",
        "title": "Dependency Recurrent Neural Language Models for Sentence Completion",
        "authors": [
            "Piotr Mirowski",
            "Andreas Vlachos"
        ],
        "abstract": "Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the state-of-the-art models on this task.\n    ",
        "submission_date": "2015-07-05T00:00:00",
        "last_modified_date": "2015-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01529",
        "title": "Correspondence Factor Analysis of Big Data Sets: A Case Study of 30 Million Words; and Contrasting Analytics using Apache Solr and Correspondence Analysis in R",
        "authors": [
            "Fionn Murtagh"
        ],
        "abstract": "We consider a large number of text data sets. These are cooking recipes. Term distribution and other distributional properties of the data are investigated. Our aim is to look at various analytical approaches which allow for mining of information on both high and low detail scales. Metric space embedding is fundamental to our interest in the semantic properties of this data. We consider the projection of all data into analyses of aggregated versions of the data. We contrast that with projection of aggregated versions of the data into analyses of all the data. Analogously for the term set, we look at analysis of selected terms. We also look at inherent term associations such as between singular and plural. In addition to our use of Correspondence Analysis in R, for latent semantic space mapping, we also use Apache Solr. Setting up the Solr server and carrying out querying is described. A further novelty is that querying is supported in Solr based on the principal factor plane mapping of all the data. This uses a bounding box query, based on factor projections.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01636",
        "title": "Reflections on Sentiment/Opinion Analysis",
        "authors": [
            "Jiwei Li",
            "Eduard Hovy"
        ],
        "abstract": "In this paper, we described possible directions for deeper understanding, helping bridge the gap between psychology / cognitive science and computational approaches in sentiment/opinion analysis literature. We focus on the opinion holder's underlying needs and their resultant goals, which, in a utilitarian model of sentiment, provides the basis for explaining the reason a sentiment valence is held. While these thoughts are still immature, scattered, unstructured, and even imaginary, we believe that these perspectives might suggest fruitful avenues for various kinds of future work.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01701",
        "title": "A Survey and Classification of Controlled Natural Languages",
        "authors": [
            "Tobias Kuhn"
        ],
        "abstract": "What is here called controlled natural language (CNL) has traditionally been given many different names. Especially during the last four decades, a wide variety of such languages have been designed. They are applied to improve communication among humans, to improve translation, or to provide natural and intuitive representations for formal notations. Despite the apparent differences, it seems sensible to put all these languages under the same umbrella. To bring order to the variety of languages, a general classification scheme is presented here. A comprehensive survey of existing English-based CNLs is given, listing and describing 100 languages from 1930 until today. Classification of these languages reveals that they form a single scattered cloud filling the conceptual space between natural languages such as English on the one end and formal languages such as propositional logic on the other. The goal of this article is to provide a common terminology and a common model for CNL, to contribute to the understanding of their general nature, to provide a starting point for researchers interested in the area, and to help developers to make design decisions.\n    ",
        "submission_date": "2015-07-07T00:00:00",
        "last_modified_date": "2015-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01839",
        "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding",
        "authors": [
            "Mingbo Ma",
            "Liang Huang",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC.\n    ",
        "submission_date": "2015-07-07T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02012",
        "title": "Hindi to English Transfer Based Machine Translation System",
        "authors": [
            "Akanksha Gehlot",
            "Vaishali Sharma",
            "Shashi Pal Singh",
            "Ajai Kumar"
        ],
        "abstract": "In large societies like India there is a huge demand to convert one human language into another. Lots of work has been done in this area. Many transfer based MTS have developed for English to other languages, as MANTRA CDAC Pune, MATRA CDAC Pune, SHAKTI IISc Bangalore and IIIT Hyderabad. Still there is a little work done for Hindi to other languages. Currently we are working on it. In this paper we focus on designing a system, that translate the document from Hindi to English by using transfer based approach. This system takes an input text check its structure through parsing. Reordering rules are used to generate the text in target language. It is better than Corpus Based MTS because Corpus Based MTS require large amount of word aligned data for translation that is not available for many languages while Transfer Based MTS requires only knowledge of both the languages(source language and target language) to make transfer rules. We get correct translation for simple assertive sentences and almost correct for complex and compound sentences.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02020",
        "title": "Generating Navigable Semantic Maps from Social Sciences Corpora",
        "authors": [
            "Thierry Poibeau",
            "Pablo Ruiz"
        ],
        "abstract": "It is now commonplace to observe that we are facing a deluge of online information. Researchers have of course long acknowledged the potential value of this information since digital traces make it possible to directly observe, describe and analyze social facts, and above all the co-evolution of ideas and communities over time. However, most online information is expressed through text, which means it is not directly usable by machines, since computers require structured, organized and typed information in order to be able to manipulate it. Our goal is thus twofold: 1. Provide new natural language processing techniques aiming at automatically extracting relevant information from texts, especially in the context of social sciences, and connect these pieces of information so as to obtain relevant socio-semantic networks; 2. Provide new ways of exploring these socio-semantic networks, thanks to tools allowing one to dynamically navigate these networks, de-construct and re-construct them interactively, from different points of view following the needs expressed by domain experts.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02045",
        "title": "What Your Username Says About You",
        "authors": [
            "Aaron Jaech",
            "Mari Ostendorf"
        ],
        "abstract": "Usernames are ubiquitous on the Internet, and they are often suggestive of user demographics. This work looks at the degree to which gender and language can be inferred from a username alone by making use of unsupervised morphology induction to decompose usernames into sub-units. Experimental results on the two tasks demonstrate the effectiveness of the proposed morphological features compared to a character n-gram baseline.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02062",
        "title": "Multi-Document Summarization via Discriminative Summary Reranking",
        "authors": [
            "Xiaojun Wan",
            "Ziqiang Cao",
            "Furu Wei",
            "Sujian Li",
            "Ming Zhou"
        ],
        "abstract": "Existing multi-document summarization systems usually rely on a specific summarization model (i.e., a summarization method with a specific parameter setting) to extract summaries for different document sets with different topics. However, according to our quantitative analysis, none of the existing summarization models can always produce high-quality summaries for different document sets, and even a summarization model with good overall performance may produce low-quality summaries for some document sets. On the contrary, a baseline summarization model may produce high-quality summaries for some document sets. Based on the above observations, we treat the summaries produced by different summarization models as candidate summaries, and then explore discriminative reranking techniques to identify high-quality summaries from the candidates for difference document sets. We propose to extract a set of candidate summaries for each document set based on an ILP framework, and then leverage Ranking SVM for summary reranking. Various useful features have been developed for the reranking process, including word-level features, sentence-level features and summary-level features. Evaluation results on the benchmark DUC datasets validate the efficacy and robustness of our proposed approach.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02086",
        "title": "The Role of Pragmatics in Legal Norm Representation",
        "authors": [
            "Shashishekar Ramakrishna",
            "Lukasz Gorski",
            "Adrian Paschke"
        ],
        "abstract": "Despite the 'apparent clarity' of a given legal provision, its application may result in an outcome that does not exactly conform to the semantic level of a statute. The vagueness within a legal text is induced intentionally to accommodate all possible scenarios under which such norms should be applied, thus making the role of pragmatics an important aspect also in the representation of a legal norm and reasoning on top of it. The notion of pragmatics considered in this paper does not focus on the aspects associated with judicial decision making. The paper aims to shed light on the aspects of pragmatics in legal linguistics, mainly focusing on the domain of patent law, only from a knowledge representation perspective. The philosophical discussions presented in this paper are grounded based on the legal theories from Grice and Marmor.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02145",
        "title": "Learning to Mine Chinese Coordinate Terms Using the Web",
        "authors": [
            "Xiaojiang Huang",
            "Xiaojun Wan",
            "Jianguo Xiao"
        ],
        "abstract": "Coordinate relation refers to the relation between instances of a concept and the relation between the directly hyponyms of a concept. In this paper, we focus on the task of extracting terms which are coordinate with a user given seed term in Chinese, and grouping the terms which belong to different concepts if the seed term has several meanings. We propose a semi-supervised method that integrates manually defined linguistic patterns and automatically learned semi-structural patterns to extract coordinate terms in Chinese from web search results. In addition, terms are grouped into different concepts based on their co-occurring terms and contexts. We further calculate the saliency scores of extracted terms and rank them accordingly. Experimental results demonstrate that our proposed method generates results with high quality and wide coverage.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02205",
        "title": "Talking to the crowd: What do people react to in online discussions?",
        "authors": [
            "Aaron Jaech",
            "Victoria Zayats",
            "Hao Fang",
            "Mari Ostendorf",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "This paper addresses the question of how language use affects community reaction to comments in online discussion forums, and the relative importance of the message vs. the messenger. A new comment ranking task is proposed based on community annotated karma in Reddit discussions, which controls for topic and timing of comments. Experimental work with discussion threads from six subreddits shows that the importance of different types of language features varies with the community of interest.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02628",
        "title": "FAQ-based Question Answering via Word Alignment",
        "authors": [
            "Zhiguo Wang",
            "Abraham Ittycheriah"
        ],
        "abstract": "In this paper, we propose a novel word-alignment-based method to solve the FAQ-based question answering task. First, we employ a neural network model to calculate question similarity, where the word alignment between two questions is used for extracting features. Second, we design a bootstrap-based feature extraction method to extract a small set of effective lexical features. Third, we propose a learning-to-rank algorithm to train parameters more suitable for the ranking tasks. Experimental results, conducted on three languages (English, Spanish and Japanese), demonstrate that the question similarity model is more effective than baseline systems, the sparse features bring 5% improvements on top-1 accuracy, and the learning-to-rank algorithm works significantly better than the traditional method. We further evaluate our method on the answer sentence selection task. Our method outperforms all the previous systems on the standard TREC data set.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03077",
        "title": "A new hybrid stemming algorithm for Persian",
        "authors": [
            "Adel Rahimi"
        ],
        "abstract": "Stemming has been an influential part in Information retrieval and search engines. There have been tremendous endeavours in making stemmer that are both efficient and accurate. Stemmers can have three method in stemming, Dictionary based stemmer, statistical-based stemmers, and rule-based stemmers. This paper aims at building a hybrid stemmer that uses both Dictionary based method and rule-based method for stemming. This ultimately helps the efficacy and accurateness of the stemmer.\n    ",
        "submission_date": "2015-07-11T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03223",
        "title": "Classifier-Based Text Simplification for Improved Machine Translation",
        "authors": [
            "Shruti Tyagi",
            "Deepti Chopra",
            "Iti Mathur",
            "Nisheeth Joshi"
        ],
        "abstract": "Machine Translation is one of the research fields of Computational Linguistics. The objective of many MT Researchers is to develop an MT System that produce good quality and high accuracy output translations and which also covers maximum language pairs. As internet and Globalization is increasing day by day, we need a way that improves the quality of translation. For this reason, we have developed a Classifier based Text Simplification Model for English-Hindi Machine Translation Systems. We have used support vector machines and Na\u00efve Bayes Classifier to develop this model. We have also evaluated the performance of these classifiers.\n    ",
        "submission_date": "2015-07-12T00:00:00",
        "last_modified_date": "2015-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03462",
        "title": "Supervised Hierarchical Classification for Student Answer Scoring",
        "authors": [
            "Itziar Aldabe",
            "Oier Lopez de Lacalle",
            "I\u00f1igo Lopez-Gazpio",
            "Montse Maritxalar"
        ],
        "abstract": "This paper describes a hierarchical system that predicts one label at a time for automated student response analysis. For the task, we build a classification binary tree that delays more easily confused labels to later stages using hierarchical processes. In particular, the paper describes how the hierarchical classifier has been built and how the classification task has been broken down into binary subtasks. It finally discusses the motivations and fundamentals of such an approach.\n    ",
        "submission_date": "2015-07-13T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03471",
        "title": "Incremental LSTM-based Dialog State Tracker",
        "authors": [
            "Lukas Zilka",
            "Filip Jurcicek"
        ],
        "abstract": "A dialog state tracker is an important component in modern spoken dialog systems. We present an incremental dialog state tracker, based on LSTM networks. It directly uses automatic speech recognition hypotheses to track the state. We also present the key non-standard aspects of the model that bring its performance close to the state-of-the-art and experimentally analyze their contribution: including the ASR confidence scores, abstracting scarcely represented values, including transcriptions in the training data, and model averaging.\n    ",
        "submission_date": "2015-07-13T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03641",
        "title": "Neural CRF Parsing",
        "authors": [
            "Greg Durrett",
            "Dan Klein"
        ],
        "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.\n    ",
        "submission_date": "2015-07-13T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03934",
        "title": "Recurrent Polynomial Network for Dialogue State Tracking",
        "authors": [
            "Kai Sun",
            "Qizhe Xie",
            "Kai Yu"
        ],
        "abstract": "Dialogue state tracking (DST) is a process to estimate the distribution of the dialogue states as a dialogue progresses. Recent studies on constrained Markov Bayesian polynomial (CMBP) framework take the first step towards bridging the gap between rule-based and statistical approaches for DST. In this paper, the gap is further bridged by a novel framework -- recurrent polynomial network (RPN). RPN's unique structure enables the framework to have all the advantages of CMBP including efficiency, portability and interpretability. Additionally, RPN achieves more properties of statistical approaches than CMBP. RPN was evaluated on the data corpora of the second and the third Dialog State Tracking Challenge (DSTC-2/3). Experiments showed that RPN can significantly outperform both traditional rule-based approaches and statistical approaches with similar feature set. Compared with the state-of-the-art statistical DST approaches with a lot richer features, RPN is also competitive.\n    ",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04019",
        "title": "Feature Normalisation for Robust Speech Recognition",
        "authors": [
            "D. S. Pavan Kumar"
        ],
        "abstract": "Speech recognition system performance degrades in noisy environments. If the acoustic models are built using features of clean utterances, the features of a noisy test utterance would be acoustically mismatched with the trained model. This gives poor likelihoods and poor recognition accuracy. Model adaptation and feature normalisation are two broad areas that address this problem. While the former often gives better performance, the latter involves estimation of lesser number of parameters, making the system feasible for practical implementations.\n",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04214",
        "title": "Associative Measures and Multi-word Unit Extraction in Turkish",
        "authors": [
            "Umit Mersinli"
        ],
        "abstract": "Associative measures are \"mathematical formulas determining the strength of association between two or more words based on their occurrences and cooccurrences in a text corpus\" (Pecina, 2010, p. 138). The purpose of this paper is to test the 12 associative measures that Text-NSP (Banerjee & Pedersen, 2003) contains on a 10-million-word subcorpus of Turkish National Corpus (TNC) (Aksan ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04420",
        "title": "Bias and population structure in the actuation of sound change",
        "authors": [
            "James Kirby",
            "Morgan Sonderegger"
        ],
        "abstract": "Why do human languages change at some times, and not others? We address this longstanding question from a computational perspective, focusing on the case of sound change. Sound change arises from the pronunciation variability ubiquitous in every speech community, but most such variability does not lead to change. Hence, an adequate model must allow for stability as well as change. Existing theories of sound change tend to emphasize factors at the level of individual learners promoting one outcome or the other, such as channel bias (which favors change) or inductive bias (which favors stability). Here, we consider how the interaction of these biases can lead to both stability and change in a population setting. We find that population structure itself can act as a source of stability, but that both stability and change are possible only when both types of bias are active, suggesting that it is possible to understand why sound change occurs at some times and not others as the population-level result of the interplay between forces promoting each outcome in individual speakers. In addition, if it is assumed that learners learn from two or more teachers, the transition from stability to change is marked by a phase transition, consistent with the abrupt transitions seen in many empirical cases of sound change. The predictions of multiple-teacher models thus match empirical cases of sound change better than the predictions of single-teacher models, underscoring the importance of modeling language change in a population setting.\n    ",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04646",
        "title": "A Dependency-Based Neural Network for Relation Classification",
        "authors": [
            "Yang Liu",
            "Furu Wei",
            "Sujian Li",
            "Heng Ji",
            "Ming Zhou",
            "Houfeng Wang"
        ],
        "abstract": "Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results.\n    ",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04808",
        "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models",
        "authors": [
            "Iulian V. Serban",
            "Alessandro Sordoni",
            "Yoshua Bengio",
            "Aaron Courville",
            "Joelle Pineau"
        ],
        "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05134",
        "title": "Persistent Topology of Syntax",
        "authors": [
            "Alexander Port",
            "Iulia Gheorghita",
            "Daniel Guth",
            "John M.Clark",
            "Crystal Liang",
            "Shival Dasu",
            "Matilde Marcolli"
        ],
        "abstract": "We study the persistent homology of the data set of syntactic parameters of the world languages. We show that, while homology generators behave erratically over the whole data set, non-trivial persistent homology appears when one restricts to specific language families. Different families exhibit different persistent homology. We focus on the cases of the Indo-European and the Niger-Congo families, for which we compare persistent homology over different cluster filtering values. We investigate the possible significance, in historical linguistic terms, of the presence of persistent generators of the first homology. In particular, we show that the persistent first homology generator we find in the Indo-European family is not due (as one might guess) to the Anglo-Norman bridge in the Indo-European phylogenetic network, but is related to the position of Ancient Greek and the Hellenic branch within the network.\n    ",
        "submission_date": "2015-07-18T00:00:00",
        "last_modified_date": "2015-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05523",
        "title": "How to Generate a Good Word Embedding?",
        "authors": [
            "Siwei Lai",
            "Kang Liu",
            "Liheng Xu",
            "Jun Zhao"
        ],
        "abstract": "We analyze three critical components of word embedding training: the model, the corpus, and the training parameters. We systematize existing neural-network-based word embedding algorithms and compare them using the same corpus. We evaluate each word embedding in three ways: analyzing its semantic properties, using it as a feature for supervised tasks and using it to initialize neural networks. We also provide several simple guidelines for training word embeddings. First, we discover that corpus domain is more important than corpus size. We recommend choosing a corpus in a suitable domain for the desired task, after that, using a larger corpus yields better results. Second, we find that faster models provide sufficient performance in most cases, and more complex models can be used if the training corpus is sufficiently large. Third, the early stopping metric for iterating should rely on the development set of the desired task rather than the validation loss of training embedding.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05630",
        "title": "Notes About a More Aware Dependency Parser",
        "authors": [
            "Matteo Grella"
        ],
        "abstract": "In this paper I explain the reasons that led me to research and conceive a novel technology for dependency parsing, mixing together the strengths of data-driven transition-based and constraint-based approaches. In particular I highlight the problem to infer the reliability of the results of a data-driven transition-based parser, which is extremely important for high-level processes that expect to use correct parsing results. I then briefly introduce a number of notes about a new parser model I'm working on, capable to proceed with the analysis in a \"more aware\" way, with a more \"robust\" concept of robustness.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06020",
        "title": "Practical Selection of SVM Supervised Parameters with Different Feature Representations for Vowel Recognition",
        "authors": [
            "Rimah Amami",
            "Dorra Ben Ayed",
            "Noureddine Ellouze"
        ],
        "abstract": "It is known that the classification performance of Support Vector Machine (SVM) can be conveniently affected by the different parameters of the kernel tricks and the regularization parameter, C. Thus, in this article, we propose a study in order to find the suitable kernel with which SVM may achieve good generalization performance as well as the parameters to use. We need to analyze the behavior of the SVM classifier when these parameters take very small or very large values. The study is conducted for a multi-class vowel recognition using the TIMIT corpus. Furthermore, for the experiments, we used different feature representations such as MFCC and PLP. Finally, a comparative study was done to point out the impact of the choice of the parameters, kernel trick and feature representations on the performance of the SVM classifier\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06021",
        "title": "An Empirical Comparison of SVM and Some Supervised Learning Algorithms for Vowel recognition",
        "authors": [
            "Rimah Amami",
            "Dorra Ben Ayed",
            "Noureddine Ellouze"
        ],
        "abstract": "In this article, we conduct a study on the performance of some supervised learning algorithms for vowel recognition. This study aims to compare the accuracy of each algorithm. Thus, we present an empirical comparison between five supervised learning classifiers and two combined classifiers: SVM, KNN, Naive Bayes, Quadratic Bayes Normal (QDC) and Nearst Mean. Those algorithms were tested for vowel recognition using TIMIT Corpus and Mel-frequency cepstral coefficients (MFCCs).\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06023",
        "title": "Robust speech recognition using consensus function based on multi-layer networks",
        "authors": [
            "Rimah Amami",
            "Ghaith Manita",
            "Abir Smiti"
        ],
        "abstract": "The clustering ensembles mingle numerous partitions of a specified data into a single clustering solution. Clustering ensemble has emerged as a potent approach for ameliorating both the forcefulness and the stability of unsupervised classification results. One of the major problems in clustering ensembles is to find the best consensus function. Finding final partition from different clustering results requires skillfulness and robustness of the classification algorithm. In addition, the major problem with the consensus function is its sensitivity to the used data sets quality. This limitation is due to the existence of noisy, silence or redundant data. This paper proposes a novel consensus function of cluster ensembles based on Multilayer networks technique and a maintenance database method. This maintenance database approach is used in order to handle any given noisy speech and, thus, to guarantee the quality of databases. This can generates good results and efficient data partitions. To show its effectiveness, we support our strategy with empirical evaluation using distorted speech from Aurora speech databases.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06025",
        "title": "Incorporating Belief Function in SVM for Phoneme Recognition",
        "authors": [
            "Rimah Amami",
            "Dorra Ben Ayed",
            "Nouerddine Ellouze"
        ],
        "abstract": "The Support Vector Machine (SVM) method has been widely used in numerous classification tasks. The main idea of this algorithm is based on the principle of the margin maximization to find an hyperplane which separates the data into two different ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06028",
        "title": "The challenges of SVM optimization using Adaboost on a phoneme recognition problem",
        "authors": [
            "Rimah Amami",
            "Dorra Ben Ayed",
            "Noureddine Ellouze"
        ],
        "abstract": "The use of digital technology is growing at a very fast pace which led to the emergence of systems based on the cognitive infocommunications. The expansion of this sector impose the use of combining methods in order to ensure the robustness in cognitive systems.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06073",
        "title": "Discriminative Segmental Cascades for Feature-Rich Phone Recognition",
        "authors": [
            "Hao Tang",
            "Weiran Wang",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "abstract": "Discriminative segmental models, such as segmental conditional random fields (SCRFs) and segmental structured support vector machines (SSVMs), have had success in speech recognition via both lattice rescoring and first-pass decoding. However, such models suffer from slow decoding, hampering the use of computationally expensive features, such as segment neural networks or other high-order features. A typical solution is to use approximate decoding, either by beam pruning in a single pass or by beam pruning to generate a lattice followed by a second pass. In this work, we study discriminative segmental models trained with a hinge loss (i.e., segmental structured SVMs). We show that beam search is not suitable for learning rescoring models in this approach, though it gives good approximate decoding performance when the model is already well-trained. Instead, we consider an approach inspired by structured prediction cascades, which use max-marginal pruning to generate lattices. We obtain a high-accuracy phonetic recognition system with several expensive feature types: a segment neural network, a second-order language model, and second-order phone boundary features.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06829",
        "title": "The Polylingual Labeled Topic Model",
        "authors": [
            "Lisa Posch",
            "Arnim Bleier",
            "Philipp Schaer",
            "Markus Strohmaier"
        ],
        "abstract": "In this paper, we present the Polylingual Labeled Topic Model, a model which combines the characteristics of the existing Polylingual Topic Model and Labeled LDA. The model accounts for multiple languages with separate topic distributions for each language while restricting the permitted topics of a document to a set of predefined labels. We explore the properties of the model in a two-language setting on a dataset from the social science domain. Our experiments show that our model outperforms LDA and Labeled LDA in terms of their held-out perplexity and that it produces semantically coherent topics which are well interpretable by human subjects.\n    ",
        "submission_date": "2015-07-24T00:00:00",
        "last_modified_date": "2015-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06837",
        "title": "YARBUS : Yet Another Rule Based belief Update System",
        "authors": [
            "Jeremy Fix",
            "Herve Frezza-buet"
        ],
        "abstract": "We introduce a new rule based system for belief tracking in dialog systems. Despite the simplicity of the rules being considered, the proposed belief tracker ranks favourably compared to the previous submissions on the second and third Dialog State Tracking challenges. The results of this simple tracker allows to reconsider the performances of previous submissions using more elaborate techniques.\n    ",
        "submission_date": "2015-07-24T00:00:00",
        "last_modified_date": "2015-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06947",
        "title": "Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition",
        "authors": [
            "Ha\u015fim Sak",
            "Andrew Senior",
            "Kanishka Rao",
            "Fran\u00e7oise Beaufays"
        ],
        "abstract": "We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.\n    ",
        "submission_date": "2015-07-24T00:00:00",
        "last_modified_date": "2015-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07636",
        "title": "Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds",
        "authors": [
            "Sridhar Mahadevan",
            "Sarath Chandar"
        ],
        "abstract": "Recent work has explored methods for learning continuous vector space word representations reflecting the underlying semantics of words. Simple vector space arithmetic using cosine distances has been shown to capture certain types of analogies, such as reasoning about plurals from singulars, past tense from present tense, etc. In this paper, we introduce a new approach to capture analogies in continuous word representations, based on modeling not just individual word vectors, but rather the subspaces spanned by groups of words. We exploit the property that the set of subspaces in n-dimensional Euclidean space form a curved manifold space called the Grassmannian, a quotient subgroup of the Lie group of rotations in n- dimensions. Based on this mathematical model, we develop a modified cosine distance model based on geodesic kernels that captures relation-specific distances across word categories. Our experiments on analogy tasks show that our approach performs significantly better than the previous approaches for the given task.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07826",
        "title": "Classifying informative and imaginative prose using complex networks",
        "authors": [
            "Henrique F. de Arruda",
            "Luciano da F. Costa",
            "Diego R. Amancio"
        ],
        "abstract": "Statistical methods have been widely employed in recent years to grasp many language properties. The application of such techniques have allowed an improvement of several linguistic applications, which encompasses machine translation, automatic summarization and document classification. In the latter, many approaches have emphasized the semantical content of texts, as it is the case of bag-of-word language models. This approach has certainly yielded reasonable performance. However, some potential features such as the structural organization of texts have been used only on a few studies. In this context, we probe how features derived from textual structure analysis can be effectively employed in a classification task. More specifically, we performed a supervised classification aiming at discriminating informative from imaginative documents. Using a networked model that describes the local topological/dynamical properties of function words, we achieved an accuracy rate of up to 95%, which is much higher than similar networked approaches. A systematic analysis of feature relevance revealed that symmetry and accessibility measurements are among the most prominent network measurements. Our results suggest that these measurements could be used in related language applications, as they play a complementary role in characterizing texts.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07998",
        "title": "Document Embedding with Paragraph Vectors",
        "authors": [
            "Andrew M. Dai",
            "Christopher Olah",
            "Quoc V. Le"
        ],
        "abstract": "Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08240",
        "title": "EESEN: End-to-End Speech Recognition using Deep RNN Models and WFST-based Decoding",
        "authors": [
            "Yajie Miao",
            "Mohammad Gowayyed",
            "Florian Metze"
        ],
        "abstract": "The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neural networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context-independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08396",
        "title": "Tag-Weighted Topic Model For Large-scale Semi-Structured Documents",
        "authors": [
            "Shuangyin Li",
            "Jiefei Li",
            "Guan Huang",
            "Ruiyang Tan",
            "Rong Pan"
        ],
        "abstract": "To date, there have been massive Semi-Structured Documents (SSDs) during the evolution of the Internet. These SSDs contain both unstructured features (e.g., plain text) and metadata (e.g., tags). Most previous works focused on modeling the unstructured text, and recently, some other methods have been proposed to model the unstructured text with specific tags. To build a general model for SSDs remains an important problem in terms of both model fitness and efficiency. We propose a novel method to model the SSDs by a so-called Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the tags and words information, not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions for text mining tasks. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. Meanwhile, we propose three large-scale solutions for our model under the MapReduce distributed computing platform for modeling large-scale SSDs. The experimental results show the effectiveness, efficiency and the robustness by comparing our model with the state-of-the-art methods in document modeling, tags prediction and text classification. We also show the performance of the three distributed solutions in terms of time and accuracy on document modeling.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08449",
        "title": "One model, two languages: training bilingual parsers with harmonized treebanks",
        "authors": [
            "David Vilares",
            "Carlos G\u00f3mez-Rodr\u00edguez",
            "Miguel A. Alonso"
        ],
        "abstract": "We introduce an approach to train lexicalized parsers using bilingual corpora obtained by merging harmonized treebanks of different languages, producing parsers that can analyze sentences in either of the learned languages, or even sentences that mix both. We test the approach on the Universal Dependency Treebanks, training with MaltParser and MaltOptimizer. The results show that these bilingual parsers are more than competitive, as most combinations not only preserve accuracy, but some even achieve significant improvements over the corresponding monolingual parsers. Preliminary experiments also show the approach to be promising on texts with code-switching and when more languages are added.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08452",
        "title": "Unsupervised Sentence Simplification Using Deep Semantics",
        "authors": [
            "Shashi Narayan",
            "Claire Gardent"
        ],
        "abstract": "We present a novel approach to sentence simplification which departs from previous work in two main ways. First, it requires neither hand written rules nor a training corpus of aligned standard and simplified sentences. Second, sentence splitting operates on deep semantic structure. We show (i) that the unsupervised framework we propose is competitive with four state-of-the-art supervised systems and (ii) that our semantic based approach allows for a principled and effective handling of sentence splitting.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08539",
        "title": "Multilayer Network of Language: a Unified Framework for Structural Analysis of Linguistic Subsystems",
        "authors": [
            "Domagoj Margan",
            "Ana Me\u0161trovi\u0107",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107"
        ],
        "abstract": "Recently, the focus of complex networks research has shifted from the analysis of isolated properties of a system toward a more realistic modeling of multiple phenomena - multilayer networks. Motivated by the prosperity of multilayer approach in social, transport or trade systems, we propose the introduction of multilayer networks for language. The multilayer network of language is a unified framework for modeling linguistic subsystems and their structural properties enabling the exploration of their mutual interactions. Various aspects of natural language systems can be represented as complex networks, whose vertices depict linguistic units, while links model their relations. The multilayer network of language is defined by three aspects: the network construction principle, the linguistic subsystem and the language of interest. More precisely, we construct a word-level (syntax, co-occurrence and its shuffled counterpart) and a subword level (syllables and graphemes) network layers, from five variations of original text (in the modeled language). The obtained results suggest that there are substantial differences between the networks structures of different language subsystems, which are hidden during the exploration of an isolated layer. The word-level layers share structural properties regardless of the language (e.g. Croatian or English), while the syllabic subword level expresses more language dependent structural properties. The preserved weighted overlap quantifies the similarity of word-level layers in weighted and directed networks. Moreover, the analysis of motifs reveals a close topological structure of the syntactic and syllabic layers for both languages. The findings corroborate that the multilayer network framework is a powerful, consistent and systematic approach to model several linguistic subsystems simultaneously and hence to provide a more unified view on language.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00106",
        "title": "Separated by an Un-common Language: Towards Judgment Language Informed Vector Space Modeling",
        "authors": [
            "Ira Leviant",
            "Roi Reichart"
        ],
        "abstract": "A common evaluation practice in the vector space models (VSMs) literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs. Most existing evaluation sets, however, consist of scores collected for English word pairs only, ignoring the potential impact of the judgment language in which word pairs are presented on the human scores. In this paper we translate two prominent evaluation sets, wordsim353 (association) and SimLex999 (similarity), from English to Italian, German and Russian and collect scores for each dataset from crowdworkers fluent in its language. Our analysis reveals that human judgments are strongly impacted by the judgment language. Moreover, we show that the predictions of monolingual VSMs do not necessarily best correlate with human judgments made with the language used for model training, suggesting that models and humans are affected differently by the language they use when making semantic judgments. Finally, we show that in a large number of setups, multilingual VSM combination results in improved correlations with human judgments, suggesting that multilingualism may partially compensate for the judgment language effect on human judgments.\n    ",
        "submission_date": "2015-08-01T00:00:00",
        "last_modified_date": "2015-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00189",
        "title": "Class Vectors: Embedding representation of Document Classes",
        "authors": [
            "Devendra Singh Sachan",
            "Shailesh Kumar"
        ],
        "abstract": "Distributed representations of words and paragraphs as semantic embeddings in high dimensional data are used across a number of Natural Language Understanding tasks such as retrieval, translation, and classification. In this work, we propose \"Class Vectors\" - a framework for learning a vector per class in the same embedding space as the word and paragraph embeddings. Similarity between these class vectors and word vectors are used as features to classify a document to a class. In experiment on several sentiment analysis tasks such as Yelp reviews and Amazon electronic product reviews, class vectors have shown better or comparable results in classification while learning very meaningful class embeddings.\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2015-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00200",
        "title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks",
        "authors": [
            "Jian Tang",
            "Meng Qu",
            "Qiaozhu Mei"
        ],
        "abstract": "Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the \\textit{predictive text embedding} (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2015-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00305",
        "title": "Compositional Semantic Parsing on Semi-Structured Tables",
        "authors": [
            "Panupong Pasupat",
            "Percy Liang"
        ],
        "abstract": "Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00504",
        "title": "Spin Glass Models of Syntax and Language Evolution",
        "authors": [
            "Karthik Siva",
            "Jim Tao",
            "Matilde Marcolli"
        ],
        "abstract": "Using the SSWL database of syntactic parameters of world languages, and the MIT Media Lab data on language interactions, we construct a spin glass model of language evolution. We treat binary syntactic parameters as spin states, with languages as vertices of a graph, and assigned interaction energies along the edges. We study a rough model of syntax evolution, under the assumption that a strong interaction energy tends to cause parameters to align, as in the case of ferromagnetic materials. We also study how the spin glass model needs to be modified to account for entailment relations between syntactic parameters. This modification leads naturally to a generalization of Potts models with external magnetic field, which consists of a coupling at the vertices of an Ising model and a Potts model with q=3, that have the same edge interactions. We describe the results of simulations of the dynamics of these models, in different temperature and energy regimes. We discuss the linguistic interpretation of the parameters of the physical model.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00657",
        "title": "Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs",
        "authors": [
            "Miguel Ballesteros",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We present extensions to a continuous-state dependency parsing method that makes it applicable to morphologically rich languages. Starting with a high-performance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.\n    ",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2015-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00715",
        "title": "Multi-Modal Bayesian Embeddings for Learning Social Knowledge Graphs",
        "authors": [
            "Zhilin Yang",
            "Jie Tang",
            "William Cohen"
        ],
        "abstract": "We study the extent to which online social networks can be connected to open knowledge bases. The problem is referred to as learning social knowledge graphs. We propose a multi-modal Bayesian embedding model, GenVector, to learn latent topics that generate word and network embeddings. GenVector leverages large-scale unlabeled data with embeddings and represents data of two modalities---i.e., social network users and knowledge concepts---in a shared latent topic space. Experiments on three datasets show that the proposed method clearly outperforms state-of-the-art methods. We then deploy the method on AMiner, a large-scale online academic search system with a network of 38,049,189 researchers with a knowledge base with 35,415,011 concepts. Our method significantly decreases the error rate in an online A/B test with live users.\n    ",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01006",
        "title": "Relation Classification via Recurrent Neural Network",
        "authors": [
            "Dongxu Zhang",
            "Dong Wang"
        ],
        "abstract": "Deep learning has gained much success in sentence-level relation classification. For example, convolutional neural networks (CNN) have delivered competitive performance without much effort on feature engineering as the conventional pattern-based methods. Thus a lot of works have been produced based on CNN structures. However, a key issue that has not been well addressed by the CNN-based method is the lack of capability to learn temporal features, especially long-distance dependency between nominal pairs. In this paper, we propose a simple framework based on recurrent neural networks (RNN) and compare it with CNN-based model. To show the limitation of popular used SemEval-2010 Task 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al., 2014). Experiments on two different datasets strongly indicates that the RNN-based model can deliver better performance on relation classification, and it is particularly capable of learning long-distance relation patterns. This makes it suitable for real-world applications where complicated expressions are often involved.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01067",
        "title": "Topic Stability over Noisy Sources",
        "authors": [
            "Jing Su",
            "Ois\u00edn Boydell",
            "Derek Greene",
            "Gerard Lynch"
        ],
        "abstract": "Topic modelling techniques such as LDA have recently been applied to speech transcripts and OCR output. These corpora may contain noisy or erroneous texts which may undermine topic stability. Therefore, it is important to know how well a topic modelling algorithm will perform when applied to noisy data. In this paper we show that different types of textual noise will have diverse effects on the stability of different topic models. From these observations, we propose guidelines for text corpus generation, with a focus on automatic speech transcription. We also suggest topic model selection methods for noisy corpora.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01211",
        "title": "Listen, Attend and Spell",
        "authors": [
            "William Chan",
            "Navdeep Jaitly",
            "Quoc V. Le",
            "Oriol Vinyals"
        ],
        "abstract": "We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01306",
        "title": "Replication and Generalization of PRECISE",
        "authors": [
            "Michael Minock",
            "Nils Everling"
        ],
        "abstract": "This report describes an initial replication study of the PRECISE system and develops a clearer, more formal description of the approach. Based on our evaluation, we conclude that the PRECISE results do not fully replicate. However the formalization developed here suggests a road map to further enhance and extend the approach pioneered by PRECISE.\n",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01321",
        "title": "On Gobbledygook and Mood of the Philippine Senate: An Exploratory Study on the Readability and Sentiment of Selected Philippine Senators' Microposts",
        "authors": [
            "Fatima M. Moncada",
            "Jaderick P. Pabico"
        ],
        "abstract": "This paper presents the findings of a readability assessment and sentiment analysis of selected six Philippine senators' microposts over the popular Twitter microblog. Using the Simple Measure of Gobbledygook (SMOG), tweets of Senators Cayetano, Defensor-Santiago, Pangilinan, Marcos, Guingona, and Escudero were assessed. A sentiment analysis was also done to determine the polarity of the senators' respective microposts. Results showed that on the average, the six senators are tweeting at an eight to ten SMOG level. This means that, at least a sixth grader will be able to understand the senators' tweets. Moreover, their tweets are mostly neutral and their sentiments vary in unison at some period of time. This could mean that a senator's tweet sentiment is affected by specific Philippine-based events.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01346",
        "title": "Word sense disambiguation: a survey",
        "authors": [
            "Alok Ranjan Pal",
            "Diganta Saha"
        ],
        "abstract": "In this paper, we made a survey on Word Sense Disambiguation (WSD). Near about in all major languages around the world, research in WSD has been conducted upto different extents. In this paper, we have gone through a survey regarding the different approaches adopted in different research works, the State of the Art in the performance in this domain, recent works in different Indian languages and finally a survey in Bengali language. We have made a survey on different competitions in this field and the bench mark results, obtained from those competitions.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01349",
        "title": "Automatic classification of bengali sentences based on sense definitions present in bengali wordnet",
        "authors": [
            "Alok Ranjan Pal",
            "Diganta Saha",
            "Niladri Sekhar Dash"
        ],
        "abstract": "Based on the sense definition of words available in the Bengali WordNet, an attempt is made to classify the Bengali sentences automatically into different groups in accordance with their underlying senses. The input sentences are collected from 50 different categories of the Bengali text corpus developed in the TDIL project of the Govt. of India, while information about the different senses of particular ambiguous lexical item is collected from Bengali WordNet. In an experimental basis we have used Naive Bayes probabilistic model as a useful classifier of sentences. We have applied the algorithm over 1747 sentences that contain a particular Bengali lexical item which, because of its ambiguous nature, is able to trigger different senses that render sentences in different meanings. In our experiment we have achieved around 84% accurate result on the sense classification over the total input sentences. We have analyzed those residual sentences that did not comply with our experiment and did affect the results to note that in many cases, wrong syntactic structures and less semantic information are the main hurdles in semantic classification of sentences. The applicational relevance of this study is attested in automatic text classification, machine learning, information extraction, and word sense disambiguation.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01447",
        "title": "Using Linguistic Analysis to Translate Arabic Natural Language Queries to SPARQL",
        "authors": [
            "Iyad AlAgha"
        ],
        "abstract": "The logic-based machine-understandable framework of the Semantic Web often challenges naive users when they try to query ontology-based knowledge bases. Existing research efforts have approached this problem by introducing Natural Language (NL) interfaces to ontologies. These NL interfaces have the ability to construct SPARQL queries based on NL user queries. However, most efforts were restricted to queries expressed in English, and they often benefited from the advancement of English NLP tools. However, little research has been done to support querying the Arabic content on the Semantic Web by using NL queries. This paper presents a domain-independent approach to translate Arabic NL queries to SPARQL by leveraging linguistic analysis. Based on a special consideration on Noun Phrases (NPs), our approach uses a language parser to extract NPs and the relations from Arabic parse trees and match them to the underlying ontology. It then utilizes knowledge in the ontology to group NPs into triple-based representations. A SPARQL query is finally generated by extracting targets and modifiers, and interpreting them into SPARQL. The interpretation of advanced semantic features including negation, conjunctive and disjunctive modifiers is also supported. The approach was evaluated by using two datasets consisting of OWL test data and queries, and the obtained results have confirmed its feasibility to translate Arabic NL queries to SPARQL.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01476",
        "title": "Hyponymy extraction of domain ontology concept based on ccrfs and hierarchy clustering",
        "authors": [
            "Qiang Zhan",
            "Chunhong Wang"
        ],
        "abstract": "Concept hierarchy is the backbone of ontology, and the concept hierarchy acquisition has been a hot topic in the field of ontology learning. this paper proposes a hyponymy extraction method of domain ontology concept based on cascaded conditional random field(CCRFs) and hierarchy clustering. It takes free text as extracting object, adopts CCRFs identifying the domain concepts. First the low layer of CCRFs is used to identify simple domain concept, then the results are sent to the high layer, in which the nesting concepts are recognized. Next we adopt hierarchy clustering to identify the hyponymy relation between domain ontology concepts. The experimental results demonstrate the proposed method is efficient.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01577",
        "title": "Automata networks model for alignment and least effort on vocabulary formation",
        "authors": [
            "Javier Vera",
            "Felipe Urbina",
            "Eric Goles"
        ],
        "abstract": "Can artificial communities of agents develop language with scaling relations close to the Zipf law? As a preliminary answer to this question, we propose an Automata Networks model of the formation of a vocabulary on a population of individuals, under two in principle opposite strategies: the alignment and the least effort principle. Within the previous account to the emergence of linguistic conventions (specially, the Naming Game), we focus on modeling speaker and hearer efforts as actions over their vocabularies and we study the impact of these actions on the formation of a shared language. The numerical simulations are essentially based on an energy function, that measures the amount of local agreement between the vocabularies. The results suggests that on one dimensional lattices the best strategy to the formation of shared languages is the one that minimizes the efforts of speakers on communicative tasks.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01580",
        "title": "Automata networks for memory loss effects in the formation of linguistic conventions",
        "authors": [
            "Javier Vera",
            "Eric Goles"
        ],
        "abstract": "This work attempts to give new theoretical insights to the absence of intermediate stages in the evolution of language. In particular, it is developed an automata networks approach to a crucial question: how a population of language users can reach agreement on a linguistic convention? To describe the appearance of sharp transitions in the self-organization of language, it is adopted an extremely simple model of (working) memory. At each time step, language users simply loss part of their word-memories. Through computer simulations of low-dimensional lattices, it appear sharp transitions at critical values that depend on the size of the vicinities of the individuals.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2015-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01585",
        "title": "Applying Deep Learning to Answer Selection: A Study and An Open Task",
        "authors": [
            "Minwei Feng",
            "Bing Xiang",
            "Michael R. Glass",
            "Lidan Wang",
            "Bowen Zhou"
        ],
        "abstract": "We apply a general deep learning framework to address the non-factoid question answering task. Our approach does not rely on any linguistic tools and can be applied to different languages or domains. Various architectures are presented and compared. We create and release a QA corpus and setup a new QA task in the insurance domain. Experimental results demonstrate superior performance compared to the baseline methods and various technologies give further improvements. For this highly challenging task, the top-1 accuracy can reach up to 65.3% on a test set, which indicates a great potential for practical use.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01718",
        "title": "Study of Phonemes Confusions in Hierarchical Automatic Phoneme Recognition System",
        "authors": [
            "Rimah Amami",
            "Noureddine Ellouze"
        ],
        "abstract": "In this paper, we have analyzed the impact of confusions on the robustness of phoneme recognitions system. The confusions are detected at the pronunciation and the confusions matrices of the phoneme recognizer. The confusions show that some similarities between phonemes at the pronunciation affect significantly the recognition rates. This paper proposes to understand those confusions in order to improve the performance of the phoneme recognition system by isolating the problematic phonemes. Confusion analysis leads to build a new hierarchical recognizer using new phoneme distribution and the information from the confusion matrices. This new hierarchical phoneme recognition system shows significant improvements of the recognition rates on TIMIT database.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2015-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01745",
        "title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems",
        "authors": [
            "Tsung-Hsien Wen",
            "Milica Gasic",
            "Nikola Mrksic",
            "Pei-Hao Su",
            "David Vandyke",
            "Steve Young"
        ],
        "abstract": "Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2015-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01755",
        "title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking",
        "authors": [
            "Tsung-Hsien Wen",
            "Milica Gasic",
            "Dongho Kim",
            "Nikola Mrksic",
            "Pei-Hao Su",
            "David Vandyke",
            "Steve Young"
        ],
        "abstract": "The natural language generation (NLG) component of a spoken dialogue system (SDS) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on. These limitations add significantly to development costs and make cross-domain, multi-lingual dialogue systems intractable. Moreover, human languages are context-aware. The most natural response should be directly learned from data rather than depending on predefined syntaxes or rules. This paper presents a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. Objective metrics suggest that this new model outperforms previous methods under the same experimental conditions. Results of an evaluation by human judges indicate that it produces not only high quality but linguistically varied utterances which are preferred compared to n-gram and rule-based systems.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2015-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01786",
        "title": "Mimicry Is Presidential: Linguistic Style Matching in Presidential Debates and Improved Polling Numbers",
        "authors": [
            "Daniel M. Romero",
            "Roderick I. Swaab",
            "Brian Uzzi",
            "Adam D. Galinsky"
        ],
        "abstract": "The current research used the contexts of U.S. presidential debates and negotiations to examine whether matching the linguistic style of an opponent in a two-party exchange affects the reactions of third-party observers. Building off communication accommodation theory (CAT), interaction alignment theory (IAT), and processing fluency, we propose that language style matching (LSM) will improve subsequent third-party evaluations because matching an opponent's linguistic style reflects greater perspective taking and will make one's arguments easier to process. In contrast, research on status inferences predicts that LSM will negatively impact third-party evaluations because LSM implies followership. We conduct two studies to test these competing hypotheses. Study 1 analyzed transcripts of U.S. presidential debates between 1976 and 2012 and found that candidates who matched their opponent's linguistic style increased their standing in the polls. Study 2 demonstrated a causal relationship between LSM and third-party observer evaluations using negotiation transcripts.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2015-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01991",
        "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
        "authors": [
            "Zhiheng Huang",
            "Wei Xu",
            "Kai Yu"
        ],
        "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2015-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01996",
        "title": "An Automatic Machine Translation Evaluation Metric Based on Dependency Parsing Model",
        "authors": [
            "Hui Yu",
            "Xiaofeng Wu",
            "Wenbin Jiang",
            "Qun Liu",
            "ShouXun Lin"
        ],
        "abstract": "Most of the syntax-based metrics obtain the similarity by comparing the sub-structures extracted from the trees of hypothesis and reference. These sub-structures are defined by human and can't express all the information in the trees because of the limited length of sub-structures. In addition, the overlapped parts between these sub-structures are computed repeatedly. To avoid these problems, we propose a novel automatic evaluation metric based on dependency parsing model, with no need to define sub-structures by human. First, we train a dependency parsing model by the reference dependency tree. Then we generate the hypothesis dependency tree and the corresponding probability by the dependency parsing model. The quality of the hypothesis can be judged by this probability. In order to obtain the lexicon similarity, we also introduce the unigram F-score to the new metric. Experiment results show that the new metric gets the state-of-the-art performance on system level, and is comparable with METEOR on sentence level.\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02060",
        "title": "Egyptian Dialect Stopword List Generation from Social Network Data",
        "authors": [
            "Walaa Medhat",
            "Ahmed H. Yousef",
            "Hoda Korashy"
        ],
        "abstract": "This paper proposes a methodology for generating a stopword list from online social network (OSN) corpora in Egyptian Dialect(ED). The aim of the paper is to investigate the effect of removingED stopwords on the Sentiment Analysis (SA) task. The stopwords lists generated before were on Modern Standard Arabic (MSA) which is not the common language used in OSN. We have generated a stopword list of Egyptian dialect to be used with the OSN corpora. We compare the efficiency of text classification when using the generated list along with previously generated lists of MSA and combining the Egyptian dialect list with the MSA list. The text classification was performed using Na\u00efve Bayes and Decision Tree classifiers and two feature selection approaches, unigram and bigram. The experiments show that removing ED stopwords give better performance than using lists of MSA stopwords only.\n    ",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02091",
        "title": "Image Representations and New Domains in Neural Image Captioning",
        "authors": [
            "Jack Hessel",
            "Nicolas Savva",
            "Michael J. Wilber"
        ],
        "abstract": "We examine the possibility that recent promising results in automatic caption generation are due primarily to language models. By varying image representation quality produced by a convolutional neural network, we find that a state-of-the-art neural captioning algorithm is able to produce quality captions even when provided with surprisingly poor image representations. We replicate this result in a new, fine-grained, transfer learned captioning domain, consisting of 66K recipe image/title pairs. We also provide some experiments regarding the appropriateness of datasets for automatic captioning, and find that having multiple captions per image is beneficial, but not an absolute requirement.\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2015-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02096",
        "title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation",
        "authors": [
            "Wang Ling",
            "Tiago Lu\u00eds",
            "Lu\u00eds Marujo",
            "Ram\u00f3n Fernandez Astudillo",
            "Silvio Amir",
            "Chris Dyer",
            "Alan W. Black",
            "Isabel Trancoso"
        ],
        "abstract": "We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our \"composed\" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02131",
        "title": "Learning Structural Kernels for Natural Language Processing",
        "authors": [
            "Daniel Beck",
            "Trevor Cohn",
            "Christian Hardmeier",
            "Lucia Specia"
        ],
        "abstract": "Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing. However, the problem of model selection in kernel-based methods is usually overlooked. Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained. In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods. In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes. Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search. The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02142",
        "title": "Feature-based Decipherment for Large Vocabulary Machine Translation",
        "authors": [
            "Iftekhar Naim",
            "Daniel Gildea"
        ],
        "abstract": "Orthographic similarities across languages provide a strong signal for probabilistic decipherment, especially for closely related language pairs. The existing decipherment models, however, are not well-suited for exploiting these orthographic similarities. We propose a log-linear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via MCMC sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence scales to large vocabularies and outperforms the existing generative decipherment models by exploiting the orthographic features.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02225",
        "title": "Improve the Evaluation of Fluency Using Entropy for Machine Translation Evaluation Metrics",
        "authors": [
            "Hui Yu",
            "Xiaofeng Wu",
            "Wenbin Jiang",
            "Qun Liu",
            "Shouxun Lin"
        ],
        "abstract": "The widely-used automatic evaluation metrics cannot adequately reflect the fluency of the translations. The n-gram-based metrics, like BLEU, limit the maximum length of matched fragments to n and cannot catch the matched fragments longer than n, so they can only reflect the fluency indirectly. METEOR, which is not limited by n-gram, uses the number of matched chunks but it does not consider the length of each chunk. In this paper, we propose an entropy-based method, which can sufficiently reflect the fluency of translations through the distribution of matched words. This method can easily combine with the widely-used automatic evaluation metrics to improve the evaluation of fluency. Experiments show that the correlations of BLEU and METEOR are improved on sentence level after combining with the entropy-based method on WMT 2010 and WMT 2012.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02285",
        "title": "Adapting Phrase-based Machine Translation to Normalise Medical Terms in Social Media Messages",
        "authors": [
            "Nut Limsopatham",
            "Nigel Collier"
        ],
        "abstract": "Previous studies have shown that health reports in social media, such as DailyStrength and Twitter, have potential for monitoring health conditions (e.g. adverse drug reactions, infectious diseases) in particular communities. However, in order for a machine to understand and make inferences on these health conditions, the ability to recognise when laymen's terms refer to a particular medical concept (i.e.\\ text normalisation) is required. To achieve this, we propose to adapt an existing phrase-based machine translation (MT) technique and a vector representation of words to map between a social media phrase and a medical concept. We evaluate our proposed approach using a collection of phrases from tweets related to adverse drug reactions. Our experimental results show that the combination of a phrase-based MT technique and the similarity between word vector representations outperforms the baselines that apply only either of them by up to 55%.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02297",
        "title": "Measuring Word Significance using Distributed Representations of Words",
        "authors": [
            "Adriaan M. J. Schakel",
            "Benjamin J. Wilson"
        ],
        "abstract": "Distributed representations of words as real-valued vectors in a relatively low-dimensional space aim at extracting syntactic and semantic features from large text corpora. A recently introduced neural network, named word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), was shown to encode semantic information in the direction of the word vectors. In this brief report, it is proposed to use the length of the vectors, together with the term frequency, as measure of word significance in a corpus. Experimental evidence using a domain-specific corpus of abstracts is presented to support this proposal. A useful visualization technique for text corpora emerges, where words are mapped onto a two-dimensional plane and automatically ranked by significance.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02354",
        "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning",
        "authors": [
            "Jianpeng Cheng",
            "Dimitri Kartsaklis"
        ],
        "abstract": "Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02375",
        "title": "Approximation-Aware Dependency Parsing by Belief Propagation",
        "authors": [
            "Matthew R. Gormley",
            "Mark Dredze",
            "Jason Eisner"
        ],
        "abstract": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n^3) runtime. It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by back-propagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs. The resulting trained parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02445",
        "title": "Removing Biases from Trainable MT Metrics by Using Self-Training",
        "authors": [
            "Milo\u0161 Stanojevi\u0107"
        ],
        "abstract": "Most trainable machine translation (MT) metrics train their weights on human judgments of state-of-the-art MT systems outputs. This makes trainable metrics biases in many ways. One of them is preferring longer translations. These biased metrics when used for tuning are evaluating different types of translations -- n-best lists of translations with very diverse quality. Systems tuned with these metrics tend to produce overly long translations that are preferred by the metric but not by humans. This is usually solved by manually tweaking metric's weights to equally value recall and precision. Our solution is more general: (1) it does not address only the recall bias but also all other biases that might be present in the data and (2) it does not require any knowledge of the types of features used which is useful in cases when manual tuning of metric's weights is not possible. This is accomplished by self-training on unlabeled n-best lists by using metric that was initially trained on standard human judgments. One way of looking at this is as domain adaptation from the domain of state-of-the-art MT translations to diverse n-best list translations.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03040",
        "title": "Syntax Evolution: Problems and Recursion",
        "authors": [
            "Ram\u00f3n Casares"
        ],
        "abstract": "To investigate the evolution of syntax, we need to ascertain the evolutionary r\u00f4le of syntax and, before that, the very nature of syntax. Here, we will assume that syntax is computing. And then, since we are computationally Turing complete, we meet an evolutionary anomaly, the anomaly of sytax: we are syntactically too competent for syntax. Assuming that problem solving is computing, and realizing that the evolutionary advantage of Turing completeness is full problem solving and not syntactic proficiency, we explain the anomaly of syntax by postulating that syntax and problem solving co-evolved in humans towards Turing completeness. Examining the requirements that full problem solving impose on language, we find firstly that semantics is not sufficient and that syntax is necessary to represent problems. Our final conclusion is that full problem solving requires a functional semantics on an infinite tree-structured syntax. Besides these results, the introduction of Turing completeness and problem solving to explain the evolution of syntax should help us to fit the evolution of language within the evolution of cognition, giving us some new clues to understand the elusive relation between language and thinking.\n    ",
        "submission_date": "2015-08-12T00:00:00",
        "last_modified_date": "2019-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03530",
        "title": "Information-theoretical analysis of the statistical dependencies among three variables: Applications to written language",
        "authors": [
            "Dami\u00e1n G. Hern\u00e1ndez",
            "Dami\u00e1n H. Zanette",
            "In\u00e9s Samengo"
        ],
        "abstract": "We develop the information-theoretical concepts required to study the statistical dependencies among three variables. Some of such dependencies are pure triple interactions, in the sense that they cannot be explained in terms of a combination of pairwise correlations. We derive bounds for triple dependencies, and characterize the shape of the joint probability distribution of three binary variables with high triple interaction. The analysis also allows us to quantify the amount of redundancy in the mutual information between pairs of variables, and to assess whether the information between two variables is or is not mediated by a third variable. These concepts are applied to the analysis of written texts. We find that the probability that a given word is found in a particular location within the text is not only modulated by the presence or absence of other nearby words, but also, on the presence or absence of nearby pairs of words. We identify the words enclosing the key semantic concepts of the text, the triplets of words with high pairwise and triple interactions, and the words that mediate the pairwise interactions between other words.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03720",
        "title": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Path",
        "authors": [
            "Xu Yan",
            "Lili Mou",
            "Ge Li",
            "Yunchuan Chen",
            "Hao Peng",
            "Zhi Jin"
        ],
        "abstract": "Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an $F_1$-score of 83.7\\%, higher than competing methods in the literature.\n    ",
        "submission_date": "2015-08-15T00:00:00",
        "last_modified_date": "2015-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03721",
        "title": "A Comparative Study on Regularization Strategies for Embedding-based Neural Networks",
        "authors": [
            "Hao Peng",
            "Lili Mou",
            "Ge Li",
            "Yunchuan Chen",
            "Yangyang Lu",
            "Zhi Jin"
        ],
        "abstract": "This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, re-embedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models.\n    ",
        "submission_date": "2015-08-15T00:00:00",
        "last_modified_date": "2015-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03826",
        "title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution",
        "authors": [
            "Shaohua Li",
            "Jun Zhu",
            "Chunyan Miao"
        ],
        "abstract": "Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)-based methods. However some models are opaque to probabilistic interpretation, and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information. In addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since generative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semidefinite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods.\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2015-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03854",
        "title": "Online Representation Learning in Recurrent Neural Language Models",
        "authors": [
            "Marek Rei"
        ],
        "abstract": "We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step.\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2015-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04025",
        "title": "Effective Approaches to Attention-based Neural Machine Translation",
        "authors": [
            "Minh-Thang Luong",
            "Hieu Pham",
            "Christopher D. Manning"
        ],
        "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04112",
        "title": "Molding CNNs for text: non-linear, non-consecutive convolutions",
        "authors": [
            "Tao Lei",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ],
        "abstract": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04257",
        "title": "Learning Meta-Embeddings by Using Ensembles of Embedding Sets",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Word embeddings -- distributed representations of words -- in deep learning are beneficial for many tasks in natural language processing (NLP). However, different embedding sets vary greatly in quality and characteristics of the captured semantics. Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning meta-embeddings. Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of meta-embeddings compared to individual embedding sets. One advantage of meta-embeddings is the increased vocabulary coverage. We will release our meta-embeddings publicly.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04271",
        "title": "Probabilistic Modelling of Morphologically Rich Languages",
        "authors": [
            "Jan A. Botha"
        ],
        "abstract": "This thesis investigates how the sub-structure of words can be accounted for in probabilistic models of language. Such models play an important role in natural language processing tasks such as translation or speech recognition, but often rely on the simplistic assumption that words are opaque symbols. This assumption does not fit morphologically complex language well, where words can have rich internal structure and sub-word elements are shared across distinct word forms.\n",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04395",
        "title": "End-to-End Attention-based Large Vocabulary Speech Recognition",
        "authors": [
            "Dzmitry Bahdanau",
            "Jan Chorowski",
            "Dmitriy Serdyuk",
            "Philemon Brakel",
            "Yoshua Bengio"
        ],
        "abstract": "Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04515",
        "title": "Exploring Metaphorical Senses and Word Representations for Identifying Metonyms",
        "authors": [
            "Wei Zhang",
            "Judith Gelernter"
        ],
        "abstract": "A metonym is a word with a figurative meaning, similar to a metaphor. Because metonyms are closely related to metaphors, we apply features that are used successfully for metaphor recognition to the task of detecting metonyms. On the ACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system achieved 86.45% accuracy on the location metonyms. Our code can be found on GitHub.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04525",
        "title": "Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles",
        "authors": [
            "Wei Zhang",
            "Yang Yu",
            "Osho Gupta",
            "Judith Gelernter"
        ],
        "abstract": "Precise geocoding and time normalization for text requires that location and time phrases be identified. Many state-of-the-art geoparsers and temporal parsers suffer from low recall. Categories commonly missed by parsers are: nouns used in a non- spatiotemporal sense, adjectival and adverbial phrases, prepositional phrases, and numerical phrases. We collected and annotated data set by querying commercial web searches API with such spatiotemporal expressions as were missed by state-of-the- art parsers. Due to the high cost of sentence annotation, active learning was used to label training data, and a new strategy was designed to better select training examples to reduce labeling cost. For the learning algorithm, we applied an average perceptron trained Featurized Hidden Markov Model (FHMM). Five FHMM instances were used to create an ensemble, with the output phrase selected by voting. Our ensemble model was tested on a range of sequential labeling tasks, and has shown competitive performance. Our contributions include (1) an new dataset annotated with named entities and expanded spatiotemporal expressions; (2) a comparison of inference algorithms for ensemble models showing the superior accuracy of Belief Propagation over Viterbi Decoding; (3) a new example re-weighting method for active ensemble learning that 'memorizes' the latest examples trained; (4) a spatiotemporal parser that jointly recognizes expanded spatiotemporal expressions as well as named entities.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04562",
        "title": "Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections",
        "authors": [
            "Jingwei Zhang",
            "Aaron Gerow",
            "Jaan Altosaar",
            "James Evans",
            "Richard Jean So"
        ],
        "abstract": "Weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models. This paper introduces two probabilistic topic models, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address problems that can arise when analyzing large, asymmetric, and potentially weakly-related collections. Topic correlations in weakly-related collections typically lie in the tail of the topic distribution, where they would be overlooked by models unable to fit large numbers of topics. To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015). The models are first evaluated on synthetic data, generated to simulate various collection-level asymmetries. We then present a case study of modeling over 300k documents in collections of sciences and humanities research from JSTOR.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05051",
        "title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models",
        "authors": [
            "Kenton Murray",
            "David Chiang"
        ],
        "abstract": "Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through $\\ell_{\\infty,1}$ and $\\ell_{2,1}$ regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05154",
        "title": "Posterior calibration and exploratory analysis for natural language processing models",
        "authors": [
            "Khanh Nguyen",
            "Brendan O'Connor"
        ],
        "abstract": "Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that (1) the quality of a model' s posterior distribution can and should be directly evaluated, as to whether probabilities correspond to empirical frequencies, and (2) NLP uncertainty can be projected not only to pipeline components, but also to exploratory data analysis, telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration, and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task.\n    ",
        "submission_date": "2015-08-21T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05163",
        "title": "Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method",
        "authors": [
            "Yustinus Eko Soelistio",
            "Martinus Raditia Sigit Surendra"
        ],
        "abstract": "Text mining can be applied to many fields. One of the application is using text mining in digital newspaper to do politic sentiment analysis. In this paper sentiment analysis is applied to get information from digital news articles about its positive or negative sentiment regarding particular politician. This paper suggests a simple model to analyze digital newspaper sentiment polarity using naive Bayes classifier method. The model uses a set of initial data to begin with which will be updated when new information appears. The model showed promising result when tested and can be implemented to some other sentiment analysis problems.\n    ",
        "submission_date": "2015-08-21T00:00:00",
        "last_modified_date": "2015-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05326",
        "title": "A large annotated corpus for learning natural language inference",
        "authors": [
            "Samuel R. Bowman",
            "Gabor Angeli",
            "Christopher Potts",
            "Christopher D. Manning"
        ],
        "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.\n    ",
        "submission_date": "2015-08-21T00:00:00",
        "last_modified_date": "2015-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05817",
        "title": "Echoes of Persuasion: The Effect of Euphony in Persuasive Communication",
        "authors": [
            "Marco Guerini",
            "G\u00f6zde \u00d6zbal",
            "Carlo Strapparava"
        ],
        "abstract": "While the effect of various lexical, syntactic, semantic and stylistic features have been addressed in persuasive language from a computational point of view, the persuasive effect of phonetics has received little attention. By modeling a notion of euphony and analyzing four datasets comprising persuasive and non-persuasive sentences in different domains (political speeches, movie quotes, slogans and tweets), we explore the impact of sounds on different forms of persuasiveness. We conduct a series of analyses and prediction experiments within and across datasets. Our results highlight the positive role of phonetic devices on persuasion.\n    ",
        "submission_date": "2015-08-24T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05902",
        "title": "A Framework for Comparing Groups of Documents",
        "authors": [
            "Arun S. Maiya"
        ],
        "abstract": "We present a general framework for comparing multiple groups of documents. A bipartite graph model is proposed where document groups are represented as one node set and the comparison criteria are represented as the other node set. Using this model, we present basic algorithms to extract insights into similarities and differences among the document groups. Finally, we demonstrate the versatility of our framework through an analysis of NSF funding programs for basic research.\n    ",
        "submission_date": "2015-08-24T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06034",
        "title": "Better Summarization Evaluation with Word Embeddings for ROUGE",
        "authors": [
            "Jun-Ping Ng",
            "Viktoria Abrecht"
        ],
        "abstract": "ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefficients.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06044",
        "title": "Visualizing NLP annotations for Crowdsourcing",
        "authors": [
            "Hanchuan Li",
            "Haichen Shen",
            "Shengliang Xu",
            "Congle Zhang"
        ],
        "abstract": "Visualizing NLP annotation is useful for the collection of training data for the statistical NLP approaches. Existing toolkits either provide limited visual aid, or introduce comprehensive operators to realize sophisticated linguistic rules. Workers must be well trained to use them. Their audience thus can hardly be scaled to large amounts of non-expert crowdsourced workers. In this paper, we present CROWDANNO, a visualization toolkit to allow crowd-sourced workers to annotate two general categories of NLP problems: clustering and parsing. Workers can finish the tasks with simplified operators in an interactive interface, and fix errors conveniently. User studies show our toolkit is very friendly to NLP non-experts, and allow them to produce high quality labels for several sophisticated problems. We release our source code and toolkit to spur future research.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06374",
        "title": "A fully data-driven method to identify (correlated) changes in diachronic corpora",
        "authors": [
            "Alexander Koplenig"
        ],
        "abstract": "In this paper, a method for measuring synchronic corpus (dis-)similarity put forward by Kilgarriff (2001) is adapted and extended to identify trends and correlated changes in diachronic text data, using the Corpus of Historical American English (Davies 2010a) and the Google Ngram Corpora (Michel et al. 2010a). This paper shows that this fully data-driven method, which extracts word types that have undergone the most pronounced change in frequency in a given period of time, is computationally very cheap and that it allows interpretations of diachronic trends that are both intuitively plausible and motivated from the perspective of information theory. Furthermore, it demonstrates that the method is able to identify correlated linguistic changes and diachronic shifts that can be linked to historical events. Finally, it can help to improve diachronic POS tagging and complement existing NLP approaches. This indicates that the approach can facilitate an improved understanding of diachronic processes in language change.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2015-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06451",
        "title": "Crossings as a side effect of dependency lengths",
        "authors": [
            "Ramon Ferrer-i-Cancho",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "abstract": "The syntactic structure of sentences exhibits a striking regularity: dependencies tend to not cross when drawn above the sentence. We investigate two competing explanations. The traditional hypothesis is that this trend arises from an independent principle of syntax that reduces crossings practically to zero. An alternative to this view is the hypothesis that crossings are a side effect of dependency lengths, i.e. sentences with shorter dependency lengths should tend to have fewer crossings. We are able to reject the traditional view in the majority of languages considered. The alternative hypothesis can lead to a more parsimonious theory of language.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06491",
        "title": "Alignment-based compositional semantics for instruction following",
        "authors": [
            "Jacob Andreas",
            "Dan Klein"
        ],
        "abstract": "This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the model's flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2017-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06615",
        "title": "Character-Aware Neural Language Models",
        "authors": [
            "Yoon Kim",
            "Yacine Jernite",
            "David Sontag",
            "Alexander M. Rush"
        ],
        "abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06669",
        "title": "Component-Enhanced Chinese Character Embeddings",
        "authors": [
            "Yanran Li",
            "Wenjie Li",
            "Fei Sun",
            "Sujian Li"
        ],
        "abstract": "Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2015-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07544",
        "title": "Computational Sociolinguistics: A Survey",
        "authors": [
            "Dong Nguyen",
            "A. Seza Do\u011fru\u00f6z",
            "Carolyn P. Ros\u00e9",
            "Franciska de Jong"
        ],
        "abstract": "Language is a social phenomenon and variation is inherent to its social nature. Recently, there has been a surge of interest within the computational linguistics (CL) community in the social dimension of language. In this article we present a survey of the emerging field of \"Computational Sociolinguistics\" that reflects this increased interest. We aim to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics such as the relation between language and social identity, language use in social interaction and multilingual communication. Moreover, we demonstrate the potential for synergy between the research communities involved, by showing how the large-scale data-driven methods that are widely used in CL can complement existing sociolinguistic studies, and how sociolinguistics can inform and challenge the methods and assumptions employed in CL studies. We hope to convey the possible benefits of a closer collaboration between the two communities and conclude with a discussion of open challenges.\n    ",
        "submission_date": "2015-08-30T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07555",
        "title": "An Event Network for Exploring Open Information",
        "authors": [
            "Yanping Chen"
        ],
        "abstract": "In this paper, an event network is presented for exploring open information, where linguistic units about an event are organized for analysing. The process is divided into three steps: document event detection, event network construction and event network analysis. First, by implementing event detection or tracking, documents are retrospectively (or on-line) organized into document events. Secondly, for each of the document event, linguistic units are extracted and combined into event networks. Thirdly, various analytic methods are proposed for event network analysis. In our application methodologies are presented for exploring open information.\n    ",
        "submission_date": "2015-08-30T00:00:00",
        "last_modified_date": "2015-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07709",
        "title": "Word Representations, Tree Models and Syntactic Functions",
        "authors": [
            "Simon \u0160uster",
            "Gertjan van Noord",
            "Ivan Titov"
        ],
        "abstract": "Word representations induced from models with discrete latent variables (e.g.\\ HMMs) have been shown to be beneficial in many NLP applications. In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components. Such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words, which, in turn, may be desirable in many NLP applications. We evaluate the word representations on two tasks -- named entity recognition and semantic frame identification. We observe improvements from exploiting syntactic function information in both cases, and the results rivaling those of state-of-the-art representation learning methods. Additionally, we revisit the relationship between sequential and unlabeled-tree models and find that the advantage of the latter is not self-evident.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07909",
        "title": "Neural Machine Translation of Rare Words with Subword Units",
        "authors": [
            "Rico Sennrich",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00685",
        "title": "A Neural Attention Model for Abstractive Sentence Summarization",
        "authors": [
            "Alexander M. Rush",
            "Sumit Chopra",
            "Jason Weston"
        ],
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00705",
        "title": "Analysis of Communication Pattern with Scammers in Enron Corpus",
        "authors": [
            "Dinesh Balaji Sashikanth"
        ],
        "abstract": "This paper is an exploratory analysis into fraud detection taking Enron email corpus as the case study. The paper posits conclusions like strict servitude and unquestionable faith among employees as breeding grounds for sham among higher executives. We also try to infer on the nature of communication between fraudulent employees and between non- fraudulent-fraudulent employees\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00838",
        "title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment",
        "authors": [
            "Hongyuan Mei",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00963",
        "title": "On TimeML-Compliant Temporal Expression Extraction in Turkish",
        "authors": [
            "Dilek K\u00fc\u00e7\u00fck",
            "Do\u011fan K\u00fc\u00e7\u00fck"
        ],
        "abstract": "It is commonly acknowledged that temporal expression extractors are important components of larger natural language processing systems like information retrieval and question answering systems. Extraction and normalization of temporal expressions in Turkish has not been given attention so far except the extraction of some date and time expressions within the course of named entity recognition. As TimeML is the current standard of temporal expression and event annotation in natural language texts, in this paper, we present an analysis of temporal expressions in Turkish based on the related TimeML classification (i.e., date, time, duration, and set expressions). We have created a lexicon for Turkish temporal expressions and devised considerably wide-coverage patterns using the lexical classes as the building blocks. We believe that the proposed patterns, together with convenient normalization rules, can be readily used by prospective temporal expression extraction tools for Turkish.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01007",
        "title": "Encoding Prior Knowledge with Eigenword Embeddings",
        "authors": [
            "Dominique Osborne",
            "Shashi Narayan",
            "Shay B. Cohen"
        ],
        "abstract": "Canonical correlation analysis (CCA) is a method for reducing the dimension of data represented using two views. It has been previously used to derive word embeddings, where one view indicates a word, and the other view indicates its context. We describe a way to incorporate prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01310",
        "title": "The influence of Chunking on Dependency Crossing and Distance",
        "authors": [
            "Qian Lu",
            "Chunshan Xu",
            "Haitao Liu"
        ],
        "abstract": "This paper hypothesizes that chunking plays important role in reducing dependency distance and dependency crossings. Computer simulations, when compared with natural languages,show that chunking reduces mean dependency distance (MDD) of a linear sequence of nodes (constrained by continuity or projectivity) to that of natural languages. More interestingly, chunking alone brings about less dependency crossings as well, though having failed to reduce them, to such rarity as found in human languages. These results suggest that chunking may play a vital role in the minimization of dependency distance, and a somewhat contributing role in the rarity of dependency crossing. In addition, the results point to a possibility that the rarity of dependency crossings is not a mere side-effect of minimization of dependency distance, but a linguistic phenomenon with its own motivations.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01599",
        "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing",
        "authors": [
            "Parminder Bhatia",
            "Yangfeng Ji",
            "Jacob Eisenstein"
        ],
        "abstract": "Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classification-based methods.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01692",
        "title": "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning",
        "authors": [
            "Ekaterina Vylomova",
            "Laura Rimell",
            "Trevor Cohn",
            "Timothy Baldwin"
        ],
        "abstract": "Recent work on word embeddings has shown that simple vector subtraction over pre-trained embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items.\n    ",
        "submission_date": "2015-09-05T00:00:00",
        "last_modified_date": "2016-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01722",
        "title": "A commentary on \"The now-or-never bottleneck: a fundamental constraint on language\", by Christiansen and Chater (2016)",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "In a recent article, Christiansen and Chater (2016) present a fundamental constraint on language, i.e. a now-or-never bottleneck that arises from our fleeting memory, and explore its implications, e.g., chunk-and-pass processing, outlining a framework that promises to unify different areas of research. Here we explore additional support for this constraint and suggest further connections from quantitative linguistics and information theory.\n    ",
        "submission_date": "2015-09-05T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01899",
        "title": "Integrate Document Ranking Information into Confidence Measure Calculation for Spoken Term Detection",
        "authors": [
            "Quan Liu",
            "Wu Guo",
            "Zhen-Hua Ling"
        ],
        "abstract": "This paper proposes an algorithm to improve the calculation of confidence measure for spoken term detection (STD). Given an input query term, the algorithm first calculates a measurement named document ranking weight for each document in the speech database to reflect its relevance with the query term by summing all the confidence measures of the hypothesized term occurrences in this document. The confidence measure of each term occurrence is then re-estimated through linear interpolation with the calculated document ranking weight to improve its reliability by integrating document-level information. Experiments are conducted on three standard STD tasks for Tamil, Vietnamese and English respectively. The experimental results all demonstrate that the proposed algorithm achieves consistent improvements over the state-of-the-art method for confidence measure calculation. Furthermore, this algorithm is still effective even if a high accuracy speech recognizer is not available, which makes it applicable for the languages with limited speech resources.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01938",
        "title": "Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical Machine Translation",
        "authors": [
            "Katrin Kirchhoff",
            "Bing Zhao",
            "Wen Wang"
        ],
        "abstract": "Statistical machine translation for dialectal Arabic is characterized by a lack of data since data acquisition involves the transcription and translation of spoken language. In this study we develop techniques for extracting parallel data for one particular dialect of Arabic (Iraqi Arabic) from out-of-domain corpora in different dialects of Arabic or in Modern Standard Arabic. We compare two different data selection strategies (cross-entropy based and submodular selection) and demonstrate that a very small but highly targeted amount of found data can improve the performance of a baseline machine translation system. We furthermore report on preliminary experiments on using automatically translated speech data as additional training data.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02208",
        "title": "Unsupervised Discovery of Linguistic Structure Including Two-level Acoustic Patterns Using Three Cascaded Stages of Iterative Optimization",
        "authors": [
            "Cheng-Tao Chung",
            "Chun-an Chan",
            "Lin-shan Lee"
        ],
        "abstract": "Techniques for unsupervised discovery of acoustic patterns are getting increasingly attractive, because huge quantities of speech data are becoming available but manual annotations remain hard to acquire. In this paper, we propose an approach for unsupervised discovery of linguistic structure for the target spoken language given raw speech data. This linguistic structure includes two-level (subword-like and word-like) acoustic patterns, the lexicon of word-like patterns in terms of subword-like patterns and the N-gram language model based on word-like patterns. All patterns, models, and parameters can be automatically learned from the unlabelled speech corpus. This is achieved by an initialization step followed by three cascaded stages for acoustic, linguistic, and lexical iterative optimization. The lexicon of word-like patterns defines allowed consecutive sequence of HMMs for subword-like patterns. In each iteration, model training and decoding produces updated labels from which the lexicon and HMMs can be further updated. In this way, model parameters and decoded labels are respectively optimized in each iteration, and the knowledge about the linguistic structure is learned gradually layer after layer. The proposed approach was tested in preliminary experiments on a corpus of Mandarin broadcast news, including a task of spoken term detection with performance compared to a parallel test using models trained in a supervised way. Results show that the proposed system not only yields reasonable performance on its own, but is also complimentary to existing large vocabulary ASR systems.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02213",
        "title": "Unsupervised Spoken Term Detection with Spoken Queries by Multi-level Acoustic Patterns with Varying Model Granularity",
        "authors": [
            "Cheng-Tao Chung",
            "Chun-an Chan",
            "Lin-shan Lee"
        ],
        "abstract": "This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16\\% in mean average precision on the TIMIT corpus.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02217",
        "title": "Enhancing Automatically Discovered Multi-level Acoustic Patterns Considering Context Consistency With Applications in Spoken Term Detection",
        "authors": [
            "Cheng-Tao Chung",
            "Wei-Ning Hsu",
            "Cheng-Yi Lee",
            "Lin-Shan Lee"
        ],
        "abstract": "This paper presents a novel approach for enhancing the multiple sets of acoustic patterns automatically discovered from a given corpus. In a previous work it was proposed that different HMM configurations (number of states per model, number of distinct models) for the acoustic patterns form a two-dimensional space. Multiple sets of acoustic patterns automatically discovered with the HMM configurations properly located on different points over this two-dimensional space were shown to be complementary to one another, jointly capturing the characteristics of the given corpus. By representing the given corpus as sequences of acoustic patterns on different HMM sets, the pattern indices in these sequences can be relabeled considering the context consistency across the different sequences. Good improvements were observed in preliminary experiments of pattern spoken term detection (STD) performed on both TIMIT and Mandarin Broadcast News with such enhanced patterns.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02301",
        "title": "Probabilistic Bag-Of-Hyperlinks Model for Entity Linking",
        "authors": [
            "Octavian-Eugen Ganea",
            "Marina Ganea",
            "Aurelien Lucchi",
            "Carsten Eickhoff",
            "Thomas Hofmann"
        ],
        "abstract": "Many fundamental problems in natural language processing rely on determining what entities appear in a given text. Commonly referenced as entity linking, this step is a fundamental component of many NLP tasks such as text understanding, automatic summarization, semantic search or machine translation. Name ambiguity, word polysemy, context dependencies and a heavy-tailed distribution of entities contribute to the complexity of this problem.\n",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2016-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02412",
        "title": "Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition",
        "authors": [
            "Mortaza Doulaty",
            "Oscar Saz",
            "Thomas Hain"
        ],
        "abstract": "Speech recognition systems are often highly domain dependent, a fact widely reported in the literature. However the concept of domain is complex and not bound to clear criteria. Hence it is often not evident if data should be considered to be out-of-domain. While both acoustic and language models can be domain specific, work in this paper concentrates on acoustic modelling. We present a novel method to perform unsupervised discovery of domains using Latent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is assumed to exist in the data, whereby each audio segment can be considered to be a weighted mixture of domain properties. The classification of audio segments into domains allows the creation of domain specific acoustic models for automatic speech recognition. Experiments are conducted on a dataset of diverse speech data covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech, with a joint training set of 60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to LDA based domains was shown to yield relative Word Error Rate (WER) improvements of up to 16% relative, compared to pooled training, and up to 10%, compared with models adapted with human-labelled prior domain knowledge.\n    ",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03208",
        "title": "Towards Understanding Egyptian Arabic Dialogues",
        "authors": [
            "Abdelrahim A Elmadany",
            "Sherif M Abdou",
            "Mervat Gheith"
        ],
        "abstract": "Labelling of user's utterances to understanding his attends which called Dialogue Act (DA) classification, it is considered the key player for dialogue language understanding layer in automatic dialogue systems. In this paper, we proposed a novel approach to user's utterances labeling for Egyptian spontaneous dialogues and Instant Messages using Machine Learning (ML) approach without relying on any special lexicons, cues, or rules. Due to the lack of Egyptian dialect dialogue corpus, the system evaluated by multi-genre corpus includes 4725 utterances for three domains, which are collected and annotated manually from Egyptian call-centers. The system achieves F1 scores of 70. 36% overall domains.\n    ",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03295",
        "title": "Liberating language research from dogmas of the 20th century",
        "authors": [
            "Ramon Ferrer-i-Cancho",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "abstract": "A commentary on the article \"Large-scale evidence of dependency length minimization in 37 languages\" by Futrell, Mahowald & Gibson (PNAS 2015 112 (33) 10336-10341).\n    ",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2016-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03488",
        "title": "Verbs Taking Clausal and Non-Finite Arguments as Signals of Modality - Revisiting the Issue of Meaning Grounded in Syntax",
        "authors": [
            "Judith Eckle-Kohler"
        ],
        "abstract": "We revisit Levin's theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments. Grasping the meaning components of Levin-classes is known to be hard. We address this challenge by setting up a multi-perspective semantic characterization of the inferred classes. To this end, we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet, VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification (available at ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03611",
        "title": "A Parallel Corpus of Translationese",
        "authors": [
            "Ella Rabinovich",
            "Shuly Wintner",
            "Ofek Luis Lewinsohn"
        ],
        "abstract": "We describe a set of bilingual English--French and English--German parallel corpora in which the direction of translation is accurately and reliably annotated. The corpora are diverse, consisting of parliamentary proceedings, literary works, transcriptions of TED talks and political commentary. They will be instrumental for research of translationese and its applications to (human and machine) translation; specifically, they can be used for the task of translationese identification, a research direction that enjoys a growing interest in recent years. To validate the quality and reliability of the corpora, we replicated previous results of supervised and unsupervised identification of translationese, and further extended the experiments to additional datasets and languages.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2016-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03739",
        "title": "Improving distant supervision using inference learning",
        "authors": [
            "Roland Roller",
            "Eneko Agirre",
            "Aitor Soroa",
            "Mark Stevenson"
        ],
        "abstract": "Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data.\n    ",
        "submission_date": "2015-09-12T00:00:00",
        "last_modified_date": "2015-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03870",
        "title": "The USFD Spoken Language Translation System for IWSLT 2014",
        "authors": [
            "Raymond W. M. Ng",
            "Mortaza Doulaty",
            "Rama Doddipatla",
            "Wilker Aziz",
            "Kashif Shah",
            "Oscar Saz",
            "Madina Hasan",
            "Ghada AlHarbi",
            "Lucia Specia",
            "Thomas Hain"
        ],
        "abstract": "The University of Sheffield (USFD) participated in the International Workshop for Spoken Language Translation (IWSLT) in 2014. In this paper, we will introduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) is achieved by two multi-pass deep neural network systems with adaptation and rescoring techniques. Machine translation (MT) is achieved by a phrase-based system. The USFD primary system incorporates state-of-the-art ASR and MT techniques and gives a BLEU score of 23.45 and 14.75 on the English-to-French and English-to-German speech-to-text translation task with the IWSLT 2014 data. The USFD contrastive systems explore the integration of ASR and MT by using a quality estimation system to rescore the ASR outputs, optimising towards better translation. This gives a further 0.54 and 0.26 BLEU improvement respectively on the IWSLT 2012 and 2014 evaluation data.\n    ",
        "submission_date": "2015-09-13T00:00:00",
        "last_modified_date": "2015-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04219",
        "title": "Twitter Sentiment Analysis",
        "authors": [
            "Afroze Ibrahim Baqapuri"
        ],
        "abstract": "This project addresses the problem of sentiment analysis in twitter; that is classifying tweets according to the sentiment expressed in them: positive, negative or neutral. Twitter is an online micro-blogging and social-networking platform which allows users to write short status updates of maximum length 140 characters. It is a rapidly expanding service with over 200 million registered users - out of which 100 million are active users and half of them log on twitter on a daily basis - generating nearly 250 million tweets per day. Due to this large amount of usage we hope to achieve a reflection of public sentiment by analysing the sentiments expressed in the tweets. Analysing the public sentiment is important for many applications such as firms trying to find out the response of their products in the market, predicting political elections and predicting socioeconomic phenomena like stock exchange. The aim of this project is to develop a functional classifier for accurate and automatic sentiment classification of an unknown tweet stream.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04385",
        "title": "Kannada named entity recognition and classification (nerc) based on multinomial na\u00efve bayes (mnb) classifier",
        "authors": [
            "S. Amarappa",
            "S. V. Sathyanarayana"
        ],
        "abstract": "Named Entity Recognition and Classification (NERC) is a process of identification of proper nouns in the text and classification of those nouns into certain predefined categories like person name, location, organization, date, and time etc. NERC in Kannada is an essential and challenging task. The aim of this work is to develop a novel model for NERC, based on Multinomial Na\u00efve Bayes (MNB) Classifier. The Methodology adopted in this paper is based on feature extraction of training corpus, by using term frequency, inverse document frequency and fitting them to a tf-idf-vectorizer. The paper discusses the various issues in developing the proposed model. The details of implementation and performance evaluation are discussed. The experiments are conducted on a training corpus of size 95,170 tokens and test corpus of 5,000 tokens. It is observed that the model works with Precision, Recall and F1-measure of 83%, 79% and 81% respectively.\n    ",
        "submission_date": "2015-09-12T00:00:00",
        "last_modified_date": "2015-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04393",
        "title": "Dependency length minimization: Puzzles and Promises",
        "authors": [
            "Haitao Liu",
            "Chunshan Xu",
            "Junying Liang"
        ],
        "abstract": "In the recent issue of PNAS, Futrell et al. claims that their study of 37 languages gives the first large scale cross-language evidence for Dependency Length Minimization, which is an overstatement that ignores similar previous researches. In addition,this study seems to pay no attention to factors like the uniformity of genres,which weakens the validity of the argument that DLM is universal. Another problem is that this study sets the baseline random language as projective, which fails to truly uncover the difference between natural language and random language, since projectivity is an important feature of many natural languages. Finally, the paper contends an \"apparent relationship between head finality and dependency length\" despite the lack of an explicit statistical comparison, which renders this conclusion rather hasty and improper.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04473",
        "title": "Splitting Compounds by Semantic Analogy",
        "authors": [
            "Joachim Daiber",
            "Lautaro Quiroz",
            "Roger Wechsler",
            "Stella Frank"
        ],
        "abstract": "Compounding is a highly productive word-formation process in some languages that is often problematic for natural language processing applications. In this paper, we investigate whether distributional semantics in the form of word embeddings can enable a deeper, i.e., more knowledge-rich, processing of compounds than the standard string-based methods. We present an unsupervised approach that exploits regularities in the semantic vector space (based on analogies such as \"bookshop is to shop as bookshelf is to shelf\") to produce compound analyses of high quality. A subsequent compound splitting algorithm based on these analyses is highly effective, particularly for ambiguous compounds. German to English machine translation experiments show that this semantic analogy-based compound splitter leads to better translations than a commonly used frequency-based method.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04811",
        "title": "amLite: Amharic Transliteration Using Key Map Dictionary",
        "authors": [
            "Tadele Tedla"
        ],
        "abstract": "amLite is a framework developed to map ASCII transliterated Amharic texts back to the original Amharic letter texts. The aim of such a framework is to make existing Amharic linguistic data consistent and interoperable among researchers. For achieving the objective, a key map dictionary is constructed using the possible ASCII combinations actively in use for transliterating Amharic letters; and a mapping of the combinations to the corresponding Amharic letters is done. The mapping is then used to replace the Amharic linguistic text back to form the original Amharic letters text. The framework indicated 97.7, 99.7 and 98.4 percentage accuracy on converting the three sample random test data. It is; however, possible to improve the accuracy of the framework by adding an exception to the implementation of the algorithm, or by preprocessing the input text prior to conversion. This paper outlined the rationales behind the need for developing the framework and the processes undertaken in the development.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05209",
        "title": "Extraction of evidence tables from abstracts of randomized clinical trials using a maximum entropy classifier and global constraints",
        "authors": [
            "Antonio Trenta",
            "Anthony Hunter",
            "Sebastian Riedel"
        ],
        "abstract": "Systematic use of the published results of randomized clinical trials is increasingly important in evidence-based medicine. In order to collate and analyze the results from potentially numerous trials, evidence tables are used to represent trials concerning a set of interventions of interest. An evidence table has columns for the patient group, for each of the interventions being compared, for the criterion for the comparison (e.g. proportion who survived after 5 years from treatment), and for each of the results. Currently, it is a labour-intensive activity to read each published paper and extract the information for each field in an evidence table. There have been some NLP studies investigating how some of the features from papers can be extracted, or at least the relevant sentences identified. However, there is a lack of an NLP system for the systematic extraction of each item of information required for an evidence table. We address this need by a combination of a maximum entropy classifier, and integer linear programming. We use the later to handle constraints on what is an acceptable classification of the features to be extracted. With experimental results, we demonstrate substantial advantages in using global constraints (such as the features describing the patient group, and the interventions, must occur before the features describing the results of the comparison).\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05281",
        "title": "Network analysis of named entity co-occurrences in written texts",
        "authors": [
            "Diego R. Amancio"
        ],
        "abstract": "The use of methods borrowed from statistics and physics to analyze written texts has allowed the discovery of unprecedent patterns of human behavior and cognition by establishing links between models features and language structure. While current models have been useful to unveil patterns via analysis of syntactical and semantical networks, only a few works have probed the relevance of investigating the structure arising from the relationship between relevant entities such as characters, locations and organizations. In this study, we represent entities appearing in the same context as a co-occurrence network, where links are established according to a null model based on random, shuffled texts. Computational simulations performed in novels revealed that the proposed model displays interesting topological features, such as the small world feature, characterized by high values of clustering coefficient. The effectiveness of our model was verified in a practical pattern recognition task in real networks. When compared with traditional word adjacency networks, our model displayed optimized results in identifying unknown references in texts. Because the proposed representation plays a complementary role in characterizing unstructured documents via topological analysis of named entities, we believe that it could be useful to improve the characterization of written texts (and related systems), specially if combined with traditional approaches based on statistical and deeper paradigms.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2016-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05488",
        "title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding",
        "authors": [
            "Han Xiao",
            "Minlie Huang",
            "Yu Hao",
            "Xiaoyan Zhu"
        ],
        "abstract": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, TransG. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05490",
        "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding",
        "authors": [
            "Han Xiao",
            "Minlie Huang",
            "Yu Hao",
            "Xiaoyan Zhu"
        ],
        "abstract": "Knowledge representation is a major topic in AI, and many studies attempt to represent entities and relations of knowledge base in a continuous vector space. Among these attempts, translation-based methods build entity and relation vectors by minimizing the translation loss from a head entity to a tail one. In spite of the success of these methods, translation-based methods also suffer from the oversimplified loss metric, and are not competitive enough to model various and complex entities/relations in knowledge bases. To address this issue, we propose \\textbf{TransA}, an adaptive metric approach for embedding, utilizing the metric learning ideas to provide a more flexible embedding method. Experiments are conducted on the benchmark datasets and our proposed method makes significant and consistent improvements over the state-of-the-art baselines.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05517",
        "title": "A Light Sliding-Window Part-of-Speech Tagger for the Apertium Free/Open-Source Machine Translation Platform",
        "authors": [
            "Gang Chen",
            "Mikel L. Forcada"
        ],
        "abstract": "This paper describes a free/open-source implementation of the light sliding-window (LSW) part-of-speech tagger for the Apertium free/open-source machine translation platform. Firstly, the mechanism and training process of the tagger are reviewed, and a new method for incorporating linguistic rules is proposed. Secondly, experiments are conducted to compare the performances of the tagger under different window settings, with or without Apertium-style \"forbid\" rules, with or without Constraint Grammar, and also with respect to the traditional HMM tagger in Apertium.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05808",
        "title": "Word, graph and manifold embedding from Markov processes",
        "authors": [
            "Tatsunori B. Hashimoto",
            "David Alvarez-Melis",
            "Tommi S. Jaakkola"
        ],
        "abstract": "Continuous vector representations of words and objects appear to carry surprisingly rich semantic content. In this paper, we advance both the conceptual and theoretical understanding of word embeddings in three ways. First, we ground embeddings in semantic spaces studied in cognitive-psychometric literature and introduce new evaluation tasks. Second, in contrast to prior work, we take metric recovery as the key object of study, unify existing algorithms as consistent metric recovery methods based on co-occurrence counts from simple Markov random walks, and propose a new recovery algorithm. Third, we generalize metric recovery to graphs and manifolds, relating co-occurence counts on random walks in graphs and random processes on manifolds to the underlying metric to be recovered, thereby reconciling manifold estimation and embedding algorithms. We compare embedding algorithms across a range of tasks, from nonlinear dimensionality reduction to three semantic language tasks, including analogies, sequence completion, and classification.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06053",
        "title": "Early text classification: a Naive solution",
        "authors": [
            "Hugo Jair Escalante",
            "Manuel Montes-y-G\u00f3mez",
            "Luis Villase\u00f1or-Pineda",
            "Marcelo Luis Errecalde"
        ],
        "abstract": "Text classification is a widely studied problem, and it can be considered solved for some domains and under certain circumstances. There are scenarios, however, that have received little or no attention at all, despite its relevance and applicability. One of such scenarios is early text classification, where one needs to know the category of a document by using partial information only. A document is processed as a sequence of terms, and the goal is to devise a method that can make predictions as fast as possible. The importance of this variant of the text classification problem is evident in domains like sexual predator detection, where one wants to identify an offender as early as possible. This paper analyzes the suitability of the standard naive Bayes classifier for approaching this problem. Specifically, we assess its performance when classifying documents after seeing an increasingly number of terms. A simple modification to the standard naive Bayes implementation allows us to make predictions with partial information. To the best of our knowledge naive Bayes has not been used for this purpose before. Throughout an extensive experimental evaluation we show the effectiveness of the classifier for early text classification. What is more, we show that this simple solution is very competitive when compared with state of the art methodologies that are more elaborated. We foresee our work will pave the way for the development of more effective early text classification techniques based in the naive Bayes formulation.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06585",
        "title": "A Review of Features for the Discrimination of Twitter Users: Application to the Prediction of Offline Influence",
        "authors": [
            "Jean-Val\u00e8re Cossu",
            "Vincent Labatut",
            "Nicolas Dugu\u00e9"
        ],
        "abstract": "Many works related to Twitter aim at characterizing its users in some way: role on the service (spammers, bots, organizations, etc.), nature of the user (socio-professional category, age, etc.), topics of interest , and others. However, for a given user classification problem, it is very difficult to select a set of appropriate features, because the many features described in the literature are very heterogeneous, with name overlaps and collisions, and numerous very close variants. In this article, we review a wide range of such features. In order to present a clear state-of-the-art description, we unify their names, definitions and relationships, and we propose a new, neutral, typology. We then illustrate the interest of our review by applying a selection of these features to the offline influence detection problem. This task consists in identifying users which are influential in real-life, based on their Twitter account and related data. We show that most features deemed efficient to predict online influence, such as the numbers of retweets and followers, are not relevant to this problem. However, We propose several content-based approaches to label Twitter users as Influencers or not. We also rank them according to a predicted influence level. Our proposals are evaluated over the CLEF RepLab 2014 dataset, and outmatch state-of-the-art methods.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06664",
        "title": "Reasoning about Entailment with Neural Attention",
        "authors": [
            "Tim Rockt\u00e4schel",
            "Edward Grefenstette",
            "Karl Moritz Hermann",
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Phil Blunsom"
        ],
        "abstract": "While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06928",
        "title": "Automatic Dialect Detection in Arabic Broadcast Speech",
        "authors": [
            "Ahmed Ali",
            "Najim Dehak",
            "Patrick Cardinal",
            "Sameer Khurana",
            "Sree Harsha Yella",
            "James Glass",
            "Peter Bell",
            "Steve Renals"
        ],
        "abstract": "We investigate different approaches for dialect identification in Arabic broadcast speech, using phonetic, lexical features obtained from a speech recognition system, and acoustic features using the i-vector framework. We studied both generative and discriminate classifiers, and we combined these features using a multi-class Support Vector Machine (SVM). We validated our results on an Arabic/English language identification task, with an accuracy of 100%. We used these features in a binary classifier to discriminate between Modern Standard Arabic (MSA) and Dialectal Arabic, with an accuracy of 100%. We further report results using the proposed method to discriminate between the five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North African, and MSA, with an accuracy of 52%. We discuss dialect identification errors in the context of dialect code-switching between Dialectal Arabic and MSA, and compare the error pattern between manually labeled data, and the output from our classifier. We also release the train and test data as standard corpus for dialect identification.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2016-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06937",
        "title": "Fully automatic multi-language translation with a catalogue of phrases - successful employment for the Swiss avalanche bulletin",
        "authors": [
            "Kurt Winkler",
            "Tobias Kuhn"
        ],
        "abstract": "The Swiss avalanche bulletin is produced twice a day in four languages. Due to the lack of time available for manual translation, a fully automated translation system is employed, based on a catalogue of predefined phrases and predetermined rules of how these phrases can be combined to produce sentences. Because this catalogue of phrases is limited to a small sublanguage, the system is able to automatically translate such sentences from German into the target languages French, Italian and English without subsequent proofreading or correction. Having been operational for two winter seasons, we assess here the quality of the produced texts based on two different surveys where participants rated texts from real avalanche bulletins from both origins, the catalogue of phrases versus manually written and translated texts. With a mean recognition rate of 55%, users can hardly distinguish between thetwo types of texts, and give very similar ratings with respect to their language quality. Overall, the output from the catalogue system can be considered virtually equivalent to a text written by avalanche forecasters and then manually translated by professional translators. Furthermore, forecasters declared that all relevant situations were captured by the system with sufficient accuracy. Forecaster's working load did not change with the introduction of the catalogue: the extra time to find matching sentences is compensated by the fact that they no longer need to double-check manually translated texts. The reduction of daily translation costs is expected to offset the initial development costs within a few years.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07175",
        "title": "Exploration and Exploitation of Victorian Science in Darwin's Reading Notebooks",
        "authors": [
            "Jaimie Murdock",
            "Colin Allen",
            "Simon DeDeo"
        ],
        "abstract": "Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. To study this decision-making process, we examine the reading choices made by one of the most celebrated scientists of the modern era: Charles Darwin. From the full-text of books listed in his chronologically-organized reading journals, we generate topic models to quantify his local (text-to-text) and global (text-to-past) reading decisions using Kullback-Liebler Divergence, a cognitively-validated, information-theoretic measure of relative surprise. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by qualitative scholarship and Darwin's own self-commentary. Our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. Our quantitative methods advance the study of cognitive search through a framework for testing interactions between individual and collective behavior and between short- and long-term consumption choices. This novel application of topic modeling to characterize individual reading complements widespread studies of collective scientific behavior.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07308",
        "title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data",
        "authors": [
            "Ivan Vuli\u0107",
            "Marie-Francine Moens"
        ],
        "abstract": "We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.\n    ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2016-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07513",
        "title": "Description of the Odin Event Extraction Framework and Rule Language",
        "authors": [
            "Marco A. Valenzuela-Esc\u00e1rcega",
            "Gus Hahn-Powell",
            "Mihai Surdeanu"
        ],
        "abstract": "This document describes the Odin framework, which is a domain-independent platform for developing rule-based event extraction models. Odin aims to be powerful (the rule language allows the modeling of complex syntactic structures) and robust (to recover from syntactic parsing errors, syntactic patterns can be freely mixed with surface, token-based patterns), while remaining simple (some domain grammars can be up and running in minutes), and fast (Odin processes over 100 sentences/second in a real-world domain with over 200 rules). Here we include a thorough definition of the Odin rule language, together with a description of the Odin API in the Scala language, which allows one to apply these rules to arbitrary texts.\n    ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07612",
        "title": "Sentiment Uncertainty and Spam in Twitter Streams and Its Implications for General Purpose Realtime Sentiment Analysis",
        "authors": [
            "Nils Haldenwang",
            "Oliver Vornberger"
        ],
        "abstract": "State of the art benchmarks for Twitter Sentiment Analysis do not consider the fact that for more than half of the tweets from the public stream a distinct sentiment cannot be chosen. This paper provides a new perspective on Twitter Sentiment Analysis by highlighting the necessity of explicitly incorporating uncertainty. Moreover, a dataset of high quality to evaluate solutions for this new problem is introduced and made publicly available.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07761",
        "title": "Sentiment of Emojis",
        "authors": [
            "Petra Kralj Novak",
            "Jasmina Smailovi\u0107",
            "Borut Sluban",
            "Igor Mozeti\u010d"
        ],
        "abstract": "There is a new generation of emoticons, called emojis, that is increasingly being used in mobile communications and social media. In the past two years, over ten billion emojis were used on Twitter. Emojis are Unicode graphic symbols, used as a shorthand to express concepts and ideas. In contrast to the small number of well-known emoticons that carry clear emotional contents, there are hundreds of emojis. But what are their emotional contents? We provide the first emoji sentiment lexicon, called the Emoji Sentiment Ranking, and draw a sentiment map of the 751 most frequently used emojis. The sentiment of the emojis is computed from the sentiment of the tweets in which they occur. We engaged 83 human annotators to label over 1.6 million tweets in 13 European languages by the sentiment polarity (negative, neutral, or positive). About 4% of the annotated tweets contain emojis. The sentiment analysis of the emojis allows us to draw several interesting conclusions. It turns out that most of the emojis are positive, especially the most popular ones. The sentiment distribution of the tweets with and without emojis is significantly different. The inter-annotator agreement on the tweets with emojis is higher. Emojis tend to occur at the end of the tweets, and their sentiment polarity increases with the distance. We observe no significant differences in the emoji rankings between the 13 languages and the Emoji Sentiment Ranking. Consequently, we propose our Emoji Sentiment Ranking as a European language-independent resource for automated sentiment analysis. Finally, the paper provides a formalization of sentiment and a novel visualization in the form of a sentiment bar.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08639",
        "title": "Tuned and GPU-accelerated parallel data mining from comparable corpora",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "The multilingual nature of the world makes translation a crucial requirement today. Parallel dictionaries constructed by humans are a widely-available resource, but they are limited and do not provide enough coverage for good quality translation purposes, due to out-of-vocabulary words and neologisms. This motivates the use of statistical translation systems, which are unfortunately dependent on the quantity and quality of training data. Such has a very limited availability especially for some languages and very narrow text domains. Is this research we present our improvements to Yalign mining methodology by reimplementing the comparison algorithm, introducing a tuning scripts and by improving performance using GPU computing acceleration. The experiments are conducted on various text domains and bi-data is extracted from the Wikipedia dumps.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08644",
        "title": "Neural-based machine translation for medical text domain. Based on European Medicines Agency leaflet texts",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "The quality of machine translation is rapidly evolving. Today one can find several machine translation systems on the web that provide reasonable translations, although the systems are not perfect. In some specific domains, the quality may decrease. A recently proposed approach to this domain is neural machine translation. It aims at building a jointly-tuned single neural network that maximizes translation performance, a very different approach from traditional statistical machine translation. Recently proposed neural machine translation models often belong to the encoder-decoder family in which a source sentence is encoded into a fixed length vector that is, in turn, decoded to generate a translation. The present research examines the effects of different training methods on a Polish-English Machine Translation system used for medical data. The European Medicines Agency parallel text corpus was used as the basis for training of neural and statistical network-based translation systems. The main machine translation evaluation metrics have also been used in analysis of the systems. A comparison and implementation of a real-time medical translator is the main focus of our experiments.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08842",
        "title": "Automatically Segmenting Oral History Transcripts",
        "authors": [
            "Ryan Shaw"
        ],
        "abstract": "Dividing oral histories into topically coherent segments can make them more accessible online. People regularly make judgments about where coherent segments can be extracted from oral histories. But making these judgments can be taxing, so automated assistance is potentially attractive to speed the task of extracting segments from open-ended interviews. When different people are asked to extract coherent segments from the same oral histories, they often do not agree about precisely where such segments begin and end. This low agreement makes the evaluation of algorithmic segmenters challenging, but there is reason to believe that for segmenting oral history transcripts, some approaches are more promising than others. The BayesSeg algorithm performs slightly better than TextTiling, while TextTiling does not perform significantly better than a uniform segmentation. BayesSeg might be used to suggest boundaries to someone segmenting oral histories, but this segmentation task needs to be better defined.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08874",
        "title": "Polish - English Speech Statistical Machine Translation Systems for the IWSLT 2014",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "This research explores effects of various training settings between Polish and English Statistical Machine Translation systems for spoken language. Various elements of the TED parallel text corpora for the IWSLT 2014 evaluation campaign were used as the basis for training of language models, and for development, tuning and testing of the translation system as well as Wikipedia based comparable corpora prepared by us. The BLEU, NIST, METEOR and TER metrics were used to evaluate the effects of data preparations on translation results. Our experiments included systems, which use lemma and morphological information on Polish words. We also conducted a deep analysis of provided Polish data as preparatory work for the automatic data correction and cleaning phase.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08881",
        "title": "Building Subject-aligned Comparable Corpora and Mining it for Truly Parallel Sentence Pairs",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "Parallel sentences are a relatively scarce but extremely useful resource for many applications including cross-lingual retrieval and statistical machine translation. This research explores our methodology for mining such data from previously obtained comparable corpora. The task is highly practical since non-parallel multilingual data exist in far greater quantities than parallel corpora, but parallel sentences are a much more useful resource. Here we propose a web crawling method for building subject-aligned comparable corpora from Wikipedia articles. We also introduce a method for extracting truly parallel sentences that are filtered out from noisy or just comparable sentence pairs. We describe our implementation of a specialized tool for this task as well as training and adaption of a machine translation system that supplies our filter with additional information about the similarity of comparable sentence pairs.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08909",
        "title": "Polish -English Statistical Machine Translation of Medical Texts",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "This new research explores the effects of various training methods on a Polish to English Statistical Machine Translation system for medical texts. Various elements of the EMEA parallel text corpora from the OPUS project were used as the basis for training of phrase tables and language models and for development, tuning and testing of the translation system. The BLEU, NIST, METEOR, RIBES and TER metrics have been used to evaluate the effects of various system and data preparations on translation results. Our experiments included systems that used POS tagging, factored phrase models, hierarchical models, syntactic taggers, and many different alignment methods. We also conducted a deep analysis of Polish data as preparatory work for automatic data correction such as true casing and punctuation normalization phase.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08967",
        "title": "Very Deep Multilingual Convolutional Neural Networks for LVCSR",
        "authors": [
            "Tom Sercu",
            "Christian Puhrsch",
            "Brian Kingsbury",
            "Yann LeCun"
        ],
        "abstract": "Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2016-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09088",
        "title": "Enhanced Bilingual Evaluation Understudy",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "Our research extends the Bilingual Evaluation Understudy (BLEU) evaluation technique for statistical machine translation to make it more adjustable and robust. We intend to adapt it to resemble human evaluation more. We perform experiments to evaluate the performance of our technique against the primary existing evaluation methods. We describe and show the improvements it makes over existing methods as well as correlation to them. When human translators translate a text, they often use synonyms, different word orders or style, and other similar variations. We propose an SMT evaluation technique that enhances the BLEU metric to consider variations such as those.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09090",
        "title": "Real-Time Statistical Speech Translation",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "This research investigates the Statistical Machine Translation approaches to translate speech in real time automatically. Such systems can be used in a pipeline with speech recognition and synthesis software in order to produce a real-time voice communication system between foreigners. We obtained three main data sets from spoken proceedings that represent three different types of human speech. TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for developmental tuning and testing of the translation system. We also conducted experiments involving part of speech tagging, compound splitting, linear language model interpolation, TrueCasing and morphosyntactic analysis. We evaluated the effects of variety of data preparations on the translation results using the BLEU, NIST, METEOR and TER metrics and tried to give answer which metric is most suitable for PL-EN language pair.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09093",
        "title": "A Sentence Meaning Based Alignment Method for Parallel Text Corpora Preparation",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "Text alignment is crucial to the accuracy of Machine Translation (MT) systems, some NLP tools or any other text processing tasks requiring bilingual data. This research proposes a language independent sentence alignment approach based on Polish (not position-sensitive language) to English experiments. This alignment approach was developed on the TED Talks corpus, but can be used for any text domain or language pair. The proposed approach implements various heuristics for sentence recognition. Some of them value synonyms and semantic text structure analysis as a part of additional information. Minimization of data loss was ensured. The solution is compared to other sentence alignment implementations. Also an improvement in MT system score with text processed with described tool is shown.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09097",
        "title": "Polish - English Speech Statistical Machine Translation Systems for the IWSLT 2013",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "This research explores the effects of various training settings from Polish to English Statistical Machine Translation system for spoken language. Various elements of the TED parallel text corpora for the IWSLT 2013 evaluation campaign were used as the basis for training of language models, and for development, tuning and testing of the translation system. The BLEU, NIST, METEOR and TER metrics were used to evaluate the effects of data preparations on translation results. Our experiments included systems, which use stems and morphological information on Polish words. We also conducted a deep analysis of provided Polish data as preparatory work for the automatic data correction and cleaning phase.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09121",
        "title": "The \"handedness\" of language: Directional symmetry breaking of sign usage in words",
        "authors": [
            "Md Izhar Ashraf",
            "Sitabhra Sinha"
        ],
        "abstract": "Language, which allows complex ideas to be communicated through symbolic sequences, is a characteristic feature of our species and manifested in a multitude of forms. Using large written corpora for many different languages and scripts, we show that the occurrence probability distributions of signs at the left and right ends of words have a distinct heterogeneous nature. Characterizing this asymmetry using quantitative inequality measures, viz. information entropy and the Gini index, we show that the beginning of a word is less restrictive in sign usage than the end. This property is not simply attributable to the use of common affixes as it is seen even when only word roots are considered. We use the existence of this asymmetry to infer the direction of writing in undeciphered inscriptions that agrees with the archaeological evidence. Unlike traditional investigations of phonotactic constraints which focus on language-specific patterns, our study reveals a property valid across languages and writing systems. As both language and writing are unique aspects of our species, this universal signature may reflect an innate feature of the human cognitive phenomenon.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2018-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00001",
        "title": "Polish to English Statistical Machine Translation",
        "authors": [
            "Krzysztof Wo\u0142k"
        ],
        "abstract": "This research explores the effects of various training settings on a Polish to English Statistical Machine Translation system for spoken language. Various elements of the TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for development, tuning and testing of the translation system. The BLEU, NIST, METEOR and TER metrics were used to evaluate the effects of the data preparations on the translation results.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00240",
        "title": "Determination of the Internet Anonymity Influence on the Level of Aggression and Usage of Obscene Lexis",
        "authors": [
            "Rodmonga Potapova",
            "Denis Gordeev"
        ],
        "abstract": "This article deals with the analysis of the semantic content of the anonymous Russian-speaking forum ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00259",
        "title": "A Generative Model of Words and Relationships from Multiple Sources",
        "authors": [
            "Stephanie L. Hyland",
            "Theofanis Karaletsos",
            "Gunnar R\u00e4tsch"
        ],
        "abstract": "Neural language models are a powerful tool to embed words into semantic vector spaces. However, learning such models generally relies on the availability of abundant and diverse training examples. In highly specialised domains this requirement may not be met due to difficulties in obtaining a large corpus, or the limited range of expression in average use. Such domains may encode prior knowledge about entities in a knowledge base or ontology. We propose a generative model which integrates evidence from diverse data sources, enabling the sharing of semantic information. We achieve this by generalising the concept of co-occurrence from distributional semantics to include other relationships between entities or words, which we model as affine transformations on the embedding space. We demonstrate the effectiveness of this approach by outperforming recent models on a link prediction task and demonstrating its ability to profit from partially or fully unobserved data training labels. We further demonstrate the usefulness of learning from different data sources with overlapping vocabularies.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00436",
        "title": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and G\u00f3mez-Rodr\u00edguez (2015) on Dependency Length Minimization",
        "authors": [
            "Richard Futrell",
            "Kyle Mahowald",
            "Edward Gibson"
        ],
        "abstract": "We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and G\u00f3mez-Rodr\u00edguez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015). First, we acknowledge error in failing to acknowledge Liu (2008)'s previous work on corpora of 20 languages with similar aims. A correction will appear in PNAS. Nevertheless, we argue that our work provides novel, strong evidence for dependency length minimization as a universal quantitative property of languages, beyond this previous work, because it provides baselines which focus on word order preferences. Second, we argue that our choices of baselines were appropriate because they control for alternative theories.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00618",
        "title": "Automatic Taxonomy Extraction from Query Logs with no Additional Sources of Information",
        "authors": [
            "Miguel Fernandez-Fernandez",
            "Daniel Gayo-Avello"
        ],
        "abstract": "Search engine logs store detailed information on Web users interactions. Thus, as more and more people use search engines on a daily basis, important trails of users common knowledge are being recorded in those files. Previous research has shown that it is possible to extract concept taxonomies from full text documents, while other scholars have proposed methods to obtain similar queries from query logs. We propose a mixture of both lines of research, that is, mining query logs not to find related queries nor query hierarchies, but actual term taxonomies that could be used to improve search engine effectiveness and efficiency. As a result, in this study we have developed a method that combines lexical heuristics with a supervised classification model to successfully extract hyponymy relations from specialization search patterns revealed from log missions, with no additional sources of information, and in a language independent way.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00726",
        "title": "A Primer on Neural Network Models for Natural Language Processing",
        "authors": [
            "Yoav Goldberg"
        ],
        "abstract": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00759",
        "title": "It is not all downhill from here: Syllable Contact Law in Persian",
        "authors": [
            "Afshin Rahimi",
            "Moharram Eslami",
            "Bahram Vazirnezhad"
        ],
        "abstract": "Syllable contact pairs crosslinguistically tend to have a falling sonority slope a constraint which is called the Syllable Contact Law SCL In this study the phonotactics of syllable contacts in 4202 CVCCVC words of Persian lexicon is investigated The consonants of Persian were divided into five sonority categories and the frequency of all possible sonority slopes is computed both in lexicon type frequency and in corpus token frequency Since an unmarked phonological structure has been shown to diachronically become more frequent we expect to see the same pattern for syllable contact pairs with falling sonority slope The correlation of sonority categories of the two consonants in a syllable contact pair is measured using Pointwise Mutual Information\n    ",
        "submission_date": "2015-10-03T00:00:00",
        "last_modified_date": "2015-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00760",
        "title": "P-trac Procedure: The Dispersion and Neutralization of Contrasts in Lexicon",
        "authors": [
            "Afshin Rahimi",
            "Bahram Vazirnezhad",
            "Moharram Eslami"
        ],
        "abstract": "Cognitive acoustic cues have an important role in shaping the phonological structure of language as a means to optimal communication. In this paper we introduced P-trac procedure in order to track dispersion of contrasts in different contexts in lexicon. The results of applying P-trac procedure to the case of dispersion of contrasts in pre- consonantal contexts and in consonantal positions of CVCC sequences in Persian provide Evidence in favor of phonetic basis of dispersion argued by Licensing by Cue hypothesis and the Dispersion Theory of Contrast. The P- trac procedure is proved to be very effective in revealing the dispersion of contrasts in lexicon especially when comparing the dispersion of contrasts in different contexts.\n    ",
        "submission_date": "2015-10-03T00:00:00",
        "last_modified_date": "2015-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01032",
        "title": "Deep convolutional acoustic word embeddings using word-pair side information",
        "authors": [
            "Herman Kamper",
            "Weiran Wang",
            "Karen Livescu"
        ],
        "abstract": "Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01315",
        "title": "Stochastic model for phonemes uncovers an author-dependency of their usage",
        "authors": [
            "Weibing Deng",
            "Armen E. Allahverdyan"
        ],
        "abstract": "We study rank-frequency relations for phonemes, the minimal units that still relate to linguistic meaning. We show that these relations can be described by the Dirichlet distribution, a direct analogue of the ideal-gas model in statistical mechanics. This description allows us to demonstrate that the rank-frequency relations for phonemes of a text do depend on its author. The author-dependency effect is not caused by the author's vocabulary (common words used in different texts), and is confirmed by several alternative means. This suggests that it can be directly related to phonemes. These features contrast to rank-frequency relations for words, which are both author and text independent and are governed by the Zipf's law.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2016-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01570",
        "title": "Analyzer and generator for Pali",
        "authors": [
            "David Alfter"
        ],
        "abstract": "This work describes a system that performs morphological analysis and generation of Pali words. The system works with regular inflectional paradigms and a lexical database. The generator is used to build a collection of inflected and derived words, which in turn is used by the analyzer. Generating and storing morphological forms along with the corresponding morphological information allows for efficient and simple look up by the analyzer. Indeed, by looking up a word and extracting the attached morphological information, the analyzer does not have to compute this information. As we must, however, assume the lexical database to be incomplete, the system can also work without the dictionary component, using a rule-based approach.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01717",
        "title": "Language Segmentation",
        "authors": [
            "David Alfter"
        ],
        "abstract": "Language segmentation consists in finding the boundaries where one language ends and another language begins in a text written in more than one language. This is important for all natural language processing tasks. The problem can be solved by training language models on language data. However, in the case of low- or no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform better than supervised methods when it is difficult or impossible to train supervised approaches. A special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language. I compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words. The weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. The results look promising, but there is room for improvement and a more thorough investigation should be undertaken.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01886",
        "title": "Using Ontology-Based Context in the Portuguese-English Translation of Homographs in Textual Dialogues",
        "authors": [
            "Diego Moussallem",
            "Ricardo Choren"
        ],
        "abstract": "This paper introduces a novel approach to tackle the existing gap on message translations in dialogue systems. Currently, submitted messages to the dialogue systems are considered as isolated sentences. Thus, missing context information impede the disambiguation of homographs words in ambiguous sentences. Our approach solves this disambiguation problem by using concepts over existing ontologies.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01949",
        "title": "Hierarchical Representation of Prosody for Statistical Speech Synthesis",
        "authors": [
            "Antti Suni",
            "Daniel Aalto",
            "Martti Vainio"
        ],
        "abstract": "Prominences and boundaries are the essential constituents of prosodic structure in speech. They provide for means to chunk the speech stream into linguistically relevant units by providing them with relative saliences and demarcating them within coherent utterance structures. Prominences and boundaries have both been widely used in both basic research on prosody as well as in text-to-speech synthesis. However, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. Here we present an unsupervised unified account for estimating and representing prosodic prominences and boundaries using a scale-space analysis based on continuous wavelet transform. The methods are evaluated and compared to earlier work using the Boston University Radio News corpus. The results show that the proposed method is comparable with the best published supervised annotation methods.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02049",
        "title": "Assisting Composition of Email Responses: a Topic Prediction Approach",
        "authors": [
            "Spandana Gella",
            "Marc Dymetman",
            "Jean Michel Renders",
            "Sriram Venkatapathy"
        ],
        "abstract": "We propose an approach for helping agents compose email replies to customer requests. To enable that, we use LDA to extract latent topics from a collection of email exchanges. We then use these latent topics to label our data, obtaining a so-called \"silver standard\" topic labelling. We exploit this labelled set to train a classifier to: (i) predict the topic distribution of the entire agent's email response, based on features of the customer's email; and (ii) predict the topic distribution of the next sentence in the agent's reply, based on the customer's email features and on features of the agent's current sentence. The experimental results on a large email collection from a contact center in the tele- com domain show that the proposed ap- proach is effective in predicting the best topic of the agent's next sentence. In 80% of the cases, the correct topic is present among the top five recommended topics (out of fifty possible ones). This shows the potential of this method to be applied in an interactive setting, where the agent is presented a small list of likely topics to choose from for the next sentence.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02125",
        "title": "Resolving References to Objects in Photographs using the Words-As-Classifiers Model",
        "authors": [
            "David Schlangen",
            "Sina Zarriess",
            "Casey Kennington"
        ],
        "abstract": "A common use of language is to refer to visually present objects. Modelling it in computers requires modelling the link between language and perception. The \"words as classifiers\" model of grounded semantics views words as classifiers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words. It was recently shown to perform well in a game-playing scenario with a small number of object types. We apply it to two large sets of real-world photographs that contain a much larger variety of types and for which referring expressions are available. Using a pre-trained convolutional neural network to extract image features, and augmenting these with in-picture positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, find bounding box of its referent), while, as we argue, being conceptually simpler and more flexible.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02358",
        "title": "Automata networks for multi-party communication in the Naming Game",
        "authors": [
            "Javier Vera",
            "Pedro Montealegre",
            "Eric Goles"
        ],
        "abstract": "The Naming Game has been studied to explore the role of self-organization in the development and negotiation of linguistic conventions. In this paper, we define an automata networks approach to the Naming Game. Two problems are faced: (1) the definition of an automata networks for multi-party communicative interactions; and (2) the proof of convergence for three different orders in which the individuals are updated (updating schemes). Finally, computer simulations are explored in two-dimensional lattices with the purpose to recover the main features of the Naming Game and to describe the dynamics under different updating schemes.\n    ",
        "submission_date": "2015-10-08T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02387",
        "title": "Mapping Unseen Words to Task-Trained Embedding Spaces",
        "authors": [
            "Pranava Swaroop Madhyastha",
            "Mohit Bansal",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "abstract": "We consider the supervised training setting in which we learn task-specific word embeddings. We assume that we start with initial embeddings learned from unlabelled data and update them to learn task-specific embeddings for words in the supervised training data. However, for new words in the test set, we must use either their initial embeddings or a single unknown embedding, which often leads to errors. We address this by learning a neural network to map from initial embeddings to the task-specific embedding space, via a multi-loss objective function. The technique is general, but here we demonstrate its use for improved dependency parsing (especially for sentences with out-of-vocabulary words), as well as for downstream improvements on sentiment analysis.\n    ",
        "submission_date": "2015-10-08T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02675",
        "title": "Controlled Experiments for Word Embeddings",
        "authors": [
            "Benjamin J. Wilson",
            "Adriaan M. J. Schakel"
        ],
        "abstract": "An experimental approach to studying the properties of word embeddings is proposed. Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct relations between word properties and word vector direction and length. The approach is demonstrated using the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise. The experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word. The coefficients of linearity depend upon the word. The special point in feature space, defined by the (artificial) word with pure noise in its co-occurrence distribution, is found to be small but non-zero.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02823",
        "title": "Human languages order information efficiently",
        "authors": [
            "Daniel Gildea",
            "T. Florian Jaeger"
        ],
        "abstract": "Most languages use the relative order between words to encode meaning relations. Languages differ, however, in what orders they use and how these orders are mapped onto different meanings. We test the hypothesis that, despite these differences, human languages might constitute different `solutions' to common pressures of language use. Using Monte Carlo simulations over data from five languages, we find that their word orders are efficient for processing in terms of both dependency length and local lexical probability. This suggests that biases originating in how the brain understands language strongly constrain how human languages change over generations.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2015-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02983",
        "title": "OmniGraph: Rich Representation and Graph Kernel Learning",
        "authors": [
            "Boyi Xie",
            "Rebecca J. Passonneau"
        ],
        "abstract": "OmniGraph, a novel representation to support a range of NLP classification tasks, integrates lexical items, syntactic dependencies and frame semantic parses into graphs. Feature engineering is folded into the learning through convolution graph kernel learning to explore different extents of the graph. A high-dimensional space of features includes individual nodes as well as complex subgraphs. In experiments on a text-forecasting problem that predicts stock price change from news for company mentions, OmniGraph beats several benchmarks based on bag-of-words, syntactic dependencies, and semantic trees. The highly expressive features OmniGraph discovers provide insights into the semantics across distinct market sectors. To demonstrate the method's generality, we also report its high performance results on a fine-grained sentiment corpus.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03021",
        "title": "Textual Analysis for Studying Chinese Historical Documents and Literary Novels",
        "authors": [
            "Chao-Lin Liu",
            "Guan-Tao Jin",
            "Hongsu Wang",
            "Qing-Feng Liu",
            "Wen-Huei Cheng",
            "Wei-Yun Chiu",
            "Richard Tzong-Han Tsai",
            "Yu-Chun Wang"
        ],
        "abstract": "We analyzed historical and literary documents in Chinese to gain insights into research issues, and overview our studies which utilized four different sources of text materials in this paper. We investigated the history of concepts and transliterated words in China with the Database for the Study of Modern China Thought and Literature, which contains historical documents about China between 1830 and 1930. We also attempted to disambiguate names that were shared by multiple government officers who served between 618 and 1912 and were recorded in Chinese local gazetteers. To showcase the potentials and challenges of computer-assisted analysis of Chinese literatures, we explored some interesting yet non-trivial questions about two of the Four Great Classical Novels of China: (1) Which monsters attempted to consume the Buddhist monk Xuanzang in the Journey to the West (JTTW), which was published in the 16th century, (2) Which was the most powerful monster in JTTW, and (3) Which major role smiled the most in the Dream of the Red Chamber, which was published in the 18th century. Similar approaches can be applied to the analysis and study of modern documents, such as the newspaper articles published about the 228 incident that occurred in 1947 in Taiwan.\n    ",
        "submission_date": "2015-10-11T00:00:00",
        "last_modified_date": "2015-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03055",
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
        "authors": [
            "Jiwei Li",
            "Michel Galley",
            "Chris Brockett",
            "Jianfeng Gao",
            "Bill Dolan"
        ],
        "abstract": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., \"I don't know\") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.\n    ",
        "submission_date": "2015-10-11T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03421",
        "title": "Towards Meaningful Maps of Polish Case Law",
        "authors": [
            "Michal Jungiewicz",
            "Micha\u0142 \u0141opuszy\u0144ski"
        ],
        "abstract": "In this work, we analyze the utility of two dimensional document maps for exploratory analysis of Polish case law. We start by comparing two methods of generating such visualizations. First is based on linear principal component analysis (PCA). Second makes use of the modern nonlinear t-Distributed Stochastic Neighbor Embedding method (t-SNE). We apply both PCA and t-SNE to a corpus of judgments from different courts in Poland. It emerges that t-SNE provides better, more interpretable results than PCA. As a next test, we apply t-SNE to randomly selected sample of common court judgments corresponding to different keywords. We show that t-SNE, in this case, reveals hidden topical structure of the documents related to keyword,,pension\". In conclusion, we find that the t-SNE method could be a promising tool to facilitate the exploitative analysis of legal texts, e.g., by complementing search or browse functionality in legal databases.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03519",
        "title": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning",
        "authors": [
            "Janarthanan Rajendran",
            "Mitesh M. Khapra",
            "Sarath Chandar",
            "Balaraman Ravindran"
        ],
        "abstract": "Recently there has been a lot of interest in learning common representations for multiple views of data. Typically, such common representations are learned using a parallel corpus between the two views (say, 1M images and their English captions). In this work, we address a real-world scenario where no direct parallel data is available between two views of interest (say, $V_1$ and $V_2$) but parallel data is available between each of these views and a pivot view ($V_3$). We propose a model for learning a common representation for $V_1$, $V_2$ and $V_3$ using only the parallel data available between $V_1V_3$ and $V_2V_3$. The proposed model is generic and even works when there are $n$ views of interest and only one pivot view which acts as a bridge between them. There are two specific downstream applications that we focus on (i) transfer learning between languages $L_1$,$L_2$,...,$L_n$ using a pivot language $L$ and (ii) cross modal access between images and a language $L_1$ using a pivot language $L_2$. Our model achieves state-of-the-art performance in multilingual document classification on the publicly available multilingual TED corpus and promising results in multilingual multimodal retrieval on a new dataset created and released as a part of this work.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03710",
        "title": "Hybrid Dialog State Tracker",
        "authors": [
            "Miroslav Vodol\u00e1n",
            "Rudolf Kadlec",
            "Jan Kleindienst"
        ],
        "abstract": "This paper presents a hybrid dialog state tracker that combines a rule based and a machine learning based approach to belief state tracking. Therefore, we call it a hybrid tracker. The machine learning in our tracker is realized by a Long Short Term Memory (LSTM) network. To our knowledge, our hybrid tracker sets a new state-of-the-art result for the Dialog State Tracking Challenge (DSTC) 2 dataset when the system uses only live SLU as its input.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2016-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03753",
        "title": "Improved Deep Learning Baselines for Ubuntu Corpus Dialogs",
        "authors": [
            "Rudolf Kadlec",
            "Martin Schmid",
            "Jan Kleindienst"
        ],
        "abstract": "This paper presents results of our experiments for the next utterance ranking on the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialog corpus. First, we use an in-house implementation of previously reported models to do an independent evaluation using the same data. Second, we evaluate the performances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging predictions of multiple models. The ensemble further improves the performance and it achieves a state-of-the-art result for the next utterance ranking on this dataset. Finally, we discuss our future plans using this corpus.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03820",
        "title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification",
        "authors": [
            "Ye Zhang",
            "Byron Wallace"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (kim 2014, kalchbrenner 2014, johnson 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04104",
        "title": "A Preliminary Study on the Learning Informativeness of Data Subsets",
        "authors": [
            "Simon Kaltenbacher",
            "Nicholas H. Kirk",
            "Dongheui Lee"
        ],
        "abstract": "Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios, a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention. Such problems need to be addressed to ensure timely and meaningful human-robot interaction. Our work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04500",
        "title": "Noisy-parallel and comparable corpora filtering methodology for the extraction of bi-lingual equivalent data at sentence level",
        "authors": [
            "Krzysztof Wo\u0142k"
        ],
        "abstract": "Text alignment and text quality are critical to the accuracy of Machine Translation (MT) systems, some NLP tools, and any other text processing tasks requiring bilingual data. This research proposes a language independent bi-sentence filtering approach based on Polish (not a position-sensitive language) to English experiments. This cleaning approach was developed on the TED Talks corpus and also initially tested on the Wikipedia comparable corpus, but it can be used for any text domain or language pair. The proposed approach implements various heuristics for sentence comparison. Some of them leverage synonyms and semantic and structural analysis of text as additional information. Minimization of data loss was ensured. An improvement in MT system score with text processed using the tool is discussed.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04600",
        "title": "Telemedicine as a special case of Machine Translation",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek",
            "Wojciech Glinkowski"
        ],
        "abstract": "Machine translation is evolving quite rapidly in terms of quality. Nowadays, we have several machine translation systems available in the web, which provide reasonable translations. However, these systems are not perfect, and their quality may decrease in some specific domains. This paper examines the effects of different training methods when it comes to Polish - English Statistical Machine Translation system used for the medical data. Numerous elements of the EMEA parallel text corpora and not related OPUS Open Subtitles project were used as the ground for creation of phrase tables and different language models including the development, tuning and testing of these translation systems. The BLEU, NIST, METEOR, and TER metrics have been used in order to evaluate the results of various systems. Our experiments deal with the systems that include POS tagging, factored phrase models, hierarchical models, syntactic taggers, and other alignment methods. We also executed a deep analysis of Polish data as preparatory work before automatized data processing such as true casing or punctuation normalization phase. Normalized metrics was used to compare results. Scores lower than 15% mean that Machine Translation engine is unable to provide satisfying quality, scores greater than 30% mean that translations should be understandable without problems and scores over 50 reflect adequate translations. The average results of Polish to English translations scores for BLEU, NIST, METEOR, and TER were relatively high and ranged from 70,58 to 82,72. The lowest score was 64,38. The average results ranges for English to Polish translations were little lower (67,58 - 78,97). The real-life implementations of presented high quality Machine Translation Systems are anticipated in general medical practice and telemedicine.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04709",
        "title": "Multilingual Image Description with Neural Sequence Models",
        "authors": [
            "Desmond Elliott",
            "Stella Frank",
            "Eva Hasler"
        ],
        "abstract": "In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04734",
        "title": "A Method for Modeling Co-Occurrence Propensity of Clinical Codes with Application to ICD-10-PCS Auto-Coding",
        "authors": [
            "Michael Subotin",
            "Anthony R. Davis"
        ],
        "abstract": "Objective. Natural language processing methods for medical auto-coding, or automatic generation of medical billing codes from electronic health records, generally assign each code independently of the others. They may thus assign codes for closely related procedures or diagnoses to the same document, even when they do not tend to occur together in practice, simply because the right choice can be difficult to infer from the clinical narrative.\n",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04780",
        "title": "A Graph Traversal Based Approach to Answer Non-Aggregation Questions Over DBpedia",
        "authors": [
            "Chenhao Zhu",
            "Kan Ren",
            "Xuan Liu",
            "Haofen Wang",
            "Yiding Tian",
            "Yong Yu"
        ],
        "abstract": "We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2018-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04972",
        "title": "Normalization of Relative and Incomplete Temporal Expressions in Clinical Narratives",
        "authors": [
            "Weiyi Sun",
            "Anna Rumshisky",
            "Ozlem Uzuner"
        ],
        "abstract": "We analyze the RI-TIMEXes in temporally annotated corpora and propose two hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative domain: the anchor point hypothesis and the anchor relation hypothesis. We annotate the RI-TIMEXes in three corpora to study the characteristics of RI-TMEXes in different domains. This informed the design of our RI-TIMEX normalization system for the clinical domain, which consists of an anchor point classifier, an anchor relation classifier and a rule-based RI-TIMEX text span parser. We experiment with different feature sets and perform error analysis for each system component. The annotation confirmed the hypotheses that we can simplify the RI-TIMEXes normalization task using two multi-label classifiers. Our system achieves anchor point classification, anchor relation classification and rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under relaxed matching criteria) respectively on the held-out test set of the 2012 i2b2 temporal relation challenge. Experiments with feature sets reveals some interesting findings such as the verbal tense feature does not inform the anchor relation classification in clinical narratives as much as the tokens near the RI-TIMEX. Error analysis shows that underrepresented anchor point and anchor relation classes are difficult to detect. We formulate the RI-TIMEX normalization problem as a pair of multi-label classification problems. Considering only the RI-TIMEX extraction and normalization, the system achieves statistically significant improvement over the RI-TIMEX results of the best systems in the 2012 i2b2 challenge.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05203",
        "title": "Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015",
        "authors": [
            "Graham Neubig",
            "Makoto Morishita",
            "Satoshi Nakamura"
        ],
        "abstract": "This year, the Nara Institute of Science and Technology (NAIST)'s submission to the 2015 Workshop on Asian Translation was based on syntax-based statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words.\n    ",
        "submission_date": "2015-10-18T00:00:00",
        "last_modified_date": "2015-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06168",
        "title": "Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network",
        "authors": [
            "Peilu Wang",
            "Yao Qian",
            "Frank K. Soong",
            "Lei He",
            "Hai Zhao"
        ],
        "abstract": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.\n    ",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2015-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06342",
        "title": "Prevalence and recoverability of syntactic parameters in sparse distributed memories",
        "authors": [
            "Jeong Joon Park",
            "Ronnel Boettcher",
            "Andrew Zhao",
            "Alex Mun",
            "Kevin Yuh",
            "Vibhor Kumar",
            "Matilde Marcolli"
        ],
        "abstract": "We propose a new method, based on Sparse Distributed Memory (Kanerva Networks), for studying dependency relations between different syntactic parameters in the Principles and Parameters model of Syntax. We store data of syntactic parameters of world languages in a Kanerva Network and we check the recoverability of corrupted parameter data from the network. We find that different syntactic parameters have different degrees of recoverability. We identify two different effects: an overall underlying relation between the prevalence of parameters across languages and their degree of recoverability, and a finer effect that makes some parameters more easily recoverable beyond what their prevalence would indicate. We interpret a higher recoverability for a syntactic parameter as an indication of the existence of a dependency relation, through which the given parameter can be determined using the remaining uncorrupted data.\n    ",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2015-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06549",
        "title": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling",
        "authors": [
            "Aaron Q Li"
        ],
        "abstract": "There is an explosion of data, documents, and other content, and people require tools to analyze and interpret these, tools to turn the content into information and knowledge. Topic modeling have been developed to solve these problems. Topic models such as LDA [Blei et. al. 2003] allow salient patterns in data to be extracted automatically. When analyzing texts, these patterns are called topics. Among numerous extensions of LDA, few of them can reliably analyze multiple groups of documents and extract topic similarities. Recently, the introduction of differential topic modeling (SPDP) [Chen et. al. 2012] performs uniformly better than many topic models in a discriminative setting.\n",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2015-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06786",
        "title": "Freshman or Fresher? Quantifying the Geographic Variation of Internet Language",
        "authors": [
            "Vivek Kulkarni",
            "Bryan Perozzi",
            "Steven Skiena"
        ],
        "abstract": "We present a new computational technique to detect and analyze statistically significant geographic variation in language. Our meta-analysis approach captures statistical properties of word usage across geographical regions and uses statistical methods to identify significant changes specific to regions. While previous approaches have primarily focused on lexical variation between regions, our method identifies words that demonstrate semantic and syntactic variation as well.\n",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06807",
        "title": "Learning in the Rational Speech Acts Model",
        "authors": [
            "Will Monroe",
            "Christopher Potts"
        ],
        "abstract": "The Rational Speech Acts (RSA) model treats language use as a recursive process in which probabilistic speaker and listener agents reason about each other's intentions to enrich the literal semantics of their language along broadly Gricean lines. RSA has been shown to capture many kinds of conversational implicature, but it has been criticized as an unrealistic model of speakers, and it has so far required the manual specification of a semantic lexicon, preventing its use in natural language processing applications that learn lexical knowledge from data. We address these concerns by showing how to define and optimize a trained statistical classifier that uses the intermediate agents of RSA as hidden layers of representation forming a non-linear activation function. This treatment opens up new application domains and new possibilities for learning effectively from data. We validate the model on a referential expression generation task, showing that the best performance is achieved by incorporating features approximating well-established insights about natural language generation into RSA.\n    ",
        "submission_date": "2015-10-23T00:00:00",
        "last_modified_date": "2015-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07099",
        "title": "Combine CRF and MMSEG to Boost Chinese Word Segmentation in Social Media",
        "authors": [
            "Yao Yushi",
            "Huang Zheng"
        ],
        "abstract": "In this paper, we propose a joint algorithm for the word segmentation on Chinese social media. Previous work mainly focus on word segmentation for plain Chinese text, in order to develop a Chinese social media processing tool, we need to take the main features of social media into account, whose grammatical structure is not rigorous, and the tendency of using colloquial and Internet terms makes the existing Chinese-processing tools inefficient to obtain good performance on social media.\n",
        "submission_date": "2015-10-24T00:00:00",
        "last_modified_date": "2015-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07193",
        "title": "Statistical Parsing by Machine Learning from a Classical Arabic Treebank",
        "authors": [
            "Kais Dukes"
        ],
        "abstract": "Research into statistical parsing for English has enjoyed over a decade of successful results. However, adapting these models to other languages has met with difficulties. Previous comparative work has shown that Modern Arabic is one of the most difficult languages to parse due to rich morphology and free word order. Classical Arabic is the ancient form of Arabic, and is understudied in computational linguistics, relative to its worldwide reach as the language of the Quran. The thesis is based on seven publications that make significant contributions to knowledge relating to annotating and parsing Classical Arabic.\n",
        "submission_date": "2015-10-25T00:00:00",
        "last_modified_date": "2015-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07385",
        "title": "How to merge three different methods for information filtering ?",
        "authors": [
            "Jean-Val\u00e8re Cossu",
            "Ludovic Bonnefoy",
            "Xavier Bost",
            "Marc El B\u00e8ze"
        ],
        "abstract": "Twitter is now a gold marketing tool for entities concerned with online reputation. To automatically monitor online reputation of entities , systems have to deal with ambiguous entity names, polarity detection and topic detection. We propose three approaches to tackle the first issue: monitoring Twitter in order to find relevant tweets about a given entity. Evaluated within the framework of the RepLab-2013 Filtering task, each of them has been shown competitive with state-of-the-art approaches. Mainly we investigate on how much merging strategies may impact performances on a filtering task according to the evaluation measure.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07482",
        "title": "Edge-Linear First-Order Dependency Parsing with Undirected Minimum Spanning Tree Inference",
        "authors": [
            "Effi Levi",
            "Roi Reichart",
            "Ari Rappoport"
        ],
        "abstract": "The run time complexity of state-of-the-art inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the resulting parse trees. Solving the inference problem in run time complexity determined solely by the number of edges (m) is hence of obvious importance.\n",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07526",
        "title": "Empirical Study on Deep Learning Models for Question Answering",
        "authors": [
            "Yang Yu",
            "Wei Zhang",
            "Chung-Wei Hang",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07586",
        "title": "Parser for Abstract Meaning Representation using Learning to Search",
        "authors": [
            "Sudha Rao",
            "Yogarshi Vyas",
            "Hal Daume III",
            "Philip Resnik"
        ],
        "abstract": "We develop a novel technique to parse English sentences into Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modeling the concept and the relation learning in a unified framework. We evaluate our parser on multiple datasets from varied domains and show an absolute improvement of 2% to 6% over the state-of-the-art. Additionally we show that using the most frequent concept gives us a baseline that is stronger than the state-of-the-art for concept prediction. We plan to release our parser for public use.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07851",
        "title": "Standards for language resources in ISO -- Looking back at 13 fruitful years",
        "authors": [
            "Laurent Romary"
        ],
        "abstract": "This paper provides an overview of the various projects carried out within ISO committee TC 37/SC 4 dealing with the management of language (digital) resources. On the basis of the technical experience gained in the committee and the wider standardization landscape the paper identifies some possible trends for the future.\n    ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2015-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08418",
        "title": "Fast k-best Sentence Compression",
        "authors": [
            "Katja Filippova",
            "Enrique Alfonseca"
        ],
        "abstract": "A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately, dependence on ILP may make the compressor prohibitively slow, and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution, we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILP-based method while producing better compressions. Moreover, an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2015-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08480",
        "title": "Emoticons vs. Emojis on Twitter: A Causal Inference Approach",
        "authors": [
            "Umashanthi Pavalanathan",
            "Jacob Eisenstein"
        ],
        "abstract": "Online writing lacks the non-verbal cues present in face-to-face communication, which provide additional contextual information about the utterance, such as the speaker's intention or affective state. To fill this void, a number of orthographic features, such as emoticons, expressive lengthening, and non-standard punctuation, have become popular in social media services including Twitter and Instagram. Recently, emojis have been introduced to social media, and are increasingly popular. This raises the question of whether these predefined pictographic characters will come to replace earlier orthographic methods of paralinguistic communication. In this abstract, we attempt to shed light on this question, using a matching approach from causal inference to test whether the adoption of emojis causes individual users to employ fewer emoticons in their text on Twitter.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2015-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08985",
        "title": "Prediction-Adaptation-Correction Recurrent Neural Networks for Low-Resource Language Speech Recognition",
        "authors": [
            "Yu Zhang",
            "Ekapol Chuangsuwanich",
            "James Glass",
            "Dong Yu"
        ],
        "abstract": "In this paper, we investigate the use of prediction-adaptation-correction recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A PAC-RNN is comprised of a pair of neural networks in which a {\\it correction} network uses auxiliary information given by a {\\it prediction} network to help estimate the state probability. The information from the correction network is also used by the prediction network in a recurrent loop. Our model outperforms other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks. Moreover, transfer learning from a language that is similar to the target language can help improve performance further.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.09079",
        "title": "SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis",
        "authors": [
            "Lorenzo Gatti",
            "Marco Guerini",
            "Marco Turchi"
        ],
        "abstract": "Deriving prior polarity lexica for sentiment analysis - where positive or negative scores are associated with words out of context - is a challenging task. Usually, a trade-off between precision and coverage is hard to find, and it depends on the methodology used to build the lexicon. Manually annotated lexica provide a high precision but lack in coverage, whereas automatic derivation from pre-existing knowledge guarantees high coverage at the cost of a lower precision. Since the automatic derivation of prior polarities is less time consuming than manual annotation, there has been a great bloom of these approaches, in particular based on the SentiWordNet resource. In this paper, we compare the most frequently used techniques based on SentiWordNet with newer ones and blend them in a learning framework (a so called 'ensemble method'). By taking advantage of manually built prior polarity lexica, our ensemble method is better able to predict the prior value of unseen words and to outperform all the other SentiWordNet approaches. Using this technique we have built SentiWords, a prior polarity lexicon of approximately 155,000 words, that has both a high precision and a high coverage. We finally show that in sentiment analysis tasks, using our lexicon allows us to outperform both the single metrics derived from SentiWordNet and popular manually annotated sentiment lexica.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.09202",
        "title": "Generating Text with Deep Reinforcement Learning",
        "authors": [
            "Hongyu Guo"
        ],
        "abstract": "We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively. The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts. Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN. Take rephrasing a natural sentence as an example. This list can contain ranked potential words. Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence. The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration. In each iteration, we also bias the reinforcement learning's attention to explore sequence portions which are previously difficult to be decoded. For evaluation, the proposed strategy was trained to decode ten thousands natural sentences. Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00060",
        "title": "Top-down Tree Long Short-Term Memory Networks",
        "authors": [
            "Xingxing Zhang",
            "Liang Lu",
            "Mirella Lapata"
        ],
        "abstract": "Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance.\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2016-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00215",
        "title": "A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding",
        "authors": [
            "Peilu Wang",
            "Yao Qian",
            "Frank K. Soong",
            "Lei He",
            "Hai Zhao"
        ],
        "abstract": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for modeling and predicting sequential data, e.g. speech utterances or handwritten documents. In this study, we propose to use BLSTM-RNN for a unified tagging solution that can be applied to various tagging tasks including part-of-speech tagging, chunking and named entity recognition. Instead of exploiting specific features carefully optimized for each task, our solution only uses one set of task-independent features and internal representations learnt from unlabeled text for all ",
        "submission_date": "2015-11-01T00:00:00",
        "last_modified_date": "2015-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00360",
        "title": "Automatic Prosody Prediction for Chinese Speech Synthesis using BLSTM-RNN and Embedding Features",
        "authors": [
            "Chuang Ding",
            "Lei Xie",
            "Jie Yan",
            "Weini Zhang",
            "Yang Liu"
        ],
        "abstract": "Prosody affects the naturalness and intelligibility of speech. However, automatic prosody prediction from text for Chinese speech synthesis is still a great challenge and the traditional conditional random fields (CRF) based method always heavily relies on feature engineering. In this paper, we propose to use neural networks to predict prosodic boundary labels directly from Chinese characters without any feature engineering. Experimental results show that stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent network layers achieves superior performance over the CRF-based method. The embedding features learned from raw text further enhance the performance.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01042",
        "title": "Detecting Interrogative Utterances with Recurrent Neural Networks",
        "authors": [
            "Junyoung Chung",
            "Jacob Devlin",
            "Hany Hassan Awadalla"
        ],
        "abstract": "In this paper, we explore different neural network architectures that can predict if a speaker of a given utterance is asking a question or making a statement. We com- pare the outcomes of regularization methods that are popularly used to train deep neural networks and study how different context functions can affect the classification performance. We also compare the efficacy of gated activation functions that are favorably used in recurrent neural networks and study how to combine multimodal inputs. We evaluate our models on two multimodal datasets: MSR-Skype and CALLHOME.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01556",
        "title": "Mining Local Gazetteers of Literary Chinese with CRF and Pattern based Methods for Biographical Information in Chinese History",
        "authors": [
            "Chao-Lin Liu",
            "Chih-Kai Huang",
            "Hongsu Wang",
            "Peter K. Bol"
        ],
        "abstract": "Person names and location names are essential building blocks for identifying events and social networks in historical documents that were written in literary Chinese. We take the lead to explore the research on algorithmically recognizing named entities in literary Chinese for historical studies with language-model based and conditional-random-field based methods, and extend our work to mining the document structures in historical documents. Practical evaluations were conducted with texts that were extracted from more than 220 volumes of local gazetteers (Difangzhi). Difangzhi is a huge and the single most important collection that contains information about officers who served in local government in Chinese history. Our methods performed very well on these realistic tests. Thousands of names and addresses were identified from the texts. A good portion of the extracted names match the biographical information currently recorded in the China Biographical Database (CBDB) of Harvard University, and many others can be verified by historians and will become as new additions to CBDB.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01559",
        "title": "Color Aesthetics and Social Networks in Complete Tang Poems: Explorations and Discoveries",
        "authors": [
            "Chao-Lin Liu",
            "Hongsu Wang",
            "Wen-Huei Cheng",
            "Chu-Ting Hsu",
            "Wei-Yun Chiu"
        ],
        "abstract": "The Complete Tang Poems (CTP) is the most important source to study Tang poems. We look into CTP with computational tools from specific linguistic perspectives, including distributional semantics and collocational analysis. From such quantitative viewpoints, we compare the usage of \"wind\" and \"moon\" in the poems of Li Bai and Du Fu. Colors in poems function like sounds in movies, and play a crucial role in the imageries of poems. Thus, words for colors are studied, and \"white\" is the main focus because it is the most frequent color in CTP. We also explore some cases of using colored words in antithesis pairs that were central for fostering the imageries of the poems. CTP also contains useful historical information, and we extract person names in CTP to study the social networks of the Tang poets. Such information can then be integrated with the China Biographical Database of Harvard University.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01574",
        "title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model",
        "authors": [
            "Ciprian Chelba",
            "Fernando Pereira"
        ],
        "abstract": "We describe Sparse Non-negative Matrix (SNM) language model estimation using multinomial loss on held-out data.\n",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01665",
        "title": "An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding",
        "authors": [
            "Yiou Lin",
            "Hang Lei",
            "Jia Wu",
            "Xiaoyu Li"
        ],
        "abstract": "In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01666",
        "title": "Comparing Writing Styles using Word Embedding and Dynamic Time Warping",
        "authors": [
            "Abhinav Tushar",
            "Abhinav Dahiya"
        ],
        "abstract": "The development of plot or story in novels is reflected in the content and the words used. The flow of sentiments, which is one aspect of writing style, can be quantified by analyzing the flow of words. This study explores literary works as signals in word embedding space and tries to compare writing styles of popular classic novels using dynamic time warping.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01756",
        "title": "\"Pale as death\" or \"p\u00e2le comme la mort\" : Frozen similes used as literary clich\u00e9s",
        "authors": [
            "Suzanne Mpouli",
            "Jean-Gabriel Ganascia"
        ],
        "abstract": "The present study is focused on the automatic identification and description of frozen similes in British and French novels written between the 19 th century and the beginning of the 20 th century. Two main patterns of frozen similes were considered: adjectival ground + simile marker + nominal vehicle (e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g. sleep like a top). All potential similes and their components were first extracted using a rule-based algorithm. Then, frozen similes were identified based on reference lists of existing similes and semantic distance between the tenor and the vehicle. The results obtained tend to confirm the fact that frozen similes are not used haphazardly in literary texts. In addition, contrary to how they are often presented, frozen similes often go beyond the ground or the eventuality and the vehicle to also include the tenor.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01974",
        "title": "Multi-lingual Geoparsing based on Machine Translation",
        "authors": [
            "Xu Chen",
            "Han Zhang",
            "Judith Gelernter"
        ],
        "abstract": "Our method for multi-lingual geoparsing uses monolingual tools and resources along with machine translation and alignment to return location words in many languages. Not only does our method save the time and cost of developing geoparsers for each language separately, but also it allows the possibility of a wide range of language capabilities within a single interface. We evaluated our method in our LanguageBridge prototype on location named entities using newswire, broadcast news and telephone conversations in English, Arabic and Chinese data from the Linguistic Data Consortium (LDC). Our results for geoparsing Chinese and Arabic text using our multi-lingual geoparsing method are comparable to our results for geoparsing English text with our English tools. Furthermore, experiments using our machine translation approach results in accuracy comparable to results from the same data that was translated manually.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02014",
        "title": "Population size predicts lexical diversity, but so does the mean sea level - why it is important to correctly account for the structure of temporal data",
        "authors": [
            "Alexander Koplenig",
            "Carolin Mueller-Spitzer"
        ],
        "abstract": "In order to demonstrate why it is important to correctly account for the (serial dependent) structure of temporal data, we document an apparently spectacular relationship between population size and lexical diversity: for five out of seven investigated languages, there is a strong relationship between population size and lexical diversity of the primary language in this country. We show that this relationship is the result of a misspecified model that does not consider the temporal aspect of the data by presenting a similar but nonsensical relationship between the global annual mean sea level and lexical diversity. Given the fact that in the recent past, several studies were published that present surprising links between different economic, cultural, political and (socio-)demographical variables on the one hand and cultural or linguistic characteristics on the other hand, but seem to suffer from exactly this problem, we explain the cause of the misspecification and show that it has profound consequences. We demonstrate how simple transformation of the time series can often solve problems of this type and argue that the evaluation of the plausibility of a relationship is important in this context. We hope that our paper will help both researchers and reviewers to understand why it is important to use special models for the analysis of data with a natural temporal ordering.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02117",
        "title": "Introducing SKYSET - a Quintuple Approach for Improving Instructions",
        "authors": [
            "Kerry Fultz",
            "Seth Filip"
        ],
        "abstract": "A new approach called SKYSET (Synthetic Knowledge Yield Social Entities Translation) is proposed to validate completeness and to reduce ambiguity from written instructional documentation. SKYSET utilizes a quintuple set of standardized categories, which differs from traditional approaches that typically use triples. The SKYSET System defines the categories required to form a standard template for representing information that is portable across different domains. It provides a standardized framework that enables sentences from written instructions to be translated into sets of category typed entities on a table or database. The SKYSET entities contain conceptual units or phrases that represent information from the original source documentation. SKYSET enables information concatenation where multiple documents from different domains can be translated and combined into a single common filterable and searchable table of entities.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02301",
        "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations",
        "authors": [
            "Felix Hill",
            "Antoine Bordes",
            "Sumit Chopra",
            "Jason Weston"
        ],
        "abstract": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02385",
        "title": "Review-Level Sentiment Classification with Sentence-Level Polarity Correction",
        "authors": [
            "Sylvester Olubolu Orimaye",
            "Saadat M. Alhashmi",
            "Eu-Gene Siew",
            "Sang Jung Kang"
        ],
        "abstract": "We propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction. Our polarity correction technique takes into account the consistency of the polarities (positive and negative) of sentences within each product review before performing the actual machine learning task. While sentences with inconsistent polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers. The technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique. Experimental results show an average of 82% F-measure on four different product review domains.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2015-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02435",
        "title": "A Chinese POS Decision Method Using Korean Translation Information",
        "authors": [
            "Son-Il Kwak",
            "O-Chol Kown",
            "Chang-Sin Kim",
            "Yong-Il Pak",
            "Gum-Chol Son",
            "Chol-Jun Hwang",
            "Hyon-Chol Kim",
            "Hyok-Chol Sin",
            "Gyong-Il Hyon",
            "Sok-Min Han"
        ],
        "abstract": "In this paper we propose a method that imitates a translation expert using the Korean translation information and analyse the performance. Korean is good at tagging than Chinese, so we can use this property in Chinese POS tagging.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02436",
        "title": "Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment using Compound Skip-grams",
        "authors": [
            "Sylvester Olubolu Orimaye",
            "Kah Yee Tai",
            "Jojo Sze-Meng Wong",
            "Chee Piau Wong"
        ],
        "abstract": "Predicting Mild Cognitive Impairment (MCI) is currently a challenge as existing diagnostic criteria rely on neuropsychological examinations. Automated Machine Learning (ML) models that are trained on verbal utterances of MCI patients can aid diagnosis. Using a combination of skip-gram features, our model learned several linguistic biomarkers to distinguish between 19 patients with MCI and 19 healthy control individuals from the DementiaBank language transcript clinical dataset. Results show that a model with compound of skip-grams has better AUC and could help ML prediction on small MCI data sample.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02506",
        "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition",
        "authors": [
            "Yi-Hsiu Liao",
            "Hung-yi Lee",
            "Lin-shan Lee"
        ],
        "abstract": "In this paper we propose the Structured Deep Neural Network (structured DNN) as a structured and deep learning framework. This approach can learn to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structures rather than item by item.\n",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02556",
        "title": "Sentiment Expression via Emoticons on Social Media",
        "authors": [
            "Hao Wang",
            "Jorge A. Castanon"
        ],
        "abstract": "Emoticons (e.g., :) and :( ) have been widely used in sentiment analysis and other NLP tasks as features to ma- chine learning algorithms or as entries of sentiment lexicons. In this paper, we argue that while emoticons are strong and common signals of sentiment expression on social media the relationship between emoticons and sentiment polarity are not always clear. Thus, any algorithm that deals with sentiment polarity should take emoticons into account but extreme cau- tion should be exercised in which emoticons to depend on. First, to demonstrate the prevalence of emoticons on social media, we analyzed the frequency of emoticons in a large re- cent Twitter data set. Then we carried out four analyses to examine the relationship between emoticons and sentiment polarity as well as the contexts in which emoticons are used. The first analysis surveyed a group of participants for their perceived sentiment polarity of the most frequent emoticons. The second analysis examined clustering of words and emoti- cons to better understand the meaning conveyed by the emoti- cons. The third analysis compared the sentiment polarity of microblog posts before and after emoticons were removed from the text. The last analysis tested the hypothesis that removing emoticons from text hurts sentiment classification by training two machine learning models with and without emoticons in the text respectively. The results confirms the arguments that: 1) a few emoticons are strong and reliable signals of sentiment polarity and one should take advantage of them in any senti- ment analysis; 2) a large group of the emoticons conveys com- plicated sentiment hence they should be treated with extreme caution.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03012",
        "title": "Information retrieval in folktales using natural language processing",
        "authors": [
            "Adrian Groza",
            "Lidia Corde"
        ],
        "abstract": "Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based on Propp's model for folktales morphology.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03053",
        "title": "Investigating the stylistic relevance of adjective and verb simile markers",
        "authors": [
            "Suzanne Mpouli",
            "Jean-Gabriel Ganascia"
        ],
        "abstract": "Similes play an important role in literary texts not only as rhetorical devices and as figures of speech but also because of their evocative power, their aptness for description and the relative ease with which they can be combined with other figures of speech (Israel et al. 2004). Detecting all types of simile constructions in a particular text therefore seems crucial when analysing the style of an author. Few research studies however have been dedicated to the study of less prominent simile markers in fictional prose and their relevance for stylistic studies. The present paper studies the frequency of adjective and verb simile markers in a corpus of British and French novels in order to determine which ones are really informative and worth including in a stylistic analysis. Furthermore, are those adjectives and verb simile markers used differently in both languages?\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03088",
        "title": "USFD: Twitter NER with Drift Compensation and Linked Data",
        "authors": [
            "Leon Derczynski",
            "Isabelle Augenstein",
            "Kalina Bontcheva"
        ],
        "abstract": "This paper describes a pilot NER system for Twitter, comprising the USFD system entry to the W-NUT 2015 NER shared task. The goal is to correctly label entities in a tweet dataset, using an inventory of ten types. We employ structured learning, drawing on gazetteers taken from Linked Data, and on unsupervised clustering features, and attempting to compensate for stylistic and topic drift - a key challenge in social media text. Our result is competitive; we provide an analysis of the components of our methodology, and an examination of the target dataset in the context of this task.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03683",
        "title": "Generative Concatenative Nets Jointly Learn to Write and Classify Reviews",
        "authors": [
            "Zachary C. Lipton",
            "Sharad Vikram",
            "Julian McAuley"
        ],
        "abstract": "A recommender system's basic task is to estimate how users will respond to unseen items. This is typically modeled in terms of how a user might rate a product, but here we aim to extend such approaches to model how a user would write about the product. To do so, we design a character-level Recurrent Neural Network (RNN) that generates personalized product reviews. The network convincingly learns styles and opinions of nearly 1000 distinct authors, using a large corpus of reviews from ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03729",
        "title": "Larger-Context Language Modelling",
        "authors": [
            "Tian Wang",
            "Kyunghyun Cho"
        ],
        "abstract": "In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank), we demon- strate that the proposed model improves perplexity significantly. In the experi- ments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained larger- context language model, we discover that content words, including nouns, adjec- tives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03924",
        "title": "A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural Language",
        "authors": [
            "Normunds Gruzitis",
            "Dana Dann\u00e9lls"
        ],
        "abstract": "Berkeley FrameNet is a lexico-semantic resource for English based on the theory of frame semantics. It has been exploited in a range of natural language processing applications and has inspired the development of framenets for many languages. We present a methodological approach to the extraction and generation of a computational multilingual FrameNet-based grammar and lexicon. The approach leverages FrameNet-annotated corpora to automatically extract a set of cross-lingual semantico-syntactic valence patterns. Based on data from Berkeley FrameNet and Swedish FrameNet, the proposed approach has been implemented in Grammatical Framework (GF), a categorial grammar formalism specialized for multilingual grammars. The implementation of the grammar and lexicon is supported by the design of FrameNet, providing a frame semantic abstraction layer, an interlingual semantic API (application programming interface), over the interlingual syntactic API already provided by GF Resource Grammar Library. The evaluation of the acquired grammar and lexicon shows the feasibility of the approach. Additionally, we illustrate how the FrameNet-based grammar and lexicon are exploited in two distinct multilingual controlled natural language applications. The produced resources are available under an open source license.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03962",
        "title": "Document Context Language Models",
        "authors": [
            "Yangfeng Ji",
            "Trevor Cohn",
            "Lingpeng Kong",
            "Chris Dyer",
            "Jacob Eisenstein"
        ],
        "abstract": "Text documents are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network language models, the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04024",
        "title": "Multimodal Skip-gram Using Convolutional Pseudowords",
        "authors": [
            "Zachary Seymour",
            "Yingming Li",
            "Zhongfei Zhang"
        ],
        "abstract": "This work studies the representational mapping across multimodal data such that given a piece of the raw data in one modality the corresponding semantic description in terms of the raw data in another modality is immediately obtained. Such a representational mapping can be found in a wide spectrum of real-world applications including image/video retrieval, object recognition, action/behavior recognition, and event understanding and prediction. To that end, we introduce a simplified training objective for learning multimodal embeddings using the skip-gram architecture by introducing convolutional \"pseudowords:\" embeddings composed of the additive combination of distributed word representations and image features from convolutional neural networks projected into the multimodal space. We present extensive results of the representational properties of these embeddings on various word similarity benchmarks to show the promise of this approach.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04108",
        "title": "LSTM-based Deep Learning Models for Non-factoid Answer Selection",
        "authors": [
            "Ming Tan",
            "Cicero dos Santos",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "In this paper, we apply a general deep learning (DL) framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One direction is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework. The other direction is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. The models are examined by two datasets, including TREC-QA and InsuranceQA. Experimental results demonstrate that the proposed models substantially outperform several strong baselines.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04586",
        "title": "Character-based Neural Machine Translation",
        "authors": [
            "Wang Ling",
            "Isabel Trancoso",
            "Chris Dyer",
            "Alan W Black"
        ],
        "abstract": "We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2015-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04623",
        "title": "Learning to Represent Words in Context with Multilingual Supervision",
        "authors": [
            "Kazuya Kawakami",
            "Chris Dyer"
        ],
        "abstract": "We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04646",
        "title": "Word Embedding based Correlation Model for Question/Answer Matching",
        "authors": [
            "Yikang Shen",
            "Wenge Rong",
            "Nan Jiang",
            "Baolin Peng",
            "Jie Tang",
            "Zhang Xiong"
        ],
        "abstract": "With the development of community based question answering (Q&A) services, a large scale of Q&A archives have been accumulated and are an important information and knowledge resource on the web. Question and answer matching has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, we try to improve the matching accuracy by overcoming the lexical gap between question and answer pairs. A Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding, given a random pair of words, WEC can score their co-occurrence probability in Q&A pairs and it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method's promising potential.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04661",
        "title": "A System for Extracting Sentiment from Large-Scale Arabic Social Data",
        "authors": [
            "Hao Wang",
            "Vijay R. Bommireddipalli",
            "Ayman Hanafy",
            "Mohamed Bahgat",
            "Sara Noeman",
            "Ossama S. Emam"
        ],
        "abstract": "Social media data in Arabic language is becoming more and more abundant. It is a consensus that valuable information lies in social media data. Mining this data and making the process easier are gaining momentum in the industries. This paper describes an enterprise system we developed for extracting sentiment from large volumes of social data in Arabic dialects. First, we give an overview of the Big Data system for information extraction from multilingual social data from a variety of sources. Then, we focus on the Arabic sentiment analysis capability that was built on top of the system including normalizing written Arabic dialects, building sentiment lexicons, sentiment classification, and performance evaluation. Lastly, we demonstrate the value of enriching sentiment results with user profiles in understanding sentiments of a specific user group.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2015-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04747",
        "title": "Learning Representations of Affect from Speech",
        "authors": [
            "Sayan Ghosh",
            "Eugene Laksana",
            "Louis-Philippe Morency",
            "Stefan Scherer"
        ],
        "abstract": "There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e. categorical and dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence (sadness and anger) from positive valence (happiness). We experiment with different input speech features (such as FFT and log-mel spectrograms with temporal context windows), and different autoencoder architectures (such as stacked and deep autoencoders). We also learn utterance specific representations by a combination of denoising autoencoders and BLSTM based recurrent autoencoders. Emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations. Experiments on a well-established real-life speech dataset (IEMOCAP) show that the learnt representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2016-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05076",
        "title": "Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation",
        "authors": [
            "Mortaza Doulaty",
            "Oscar Saz",
            "Raymond W. M. Ng",
            "Thomas Hain"
        ],
        "abstract": "This paper presents a new method for the discovery of latent domains in diverse speech data, for the use of adaptation of Deep Neural Networks (DNNs) for Automatic Speech Recognition. Our work focuses on transcription of multi-genre broadcast media, which is often only categorised broadly in terms of high level genres such as sports, news, documentary, etc. However, in terms of acoustic modelling these categories are coarse. Instead, it is expected that a mixture of latent domains can better represent the complex and diverse behaviours within a TV show, and therefore lead to better and more robust performance. We propose a new method, whereby these latent domains are discovered with Latent Dirichlet Allocation, in an unsupervised manner. These are used to adapt DNNs using the Unique Binary Code (UBIC) representation for the LDA domains. Experiments conducted on a set of BBC TV broadcasts, with more than 2,000 shows for training and 47 shows for testing, show that the use of LDA-UBIC DNNs reduces the error up to 13% relative compared to the baseline hybrid DNN models.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05099",
        "title": "Yin and Yang: Balancing and Answering Binary Visual Questions",
        "authors": [
            "Peng Zhang",
            "Yash Goyal",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is \"yes\", and otherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on the high-level semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect fine-grained pairs of scenes for every question, such that the answer to the question is \"yes\" for one scene, and \"no\" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach matches the performance of a state-of-the-art VQA approach on the unbalanced dataset, and outperforms it on the balanced dataset.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05389",
        "title": "Learning to retrieve out-of-vocabulary words in speech recognition",
        "authors": [
            "Imran Sheikh",
            "Irina Illina",
            "Dominique Fohr",
            "Georges Linar\u00e8s"
        ],
        "abstract": "Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech recognition systems used to process diachronic audio data. To help recovery of the PNs missed by the system, relevant OOV PNs can be retrieved out of the many OOVs by exploiting semantic context of the spoken content. In this paper, we propose two neural network models targeted to retrieve OOV PNs relevant to an audio document: (a) Document level Continuous Bag of Words (D-CBOW), (b) Document level Continuous Bag of Weighted Words (D-CBOW2). Both these models take document words as input and learn with an objective to maximise the retrieval of co-occurring OOV PNs. With the D-CBOW2 model we propose a new approach in which the input embedding layer is augmented with a context anchor layer. This layer learns to assign importance to input words and has the ability to capture (task specific) key-words in a bag-of-word neural network model. With experiments on French broadcast news videos we show that these two models outperform the baseline methods based on raw embeddings from LDA, Skip-gram and Paragraph Vectors. Combining the D-CBOW and D-CBOW2 models gives faster convergence during training.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05926",
        "title": "Combining Neural Networks and Log-linear Models to Improve Relation Extraction",
        "authors": [
            "Thien Huu Nguyen",
            "Ralph Grishman"
        ],
        "abstract": "The last decade has witnessed the success of the traditional feature-based method on exploiting the discrete structures such as words or lexical patterns to extract relations from text. Recently, convolutional and recurrent neural networks has provided very effective mechanisms to capture the hidden structures within sentences via continuous representations, thereby significantly advancing the performance of relation extraction. The advantage of convolutional neural networks is their capacity to generalize the consecutive k-grams in the sentences while recurrent neural networks are effective to encode long ranges of sentence context. This paper proposes to combine the traditional feature-based method, the convolutional and recurrent neural networks to simultaneously benefit from their advantages. Our systematic evaluation of different network architectures and combination methods demonstrates the effectiveness of this approach and results in the state-of-the-art performance on the ACE 2005 and SemEval dataset.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06018",
        "title": "Segmental Recurrent Neural Networks",
        "authors": [
            "Lingpeng Kong",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We introduce segmental recurrent neural networks (SRNNs) which define, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments. Representations of the input segments (i.e., contiguous subsequences of the input) are computed by encoding their constituent tokens using bidirectional recurrent neural nets, and these \"segment embeddings\" are used to define compatibility scores with output labels. These local compatibility scores are integrated using a global semi-Markov conditional random field. Both fully supervised training -- in which segment boundaries and labels are observed -- as well as partially supervised training -- in which segment boundaries are latent -- are straightforward. Experiments on handwriting recognition and joint Chinese word segmentation/POS tagging show that, compared to models that do not explicitly represent segments such as BIO tagging schemes and connectionist temporal classification (CTC), SRNNs obtain substantially higher accuracies.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06038",
        "title": "Neural Variational Inference for Text Processing",
        "authors": [
            "Yishu Miao",
            "Lei Yu",
            "Phil Blunsom"
        ],
        "abstract": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06052",
        "title": "Overcoming Language Variation in Sentiment Analysis with Social Attention",
        "authors": [
            "Yi Yang",
            "Jacob Eisenstein"
        ],
        "abstract": "Variation in language is ubiquitous, particularly in newer forms of writing such as social media. Fortunately, variation is not random, it is often linked to social properties of the author. In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation. The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author's position in the social network. This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This model significantly improves the accuracies of sentiment analysis on Twitter and on review data.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2017-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06066",
        "title": "Transfer Learning for Speech and Language Processing",
        "authors": [
            "Dong Wang",
            "Thomas Fang Zheng"
        ],
        "abstract": "Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of `model adaptation'. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the `transfer' can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing. We also report some results from our group and highlight the potential of this very interesting research field.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06219",
        "title": "Knowledge Base Population using Semantic Label Propagation",
        "authors": [
            "Lucas Sterckx",
            "Thomas Demeester",
            "Johannes Deleu",
            "Chris Develder"
        ],
        "abstract": "A crucial aspect of a knowledge base population system that extracts new facts from text corpora, is the generation of training data for its relation extractors. In this paper, we present a method that maximizes the effectiveness of newly trained relation extractors at a minimal annotation cost. Manual labeling can be significantly reduced by Distant Supervision, which is a method to construct training data automatically by aligning a large text corpus with an existing knowledge base of known facts. For example, all sentences mentioning both 'Barack Obama' and 'US' may serve as positive training instances for the relation born_in(subject,object). However, distant supervision typically results in a highly noisy training set: many training sentences do not really express the intended relation. We propose to combine distant supervision with minimal manual supervision in a technique called feature labeling, to eliminate noise from the large and noisy initial training set, resulting in a significant increase of precision. We further improve on this approach by introducing the Semantic Label Propagation method, which uses the similarity between low-dimensional representations of candidate training instances, to extend the training set in order to increase recall while maintaining high precision. Our proposed strategy for generating training data is studied and evaluated on an established test collection designed for knowledge base population tasks. The experimental results show that the Semantic Label Propagation strategy leads to substantial performance gains when compared to existing approaches, while requiring an almost negligible manual annotation effort.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06246",
        "title": "Gaussian Mixture Embeddings for Multiple Word Prototypes",
        "authors": [
            "Xinchi Chen",
            "Xipeng Qiu",
            "Jingxiang Jiang",
            "Xuanjing Huang"
        ],
        "abstract": "Recently, word representation has been increasingly focused on for its excellent properties in representing the word semantics. Previous works mainly suffer from the problem of polysemy phenomenon. To address this problem, most of previous models represent words as multiple distributed vectors. However, it cannot reflect the rich relations between words by representing words as points in the embedded space. In this paper, we propose the Gaussian mixture skip-gram (GMSG) model to learn the Gaussian mixture embeddings for words based on skip-gram framework. Each word can be regarded as a gaussian mixture distribution in the embedded space, and each gaussian component represents a word sense. Since the number of senses varies from word to word, we further propose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense number of words during training. Experiments on four benchmarks show the effectiveness of our proposed model.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06285",
        "title": "Harvesting comparable corpora and mining them for equivalent bilingual sentences using statistical classification and analogy- based heuristics",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Emilia Rejmund",
            "Krzysztof Marasek"
        ],
        "abstract": "Parallel sentences are a relatively scarce but extremely useful resource for many applications including cross-lingual retrieval and statistical machine translation. This research explores our new methodologies for mining such data from previously obtained comparable corpora. The task is highly practical since non-parallel multilingual data exist in far greater quantities than parallel corpora, but parallel sentences are a much more useful resource. Here we propose a web crawling method for building subject-aligned comparable corpora from e.g. Wikipedia dumps and Euronews web page. The improvements in machine translation are shown on Polish-English language pair for various text domains. We also tested another method of building parallel corpora based on comparable corpora data. It lets automatically broad existing corpus of sentences from subject of corpora based on analogies between them.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06312",
        "title": "Good, Better, Best: Choosing Word Embedding Context",
        "authors": [
            "James Cross",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "We propose two methods of learning vector representations of words and phrases that each combine sentence context with structural features extracted from dependency trees. Using several variations of neural network classifier, we show that these combined methods lead to improved performance when used as input features for supervised term-matching.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06341",
        "title": "Communicating Semantics: Reference by Description",
        "authors": [
            "Ramanathan V Guha",
            "Vineet Gupta"
        ],
        "abstract": "Messages often refer to entities such as people, places and events. Correct identification of the intended reference is an essential part of communication. Lack of shared unique names often complicates entity reference. Shared knowledge can be used to construct uniquely identifying descriptive references for entities with ambiguous names. We introduce a mathematical model for `Reference by Description', derive results on the conditions under which, with high probability, programs can construct unambiguous references to most entities in the domain of discourse and provide empirical validation of these results.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06379",
        "title": "Dynamic Adaptive Network Intelligence",
        "authors": [
            "Richard Searle",
            "Megan Bingham-Walker"
        ],
        "abstract": "Accurate representational learning of both the explicit and implicit relationships within data is critical to the ability of machines to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06388",
        "title": "sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings",
        "authors": [
            "Andrew Trask",
            "Phil Michalak",
            "John Liu"
        ],
        "abstract": "Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or \"senses\". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06396",
        "title": "Multilingual Relation Extraction using Compositional Universal Schema",
        "authors": [
            "Patrick Verga",
            "David Belanger",
            "Emma Strubell",
            "Benjamin Roth",
            "Andrew McCallum"
        ],
        "abstract": "Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns expressing relations from raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns' compositional semantics, providing generalization to all possible input text. In response, this paper introduces significant further improvements to the coverage and flexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-filling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we find that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06397",
        "title": "Compressing Word Embeddings",
        "authors": [
            "Martin Andrews"
        ],
        "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic. However, these vector space representations (created through large-scale text analysis) are typically stored verbatim, since their internal structure is opaque. Using word-analogy tests to monitor the level of detail stored in compressed re-representations of the same vector space, the trade-offs between the reduction in memory usage and expressiveness are investigated. A simple scheme is outlined that can reduce the memory footprint of a state-of-the-art embedding by a factor of 10, with only minimal impact on performance. Then, using the same `bit budget', a binary (approximate) factorisation of the same space is also explored, with the aim of creating an equivalent representation with better interpretability.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06426",
        "title": "Reasoning in Vector Space: An Exploratory Study of Question Answering",
        "authors": [
            "Moontae Lee",
            "Xiaodong He",
            "Wen-tau Yih",
            "Jianfeng Gao",
            "Li Deng",
            "Paul Smolensky"
        ],
        "abstract": "Question answering tasks have shown remarkable progress with distributed vector representation. In this paper, we investigate the recently proposed Facebook bAbI tasks which consist of twenty different categories of questions that require complex reasoning. Because the previous work on bAbI are all end-to-end models, errors could come from either an imperfect understanding of semantics or in certain steps of the reasoning. For clearer analysis, we propose two vector space models inspired by Tensor Product Representation (TPR) to perform knowledge encoding and logical reasoning based on common-sense inference. They together achieve near-perfect accuracy on all categories including positional reasoning and path finding that have proved difficult for most of the previous approaches. We hypothesize that the difficulties in these categories are due to the multi-relations in contrast to uni-relational characteristic of other categories. Our exploration sheds light on designing more sophisticated dataset and moving one step toward integrating transparent and interpretable formalism of TPR into existing learning paradigms.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06438",
        "title": "Joint Word Representation Learning using a Corpus and a Semantic Lexicon",
        "authors": [
            "Danushka Bollegala",
            "Alsuhaibani Mohammed",
            "Takanori Maehara",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "Methods for learning word representations using large text corpora have received much attention lately due to their impressive performance in numerous natural language processing (NLP) tasks such as, semantic similarity measurement, and word analogy detection. Despite their success, these data-driven word representation learning methods do not consider the rich semantic relational structure between words in a co-occurring context. On the other hand, already much manual effort has gone into the construction of semantic lexicons such as the WordNet that represent the meanings of words by defining the various relationships that exist among the words in a language. We consider the question, can we improve the word representations learnt using a corpora by integrating the knowledge from semantic lexicons?. For this purpose, we propose a joint word representation learning method that simultaneously predicts the co-occurrences of two words in a sentence subject to the relational constrains given by the semantic lexicon. We use relations that exist between words in the lexicon to regularize the word representations learnt from the corpus. Our proposed method statistically significantly outperforms previously proposed methods for incorporating semantic lexicons into word representations on several benchmark datasets for semantic similarity and word analogy.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06591",
        "title": "Polysemy in Controlled Natural Language Texts",
        "authors": [
            "Normunds Gruzitis",
            "Guntis Barzdins"
        ],
        "abstract": "Computational semantics and logic-based controlled natural languages (CNL) do not address systematically the word sense disambiguation problem of content words, i.e., they tend to interpret only some functional words that are crucial for construction of discourse representation structures. We show that micro-ontologies and multi-word units allow integration of the rich and polysemous multi-domain background knowledge into CNL thus providing interpretation for the content words. The proposed approach is demonstrated by extending the Attempto Controlled English (ACE) with polysemous and procedural constructs resulting in a more natural CNL named PAO covering narrative multi-domain texts.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06709",
        "title": "Improving Neural Machine Translation Models with Monolingual Data",
        "authors": [
            "Rico Sennrich",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06798",
        "title": "Conducting sparse feature selection on arbitrarily long phrases in text corpora with a focus on interpretability",
        "authors": [
            "Luke Miratrix",
            "Robin Ackerman"
        ],
        "abstract": "We propose a general framework for topic-specific summarization of large text corpora, and illustrate how it can be used for analysis in two quite different contexts: an OSHA database of fatality and catastrophe reports (to facilitate surveillance for patterns in circumstances leading to injury or death) and legal decisions on workers' compensation claims (to explore relevant case law). Our summarization framework, built on sparse classification methods, is a compromise between simple word frequency based methods currently in wide use, and more heavyweight, model-intensive methods such as Latent Dirichlet Allocation (LDA). For a particular topic of interest (e.g., mental health disability, or chemical reactions), we regress a labeling of documents onto the high-dimensional counts of all the other words and phrases in the documents. The resulting small set of phrases found as predictive are then harvested as the summary. Using a branch-and-bound approach, this method can be extended to allow for phrases of arbitrary length, which allows for potentially rich summarization. We discuss how focus on the purpose of the summaries can inform choices of regularization parameters and model constraints. We evaluate this tool by comparing computational time and summary statistics of the resulting word lists to three other methods in the literature. We also present a new R package, textreg. Overall, we argue that sparse methods have much to offer text analysis, and is a branch of research that should be considered further in this context.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06833",
        "title": "Semi-supervised Bootstrapping approach for Named Entity Recognition",
        "authors": [
            "S. Thenmalar",
            "J. Balaji",
            "T.V. Geetha"
        ],
        "abstract": "The aim of Named Entity Recognition (NER) is to identify references of named entities in unstructured documents, and to classify them into pre-defined semantic categories. NER often aids from added background knowledge in the form of gazetteers. However using such a collection does not deal with name variants and cannot resolve ambiguities associated in identifying the entities in context and associating them with predefined categories. We present a semi-supervised NER approach that starts with identifying named entities with a small set of training data. Using the identified named entities, the word and the context features are used to define the pattern. This pattern of each named entity category is used as a seed pattern to identify the named entities in the test set. Pattern scoring and tuple value score enables the generation of the new patterns to identify the named entity categories. We have evaluated the proposed system for English language with the dataset of tagged (IEER) and untagged (CoNLL 2003) named entity corpus and for Tamil language with the documents from the FIRE corpus and yield an average f-measure of 75% for both the languages.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06931",
        "title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems",
        "authors": [
            "Jesse Dodge",
            "Andreea Gane",
            "Xiang Zhang",
            "Antoine Bordes",
            "Sumit Chopra",
            "Alexander Miller",
            "Arthur Szlam",
            "Jason Weston"
        ],
        "abstract": "A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering 75k movie entities and with 3.5M training examples. We present results of various models on these tasks, and evaluate their performance.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06961",
        "title": "On the Linear Algebraic Structure of Distributed Word Representations",
        "authors": [
            "Lisa Seung-Yeon Lee"
        ],
        "abstract": "In this work, we leverage the linear algebraic structure of distributed word representations to automatically extend knowledge bases and allow a machine to learn new facts about the world. Our goal is to extract structured facts from corpora in a simpler manner, without applying classifiers or patterns, and using only the co-occurrence statistics of words. We demonstrate that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In particular, we demonstrate that words belonging to a common category, or pairs of words satisfying a certain relation, form a low-rank subspace in the projected space. We compute a basis for this low-rank subspace using singular value decomposition (SVD), then use this basis to discover new facts and to fit vectors for less frequent words which we do not yet have vectors for.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2015-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06995",
        "title": "Non-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation",
        "authors": [
            "Paolo Dragone"
        ],
        "abstract": "Non-sentential utterances (NSUs) are utterances that lack a complete sentential form but whose meaning can be inferred from the dialogue context, such as \"OK\", \"where?\", \"probably at his apartment\". The interpretation of non-sentential utterances is an important problem in computational linguistics since they constitute a frequent phenomena in dialogue and they are intrinsically context-dependent. The interpretation of NSUs is the task of retrieving their full semantic content from their form and the dialogue context. The first half of this thesis is devoted to the NSU classification task. Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \"resolution rules\" built on top of the Type Theory with Records (TTR). Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism Lison (2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation. However, the probabilistic rules can also encode probabilistic knowledge, thereby providing a principled account of ambiguities in the NSU resolution process.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2015-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07788",
        "title": "Spoken Language Translation for Polish",
        "authors": [
            "Krzysztof Marasek",
            "\u0141ukasz Brocki",
            "Danijel Korzinek",
            "Krzysztof Wo\u0142k",
            "Ryszard Gubrynowicz"
        ],
        "abstract": "Spoken language translation (SLT) is becoming more important in the increasingly globalized world, both from a social and economic point of view. It is one of the major challenges for automatic speech recognition (ASR) and machine translation (MT), driving intense research activities in these areas. While past research in SLT, due to technology limitations, dealt mostly with speech recorded under controlled conditions, today's major challenge is the translation of spoken language as it can be found in real life. Considered application scenarios range from portable translators for tourists, lectures and presentations translation, to broadcast news and shows with live captioning. We would like to present PJIIT's experiences in the SLT gained from the Eu-Bridge 7th framework project and the U-Star consortium activities for the Polish/English language pair. Presented research concentrates on ASR adaptation for Polish (state-of-the-art acoustic models: DBN-BLSTM training, Kaldi: LDA+MLLT+SAT+MMI), language modeling for ASR & MT (text normalization, RNN-based LMs, n-gram model domain interpolation) and statistical translation techniques (hierarchical models, factored translation models, automatic casing and punctuation, comparable and bilingual corpora preparation). While results for the well-defined domains (phrases for travelers, parliament speeches, medical documentation, movie subtitling) are very encouraging, less defined domains (presentation, lectures) still form a challenge. Our progress in the IWSLT TED task (MT only) will be presented, as well as current progress in the Polish ASR.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07916",
        "title": "Natural Language Understanding with Distributed Representation",
        "authors": [
            "Kyunghyun Cho"
        ],
        "abstract": "This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced. On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08198",
        "title": "Towards Universal Paraphrastic Sentence Embeddings",
        "authors": [
            "John Wieting",
            "Mohit Bansal",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use.\n",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08308",
        "title": "Named Entity Recognition with Bidirectional LSTM-CNNs",
        "authors": [
            "Jason P.C. Chiu",
            "Eric Nichols"
        ],
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08407",
        "title": "The Mechanism of Additive Composition",
        "authors": [
            "Ran Tian",
            "Naoaki Okazaki",
            "Kentaro Inui"
        ],
        "abstract": "Additive composition (Foltz et al, 1998; Landauer and Dumais, 1997; Mitchell and Lapata, 2010) is a widely used method for computing meanings of phrases, which takes the average of vector representations of the constituent words. In this article, we prove an upper bound for the bias of additive composition, which is the first theoretical analysis on compositional frameworks from a machine learning point of view. The bound is written in terms of collocation strength; we prove that the more exclusively two successive words tend to occur together, the more accurate one can guarantee their additive composition as an approximation to the natural phrase vector. Our proof relies on properties of natural language data that are empirically verified, and can be theoretically derived from an assumption that the data is generated from a Hierarchical Pitman-Yor Process. The theory endorses additive composition as a reasonable operation for calculating meanings of phrases, and suggests ways to improve additive compositionality, including: transforming entries of distributional word vectors by a function that meets a specific condition, constructing a novel type of vector representations to make additive composition sensitive to word order, and utilizing singular value decomposition to train word vectors.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08411",
        "title": "OntoSeg: a Novel Approach to Text Segmentation using Ontological Similarity",
        "authors": [
            "Mostafa Bayomi",
            "Killian Levacher",
            "M. Rami Ghorab",
            "S\u00e9amus Lawless"
        ],
        "abstract": "Text segmentation (TS) aims at dividing long text into coherent segments which reflect the subtopic structure of the text. It is beneficial to many natural language processing tasks, such as Information Retrieval (IR) and document summarisation. Current approaches to text segmentation are similar in that they all use word-frequency metrics to measure the similarity between two regions of text, so that a document is segmented based on the lexical cohesion between its words. Various NLP tasks are now moving towards the semantic web and ontologies, such as ontology-based IR systems, to capture the conceptualizations associated with user needs and contents. Text segmentation based on lexical cohesion between words is hence not sufficient anymore for such tasks. This paper proposes OntoSeg, a novel approach to text segmentation based on the ontological similarity between text blocks. The proposed method uses ontological similarity to explore conceptual relations between text segments and a Hierarchical Agglomerative Clustering (HAC) algorithm to represent the text as a tree-like hierarchy that is conceptually structured. The rich structure of the created tree further allows the segmentation of text in a linear fashion at various levels of granularity. The proposed method was evaluated on a wellknown dataset, and the results show that using ontological similarity in text segmentation is very promising. Also we enhance the proposed method by combining ontological similarity with lexical similarity and the results show an enhancement of the segmentation quality.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08629",
        "title": "Category Enhanced Word Embedding",
        "authors": [
            "Chunting Zhou",
            "Chonglin Sun",
            "Zhiyuan Liu",
            "Francis C.M. Lau"
        ],
        "abstract": "Distributed word representations have been demonstrated to be effective in capturing semantic and syntactic regularities. Unsupervised representation learning from large unlabeled corpora can learn similar representations for those words that present similar co-occurrence statistics. Besides local occurrence statistics, global topical information is also important knowledge that may help discriminate a word from another. In this paper, we incorporate category information of documents in the learning of word representations and to learn the proposed models in a document-wise manner. Our models outperform several state-of-the-art models in word analogy and word similarity tasks. Moreover, we evaluate the learned word vectors on sentiment analysis and text classification tasks, which shows the superiority of our learned word vectors. We also learn high-quality category embeddings that reflect topical meanings.\n    ",
        "submission_date": "2015-11-27T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08630",
        "title": "A C-LSTM Neural Network for Text Classification",
        "authors": [
            "Chunting Zhou",
            "Chonglin Sun",
            "Zhiyuan Liu",
            "Francis C.M. Lau"
        ],
        "abstract": "Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.\n    ",
        "submission_date": "2015-11-27T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08952",
        "title": "Bootstrapping Ternary Relation Extractors",
        "authors": [
            "Ndapandula Nakashole"
        ],
        "abstract": "Binary relation extraction methods have been widely studied in recent years. However, few methods have been developed for higher n-ary relation extraction. One limiting factor is the effort required to generate training data. For binary relations, one only has to provide a few dozen pairs of entities per relation, as training data. For ternary relations (n=3), each training instance is a triplet of entities, placing a greater cognitive load on people. For example, many people know that Google acquired Youtube but not the dollar amount or the date of the acquisition and many people know that Hillary Clinton is married to Bill Clinton by not the location or date of their wedding. This makes higher n-nary training data generation a time consuming exercise in searching the Web. We present a resource for training ternary relation extractors. This was generated using a minimally supervised yet effective approach. We present statistics on the size and the quality of the dataset.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2019-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09107",
        "title": "Machine Learning Sentiment Prediction based on Hybrid Document Representation",
        "authors": [
            "Panagiotis Stalidis",
            "Maria Giatsoglou",
            "Konstantinos Diamantaras",
            "George Sarigiannidis",
            "Konstantinos Ch. Chatzisavvas"
        ],
        "abstract": "Automated sentiment analysis and opinion mining is a complex process concerning the extraction of useful subjective information from text. The explosion of user generated content on the Web, especially the fact that millions of users, on a daily basis, express their opinions on products and services to blogs, wikis, social networks, message boards, etc., render the reliable, automated export of sentiments and opinions from unstructured text crucial for several commercial applications. In this paper, we present a novel hybrid vectorization approach for textual resources that combines a weighted variant of the popular Word2Vec representation (based on Term Frequency-Inverse Document Frequency) representation and with a Bag- of-Words representation and a vector of lexicon-based sentiment values. The proposed text representation approach is assessed through the application of several machine learning classification algorithms on a dataset that is used extensively in literature for sentiment detection. The classification accuracy derived through the proposed hybrid vectorization approach is higher than when its individual components are used for text represenation, and comparable with state-of-the-art sentiment detection methodologies.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09128",
        "title": "Aspect-based Opinion Summarization with Convolutional Neural Networks",
        "authors": [
            "Haibing Wu",
            "Yiwei Gu",
            "Shangdi Sun",
            "Xiaodong Gu"
        ],
        "abstract": "This paper considers Aspect-based Opinion Summarization (AOS) of reviews on particular products. To enable real applications, an AOS system needs to address two core subtasks, aspect extraction and sentiment classification. Most existing approaches to aspect extraction, which use linguistic analysis or topic modeling, are general across different products but not precise enough or suitable for particular products. Instead we take a less general but more precise scheme, directly mapping each review sentence into pre-defined aspects. To tackle aspect mapping and sentiment classification, we propose two Convolutional Neural Network (CNN) based methods, cascaded CNN and multitask CNN. Cascaded CNN contains two levels of convolutional networks. Multiple CNNs at level 1 deal with aspect mapping task, and a single CNN at level 2 deals with sentiment classification. Multitask CNN also contains multiple aspect CNNs and a sentiment CNN, but different networks share the same word embeddings. Experimental results indicate that both cascaded and multitask CNNs outperform SVM-based methods by large margins. Multitask CNN generally performs better than cascaded CNN.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09376",
        "title": "Modeling Dynamic Relationships Between Characters in Literary Novels",
        "authors": [
            "Snigdha Chaturvedi",
            "Shashank Srivastava",
            "Hal Daume III",
            "Chris Dyer"
        ],
        "abstract": "Studying characters plays a vital role in computationally representing and interpreting narratives. Unlike previous work, which has focused on inferring character roles, we focus on the problem of modeling their relationships. Rather than assuming a fixed relationship for a character pair, we hypothesize that relationships are dynamic and temporally evolve with the progress of the narrative, and formulate the problem of relationship modeling as a structured prediction problem. We propose a semi-supervised framework to learn relationship sequences from fully as well as partially labeled data. We present a Markovian model capable of accumulating historical beliefs about the relationship and status changes. We use a set of rich linguistic and semantically motivated features that incorporate world knowledge to investigate the textual content of narrative. We empirically demonstrate that such a framework outperforms competitive baselines.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09392",
        "title": "Enhancements in statistical spoken language translation by de-normalization of ASR results",
        "authors": [
            "Agnieszka Wo\u0142k",
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "Spoken language translation (SLT) has become very important in an increasingly globalized world. Machine translation (MT) for automatic speech recognition (ASR) systems is a major challenge of great interest. This research investigates that automatic sentence segmentation of speech that is important for enriching speech recognition output and for aiding downstream language processing. This article focuses on the automatic sentence segmentation of speech and improving MT results. We explore the problem of identifying sentence boundaries in the transcriptions produced by automatic speech recognition systems in the Polish language. We also experiment with reverse normalization of the recognized speech samples.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00103",
        "title": "Multilingual Language Processing From Bytes",
        "authors": [
            "Dan Gillick",
            "Cliff Brunk",
            "Oriol Vinyals",
            "Amarnag Subramanya"
        ],
        "abstract": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of- the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \"from scratch\" in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00112",
        "title": "Inferring Interpersonal Relations in Narrative Summaries",
        "authors": [
            "Shashank Srivastava",
            "Snigdha Chaturvedi",
            "Tom Mitchell"
        ],
        "abstract": "Characterizing relationships between people is fundamental for the understanding of narratives. In this work, we address the problem of inferring the polarity of relationships between people in narrative summaries. We formulate the problem as a joint structured prediction for each narrative, and present a model that combines evidence from linguistic and semantic features, as well as features based on the structure of the social community in the text. We also provide a clustering-based approach that can exploit regularities in narrative types. e.g., learn an affinity for love-triangles in romantic stories. On a dataset of movie summaries from Wikipedia, our structured models provide more than a 30% error-reduction over a competitive baseline that considers pairs of characters in isolation.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00170",
        "title": "Augmenting Phrase Table by Employing Lexicons for Pivot-based SMT",
        "authors": [
            "Yiming Cui",
            "Conghui Zhu",
            "Xiaoning Zhu",
            "Tiejun Zhao"
        ],
        "abstract": "Pivot language is employed as a way to solve the data sparseness problem in machine translation, especially when the data for a particular language pair does not exist. The combination of source-to-pivot and pivot-to-target translation models can induce a new translation model through the pivot language. However, the errors in two models may compound as noise, and still, the combined model may suffer from a serious phrase sparsity problem. In this paper, we directly employ the word lexical model in IBM models as an additional resource to augment pivot phrase table. In addition, we also propose a phrase table pruning method which takes into account both of the source and target phrasal coverage. Experimental result shows that our pruning method significantly outperforms the conventional one, which only considers source side phrasal coverage. Furthermore, by including the entries in the lexicon model, the phrase coverage increased, and we achieved improved results in Chinese-to-Japanese translation using English as pivot language.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00177",
        "title": "LSTM Neural Reordering Feature for Statistical Machine Translation",
        "authors": [
            "Yiming Cui",
            "Shijin Wang",
            "Jianfeng Li"
        ],
        "abstract": "Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00531",
        "title": "Benchmarking sentiment analysis methods for large-scale texts: A case for using continuum-scored words and word shift graphs",
        "authors": [
            "Andrew J. Reagan",
            "Brian Tivnan",
            "Jake Ryland Williams",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "The emergence and global adoption of social media has rendered possible the real-time estimation of population-scale sentiment, bearing profound implications for our understanding of human behavior. Given the growing assortment of sentiment measuring instruments, comparisons between them are evidently required. Here, we perform detailed tests of 6 dictionary-based methods applied to 4 different corpora, and briefly examine a further 20 methods. We show that a dictionary-based method will only perform both reliably and meaningfully if (1) the dictionary covers a sufficiently large enough portion of a given text's lexicon when weighted by word usage frequency; and (2) words are scored on a continuous scale.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00576",
        "title": "Probabilistic Latent Semantic Analysis (PLSA) untuk Klasifikasi Dokumen Teks Berbahasa Indonesia",
        "authors": [
            "Derwin Suhartono"
        ],
        "abstract": "One task that is included in managing documents is how to find substantial information inside. Topic modeling is a technique that has been developed to produce document representation in form of keywords. The keywords will be used in the indexing process and document retrieval as needed by users. In this research, we will discuss specifically about Probabilistic Latent Semantic Analysis (PLSA). It will cover PLSA mechanism which involves Expectation Maximization (EM) as the training algorithm, how to conduct testing, and obtain the accuracy result.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00578",
        "title": "Klasifikasi Komponen Argumen Secara Otomatis pada Dokumen Teks berbentuk Esai Argumentatif",
        "authors": [
            "Derwin Suhartono"
        ],
        "abstract": "By automatically recognize argument component, essay writers can do some inspections to texts that they have written. It will assist essay scoring process objectively and precisely because essay grader is able to see how well the argument components are constructed. Some reseachers have tried to do argument detection and classification along with its implementation in some domains. The common approach is by doing feature extraction to the text. Generally, the features are structural, lexical, syntactic, indicator, and contextual. In this research, we add new feature to the existing features. It adopts keywords list by Knott and Dale (1993). The experiment result shows the argument classification achieves 72.45% accuracy. Moreover, we still get the same accuracy without the keyword lists. This concludes that the keyword lists do not affect significantly to the features. All features are still weak to classify major claim and claim, so we need other features which are useful to differentiate those two kind of argument components.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00728",
        "title": "Annotating Character Relationships in Literary Texts",
        "authors": [
            "Philip Massey",
            "Patrick Xia",
            "David Bamman",
            "Noah A. Smith"
        ],
        "abstract": "We present a dataset of manually annotated relationships between characters in literary texts, in order to support the training and evaluation of automatic methods for relation type prediction in this domain (Makazhanov et al., 2014; Kokkinakis, 2013) and the broader computational analysis of literary character (Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and Gurevych, 2015). In this work, we solicit annotations from workers on Amazon Mechanical Turk for 109 texts ranging from Homer's _Iliad_ to Joyce's _Ulysses_ on four dimensions of interest: for a given pair of characters, we collect judgments as to the coarse-grained category (professional, social, familial), fine-grained category (friend, lover, parent, rival, employer), and affinity (positive, negative, neutral) that describes their primary relationship in a text. We do not assume that this relationship is static; we also collect judgments as to whether it changes at any point in the course of the text.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01100",
        "title": "Effective LSTMs for Target-Dependent Sentiment Classification",
        "authors": [
            "Duyu Tang",
            "Bing Qin",
            "Xiaocheng Feng",
            "Ting Liu"
        ],
        "abstract": "Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01173",
        "title": "Building Memory with Concept Learning Capabilities from Large-scale Knowledge Base",
        "authors": [
            "Jiaxin Shi",
            "Jun Zhu"
        ],
        "abstract": "We present a new perspective on neural knowledge base (KB) embeddings, from which we build a framework that can model symbolic knowledge in the KB together with its learning process. We show that this framework well regularizes previous neural KB embedding model for superior performance in reasoning tasks, while having the capabilities of dealing with unseen entities, that is, to learn their embeddings from natural language descriptions, which is very like human's behavior of learning semantic concepts.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01283",
        "title": "Predicting the top and bottom ranks of billboard songs using Machine Learning",
        "authors": [
            "Vivek Datla",
            "Abhinav Vishnu"
        ],
        "abstract": "The music industry is a $130 billion industry. Predicting whether a song catches the pulse of the audience impacts the industry. In this paper we analyze language inside the lyrics of the songs using several computational linguistic algorithms and predict whether a song would make to the top or bottom of the billboard rankings based on the language features. We trained and tested an SVM classifier with a radial kernel function on the linguistic features. Results indicate that we can classify whether a song belongs to top and bottom of the billboard charts with a precision of 0.76.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01337",
        "title": "Neural Generative Question Answering",
        "authors": [
            "Jun Yin",
            "Xin Jiang",
            "Zhengdong Lu",
            "Lifeng Shang",
            "Hang Li",
            "Xiaoming Li"
        ],
        "abstract": "This paper presents an end-to-end neural network model, named Neural Generative Question Answering (GENQA), that can generate answers to simple factoid questions, based on the facts in a knowledge-base. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to enquire the knowledge-base, and is trained on a corpus of question-answer pairs, with their associated triples in the knowledge-base. Empirical study shows the proposed model can effectively deal with the variations of questions and answers, and generate right and natural answers by referring to the facts in the knowledge-base. The experiment on question answering demonstrates that the proposed model can outperform an embedding-based QA model as well as a neural dialogue model trained on the same data.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01384",
        "title": "Topic segmentation via community detection in complex networks",
        "authors": [
            "Henrique F. de Arruda",
            "Luciano da F. Costa",
            "Diego R. Amancio"
        ],
        "abstract": "Many real systems have been modelled in terms of network concepts, and written texts are a particular example of information networks. In recent years, the use of network methods to analyze language has allowed the discovery of several interesting findings, including the proposition of novel models to explain the emergence of fundamental universal patterns. While syntactical networks, one of the most prevalent networked models of written texts, display both scale-free and small-world properties, such representation fails in capturing other textual features, such as the organization in topics or subjects. In this context, we propose a novel network representation whose main purpose is to capture the semantical relationships of words in a simple way. To do so, we link all words co-occurring in the same semantic context, which is defined in a threefold way. We show that the proposed representations favours the emergence of communities of semantically related words, and this feature may be used to identify relevant topics. The proposed methodology to detect topics was applied to segment selected Wikipedia articles. We have found that, in general, our methods outperform traditional bag-of-words representations, which suggests that a high-level textual representation may be useful to study semantical features of texts.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01409",
        "title": "What Makes it Difficult to Understand a Scientific Literature?",
        "authors": [
            "Mengyun Cao",
            "Jiao Tian",
            "Dezhi Cheng",
            "Jin Liu",
            "Xiaoping Sun"
        ],
        "abstract": "In the artificial intelligence area, one of the ultimate goals is to make computers understand human language and offer assistance. In order to achieve this ideal, researchers of computer science have put forward a lot of models and algorithms attempting at enabling the machine to analyze and process human natural language on different levels of semantics. Although recent progress in this field offers much hope, we still have to ask whether current research can provide assistance that people really desire in reading and comprehension. To this end, we conducted a reading comprehension test on two scientific papers which are written in different styles. We use the semantic link models to analyze the understanding obstacles that people will face in the process of reading and figure out what makes it difficult for human to understand a scientific literature. Through such analysis, we summarized some characteristics and problems which are reflected by people with different levels of knowledge on the comprehension of difficult science and technology literature, which can be modeled in semantic link network. We believe that these characteristics and problems will help us re-examine the existing machine models and are helpful in the designing of new one.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01587",
        "title": "Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text",
        "authors": [
            "Sahil Garg",
            "Aram Galstyan",
            "Ulf Hermjakob",
            "Daniel Marcu"
        ],
        "abstract": "We advance the state of the art in biomolecular interaction extraction with three contributions: (i) We show that deep, Abstract Meaning Representations (AMR) significantly improve the accuracy of a biomolecular interaction extraction system when compared to a baseline that relies solely on surface- and syntax-based features; (ii) In contrast with previous approaches that infer relations on a sentence-by-sentence basis, we expand our framework to enable consistent predictions over sets of sentences (documents); (iii) We further modify and expand a graph kernel learning framework to enable concurrent exploitation of automatically induced AMR (semantic) and dependency structure (syntactic) representations. Our experiments show that our approach yields interaction extraction systems that are more robust in environments where there is a significant mismatch between training and test conditions.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01639",
        "title": "PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by Comparable Corpora",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "In this paper, we attempt to improve Statistical Machine Translation (SMT) systems on a very diverse set of language pairs (in both directions): Czech - English, Vietnamese - English, French - English and German - English. To accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our SMT systems. Innovative tools and data adaptation techniques were employed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign were used to train language models, and to develop, tune, and test the system. In addition, we prepared Wikipedia-based comparable corpora for use with our SMT system. This data was specified as permissible for the IWSLT 2015 evaluation. We explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. To evaluate the effects of different preparations on translation results, we conducted experiments and used the BLEU, NIST and TER metrics. Our results indicate that our approach produced a positive impact on SMT quality.\n    ",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01641",
        "title": "Unsupervised comparable corpora preparation and exploration for bi-lingual translation equivalents",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Krzysztof Marasek"
        ],
        "abstract": "The multilingual nature of the world makes translation a crucial requirement today. Parallel dictionaries constructed by humans are a widely-available resource, but they are limited and do not provide enough coverage for good quality translation purposes, due to out-of-vocabulary words and neologisms. This motivates the use of statistical translation systems, which are unfortunately dependent on the quantity and quality of training data. Such systems have a very limited availability especially for some languages and very narrow text domains. In this research we present our improvements to current comparable corpora mining methodologies by re- implementation of the comparison algorithms (using Needleman-Wunch algorithm), introduction of a tuning script and computation time improvement by GPU acceleration. Experiments are carried out on bilingual data extracted from the Wikipedia, on various domains. For the Wikipedia itself, additional cross-lingual comparison heuristics were introduced. The modifications made a positive impact on the quality and quantity of mined data and on the translation quality.\n    ",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01712",
        "title": "Generating News Headlines with Recurrent Neural Networks",
        "authors": [
            "Konstantin Lopyrev"
        ],
        "abstract": "We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.\n    ",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01768",
        "title": "Want Answers? A Reddit Inspired Study on How to Pose Questions",
        "authors": [
            "Danish",
            "Yogesh Dahiya",
            "Partha Talukdar"
        ],
        "abstract": "Questions form an integral part of our everyday communication, both offline and online. Getting responses to our questions from others is fundamental to satisfying our information need and in extending our knowledge boundaries. A question may be represented using various factors such as social, syntactic, semantic, etc. We hypothesize that these factors contribute with varying degrees towards getting responses from others for a given question. We perform a thorough empirical study to measure effects of these factors using a novel question and answer dataset from the website ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2015-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01818",
        "title": "SentiBench - a benchmark comparison of state-of-the-practice sentiment analysis methods",
        "authors": [
            "Filipe Nunes Ribeiro",
            "Matheus Ara\u00fajo",
            "Pollyanna Gon\u00e7alves",
            "Fabr\u00edcio Benevenuto",
            "Marcos Andr\u00e9 Gon\u00e7alves"
        ],
        "abstract": "In the last few years thousands of scientific papers have investigated sentiment analysis, several startups that measure opinions on real data have emerged and a number of innovative products related to this theme have been developed. There are multiple methods for measuring sentiments, including lexical-based and supervised machine learning methods. Despite the vast interest on the theme and wide popularity of some methods, it is unclear which one is better for identifying the polarity (i.e., positive or negative) of a message. Accordingly, there is a strong need to conduct a thorough apple-to-apple comparison of sentiment analysis methods, \\textit{as they are used in practice}, across multiple datasets originated from different data sources. Such a comparison is key for understanding the potential limitations, advantages, and disadvantages of popular methods. This article aims at filling this gap by presenting a benchmark comparison of twenty-four popular sentiment analysis methods (which we call the state-of-the-practice methods). Our evaluation is based on a benchmark of eighteen labeled datasets, covering messages posted on social networks, movie and product reviews, as well as opinions and comments in news articles. Our results highlight the extent to which the prediction performance of these methods varies considerably across datasets. Aiming at boosting the development of this research area, we open the methods' codes and datasets used in this article, deploying them in a benchmark system, which provides an open API for accessing and comparing sentence-level sentiment analysis methods.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01882",
        "title": "THCHS-30 : A Free Chinese Speech Corpus",
        "authors": [
            "Dong Wang",
            "Xuewei Zhang"
        ],
        "abstract": "Speech data is crucially important for speech recognition research. There are quite some speech databases that can be purchased at prices that are reasonable for most research institutes. However, for young people who just start research activities or those who just gain initial interest in this direction, the cost for data is still an annoying barrier. We support the `free data' movement in speech recognition: research institutes (particularly supported by public funds) publish their data freely so that new researchers can obtain sufficient data to kick of their career. In this paper, we follow this trend and release a free Chinese speech database THCHS-30 that can be used to build a full- edged Chinese speech recognition system. We report the baseline system established with this database, including the performance under highly noisy conditions.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02009",
        "title": "Jointly Modeling Topics and Intents with Global Order Structure",
        "authors": [
            "Bei Chen",
            "Jun Zhu",
            "Nan Yang",
            "Tian Tian",
            "Ming Zhou",
            "Bo Zhang"
        ],
        "abstract": "Modeling document structure is of great importance for discourse analysis and related applications. The goal of this research is to capture the document intent structure by modeling documents as a mixture of topic words and rhetorical words. While the topics are relatively unchanged through one document, the rhetorical functions of sentences usually change following certain orders in discourse. We propose GMM-LDA, a topic modeling based Bayesian unsupervised model, to analyze the document intent structure cooperated with order information. Our model is flexible that has the ability to combine the annotations and do supervised learning. Additionally, entropic regularization can be introduced to model the significant divergence between topics and intents. We perform experiments in both unsupervised and supervised settings, results show the superiority of our model over several state-of-the-art baselines.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02433",
        "title": "Minimum Risk Training for Neural Machine Translation",
        "authors": [
            "Shiqi Shen",
            "Yong Cheng",
            "Zhongjun He",
            "Wei He",
            "Hua Wu",
            "Maosong Sun",
            "Yang Liu"
        ],
        "abstract": "We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02595",
        "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
        "authors": [
            "Dario Amodei",
            "Rishita Anubhai",
            "Eric Battenberg",
            "Carl Case",
            "Jared Casper",
            "Bryan Catanzaro",
            "Jingdong Chen",
            "Mike Chrzanowski",
            "Adam Coates",
            "Greg Diamos",
            "Erich Elsen",
            "Jesse Engel",
            "Linxi Fan",
            "Christopher Fougner",
            "Tony Han",
            "Awni Hannun",
            "Billy Jun",
            "Patrick LeGresley",
            "Libby Lin",
            "Sharan Narang",
            "Andrew Ng",
            "Sherjil Ozair",
            "Ryan Prenger",
            "Jonathan Raiman",
            "Sanjeev Satheesh",
            "David Seetapun",
            "Shubho Sengupta",
            "Yi Wang",
            "Zhiqian Wang",
            "Chong Wang",
            "Bo Xiao",
            "Dani Yogatama",
            "Jun Zhan",
            "Zhenyao Zhu"
        ],
        "abstract": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2015-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03465",
        "title": "Mined Semantic Analysis: A New Concept Space Model for Semantic Representation of Textual Data",
        "authors": [
            "Walid Shalaby",
            "Wlodek Zadrozny"
        ],
        "abstract": "Mined Semantic Analysis (MSA) is a novel concept space model which employs unsupervised learning to generate semantic representations of text. MSA represents textual structures (terms, phrases, documents) as a Bag of Concepts (BoC) where concepts are derived from concept rich encyclopedic corpora. Traditional concept space models exploit only target corpus content to construct the concept space. MSA, alternatively, uncovers implicit relations between concepts by mining for their associations (e.g., mining Wikipedia's \"See also\" link graph). We evaluate MSA's performance on benchmark datasets for measuring semantic relatedness of words and sentences. Empirical results show competitive performance of MSA compared to prior state-of-the-art methods. Additionally, we introduce the first analytical study to examine statistical significance of results reported by different semantic relatedness methods. Our study shows that, the nuances of results across top performing methods could be statistically insignificant. The study positions MSA as one of state-of-the-art methods for measuring semantic relatedness, besides the inherent interpretability and simplicity of the generated semantic representation.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2017-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03549",
        "title": "Words are not Equal: Graded Weighting Model for building Composite Document Vectors",
        "authors": [
            "Pranjal Singh",
            "Amitabha Mukerjee"
        ],
        "abstract": "Despite the success of distributional semantics, composing phrases from word vectors remains an important challenge. Several methods have been tried for benchmark tasks such as sentiment classification, including word vector averaging, matrix-vector approaches based on parsing, and on-the-fly learning of paragraph vectors. Most models usually omit stop words from the composition. Instead of such an yes-no decision, we consider several graded schemes where words are weighted according to their discriminatory relevance with respect to its use in the document (e.g., idf). Some of these methods (particularly tf-idf) are seen to result in a significant improvement in performance over prior state of the art. Further, combining such approaches into an ensemble based on alternate classifiers such as the RNN model, results in an 1.6% performance improvement on the standard IMDB movie review dataset, and a 7.01% improvement on Amazon product reviews. Since these are language free models and can be obtained in an unsupervised manner, they are of interest also for under-resourced languages such as Hindi as well and many more languages. We demonstrate the language free aspects by showing a gain of 12% for two review datasets over earlier results, and also release a new larger dataset for future testing (Singh,2015).\n    ",
        "submission_date": "2015-12-11T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03950",
        "title": "A Hidden Markov Model Based System for Entity Extraction from Social Media English Text at FIRE 2015",
        "authors": [
            "Kamal Sarkar"
        ],
        "abstract": "This paper presents the experiments carried out by us at Jadavpur University as part of the participation in FIRE 2015 task: Entity Extraction from Social Media Text - Indian Languages (ESM-IL). The tool that we have developed for the task is based on Trigram Hidden Markov Model that utilizes information like gazetteer list, POS tag and some other word level features to enhance the observation probabilities of the known tokens as well as unknown tokens. We submitted runs for English only. A statistical HMM (Hidden Markov Models) based model has been used to implement our system. The system has been trained and tested on the datasets released for FIRE 2015 task: Entity Extraction from Social Media Text - Indian Languages (ESM-IL). Our system is the best performer for English language and it obtains precision, recall and F-measures of 61.96, 39.46 and 48.21 respectively.\n    ",
        "submission_date": "2015-12-12T00:00:00",
        "last_modified_date": "2015-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04092",
        "title": "Stack Exchange Tagger",
        "authors": [
            "Sanket Mehta",
            "Shagun Sodhani"
        ],
        "abstract": "The goal of our project is to develop an accurate tagger for questions posted on Stack Exchange. Our problem is an instance of the more general problem of developing accurate classifiers for large scale text datasets. We are tackling the multilabel classification problem where each item (in this case, question) can belong to multiple classes (in this case, tags). We are predicting the tags (or keywords) for a particular Stack Exchange post given only the question text and the title of the post. In the process, we compare the performance of Support Vector Classification (SVC) for different kernel functions, loss function, etc. We found linear SVC with Crammer Singer technique produces best results.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04280",
        "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition",
        "authors": [
            "Liang Lu",
            "Steve Renals"
        ],
        "abstract": "For speech recognition, deep neural networks (DNNs) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models, DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g., mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are {\\it thinner} and {\\it deeper}, and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 70 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2017-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04419",
        "title": "Sentence Entailment in Compositional Distributional Semantics",
        "authors": [
            "Esma Balkir",
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "Distributional semantic models provide vector representations for words by gathering co-occurrence frequencies from corpora of text. Compositional distributional models extend these from words to phrases and sentences. In categorical compositional distributional semantics, phrase and sentence representations are functions of their grammatical structure and representations of the words therein. In this setting, grammatical structures are formalised by morphisms of a compact closed category and meanings of words are formalised by objects of the same category. These can be instantiated in the form of vectors or density matrices. This paper concerns the applications of this model to phrase and sentence level entailment. We argue that entropy-based distances of vectors and density matrices provide a good candidate to measure word-level entailment, show the advantage of density matrices over vectors for word level entailments, and prove that these distances extend compositionally from words to phrases and sentences. We exemplify our theoretical constructions on real data and a toy entailment dataset and provide preliminary experimental evidence.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2018-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04650",
        "title": "Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation",
        "authors": [
            "Yong Cheng",
            "Shiqi Shen",
            "Zhongjun He",
            "Wei He",
            "Hua Wu",
            "Maosong Sun",
            "Yang Liu"
        ],
        "abstract": "The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently,our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on Chinese-English and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04906",
        "title": "Strategies for Training Large Vocabulary Neural Language Models",
        "authors": [
            "Welin Chen",
            "David Grangier",
            "Michael Auli"
        ],
        "abstract": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05030",
        "title": "Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised Learning",
        "authors": [
            "Manaal Faruqui",
            "Ryan McDonald",
            "Radu Soricut"
        ],
        "abstract": "Morpho-syntactic lexicons provide information about the morphological and syntactic roles of words in a language. Such lexicons are not available for all languages and even when available, their coverage can be limited. We present a graph-based semi-supervised learning method that uses the morphological, syntactic and semantic relations between words to automatically construct wide coverage lexicons from small seed sets. Our method is language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing.\n    ",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2016-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05193",
        "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence's representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNN; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNN achieves state-of-the-art performance on AS, PI and TE tasks.\n    ",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2018-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05582",
        "title": "Kauffman's adjacent possible in word order evolution",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Word order evolution has been hypothesized to be constrained by a word order permutation ring: transitions involving orders that are closer in the permutation ring are more likely. The hypothesis can be seen as a particular case of Kauffman's adjacent possible in word order evolution. Here we consider the problem of the association of the six possible orders of S, V and O to yield a couple of primary alternating orders as a window to word order evolution. We evaluate the suitability of various competing hypotheses to predict one member of the couple from the other with the help of information theoretic model selection. Our ensemble of models includes a six-way model that is based on the word order permutation ring (Kauffman's adjacent possible) and another model based on the dual two-way of standard typology, that reduces word order to basic orders preferences (e.g., a preference for SV over VS and another for SO over OS). Our analysis indicates that the permutation ring yields the best model when favoring parsimony strongly, providing support for Kauffman's general view and a six-way typology.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2016-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05670",
        "title": "Towards automating the generation of derivative nouns in Sanskrit by simulating Panini",
        "authors": [
            "Amrith Krishna",
            "Pawan Goyal"
        ],
        "abstract": "About 1115 rules in Astadhyayi from A.4.1.76 to A.5.4.160 deal with generation of derivative nouns, making it one of the largest topical sections in Astadhyayi, called as the Taddhita section owing to the head rule A.4.1.76. This section is a systematic arrangement of rules that enumerates various affixes that are used in the derivation under specific semantic relations. We propose a system that automates the process of generation of derivative nouns as per the rules in Astadhyayi. The proposed system follows a completely object oriented approach, that models each rule as a class of its own and then groups them as rule groups. The rule groups are decided on the basis of selective grouping of rules by virtue of anuvrtti. The grouping of rules results in an inheritance network of rules which is a directed acyclic graph. Every rule group has a head rule and the head rule notifies all the direct member rules of the group about the environment which contains all the details about data entities, participating in the derivation process. The system implements this mechanism using multilevel inheritance and observer design patterns. The system focuses not only on generation of the desired final form, but also on the correctness of sequence of rules applied to make sure that the derivation has taken place in strict adherence to Astadhyayi. The proposed system's design allows to incorporate various conflict resolution methods mentioned in authentic texts and hence the effectiveness of those rules can be validated with the results from the system. We also present cases where we have checked the applicability of the system with the rules which are not specifically applicable to derivation of derivative nouns, in order to see the effectiveness of the proposed schema as a generic system for modeling Astadhyayi.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05726",
        "title": "Semi-supervised Question Retrieval with Gated Convolutions",
        "authors": [
            "Tao Lei",
            "Hrishikesh Joshi",
            "Regina Barzilay",
            "Tommi Jaakkola",
            "Katerina Tymoshenko",
            "Alessandro Moschitti",
            "Lluis Marquez"
        ],
        "abstract": "Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs).\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05742",
        "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems",
        "authors": [
            "Iulian Vlad Serban",
            "Ryan Lowe",
            "Peter Henderson",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "abstract": "During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research in this area, we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems. We discuss important characteristics of these datasets, how they can be used to learn diverse dialogue strategies, and their other potential uses. We also examine methods for transfer learning between datasets and the use of external knowledge. Finally, we discuss appropriate choice of evaluation metrics for the learning objective.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05919",
        "title": "A Planning based Framework for Essay Generation",
        "authors": [
            "Bing Qin",
            "Duyu Tang",
            "Xinwei Geng",
            "Dandan Ning",
            "Jiahao Liu",
            "Ting Liu"
        ],
        "abstract": "Generating an article automatically with computer program is a challenging task in artificial intelligence and natural language processing. In this paper, we target at essay generation, which takes as input a topic word in mind and generates an organized article under the theme of the topic. We follow the idea of text planning \\cite{Reiter1997} and develop an essay generation framework. The framework consists of three components, including topic understanding, sentence extraction and sentence reordering. For each component, we studied several statistical algorithms and empirically compared between them in terms of qualitative or quantitative analysis. Although we run experiments on Chinese corpus, the method is language independent and can be easily adapted to other language. We lay out the remaining challenges and suggest avenues for future research.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06110",
        "title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning",
        "authors": [
            "Manaal Faruqui",
            "Yulia Tsvetkov",
            "Graham Neubig",
            "Chris Dyer"
        ],
        "abstract": "Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06612",
        "title": "Backward and Forward Language Modeling for Constrained Sentence Generation",
        "authors": [
            "Lili Mou",
            "Rui Yan",
            "Ge Li",
            "Lu Zhang",
            "Zhi Jin"
        ],
        "abstract": "Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamically) fed to RNN's hidden layer. In many applications, we are likely to impose hard constraints on the generated texts, i.e., a particular word must appear in the sentence. Unfortunately, existing approaches could not solve this problem. In this paper, we propose a novel backward and forward language model. Provided a specific word, we use RNNs to generate previous words and future words, either simultaneously or asynchronously, resulting in two model variants. In this way, the given word could appear at any position in the sentence. Experimental results show that the generated texts are comparable to sequential LMs in quality.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2016-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06643",
        "title": "The 2015 Sheffield System for Transcription of Multi-Genre Broadcast Media",
        "authors": [
            "Oscar Saz",
            "Mortaza Doulaty",
            "Salil Deena",
            "Rosanna Milner",
            "Raymond W.M. Ng",
            "Madina Hasan",
            "Yulan Liu",
            "Thomas Hain"
        ],
        "abstract": "We describe the University of Sheffield system for participation in the 2015 Multi-Genre Broadcast (MGB) challenge task of transcribing multi-genre broadcast shows. Transcription was one of four tasks proposed in the MGB challenge, with the aim of advancing the state of the art of automatic speech recognition, speaker diarisation and automatic alignment of subtitles for broadcast media. Four topics are investigated in this work: Data selection techniques for training with unreliable data, automatic speech segmentation of broadcast media shows, acoustic modelling and adaptation in highly variable environments, and language modelling of multi-genre shows. The final system operates in multiple passes, using an initial unadapted decoding stage to refine segmentation, followed by three adapted passes: a hybrid DNN pass with input features normalised by speaker-based cepstral normalisation, another hybrid stage with input features normalised by speaker feature-MLLR transformations, and finally a bottleneck-based tandem stage with noise and speaker factorisation. The combination of these three system outputs provides a final error rate of 27.5% on the official development set, consisting of 47 multi-genre shows.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08066",
        "title": "The Improvement of Negative Sentences Translation in English-to-Korean Machine Translation",
        "authors": [
            "Chung-Hyok Jang",
            "Kwang-Hyok Kim"
        ],
        "abstract": "This paper describes the algorithm for translating English negative sentences into Korean in English-Korean Machine Translation (EKMT). The proposed algorithm is based on the comparative study of English and Korean negative sentences. The earlier translation software cannot translate English negative sentences into accurate Korean equivalents. We established a new algorithm for the negative sentence translation and evaluated it.\n    ",
        "submission_date": "2015-12-26T00:00:00",
        "last_modified_date": "2015-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08183",
        "title": "Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews",
        "authors": [
            "Bofang Li",
            "Tao Liu",
            "Xiaoyong Du",
            "Deyuan Zhang",
            "Zhe Zhao"
        ],
        "abstract": "Despite the loss of semantic information, bag-of-ngram based methods still achieve state-of-the-art results for tasks such as sentiment classification of long movie reviews. Many document embeddings methods have been proposed to capture semantics, but they still can't outperform bag-of-ngram based methods on this task. In this paper, we modify the architecture of the recently proposed Paragraph Vector, allowing it to learn document vectors by predicting not only words, but n-gram features as well. Our model is able to capture both semantics and word order in documents while keeping the expressive power of learned vectors. Experimental results on IMDB movie review dataset shows that our model outperforms previous deep learning models and bag-of-ngram based models due to the above advantages. More robust results are also obtained when our model is combined with other models. The source code of our model will be also published together with this paper.\n    ",
        "submission_date": "2015-12-27T00:00:00",
        "last_modified_date": "2016-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08347",
        "title": "Communicating with sentences: A multi-word naming game model",
        "authors": [
            "Yang Lou",
            "Guanrong Chen",
            "Jianwei Hu"
        ],
        "abstract": "Naming game simulates the process of naming an object by a single word, in which a population of communicating agents can reach global consensus asymptotically through iteratively pair-wise conversations. We propose an extension of the single-word model to a multi-word naming game (MWNG), simulating the case of describing a complex object by a sentence (multiple words). Words are defined in categories, and then organized as sentences by combining them from different categories. We refer to a formatted combination of several words as a pattern. In such an MWNG, through a pair-wise conversation, it requires the hearer to achieve consensus with the speaker with respect to both every single word in the sentence as well as the sentence pattern, so as to guarantee the correct meaning of the saying, otherwise, they fail reaching consensus in the interaction. We validate the model in three typical topologies as the underlying communication network, and employ both conventional and man-designed patterns in performing the MWNG.\n    ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08422",
        "title": "Natural Language Inference by Tree-Based Convolution and Heuristic Matching",
        "authors": [
            "Lili Mou",
            "Rui Men",
            "Ge Li",
            "Yan Xu",
            "Lu Zhang",
            "Rui Yan",
            "Zhi Jin"
        ],
        "abstract": "In this paper, we propose the TBCNN-pair model to recognize entailment and contradiction between two sentences. In our model, a tree-based convolutional neural network (TBCNN) captures sentence-level semantics; then heuristic matching layers like concatenation, element-wise product/difference combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin.\n    ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08849",
        "title": "Learning Natural Language Inference with LSTM",
        "authors": [
            "Shuohang Wang",
            "Jing Jiang"
        ],
        "abstract": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI). In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a match-LSTM to perform word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. On the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the state of the art.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08903",
        "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network",
        "authors": [
            "Kyuyeon Hwang",
            "Minjae Lee",
            "Wonyong Sung"
        ],
        "abstract": "In this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-to-end trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, keywords can easily be added and modified by editing the text based keyword list without retraining the RNN. Moreover, the unidirectional RNN processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished. Experimental results show that the proposed keyword spotter significantly outperforms the deep neural network (DNN) and hidden Markov model (HMM) based keyword-filler model even with less computations.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2015-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00311",
        "title": "QANUS: An Open-source Question-Answering Platform",
        "authors": [
            "Jun-Ping Ng",
            "Min-Yen Kan"
        ],
        "abstract": "In this paper, we motivate the need for a publicly available, generic software framework for question-answering (QA) systems. We present an open-source QA framework QANUS which researchers can leverage on to build new QA systems easily and rapidly. The framework implements much of the code that will otherwise have been repeated across different QA systems. To demonstrate the utility and practicality of the framework, we further present a fully functioning factoid QA system QA-SYS built on top of QANUS.\n    ",
        "submission_date": "2015-01-01T00:00:00",
        "last_modified_date": "2015-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00657",
        "title": "Cross-language Wikipedia Editing of Okinawa, Japan",
        "authors": [
            "Scott A. Hale"
        ],
        "abstract": "This article analyzes users who edit Wikipedia articles about Okinawa, Japan, in English and Japanese. It finds these users are among the most active and dedicated users in their primary languages, where they make many large, high-quality edits. However, when these users edit in their non-primary languages, they tend to make edits of a different type that are overall smaller in size and more often restricted to the narrow set of articles that exist in both languages. Design changes to motivate wider contributions from users in their non-primary languages and to encourage multilingual users to transfer more information across language divides are presented.\n    ",
        "submission_date": "2015-01-04T00:00:00",
        "last_modified_date": "2015-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00960",
        "title": "Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution",
        "authors": [
            "Eitan Adam Pechenick",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "It is tempting to treat frequency trends from the Google Books data sets as indicators of the \"true\" popularity of various words and phrases. Doing so allows us to draw quantitatively strong conclusions about the evolution of cultural perception of a given topic, such as time or gender. However, the Google Books corpus suffers from a number of limitations which make it an obscure mask of cultural popularity. A primary issue is that the corpus is in effect a library, containing one of each book. A single, prolific author is thereby able to noticeably insert new phrases into the Google Books lexicon, whether the author is widely read or not. With this understood, the Google Books corpus remains an important data set to be considered more lexicon-like than text-like. Here, we show that a distinct problematic feature arises from the inclusion of scientific texts, which have become an increasingly substantive portion of the corpus throughout the 1900s. The result is a surge of phrases typical to academic articles but less common in general, such as references to time in the form of citations. We highlight these dynamics by examining and comparing major contributions to the statistical divergence of English data sets between decades in the period 1800--2000. We find that only the English Fiction data set from the second version of the corpus is not heavily affected by professional texts, in clear contrast to the first version of the fiction data set and both unfiltered English data sets. Our findings emphasize the need to fully characterize the dynamics of the Google Books corpus before using these data sets to draw broad conclusions about cultural and linguistic evolution.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2020-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01252",
        "title": "Optimisation using Natural Language Processing: Personalized Tour Recommendation for Museums",
        "authors": [
            "Mayeul Mathias",
            "Assema Moussa",
            "Fen Zhou",
            "Juan-Manuel Torres-Moreno",
            "Marie-Sylvie Poli",
            "Didier Josselin",
            "Marc El-B\u00e8ze",
            "Andr\u00e9a Carneiro Linhares",
            "Francoise Rigat"
        ],
        "abstract": "This paper proposes a new method to provide personalized tour recommendation for museum visits. It combines an optimization of preference criteria of visitors with an automatic extraction of artwork importance from museum information based on Natural Language Processing using textual energy. This project includes researchers from computer and social sciences. Some results are obtained with numerical experiments. They show that our model clearly improves the satisfaction of the visitor who follows the proposed tour. This work foreshadows some interesting outcomes and applications about on-demand personalized visit of museums in a very near future.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01318",
        "title": "Arabic Text Categorization Algorithm using Vector Evaluation Method",
        "authors": [
            "Ashraf Odeh",
            "Aymen Abu-Errub",
            "Qusai Shambour",
            "Nidal Turab"
        ],
        "abstract": "Text categorization is the process of grouping documents into categories based on their contents. This process is important to make information retrieval easier, and it became more important due to the huge textual information available online. The main problem in text categorization is how to improve the classification accuracy. Although Arabic text categorization is a new promising field, there are a few researches in this field. This paper proposes a new method for Arabic text categorization using vector evaluation. The proposed method uses a categorized Arabic documents corpus, and then the weights of the tested document's words are calculated to determine the document keywords which will be compared with the keywords of the corpus categorizes to determine the tested document's best category.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02530",
        "title": "A Dataset for Movie Description",
        "authors": [
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Niket Tandon",
            "Bernt Schiele"
        ],
        "abstract": "Descriptive video service (DVS) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed DVS, which is temporally aligned to full length HD movies. In addition we also collected the aligned movie scripts which have been used in prior work and compare the two different sources of descriptions. In total the Movie Description dataset contains a parallel corpus of over 54,000 sentences and video snippets from 72 HD movies. We characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing DVS to scripts, we find that DVS is far more visual and describes precisely what is shown rather than what should happen according to the scripts created prior to movie production.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03210",
        "title": "Towards Deep Semantic Analysis Of Hashtags",
        "authors": [
            "Piyush Bansal",
            "Romil Bansal",
            "Vasudeva Varma"
        ],
        "abstract": "Hashtags are semantico-syntactic constructs used across various social networking and microblogging platforms to enable users to start a topic specific discussion or classify a post into a desired category. Segmenting and linking the entities present within the hashtags could therefore help in better understanding and extraction of information shared across the social media. However, due to lack of space delimiters in the hashtags (e.g #nsavssnowden), the segmentation of hashtags into constituent entities (\"NSA\" and \"Edward Snowden\" in this case) is not a trivial task. Most of the current state-of-the-art social media analytics systems like Sentiment Analysis and Entity Linking tend to either ignore hashtags, or treat them as a single word. In this paper, we present a context aware approach to segment and link entities in the hashtags to a knowledge base (KB) entry, based on the context within the tweet. Our approach segments and links the entities in hashtags such that the coherence between hashtag semantics and the tweet is maximized. To the best of our knowledge, no existing study addresses the issue of linking entities in hashtags for extracting semantic information. We evaluate our method on two different datasets, and demonstrate the effectiveness of our technique in improving the overall entity linking in tweets via additional semantic information provided by segmenting and linking entities in a hashtag.\n    ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03214",
        "title": "Quantifying Prosodic Variability in Middle English Alliterative Poetry",
        "authors": [
            "Roger Bilisoly"
        ],
        "abstract": "Interest in the mathematical structure of poetry dates back to at least the 19th century: after retiring from his mathematics position, J. J. Sylvester wrote a book on prosody called $\\textit{The Laws of Verse}$. Today there is interest in the computer analysis of poems, and this paper discusses how a statistical approach can be applied to this task. Starting with the definition of what Middle English alliteration is, $\\textit{Sir Gawain and the Green Knight}$ and William Langland's $\\textit{Piers Plowman}$ are used to illustrate the methodology. Theory first developed for analyzing data from a Riemannian manifold turns out to be applicable to strings allowing one to compute a generalized mean and variance for textual data, which is applied to the poems above. The ratio of these two variances produces the analogue of the F test, and resampling allows p-values to be estimated. Consequently, this methodology provides a way to compare prosodic variability between two texts.\n    ",
        "submission_date": "2015-01-14T00:00:00",
        "last_modified_date": "2015-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03302",
        "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for \"question answering about images\" as a particular appealing instance of such a holistic task. In particular, we point out that it is a version of a Turing Test that is likely to be more robust to over-interpretations and contrast it with tasks like grounding and generation of descriptions. Finally, we discuss tools to measure progress in this field.\n    ",
        "submission_date": "2015-01-14T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04346",
        "title": "Mathematical Language Processing: Automatic Grading and Feedback for Open Response Mathematical Questions",
        "authors": [
            "Andrew S. Lan",
            "Divyanshu Vats",
            "Andrew E. Waters",
            "Richard G. Baraniuk"
        ],
        "abstract": "While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04920",
        "title": "Regroupement s\u00e9mantique de d\u00e9finitions en espagnol",
        "authors": [
            "Gerardo Sierra",
            "Juan-Manuel Torres-Moreno",
            "Alejandro Molina"
        ],
        "abstract": "This article focuses on the description and evaluation of a new unsupervised learning method of clustering of definitions in Spanish according to their semantic. Textual Energy was used as a clustering measure, and we study an adaptation of the Precision and Recall to evaluate our method.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05940",
        "title": "A New Efficient Method for Calculating Similarity Between Web Services",
        "authors": [
            "T. Rachad",
            "J. Boutahar",
            "S. El ghazi"
        ],
        "abstract": "Web services allow communication between heterogeneous systems in a distributed environment. Their enormous success and their increased use led to the fact that thousands of Web services are present on the Internet. This significant number of Web services which not cease to increase has led to problems of the difficulty in locating and classifying web services, these problems are encountered mainly during the operations of web services discovery and substitution. Traditional ways of search based on keywords are not successful in this context, their results do not support the structure of Web services and they consider in their search only the identifiers of the web service description language (WSDL) interface elements. The methods based on semantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web service with a semantic description allow raising partially this problem, but their complexity and difficulty delays their adoption in real cases. Measuring the similarity between the web services interfaces is the most suitable solution for this kind of problems, it will classify available web services so as to know those that best match the searched profile and those that do not match. Thus, the main goal of this work is to study the degree of similarity between any two web services by offering a new method that is more effective than existing works.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06587",
        "title": "Measuring academic influence: Not all citations are equal",
        "authors": [
            "Xiaodan Zhu",
            "Peter Turney",
            "Daniel Lemire",
            "Andr\u00e9 Vellino"
        ],
        "abstract": "The importance of a research article is routinely measured by counting how many times it has been cited. However, treating all citations with equal weight ignores the wide variety of functions that citations perform. We want to automatically identify the subset of references in a bibliography that have a central academic influence on the citing paper. For this purpose, we examine the effectiveness of a variety of features for determining the academic influence of a citation. By asking authors to identify the key references in their own work, we created a data set in which citations were labeled according to their academic influence. Using automatic feature selection with supervised machine learning, we found a model for predicting academic influence that achieves good performance on this data set using only four features. The best features, among those we evaluated, were those based on the number of times a reference is mentioned in the body of a citing paper. The performance of these features inspired us to design an influence-primed h-index (the hip-index). Unlike the conventional h-index, it weights citations by how many times a reference is mentioned. According to our experiments, the hip-index is a better indicator of researcher performance than the conventional h-index.\n    ",
        "submission_date": "2015-01-26T00:00:00",
        "last_modified_date": "2015-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07496",
        "title": "Implementation of an Automatic Syllabic Division Algorithm from Speech Files in Portuguese Language",
        "authors": [
            "E.L.F. Da Silva",
            "H.M. de Oliveira"
        ],
        "abstract": "A new algorithm for voice automatic syllabic splitting in the Portuguese language is proposed, which is based on the envelope of the speech signal of the input audio file. A computational implementation in MatlabTM is presented and made available at the URL ",
        "submission_date": "2015-01-29T00:00:00",
        "last_modified_date": "2015-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07676",
        "title": "Towards Resolving Software Quality-in-Use Measurement Challenges",
        "authors": [
            "Issa Atoum",
            "Chih How Bong",
            "Narayanan Kulathuramaiyer"
        ],
        "abstract": "Software quality-in-use comprehends the quality from user's perspectives. It has gained its importance in e-learning applications, mobile service based applications and project management tools. User's decisions on software acquisitions are often ad hoc or based on preference due to difficulty in quantitatively measure software quality-in-use. However, why quality-in-use measurement is difficult? Although there are many software quality models to our knowledge, no works surveys the challenges related to software quality-in-use measurement. This paper has two main contributions; 1) presents major issues and challenges in measuring software quality-in-use in the context of the ISO SQuaRE series and related software quality models, 2) Presents a novel framework that can be used to predict software quality-in-use, and 3) presents preliminary results of quality-in-use topic prediction. Concisely, the issues are related to the complexity of the current standard models and the limitations and incompleteness of the customized software quality models. The proposed framework employs sentiment analysis techniques to predict software quality-in-use.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00731",
        "title": "Incremental Knowledge Base Construction Using DeepDive",
        "authors": [
            "Jaeho Shin",
            "Sen Wu",
            "Feiran Wang",
            "Christopher De Sa",
            "Ce Zhang",
            "Christopher R\u00e9"
        ],
        "abstract": "Populating a database with unstructured information is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. Recent names used for this problem include dealing with dark data and knowledge base construction (KBC). In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based respectively on sampling and variational techniques. We also study the tradeoff space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01710",
        "title": "Text Understanding from Scratch",
        "authors": [
            "Xiang Zhang",
            "Yann LeCun"
        ],
        "abstract": "This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02233",
        "title": "Hierarchical Dirichlet process for tracking complex topical structure evolution and its application to autism research literature",
        "authors": [
            "Adham Beykikhoshk",
            "Ognjen Arandjelovic",
            "Dinh Phung",
            "Svetha Venkatesh"
        ],
        "abstract": "In this paper we describe a novel framework for the discovery of the topical content of a data corpus, and the tracking of its complex structural changes across the temporal dimension. In contrast to previous work our model does not impose a prior on the rate at which documents are added to the corpus nor does it adopt the Markovian assumption which overly restricts the type of changes that the model can capture. Our key technical contribution is a framework based on (i) discretization of time into epochs, (ii) epoch-wise topic discovery using a hierarchical Dirichlet process-based model, and (iii) a temporal similarity graph which allows for the modelling of complex topic changes: emergence and disappearance, evolution, and splitting and merging. The power of the proposed framework is demonstrated on the medical literature corpus concerned with the autism spectrum disorder (ASD) - an increasingly important research subject of significant social and healthcare importance. In addition to the collected ASD literature corpus which we will make freely available, our contributions also include two free online tools we built as aids to ASD researchers. These can be used for semantically meaningful navigation and searching, as well as knowledge discovery from this large and rapidly growing corpus of literature.\n    ",
        "submission_date": "2015-02-08T00:00:00",
        "last_modified_date": "2015-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02277",
        "title": "Improving Term Frequency Normalization for Multi-topical Documents, and Application to Language Modeling Approaches",
        "authors": [
            "Seung-Hoon Na",
            "In-Su Kang",
            "Jong-Hyeok Lee"
        ],
        "abstract": "Term frequency normalization is a serious issue since lengths of documents are various. Generally, documents become long due to two different reasons - verbosity and multi-topicality. First, verbosity means that the same topic is repeatedly mentioned by terms related to the topic, so that term frequency is more increased than the well-summarized one. Second, multi-topicality indicates that a document has a broad discussion of multi-topics, rather than single topic. Although these document characteristics should be differently handled, all previous methods of term frequency normalization have ignored these differences and have used a simplified length-driven approach which decreases the term frequency by only the length of a document, causing an unreasonable penalization. To attack this problem, we propose a novel TF normalization method which is a type of partially-axiomatic approach. We first formulate two formal constraints that the retrieval model should satisfy for documents having verbose and multi-topicality characteristic, respectively. Then, we modify language modeling approaches to better satisfy these two constraints, and derive novel smoothing methods. Experimental results show that the proposed method increases significantly the precision for keyword queries, and substantially improves MAP (Mean Average Precision) for verbose queries.\n    ",
        "submission_date": "2015-02-08T00:00:00",
        "last_modified_date": "2015-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03520",
        "title": "A Latent Variable Model Approach to PMI-based Word Embeddings",
        "authors": [
            "Sanjeev Arora",
            "Yuanzhi Li",
            "Yingyu Liang",
            "Tengyu Ma",
            "Andrej Risteski"
        ],
        "abstract": "Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods.\n",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2019-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03630",
        "title": "Ordering-sensitive and Semantic-aware Topic Modeling",
        "authors": [
            "Min Yang",
            "Tianyi Cui",
            "Wenting Tu"
        ],
        "abstract": "Topic modeling of textual corpora is an important and challenging problem. In most previous work, the \"bag-of-words\" assumption is usually made which ignores the ordering of words. This assumption simplifies the computation, but it unrealistically loses the ordering information and the semantic of words in the context. In this paper, we present a Gaussian Mixture Neural Topic Model (GMNTM) which incorporates both the ordering of words and the semantic meaning of sentences into topic modeling. Specifically, we represent each topic as a cluster of multi-dimensional vectors and embed the corpus into a collection of vectors generated by the Gaussian mixture model. Each word is affected not only by its topic, but also by the embedding vector of its surrounding words and the context. The Gaussian mixture components and the topic of documents, sentences and words can be learnt jointly. Extensive experiments show that our model can learn better topics and more accurate word distributions for each topic. Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM obtains significantly better performance in terms of perplexity, retrieval accuracy and classification accuracy.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04049",
        "title": "How essential are unstructured clinical narratives and information fusion to clinical trial recruitment?",
        "authors": [
            "Preethi Raghavan",
            "James L. Chen",
            "Eric Fosler-Lussier",
            "Albert M. Lai"
        ],
        "abstract": "Electronic health records capture patient information using structured controlled vocabularies and unstructured narrative text. While structured data typically encodes lab values, encounters and medication lists, unstructured data captures the physician's interpretation of the patient's condition, prognosis, and response to therapeutic intervention. In this paper, we demonstrate that information extraction from unstructured clinical narratives is essential to most clinical applications. We perform an empirical study to validate the argument and show that structured data alone is insufficient in resolving eligibility criteria for recruiting patients onto clinical trials for chronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is essential to solving 59% of the CLL trial criteria and 77% of the prostate cancer trial criteria. More specifically, for resolving eligibility criteria with temporal constraints, we show the need for temporal reasoning and information integration with medical events within and across unstructured clinical narratives and structured data.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04081",
        "title": "A Linear Dynamical System Model for Text",
        "authors": [
            "David Belanger",
            "Sham Kakade"
        ],
        "abstract": "Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words' local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words' representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple cooccurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05441",
        "title": "Rule-and Dictionary-based Solution for Variations in Written Arabic Names in Social Networks, Big Data, Accounting Systems and Large Databases",
        "authors": [
            "Ahmad B.A. Hassanat",
            "Ghada Awad Altarawneh"
        ],
        "abstract": "This paper investigates the problem that some Arabic names can be written in multiple ways. When someone searches for only one form of a name, neither exact nor approximate matching is appropriate for returning the multiple variants of the name. Exact matching requires the user to enter all forms of the name for the search, and approximate matching yields names not among the variations of the one being sought. In this paper, we attempt to solve the problem with a dictionary of all Arabic names mapped to their different (alternative) writing forms. We generated alternatives based on rules we derived from reviewing the first names of 9.9 million citizens and former citizens of Jordan. This dictionary can be used for both standardizing the written form when inserting a new name into a database and for searching for the name and all its alternative written forms. Creating the dictionary automatically based on rules resulted in at least 7% erroneous acceptance errors and 7.9% erroneous rejection errors. We addressed the errors by manually editing the dictionary. The dictionary can be of help to real world-databases, with the qualification that manual editing does not guarantee 100% correctness.\n    ",
        "submission_date": "2015-02-18T00:00:00",
        "last_modified_date": "2015-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05472",
        "title": "On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports",
        "authors": [
            "Diego Marcheggiani",
            "Fabrizio Sebastiani"
        ],
        "abstract": "In the last five years there has been a flurry of work on information extraction from clinical documents, i.e., on algorithms capable of extracting, from the informal and unstructured texts that are generated during everyday clinical practice, mentions of concepts relevant to such practice. Most of this literature is about methods based on supervised learning, i.e., methods for training an information extraction system from manually annotated examples. While a lot of work has been devoted to devising learning methods that generate more and more accurate information extractors, no work has been devoted to investigating the effect of the quality of training data on the learning process. Low quality in training data often derives from the fact that the person who has annotated the data is different from the one against whose judgment the automatically annotated data must be evaluated. In this paper we test the impact of such data quality issues on the accuracy of information extraction systems as applied to the clinical domain. We do this by comparing the accuracy deriving from training data annotated by the authoritative coder (i.e., the one who has also annotated the test data, and by whose judgment we must abide), with the accuracy deriving from training data annotated by a different coder. The results indicate that, although the disagreement between the two coders (as measured on the training set) is substantial, the difference is (surprisingly enough) not always statistically significant.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05698",
        "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
        "authors": [
            "Jason Weston",
            "Antoine Bordes",
            "Sumit Chopra",
            "Alexander M. Rush",
            "Bart van Merri\u00ebnboer",
            "Armand Joulin",
            "Tomas Mikolov"
        ],
        "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05957",
        "title": "Web Similarity in Sets of Search Terms using Database Queries",
        "authors": [
            "Andrew R. Cohen",
            "Paul M.B. Vitanyi"
        ],
        "abstract": "Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or another large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a common similarity (common semantics) on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity of members of a set according to all (upper semi)computable properties. We develop the theory and give applications of classifying using Amazon, Wikipedia, and the NCBI website from the National Institutes of Health. The last gives new correlations between health hazards. A restriction of the NWD to a set of two yields the earlier normalized google distance (NGD) but no combination of the NGD's of pairs in a set can extract the information the NWD extracts from the set. The NWD enables a new contextual (different databases) learning approachbased on Kolmogorov complexity theory that incorporates knowledge from these databases.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2020-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07157",
        "title": "Exploiting a comparability mapping to improve bi-lingual data categorization: a three-mode data analysis perspective",
        "authors": [
            "Pierre-Fran\u00e7ois Marteau",
            "Guiyao Ke"
        ],
        "abstract": "We address in this paper the co-clustering and co-classification of bilingual data laying in two linguistic similarity spaces when a comparability measure defining a mapping between these two spaces is available. A new approach that we can characterized as a three-mode analysis scheme,  is proposed to mix the comparability measure with the two similarity measures.  Our  aim is to improve jointly the accuracy of classification and clustering tasks performed in each of the two linguistic spaces, as well as the quality of the final alignment of comparable clusters that can be obtained. We used first some purely synthetic random data sets to assess our formal similarity-comparability mixing model. We then propose two variants of the comparability measure that has been defined by (Li and Gaussier 2010) in the context of bilingual lexicon extraction to adapt it to clustering or categorizing tasks. These two variant measures are subsequently used to evaluate our similarity-comparability mixing model in the context of the co-classification and co-clustering of comparable textual data sets collected from Wikipedia categories for the English and French languages. Our experiments show clear improvements in clustering and classification accuracies when mixing comparability with similarity measures, with, as expected, a higher robustness obtained when the two comparability variant measures that we propose are used. We believe that this approach is particularly well suited for the construction of thematic comparable corpora of controllable quality.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08029",
        "title": "Describing Videos by Exploiting Temporal Structure",
        "authors": [
            "Li Yao",
            "Atousa Torabi",
            "Kyunghyun Cho",
            "Nicolas Ballas",
            "Christopher Pal",
            "Hugo Larochelle",
            "Aaron Courville"
        ],
        "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08030",
        "title": "Author Name Disambiguation by Using Deep Neural Network",
        "authors": [
            "Hung Nghiep Tran",
            "Tin Huynh",
            "Tien Do"
        ],
        "abstract": "Author name ambiguity decreases the quality and reliability of information retrieved from digital libraries. Existing methods have tried to solve this problem by predefining a feature set based on expert's knowledge for a specific dataset. In this paper, we propose a new approach which uses deep neural network to learn features automatically from data. Additionally, we propose the general system architecture for author name disambiguation on any dataset. In this research, we evaluate the proposed method on a dataset containing Vietnamese author names. The results show that this method significantly outperforms other methods that use predefined feature set. The proposed method achieves 99.31% in terms of accuracy. Prediction error rate decreases from 1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with other methods that use predefined feature set (Table 3).\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08033",
        "title": "SciRecSys: A Recommendation System for Scientific Publication by Discovering Keyword Relationships",
        "authors": [
            "Vu Le Anh",
            "Vo Hoang Hai",
            "Hung Nghiep Tran",
            "Jason J. Jung"
        ],
        "abstract": "In this work, we propose a new approach for discovering various relationships among keywords over the scientific publications based on a Markov Chain model. It is an important problem since keywords are the basic elements for representing abstract objects such as documents, user profiles, topics and many things else. Our model is very effective since it combines four important factors in scientific publications: content, publicity, impact and randomness. Particularly, a recommendation system (called SciRecSys) has been presented to support users to efficiently find out relevant articles.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00064",
        "title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes",
        "authors": [
            "Dahua Lin",
            "Chen Kong",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account the coherence among sentences. Experiments on the augmented NYU-v2 dataset show that our framework can generate natural descriptions with substantially higher ROGUE scores compared to those produced by the baseline.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00185",
        "title": "When Are Tree Structures Necessary for Deep Learning of Representations?",
        "authors": [
            "Jiwei Li",
            "Minh-Thang Luong",
            "Dan Jurafsky",
            "Eudard Hovy"
        ],
        "abstract": "Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. But there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper we benchmark {\\bf recursive} neural models against sequential {\\bf recurrent} neural models (simple recurrent and LSTM models), enforcing apples-to-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answer-phrases; (3) discourse parsing; (4) semantic relation extraction (e.g., {\\em component-whole} between nouns).\n",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01180",
        "title": "All Who Wander: On the Prevalence and Characteristics of Multi-community Engagement",
        "authors": [
            "Chenhao Tan",
            "Lillian Lee"
        ],
        "abstract": "Although analyzing user behavior within individual communities is an active and rich research domain, people usually interact with multiple communities both on- and off-line. How do users act in such multi-community environments? Although there are a host of intriguing aspects to this question, it has received much less attention in the research community in comparison to the intra-community case. In this paper, we examine three aspects of multi-community engagement: the sequence of communities that users post to, the language that users employ in those communities, and the feedback that users receive, using longitudinal posting behavior on Reddit as our main data source, and DBLP for auxiliary experiments. We also demonstrate the effectiveness of features drawn from these aspects in predicting users' future level of activity.\n",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01258",
        "title": "The concept \"altruism\" for sociological research: from conceptualization to operationalization",
        "authors": [
            "Oleg V. Pavenkov",
            "Vladimir G. Pavenkov",
            "Mariia V. Rubtcova"
        ],
        "abstract": "This article addresses the question of the relevant conceptualization of \u00abaltruism\u00bb in Russian from the perspective sociological research operationalization. It investigates the spheres of social application of the word \u00abaltruism\u00bb, include Russian equivalent \u00abvzaimopomoshh`\u00bb (mutual help). The data for the study comes from Russian National Corpus (Russian). The theoretical framework consists of Paul F. Lazarsfeld`s Theory of Sociological Research Methodology and the Natural Semantic Metalanguage (NSM). Quantitative analysis shows features in the representation of altruism in Russian that sociologists need to know in the preparation of questionnaires, interview guides and analysis of transcripts.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01397",
        "title": "Bethe Projections for Non-Local Inference",
        "authors": [
            "Luke Vilnis",
            "David Belanger",
            "Daniel Sheldon",
            "Andrew McCallum"
        ],
        "abstract": "Many inference problems in structured prediction are naturally solved by augmenting a tractable dependency structure with complex, non-local auxiliary objectives. This includes the mean field family of variational inference algorithms, soft- or hard-constrained inference using Lagrangian relaxation or linear programming, collective graphical models, and forms of semi-supervised learning such as posterior regularization. We present a method to discriminatively learn broad families of inference objectives, capturing powerful non-local statistics of the latent variables, while maintaining tractable and provably fast inference using non-Euclidean projected gradient descent with a distance-generating function given by the Bethe entropy. We demonstrate the performance and flexibility of our method by (1) extracting structured citations from research papers by learning soft global constraints, (2) achieving state-of-the-art results on a widely-used handwriting recognition task using a novel learned non-convex inference procedure, and (3) providing a fast and highly scalable algorithm for the challenging problem of inference in a collective graphical model applied to bird migration.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01549",
        "title": "Visualization of Clandestine Labs from Seizure Reports: Thematic Mapping and Data Mining Research Directions",
        "authors": [
            "William Hsu",
            "Mohammed Abduljabbar",
            "Ryuichi Osuga",
            "Max Lu",
            "Wesam Elshamy"
        ],
        "abstract": "The problem of spatiotemporal event visualization based on reports entails subtasks ranging from named entity recognition to relationship extraction and mapping of events. We present an approach to event extraction that is driven by data mining and visualization goals, particularly thematic mapping and trend analysis. This paper focuses on bridging the information extraction and visualization tasks and investigates topic modeling approaches. We develop a static, finite topic model and examine the potential benefits and feasibility of extending this to dynamic topic modeling with a large number of topics and continuous time. We describe an experimental test bed for event mapping that uses this end-to-end information retrieval system, and report preliminary results on a geoinformatics problem: tracking of methamphetamine lab seizure events across time and space.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02108",
        "title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models",
        "authors": [
            "Zhen Huang",
            "Sabato Marco Siniscalchi",
            "I-Fan Chen",
            "Jiadong Wu",
            "Chin-Hui Lee"
        ],
        "abstract": "We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output tied states, or senones, and compare it to feature space MAP linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation approach provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02417",
        "title": "Structured Prediction of Sequences and Trees using Infinite Contexts",
        "authors": [
            "Ehsan Shareghi",
            "Gholamreza Haffari",
            "Trevor Cohn",
            "Ann Nicholson"
        ],
        "abstract": "Linguistic structures exhibit a rich array of global phenomena, however commonly used Markov models are unable to adequately describe these phenomena due to their strong locality assumptions. We propose a novel hierarchical model for structured prediction over sequences and trees which exploits global context by conditioning each generation decision on an unbounded context of prior decisions. This builds on the success of Markov models but without imposing a fixed bound in order to better represent global phenomena. To facilitate learning of this large and unbounded model, we use a hierarchical Pitman-Yor process prior which provides a recursive form of smoothing. We propose prediction algorithms based on A* and Markov Chain Monte Carlo sampling. Empirical results demonstrate the potential of our model compared to baseline finite-context Markov models on part-of-speech tagging and syntactic parsing.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02801",
        "title": "Short Text Hashing Improved by Integrating Multi-Granularity Topics and Tags",
        "authors": [
            "Jiaming Xu",
            "Bo Xu",
            "Guanhua Tian",
            "Jun Zhao",
            "Fangyuan Wang",
            "Hongwei Hao"
        ],
        "abstract": "Due to computational and storage efficiencies of compact binary codes, hashing has been widely used for large-scale similarity search. Unfortunately, many existing hashing methods based on observed keyword features are not effective for short texts due to the sparseness and shortness. Recently, some researchers try to utilize latent topics of certain granularity to preserve semantic similarity in hash codes beyond keyword matching. However, topics of certain granularity are not adequate to represent the intrinsic semantic information. In this paper, we present a novel unified approach for short text Hashing using Multi-granularity Topics and Tags, dubbed HMTT. In particular, we propose a selection method to choose the optimal multi-granularity topics depending on the type of dataset, and design two distinct hashing strategies to incorporate multi-granularity topics. We also propose a simple and effective method to exploit tags to enhance the similarity of related texts. We carry out extensive experiments on one short text dataset as well as on one normal text dataset. The results demonstrate that our approach is effective and significantly outperforms baselines on several evaluation metrics.\n    ",
        "submission_date": "2015-03-10T00:00:00",
        "last_modified_date": "2015-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04250",
        "title": "The YLI-MED Corpus: Characteristics, Procedures, and Plans",
        "authors": [
            "Julia Bernd",
            "Damian Borth",
            "Benjamin Elizalde",
            "Gerald Friedland",
            "Heather Gallagher",
            "Luke Gottlieb",
            "Adam Janin",
            "Sara Karabashlieva",
            "Jocelyn Takahashi",
            "Jennifer Won"
        ],
        "abstract": "The YLI Multimedia Event Detection corpus is a public-domain index of videos with annotations and computed features, specialized for research in multimedia event detection (MED), i.e., automatically identifying what's happening in a video by analyzing the audio and visual content. The videos indexed in the YLI-MED corpus are a subset of the larger YLI feature corpus, which is being developed by the International Computer Science Institute and Lawrence Livermore National Laboratory based on the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting one of ten target events, or no target event, and are annotated for additional attributes like language spoken and whether the video has a musical score. The annotations also include degree of annotator agreement and average annotator confidence scores for the event categorization of each video. Version 1.0 of YLI-MED includes 1823 \"positive\" videos that depict the target events and 48,138 \"negative\" videos, as well as 177 supplementary videos that are similar to event videos but are not positive examples. Our goal in producing YLI-MED is to be as open about our data and procedures as possible. This report describes the procedures used to collect the corpus; gives detailed descriptive statistics about the corpus makeup (and how video attributes affected annotators' judgments); discusses possible biases in the corpus introduced by our procedural choices and compares it with the most similar existing dataset, TRECVID MED's HAVIC corpus; and gives an overview of our future plans for expanding the annotation effort.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04723",
        "title": "Deep Feelings: A Massive Cross-Lingual Study on the Relation between Emotions and Virality",
        "authors": [
            "Marco Guerini",
            "Jacopo Staiano"
        ],
        "abstract": "This article provides a comprehensive investigation on the relations between virality of news articles and the emotions they are found to evoke. Virality, in our view, is a phenomenon with many facets, i.e. under this generic term several different effects of persuasive communication are comprised. By exploiting a high-coverage and bilingual corpus of documents containing metrics of their spread on social networks as well as a massive affective annotation provided by readers, we present a thorough analysis of the interplay between evoked emotions and viral facets. We highlight and discuss our findings in light of a cross-lingual approach: while we discover differences in evoked emotions and corresponding viral effects, we provide preliminary evidence of a generalized explanatory model rooted in the deep structure of emotions: the Valence-Arousal-Dominance (VAD) circumplex. We find that viral facets appear to be consistently affected by particular VAD configurations, and these configurations indicate a clear connection with distinct phenomena underlying persuasive communication.\n    ",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06410",
        "title": "What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes",
        "authors": [
            "David M. W. Powers"
        ],
        "abstract": "The F-measure or F-score is one of the most commonly used single number measures in Information Retrieval, Natural Language Processing and Machine Learning, but it is based on a mistake, and the flawed assumptions render it unsuitable for use in most contexts! Fortunately, there are better alternatives.\n    ",
        "submission_date": "2015-03-22T00:00:00",
        "last_modified_date": "2019-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06934",
        "title": "Measuring Software Quality in Use: State-of-the-Art and Research Challenges",
        "authors": [
            "Issa Atoum",
            "Chih How Bong"
        ],
        "abstract": "Software quality in use comprises quality from the user's perspective. It has gained its importance in e-government applications, mobile-based applications, embedded systems, and even business process development. User's decisions on software acquisitions are often ad hoc or based on preference due to difficulty in quantitatively measuring software quality in use. But, why is quality-in-use measurement difficult? Although there are many software quality models, to the authors' knowledge no works survey the challenges related to software quality-in-use measurement. This article has two main contributions: 1) it identifies and explains major issues and challenges in measuring software quality in use in the context of the ISO SQuaRE series and related software quality models and highlights open research areas; and 2) it sheds light on a research direction that can be used to predict software quality in use. In short, the quality-in-use measurement issues are related to the complexity of the current standard models and the limitations and incompleteness of the customized software quality models. A sentiment analysis of software reviews is proposed to deal with these issues.\n    ",
        "submission_date": "2015-03-24T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07921",
        "title": "Breaking the News: First Impressions Matter on Online News",
        "authors": [
            "Julio Reis",
            "Fabr\u0131cio Benevenuto",
            "Pedro O.S. Vaz de Melo",
            "Raquel Prates",
            "Haewoon Kwak",
            "Jisun An"
        ],
        "abstract": "A growing number of people are changing the way they consume news, replacing the traditional physical newspapers and magazines by their virtual online versions or/and weblogs. The interactivity and immediacy present in online news are changing the way news are being produced and exposed by media corporations. News websites have to create effective strategies to catch people's attention and attract their clicks. In this paper we investigate possible strategies used by online news corporations in the design of their news headlines. We analyze the content of 69,907 headlines produced by four major global media corporations during a minimum of eight consecutive months in 2014. In order to discover strategies that could be used to attract clicks, we extracted features from the text of the news headlines related to the sentiment polarity of the headline. We discovered that the sentiment of the headline is strongly related to the popularity of the news and also with the dynamics of the posted comments on that particular news.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08155",
        "title": "Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories",
        "authors": [
            "Miao Fan",
            "Qiang Zhou",
            "Thomas Fang Zheng"
        ],
        "abstract": "This paper considers the problem of knowledge inference on large-scale imperfect repositories with incomplete coverage by means of embedding entities and relations at the first attempt. We propose IIKE (Imperfect and Incomplete Knowledge Embedding), a probabilistic model which measures the probability of each belief, i.e. $\\langle h,r,t\\rangle$, in large-scale knowledge bases such as NELL and Freebase, and our objective is to learn a better low-dimensional vector representation for each entity ($h$ and $t$) and relation ($r$) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning (NELL) or crowdsouring (Freebase), so that we can use $||{\\bf h} + {\\bf r} - {\\bf t}||$ to assess the plausibility of a belief when conducting inference. We use subsets of those inexact knowledge bases to train our model and test the performances of link prediction and triplet classification on ground truth beliefs, respectively. The results of extensive experiments show that IIKE achieves significant improvement compared with the baseline and state-of-the-art approaches.\n    ",
        "submission_date": "2015-03-27T00:00:00",
        "last_modified_date": "2015-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08542",
        "title": "Nonparametric Relational Topic Models through Dependent Gamma Processes",
        "authors": [
            "Junyu Xuan",
            "Jie Lu",
            "Guangquan Zhang",
            "Richard Yi Da Xu",
            "Xiangfeng Luo"
        ],
        "abstract": "Traditional Relational Topic Models provide a way to discover the hidden topics from a document network. Many theoretical and practical tasks, such as dimensional reduction, document clustering, link prediction, benefit from this revealed knowledge. However, existing relational topic models are based on an assumption that the number of hidden topics is known in advance, and this is impractical in many real-world applications. Therefore, in order to relax this assumption, we propose a nonparametric relational topic model in this paper. Instead of using fixed-dimensional probability distributions in its generative model, we use stochastic processes. Specifically, a gamma process is assigned to each document, which represents the topic interest of this document. Although this method provides an elegant solution, it brings additional challenges when mathematically modeling the inherent network structure of typical document network, i.e., two spatially closer documents tend to have more similar topics. Furthermore, we require that the topics are shared by all the documents. In order to resolve these challenges, we use a subsampling strategy to assign each document a different gamma process from the global gamma process, and the subsampling probabilities of documents are assigned with a Markov Random Field constraint that inherits the document network structure. Through the designed posterior inference algorithm, we can discover the hidden topics and its number simultaneously. Experimental results on both synthetic and real-world network datasets demonstrate the capabilities of learning the hidden topics and, more importantly, the number of topics.\n    ",
        "submission_date": "2015-03-30T00:00:00",
        "last_modified_date": "2015-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08581",
        "title": "LSHTC: A Benchmark for Large-Scale Text Classification",
        "authors": [
            "Ioannis Partalas",
            "Aris Kosmopoulos",
            "Nicolas Baskiotis",
            "Thierry Artieres",
            "George Paliouras",
            "Eric Gaussier",
            "Ion Androutsopoulos",
            "Massih-Reza Amini",
            "Patrick Galinari"
        ],
        "abstract": "LSHTC is a series of challenges which aims to assess the performance of classification systems in large-scale classification in a a large number of classes (up to hundreds of thousands). This paper describes the dataset that have been released along the LSHTC series. The paper details the construction of the datsets and the design of the tracks as well as the evaluation measures that we implemented and a quick overview of the results. All of these datasets are available online and runs may still be submitted on the online server of the challenges.\n    ",
        "submission_date": "2015-03-30T00:00:00",
        "last_modified_date": "2015-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08895",
        "title": "End-To-End Memory Networks",
        "authors": [
            "Sainbayar Sukhbaatar",
            "Arthur Szlam",
            "Jason Weston",
            "Rob Fergus"
        ],
        "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.\n    ",
        "submission_date": "2015-03-31T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00325",
        "title": "Microsoft COCO Captions: Data Collection and Evaluation Server",
        "authors": [
            "Xinlei Chen",
            "Hao Fang",
            "Tsung-Yi Lin",
            "Ramakrishna Vedantam",
            "Saurabh Gupta",
            "Piotr Dollar",
            "C. Lawrence Zitnick"
        ],
        "abstract": "In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.\n    ",
        "submission_date": "2015-04-01T00:00:00",
        "last_modified_date": "2015-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00657",
        "title": "Eliciting Disease Data from Wikipedia Articles",
        "authors": [
            "Geoffrey Fairchild",
            "Lalindra De Silva",
            "Sara Y. Del Valle",
            "Alberto M. Segre"
        ],
        "abstract": "Traditional disease surveillance systems suffer from several disadvantages, including reporting lags and antiquated technology, that have caused a movement towards internet-based disease surveillance systems. Internet systems are particularly attractive for disease outbreaks because they can provide data in near real-time and can be verified by individuals around the globe. However, most existing systems have focused on disease monitoring and do not provide a data repository for policy makers or researchers. In order to fill this gap, we analyzed Wikipedia article content.\n",
        "submission_date": "2015-04-02T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00854",
        "title": "Evaluation Evaluation a Monte Carlo study",
        "authors": [
            "David M. W. Powers"
        ],
        "abstract": "Over the last decade there has been increasing concern about the biases embodied in traditional evaluation methods for Natural Language Processing/Learning, particularly methods borrowed from Information Retrieval. Without knowledge of the Bias and Prevalence of the contingency being tested, or equivalently the expectation due to chance, the simple conditional probabilities Recall, Precision and Accuracy are not meaningful as evaluation measures, either individually or in combinations such as F-factor. The existence of bias in NLP measures leads to the 'improvement' of systems by increasing their bias, such as the practice of improving tagging and parsing scores by using most common value (e.g. water is always a Noun) rather than the attempting to discover the correct one. The measures Cohen Kappa and Powers Informedness are discussed as unbiased alternative to Recall and related to the psychologically significant measure DeltaP. In this paper we will analyze both biased and unbiased measures theoretically, characterizing the precise relationship between all these measures as well as evaluating the evaluation measures themselves empirically using a Monte Carlo simulation.\n    ",
        "submission_date": "2015-04-03T00:00:00",
        "last_modified_date": "2015-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01255",
        "title": "Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding",
        "authors": [
            "Rie Johnson",
            "Tong Zhang"
        ],
        "abstract": "This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.\n    ",
        "submission_date": "2015-04-06T00:00:00",
        "last_modified_date": "2015-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01482",
        "title": "Deep Recurrent Neural Networks for Acoustic Modelling",
        "authors": [
            "William Chan",
            "Ian Lane"
        ],
        "abstract": "We present a novel deep Recurrent Neural Network (RNN) model for acoustic modelling in Automatic Speech Recognition (ASR). We term our contribution as a TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory (BLSTM), and a final DNN. The first DNN acts as a feature processor to our model, the BLSTM then generates a context from the sequence acoustic signal, and the final DNN takes the context and models the posterior probabilities of the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ) eval92 task or more than 8% relative improvement over the baseline DNN models.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01483",
        "title": "Transferring Knowledge from a RNN to a DNN",
        "authors": [
            "William Chan",
            "Nan Rosemary Ke",
            "Ian Lane"
        ],
        "abstract": "Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent Neural Network (RNN) models have been shown to outperform DNNs counterparts. However, state-of-the-art DNN and RNN models tend to be impractical to deploy on embedded systems with limited computational capacity. Traditionally, the approach for embedded platforms is to either train a small DNN directly, or to train a small DNN that learns the output distribution of a large DNN. In this paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We use the RNN model to generate soft alignments and minimize the Kullback-Leibler divergence against the small DNN. The small DNN trained on the soft RNN alignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task compared to a baseline 4.54 WER or more than 13% relative improvement.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01684",
        "title": "Large Margin Nearest Neighbor Embedding for Knowledge Representation",
        "authors": [
            "Miao Fan",
            "Qiang Zhou",
            "Thomas Fang Zheng",
            "Ralph Grishman"
        ],
        "abstract": "Traditional way of storing facts in triplets ({\\it head\\_entity, relation, tail\\_entity}), abbreviated as ({\\it h, r, t}), makes the knowledge intuitively displayed and easily acquired by mankind, but hardly computed or even reasoned by AI machines. Inspired by the success in applying {\\it Distributed Representations} to AI-related fields, recent studies expect to represent each entity and relation with a unique low-dimensional embedding, which is different from the symbolic and atomic framework of displaying knowledge in triplets. In this way, the knowledge computing and reasoning can be essentially facilitated by means of a simple {\\it vector calculation}, i.e. ${\\bf h} + {\\bf r} \\approx {\\bf t}$. We thus contribute an effective model to learn better embeddings satisfying the formula by pulling the positive tail entities ${\\bf t^{+}}$ to get together and close to {\\bf h} + {\\bf r} ({\\it Nearest Neighbor}), and simultaneously pushing the negatives ${\\bf t^{-}}$ away from the positives ${\\bf t^{+}}$ via keeping a {\\it Large Margin}. We also design a corresponding learning algorithm to efficiently find the optimal solution based on {\\it Stochastic Gradient Descent} in iterative fashion. Quantitative experiments illustrate that our approach can achieve the state-of-the-art performance, compared with several latest methods on some benchmark datasets for two classical applications, i.e. {\\it Link prediction} and {\\it Triplet classification}. Moreover, we analyze the parameter complexities among all the evaluated models, and analytical results indicate that our model needs fewer computational resources on outperforming the other methods.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02059",
        "title": "Supporting Language Learners with the Meanings Of Closed Class Items",
        "authors": [
            "Hayat Alrefaie",
            "Allan Ramsay"
        ],
        "abstract": "The process of language learning involves the mastery of countless tasks: making the constituent sounds of the language being learned, learning the grammatical patterns, and acquiring the requisite vocabulary for reception and production. While a plethora of computational tools exist to facilitate the first and second of these tasks, a number of challenges arise with respect to enabling the third. This paper describes a tool that has been designed to support language learners with the challenge of understanding the use of closed-class lexical items. The process of learning the Arabic for office is (mktb) is relatively simple and should be possible by means of simple repetition of the word. However, it is much more difficult to learn and correctly use the Arabic equivalent of the word on. The current paper describes a mechanism for the delivery of diagnostic information regarding specific lexical examples, with the aim of clearly demonstrating why a particular translation of a given closed-class item may be appropriate in certain situations but not others, thereby helping learners to understand and use the term correctly.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03068",
        "title": "Review Mining for Feature Based Opinion Summarization and Visualization",
        "authors": [
            "Ahmad Kamal"
        ],
        "abstract": "The application and usage of opinion mining, especially for business intelligence, product recommendation, targeted marketing etc. have fascinated many research attentions around the globe. Various research efforts attempted to mine opinions from customer reviews at different levels of granularity, including word-, sentence-, and document-level. However, development of a fully automatic opinion mining and sentiment analysis system is still elusive. Though the development of opinion mining and sentiment analysis systems are getting momentum, most of them attempt to perform document-level sentiment analysis, classifying a review document as positive, negative, or neutral. Such document-level opinion mining approaches fail to provide insight about users sentiment on individual features of a product or service. Therefore, it seems to be a great help for both customers and manufacturers, if the reviews could be processed at a finer-grained level and presented in a summarized form through some visual means, highlighting individual features of a product and users sentiment expressed over them. In this paper, the design of a unified opinion mining and sentiment analysis framework is presented at the intersection of both machine learning and natural language processing approaches. Also, design of a novel feature-level review summarization scheme is proposed to visualize mined features, opinions and their polarity values in a comprehendible way.\n    ",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03425",
        "title": "Automated Analysis and Prediction of Job Interview Performance",
        "authors": [
            "Iftekhar Naim",
            "M. Iftekhar Tanveer",
            "Daniel Gildea",
            "Mohammed",
            "Hoque"
        ],
        "abstract": "We present a computational framework for automatically quantifying verbal and nonverbal behaviors in the context of job interviews. The proposed framework is trained by analyzing the videos of 138 interview sessions with 69 internship-seeking undergraduates at the Massachusetts Institute of Technology (MIT). Our automated analysis includes facial expressions (e.g., smiles, head gestures, facial tracking points), language (e.g., word counts, topic modeling), and prosodic information (e.g., pitch, intonation, and pauses) of the interviewees. The ground truth labels are derived by taking a weighted average over the ratings of 9 independent judges. Our framework can automatically predict the ratings for interview traits such as excitement, friendliness, and engagement with correlation coefficients of 0.75 or higher, and can quantify the relative importance of prosody, language, and facial expressions. By analyzing the relative feature weights learned by the regression models, our framework recommends to speak more fluently, use less filler words, speak as \"we\" (vs. \"I\"), use more unique words, and smile more. We also find that the students who were rated highly while answering the first interview question were also rated highly overall (i.e., first impression matters). Finally, our MIT Interview dataset will be made available to other researchers to further validate and expand our findings.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03608",
        "title": "A data-based classification of Slavic languages: Indices of qualitative variation applied to grapheme frequencies",
        "authors": [
            "Michaela Koscov\u00e1",
            "J\u00e1n Macutek",
            "Emmerich Kelih"
        ],
        "abstract": "The Ord's graph is a simple graphical method for displaying frequency distributions of data or theoretical distributions in the two-dimensional plane. Its coordinates are proportions of the first three moments, either empirical or theoretical ones. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories. Both the original and the new graph are used to display grapheme frequencies in eleven Slavic languages. As the original Ord's graph requires an assignment of numbers to the categories, graphemes were ordered decreasingly according to their frequencies. Data were taken from parallel corpora, i.e., we work with grapheme frequencies from a Russian novel and its translations to ten other Slavic languages. Then, cluster analysis is applied to the graph coordinates. While the original graph yields results which are not linguistically interpretable, the modification reveals meaningful relations among the languages.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04317",
        "title": "Towards a relation extraction framework for cyber-security concepts",
        "authors": [
            "Corinne L. Jones",
            "Robert A. Bridges",
            "Kelly Huffer",
            "John Goodall"
        ],
        "abstract": "In order to assist security analysts in obtaining information pertaining to their network, such as novel vulnerabilities, exploits, or patches, information retrieval methods tailored to the security domain are needed. As labeled text data is scarce and expensive, we follow developments in semi-supervised Natural Language Processing and implement a bootstrapping algorithm for extracting security entities and their relationships from text. The algorithm requires little input data, specifically, a few relations or patterns (heuristics for identifying relations), and incorporates an active learning component which queries the user on the most important decisions to prevent drifting from the desired relations. Preliminary testing on a small corpus shows promising results, obtaining precision of .82.\n    ",
        "submission_date": "2015-04-16T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04802",
        "title": "Gradual Classical Logic for Attributed Objects - Extended in Re-Presentation",
        "authors": [
            "Ryuta Arisaka"
        ],
        "abstract": "Our understanding about things is conceptual. By stating that we reason about objects, it is in fact not the objects but concepts referring to them that we manipulate. Now, so long just as we acknowledge infinitely extending notions such as space, time, size, colour, etc, - in short, any reasonable quality - into which an object is subjected, it becomes infeasible to affirm atomicity in the concept referring to the object. However, formal/symbolic logics typically presume atomic entities upon which other expressions are built. Can we reflect our intuition about the concept onto formal/symbolic logics at all? I assure that we can, but the usual perspective about the atomicity needs inspected. In this work, I present gradual logic which materialises the observation that we cannot tell apart whether a so-regarded atomic entity is atomic or is just atomic enough not to be considered non-atomic. The motivation is to capture certain phenomena that naturally occur around concepts with attributes, including presupposition and contraries. I present logical particulars of the logic, which is then mapped onto formal semantics. Two linguistically interesting semantics will be considered. Decidability is shown.\n    ",
        "submission_date": "2015-04-19T00:00:00",
        "last_modified_date": "2015-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04884",
        "title": "Compression and the origins of Zipf's law of abbreviation",
        "authors": [
            "R. Ferrer-i-Cancho",
            "C. Bentz",
            "C. Seguin"
        ],
        "abstract": "Languages across the world exhibit Zipf's law of abbreviation, namely more frequent words tend to be shorter. The generalized version of the law - an inverse relationship between the frequency of a unit and its magnitude - holds also for the behaviours of other species and the genetic code. The apparent universality of this pattern in human language and its ubiquity in other domains calls for a theoretical understanding of its origins. To this end, we generalize the information theoretic concept of mean code length as a mean energetic cost function over the probability and the magnitude of the types of the repertoire. We show that the minimization of that cost function and a negative correlation between probability and the magnitude of types are intimately related.\n    ",
        "submission_date": "2015-04-19T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06063",
        "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence",
        "authors": [
            "Lin Ma",
            "Zhengdong Lu",
            "Lifeng Shang",
            "Hang Li"
        ],
        "abstract": "In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and Microsoft COCO databases achieve the state-of-the-art performances.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06077",
        "title": "Open Data Platform for Knowledge Access in Plant Health Domain : VESPA Mining",
        "authors": [
            "Nicolas Turenne",
            "Mathieu Andro",
            "Roselyne Corbi\u00e8re",
            "Tien T. Phan"
        ],
        "abstract": "Important data are locked in ancient literature. It would be uneconomic to produce these data again and today or to extract them without the help of text mining technologies. Vespa is a text mining project whose aim is to extract data on pest and crops interactions, to model and predict attacks on crops, and to reduce the use of pesticides. A few attempts proposed an agricultural information access. Another originality of our work is to parse documents with a dependency of the document architecture.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06080",
        "title": "svcR: An R Package for Support Vector Clustering improved with Geometric Hashing applied to Lexical Pattern Discovery",
        "authors": [
            "Nicolas Turenne"
        ],
        "abstract": "We present a new R package which takes a numerical matrix format as data input, and computes clusters using a support vector clustering method (SVC). We have implemented an original 2D-grid labeling approach to speed up cluster extraction. In this sense, SVC can be seen as an efficient cluster extraction if clusters are separable in a 2-D map. Secondly we showed that this SVC approach using a Jaccard-Radial base kernel can help to classify well enough a set of terms into ontological classes and help to define regular expression rules for information extraction in documents; our case study concerns a set of terms and documents about developmental and molecular biology.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06329",
        "title": "Analysis of Stopping Active Learning based on Stabilizing Predictions",
        "authors": [
            "Michael Bloodgood",
            "John Grothendieck"
        ],
        "abstract": "Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies. This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP). The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen's Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained. Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented. Specifically, if the Kappa agreement between two models exceeds a threshold T (where $T>0$), then the difference in F-measure performance between those models is bounded above by $\\frac{4(1-T)}{T}$ in all cases. If precision of the positive conjunction of the models is assumed to be $p$, then the bound can be tightened to $\\frac{4(1-T)}{(p+1)T}$.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06692",
        "title": "Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images",
        "authors": [
            "Junhua Mao",
            "Wei Xu",
            "Yi Yang",
            "Jiang Wang",
            "Zhiheng Huang",
            "Alan Yuille"
        ],
        "abstract": "In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is ",
        "submission_date": "2015-04-25T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06936",
        "title": "Concept Extraction to Identify Adverse Drug Reactions in Medical Forums: A Comparison of Algorithms",
        "authors": [
            "Alejandro Metke-Jimenez",
            "Sarvnaz Karimi"
        ],
        "abstract": "Social media is becoming an increasingly important source of information to complement traditional pharmacovigilance methods. In order to identify signals of potential adverse drug reactions, it is necessary to first identify medical concepts in the social media text. Most of the existing studies use dictionary-based methods which are not evaluated independently from the overall signal detection task.\n",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07843",
        "title": "On the universal structure of human lexical semantics",
        "authors": [
            "Hyejin Youn",
            "Logan Sutton",
            "Eric Smith",
            "Cristopher Moore",
            "Jon F. Wilkins",
            "Ian Maddieson",
            "William Croft",
            "Tanmoy Bhattacharya"
        ],
        "abstract": "How universal is human conceptual structure? The way concepts are organized in the human brain may reflect distinct features of cultural, historical, and environmental background in addition to properties universal to human cognition. Semantics, or meaning expressed through language, provides direct access to the underlying conceptual structure, but meaning is notoriously difficult to measure, let alone parameterize. Here we provide an empirical measure of semantic proximity between concepts using cross-linguistic dictionaries. Across languages carefully selected from a phylogenetically and geographically stratified sample of genera, translations of words reveal cases where a particular language uses a single polysemous word to express concepts represented by distinct words in another. We use the frequency of polysemies linking two concepts as a measure of their semantic proximity, and represent the pattern of such linkages by a weighted network. This network is highly uneven and fragmented: certain concepts are far more prone to polysemy than others, and there emerge naturally interpretable clusters loosely connected to each other. Statistical analysis shows such structural properties are consistent across different language groups, largely independent of geography, environment, and literacy. It is therefore possible to conclude the conceptual structure connecting basic vocabulary studied is primarily due to universal features of human cognition and language use.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00122",
        "title": "Hierarchy of Scales in Language Dynamics",
        "authors": [
            "Richard A. Blythe"
        ],
        "abstract": "Methods and insights from statistical physics are finding an increasing variety of applications where one seeks to understand the emergent properties of a complex interacting system. One such area concerns the dynamics of language at a variety of levels of description, from the behaviour of individual agents learning simple artificial languages from each other, up to changes in the structure of languages shared by large groups of speakers over historical timescales. In this Colloquium, we survey a hierarchy of scales at which language and linguistic behaviour can be described, along with the main progress in understanding that has been made at each of them---much of which has come from the statistical physics community. We argue that future developments may arise by linking the different levels of the hierarchy together in a more coherent fashion, in particular where this allows more effective use of rich empirical data sets.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00863",
        "title": "A Feature-based Classification Technique for Answering Multi-choice World History Questions",
        "authors": [
            "Shuangyong Song",
            "Yao Meng",
            "Zhongguang Zheng",
            "Jun Sun"
        ],
        "abstract": "Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01121",
        "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
        "authors": [
            "Mateusz Malinowski",
            "Marcus Rohrbach",
            "Mario Fritz"
        ],
        "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01393",
        "title": "Mining Scientific Papers for Bibliometrics: a (very) Brief Survey of Methods and Tools",
        "authors": [
            "Iana Atanassova",
            "Marc Bertin",
            "Philipp Mayr"
        ],
        "abstract": "The Open Access movement in scientific publishing and search engines like Google Scholar have made scientific articles more broadly accessible. During the last decade, the availability of scientific papers in full text has become more and more widespread thanks to the growing number of publications on online platforms such as ArXiv and CiteSeer. The efforts to provide articles in machine-readable formats and the rise of Open Access publishing have resulted in a number of standardized formats for scientific papers (such as NLM-JATS, TEI, DocBook). Our aim is to stimulate research at the intersection of Bibliometrics and Computational Linguistics in order to study the ways Bibliometrics can benefit from large-scale text analytics and sense mining of scientific papers, thus exploring the interdisciplinarity of Bibliometrics and Natural Language Processing.\n    ",
        "submission_date": "2015-05-06T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01504",
        "title": "A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models",
        "authors": [
            "Shiliang Zhang",
            "Hui Jiang",
            "Mingbin Xu",
            "Junfeng Hou",
            "Lirong Dai"
        ],
        "abstract": "In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.\n    ",
        "submission_date": "2015-05-06T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02074",
        "title": "Exploring Models and Data for Image Question Answering",
        "authors": [
            "Mengye Ren",
            "Ryan Kiros",
            "Richard Zemel"
        ],
        "abstract": "This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02251",
        "title": "Probabilistic Cascading for Large Scale Hierarchical Classification",
        "authors": [
            "Aris Kosmopoulos",
            "Georgios Paliouras",
            "Ion Androutsopoulos"
        ],
        "abstract": "Hierarchies are frequently used for the organization of objects. Given a hierarchy of classes, two main approaches are used, to automatically classify new instances: flat classification and cascade classification. Flat classification ignores the hierarchy, while cascade classification greedily traverses the hierarchy from the root to the predicted leaf. In this paper we propose a new approach, which extends cascade classification to predict the right leaf by estimating the probability of each root-to-leaf path. We provide experimental results which indicate that, using the same classification algorithm, one can achieve better results with our approach, compared to the traditional flat and cascade classifications.\n    ",
        "submission_date": "2015-05-09T00:00:00",
        "last_modified_date": "2015-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04630",
        "title": "Recurrent Neural Network Training with Dark Knowledge Transfer",
        "authors": [
            "Zhiyuan Tang",
            "Dong Wang",
            "Zhiyong Zhang"
        ],
        "abstract": "Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2016-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04657",
        "title": "Mining User Opinions in Mobile App Reviews: A Keyword-based Approach",
        "authors": [
            "Phong Minh Vu",
            "Tam The Nguyen",
            "Hung Viet Pham",
            "Tung Thanh Nguyen"
        ],
        "abstract": "User reviews of mobile apps often contain complaints or suggestions which are valuable for app developers to improve user experience and satisfaction. However, due to the large volume and noisy-nature of those reviews, manually analyzing them for useful opinions is inherently challenging. To address this problem, we propose MARK, a keyword-based framework for semi-automated review analysis. MARK allows an analyst describing his interests in one or some mobile apps by a set of keywords. It then finds and lists the reviews most relevant to those keywords for further analysis. It can also draw the trends over time of those keywords and detect their sudden changes, which might indicate the occurrences of serious issues. To help analysts describe their interests more effectively, MARK can automatically extract keywords from raw reviews and rank them by their associations with negative reviews. In addition, based on a vector-based semantic representation of keywords, MARK can divide a large set of keywords into more cohesive subsets, or suggest keywords similar to the selected ones.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04771",
        "title": "DopeLearning: A Computational Approach to Rap Lyrics Generation",
        "authors": [
            "Eric Malmi",
            "Pyry Takala",
            "Hannu Toivonen",
            "Tapani Raiko",
            "Aristides Gionis"
        ],
        "abstract": "Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04870",
        "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
        "authors": [
            "Bryan A. Plummer",
            "Liwei Wang",
            "Chris M. Cervantes",
            "Juan C. Caicedo",
            "Julia Hockenmaier",
            "Svetlana Lazebnik"
        ],
        "abstract": "The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05612",
        "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering",
        "authors": [
            "Haoyuan Gao",
            "Junhua Mao",
            "Jie Zhou",
            "Zhiheng Huang",
            "Lei Wang",
            "Wei Xu"
        ],
        "abstract": "In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06027",
        "title": "Weakly-Supervised Alignment of Video With Text",
        "authors": [
            "Piotr Bojanowski",
            "R\u00e9mi Lajugie",
            "Edouard Grave",
            "Francis Bach",
            "Ivan Laptev",
            "Jean Ponce",
            "Cordelia Schmid"
        ],
        "abstract": "Suppose that we are given a set of videos, along with natural language descriptions in the form of multiple sentences (e.g., manual annotations, movie scripts, sport summaries etc.), and that these sentences appear in the same temporal order as their visual counterparts. We propose in this paper a method for aligning the two modalities, i.e., automatically providing a time stamp for every sentence. Given vectorial features for both video and text, we propose to cast this task as a temporal assignment problem, with an implicit linear mapping between the two feature modalities. We formulate this problem as an integer quadratic program, and solve its continuous convex relaxation using an efficient conditional gradient algorithm. Several rounding procedures are proposed to construct the final integer solution. After demonstrating significant improvements over the state of the art on the related task of aligning video with symbolic labels [7], we evaluate our method on a challenging dataset of videos with associated textual descriptions [36], using both bag-of-words and continuous representations for text.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06750",
        "title": "Reply to Garcia et al.: Common mistakes in measuring frequency dependent word characteristics",
        "authors": [
            "P. S. Dodds",
            "E. M. Clark",
            "S. Desu",
            "M. R. Frank",
            "A. J. Reagan",
            "J. R. Williams",
            "L. Mitchell",
            "K. D. Harris",
            "I. M. Kloumann",
            "J. P. Bagrow",
            "K. Megerdoomian",
            "M. T. McMahon",
            "B. F. Tivnan",
            "C. M. Danforth"
        ],
        "abstract": "We demonstrate that the concerns expressed by Garcia et al. are misplaced, due to (1) a misreading of our findings in [1]; (2) a widespread failure to examine and present words in support of asserted summary quantities based on word usage frequencies; and (3) a range of misconceptions about word usage frequency, word rank, and expert-constructed word lists. In particular, we show that the English component of our study compares well statistically with two related surveys, that no survey design influence is apparent, and that estimates of measurement error do not explain the positivity biases reported in our work and that of others. We further demonstrate that for the frequency dependence of positivity---of which we explored the nuances in great detail in [1]---Garcia et al. did not perform a reanalysis of our data---they instead carried out an analysis of a different, statistically improper data set and introduced a nonlinearity before performing linear regression.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07712",
        "title": "A Category Theory of Communication Theory",
        "authors": [
            "Eric Werner"
        ],
        "abstract": "A theory of how agents can come to understand a language is presented. If understanding a sentence $\\alpha$ is to associate an operator with $\\alpha$ that transforms the representational state of the agent as intended by the sender, then coming to know a language involves coming to know the operators that correspond to the meaning of any sentence. This involves a higher order operator that operates on the possible transformations that operate on the representational capacity of the agent. We formalize these constructs using concepts and diagrams analogous to category theory.\n    ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00278",
        "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering",
        "authors": [
            "Licheng Yu",
            "Eunbyung Park",
            "Alexander C. Berg",
            "Tamara L. Berg"
        ],
        "abstract": "In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.\n    ",
        "submission_date": "2015-05-31T00:00:00",
        "last_modified_date": "2015-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00301",
        "title": "Interactive Knowledge Base Population",
        "authors": [
            "Travis Wolfe",
            "Mark Dredze",
            "James Mayfield",
            "Paul McNamee",
            "Craig Harman",
            "Tim Finin",
            "Benjamin Van Durme"
        ],
        "abstract": "Most work on building knowledge bases has focused on collecting entities and facts from as large a collection of documents as possible. We argue for and describe a new paradigm where the focus is on a high-recall extraction over a small collection of documents under the supervision of a human expert, that we call Interactive Knowledge Base Population (IKBP).\n    ",
        "submission_date": "2015-05-31T00:00:00",
        "last_modified_date": "2015-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00468",
        "title": "Classifying Tweet Level Judgements of Rumours in Social Media",
        "authors": [
            "Michal Lukasik",
            "Trevor Cohn",
            "Kalina Bontcheva"
        ],
        "abstract": "Social media is a rich source of rumours and corresponding community reactions. Rumours reflect different characteristics, some shared and some individual. We formulate the problem of classifying tweet level judgements of rumours as a supervised learning task. Both supervised and unsupervised domain adaptation are considered, in which tweets from a rumour are classified on the basis of other annotated rumours. We demonstrate how multi-task learning helps achieve good results on rumours from the 2011 England riots.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00572",
        "title": "How much is said in a microblog? A multilingual inquiry based on Weibo and Twitter",
        "authors": [
            "Han-Teng Liao",
            "King-wa Fu",
            "Scott A. Hale"
        ],
        "abstract": "This paper presents a multilingual study on, per single post of microblog text, (a) how much can be said, (b) how much is written in terms of characters and bytes, and (c) how much is said in terms of information content in posts by different organizations in different languages. Focusing on three different languages (English, Chinese, and Japanese), this research analyses Weibo and Twitter accounts of major embassies and news agencies. We first establish our criterion for quantifying \"how much can be said\" in a digital text based on the openly available Universal Declaration of Human Rights and the translated subtitles from TED talks. These parallel corpora allow us to determine the number of characters and bits needed to represent the same content in different languages and character encodings. We then derive the amount of information that is actually contained in microblog posts authored by selected accounts on Weibo and Twitter. Our results confirm that languages with larger character sets such as Chinese and Japanese contain more information per character than English, but the actual information content contained within a microblog text varies depending on both the type of organization and the language of the post. We conclude with a discussion on the design implications of microblog text limits for different languages.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00578",
        "title": "On Quantum Generalizations of Information-Theoretic Measures and their Contribution to Distributional Semantics",
        "authors": [
            "William Blacoe"
        ],
        "abstract": "Information-theoretic measures such as relative entropy and correlation are extremely useful when modeling or analyzing the interaction of probabilistic systems. We survey the quantum generalization of 5 such measures and point out some of their commonalities and interpretations. In particular we find the application of information theory to distributional semantics useful. By modeling the distributional meaning of words as density operators rather than vectors, more of their semantic structure may be exploited. Furthermore, properties of and interactions between words such as ambiguity, similarity and entailment can be simulated more richly and intuitively when using methods from quantum information theory.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00765",
        "title": "Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology",
        "authors": [
            "Zheng Cai",
            "Donglin Cao",
            "Rongrong Ji"
        ],
        "abstract": "With faster connection speed, Internet users are now making social network a huge reservoir of texts, images and video clips (GIF). Sentiment analysis for such online platform can be used to predict political elections, evaluates economic indicators and so on. However, GIF sentiment analysis is quite challenging, not only because it hinges on spatio-temporal visual contentabstraction, but also for the relationship between such abstraction and final sentiment remains ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00999",
        "title": "Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases",
        "authors": [
            "Alberto Garcia-Duran",
            "Antoine Bordes",
            "Nicolas Usunier",
            "Yves Grandvalet"
        ],
        "abstract": "This paper tackles the problem of endogenous link prediction for Knowledge Base completion. Knowledge Bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose Tatec a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01698",
        "title": "The Long-Short Story of Movie Description",
        "authors": [
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Bernt Schiele"
        ],
        "abstract": "Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII Movie Description allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging MPII-MD dataset. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02075",
        "title": "Large-scale Simple Question Answering with Memory Networks",
        "authors": [
            "Antoine Bordes",
            "Nicolas Usunier",
            "Sumit Chopra",
            "Jason Weston"
        ],
        "abstract": "Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02078",
        "title": "Visualizing and Understanding Recurrent Networks",
        "authors": [
            "Andrej Karpathy",
            "Justin Johnson",
            "Li Fei-Fei"
        ],
        "abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02170",
        "title": "Hybridized Feature Extraction and Acoustic Modelling Approach for Dysarthric Speech Recognition",
        "authors": [
            "Megha Rughani",
            "D.Shivakrishna"
        ],
        "abstract": "Dysarthria is malfunctioning of motor speech caused by faintness in the human nervous system. It is characterized by the slurred speech along with physical impairment which restricts their communication and creates the lack of confidence and affects the lifestyle. This paper attempt to increase the efficiency of Automatic Speech Recognition (ASR) system for unimpaired speech signal. It describes state of art of research into improving ASR for speakers with dysarthria by means of incorporated knowledge of their speech production. Hybridized approach for feature extraction and acoustic modelling technique along with evolutionary algorithm is proposed for increasing the efficiency of the overall system. Here number of feature vectors are varied and tested the system performance. It is observed that system performance is boosted by genetic algorithm. System with 16 acoustic features optimized with genetic algorithm has obtained highest recognition rate of 98.28% with training time of 5:30:17.\n    ",
        "submission_date": "2015-06-06T00:00:00",
        "last_modified_date": "2015-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02516",
        "title": "Learning to Transduce with Unbounded Memory",
        "authors": [
            "Edward Grefenstette",
            "Karl Moritz Hermann",
            "Mustafa Suleyman",
            "Phil Blunsom"
        ],
        "abstract": "Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03099",
        "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
        "authors": [
            "Samy Bengio",
            "Oriol Vinyals",
            "Navdeep Jaitly",
            "Noam Shazeer"
        ],
        "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03500",
        "title": "Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation",
        "authors": [
            "Angeliki Lazaridou",
            "Dat Tien Nguyen",
            "Raffaella Bernardi",
            "Marco Baroni"
        ],
        "abstract": "We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04147",
        "title": "On the accuracy of self-normalized log-linear models",
        "authors": [
            "Jacob Andreas",
            "Maxim Rabinovich",
            "Dan Klein",
            "Michael I. Jordan"
        ],
        "abstract": "Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as \"self-normalization\", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05514",
        "title": "Learning Contextualized Semantics from Co-occurring Terms via a Siamese Architecture",
        "authors": [
            "Ubai Sandouk",
            "Ke Chen"
        ],
        "abstract": "One of the biggest challenges in Multimedia information retrieval and understanding is to bridge the semantic gap by properly modeling concept semantics in context. The presence of out of vocabulary (OOV) concepts exacerbates this difficulty. To address the semantic gap issues, we formulate a problem on learning contextualized semantics from descriptive terms and propose a novel Siamese architecture to model the contextualized semantics from descriptive terms. By means of pattern aggregation and probabilistic topic models, our Siamese architecture captures contextualized semantics from the co-occurring descriptive terms via unsupervised learning, which leads to a concept embedding space of the terms in context. Furthermore, the co-occurring OOV concepts can be easily represented in the learnt concept embedding space. The main properties of the concept embedding space are demonstrated via visualization. Using various settings in semantic priming, we have carried out a thorough evaluation by comparing our approach to a number of state-of-the-art methods on six annotation corpora in different domains, i.e., MagTag5K, CAL500 and Million Song Dataset in the music domain as well as Corel5K, LabelMe and SUNDatabase in the image domain. Experimental results on semantic priming suggest that our approach outperforms those state-of-the-art methods considerably in various aspects.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06646",
        "title": "Nonparametric Bayesian Double Articulation Analyzer for Direct Language Acquisition from Continuous Speech Signals",
        "authors": [
            "Tadahiro Taniguchi",
            "Ryo Nakashima",
            "Shogo Nagasaka"
        ],
        "abstract": "Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the \"hierarchical Dirichlet process hidden language model\" (HDP-HLM). The HDP-HLM is obtained by extending the hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. An inference procedure for the HDP-HLM is derived using the blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. Based on the HDP-HLM and its inference procedure, we developed a novel double articulation analyzer. By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, the method can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner. The novel unsupervised double articulation analyzer is called NPB-DAA.\n",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06724",
        "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books",
        "authors": [
            "Yukun Zhu",
            "Ryan Kiros",
            "Richard Zemel",
            "Ruslan Salakhutdinov",
            "Raquel Urtasun",
            "Antonio Torralba",
            "Sanja Fidler"
        ],
        "abstract": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06832",
        "title": "Detection and Analysis of Emotion From Speech Signals",
        "authors": [
            "Assel Davletcharova",
            "Sherin Sugathan",
            "Bibia Abraham",
            "Alex Pappachen James"
        ],
        "abstract": "Recognizing emotion from speech has become one the active research themes in speech processing and in applications based on human-computer interaction. This paper conducts an experimental study on recognizing emotions from human speech. The emotions considered for the experiments include neutral, anger, joy and sadness. The distinuishability of emotional features in speech were studied first followed by emotion classification performed on a custom dataset. The classification was performed for different classifiers. One of the main feature attribute considered in the prepared dataset was the peak-to-peak distance obtained from the graphical representation of the speech signals. After performing the classification tests on a dataset formed from 30 different subjects, it was found that for getting better accuracy, one should consider the data collected from one person rather than considering the data from a group of people.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07220",
        "title": "Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks",
        "authors": [
            "Yangtuo Peng",
            "Hui Jiang"
        ],
        "abstract": "Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage financial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can significantly improve the stock prediction accuracy on a standard financial database over the baseline system using only the historical price information.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07477",
        "title": "Efficient Learning for Undirected Topic Models",
        "authors": [
            "Jiatao Gu",
            "Victor O.K. Li"
        ],
        "abstract": "Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07732",
        "title": "How to improve robustness in Kohonen maps and display additional information in Factorial Analysis: application to text mining",
        "authors": [
            "Nicolas Bourgeois",
            "Marie Cottrell",
            "Benjamin D\u00e9ruelle",
            "St\u00e9phane Lamass\u00e9",
            "Patrick Letr\u00e9my"
        ],
        "abstract": "This article is an extended version of a paper presented in the WSOM'2012 conference [1]. We display a combination of factorial projections, SOM algorithm and graph techniques applied to a text mining problem. The corpus contains 8 medieval manuscripts which were used to teach arithmetic techniques to merchants. Among the techniques for Data Analysis, those used for Lexicometry (such as Factorial Analysis) highlight the discrepancies between manuscripts. The reason for this is that they focus on the deviation from the independence between words and manuscripts. Still, we also want to discover and characterize the common vocabulary among the whole corpus. Using the properties of stochastic Kohonen maps, which define neighborhood between inputs in a non-deterministic way, we highlight the words which seem to play a special role in the vocabulary. We call them fickle and use them to improve both Kohonen map robustness and significance of FCA visualization. Finally we use graph algorithmic to exploit this fickleness for classification of words.\n    ",
        "submission_date": "2015-06-25T00:00:00",
        "last_modified_date": "2015-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08789",
        "title": "Requirement Tracing using Term Extraction",
        "authors": [
            "Najla Al-Saati",
            "Raghda Abdul-Jaleel"
        ],
        "abstract": "Requirements traceability is an essential step in ensuring the quality of software during the early stages of its development life cycle. Requirements tracing usually consists of document parsing, candidate link generation and evaluation and traceability analysis. This paper demonstrates the applicability of Statistical Term Extraction metrics to generate candidate links. It is applied and validated using two data sets and four types of filters two for each data set, 0.2 and 0.25 for MODIS, 0 and 0.05 for CM1. This method generates requirements traceability matrices between textual requirements artifacts (such as high-level requirements traced to low-level requirements). The proposed method includes ten word frequency metrics divided into three main groups for calculating the frequency of terms. The results show that the proposed method gives better result when compared with the traditional TF-IDF method.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01053",
        "title": "Describing Multimedia Content using Attention-based Encoder--Decoder Networks",
        "authors": [
            "Kyunghyun Cho",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.\n    ",
        "submission_date": "2015-07-04T00:00:00",
        "last_modified_date": "2015-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01526",
        "title": "Grid Long Short-Term Memory",
        "authors": [
            "Nal Kalchbrenner",
            "Ivo Danihelka",
            "Alex Graves"
        ],
        "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02140",
        "title": "Mining and Analyzing the Future Works in Scientific Articles",
        "authors": [
            "Yue Hu",
            "Xiaojun Wan"
        ],
        "abstract": "Future works in scientific articles are valuable for researchers and they can guide researchers to new research directions or ideas. In this paper, we mine the future works in scientific articles in order to 1) provide an insight for future work analysis and 2) facilitate researchers to search and browse future works in a research area. First, we study the problem of future work extraction and propose a regular expression based method to address the problem. Second, we define four different categories for the future works by observing the data and investigate the multi-class future work classification problem. Third, we apply the extraction method and the classification model to a paper dataset in the computer science field and conduct a further analysis of the future works. Finally, we design a prototype system to search and demonstrate the future works mined from the scientific papers. Our evaluation results show that our extraction method can get high precision and recall values and our classification model can also get good results and it outperforms several baseline models. Further analysis of the future work sentences also indicates interesting results.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02447",
        "title": "Data Mining of Causal Relations from Text: Analysing Maritime Accident Investigation Reports",
        "authors": [
            "Santosh Tirunagari"
        ],
        "abstract": "Text mining is a process of extracting information of interest from text. Such a method includes techniques from various areas such as Information Retrieval (IR), Natural Language Processing (NLP), and Information Extraction (IE). In this study, text mining methods are applied to extract causal relations from maritime accident investigation reports collected from the Marine Accident Investigation Branch (MAIB). These causal relations provide information on various mechanisms behind accidents, including human and organizational factors relating to the accident. The objective of this study is to facilitate the analysis of the maritime accident investigation reports, by means of extracting contributory causes with more feasibility. A careful investigation of contributory causes from the reports provide opportunity to improve safety in future.\n",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02907",
        "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach",
        "authors": [
            "Lu\u00eds Marujo",
            "Ricardo Ribeiro",
            "David Martins de Matos",
            "Jo\u00e3o P. Neto",
            "Anatole Gershman",
            "Jaime Carbonell"
        ],
        "abstract": "The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03045",
        "title": "Markov Logic Networks for Natural Language Question Answering",
        "authors": [
            "Tushar Khot",
            "Niranjan Balasubramanian",
            "Eric Gribkoff",
            "Ashish Sabharwal",
            "Peter Clark",
            "Oren Etzioni"
        ],
        "abstract": "Our goal is to answer elementary-level science questions using knowledge extracted automatically from science textbooks, expressed in a subset of first-order logic. Given the incomplete and noisy nature of these automatically extracted rules, Markov Logic Networks (MLNs) seem a natural model to use, but the exact way of leveraging MLNs is by no means obvious. We investigate three ways of applying MLNs to our task. In the first, we simply use the extracted science rules directly as MLN clauses. Unlike typical MLN applications, our domain has long and complex rules, leading to an unmanageable number of groundings. We exploit the structure present in hard constraints to improve tractability, but the formulation remains ineffective. In the second approach, we instead interpret science rules as describing prototypical entities, thus mapping rules directly to grounded MLN assertions, whose constants are then clustered using existing entity resolution methods. This drastically simplifies the network, but still suffers from brittleness. Finally, our third approach, called Praline, uses MLNs to align the lexical elements as well as define and control how inference should be performed in this task. Our experiments, demonstrating a 15\\% accuracy boost and a 10x reduction in runtime, suggest that the flexibility and different inference semantics of Praline are a better fit for the natural language question answering task.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04116",
        "title": "Language discrimination and clustering via a neural network approach",
        "authors": [
            "Angelo Mariano",
            "Giorgio Parisi",
            "Saverio Pascazio"
        ],
        "abstract": "We classify twenty-one Indo-European languages starting from written text. We use neural networks in order to define a distance among different languages, construct a dendrogram and analyze the ultrametric structure that emerges. Four or five subgroups of languages are identified, according to the \"cut\" of the dendrogram, drawn with an entropic criterion. The results and the method are discussed.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04798",
        "title": "Exploratory topic modeling with distributional semantics",
        "authors": [
            "Samuel R\u00f6nnqvist"
        ],
        "abstract": "As we continue to collect and store textual data in a multitude of domains, we are regularly confronted with material whose largely unknown thematic structure we want to uncover. With unsupervised, exploratory analysis, no prior knowledge about the content is required and highly open-ended tasks can be supported. In the past few years, probabilistic topic modeling has emerged as a popular approach to this problem. Nevertheless, the representation of the latent topics as aggregations of semi-coherent terms limits their interpretability and level of detail.\n",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04908",
        "title": "Analysis of the South Slavic Scripts by Run-Length Features of the Image Texture",
        "authors": [
            "Darko Brodic",
            "Zoran N. Milivojevic",
            "Alessia Amelio"
        ],
        "abstract": "The paper proposes an algorithm for the script recognition based on the texture characteristics. The image texture is achieved by coding each letter with the equivalent script type (number code) according to its position in the text line. Each code is transformed into equivalent gray level pixel creating an 1-D image. Then, the image texture is subjected to the run-length analysis. This analysis extracts the run-length features, which are classified to make a distinction between the scripts under consideration. In the experiment, a custom oriented database is subject to the proposed algorithm. The database consists of some text documents written in Cyrillic, Latin and Glagolitic scripts. Furthermore, it is divided into training and test parts. The results of the experiment show that 3 out of 5 run-length features can be used for effective differentiation between the analyzed South Slavic scripts.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05910",
        "title": "Clustering is Efficient for Approximate Maximum Inner Product Search",
        "authors": [
            "Alex Auvolat",
            "Sarath Chandar",
            "Pascal Vincent",
            "Hugo Larochelle",
            "Yoshua Bengio"
        ],
        "abstract": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06711",
        "title": "The SYSU System for the Interspeech 2015 Automatic Speaker Verification Spoofing and Countermeasures Challenge",
        "authors": [
            "Shitao Weng",
            "Shushan Chen",
            "Lei Yu",
            "Xuewei Wu",
            "Weicheng Cai",
            "Zhi Liu",
            "Ming Li"
        ],
        "abstract": "Many existing speaker verification systems are reported to be vulnerable against different spoofing attacks, for example speaker-adapted speech synthesis, voice conversion, play back, etc. In order to detect these spoofed speech signals as a countermeasure, we propose a score level fusion approach with several different i-vector subsystems. We show that the acoustic level Mel-frequency cepstral coefficients (MFCC) features, the phase level modified group delay cepstral coefficients (MGDCC) and the phonetic level phoneme posterior probability (PPP) tandem features are effective for the countermeasure. Furthermore, feature level fusion of these features before i-vector modeling also enhance the performance. A polynomial kernel support vector machine is adopted as the supervised classifier. In order to enhance the generalizability of the countermeasure, we also adopted the cosine similarity and PLDA scoring as one-class classifications methods. By combining the proposed i-vector subsystems with the OpenSMILE baseline which covers the acoustic and prosodic information further improves the final performance. The proposed fusion system achieves 0.29% and 3.26% EER on the development and test set of the database provided by the INTERSPEECH 2015 automatic speaker verification spoofing and countermeasures challenge.\n    ",
        "submission_date": "2015-07-24T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00354",
        "title": "Significance of Maximum Spectral Amplitude in Sub-bands for Spectral Envelope Estimation and Its Application to Statistical Parametric Speech Synthesis",
        "authors": [
            "Sivanand Achanta",
            "Anandaswarup Vadapalli",
            "Sai Krishna R.",
            "Suryakanth V. Gangashetty"
        ],
        "abstract": "In this paper we propose a technique for spectral envelope estimation using maximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Most other methods in the literature parametrize spectral envelope in cepstral domain such as Mel-generalized cepstrum etc. Such cepstral domain representations, although compact, are not readily interpretable. This difficulty is overcome by our method which parametrizes in the spectral domain itself. In our experiments, spectral envelope estimated using MSASB method was incorporated in the STRAIGHT vocoder. Both objective and subjective results of analysis-by-synthesis indicate that the proposed method is comparable to STRAIGHT. We also evaluate the effectiveness of the proposed parametrization in a statistical parametric speech synthesis framework using deep neural networks.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00973",
        "title": "Progressive EM for Latent Tree Models and Hierarchical Topic Detection",
        "authors": [
            "Peixian Chen",
            "Nevin L. Zhang",
            "Leonard K.M. Poon",
            "Zhourong Chen"
        ],
        "abstract": "Hierarchical latent tree analysis (HLTA) is recently proposed as a new method for topic detection. It differs fundamentally from the LDA-based methods in terms of topic definition, topic-document relationship, and learning method. It has been shown to discover significantly more coherent topics and better topic hierarchies. However, HLTA relies on the Expectation-Maximization (EM) algorithm for parameter estimation and hence is not efficient enough to deal with large datasets. In this paper, we propose a method to drastically speed up HLTA using a technique inspired by recent advances in the moments method. Empirical experiments show that our method greatly improves the efficiency of HLTA. It is as efficient as the state-of-the-art LDA-based method for hierarchical topic detection and finds substantially better topics and topic hierarchies.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01011",
        "title": "Learning from LDA using Deep Neural Networks",
        "authors": [
            "Dongxu Zhang",
            "Tianyi Luo",
            "Dong Wang",
            "Rong Liu"
        ],
        "abstract": "Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian model for topic inference. In spite of its great success, inferring the latent topic distribution with LDA is time-consuming. Motivated by the transfer learning approach proposed by~\\newcite{hinton2015distilling}, we present a novel method that uses LDA to supervise the training of a deep neural network (DNN), so that the DNN can approximate the costly LDA inference with less computation. Our experiments on a document classification task show that a simple DNN can learn the LDA behavior pretty well, while the inference is speeded up tens or hundreds of times.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01420",
        "title": "Privacy-Preserving Multi-Document Summarization",
        "authors": [
            "Lu\u00eds Marujo",
            "Jos\u00e9 Port\u00ealo",
            "Wang Ling",
            "David Martins de Matos",
            "Jo\u00e3o P. Neto",
            "Anatole Gershman",
            "Jaime Carbonell",
            "Isabel Trancoso",
            "Bhiksha Raj"
        ],
        "abstract": "State-of-the-art extractive multi-document summarization systems are usually designed without any concern about privacy issues, meaning that all documents are open to third parties. In this paper we propose a privacy-preserving approach to multi-document summarization. Our approach enables other parties to obtain summaries without learning anything else about the original documents' content. We use a hashing scheme known as Secure Binary Embeddings to convert documents representation containing key phrases and bag-of-words into bit strings, allowing the computation of approximate distances, instead of exact ones. Our experiments indicate that our system yields similar results to its non-private counterpart on standard multi-document evaluation datasets.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01571",
        "title": "A Mood-based Genre Classification of Television Content",
        "authors": [
            "Humberto Corona",
            "Michael P. O'Mahony"
        ],
        "abstract": "The classification of television content helps users organise and navigate through the large list of channels and programs now available. In this paper, we address the problem of television content classification by exploiting text information extracted from program transcriptions. We present an analysis which adapts a model for sentiment that has been widely and successfully applied in other fields such as music or blog posts. We use a real-world dataset obtained from the Boxfish API to compare the performance of classifiers trained on a number of different feature sets. Our experiments show that, over a large collection of television content, program genres can be represented in a three-dimensional space of valence, arousal and dominance, and that promising classification results can be achieved using features based on this representation. This finding supports the use of the proposed representation of television content as a feature space for similarity computation and recommendation generation.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01746",
        "title": "Using Deep Learning for Detecting Spoofing Attacks on Speech Signals",
        "authors": [
            "Alan Godoy",
            "Fl\u00e1vio Sim\u00f5es",
            "Jos\u00e9 Augusto Stuchi",
            "Marcus de Assis Angeloni",
            "M\u00e1rio Uliani",
            "Ricardo Violato"
        ],
        "abstract": "It is well known that speaker verification systems are subject to spoofing attacks. The Automatic Speaker Verification Spoofing and Countermeasures Challenge -- ASVSpoof2015 -- provides a standard spoofing database, containing attacks based on synthetic speech, along with a protocol for experiments. This paper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based on deep neural networks, working both as a classifier and as a feature extraction module for a GMM and a SVM classifier. Results show the validity of this approach, achieving less than 0.5\\% EER for known attacks.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01993",
        "title": "Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures",
        "authors": [
            "Stefan Feuerriegel",
            "Ralph Fehrer"
        ],
        "abstract": "Decision analytics commonly focuses on the text mining of financial news sources in order to provide managerial decision support and to predict stock market movements. Existing predictive frameworks almost exclusively apply traditional machine learning methods, whereas recent research indicates that traditional machine learning methods are not sufficiently capable of extracting suitable features and capturing the non-linear nature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending traditional neural network models with additional hidden layers. Indeed, deep learning has been shown to outperform traditional methods in terms of predictive performance. In this paper, we adapt the novel deep learning technique to financial decision support. In this instance, we aim to predict the direction of stock movements following financial disclosures. As a result, we show how deep learning can outperform the accuracy of random forests as a benchmark for machine learning by 5.66%.\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2018-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03170",
        "title": "Generation of Multimedia Artifacts: An Extractive Summarization-based Approach",
        "authors": [
            "Paulo Figueiredo",
            "Marta Apar\u00edcio",
            "David Martins de Matos",
            "Ricardo Ribeiro"
        ],
        "abstract": "We explore methods for content selection and address the issue of coherence in the context of the generation of multimedia artifacts. We use audio and video to present two case studies: generation of film tributes, and lecture-driven science talks. For content selection, we use centrality-based and diversity-based summarization, along with topic analysis. To establish coherence, we use the emotional content of music, for film tributes, and ensure topic similarity between lectures and documentaries, for science talks. Composition techniques for the production of multimedia artifacts are addressed as a means of organizing content, in order to improve coherence. We discuss our results considering the above aspects.\n    ",
        "submission_date": "2015-08-13T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03276",
        "title": "Talking about the Moving Image: A Declarative Model for Image Schema Based Embodied Perception Grounding and Language Generation",
        "authors": [
            "Jakob Suchan",
            "Mehul Bhatt",
            "Harshita Jhavar"
        ],
        "abstract": "We present a general theory and corresponding declarative model for the embodied grounding and natural language based analytical summarisation of dynamic visuo-spatial imagery. The declarative model ---ecompassing spatio-linguistic abstractions, image schemas, and a spatio-temporal feature based language generator--- is modularly implemented within Constraint Logic Programming (CLP). The implemented model is such that primitives of the theory, e.g., pertaining to space and motion, image schemata, are available as first-class objects with `deep semantics' suited for inference and query. We demonstrate the model with select examples broadly motivated by areas such as film, design, geography, smart environments where analytical natural language based externalisations of the moving image are central from the viewpoint of human interaction, evidence-based qualitative analysis, and sensemaking.\n",
        "submission_date": "2015-08-13T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03386",
        "title": "Learning from Real Users: Rating Dialogue Success with Neural Networks for Reinforcement Learning in Spoken Dialogue Systems",
        "authors": [
            "Pei-Hao Su",
            "David Vandyke",
            "Milica Gasic",
            "Dongho Kim",
            "Nikola Mrksic",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "To train a statistical spoken dialogue system (SDS) it is essential that an accurate method for measuring task success is available. To date training has relied on presenting a task to either simulated or paid users and inferring the dialogue's success by observing whether this presented task was achieved or not. Our aim however is to be able to learn from real users acting under their own volition, in which case it is non-trivial to rate the success as any prior knowledge of the task is simply unavailable. User feedback may be utilised but has been found to be inconsistent. Hence, here we present two neural network models that evaluate a sequence of turn-level features to rate the success of a dialogue. Importantly these models make no use of any prior knowledge of the user's task. The models are trained on dialogues generated by a simulated user and the best model is then used to train a policy on-line which is shown to perform at least as well as a baseline system using prior knowledge of the user's task. We note that the models should also be of interest for evaluating SDS and for monitoring a dialogue in rule-based SDS.\n    ",
        "submission_date": "2015-08-13T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03391",
        "title": "Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems",
        "authors": [
            "Pei-Hao Su",
            "David Vandyke",
            "Milica Gasic",
            "Nikola Mrksic",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "Statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users. However in the reinforcement learning paradigm the dialogue manager (agent) often requires significant time to explore the state-action space to learn to behave in a desirable manner. This is a critical issue when the system is trained on-line with real users where learning costs are expensive. Reward shaping is one promising technique for addressing these concerns. Here we examine three recurrent neural network (RNN) approaches for providing reward shaping information in addition to the primary (task-orientated) environmental feedback. These RNNs are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster. In both simulated and real user scenarios these RNNs are shown to increase policy learning speed. Importantly, they do not require prior knowledge of the user's goal.\n    ",
        "submission_date": "2015-08-14T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03601",
        "title": "Is Stack Overflow Overflowing With Questions and Tags",
        "authors": [
            "Ranjitha R. K.",
            "Sanjay Singh"
        ],
        "abstract": "Programming question and answer (Q & A) websites, such as Quora, Stack Overflow, and Yahoo! Answer etc. helps us to understand the programming concepts easily and quickly in a way that has been tested and applied by many software developers. Stack Overflow is one of the most frequently used programming Q\\&A website where the questions and answers posted are presently analyzed manually, which requires a huge amount of time and resource. To save the effort, we present a topic modeling based technique to analyze the words of the original texts to discover the themes that run through them. We also propose a method to automate the process of reviewing the quality of questions on Stack Overflow dataset in order to avoid ballooning the stack overflow with insignificant questions. The proposed method also recommends the appropriate tags for the new post, which averts the creation of unnecessary tags on Stack Overflow.\n    ",
        "submission_date": "2015-08-14T00:00:00",
        "last_modified_date": "2015-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03790",
        "title": "Depth-Gated LSTM",
        "authors": [
            "Kaisheng Yao",
            "Trevor Cohn",
            "Katerina Vylomova",
            "Kevin Duh",
            "Chris Dyer"
        ],
        "abstract": "In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03868",
        "title": "Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology",
        "authors": [
            "Brendan Jou",
            "Tao Chen",
            "Nikolaos Pappas",
            "Miriam Redi",
            "Mercan Topkara",
            "Shih-Fu Chang"
        ],
        "abstract": "Every culture and language is unique. Our work expressly focuses on the uniqueness of culture and language in relation to human affect, specifically sentiment and emotion semantics, and how they manifest in social multimedia. We develop sets of sentiment- and emotion-polarized visual concepts by adapting semantic structures called adjective-noun pairs, originally introduced by Borth et al. (2013), but in a multilingual context. We propose a new language-dependent method for automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform for the creation of a large-scale multilingual visual sentiment concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns. In addition, we present an image-based prediction task to show how generalizable language-specific models are in a multilingual context. A new, publicly available dataset of >15.6K sentiment-biased visual concepts across 12 languages with language-specific detector banks, >7.36M images and their metadata is also released.\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05508",
        "title": "Towards Neural Network-based Reasoning",
        "authors": [
            "Baolin Peng",
            "Zhengdong Lu",
            "Hang Li",
            "Kam-Fai Wong"
        ],
        "abstract": "We propose Neural Reasoner, a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular structure exists in the question and facts, Neural Reasoner is able to accommodate different types of reasoning and different forms of language expressions. Despite the model complexity, Neural Reasoner can still be trained effectively in an end-to-end manner. Our empirical studies show that Neural Reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks (Positional Reasoning and Path Finding) proposed in [8]. For example, it improves the accuracy on Path Finding(10K) from 33.4% [6] to over 98%.\n    ",
        "submission_date": "2015-08-22T00:00:00",
        "last_modified_date": "2015-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05565",
        "title": "Necessary and Sufficient Conditions and a Provably Efficient Algorithm for Separable Topic Discovery",
        "authors": [
            "Weicong Ding",
            "Prakash Ishwar",
            "Venkatesh Saligrama"
        ],
        "abstract": "We develop necessary and sufficient conditions and a novel provably consistent and efficient algorithm for discovering topics (latent factors) from observations (documents) that are realized from a probabilistic mixture of shared latent factors that have certain properties. Our focus is on the class of topic models in which each shared latent factor contains a novel word that is unique to that factor, a property that has come to be known as separability. Our algorithm is based on the key insight that the novel words correspond to the extreme points of the convex hull formed by the row-vectors of a suitably normalized word co-occurrence matrix. We leverage this geometric insight to establish polynomial computation and sample complexity bounds based on a few isotropic random projections of the rows of the normalized word co-occurrence matrix. Our proposed random-projections-based algorithm is naturally amenable to an efficient distributed implementation and is attractive for modern web-scale distributed data mining applications.\n    ",
        "submission_date": "2015-08-23T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06161",
        "title": "Robot Language Learning, Generation, and Comprehension",
        "authors": [
            "Daniel Paul Barrett",
            "Scott Alan Bronikowski",
            "Haonan Yu",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We present a unified framework which supports grounding natural-language semantics in robotic driving. This framework supports acquisition (learning grounded meanings of nouns and prepositions from human annotation of robotic driving paths), generation (using such acquired meanings to generate sentential description of new robotic driving paths), and comprehension (using such acquired meanings to support automated driving to accomplish navigational goals specified in natural language). We evaluate the performance of these three tasks by having independent human judges rate the semantic fidelity of the sentences associated with paths, achieving overall average correctness of 94.6% and overall average completeness of 85.6%.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07266",
        "title": "Understanding Editing Behaviors in Multilingual Wikipedia",
        "authors": [
            "Suin Kim",
            "Sungjoon Park",
            "Scott A. Hale",
            "Sooyoung Kim",
            "Jeongmin Byun",
            "Alice Oh"
        ],
        "abstract": "Multilingualism is common offline, but we have a more limited understanding of the ways multilingualism is displayed online and the roles that multilinguals play in the spread of content between speakers of different languages. We take a computational approach to studying multilingualism using one of the largest user-generated content platforms, Wikipedia. We study multilingualism by collecting and analyzing a large dataset of the content written by multilingual editors of the English, German, and Spanish editions of Wikipedia. This dataset contains over two million paragraphs edited by over 15,000 multilingual users from July 8 to August 9, 2013. We analyze these multilingual editors in terms of their engagement, interests, and language proficiency in their primary and non-primary (secondary) languages and find that the English edition of Wikipedia displays different dynamics from the Spanish and German editions. Users primarily editing the Spanish and German editions make more complex edits than users who edit these editions as a second language. In contrast, users editing the English edition as a second language make edits that are just as complex as the edits by users who primarily edit the English edition. In this way, English serves a special role bringing together content written by multilinguals from many language editions. Nonetheless, language remains a formidable hurdle to the spread of content: we find evidence for a complexity barrier whereby editors are less likely to edit complex content in a second language. In addition, we find that multilinguals are less engaged and show lower levels of language proficiency in their second languages. We also examine the topical interests of multilingual editors and find that there is no significant difference between primary and non-primary editors in each language.\n    ",
        "submission_date": "2015-08-28T00:00:00",
        "last_modified_date": "2015-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00533",
        "title": "Enhancement and Recognition of Reverberant and Noisy Speech by Extending Its Coherence",
        "authors": [
            "Scott Wisdom",
            "Thomas Powers",
            "Les Atlas",
            "James Pitton"
        ],
        "abstract": "Most speech enhancement algorithms make use of the short-time Fourier transform (STFT), which is a simple and flexible time-frequency decomposition that estimates the short-time spectrum of a signal. However, the duration of short STFT frames are inherently limited by the nonstationarity of speech signals. The main contribution of this paper is a demonstration of speech enhancement and automatic speech recognition in the presence of reverberation and noise by extending the length of analysis windows. We accomplish this extension by performing enhancement in the short-time fan-chirp transform (STFChT) domain, an overcomplete time-frequency representation that is coherent with speech signals over longer analysis window durations than the STFT. This extended coherence is gained by using a linear model of fundamental frequency variation of voiced speech signals. Our approach centers around using a single-channel minimum mean-square error log-spectral amplitude (MMSE-LSA) estimator proposed by Habets, which scales coefficients in a time-frequency domain to suppress noise and reverberation. In the case of multiple microphones, we preprocess the data with either a minimum variance distortionless response (MVDR) beamformer, or a delay-and-sum beamformer (DSB). We evaluate our algorithm on both speech enhancement and recognition tasks for the REVERB challenge dataset. Compared to the same processing done in the STFT domain, our approach achieves significant improvement in terms of objective enhancement metrics (including PESQ---the ITU-T standard measurement for speech quality). In terms of automatic speech recognition (ASR) performance as measured by word error rate (WER), our experiments indicate that the STFT with a long window is more effective for ASR.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01023",
        "title": "Generating Weather Forecast Texts with Case Based Reasoning",
        "authors": [
            "Ibrahim Adeyanju"
        ],
        "abstract": "Several techniques have been used to generate weather forecast texts. In this paper, case based reasoning (CBR) is proposed for weather forecast text generation because similar weather conditions occur over time and should have similar forecast texts. CBR-METEO, a system for generating weather forecast texts was developed using a generic framework (jCOLIBRI) which provides modules for the standard components of the CBR architecture. The advantage in a CBR approach is that systems can be built in minimal time with far less human effort after initial consultation with experts. The approach depends heavily on the goodness of the retrieval and revision components of the CBR process. We evaluated CBRMETEO with NIST, an automated metric which has been shown to correlate well with human judgements for this domain. The system shows comparable performance with other NLG systems that perform the same task.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01288",
        "title": "Incremental Active Opinion Learning Over a Stream of Opinionated Documents",
        "authors": [
            "Max Zimmermann",
            "Eirini Ntoutsi",
            "Myra Spiliopoulou"
        ],
        "abstract": "Applications that learn from opinionated documents, like tweets or product reviews, face two challenges. First, the opinionated documents constitute an evolving stream, where both the author's attitude and the vocabulary itself may change. Second, labels of documents are scarce and labels of words are unreliable, because the sentiment of a word depends on the (unknown) context in the author's mind. Most of the research on mining over opinionated streams focuses on the first aspect of the problem, whereas for the second a continuous supply of labels from the stream is assumed. Such an assumption though is utopian as the stream is infinite and the labeling cost is prohibitive. To this end, we investigate the potential of active stream learning algorithms that ask for labels on demand. Our proposed ACOSTREAM 1 approach works with limited labels: it uses an initial seed of labeled documents, occasionally requests additional labels for documents from the human expert and incrementally adapts to the underlying stream while exploiting the available labeled documents. In its core, ACOSTREAM consists of a MNB classifier coupled with \"sampling\" strategies for requesting class labels for new unlabeled documents. In the experiments, we evaluate the classifier performance over time by varying: (a) the class distribution of the opinionated stream, while assuming that the set of the words in the vocabulary is fixed but their polarities may change with the class distribution; and (b) the number of unknown words arriving at each moment, while the class polarity may also change. Our results show that active learning on a stream of opinionated documents, delivers good performance while requiring a small selection of labels\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01626",
        "title": "Character-level Convolutional Networks for Text Classification",
        "authors": [
            "Xiang Zhang",
            "Junbo Zhao",
            "Yann LeCun"
        ],
        "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01771",
        "title": "Sampled Weighted Min-Hashing for Large-Scale Topic Mining",
        "authors": [
            "Gibran Fuentes-Pineda",
            "Ivan Vladimir Meza-Ruiz"
        ],
        "abstract": "We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to automatically mine topics from large-scale corpora. SWMH generates multiple random partitions of the corpus vocabulary based on term co-occurrence and agglomerates highly overlapping inter-partition cells to produce the mined topics. While other approaches define a topic as a probabilistic distribution over a vocabulary, SWMH topics are ordered subsets of such vocabulary. Interestingly, the topics mined by SWMH underlie themes from the corpus at different levels of granularity. We extensively evaluate the meaningfulness of the mined topics both qualitatively and quantitatively on the NIPS (1.7 K documents), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora. Additionally, we compare the quality of SWMH with Online LDA topics for document representation in classification.\n    ",
        "submission_date": "2015-09-06T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01865",
        "title": "A Hybrid Approach to Domain-Specific Entity Linking",
        "authors": [
            "Alex Olieman",
            "Jaap Kamps",
            "Maarten Marx",
            "Arjan Nusselder"
        ],
        "abstract": "The current state-of-the-art Entity Linking (EL) systems are geared towards corpora that are as heterogeneous as the Web, and therefore perform sub-optimally on domain-specific corpora. A key open problem is how to construct effective EL systems for specific domains, as knowledge of the local context should in principle increase, rather than decrease, effectiveness. In this paper we propose the hybrid use of simple specialist linkers in combination with an existing generalist system to address this problem. Our main findings are the following. First, we construct a new reusable benchmark for EL on a corpus of domain-specific conversations. Second, we test the performance of a range of approaches under the same conditions, and show that specialist linkers obtain high precision in isolation, and high recall when combined with generalist linkers. Hence, we can effectively exploit local context and get the best of both worlds.\n    ",
        "submission_date": "2015-09-06T00:00:00",
        "last_modified_date": "2015-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01978",
        "title": "An Approach to the Analysis of the South Slavic Medieval Labels Using Image Texture",
        "authors": [
            "Darko Brodic",
            "Alessia Amelio",
            "Zoran N. Milivojevic"
        ],
        "abstract": "The paper presents a new script classification method for the discrimination of the South Slavic medieval labels. It consists in the textural analysis of the script types. In the first step, each letter is coded by the equivalent script type, which is defined by its typographical features. Obtained coded text is subjected to the run-length statistical analysis and to the adjacent local binary pattern analysis in order to extract the features. The result shows a diversity between the extracted features of the scripts, which makes the feature classification more effective. It is the basis for the classification process of the script identification by using an extension of a state-of-the-art approach for document clustering. The proposed method is evaluated on an example of hand-engraved in stone and hand-printed in paper labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate very positive results, which prove the effectiveness of the proposed method.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02409",
        "title": "Data-selective Transfer Learning for Multi-Domain Speech Recognition",
        "authors": [
            "Mortaza Doulaty",
            "Oscar Saz",
            "Thomas Hain"
        ],
        "abstract": "Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide-domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6--hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.\n    ",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02437",
        "title": "Improved Twitter Sentiment Prediction through Cluster-then-Predict Model",
        "authors": [
            "Rishabh Soni",
            "K. James Mathai"
        ],
        "abstract": "Over the past decade humans have experienced exponential growth in the use of online resources, in particular social media and microblogging websites such as Facebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line, etc. Many companies have identified these resources as a rich mine of marketing knowledge. This knowledge provides valuable feedback which allows them to further develop the next generation of their product. In this paper, sentiment analysis of a product is performed by extracting tweets about that product and classifying the tweets showing it as positive and negative sentiment. The authors propose a hybrid approach which combines unsupervised learning in the form of K-means clustering to cluster the tweets and then performing supervised learning methods such as Decision Trees and Support Vector Machines for classification.\n    ",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04556",
        "title": "On the evolution of word usage of classical Chinese poetry",
        "authors": [
            "Liang Liu",
            "Lili Yu"
        ],
        "abstract": "The hierarchy of classical Chinese poetry has been broadly acknowledged by a number of studies in Chinese literature. However, quantitative investigations about the evolutionary linkages of classical Chinese poetry are limited. The primary goal of this study is to provide quantitative evidence of the evolutionary linkages, with emphasis on character usage, among different period genres of classical Chinese poetry. Specifically, various statistical analyses are performed to find and compare the patterns of character usage in the poems of nine period genres, including shi jing, chu ci, Han shi , Jin shi, Tang shi, Song shi, Yuan shi, Ming shi, and Qing shi. The result of analysis indicates that each of nine period genres has unique patterns of character usage, with some Chinese characters that are preferably used in the poems of a particular period genre. The analysis on the general pattern of character preference implies a decreasing trend in the use of Chinese characters that rarely occur in modern Chinese literature along the timeline of dynastic types of classical Chinese poetry. The phylogenetic analysis based on the distance matrix suggests that the evolutionary linkages of different types of classical Chinese poetry are congruent with their chronological order, suggesting that character frequencies contain phylogenetic information that is useful for inferring evolutionary linkages among various types of classical Chinese poetry. The estimated phylogenetic tree identifies four groups (shi jing, chu ci), (Han shi, Jin shi), (Tang shi, Song shi, Yuan shi), and (Ming shi, Qing shi). The statistical analyses conducted in this study can be generalized to analyze the data sets of general Chinese literature. Such analyses can provide quantitative insights about the evolutionary linkages of general Chinese literature.\n    ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2020-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05736",
        "title": "Building a Pilot Software Quality-in-Use Benchmark Dataset",
        "authors": [
            "Issa Atoum",
            "Chih How Bong",
            "Narayanan Kulathuramaiyer"
        ],
        "abstract": "Prepared domain specific datasets plays an important role to supervised learning approaches. In this article a new sentence dataset for software quality-in-use is proposed. Three experts were chosen to annotate the data using a proposed annotation scheme. Then the data were reconciled in a (no match eliminate) process to reduce bias. The Kappa, k statistics revealed an acceptable level of agreement; moderate to substantial agreement between the experts. The built data can be used to evaluate software quality-in-use models in sentiment analysis models. Moreover, the annotation scheme can be used to extend the current dataset.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06103",
        "title": "Noise Robust IOA/CAS Speech Separation and Recognition System For The Third 'CHIME' Challenge",
        "authors": [
            "Xiaofei Wang",
            "Chao Wu",
            "Pengyuan Zhang",
            "Ziteng Wang",
            "Yong Liu",
            "Xu Li",
            "Qiang Fu",
            "Yonghong Yan"
        ],
        "abstract": "This paper presents the contribution to the third 'CHiME' speech separation and recognition challenge including both front-end signal processing and back-end speech recognition. In the front-end, Multi-channel Wiener filter (MWF) is designed to achieve background noise reduction. Different from traditional MWF, optimized parameter for the tradeoff between noise reduction and target signal distortion is built according to the desired noise reduction level. In the back-end, several techniques are taken advantage to improve the noisy Automatic Speech Recognition (ASR) performance including Deep Neural Network (DNN), Convolutional Neural Network (CNN) and Long short-term memory (LSTM) using medium vocabulary, Lattice rescoring with a big vocabulary language model finite state transducer, and ROVER scheme. Experimental results show the proposed system combining front-end and back-end is effective to improve the ASR performance.\n    ",
        "submission_date": "2015-09-21T00:00:00",
        "last_modified_date": "2015-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06594",
        "title": "A Compositional Explanation of the Pet Fish Phenomenon",
        "authors": [
            "Bob Coecke",
            "Martha Lewis"
        ],
        "abstract": "The `pet fish' phenomenon is often cited as a paradigm example of the `non-compositionality' of human concept use. We show here how this phenomenon is naturally accommodated within a compositional distributional model of meaning. This model describes the meaning of a composite concept by accounting for interaction between its constituents via their grammatical roles. We give two illustrative examples to show how the qualitative phenomena are exhibited. We go on to apply the model to experimental data, and finally discuss extensions of the formalism.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07179",
        "title": "IllinoisSL: A JAVA Library for Structured Prediction",
        "authors": [
            "Kai-Wei Chang",
            "Shyam Upadhyay",
            "Ming-Wei Chang",
            "Vivek Srikumar",
            "Dan Roth"
        ],
        "abstract": "IllinoisSL is a Java library for learning structured prediction models. It supports structured Support Vector Machines and structured Perceptron. The library consists of a core learning module and several applications, which can be executed from command-lines. Documentation is provided to guide users. In Comparison to other structured learning libraries, IllinoisSL is efficient, general, and easy to use.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07211",
        "title": "Noise-Robust ASR for the third 'CHiME' Challenge Exploiting Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent Neural Network",
        "authors": [
            "Zaihu Pang",
            "Fengyun Zhu"
        ],
        "abstract": "In this paper, the Lingban entry to the third 'CHiME' speech separation and recognition challenge is presented. A time-frequency masking based speech enhancement front-end is proposed to suppress the environmental noise utilizing multi-channel coherence and spatial cues. The state-of-the-art speech recognition techniques, namely recurrent neural network based acoustic and language modeling, state space minimum Bayes risk based discriminative acoustic modeling, and i-vector based acoustic condition modeling, are carefully integrated into the speech recognition back-end. To further improve the system performance by fully exploiting the advantages of different technologies, the final recognition results are obtained by lattice combination and rescoring. Evaluations carried out on the official dataset prove the effectiveness of the proposed systems. Comparing with the best baseline result, the proposed system obtains consistent improvements with over 57% relative word error rate reduction on the real-data test set.\n    ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07845",
        "title": "Selecting Relevant Web Trained Concepts for Automated Event Retrieval",
        "authors": [
            "Bharat Singh",
            "Xintong Han",
            "Zhe Wu",
            "Vlad I. Morariu",
            "Larry S. Davis"
        ],
        "abstract": "Complex event retrieval is a challenging research problem, especially when no training videos are available. An alternative to collecting training videos is to train a large semantic concept bank a priori. Given a text description of an event, event retrieval is performed by selecting concepts linguistically related to the event description and fusing the concept responses on unseen videos. However, defining an exhaustive concept lexicon and pre-training it requires vast computational resources. Therefore, recent approaches automate concept discovery and training by leveraging large amounts of weakly annotated web data. Compact visually salient concepts are automatically obtained by the use of concept pairs or, more generally, n-grams. However, not all visually salient n-grams are necessarily useful for an event query--some combinations of concepts may be visually compact but irrelevant--and this drastically affects performance. We propose an event retrieval algorithm that constructs pairs of automatically discovered concepts and then prunes those concepts that are unlikely to be helpful for retrieval. Pruning depends both on the query and on the specific video instance being evaluated. Our approach also addresses calibration and domain adaptation issues that arise when applying concept detectors to unseen videos. We demonstrate large improvements over other vision based systems on the TRECVID MED 13 dataset.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08973",
        "title": "Symbol Emergence in Robotics: A Survey",
        "authors": [
            "Tadahiro Taniguchi",
            "Takayuki Nagai",
            "Tomoaki Nakamura",
            "Naoto Iwahashi",
            "Tetsuya Ogata",
            "Hideki Asoh"
        ],
        "abstract": "Humans can learn the use of language through physical interaction with their environment and semiotic communication with other people. It is very important to obtain a computational understanding of how humans can form a symbol system and obtain semiotic skills through their autonomous mental development. Recently, many studies have been conducted on the construction of robotic systems and machine-learning methods that can learn the use of language through embodied multimodal interaction with their environment and other systems. Understanding human social interactions and developing a robot that can smoothly communicate with human users in the long term, requires an understanding of the dynamics of symbol systems and is crucially important. The embodied cognition and social interaction of participants gradually change a symbol system in a constructive manner. In this paper, we introduce a field of research called symbol emergence in robotics (SER). SER is a constructive approach towards an emergent symbol system. The emergent symbol system is socially self-organized through both semiotic communications and physical interactions with autonomous cognitive developmental agents, i.e., humans and developmental robots. Specifically, we describe some state-of-art research topics concerning SER, e.g., multimodal categorization, word discovery, and a double articulation analysis, that enable a robot to obtain words and their embodied meanings from raw sensory--motor information, including visual information, haptic information, auditory information, and acoustic speech signals, in a totally unsupervised manner. Finally, we suggest future directions of research in SER.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00244",
        "title": "RDF Knowledge Graph Visualization From a Knowledge Extraction System",
        "authors": [
            "Fadhela Kerdjoudj",
            "Olivier Cur\u00e9"
        ],
        "abstract": "In this paper, we present a system to visualize RDF knowledge graphs. These graphs are obtained from a knowledge extraction system designed by GEOLSemantics. This extraction is performed using natural language processing and trigger detection. The user can visualize subgraphs by selecting some ontology features like concepts or individuals. The system is also multilingual, with the use of the annotated ontology in English, French, Arabic and Chinese.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00277",
        "title": "Similarity of symbol frequency distributions with heavy tails",
        "authors": [
            "Martin Gerlach",
            "Francesc Font-Clos",
            "Eduardo G. Altmann"
        ],
        "abstract": "Quantifying the similarity between symbolic sequences is a traditional problem in Information Theory which requires comparing the frequencies of symbols in different sequences. In numerous modern applications, ranging from DNA over music to texts, the distribution of symbol frequencies is characterized by heavy-tailed distributions (e.g., Zipf's law). The large number of low-frequency symbols in these distributions poses major difficulties to the estimation of the similarity between sequences, e.g., they hinder an accurate finite-size estimation of entropies. Here we show analytically how the systematic (bias) and statistical (fluctuations) errors in these estimations depend on the sample size~$N$ and on the exponent~$\\gamma$ of the heavy-tailed distribution. Our results are valid for the Shannon entropy $(\\alpha=1)$, its corresponding similarity measures (e.g., the Jensen-Shanon divergence), and also for measures based on the generalized entropy of order $\\alpha$. For small $\\alpha$'s, including $\\alpha=1$, the errors decay slower than the $1/N$-decay observed in short-tailed distributions. For $\\alpha$ larger than a critical value $\\alpha^* = 1+1/\\gamma \\leq 2$, the $1/N$-decay is recovered. We show the practical significance of our results by quantifying the evolution of the English language over the last two centuries using a complete $\\alpha$-spectrum of measures. We find that frequent words change more slowly than less frequent words and that $\\alpha=2$ provides the most robust measure to quantify language change.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01026",
        "title": "Calculating entropy at different scales among diverse communication systems",
        "authors": [
            "Gerardo Febres",
            "Klaus Jaffe"
        ],
        "abstract": "We evaluated the impact of changing the observation scale over the entropy measures for text descriptions. MIDI coded Music, computer code and two human natural languages were studied at the scale of characters, words, and at the Fundamental Scale resulting from adjusting the symbols length used to interpret each text-description until it produced minimum entropy. The results show that the Fundamental Scale method is comparable with the use of words when measuring entropy levels in written texts. However, this method can also be used in communication systems lacking words such as music. Measuring symbolic entropy at the fundamental scale allows to calculate quantitatively, relative levels of complexity for different communication systems. The results open novel vision on differences among the structure of the communication systems studied.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01431",
        "title": "SentiCap: Generating Image Descriptions with Sentiments",
        "authors": [
            "Alexander Mathews",
            "Lexing Xie",
            "Xuming He"
        ],
        "abstract": "The recent progress on image recognition and language modeling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. One such style is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6% of cases the generated positive captions were judged as being at least as descriptive as the factual captions. Of these positive captions 88% were confirmed by the crowd-sourced workers as having the appropriate sentiment.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01562",
        "title": "Parameterized Neural Network Language Models for Information Retrieval",
        "authors": [
            "Benjamin Piwowarski",
            "Sylvain Lamprier",
            "Nicolas Despres"
        ],
        "abstract": "Information Retrieval (IR) models need to deal with two difficult issues, vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to the difficulty of retrieving relevant documents that do not contain exact query terms but semantically related terms. Term dependencies refers to the need of considering the relationship between the words of the query when estimating the relevance of a document. A multitude of solutions has been proposed to solve each of these two problems, but no principled model solve both. In parallel, in the last few years, language models based on neural networks have been used to cope with complex natural language processing tasks like emotion and paraphrase detection. Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems, thanks to the distributed representation of words they are based upon, such models could not be used readily in IR, where the estimation of one language model per document (or query) is required. This is both computationally unfeasible and prone to over-fitting. Based on a recent work that proposed to learn a generic language model that can be modified through a set of document-specific parameters, we explore use of new neural network models that are adapted to ad-hoc IR tasks. Within the language model IR framework, we propose and study the use of a generic language model as well as a document-specific language model. Both can be used as a smoothing component, but the latter is more adapted to the document at hand and has the potential of being used as a full document language model. We experiment with such models and analyze their results on TREC-1 to 8 datasets.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01942",
        "title": "Helping Domain Experts Build Speech Translation Systems",
        "authors": [
            "Manny Rayner",
            "Alejandro Armando",
            "Pierrette Bouillon",
            "Sarah Ebling",
            "Johanna Gerlach",
            "Sonia Halimi",
            "Irene Strasly",
            "Nikos Tsourakis"
        ],
        "abstract": "We present a new platform, \"Regulus Lite\", which supports rapid development and web deployment of several types of phrasal speech translation systems using a minimal formalism. A distinguishing feature is that most development work can be performed directly by domain experts. We motivate the need for platforms of this type and discuss three specific cases: medical speech translation, speech-to-sign-language translation and voice questionnaires. We briefly describe initial experiences in developing practical systems.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02693",
        "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback",
        "authors": [
            "ShiLiang Zhang",
            "Hui Jiang",
            "Si Wei",
            "LiRong Dai"
        ],
        "abstract": "We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2015-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02755",
        "title": "A Novel Approach to Document Classification using WordNet",
        "authors": [
            "Koushiki Sarkar",
            "Ritwika Law"
        ],
        "abstract": "Content based Document Classification is one of the biggest challenges in the context of free text mining. Current algorithms on document classifications mostly rely on cluster analysis based on bag-of-words approach. However that method is still being applied to many modern scientific dilemmas. It has established a strong presence in fields like economics and social science to merit serious attention from the researchers. In this paper we would like to propose and explore an alternative grounded more securely on the dictionary classification and correlatedness of words and phrases. It is expected that application of our existing knowledge about the underlying classification structure may lead to improvement of the classifier's performance.\n    ",
        "submission_date": "2015-10-04T00:00:00",
        "last_modified_date": "2015-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03602",
        "title": "A language model based approach towards large scale and lightweight language identification systems",
        "authors": [
            "Brij Mohan Lal Srivastava",
            "Hari Krishna Vydana",
            "Anil Kumar Vuppala",
            "Manish Shrivastava"
        ],
        "abstract": "Multilingual spoken dialogue systems have gained prominence in the recent past necessitating the requirement for a front-end Language Identification (LID) system. Most of the existing LID systems rely on modeling the language discriminative information from low-level acoustic features. Due to the variabilities of speech (speaker and emotional variabilities, etc.), large-scale LID systems developed using low-level acoustic features suffer from a degradation in the performance. In this approach, we have attempted to model the higher level language discriminative phonotactic information for developing an LID system. In this paper, the input speech signal is tokenized to phone sequences by using a language independent phone recognizer. The language discriminative phonotactic information in the obtained phone sequences are modeled using statistical and recurrent neural network based language modeling approaches. As this approach, relies on higher level phonotactical information it is more robust to variabilities of speech. Proposed approach is computationally light weight, highly scalable and it can be used in complement with the existing LID systems.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03797",
        "title": "Complex Politics: A Quantitative Semantic and Topological Analysis of UK House of Commons Debates",
        "authors": [
            "Stefano Gurciullo",
            "Michael Smallegan",
            "Mar\u00eda Pereda",
            "Federico Battiston",
            "Alice Patania",
            "Sebastian Poledna",
            "Daniel Hedblom",
            "Bahattin Tolga Oztan",
            "Alexander Herzog",
            "Peter John",
            "Slava Mikhaylov"
        ],
        "abstract": "This study is a first, exploratory attempt to use quantitative semantics techniques and topological analysis to analyze systemic patterns arising in a complex political system. In particular, we use a rich data set covering all speeches and debates in the UK House of Commons between 1975 and 2014. By the use of dynamic topic modeling (DTM) and topological data analysis (TDA) we show that both members and parties feature specific roles within the system, consistent over time, and extract global patterns indicating levels of political cohesion. Our results provide a wide array of novel hypotheses about the complex dynamics of political systems, with valuable policy applications.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05198",
        "title": "Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks",
        "authors": [
            "Jiwei Li",
            "Alan Ritter",
            "Dan Jurafsky"
        ],
        "abstract": "Inferring latent attributes of people online is an important social computing task, but requires integrating the many heterogeneous sources of information available on the web. We propose learning individual representations of people using neural nets to integrate rich linguistic and network evidence gathered from social media. The algorithm is able to combine diverse cues, such as the text a person writes, their attributes (e.g. gender, employer, education, location) and social relations to other people. We show that by integrating both textual and network evidence, these representations offer improved performance at four important tasks in social media inference on Twitter: predicting (1) gender, (2) occupation, (3) location, and (4) friendships for users. Our approach scales to large datasets and the learned representations can be used as general features in and have the potential to benefit a large number of downstream tasks including link prediction, community detection, or probabilistic reasoning over social networks.\n    ",
        "submission_date": "2015-10-18T00:00:00",
        "last_modified_date": "2017-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06646",
        "title": "A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya Parameters and Topic Models",
        "authors": [
            "Osama Khalifa",
            "David Wolfe Corne",
            "Mike Chantler"
        ],
        "abstract": "Hyper-parameters play a major role in the learning and inference process of latent Dirichlet allocation (LDA). In order to begin the LDA latent variables learning process, these hyper-parameters values need to be pre-determined. We propose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs Newton' (LDA-GN), which places non-informative priors over these hyper-parameters and uses Gibbs sampling to learn appropriate values for them. At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new technique for learning the parameters of multivariate Polya distributions. We report Gibbs-Newton performance results compared with two prominent existing approaches to the latter task: Minka's fixed-point iteration method and the Moments method. We then evaluate LDA-GN in two ways: (i) by comparing it with standard LDA in terms of the ability of the resulting topic models to generalize to unseen documents; (ii) by comparing it with standard LDA in its performance on a binary classification task.\n    ",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2016-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07035",
        "title": "Fast Latent Variable Models for Inference and Visualization on Mobile Devices",
        "authors": [
            "Joseph W Robinson",
            "Aaron Q Li"
        ],
        "abstract": "In this project we outline Vedalia, a high performance distributed network for performing inference on latent variable models in the context of Amazon review visualization. We introduce a new model, RLDA, which extends Latent Dirichlet Allocation (LDA) [Blei et al., 2003] for the review space by incorporating auxiliary data available in online reviews to improve modeling while simultaneously remaining compatible with pre-existing fast sampling techniques such as [Yao et al., 2009; Li et al., 2014a] to achieve high performance. The network is designed such that computation is efficiently offloaded to the client devices using the Chital system [Robinson & Li, 2015], improving response times and reducing server costs. The resulting system is able to rapidly compute a large number of specialized latent variable models while requiring minimal server resources.\n    ",
        "submission_date": "2015-10-23T00:00:00",
        "last_modified_date": "2015-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07439",
        "title": "Object Oriented Analysis using Natural Language Processing concepts: A Review",
        "authors": [
            "Abinash Tripathy",
            "Santanu Kumar Rath"
        ],
        "abstract": "The Software Development Life Cycle (SDLC) starts with eliciting requirements of the customers in the form of Software Requirement Specification (SRS). SRS document needed for software development is mostly written in Natural Language(NL) convenient for the client. From the SRS document only, the class name, its attributes and the functions incorporated in the body of the class are traced based on pre-knowledge of analyst. The paper intends to present a review on Object Oriented (OO) analysis using Natural Language Processing (NLP) techniques. This analysis can be manual where domain expert helps to generate the required diagram or automated system, where the system generates the required diagram, from the input in the form of SRS.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08983",
        "title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition",
        "authors": [
            "Yu Zhang",
            "Guoguo Chen",
            "Dong Yu",
            "Kaisheng Yao",
            "Sanjeev Khudanpur",
            "James Glass"
        ],
        "abstract": "In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and $5.3\\%$ relative improvement respectively.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00040",
        "title": "Quantifying the Cognitive Extent of Science",
        "authors": [
            "Sta\u0161a Milojevi\u0107"
        ],
        "abstract": "While the modern science is characterized by an exponential growth in scientific literature, the increase in publication volume clearly does not reflect the expansion of the cognitive boundaries of science. Nevertheless, most of the metrics for assessing the vitality of science or for making funding and policy decisions are based on productivity. Similarly, the increasing level of knowledge production by large science teams, whose results often enjoy greater visibility, does not necessarily mean that \"big science\" leads to cognitive expansion. Here we present a novel, big-data method to quantify the extents of cognitive domains of different bodies of scientific literature independently from publication volume, and apply it to 20 million articles published over 60-130 years in physics, astronomy, and biomedicine. The method is based on the lexical diversity of titles of fixed quotas of research articles. Owing to large size of quotas, the method overcomes the inherent stochasticity of article titles to achieve <1% precision. We show that the periods of cognitive growth do not necessarily coincide with the trends in publication volume. Furthermore, we show that the articles produced by larger teams cover significantly smaller cognitive territory than (the same quota of) articles from smaller teams. Our findings provide a new perspective on the role of small teams and individual researchers in expanding the cognitive boundaries of science. The proposed method of quantifying the extent of the cognitive territory can also be applied to study many other aspects of \"science of science.\"\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00352",
        "title": "Spatial Semantic Scan: Jointly Detecting Subtle Events and their Spatial Footprint",
        "authors": [
            "Abhinav Maurya"
        ],
        "abstract": "Many methods have been proposed for detecting emerging events in text streams using topic modeling. However, these methods have shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams. We describe Spatially Compact Semantic Scan (SCSS) that has been developed specifically to overcome the shortcomings of current methods in detecting new spatially compact events in text streams. SCSS employs alternating optimization between using semantic scan to estimate contrastive foreground topics in documents, and discovering spatial neighborhoods with high occurrence of documents containing the foreground topics. We evaluate our method on Emergency Department chief complaints dataset (ED dataset) to verify the effectiveness of our method in detecting real-world disease outbreaks from free-text ED chief complaint data.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2016-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00622",
        "title": "On the Number of Many-to-Many Alignments of Multiple Sequences",
        "authors": [
            "Steffen Eger"
        ],
        "abstract": "We count the number of alignments of $N \\ge 1$ sequences when match-up types are from a specified set $S\\subseteq \\mathbb{N}^N$. Equivalently, we count the number of nonnegative integer matrices whose rows sum to a given fixed vector and each of whose columns lie in $S$. We provide a new asymptotic formula for the case $S=\\{(s_1,\\ldots,s_N) \\:|\\: 1\\le s_i\\le 2\\}$.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2016-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01158",
        "title": "Distributed Deep Learning for Question Answering",
        "authors": [
            "Minwei Feng",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "This paper is an empirical study of the distributed deep learning for question answering subtasks: answer selection and question classification. Comparison studies of SGD, MSGD, ADADELTA, ADAGRAD, ADAM/ADAMAX, RMSPROP, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the distributed framework based on the message passing interface can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training. For example, with 48 workers, a 24x speedup is achievable for the answer selection task and running time is decreased from 138.2 hours to 5.81 hours, which will increase the productivity significantly.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01259",
        "title": "Transforming Wikipedia into an Ontology-based Information Retrieval Search Engine for Local Experts using a Third-Party Taxonomy",
        "authors": [
            "Gregory Grefenstette",
            "Karima Rafes"
        ],
        "abstract": "Wikipedia is widely used for finding general information about a wide variety of topics. Its vocation is not to provide local information. For example, it provides plot, cast, and production information about a given movie, but not showing times in your local movie theatre. Here we describe how we can connect local information to Wikipedia, without altering its content. The case study we present involves finding local scientific experts. Using a third-party taxonomy, independent from Wikipedia's category hierarchy, we index information connected to our local experts, present in their activity reports, and we re-index Wikipedia content using the same taxonomy. The connections between Wikipedia pages and local expert reports are stored in a relational database, accessible through as public SPARQL endpoint. A Wikipedia gadget (or plugin) activated by the interested user, accesses the endpoint as each Wikipedia page is accessed. An additional tab on the Wikipedia page allows the user to open up a list of teams of local experts associated with the subject matter in the Wikipedia page. The technique, though presented here as a way to identify local experts, is generic, in that any third party taxonomy, can be used in this to connect Wikipedia to any non-Wikipedia data source.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2016-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01432",
        "title": "Semi-supervised Sequence Learning",
        "authors": [
            "Andrew M. Dai",
            "Quoc V. Le"
        ],
        "abstract": "We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01480",
        "title": "Approximation of the truncated Zeta distribution and Zipf's law",
        "authors": [
            "Maurizio Naldi"
        ],
        "abstract": "Zipf's law appears in many application areas but does not have a closed form expression, which may make its use cumbersome. Since it coincides with the truncated version of the Zeta distribution, in this paper we propose three approximate closed form expressions for the truncated Zeta distribution, which may be employed for Zipf's law as well. The three approximations are based on the replacement of the sum occurring in Zipf's law with an integral, and are named respectively the integral approximation, the average integral approximation, and the trapezoidal approximation. While the first one is shown to be of little use, the trapezoidal approximation exhibits an error which is typically lower than 1\\%, but is as low as 0.1\\% for the range of values of the Zipf parameter below 1.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02024",
        "title": "Towards a Better Understanding of Predict and Count Models",
        "authors": [
            "S. Sathiya Keerthi",
            "Tobias Schnabel",
            "Rajiv Khanna"
        ],
        "abstract": "In a recent paper, Levy and Goldberg pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information. Under certain conditions, they showed that both models end up optimizing equivalent objective functions. This paper explores this connection in more detail and lays out the factors leading to differences between these models. We find that the most relevant differences from an optimization perspective are (i) predict models work in a low dimensional space where embedding vectors can interact heavily; (ii) since predict models have fewer parameters, they are less prone to overfitting.\n",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02274",
        "title": "Stacked Attention Networks for Image Question Answering",
        "authors": [
            "Zichao Yang",
            "Xiaodong He",
            "Jianfeng Gao",
            "Li Deng",
            "Alex Smola"
        ],
        "abstract": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02283",
        "title": "Generation and Comprehension of Unambiguous Object Descriptions",
        "authors": [
            "Junhua Mao",
            "Jonathan Huang",
            "Alexander Toshev",
            "Oana Camburu",
            "Alan Yuille",
            "Kevin Murphy"
        ],
        "abstract": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MS-COCO. We have released the dataset and a toolbox for visualization and evaluation, see ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02570",
        "title": "Explicit Knowledge-based Reasoning for Visual Question Answering",
        "authors": [
            "Peng Wang",
            "Qi Wu",
            "Chunhua Shen",
            "Anton van den Hengel",
            "Anthony Dick"
        ],
        "abstract": "We describe a method for visual question answering which is capable of reasoning about contents of an image on the basis of information extracted from a large-scale knowledge base. The method not only answers natural language questions using concepts not contained in the image, but can provide an explanation of the reasoning by which it developed its answer. The method is capable of answering far more complex questions than the predominant long short-term memory-based approach, and outperforms it significantly in the testing. We also provide a dataset and a protocol by which to evaluate such methods, thus addressing one of the key issues in general visual ques- tion answering.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02669",
        "title": "Enacting textual entailment and ontologies for automated essay grading in chemical domain",
        "authors": [
            "Adrian Groza",
            "Roxana Szabo"
        ],
        "abstract": "We propose a system for automated essay grading using ontologies and textual entailment. The process of textual entailment is guided by hypotheses, which are extracted from a domain ontology. Textual entailment checks if the truth of the hypothesis follows from a given text. We enact textual entailment to compare students answer to a model answer obtained from ontology. We validated the solution against various essays written by students in the chemistry domain.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02799",
        "title": "Neural Module Networks",
        "authors": [
            "Jacob Andreas",
            "Marcus Rohrbach",
            "Trevor Darrell",
            "Dan Klein"
        ],
        "abstract": "Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\" shares substructure with questions like \"what color is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03292",
        "title": "From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge",
        "authors": [
            "Somak Aditya",
            "Yezhou Yang",
            "Chitta Baral",
            "Cornelia Fermuller",
            "Yiannis Aloimonos"
        ],
        "abstract": "In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a \"commonsense\" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03546",
        "title": "Hierarchical Latent Semantic Mapping for Automated Topic Generation",
        "authors": [
            "Guorui Zhou",
            "Guang Chen"
        ],
        "abstract": "Much of information sits in an unprecedented amount of text data. Managing allocation of these large scale text data is an important problem for many areas. Topic modeling performs well in this problem. The traditional generative models (PLSA,LDA) are the state-of-the-art approaches in topic modeling and most recent research on topic generation has been focusing on improving or extending these models. However, results of traditional generative models are sensitive to the number of topics K, which must be specified manually. The problem of generating topics from corpus resembles community detection in networks. Many effective algorithms can automatically detect communities from networks without a manually specified number of the communities. Inspired by these algorithms, in this paper, we propose a novel method named Hierarchical Latent Semantic Mapping (HLSM), which automatically generates topics from corpus. HLSM calculates the association between each pair of words in the latent topic space, then constructs a unipartite network of words with this association and hierarchically generates topics from this network. We apply HLSM to several document collections and the experimental comparisons against several state-of-the-art approaches demonstrate the promising performance.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03690",
        "title": "Deep Multimodal Semantic Embeddings for Speech and Images",
        "authors": [
            "David Harwath",
            "James Glass"
        ],
        "abstract": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03745",
        "title": "Grounding of Textual Phrases in Images by Reconstruction",
        "authors": [
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Ronghang Hu",
            "Trevor Darrell",
            "Bernt Schiele"
        ],
        "abstract": "Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision. We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly. During training our approach encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase. At test time, the correct attention, i.e., the grounding, is evaluated. If grounding supervision is available it can be directly applied via a loss over the attention mechanism. We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision. Our supervised variant improves by a large margin over the state-of-the-art on both datasets.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2017-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04164",
        "title": "Natural Language Object Retrieval",
        "authors": [
            "Ronghang Hu",
            "Huazhe Xu",
            "Marcus Rohrbach",
            "Jiashi Feng",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04401",
        "title": "Symbol Grounding Association in Multimodal Sequences with Missing Elements",
        "authors": [
            "Federico Raue",
            "Andreas Dengel",
            "Thomas M. Breuel",
            "Marcus Liwicki"
        ],
        "abstract": "In this paper, we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development in infants. In other words, two different representations of the same abstract concepts can associate in both directions. This scenario has been long interested in Artificial Intelligence, Psychology, and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our method uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with the max operation for exploiting the common elements between both sequences. The motivation behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and sound). The performance of our extension reaches better results than the original model and similar results to individual LSTM trained in each modality.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2017-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04590",
        "title": "Oracle performance for visual captioning",
        "authors": [
            "Li Yao",
            "Nicolas Ballas",
            "Kyunghyun Cho",
            "John R. Smith",
            "Yoshua Bengio"
        ],
        "abstract": "The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates the possibility of empirically establishing performance upper bounds on various visual captioning datasets without extra data labelling effort or human evaluation. In particular, it is assumed that visual captioning is decomposed into two steps: from visual inputs to visual concepts, and from visual concepts to natural language descriptions. One would be able to obtain an upper bound when assuming the first step is perfect and only requiring training a conditional language model for the second step. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used for visual concept extraction in the first step and the simplicity of the language model for the second step, we show that current state-of-the-art models fall short when being compared with the learned upper bounds. Furthermore, with such a bound, we quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy, and the intrinsic difficulty and blessing of different datasets.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04636",
        "title": "Deep Reinforcement Learning with a Natural Language Action Space",
        "authors": [
            "Ji He",
            "Jianshu Chen",
            "Xiaodong He",
            "Jianfeng Gao",
            "Lihong Li",
            "Li Deng",
            "Mari Ostendorf"
        ],
        "abstract": "This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04834",
        "title": "Neural Programmer: Inducing Latent Programs with Gradient Descent",
        "authors": [
            "Arvind Neelakantan",
            "Quoc V. Le",
            "Ilya Sutskever"
        ],
        "abstract": "Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04868",
        "title": "A Neural Transducer",
        "authors": [
            "Navdeep Jaitly",
            "David Sussillo",
            "Quoc V. Le",
            "Oriol Vinyals",
            "Ilya Sutskever",
            "Samy Bengio"
        ],
        "abstract": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04891",
        "title": "Sherlock: Scalable Fact Learning in Images",
        "authors": [
            "Mohamed Elhoseiny",
            "Scott Cohen",
            "Walter Chang",
            "Brian Price",
            "Ahmed Elgammal"
        ],
        "abstract": "We study scalable and uniform understanding of facts in images. Existing visual recognition systems are typically modeled differently for each fact type such as objects, actions, and interactions. We propose a setting where all these facts can be modeled simultaneously with a capacity to understand unbounded number of facts in a structured way. The training data comes as structured facts in images, including (1) objects (e.g., $<$boy$>$), (2) attributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and (4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic language view (e.g., $<$ boy, playing$>$) and a visual view (an image with this fact). We show that learning visual facts in a structured way enables not only a uniform but also generalizable visual understanding. We propose and investigate recent and strong approaches from the multiview learning literature and also introduce two learning representation models as potential baselines. We applied the investigated methods on several datasets that we augmented with structured facts and a large scale dataset of more than 202,000 facts and 814,000 images. Our experiments show the advantage of relating facts by the structure by the proposed models compared to the designed baselines on bidirectional fact retrieval.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04970",
        "title": "Learning about Spanish dialects through Twitter",
        "authors": [
            "Bruno Gon\u00e7alves",
            "David S\u00e1nchez"
        ],
        "abstract": "This paper maps the large-scale variation of the Spanish language by employing a corpus based on geographically tagged Twitter messages. Lexical dialects are extracted from an analysis of variants of tens of concepts. The resulting maps show linguistic variation on an unprecedented scale across the globe. We discuss the properties of the main dialects within a machine learning approach and find that varieties spoken in urban areas have an international character in contrast to country areas where dialects show a more regional uniformity.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05234",
        "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering",
        "authors": [
            "Huijuan Xu",
            "Kate Saenko"
        ],
        "abstract": "We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single \"hop\" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3].\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05284",
        "title": "Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data",
        "authors": [
            "Lisa Anne Hendricks",
            "Subhashini Venugopalan",
            "Marcus Rohrbach",
            "Raymond Mooney",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-caption data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05392",
        "title": "Learning the Dimensionality of Word Embeddings",
        "authors": [
            "Eric Nalisnick",
            "Sachin Ravi"
        ],
        "abstract": "We describe a method for learning word embeddings with data-dependent dimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic Dimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of Mikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is made dynamic by employing techniques used by Cote & Larochelle (2016) to define an RBM with an infinite number of hidden units. We show qualitatively and quantitatively that SD-SG and SD-CBOW are competitive with their fixed-dimension counterparts while providing a distribution over embedding dimensionalities, which offers a window into how semantics distribute across dimensions.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2017-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05526",
        "title": "Learning Articulated Motion Models from Visual and Lingual Signals",
        "authors": [
            "Zhengyang Wu",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "In order for robots to operate effectively in homes and workplaces, they must be able to manipulate the articulated objects common within environments built for and by humans. Previous work learns kinematic models that prescribe this manipulation from visual demonstrations. Lingual signals, such as natural language descriptions and instructions, offer a complementary means of conveying knowledge of such manipulation models and are suitable to a wide range of interactions (e.g., remote manipulation). In this paper, we present a multimodal learning framework that incorporates both visual and lingual information to estimate the structure and parameters that define kinematic models of articulated objects. The visual signal takes the form of an RGB-D image stream that opportunistically captures object motion in an unprepared scene. Accompanying natural language descriptions of the motion constitute the lingual signal. We present a probabilistic language model that uses word embeddings to associate lingual verbs with their corresponding kinematic structures. By exploiting the complementary nature of the visual and lingual input, our method infers correct kinematic structures for various multiple-part objects on which the previous state-of-the-art, visual-only system fails. We evaluate our multimodal learning framework on a dataset comprised of a variety of household objects, and demonstrate a 36% improvement in model accuracy over the vision-only baseline.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05756",
        "title": "Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction",
        "authors": [
            "Hyeonwoo Noh",
            "Paul Hongsuck Seo",
            "Bohyung Han"
        ],
        "abstract": "We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06078",
        "title": "Learning Deep Structure-Preserving Image-Text Embeddings",
        "authors": [
            "Liwei Wang",
            "Yin Li",
            "Svetlana Lazebnik"
        ],
        "abstract": "This paper proposes a method for learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase localization on the Flickr30K Entities dataset.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06114",
        "title": "Multi-task Sequence to Sequence Learning",
        "authors": [
            "Minh-Thang Luong",
            "Quoc V. Le",
            "Ilya Sutskever",
            "Oriol Vinyals",
            "Lukasz Kaiser"
        ],
        "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06303",
        "title": "Alternative structures for character-level RNNs",
        "authors": [
            "Piotr Bojanowski",
            "Armand Joulin",
            "Tomas Mikolov"
        ],
        "abstract": "Recurrent neural networks are convenient and efficient models for language modeling. However, when applied on the level of characters instead of words, they suffer from several problems. In order to successfully model long-term dependencies, the hidden representation needs to be large. This in turn implies higher computational costs, which can become prohibitive in practice. We propose two alternative structural modifications to the classical RNN model. The first one consists on conditioning the character level representation on the previous word representation. The other one uses the character history to condition the output probability. We evaluate the performance of the two proposed modifications on challenging, multi-lingual real world data.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06349",
        "title": "Generating Sentences from a Continuous Space",
        "authors": [
            "Samuel R. Bowman",
            "Luke Vilnis",
            "Oriol Vinyals",
            "Andrew M. Dai",
            "Rafal Jozefowicz",
            "Samy Bengio"
        ],
        "abstract": "The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06361",
        "title": "Order-Embeddings of Images and Language",
        "authors": [
            "Ivan Vendrov",
            "Ryan Kiros",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06391",
        "title": "Order Matters: Sequence to sequence for sets",
        "authors": [
            "Oriol Vinyals",
            "Samy Bengio",
            "Manjunath Kudlur"
        ],
        "abstract": "Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06407",
        "title": "Recurrent Models for Auditory Attention in Multi-Microphone Distance Speech Recognition",
        "authors": [
            "Suyoun Kim",
            "Ian Lane"
        ],
        "abstract": "Integration of multiple microphone data is one of the key ways to achieve robust speech recognition in noisy environments or when the speaker is located at some distance from the input device. Signal processing techniques such as beamforming are widely used to extract a speech signal of interest from background noise. These techniques, however, are highly dependent on prior spatial information about the microphones and the environment in which the system is being used. In this work, we present a neural attention network that directly combines multi-channel audio to generate phonetic states without requiring any prior knowledge of the microphone layout or any explicit signal preprocessing for speech enhancement. We embed an attention mechanism within a Recurrent Neural Network (RNN) based acoustic model to automatically tune its attention to a more reliable input source. Unlike traditional multi-channel preprocessing, our system can be optimized towards the desired output in one step. Although attention-based models have recently achieved impressive results on sequence-to-sequence learning, no attention mechanisms have previously been applied to learn potentially asynchronous and non-stationary multiple inputs. We evaluate our neural attention model on the CHiME-3 challenge task, and show that the model achieves comparable performance to beamforming using a purely data-driven method.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06420",
        "title": "Skip-Thought Memory Networks",
        "authors": [
            "Ethan Caballero"
        ],
        "abstract": "Question Answering (QA) is fundamental to natural language processing in that most nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly supervised memory network models that have been proposed so far struggle at answering questions that involve relations among multiple entities (such as facebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address this problem of learning multi-argument multi-hop semantic relations for the purpose of QA, we propose a method that combines the jointly learned long-term read-write memory and attentive inference components of end-to-end memory networks (MemN2N) (Sukhbaatar et al., 2015) with distributed sentence vector representations encoded by a Skip-Thought model (Kiros et al., 2015). This choice to append Skip-Thought Vectors to the existing MemN2N framework is motivated by the fact that Skip-Thought Vectors have been shown to accurately model multi-argument semantic relations (Kiros et al., 2015).\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06674",
        "title": "Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation",
        "authors": [
            "Anirudh Goyal",
            "Marius Leordeanu"
        ],
        "abstract": "Integrating higher level visual and linguistic interpretations is at the heart of human intelligence. As automatic visual category recognition in images is approaching human performance, the high level understanding in the dynamic spatiotemporal domain of videos and its translation into natural language is still far from being solved. While most works on vision-to-text translations use pre-learned or pre-established computational linguistic models, in this paper we present an approach that uses vision alone to efficiently learn how to translate into language the video content. We discover, in simple form, the story played by main actors, while using only visual cues for representing objects and their interactions. Our method learns in a hierarchical manner higher level representations for recognizing subjects, actions and objects involved, their relevant contextual background and their interaction to one another over time. We have a three stage approach: first we take in consideration features of the individual entities at the local level of appearance, then we consider the relationship between these objects and actions and their video background, and third, we consider their spatiotemporal relations as inputs to classifiers at the highest level of interpretation. Thus, our approach finds a coherent linguistic description of videos in the form of a subject, verb and object based on their role played in the overall visual story learned directly from training data, without using a known language model. We test the efficiency of our approach on a large scale dataset containing YouTube clips taken in the wild and demonstrate state-of-the-art performance, often superior to current approaches that use more complex, pre-learned linguistic knowledge.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06732",
        "title": "Sequence Level Training with Recurrent Neural Networks",
        "authors": [
            "Marc'Aurelio Ranzato",
            "Sumit Chopra",
            "Michael Auli",
            "Wojciech Zaremba"
        ],
        "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06838",
        "title": "Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural Nets",
        "authors": [
            "Takuya Narihira",
            "Damian Borth",
            "Stella X. Yu",
            "Karl Ni",
            "Trevor Darrell"
        ],
        "abstract": "We consider the visual sentiment task of mapping an image to an adjective noun pair (ANP) such as \"cute baby\". To capture the two-factor structure of our ANP semantics as well as to overcome annotation noise and ambiguity, we propose a novel factorized CNN model which learns separate representations for adjectives and nouns but optimizes the classification performance over their product. Our experiments on the publicly available SentiBank dataset show that our model significantly outperforms not only independent ANP classifiers on unseen ANPs and on retrieving images of novel ANPs, but also image captioning models which capture word semantics from co-occurrence of natural text; the latter turn out to be surprisingly poor at capturing the sentiment evoked by pure visual experience. That is, our factorized ANP CNN not only trains better from noisy labels, generalizes better to new images, but can also expands the ANP vocabulary on its own.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06909",
        "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies",
        "authors": [
            "Shihao Ji",
            "S. V. N. Vishwanathan",
            "Nadathur Satish",
            "Michael J. Anderson",
            "Pradeep Dubey"
        ],
        "abstract": "We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07001",
        "title": "Analysis of a Play by Means of CHAPLIN, the Characters and Places Interaction Network Software",
        "authors": [
            "A.C. Sparavigna",
            "R. Marazzato"
        ],
        "abstract": "Recently, we have developed a software able of gathering information on social networks from written texts. This software, the CHAracters and PLaces Interaction Network (CHAPLIN) tool, is implemented in Visual Basic. By means of it, characters and places of a literary work can be extracted from a list of raw words. The software interface helps users to select their names out of this list. Setting some parameters, CHAPLIN creates a network where nodes represent characters/places and edges give their interactions. Nodes and edges are labelled by performances. In this paper, we propose to use CHAPLIN for the analysis a William Shakespeare's play, the famous 'Tragedy of Hamlet, Prince of Denmark'. Performances of characters in the play as a whole and in each act of it are given by graphs.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2015-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07067",
        "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes",
        "authors": [
            "Satwik Kottur",
            "Ramakrishna Vedantam",
            "Jos\u00e9 M. F. Moura",
            "Devi Parikh"
        ],
        "abstract": "We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, although \"eats\" and \"stares at\" seem unrelated in text, they share semantics visually. When people are eating something, they also tend to stare at the food. Grounding diverse relations like \"eats\" and \"stares at\" into vision remains challenging, despite recent progress in vision. We note that the visual grounding of words depends on semantics, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained, visually grounded notions of semantic relatedness. We show improvements over text-only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets are available online.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07607",
        "title": "Fine-Grain Annotation of Cricket Videos",
        "authors": [
            "Rahul Anand Sharma",
            "Pramod Sankar K",
            "CV Jawahar"
        ],
        "abstract": "The recognition of human activities is one of the key problems in video understanding. Action recognition is challenging even for specific categories of videos, such as sports, that contain only a small set of actions. Interestingly, sports videos are accompanied by detailed commentaries available online, which could be used to perform action annotation in a weakly-supervised setting. For the specific case of Cricket videos, we address the challenge of temporal segmentation and annotation of ctions with semantic descriptions. Our solution consists of two stages. In the first stage, the video is segmented into \"scenes\", by utilizing the scene category information extracted from text-commentary. The second stage consists of classifying video-shots as well as the phrases in the textual description into various categories. The relevant phrases are then suitably mapped to the video-shots. The novel aspect of this work is the fine temporal scale at which semantic information is assigned to the video. As a result of our approach, we enable retrieval of specific actions that last only a few seconds, from several hours of video. This solution yields a large number of labeled exemplars, with no manual effort, that could be used by machine learning algorithms to learn complex actions.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07972",
        "title": "Learning with Memory Embeddings",
        "authors": [
            "Volker Tresp",
            "Crist\u00f3bal Esteban",
            "Yinchong Yang",
            "Stephan Baier",
            "Denis Krompa\u00df"
        ],
        "abstract": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider time evolutions, time patterns and subsymbolic representations. In this paper we map embedding models, which were developed purely as solutions to technical problems for modelling temporal knowledge graphs, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory, sensory memory, short-term memory, and working memory. We discuss learning, query answering, the path from sensory input to semantic decoding, and the relationship between episodic memory and semantic memory. We introduce a number of hypotheses on human memory that can be derived from the developed mathematical models.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08130",
        "title": "A Roadmap towards Machine Intelligence",
        "authors": [
            "Tomas Mikolov",
            "Armand Joulin",
            "Marco Baroni"
        ],
        "abstract": "The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08277",
        "title": "A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations",
        "authors": [
            "Shengxian Wan",
            "Yanyan Lan",
            "Jiafeng Guo",
            "Jun Xu",
            "Liang Pang",
            "Xueqi Cheng"
        ],
        "abstract": "Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through $k$-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08299",
        "title": "Hierarchical classification of e-commerce related social media",
        "authors": [
            "Matthew Long",
            "Aditya Jami",
            "Ashutosh Saxena"
        ],
        "abstract": "In this paper, we attempt to classify tweets into root categories of the Amazon browse node hierarchy using a set of tweets with browse node ID labels, a much larger set of tweets without labels, and a set of Amazon reviews. Examining twitter data presents unique challenges in that the samples are short (under 140 characters) and often contain misspellings or abbreviations that are trivial for a human to decipher but difficult for a computer to parse. A variety of query and document expansion techniques are implemented in an effort to improve information retrieval to modest success.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08400",
        "title": "Regularizing RNNs by Stabilizing Activations",
        "authors": [
            "David Krueger",
            "Roland Memisevic"
        ],
        "abstract": "We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms.\n",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08417",
        "title": "TGSum: Build Tweet Guided Multi-Document Summarization Dataset",
        "authors": [
            "Ziqiang Cao",
            "Chengyao Chen",
            "Wenjie Li",
            "Sujian Li",
            "Furu Wei",
            "Ming Zhou"
        ],
        "abstract": "The development of summarization research has been significantly hampered by the costly acquisition of reference summaries. This paper proposes an effective way to automatically collect large scales of news-related multi-document summaries with reference to social media's reactions. We utilize two types of social labels in tweets, i.e., hashtags and hyper-links. Hashtags are used to cluster documents into different topic sets. Also, a tweet with a hyper-link often highlights certain key points of the corresponding document. We synthesize a linked document cluster to form a reference summary which can cover most key points. To this aim, we adopt the ROUGE metrics to measure the coverage ratio, and develop an Integer Linear Programming solution to discover the sentence set reaching the upper bound of ROUGE. Since we allow summary sentences to be selected from both documents and high-quality tweets, the generated reference summaries could be abstractive. Both informativeness and readability of the collected summaries are verified by manual judgment. In addition, we train a Support Vector Regression summarizer on DUC generic multi-document summarization benchmarks. With the collected data as extra training resource, the performance of the summarizer improves a lot on all the test sets. We release this dataset for further research.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08855",
        "title": "Semantic Folding Theory And its Application in Semantic Fingerprinting",
        "authors": [
            "Francisco De Sousa Webber"
        ],
        "abstract": "Human language is recognized as a very complex domain since decades. No computer system has been able to reach human levels of performance so far. The only known computational system capable of proper language processing is the human brain. While we gather more and more data about the brain, its fundamental computational processes still remain obscure. The lack of a sound computational brain theory also prevents the fundamental understanding of Natural Language Processing. As always when science lacks a theoretical foundation, statistical modeling is applied to accommodate as many sampled real-world data as possible. An unsolved fundamental issue is the actual representation of language (data) within the brain, denoted as the Representational Problem. Starting with Jeff Hawkins' Hierarchical Temporal Memory (HTM) theory, a consistent computational theory of the human cortex, we have developed a corresponding theory of language data representation: The Semantic Folding Theory. The process of encoding words, by using a topographic semantic space as distributional reference frame into a sparse binary representational vector is called Semantic Folding and is the central topic of this document. Semantic Folding describes a method of converting language from its symbolic representation (text) into an explicit, semantically grounded representation that can be generically processed by Hawkins' HTM networks. As it turned out, this change in representation, by itself, can solve many complex NLP problems by applying Boolean operators and a generic similarity function like the Euclidian Distance. Many practical problems of statistical NLP systems, like the high cost of computation, the fundamental incongruity of precision and recall , the complex tuning procedures etc., can be elegantly overcome by applying Semantic Folding.\n    ",
        "submission_date": "2015-11-28T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09173",
        "title": "Recognizing Temporal Linguistic Expression Pattern of Individual with Suicide Risk on Social Media",
        "authors": [
            "Aiqi Zhang",
            "Ang Li",
            "Tingshao Zhu"
        ],
        "abstract": "Suicide is a global public health problem. Early detection of individual suicide risk plays a key role in suicide prevention. In this paper, we propose to look into individual suicide risk through time series analysis of personal linguistic expression on social media (Weibo). We examined temporal patterns of the linguistic expression of individuals on Chinese social media (Weibo). Then, we used such temporal patterns as predictor variables to build classification models for estimating levels of individual suicide risk. Characteristics of time sequence curves to linguistic features including parentheses, auxiliary verbs, personal pronouns and body words are reported to affect performance of suicide most, and the predicting model has a accuracy higher than 0.60, shown by the results. This paper confirms the efficiency of the social media data in detecting individual suicide risk. Results of this study may be insightful for improving the performance of suicide prevention programs.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09460",
        "title": "Ask, and shall you receive?: Understanding Desire Fulfillment in Natural Language Text",
        "authors": [
            "Snigdha Chaturvedi",
            "Dan Goldwasser",
            "Hal Daume III"
        ],
        "abstract": "The ability to comprehend wishes or desires and their fulfillment is important to Natural Language Understanding. This paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled. We propose various unstructured and structured models that capture fulfillment cues such as the subject's emotional state and actions. Our experiments with two different datasets demonstrate the importance of understanding the narrative and discourse structure to address this task.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00765",
        "title": "Learning Semantic Similarity for Very Short Texts",
        "authors": [
            "Cedric De Boom",
            "Steven Van Canneyt",
            "Steven Bohez",
            "Thomas Demeester",
            "Bart Dhoedt"
        ],
        "abstract": "Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosine-similarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or non-existent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments - as a concatenation of separate words - an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations - as opposed to sparse term matching - with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00818",
        "title": "Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos",
        "authors": [
            "Mohamed Elhoseiny",
            "Jingen Liu",
            "Hui Cheng",
            "Harpreet Sawhney",
            "Ahmed Elgammal"
        ],
        "abstract": "We propose a new zero-shot Event Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) automatically determining relevance of concepts/attributes to a free text query, which could be useful for other applications, and (c) retrieving videos by free text event query (e.g., \"changing a vehicle tire\") based on their content. We embed videos into a distributional semantic space and then measure the similarity between videos and the event query in a free text form. We validated our method on the large TRECVID MED (Multimedia Event Detection) challenge. Using only the event title as a query, our method outperformed the state-of-the-art that uses big descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00965",
        "title": "Neural Enquirer: Learning to Query Tables with Natural Language",
        "authors": [
            "Pengcheng Yin",
            "Zhengdong Lu",
            "Hang Li",
            "Ben Kao"
        ],
        "abstract": "We proposed Neural Enquirer as a neural network architecture to execute a natural language (NL) query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parsers, Neural Enquirer is fully \"neuralized\": it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated NL queries on tables with rich structures.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2016-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01043",
        "title": "Approaches for Sentiment Analysis on Twitter: A State-of-Art study",
        "authors": [
            "Harsh Thakkar",
            "Dhiren Patel"
        ],
        "abstract": "Microbloging is an extremely prevalent broadcast medium amidst the Internet fraternity these days. People share their opinions and sentiments about variety of subjects like products, news, institutions, etc., every day on microbloging websites. Sentiment analysis plays a key role in prediction systems, opinion mining systems, etc. Twitter, one of the microbloging platforms allows a limit of 140 characters to its users. This restriction stimulates users to be very concise about their opinion and twitter an ocean of sentiments to analyze. Twitter also provides developer friendly streaming API for data retrieval purpose allowing the analyst to search real time tweets from various users. In this paper, we discuss the state-of-art of the works which are focused on Twitter, the online social network platform, for sentiment analysis. We survey various lexical, machine learning and hybrid approaches for sentiment analysis on Twitter.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01370",
        "title": "Locally Adaptive Translation for Knowledge Graph Embedding",
        "authors": [
            "Yantao Jia",
            "Yuanzhuo Wang",
            "Hailun Lin",
            "Xiaolong Jin",
            "Xueqi Cheng"
        ],
        "abstract": "Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01525",
        "title": "Learning the Semantics of Manipulation Action",
        "authors": [
            "Yezhou Yang",
            "Yiannis Aloimonos",
            "Cornelia Fermuller",
            "Eren Erdal Aksoy"
        ],
        "abstract": "In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs $\\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learn the $\\lambda$-calculus representation of manipulation action from an annotated action corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01926",
        "title": "Thinking Required",
        "authors": [
            "Kamil Rocki"
        ],
        "abstract": "There exists a theory of a single general-purpose learning algorithm which could explain the principles its operation. It assumes the initial rough architecture, a small library of simple innate circuits which are prewired at birth. and proposes that all significant mental algorithms are learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from architectural and functional perspectives.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02167",
        "title": "Simple Baseline for Visual Question Answering",
        "authors": [
            "Bolei Zhou",
            "Yuandong Tian",
            "Sainbayar Sukhbaatar",
            "Arthur Szlam",
            "Rob Fergus"
        ],
        "abstract": "We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo and open-source code. .\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02567",
        "title": "Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in Gaussian Mixture Noise",
        "authors": [
            "Mojtaba Hajiabadi"
        ],
        "abstract": "A distributed adaptive algorithm for estimation of sparse unknown parameters in the presence of nonGaussian noise is proposed in this paper based on normalized least mean fourth (NLMF) criterion. At the first step, local adaptive NLMF algorithm is modified by zero norm in order to speed up the convergence rate and also to reduce the steady state error power in sparse conditions. Then, the proposed algorithm is extended for distributed scenario in which more improvement in estimation performance is achieved due to cooperation of local adaptive filters. Simulation results show the superiority of the proposed algorithm in comparison with conventional NLMF algorithms.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2015-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02902",
        "title": "MovieQA: Understanding Stories in Movies through Question-Answering",
        "authors": [
            "Makarand Tapaswi",
            "Yukun Zhu",
            "Rainer Stiefelhagen",
            "Antonio Torralba",
            "Raquel Urtasun",
            "Sanja Fidler"
        ],
        "abstract": "We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- video clips, plots, subtitles, scripts, and DVS. We analyze our data through various statistics and methods. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We make this data set public along with an evaluation benchmark to encourage inspiring work in this challenging domain.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03460",
        "title": "Neural Self Talk: Image Understanding via Continuous Questioning and Answering",
        "authors": [
            "Yezhou Yang",
            "Yi Li",
            "Cornelia Fermuller",
            "Yiannis Aloimonos"
        ],
        "abstract": "In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04407",
        "title": "We Are Humor Beings: Understanding and Predicting Visual Humor",
        "authors": [
            "Arjun Chandrasekaran",
            "Ashwin K. Vijayakumar",
            "Stanislaw Antol",
            "Mohit Bansal",
            "Dhruv Batra",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "abstract": "Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question - what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies. We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene. We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04701",
        "title": "Joint Image-Text News Topic Detection and Tracking with And-Or Graph Representation",
        "authors": [
            "Weixin Li",
            "Jungseock Joo",
            "Hang Qi",
            "Song-Chun Zhu"
        ],
        "abstract": "In this paper, we aim to develop a method for automatically detecting and tracking topics in broadcast news. We present a hierarchical And-Or graph (AOG) to jointly represent the latent structure of both texts and visuals. The AOG embeds a context sensitive grammar that can describe the hierarchical composition of news topics by semantic elements about people involved, related places and what happened, and model contextual relationships between elements in the hierarchy. We detect news topics through a cluster sampling process which groups stories about closely related events. Swendsen-Wang Cuts (SWC), an effective cluster sampling algorithm, is adopted for traversing the solution space and obtaining optimal clustering solutions by maximizing a Bayesian posterior probability. Topics are tracked to deal with the continuously updated news streams. We generate topic trajectories to show how topics emerge, evolve and disappear over time. The experimental results show that our method can explicitly describe the textual and visual data in news videos and produce meaningful topic trajectories. Our method achieves superior performance compared to state-of-the-art methods on both a public dataset Reuters-21578 and a self-collected dataset named UCLA Broadcast News Dataset.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04973",
        "title": "An Operator for Entity Extraction in MapReduce",
        "authors": [
            "Ndapandula Nakashole"
        ],
        "abstract": "Dictionary-based entity extraction involves finding mentions of dictionary entities in text. Text mentions are often noisy, containing spurious or missing words. Efficient algorithms for detecting approximate entity mentions follow one of two general techniques. The first approach is to build an index on the entities and perform index lookups of document substrings. The second approach recognizes that the number of substrings generated from documents can explode to large numbers, to get around this, they use a filter to prune many such substrings which do not match any dictionary entity and then only verify the remaining substrings if they are entity mentions of dictionary entities, by means of a text join. The choice between the index-based approach and the filter & verification-based approach is a case-to-case decision as the best approach depends on the characteristics of the input entity dictionary, for example frequency of entity mentions. Choosing the right approach for the setting can make a substantial difference in execution time. Making this choice is however non-trivial as there are parameters within each of the approaches that make the space of possible approaches very large. In this paper, we present a cost-based operator for making the choice among execution plans for entity extraction. Since we need to deal with large dictionaries and even larger large datasets, our operator is developed for implementations of MapReduce distributed algorithms.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05004",
        "title": "Towards Evaluation of Cultural-scale Claims in Light of Topic Model Sampling Effects",
        "authors": [
            "Jaimie Murdock",
            "Jiaan Zeng",
            "Colin Allen"
        ],
        "abstract": "Cultural-scale models of full text documents are prone to over-interpretation by researchers making unintentionally strong socio-linguistic claims (Pechenick et al., 2015) without recognizing that even large digital libraries are merely samples of all the books ever produced. In this study, we test the sensitivity of the topic models to the sampling process by taking random samples of books in the Hathi Trust Digital Library from different areas of the Library of Congress Classification Outline. For each classification area, we train several topic models over the entire class with different random seeds, generating a set of spanning models. Then, we train topic models on random samples of books from the classification area, generating a set of sample models. Finally, we perform a topic alignment between each pair of models by computing the Jensen-Shannon distance (JSD) between the word probability distributions for each topic. We take two measures on each model alignment: alignment distance and topic overlap. We find that sample models with a large sample size typically have an alignment distance that falls in the range of the alignment distance between spanning models. Unsurprisingly, as sample size increases, alignment distance decreases. We also find that the topic overlap increases as sample size increases. However, the decomposition of these measures by sample size differs by number of topics and by classification area. We speculate that these measures could be used to find classes which have a common \"canon\" discussed among all books in the area, as shown by high topic overlap and low alignment distance even in small sample sizes.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07046",
        "title": "News Across Languages - Cross-Lingual Document Similarity and Event Tracking",
        "authors": [
            "Jan Rupnik",
            "Andrej Muhic",
            "Gregor Leban",
            "Primoz Skraba",
            "Blaz Fortuna",
            "Marko Grobelnik"
        ],
        "abstract": "In today's world, we follow news which is distributed globally. Significant events are reported by different sources and in different languages. In this work, we address the problem of tracking of events in a large multilingual stream. Within a recently developed system Event Registry we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event. Taking a multilingual stream and clusters of articles from each language, we compare different cross-lingual document similarity measures based on Wikipedia. This allows us to compute the similarity of any two articles regardless of language. Building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. Using this capability, we then propose an approach to link clusters of articles across languages which represent the same event. We provide an extensive evaluation of the system as a whole, as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07281",
        "title": "Topical differences between Chinese language Twitter and Sina Weibo",
        "authors": [
            "Qian Zhang",
            "Bruno Gon\u00e7alves"
        ],
        "abstract": "Sina Weibo, China's most popular microblogging platform, is currently used by over $500M$ users and is considered to be a proxy of Chinese social life. In this study, we contrast the discussions occurring on Sina Weibo and on Chinese language Twitter in order to observe two different strands of Chinese culture: people within China who use Sina Weibo with its government imposed restrictions and those outside that are free to speak completely anonymously. We first propose a simple ad-hoc algorithm to identify topics of Tweets and Weibo. Different from previous works on micro-message topic detection, our algorithm considers topics of the same contents but with different \\#tags. Our algorithm can also detect topics for Tweets and Weibos without any \\#tags. Using a large corpus of Weibo and Chinese language tweets, covering the period from January $1$ to December $31$, $2012$, we obtain a list of topics using clustered \\#tags that we can then use to compare the two platforms. Surprisingly, we find that there are no common entries among the Top $100$ most popular topics. Furthermore, only $9.2\\%$ of tweets correspond to the Top $1000$ topics on Sina Weibo platform, and conversely only $4.4\\%$ of weibos were found to discuss the most popular Twitter topics. Our results reveal significant differences in social attention on the two platforms, with most popular topics on Sina Weibo relating to entertainment while most tweets corresponded to cultural or political contents that is practically non existent in Sina Weibo.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07685",
        "title": "Service Choreography, SBVR, and Time",
        "authors": [
            "Nurulhuda A. Manaf",
            "Sotiris Moschoyiannis",
            "Paul Krause"
        ],
        "abstract": "We propose the use of structured natural language (English) in specifying service choreographies, focusing on the what rather than the how of the required coordination of participant services in realising a business application scenario. The declarative approach we propose uses the OMG standard Semantics of Business Vocabulary and Rules (SBVR) as a modelling language. The service choreography approach has been proposed for describing the global orderings of the invocations on interfaces of participant services. We therefore extend SBVR with a notion of time which can capture the coordination of the participant services, in terms of the observable message exchanges between them. The extension is done using existing modelling constructs in SBVR, and hence respects the standard specification. The idea is that users - domain specialists rather than implementation specialists - can verify the requested service composition by directly reading the structured English used by SBVR. At the same time, the SBVR model can be represented in formal logic so it can be parsed and executed by a machine.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08569",
        "title": "Analyzing Walter Skeat's Forty-Five Parallel Extracts of William Langland's Piers Plowman",
        "authors": [
            "Roger Bilisoly"
        ],
        "abstract": "Walter Skeat published his critical edition of William Langland's 14th century alliterative poem, Piers Plowman, in 1886. In preparation for this he located forty-five manuscripts, and to compare dialects, he published excerpts from each of these. This paper does three statistical analyses using these excerpts, each of which mimics a task he did in writing his critical edition. First, he combined multiple versions of a poetic line to create a best line, which is compared to the mean string that is computed by a generalization of the arithmetic mean that uses edit distance. Second, he claims that a certain subset of manuscripts varies little. This is quantified by computing a string variance, which is closely related to the above generalization of the mean. Third, he claims that the manuscripts fall into three groups, which is a clustering problem that is addressed by using edit distance. The overall goal is to develop methodology that would be of use to a literary critic.\n    ",
        "submission_date": "2015-12-29T00:00:00",
        "last_modified_date": "2015-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08982",
        "title": "Technical Report: a tool for measuring Prosodic Accommodation",
        "authors": [
            "Sucheta Ghosh"
        ],
        "abstract": "This article has been withdrawn by arXiv administrators because the submitter did not have the legal authority to grant the license applied to the work.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2015-12-30T00:00:00"
    }
]