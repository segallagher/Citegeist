[
    {
        "url": "https://arxiv.org/abs/1401.0509",
        "title": "Zero-Shot Learning for Semantic Utterance Classification",
        "authors": [
            "Yann N. Dauphin",
            "Gokhan Tur",
            "Dilek Hakkani-Tur",
            "Larry Heck"
        ],
        "abstract": "We propose a novel zero-shot learning method for semantic utterance classification (SUC). It learns a classifier $f: X \\to Y$ for problems where none of the semantic categories $Y$ are present in the training set. The framework uncovers the link between categories and utterances using a semantic space. We show that this semantic space can be learned by deep neural networks trained on large amounts of search engine query log data. More precisely, we propose a novel method that can learn discriminative semantic features without supervision. It uses the zero-shot learning framework to guide the learning of the semantic features. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by (Tur, 2012). Furthermore, we achieve state-of-the-art results by combining the semantic features with a supervised method.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0569",
        "title": "Natural Language Processing in Biomedicine: A Unified System Architecture Overview",
        "authors": [
            "Son Doan",
            "Mike Conway",
            "Tu Minh Phuong",
            "Lucila Ohno-Machado"
        ],
        "abstract": "  In modern electronic medical records (EMR) much of the clinically important data - signs and symptoms, symptom severity, disease status, etc. - are not provided in structured data fields, but rather are encoded in clinician generated narrative text. Natural language processing (NLP) provides a means of \"unlocking\" this important data source for applications in clinical decision support, quality assurance, and public health. This chapter provides an overview of representative NLP systems in biomedicine based on a unified architectural view. A general architecture in an NLP system consists of two main components: background knowledge that includes biomedical knowledge resources and a framework that integrates NLP tools to process text. Systems differ in both components, which we will review briefly. Additionally, challenges facing current research efforts in biomedical NLP include the paucity of large, publicly available annotated corpora, although initiatives that facilitate data sharing, system evaluation, and collaborative work between researchers in clinical NLP are starting to emerge.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0640",
        "title": "Multi-Topic Multi-Document Summarizer",
        "authors": [
            "Fatma El-Ghannam",
            "Tarek El-Shishtawy"
        ],
        "abstract": "Current multi-document summarization systems can successfully extract summary sentences, however with many limitations including: low coverage, inaccurate extraction to important sentences, redundancy and poor coherence among the selected sentences. The present study introduces a new concept of centroid approach and reports new techniques for extracting summary sentences for multi-document. In both techniques keyphrases are used to weigh sentences and documents. The first summarization technique (Sen-Rich) prefers maximum richness sentences. While the second (Doc-Rich), prefers sentences from centroid document. To demonstrate the new summarization system application to extract summaries of Arabic documents we performed two experiments. First, we applied Rouge measure to compare the new techniques among systems presented at TAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S. Second, the system was applied to summarize multi-topic documents. Using human evaluators, the results show that Doc-Rich is the superior, where summary sentences characterized by extra coverage and more cohesion.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0660",
        "title": "Plurals: individuals and sets in a richly typed semantics",
        "authors": [
            "Bruno Mery",
            "Richard Moot",
            "Christian Retor\u00e9"
        ],
        "abstract": "We developed a type-theoretical framework for natural lan- guage semantics that, in addition to the usual Montagovian treatment of compositional semantics, includes a treatment of some phenomena of lex- ical semantic: coercions, meaning, transfers, (in)felicitous co-predication. In this setting we see how the various readings of plurals (collective, dis- tributive, coverings,...) can be modelled.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0708",
        "title": "Quantitative methods for Phylogenetic Inference in Historical Linguistics: An experimental case study of South Central Dravidian",
        "authors": [
            "Taraka Rama",
            "Sudheer Kolachina",
            "Lakshmi Bai B"
        ],
        "abstract": "In this paper we examine the usefulness of two classes of algorithms Distance Methods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widely used in genetics, for predicting the family relationships among a set of related languages and therefore, diachronic language change. Applying these algorithms to the data on the numbers of shared cognates- with-change and changed as well as unchanged cognates for a group of six languages belonging to a Dravidian language sub-family given in Krishnamurti et al. (1983), we observed that the resultant phylogenetic trees are largely in agreement with the linguistic family tree constructed using the comparative method of reconstruction with only a few minor differences. Furthermore, we studied these minor differences and found that they were cases of genuine ambiguity even for a well-trained historical linguist. We evaluated the trees obtained through our experiments using a well-defined criterion and report the results here. We finally conclude that quantitative methods like the ones we examined are quite useful in predicting family relationships among languages. In addition, we conclude that a modest degree of confidence attached to the intuition that there could indeed exist a parallelism between the processes of linguistic and genetic change is not totally misplaced.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0794",
        "title": "Properties of phoneme N -grams across the world's language families",
        "authors": [
            "Taraka Rama",
            "Lars Borin"
        ],
        "abstract": "In this article, we investigate the properties of phoneme N-grams across half of the world's languages. We investigate if the sizes of three different N-gram distributions of the world's language families obey a power law. Further, the N-gram distributions of language families parallel the sizes of the families, which seem to obey a power law distribution. The correlation between N-gram distributions and language family sizes improves with increasing values of N. We applied statistical tests, originally given by physicists, to test the hypothesis of power law fit to twelve different datasets. The study also raises some new questions about the use of N-gram distributions in linguistic research, which we answer by running a statistical test.\n    ",
        "submission_date": "2014-01-04T00:00:00",
        "last_modified_date": "2014-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1158",
        "title": "Effective Slot Filling Based on Shallow Distant Supervision Methods",
        "authors": [
            "Benjamin Roth",
            "Tassilo Barth",
            "Michael Wiegand",
            "Mittul Singh",
            "Dietrich Klakow"
        ],
        "abstract": "Spoken Language Systems at Saarland University (LSV) participated this year with 5 runs at the TAC KBP English slot filling track. Effective algorithms for all parts of the pipeline, from document retrieval to relation prediction and response post-processing, are bundled in a modular end-to-end relation extraction system called RelationFactory. The main run solely focuses on shallow techniques and achieved significant improvements over LSV's last year's system, while using the same training data and patterns. Improvements mainly have been obtained by a feature representation focusing on surface skip n-grams and improved scoring for extracted distant supervision patterns. Important factors for effective extraction are the training and tuning scheme for distant supervision classifiers, and the query expansion by a translation model based on Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the submitted main run of the LSV RelationFactory system achieved the top-ranked F1-score of 37.3%.\n    ",
        "submission_date": "2014-01-06T00:00:00",
        "last_modified_date": "2014-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1803",
        "title": "Learning Multilingual Word Representations using a Bag-of-Words Autoencoder",
        "authors": [
            "Stanislas Lauly",
            "Alex Boulanger",
            "Hugo Larochelle"
        ],
        "abstract": "Recent work on learning multilingual word representations usually relies on the use of word-level alignements (e.g. infered with the help of GIZA++) between translated sentences, in order to align the word embeddings in different languages. In this workshop paper, we investigate an autoencoder model for learning multilingual word representations that does without such word-level alignements. The autoencoder is trained to reconstruct the bag-of-word representation of given sentence from an encoded representation extracted from its translation. We evaluate our approach on a multilingual document classification task, where labeled data is available only for one language (e.g. English) while classification must be performed in a different language (e.g. French). In our experiments, we observe that our method compares favorably with a previously proposed method that exploits word-level alignments to learn word representations.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2517",
        "title": "The semantic similarity ensemble",
        "authors": [
            "Andrea Ballatore",
            "Michela Bertolotto",
            "David C. Wilson"
        ],
        "abstract": "Computational measures of semantic similarity between geographic terms provide valuable support across geographic information retrieval, data mining, and information integration. To date, a wide variety of approaches to geo-semantic similarity have been devised. A judgment of similarity is not intrinsically right or wrong, but obtains a certain degree of cognitive plausibility, depending on how closely it mimics human behavior. Thus selecting the most appropriate measure for a specific task is a significant challenge. To address this issue, we make an analogy between computational similarity measures and soliciting domain expert opinions, which incorporate a subjective set of beliefs, perceptions, hypotheses, and epistemic biases. Following this analogy, we define the semantic similarity ensemble (SSE) as a composition of different similarity measures, acting as a panel of experts having to reach a decision on the semantic similarity of a set of geographic terms. The approach is evaluated in comparison to human judgments, and results indicate that an SSE performs better than the average of its parts. Although the best member tends to outperform the ensemble, all ensembles outperform the average performance of each ensemble's member. Hence, in contexts where the best measure is unknown, the ensemble provides a more cognitively plausible approach.\n    ",
        "submission_date": "2014-01-11T00:00:00",
        "last_modified_date": "2014-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2641",
        "title": "Towards a Generic Framework for the Development of Unicode Based Digital Sindhi Dictionaries",
        "authors": [
            "Imdad Ali Ismaili",
            "Zeeshan Bhatti",
            "Azhar Ali Shah"
        ],
        "abstract": "Dictionaries are essence of any language providing vital linguistic recourse for the language learners, researchers and scholars. This paper focuses on the methodology and techniques used in developing software architecture for a UBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The proposed system provides an accurate solution for construction and representation of Unicode based Sindhi characters in a dictionary implementing Hash Structure algorithm and a custom java Object as its internal data structure saved in a file. The System provides facilities for Insertion, Deletion and Editing of new records of Sindhi. Through this framework any type of Sindhi to English and English to Sindhi Dictionary (belonging to different domains of knowledge, e.g. engineering, medicine, computer, biology etc.) could be developed easily with accurate representation of Unicode Characters in font independent manner.\n    ",
        "submission_date": "2014-01-12T00:00:00",
        "last_modified_date": "2014-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2663",
        "title": "Dictionary-Based Concept Mining: An Application for Turkish",
        "authors": [
            "Cem R\u0131fk\u0131 Ayd\u0131n",
            "Ali Erkan",
            "Tunga G\u00fcng\u00f6r",
            "Hidayet Tak\u00e7\u0131"
        ],
        "abstract": "In this study, a dictionary-based method is used to extract expressive concepts from documents. So far, there have been many studies concerning concept mining in English, but this area of study for Turkish, an agglutinative language, is still immature. We used dictionary instead of WordNet, a lexical database grouping words into synsets that is widely used for concept extraction. The dictionaries are rarely used in the domain of concept mining, but taking into account that dictionary entries have synonyms, hypernyms, hyponyms and other relationships in their meaning texts, the success rate has been high for determining concepts. This concept extraction method is implemented on documents, that are collected from different corpora.\n    ",
        "submission_date": "2014-01-12T00:00:00",
        "last_modified_date": "2014-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2937",
        "title": "A survey of methods to ease the development of highly multilingual text mining applications",
        "authors": [
            "Ralf Steinberger"
        ],
        "abstract": "Multilingual text processing is useful because the information content found in different languages is complementary, both regarding facts and opinions. While Information Extraction and other text mining software can, in principle, be developed for many languages, most text analysis tools have only been applied to small sets of languages because the development effort per language is large. Self-training tools obviously alleviate the problem, but even the effort of providing training data and of manually tuning the results is usually considerable. In this paper, we gather insights by various multilingual system developers on how to minimise the effort of developing natural language processing applications for many languages. We also explain the main guidelines underlying our own effort to develop complex text mining software for tens of languages. While these guidelines - most of all: extreme simplicity - can be very restrictive and limiting, we believe to have shown the feasibility of the approach through the development of the Europe Media Monitor (EMM) family of applications (",
        "submission_date": "2014-01-13T00:00:00",
        "last_modified_date": "2014-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2943",
        "title": "ONTS: \"Optima\" News Translation System",
        "authors": [
            "Marco Turchi",
            "Martin Atkinson",
            "Alastair Wilcox",
            "Brett Crawley",
            "Stefano Bucci",
            "Ralf Steinberger",
            "Erik Van der Goot"
        ],
        "abstract": "We propose a real-time machine translation system that allows users to select a news category and to translate the related live news articles from Arabic, Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and Turkish into English. The Moses-based system was optimised for the news domain and differs from other available systems in four ways: (1) News items are automatically categorised on the source side, before translation; (2) Named entity translation is optimised by recognising and extracting them on the source side and by re-inserting their translation in the target language, making use of a separate entity repository; (3) News titles are translated with a separate translation system which is optimised for the specific style of news titles; (4) The system was optimised for speed in order to cope with the large volume of daily news articles.\n    ",
        "submission_date": "2014-01-13T00:00:00",
        "last_modified_date": "2014-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3230",
        "title": "Optimization Of Cross Domain Sentiment Analysis Using Sentiwordnet",
        "authors": [
            "K Paramesha",
            "K C Ravishankar"
        ],
        "abstract": "The task of sentiment analysis of reviews is carried out using manually built / automatically generated lexicon resources of their own with which terms are matched with lexicon to compute the term count for positive and negative polarity. On the other hand the Sentiwordnet, which is quite different from other lexicon resources that gives scores (weights) of the positive and negative polarity for each word. The polarity of a word namely positive, negative and neutral have the score ranging between 0 to 1 indicates the strength/weight of the word with that sentiment orientation. In this paper, we show that using the Sentiwordnet, how we could enhance the performance of the classification at both sentence and document level.\n    ",
        "submission_date": "2013-10-08T00:00:00",
        "last_modified_date": "2013-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3322",
        "title": "A Subband-Based SVM Front-End for Robust ASR",
        "authors": [
            "Jibran Yousafzai",
            "Zoran Cvetkovic",
            "Peter Sollich",
            "Matthew Ager"
        ],
        "abstract": "This work proposes a novel support vector machine (SVM) based robust automatic speech recognition (ASR) front-end that operates on an ensemble of the subband components of high-dimensional acoustic waveforms. The key issues of selecting the appropriate SVM kernels for classification in frequency subbands and the combination of individual subband classifiers using ensemble methods are addressed. The proposed front-end is compared with state-of-the-art ASR front-ends in terms of robustness to additive noise and linear filtering. Experiments performed on the TIMIT phoneme classification task demonstrate the benefits of the proposed subband based SVM front-end: it outperforms the standard cepstral front-end in the presence of noise and linear filtering for signal-to-noise ratio (SNR) below 12-dB. A combination of the proposed front-end with a conventional front-end such as MFCC yields further improvements over the individual front ends across the full range of noise levels.\n    ",
        "submission_date": "2013-12-24T00:00:00",
        "last_modified_date": "2013-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3372",
        "title": "Learning Language from a Large (Unannotated) Corpus",
        "authors": [
            "Linas Vepstas",
            "Ben Goertzel"
        ],
        "abstract": "A novel approach to the fully automated, unsupervised extraction of dependency grammars and associated syntax-to-semantic-relationship mappings from large text corpora is described. The suggested approach builds on the authors' prior work with the Link Grammar, RelEx and OpenCog systems, as well as on a number of prior papers and approaches from the statistical language learning literature. If successful, this approach would enable the mining of all the information needed to power a natural language comprehension and generation system, directly from a large, unannotated corpus.\n    ",
        "submission_date": "2014-01-14T00:00:00",
        "last_modified_date": "2014-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3457",
        "title": "Learning Document-Level Semantic Properties from Free-Text Annotations",
        "authors": [
            "S.R.K. Branavan",
            "Harr Chen",
            "Jacob Eisenstein",
            "Regina Barzilay"
        ],
        "abstract": "This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations.  Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as a real bargain or good value. These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing.  To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases.  The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews.  Our approach is implemented as a hierarchical Bayesian model with joint inference.  We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties.  Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3479",
        "title": "Complex Question Answering: Unsupervised Learning Approaches and Experiments",
        "authors": [
            "Yllias Chali",
            "Shafiq Rayhan Joty",
            "Sadid A. Hasan"
        ],
        "abstract": "Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3482",
        "title": "Enhancing QA Systems with Complex Temporal Question Processing Capabilities",
        "authors": [
            "Estela Saquete",
            "Jose Luis Vicedo",
            "Patricio Mart\u00ednez-Barco",
            "Rafael Mu\u00f1oz",
            "Hector Llorens"
        ],
        "abstract": "This paper presents a multilayered architecture that enhances the capabilities of current QA systems and allows different types of complex questions or queries to be processed. The answers to these questions need to be gathered from factual information scattered throughout different documents. Specifically, we designed a specialized layer to process the different types of temporal questions. Complex temporal questions are first decomposed into simple questions, according to the temporal relations expressed in the original question. In the same way, the answers to the resulting simple questions are recomposed, fulfilling the temporal restrictions of the original complex question. A novel aspect of this approach resides in the decomposition which uses a minimal quantity of resources, with the final aim of obtaining a portable platform that is easily extensible to other languages. In this paper we also present a methodology for evaluation of the decomposition of the questions as well as the ability of the implemented temporal layer to perform at a multilingual level. The temporal layer was first performed for English, then evaluated and compared with: a) a general purpose QA system (F-measure 65.47% for QA plus English temporal layer vs. 38.01% for the general QA system), and b) a well-known QA system. Much better results were obtained for temporal questions with the multilayered system. This system was therefore extended to Spanish and very good results were again obtained in the evaluation (F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general QA system).\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3669",
        "title": "Hrebs and Cohesion Chains as similar tools for semantic text properties research",
        "authors": [
            "D. Tatar",
            "M.Lupea",
            "E. Kapetanios"
        ],
        "abstract": "In this study it is proven that the Hrebs used in Denotation analysis of texts and Cohesion Chains (defined as a fusion between Lexical Chains and Coreference Chains) represent similar linguistic tools. This result gives us the possibility to extend to Cohesion Chains (CCs) some important indicators as, for example the Kernel of CCs, the topicality of a CC, text concentration, CC-diffuseness and mean diffuseness of the text. Let us mention that nowhere in the Lexical Chains or Coreference Chains literature these kinds of indicators are introduced and used since now. Similarly, some applications of CCs in the study of a text (as for example segmentation or summarization of a text) could be realized starting from hrebs. As an illustration of the similarity between Hrebs and CCs a detailed analyze of the poem \"Lacul\" by Mihai Eminescu is given.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3832",
        "title": "Constructing Reference Sets from Unstructured, Ungrammatical Text",
        "authors": [
            "Matthew Michelson",
            "Craig A. Knoblock"
        ],
        "abstract": "Vast amounts of text on the Web are unstructured and ungrammatical, such as classified ads, auction listings, forum postings, etc. We call such text \"posts.\" Despite their inconsistent structure and lack of grammar, posts are full of useful information. This paper presents work on semi-automatically building tables of relational information, called \"reference sets,\" by analyzing such posts directly. Reference sets can be applied to a number of tasks such as ontology maintenance and information extraction. Our reference-set construction method starts with just a small amount of background knowledge, and constructs tuples representing the entities in the posts to form a reference set. We also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use. To evaluate the utility of the machine-constructed reference sets, we compare them to manually constructed reference sets in the context of reference-set-based information extraction. Our results show the reference sets constructed by our method outperform manually constructed reference sets. We also compare the reference-set-based extraction approach using the machine-constructed reference set to supervised extraction approaches using generic features. These results demonstrate that using machine-constructed reference sets outperforms the supervised methods, even though the supervised methods require training data.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3865",
        "title": "Evaluating Temporal Graphs Built from Texts via Transitive Reduction",
        "authors": [
            "Xavier Tannier",
            "Philippe Muller"
        ],
        "abstract": "Temporal information has been the focus of recent attention in information extraction, leading to some standardization effort, in particular for the task of relating events in a text. This task raises the problem of comparing two annotations of a given text, because relations between events in a story are intrinsically interdependent and cannot be evaluated separately.  A proper evaluation measure is also crucial in the context of a machine learning approach to the problem.  Finding a common comparison referent at the text level is not obvious, and we argue here in favor of a shift from event-based measures to measures on a unique textual object, a minimal underlying temporal graph, or more formally the transitive reduction of the graph of relations between event boundaries. We support it by an  investigation of its properties on synthetic data and on a well-know  temporal corpus.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4205",
        "title": "Entropy analysis of word-length series of natural language texts: Effects of text language and genre",
        "authors": [
            "Maria Kalimeri",
            "Vassilios Constantoudis",
            "Constantinos Papadimitriou",
            "Kostantinos Karamanos",
            "Fotis K. Diakonos",
            "Haris Papageorgiou"
        ],
        "abstract": "We estimate the $n$-gram entropies of natural language texts in word-length representation and find that these are sensitive to text language and genre. We attribute this sensitivity to changes in the probability distribution of the lengths of single words and emphasize the crucial role of the uniformity of probabilities of having words with length between five and ten. Furthermore, comparison with the entropies of shuffled data reveals the impact of word length correlations on the estimated $n$-gram entropies.\n    ",
        "submission_date": "2014-01-17T00:00:00",
        "last_modified_date": "2014-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4436",
        "title": "Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction",
        "authors": [
            "Muhammad Arshad Ul Abedin",
            "Vincent Ng",
            "Latifur Khan"
        ],
        "abstract": "The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. We investigate two approaches to cause identification. Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloffs Basilisk framework augmented with our linguistic and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. Our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4869",
        "title": "Does Syntactic Knowledge help English-Hindi SMT?",
        "authors": [
            "Taraka Rama",
            "Karthik Gali",
            "Avinesh PVS"
        ],
        "abstract": "In this paper we explore various parameter settings of the state-of-art Statistical Machine Translation system to improve the quality of the translation for a `distant' language pair like English-Hindi. We proposed new techniques for efficient reordering. A slight improvement over the baseline is reported using these techniques. We also show that a simple pre-processing step can improve the quality of the translation significantly.\n    ",
        "submission_date": "2014-01-20T00:00:00",
        "last_modified_date": "2014-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5327",
        "title": "Compositional Operators in Distributional Semantics",
        "authors": [
            "Dimitri Kartsaklis"
        ],
        "abstract": "This survey presents in some detail the main advances that have been recently taking place in Computational Linguistics towards the unification of the two prominent semantic paradigms: the compositional formal semantics view and the distributional models of meaning based on vector spaces. After an introduction to these two approaches, I review the most important models that aim to provide compositionality in distributional semantics. Then I proceed and present in more detail a particular framework by Coecke, Sadrzadeh and Clark (2010) based on the abstract mathematical setting of category theory, as a more complete example capable to demonstrate the diversity of techniques and scientific disciplines that this kind of research can draw from. This paper concludes with a discussion about important open issues that need to be addressed by the researchers in the future.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2014-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5390",
        "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework",
        "authors": [
            "S.R.K. Branavan",
            "David Silver",
            "Regina Barzilay"
        ],
        "abstract": "Domain knowledge is crucial for effective performance in autonomous control systems.  Typically, human effort is required to encode this knowledge into a control algorithm.  In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance.  Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application.  To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure.  This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space.  We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer.  Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games.  We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide.  Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5644",
        "title": "A new keyphrases extraction method based on suffix tree data structure for arabic documents clustering",
        "authors": [
            "Issam Sahmoudi",
            "Hanane Froud",
            "Abdelmonaime Lachkar"
        ],
        "abstract": "Document Clustering is a branch of a larger area of scientific study known as data mining .which is an unsupervised classification using to find a structure in a collection of unlabeled data. The useful information in the documents can be accompanied by a large amount of noise words when using Full Text Representation, and therefore will affect negatively the result of the clustering process. So it is with great need to eliminate the noise words and keeping just the useful information in order to enhance the quality of the clustering results. This problem occurs with different degree for any language such as English, European, Hindi, Chinese, and Arabic Language. To overcome this problem, in this paper, we propose a new and efficient Keyphrases extraction method based on the Suffix Tree data structure (KpST), the extracted Keyphrases are then used in the clustering process instead of Full Text Representation. The proposed method for Keyphrases extraction is language independent and therefore it may be applied to any language. In this investigation, we are interested to deal with the Arabic language which is one of the most complex languages. To evaluate our method, we conduct an experimental study on Arabic Documents using the most popular Clustering approach of Hierarchical algorithms: Agglomerative Hierarchical algorithm with seven linkage techniques and a variety of distance functions and similarity measures to perform Arabic Document Clustering task. The obtained results show that our method for extracting Keyphrases increases the quality of the clustering results. We propose also to study the effect of using the stemming for the testing dataset to cluster it with the same documents clustering techniques and similarity/distance measures.\n    ",
        "submission_date": "2014-01-22T00:00:00",
        "last_modified_date": "2014-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5674",
        "title": "Generalized Biwords for Bitext Compression and Translation Spotting",
        "authors": [
            "Felipe S\u00e1nchez-Mart\u00ednez",
            "Rafael C. Carrasco",
            "Miguel A. Mart\u00ednez-Prieto",
            "Joaquin Adiego"
        ],
        "abstract": "Large bilingual parallel texts (also known as bitexts) are usually stored in a compressed form, and previous work has shown that they can be more efficiently compressed if the fact that the two texts are mutual translations is exploited. For example, a bitext can be seen as a sequence of biwords ---pairs of parallel words with a high probability of co-occurrence--- that can be used as an intermediate representation in the compression process.  However, the simple biword approach described in the literature can only exploit one-to-one word alignments and cannot tackle the reordering of words. We therefore introduce a generalization of biwords which can describe multi-word expressions and reorderings.  We also describe some methods for the binary compression of generalized biword sequences, and compare their performance when different schemes are applied to the extraction of the biword sequence. In addition, we show that this generalization of biwords allows for the implementation of an efficient algorithm to look on the compressed bitext for words or text segments in one of the texts and retrieve their counterpart translations in the other text ---an application usually referred to as translation spotting--- with only some minor modifications in the compression algorithm.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5693",
        "title": "Sentence Compression as Tree Transduction",
        "authors": [
            "Trevor Anthony Cohn",
            "Mirella Lapata"
        ],
        "abstract": "This paper presents a tree-to-tree transduction method for sentence compression. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework.  Experimental results on sentence compression bring significant improvements over a state-of-the-art model.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5694",
        "title": "Cross-lingual Annotation Projection for Semantic Roles",
        "authors": [
            "Sebastian Pado",
            "Mirella Lapata"
        ],
        "abstract": " This article considers the task of automatically inducing role-semantic annotations in the FrameNet paradigm for new languages.  We propose a general framework that is based on annotation projection, phrased as a graph optimization problem. It is relatively inexpensive and has the potential to reduce the human effort involved in creating role-semantic resources. Within this framework, we present projection models that exploit lexical and syntactic information. We provide an experimental evaluation on an English-German parallel corpus which demonstrates the feasibility of inducing high-precision German semantic role annotation both for manually and automatically annotated English data.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5695",
        "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches",
        "authors": [
            "Tahira Naseem",
            "Benjamin Snyder",
            "Jacob Eisenstein",
            "Regina Barzilay"
        ],
        "abstract": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5696",
        "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the Web",
        "authors": [
            "Alexander Pieter Yates",
            "Oren Etzioni"
        ],
        "abstract": "The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called Resolver, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, Resolver resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of resolvers probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic Resolver system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5697",
        "title": "Wikipedia-based Semantic Interpretation for Natural Language Processing",
        "authors": [
            "Evgeniy Gabrilovich",
            "Shaul Markovitch"
        ],
        "abstract": "Adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. Prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as WordNet, or on huge manual efforts such as the CYC project. Here we propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic interpretation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. Using ESA results in significant improvements over the previous state of the art in both tasks. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5698",
        "title": "Identification of Pleonastic It Using the Web",
        "authors": [
            "Yifan Li",
            "Petr Musilek",
            "Marek Reformat",
            "Loren Wyard-Scott"
        ],
        "abstract": "In a significant minority of cases, certain pronouns, especially the pronoun it, can be used without referring to any specific entity. This phenomenon of pleonastic pronoun usage poses serious problems for systems aiming at even a shallow understanding of natural language texts. In this paper, a novel approach is proposed to identify such uses of it: the extrapositional cases are identified using a series of queries against the web, and the cleft cases are identified using a simple set of syntactic rules. The system is evaluated with four sets of news articles containing 679 extrapositional cases as well as 78 cleft constructs. The identification results are comparable to those obtained by human efforts.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5699",
        "title": "Text Relatedness Based on a Word Thesaurus",
        "authors": [
            "George Tsatsaronis",
            "Iraklis Varlamis",
            "Michalis Vazirgiannis"
        ],
        "abstract": "The computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey, and the pairwise relations between their words. Without doubt, a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words. Such a measure that captures well both aspects of text relatedness may help in many tasks, such as text retrieval, classification and clustering. In this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links. The approach exploits only a word thesaurus in order to devise implicit semantic links between words. Based on this approach, we introduce Omiotis, a new measure of semantic relatedness between texts which capitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the relatedness between texts. We gradually validate our method: we first evaluate the performance of the semantic relatedness measure between individual words, covering word-to-word similarity and relatedness, synonym identification and word analogy; then, we proceed with evaluating the performance of our method in measuring text-to-text semantic relatedness in two tasks, namely sentence-to-sentence similarity and paraphrase recognition. Experimental evaluation shows that the proposed method outperforms every lexicon-based method of semantic relatedness in the selected tasks and the used data sets, and competes well against corpus-based and hybrid approaches.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5700",
        "title": "Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora",
        "authors": [
            "Felipe S\u00e1nchez-Mart\u00ednez",
            "Mikel L. Forcada"
        ],
        "abstract": "This paper describes a method for the automatic inference of structural transfer rules to be used in a shallow-transfer machine translation (MT) system from small parallel corpora. The structural transfer rules are based on alignment templates, like those used in statistical MT. Alignment templates are extracted from sentence-aligned parallel corpora and extended with a set of restrictions which are derived from the bilingual dictionary of the MT system and control their application as transfer rules. The experiments conducted using three different language pairs in the free/open-source MT platform Apertium show that translation quality is improved as compared to word-for-word translation (when no transfer rules are used), and that the resulting translation quality is close to that obtained using hand-coded transfer rules. The method we present is entirely unsupervised and benefits from information in the rest of modules of the MT system in which the inferred rules are applied.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5980",
        "title": "Reasoning about Meaning in Natural Language with Compact Closed Categories and Frobenius Algebras",
        "authors": [
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh",
            "Stephen Pulman",
            "Bob Coecke"
        ],
        "abstract": "Compact closed categories have found applications in modeling quantum information protocols by Abramsky-Coecke. They also provide semantics for Lambek's pregroup algebras, applied to formalizing the grammatical structure of natural language, and are implicit in a distributional model of word meaning based on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh used the product category of pregroups with vector spaces and provided a distributional model of meaning for sentences. We recast this theory in terms of strongly monoidal functors and advance it via Frobenius algebras over vector spaces. The former are used to formalize topological quantum field theories by Atiyah and Baez-Dolan, and the latter are used to model classical data in quantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us to work in a single space in which meanings of words, phrases, and sentences of any structure live. Hence we can compare meanings of different language constructs and enhance the applicability of the theory. We report on experimental results on a number of language tasks and verify the theoretical predictions.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6050",
        "title": "Integrative Semantic Dependency Parsing via Efficient Large-scale Feature Selection",
        "authors": [
            "Hai Zhao",
            "Xiaotian Zhang",
            "Chunyu Kit"
        ],
        "abstract": "Semantic parsing, i.e., the automatic derivation of meaning representation such as an instantiated predicate-argument structure for a sentence, plays a critical role in deep processing of natural language. Unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask, the one presented in this article integrates everything into one model, in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance. This integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier. We leverage adaptive pruning of argument candidates and large-scale feature selection engineering to allow the largest feature space ever in use so far in this field, it achieves a state-of-the-art performance on the evaluation data set for CoNLL-2008 shared task, on top of all but one top pipeline system, confirming its feasibility and effectiveness.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6122",
        "title": "Identifying Bengali Multiword Expressions using Semantic Clustering",
        "authors": [
            "Tanmoy Chakraborty",
            "Dipankar Das",
            "Sivaji Bandyopadhyay"
        ],
        "abstract": "One of the key issues in both natural language understanding and generation is the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge problem to the precise language processing due to their idiosyncratic nature and diversity in lexical, syntactical and semantic properties. The semantics of a MWE cannot be expressed after combining the semantics of its constituents. Therefore, the formalism of semantic clustering is often viewed as an instrument for extracting MWEs especially for resource constraint languages like Bengali. The present semantic clustering approach contributes to locate clusters of the synonymous noun tokens present in the document. These clusters in turn help measure the similarity between the constituent words of a potentially candidate phrase using a vector space model and judge the suitability of this phrase to be a MWE. In this experiment, we apply the semantic clustering approach for noun-noun bigram MWEs, though it can be extended to any types of MWEs. In parallel, the well known statistical models, namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR), Significance function are also employed to extract MWEs from the Bengali corpus. The comparative evaluation shows that the semantic clustering approach outperforms all other competing statistical models. As a by-product of this experiment, we have started developing a standard lexicon in Bengali that serves as a productive Bengali linguistic thesaurus.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6131",
        "title": "Controlling Complexity in Part-of-Speech Induction",
        "authors": [
            "Jo\u00e3o V. Gra\u00e7a",
            "Kuzman Ganchev",
            "Luisa Coheur",
            "Fernando Pereira",
            "Ben Taskar"
        ],
        "abstract": "We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via para- metric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6224",
        "title": "Word-length entropies and correlations of natural language written texts",
        "authors": [
            "Maria Kalimeri",
            "Vassilios Constantoudis",
            "Constantinos Papadimitriou",
            "Konstantinos Karamanos",
            "Fotis K. Diakonos",
            "Harris Papageorgiou"
        ],
        "abstract": "We study the frequency distributions and correlations of the word lengths of ten European languages. Our findings indicate that a) the word-length distribution of short words quantified by the mean value and the entropy distinguishes the Uralic (Finnish) corpus from the others, b) the tails at long words, manifested in the high-order moments of the distributions, differentiate the Germanic languages (except for English) from the Romanic languages and Greek and c) the correlations between nearby word lengths measured by the comparison of the real entropies with those of the shuffled texts are found to be smaller in the case of Germanic and Finnish languages.\n    ",
        "submission_date": "2014-01-24T00:00:00",
        "last_modified_date": "2014-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6330",
        "title": "A Statistical Parsing Framework for Sentiment Classification",
        "authors": [
            "Li Dong",
            "Furu Wei",
            "Shujie Liu",
            "Ming Zhou",
            "Ke Xu"
        ],
        "abstract": "We present a statistical parsing framework for sentence-level sentiment classification in this article. Unlike previous works that employ syntactic parsing results for sentiment analysis, we develop a statistical parser to directly analyze the sentiment structure of a sentence. We show that complicated phenomena in sentiment analysis (e.g., negation, intensification, and contrast) can be handled the same as simple and straightforward sentiment expressions in a unified and probabilistic way. We formulate the sentiment grammar upon Context-Free Grammars (CFGs), and provide a formal description of the sentiment parsing framework. We develop the parsing model to obtain possible sentiment parse trees for a sentence, from which the polarity model is proposed to derive the sentiment strength and polarity, and the ranking model is dedicated to selecting the best sentiment tree. We train the parser directly from examples of sentences annotated only with sentiment polarity labels but without any syntactic annotations or polarity annotations of constituents within sentences. Therefore we can obtain training data easily. In particular, we train a sentiment parser, ",
        "submission_date": "2014-01-24T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6422",
        "title": "Automatic Aggregation by Joint Modeling of Aspects and Values",
        "authors": [
            "Christina Sauper",
            "Regina Barzilay"
        ],
        "abstract": "We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis.  Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect.  This approach directly enables discovery of highly-rated or inconsistent aspects of a product.  Our generative model admits an efficient variational mean-field inference algorithm.  It is also easily extensible, and we describe several modifications and their effects on model structure and inference.  We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries.  We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy.  We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6567",
        "title": "A Machine Learning Approach for the Identification of Bengali Noun-Noun Compound Multiword Expressions",
        "authors": [
            "Vivekananda Gayen",
            "Kamal Sarkar"
        ],
        "abstract": "This paper presents a machine learning approach for identification of Bengali multiword expressions (MWE) which are bigram nominal compounds. Our proposed approach has two steps: (1) candidate extraction using chunk information and various heuristic rules and (2) training the machine learning algorithm called Random Forest to classify the candidates into two groups: bigram nominal compound MWE or not bigram nominal compound MWE. A variety of association measures, syntactic and linguistic clues and a set of WordNet-based similarity features have been used for our MWE identification task. The approach presented in this paper can be used to identify bigram nominal compound MWE in Bengali running text.\n    ",
        "submission_date": "2014-01-25T00:00:00",
        "last_modified_date": "2014-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6571",
        "title": "Keyword and Keyphrase Extraction Using Centrality Measures on Collocation Networks",
        "authors": [
            "Shibamouli Lahiri",
            "Sagnik Ray Choudhury",
            "Cornelia Caragea"
        ],
        "abstract": "Keyword and keyphrase extraction is an important problem in natural language processing, with applications ranging from summarization to semantic search to document clustering. Graph-based approaches to keyword and keyphrase extraction avoid the problem of acquiring a large in-domain training corpus by applying variants of PageRank algorithm on a network of words. Although graph-based approaches are knowledge-lean and easily adoptable in online systems, it remains largely open whether they can benefit from centrality measures other than PageRank. In this paper, we experiment with an array of centrality measures on word and noun phrase collocation networks, and analyze their performance on four benchmark datasets. Not only are there centrality measures that perform as well as or better than PageRank, but they are much simpler (e.g., degree, strength, and neighborhood size). Furthermore, centrality-based methods give results that are competitive with and, in some cases, better than two strong unsupervised baselines.\n    ",
        "submission_date": "2014-01-25T00:00:00",
        "last_modified_date": "2014-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6573",
        "title": "Deverbal semantics and the Montagovian generative lexicon",
        "authors": [
            "Livy-Maria Real-Coelho",
            "Christian Retor\u00e9"
        ],
        "abstract": "We propose a lexical account of action nominals, in particular of deverbal nominalisations, whose meaning is related to the event expressed by their base verb. The literature about nominalisations often assumes that the semantics of the base verb completely defines the structure of action nominals. We argue that the information in the base verb is not sufficient to completely determine the semantics of action nominals. We exhibit some data from different languages, especially from Romance language, which show that nominalisations focus on some aspects of the verb semantics. The selected aspects, however, seem to be idiosyncratic and do not automatically result from the internal structure of the verb nor from its interaction with the morphological suffix. We therefore propose a partially lexicalist approach view of deverbal nouns. It is made precise and computable by using the Montagovian Generative Lexicon, a type theoretical framework introduced by Bassac, Mery and Retor\u00e9 in this journal in 2010. This extension of Montague semantics with a richer type system easily incorporates lexical phenomena like the semantics of action nominals in particular deverbals, including their polysemy and (in)felicitous copredications.\n    ",
        "submission_date": "2014-01-25T00:00:00",
        "last_modified_date": "2014-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6875",
        "title": "Context-based Word Acquisition for Situated Dialogue in a Virtual World",
        "authors": [
            "Shaolin Qu",
            "Joyce Y. Chai"
        ],
        "abstract": "To tackle the vocabulary problem in conversational systems, previous work has applied unsupervised learning approaches on co-occurring  speech and eye gaze during interaction to automatically acquire new words. Although these approaches have shown promise, several issues related to human language behavior and human-machine conversation have not been addressed. First, psycholinguistic studies have shown certain temporal regularities between human eye movement and language production. While these regularities can potentially guide the acquisition process, they have not been incorporated in the previous unsupervised approaches. Second, conversational systems generally have an existing knowledge base about the domain and vocabulary. While the existing knowledge can potentially help bootstrap and constrain the acquired new words, it has not been incorporated in the previous models. Third, eye gaze could serve different functions in human-machine conversation. Some gaze streams may not be closely coupled with speech stream, and thus are potentially detrimental to word acquisition. Automated recognition of closely-coupled speech-gaze streams based on conversation context is important. To address these issues, we developed new approaches that incorporate user language behavior, domain knowledge, and conversation context in word acquisition. We evaluated these approaches in the context of situated dialogue in a virtual world. Our experimental results have shown that incorporating the above three types of contextual information significantly improves word acquisition performance.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6876",
        "title": "Improving Statistical Machine Translation for a Resource-Poor Language Using Related Resource-Rich Languages",
        "authors": [
            "Preslav Ivanov Nakov",
            "Hwee Tou Ng"
        ],
        "abstract": "We propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X_1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X_1-Y and a larger bi-text for X_2-Y for some resource-rich language X_2 that is closely related to X_1. This is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages X_1 and X_2 in spelling, word order, and syntax offer: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. The evaluation for Indonesian- >English using Malay and for Spanish -> English using Portuguese and pretending Spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. Overall, our method cuts the amount of necessary \"real training data by a factor of 2--5.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7077",
        "title": "Quantifying literature quality using complexity criteria",
        "authors": [
            "Gerardo Febres",
            "Klaus Jaffe"
        ],
        "abstract": "We measured entropy and symbolic diversity for English and Spanish texts including literature Nobel laureates and other famous authors. Entropy, symbol diversity and symbol frequency profiles were compared for these four groups. We also built a scale sensitive to the quality of writing and evaluated its relationship with the Flesch's readability index for English and the Szigriszt's perspicuity index for Spanish. Results suggest a correlation between entropy and word diversity with quality of writing. Text genre also influences the resulting entropy and diversity of the text. Results suggest the plausibility of automated quality assessment of texts.\n    ",
        "submission_date": "2014-01-28T00:00:00",
        "last_modified_date": "2017-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.8269",
        "title": "Experiments with Three Approaches to Recognizing Lexical Entailment",
        "authors": [
            "Peter D. Turney",
            "Saif M. Mohammad"
        ],
        "abstract": "Inference in natural language often involves recognizing lexical entailment (RLE); that is, identifying whether one word entails another. For example, \"buy\" entails \"own\". Two general strategies for RLE have been proposed: One strategy is to manually construct an asymmetric similarity measure for context vectors (directional similarity) and another is to treat RLE as a problem of learning to recognize semantic relations using supervised machine learning techniques (relation classification). In this paper, we experiment with two recent state-of-the-art representatives of the two general strategies. The first approach is an asymmetric similarity measure (an instance of the directional similarity strategy), designed to capture the degree to which the contexts of a word, a, form a subset of the contexts of another word, b. The second approach (an instance of the relation classification strategy) represents a word pair, a:b, with a feature vector that is the concatenation of the context vectors of a and b, and then applies supervised learning to a training set of labeled feature vectors. Additionally, we introduce a third approach that is a new instance of the relation classification strategy. The third approach represents a word pair, a:b, with a feature vector in which the features are the differences in the similarities of a and b to a set of reference words. All three approaches use vector space models (VSMs) of semantics, based on word-context matrices. We perform an extensive evaluation of the three approaches using three different datasets. The proposed new approach (similarity differences) performs significantly better than the other two approaches on some datasets and there is no dataset for which it is significantly worse. Our results suggest it is beneficial to make connections between the research in lexical entailment and the research in semantic relation classification.\n    ",
        "submission_date": "2014-01-31T00:00:00",
        "last_modified_date": "2014-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0543",
        "title": "How Does Latent Semantic Analysis Work? A Visualisation Approach",
        "authors": [
            "Jan Koeman",
            "William Rea"
        ],
        "abstract": "By using a small example, an analogy to photographic compression, and a simple visualization using heatmaps, we show that latent semantic analysis (LSA) is able to extract what appears to be semantic meaning of words from a set of documents by blurring the distinctions between the words.\n    ",
        "submission_date": "2014-02-03T00:00:00",
        "last_modified_date": "2014-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0563",
        "title": "Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine Translation",
        "authors": [
            "Marta R. Costa-juss\u00e0",
            "Carlos A. Henr\u00edquez",
            "Rafael E. Banchs"
        ],
        "abstract": "Although, Chinese and Spanish are two of the most spoken languages in the world, not much research has been done in machine translation for this language pair. This paper focuses on investigating the state-of-the-art of Chinese-to-Spanish statistical machine translation (SMT), which nowadays is one of the most popular approaches to machine translation. For this purpose, we report details of the available parallel corpus which are Basic Traveller Expressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we conduct experimental work with the largest of these three corpora to explore alternative SMT strategies by means of using a pivot language. Three alternatives are considered for pivoting: cascading, pseudo-corpus and triangulation. As pivot language, we use either English, Arabic or French. Results show that, for a phrase-based SMT system, English is the best pivot language between Chinese and Spanish. We propose a system output combination using the pivot strategies which is capable of outperforming the direct translation strategy. The main objective of this work is motivating and involving the research community to work in this important pair of languages given their demographic impact.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0574",
        "title": "Learning to Predict from Textual Data",
        "authors": [
            "Kira Radinsky",
            "Sagie Davidovich",
            "Shaul Markovitch"
        ],
        "abstract": "Given a current news event, we tackle the problem of generating plausible predictions of future events it might cause.  We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor.  To obtain precisely labeled causality examples, we mine 150 years of news articles and apply semantic natural language modeling techniques to headlines containing certain predefined causality patterns.  For generalization, the model uses a vast number of world knowledge ontologies.  Empirical evaluation on real news articles shows that our Pundit algorithm performs as well as non-expert humans.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0578",
        "title": "Natural Language Inference for Arabic Using Extended Tree Edit Distance with Subtrees",
        "authors": [
            "Maytham Alabbas",
            "Allan Ramsay"
        ],
        "abstract": "Many natural language processing (NLP) applications require the computation of similarities between pairs of syntactic or semantic trees. Many researchers have used tree edit distance for this task, but this technique suffers from the drawback that it deals with single node operations only. We have extended the standard tree edit distance algorithm to deal with subtree transformation operations as well as single nodes. The extended algorithm with subtree operations, TED+ST, is more effective and flexible than the standard algorithm, especially for applications that pay attention to relations among nodes (e.g. in linguistic trees, deleting a modifier subtree should be cheaper than the sum of deleting its components individually). We describe the use of TED+ST for checking entailment between two Arabic text snippets. The preliminary results of using TED+ST were encouraging when compared with two string-based approaches and with the standard algorithm.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0586",
        "title": "Topic Segmentation and Labeling in Asynchronous Conversations",
        "authors": [
            "Shafiq Rayhan Joty",
            "Giuseppe Carenini",
            "Raymond T Ng"
        ],
        "abstract": "Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing (NLP) applications. We present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. Our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for NLP. For topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. For topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1454",
        "title": "An Autoencoder Approach to Learning Bilingual Word Representations",
        "authors": [
            "Sarath Chandar A P",
            "Stanislas Lauly",
            "Hugo Larochelle",
            "Mitesh M. Khapra",
            "Balaraman Ravindran",
            "Vikas Raykar",
            "Amrita Saha"
        ],
        "abstract": "Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. Since training autoencoders on word observations presents certain computational issues, we propose and compare different variations adapted to this setting. We also propose an explicit correlation maximizing regularizer that leads to significant improvement in the performance. We empirically investigate the success of our approach on the problem of cross-language test classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). These experiments demonstrate that our approaches are competitive with the state-of-the-art, achieving up to 10-14 percentage point improvements over the best reported results on this task.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2561",
        "title": "The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary",
        "authors": [
            "Tiziano Flati",
            "Roberto Navigli"
        ],
        "abstract": "Bilingual machine-readable dictionaries are knowledge resources useful in many automatic tasks. However, compared to monolingual computational lexicons like WordNet, bilingual dictionaries typically provide a lower amount of structured information, such as lexical and semantic relations, and often do not cover the entire range of possible translations for a word of interest. In this paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the automated disambiguation of ambiguous translations in the lexical entries of a bilingual machine-readable dictionary. The dictionary is represented as a graph, and cyclic patterns are sought in the graph to assign an appropriate sense tag to each translation in a lexical entry. Further, we use the algorithms output to improve the quality of the dictionary itself, by suggesting accurate solutions to structural problems such as misalignments, partial alignments and missing entries. Finally, we successfully apply CQC to the task of synonym extraction.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2796",
        "title": "PR2: A Language Independent Unsupervised Tool for Personality Recognition from Text",
        "authors": [
            "Fabio Celli",
            "Massimo Poesio"
        ],
        "abstract": "We present PR2, a personality recognition system available online, that performs instance-based classification of Big5 personality types from unstructured text, using language-independent features. It has been tested on English and Italian, achieving performances up to f=.68.\n    ",
        "submission_date": "2014-02-12T00:00:00",
        "last_modified_date": "2014-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3040",
        "title": "Event Structure of Transitive Verb: A MARVS perspective",
        "authors": [
            "Jia-Fei Hong",
            "Kathleen Ahrens",
            "Chu-Ren Huang"
        ],
        "abstract": "Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of the representation of verbal semantics that is based on Mandarin Chinese data (Huang et al. 2000). In the MARVS theory, there are two different types of modules: Event Structure Modules and Role Modules. There are also two sets of attributes: Event-Internal Attributes and Role-Internal Attributes, which are linked to the Event Structure Module and the Role Module, respectively. In this study, we focus on four transitive verbs as chi1(eat), wan2(play), huan4(change) and shao1(burn) and explore their event structures by the MARVS theory.\n    ",
        "submission_date": "2014-02-13T00:00:00",
        "last_modified_date": "2014-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3080",
        "title": "Software Requirement Specification Using Reverse Speech Technology",
        "authors": [
            "Santhy Viswam",
            "Sajeer Karattil"
        ],
        "abstract": "Speech analysis had been taken to a new level with the discovery of Reverse Speech (RS). RS is the discovery of hidden messages, referred as reversals, in normal speech. Works are in progress for exploiting the relevance of RS in different real world applications such as investigation, medical field etc. In this paper we represent an innovative method for preparing a reliable Software Requirement Specification (SRS) document with the help of reverse speech. As SRS act as the backbone for the successful completion of any project, a reliable method is needed to overcome the inconsistencies. Using RS such a reliable method for SRS documentation was developed.\n    ",
        "submission_date": "2014-02-13T00:00:00",
        "last_modified_date": "2014-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3371",
        "title": "An evaluative baseline for geo-semantic relatedness and similarity",
        "authors": [
            "Andrea Ballatore",
            "Michela Bertolotto",
            "David C. Wilson"
        ],
        "abstract": "In geographic information science and semantics, the computation of semantic similarity is widely recognised as key to supporting a vast number of tasks in information integration and retrieval. By contrast, the role of geo-semantic relatedness has been largely ignored. In natural language processing, semantic relatedness is often confused with the more specific semantic similarity. In this article, we discuss a notion of geo-semantic relatedness based on Lehrer's semantic fields, and we compare it with geo-semantic similarity. We then describe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a new open dataset designed to evaluate computational measures of geo-semantic relatedness and similarity. This dataset is larger than existing datasets of this kind, and includes 97 geographic terms combined into 50 term pairs rated by 203 human subjects. GeReSiD is available online and can be used as an evaluation baseline to determine empirically to what degree a given computational model approximates geo-semantic relatedness and similarity.\n    ",
        "submission_date": "2014-02-14T00:00:00",
        "last_modified_date": "2014-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3382",
        "title": "Machine Learning of Phonologically Conditioned Noun Declensions For Tamil Morphological Generators",
        "authors": [
            "K.Rajan",
            "Dr.V.Ramalingam",
            "Dr.M.Ganesan"
        ],
        "abstract": "This paper presents machine learning solutions to a practical problem of Natural Language Generation (NLG), particularly the word formation in agglutinative languages like Tamil, in a supervised manner. The morphological generator is an important component of Natural Language Processing in Artificial Intelligence. It generates word forms given a root and affixes. The morphophonemic changes like addition, deletion, alternation etc., occur when two or more morphemes or words joined together. The Sandhi rules should be explicitly specified in the rule based morphological analyzers and generators. In machine learning framework, these rules can be learned automatically by the system from the training samples and subsequently be applied for new inputs. In this paper we proposed the machine learning models which learn the morphophonemic rules for noun declensions from the given training data. These models are trained to learn sandhi rules using various learning algorithms and the performance of those algorithms are presented. From this we conclude that machine learning of morphological processing such as word form generation can be successfully learned in a supervised manner, without explicit description of rules. The performance of Decision trees and Bayesian machine learning algorithms on noun declensions are discussed.\n    ",
        "submission_date": "2014-02-14T00:00:00",
        "last_modified_date": "2014-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3405",
        "title": "Authorship Analysis based on Data Compression",
        "authors": [
            "Daniele Cerra",
            "Mihai Datcu",
            "Peter Reinartz"
        ],
        "abstract": "This paper proposes to perform authorship analysis using the Fast Compression Distance (FCD), a similarity measure based on compression with dictionaries directly extracted from the written texts. The FCD computes a similarity between two documents through an effective binary search on the intersection set between the two related dictionaries. In the reported experiments the proposed method is applied to documents which are heterogeneous in style, written in five different languages and coming from different historical periods. Results are comparable to the state of the art and outperform traditional compression-based methods.\n    ",
        "submission_date": "2014-02-14T00:00:00",
        "last_modified_date": "2014-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3648",
        "title": "Auto Spell Suggestion for High Quality Speech Synthesis in Hindi",
        "authors": [
            "Shikha Kabra",
            "Ritika Agarwal"
        ],
        "abstract": "The goal of Text-to-Speech (TTS) synthesis in a particular language is to convert arbitrary input text to intelligible and natural sounding speech. However, for a particular language like Hindi, which is a highly confusing language (due to very close spellings), it is not an easy task to identify errors/mistakes in input text and an incorrect text degrade the quality of output speech hence this paper is a contribution to the development of high quality speech synthesis with the involvement of Spellchecker which generates spell suggestions for misspelled words automatically. Involvement of spellchecker would increase the efficiency of speech synthesis by providing spell suggestions for incorrect input text. Furthermore, we have provided the comparative study for evaluating the resultant effect on to phonetic text by adding spellchecker on to input text.\n    ",
        "submission_date": "2014-02-15T00:00:00",
        "last_modified_date": "2014-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3722",
        "title": "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method",
        "authors": [
            "Yoav Goldberg",
            "Omer Levy"
        ],
        "abstract": "The word2vec software of Tomas Mikolov and colleagues (",
        "submission_date": "2014-02-15T00:00:00",
        "last_modified_date": "2014-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4380",
        "title": "A Comparative Study of Machine Learning Methods for Verbal Autopsy Text Classification",
        "authors": [
            "Samuel Danso",
            "Eric Atwell",
            "Owen Johnson"
        ],
        "abstract": "A Verbal Autopsy is the record of an interview about the circumstances of an uncertified death. In developing countries, if a death occurs away from health facilities, a field-worker interviews a relative of the deceased about the circumstances of the death; this Verbal Autopsy can be reviewed off-site. We report on a comparative study of the processes involved in Text Classification applied to classifying Cause of Death: feature value representation; machine learning classification algorithms; and feature reduction strategies in order to identify the suitable approaches applicable to the classification of Verbal Autopsy text. We demonstrate that normalised term frequency and the standard TFiDF achieve comparable performance across a number of classifiers. The results also show Support Vector Machine is superior to other classification algorithms employed in this research. Finally, we demonstrate the effectiveness of employing a \"locally-semi-supervised\" feature reduction strategy in order to increase performance accuracy.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4678",
        "title": "When Learners Surpass their Sources: Mathematical Modeling of Learning from an Inconsistent Source",
        "authors": [
            "Yelena Mandelshtam",
            "Natalia Komarova"
        ],
        "abstract": "We present a new algorithm to model and investigate the learning process of a learner mastering a set of grammatical rules from an inconsistent source. The compelling interest of human language acquisition is that the learning succeeds in virtually every case, despite the fact that the input data are formally inadequate to explain the success of learning. Our model explains how a learner can successfully learn from or even surpass its imperfect source without possessing any additional biases or constraints about the types of patterns that exist in the language. We use the data collected by Singleton and Newport (2004) on the performance of a 7-year boy Simon, who mastered the American Sign Language (ASL) by learning it from his parents, both of whom were imperfect speakers of ASL. We show that the algorithm possesses a frequency-boosting property, whereby the frequency of the most common form of the source is increased by the learner. We also explain several key features of Simon's ASL.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5123",
        "title": "Detecting Opinions in Tweets",
        "authors": [
            "Abdelmalek Amine",
            "Reda Mohamed Hamou",
            "Michel Simonet"
        ],
        "abstract": "Given the incessant growth of documents describing the opinions of different people circulating on the web, including Web 2.0 has made it possible to give an opinion on any product in the net. In this paper, we examine the various opinions expressed in the tweets and classify them positive, negative or neutral by using the emoticons for the Bayesian method and adjectives and adverbs for the Turney's method\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6516",
        "title": "Modelling the Lexicon in Unsupervised Part of Speech Induction",
        "authors": [
            "Greg Dubbin",
            "Phil Blunsom"
        ],
        "abstract": "Automatically inducing the syntactic part-of-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single part-of-speech tag. This one-tag-per-type heuristic counters the tendency of Hidden Markov Model based taggers to over generate tags for a given word type. However, it is clearly incompatible with basic syntactic theory. In this paper we extend a state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model of the lexicon. In doing so we are able to incorporate a soft bias towards inducing few tags per type. We develop a particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions.\n    ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6880",
        "title": "It's distributions all the way down!: Second order changes in statistical distributions also occur",
        "authors": [
            "M.T. Keane",
            "A. Gerow"
        ],
        "abstract": "The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley et als) message on distributions; it largely examines the first-order effects of how a single, signature distribution can predict population behaviour, neglecting second-order effects involving distributional shifts, either between signature distributions or within a given signature distribution. Indeed, Bentley et al. themselves under-emphasise the potential richness of the latter, within-distribution effects.\n    ",
        "submission_date": "2014-02-27T00:00:00",
        "last_modified_date": "2014-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.7265",
        "title": "Semantics, Modelling, and the Problem of Representation of Meaning -- a Brief Survey of Recent Literature",
        "authors": [
            "Yarin Gal"
        ],
        "abstract": "Over the past 50 years many have debated what representation should be used to capture the meaning of natural language utterances. Recently new needs of such representations have been raised in research. Here I survey some of the interesting representations suggested to answer for these new needs.\n    ",
        "submission_date": "2014-02-28T00:00:00",
        "last_modified_date": "2014-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0052",
        "title": "TBX goes TEI -- Implementing a TBX basic extension for the Text Encoding Initiative guidelines",
        "authors": [
            "Laurent Romary"
        ],
        "abstract": "This paper presents an attempt to customise the TEI (Text Encoding Initiative) guidelines in order to offer the possibility to incorporate TBX (TermBase eXchange) based terminological entries within any kind of TEI documents. After presenting the general historical, conceptual and technical contexts, we describe the various design choices we had to take while creating this customisation, which in turn have led to make various changes in the actual TBX serialisation. Keeping in mind the objective to provide the TEI guidelines with, again, an onomasiological model, we try to identify the best comprise in maintaining both the isomorphism with the existing TBX Basic standard and the characteristics of the TEI framework.\n    ",
        "submission_date": "2014-03-01T00:00:00",
        "last_modified_date": "2014-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0531",
        "title": "We Tweet Like We Talk and Other Interesting Observations: An Analysis of English Communication Modalities",
        "authors": [
            "Josiah P. Zayner"
        ],
        "abstract": "Modalities of communication for human beings are gradually increasing in number with the advent of new forms of technology. Many human beings can readily transition between these different forms of communication with little or no effort, which brings about the question: How similar are these different communication modalities? To understand technology$\\text{'}$s influence on English communication, four different corpora were analyzed and compared: Writing from Books using the 1-grams database from the Google Books project, Twitter, IRC Chat, and transcribed Talking. Multi-word confusion matrices revealed that Talking has the most similarity when compared to the other modes of communication, while 1-grams were the least similar form of communication analyzed. Based on the analysis of word usage, word usage frequency distributions, and word class usage, among other things, Talking is also the most similar to Twitter and IRC Chat. This suggests that communicating using Twitter and IRC Chat evolved from Talking rather than Writing. When we communicate online, even though we are writing, we do not Tweet or Chat how we write books; we Tweet and Chat how we Speak. Nonfiction and Fiction writing were clearly differentiable from our analysis with Twitter and Chat being much more similar to Fiction than Nonfiction writing. These hypotheses were then tested using author and journalists Cory Doctorow. Mr. Doctorow$\\text{'}$s Writing, Twitter usage, and Talking were all found to have very similar vocabulary usage patterns as the amalgamized populations, as long as the writing was Fiction. However, Mr. Doctorow$\\text{'}$s Nonfiction writing is different from 1-grams and other collected Nonfiction writings. This data could perhaps be used to create more entertaining works of Nonfiction.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0801",
        "title": "Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring",
        "authors": [
            "Derrick Higgins",
            "Chris Brew",
            "Michael Heilman",
            "Ramon Ziai",
            "Lei Chen",
            "Aoife Cahill",
            "Michael Flor",
            "Nitin Madnani",
            "Joel Tetreault",
            "Daniel Blanchard",
            "Diane Napolitano",
            "Chong Min Lee",
            "John Blackmore"
        ],
        "abstract": "Developments in the educational landscape have spurred greater interest in the problem of automatically scoring short answer questions. A recent shared task on this topic revealed a fundamental divide in the modeling approaches that have been applied to this problem, with the best-performing systems split between those that employ a knowledge engineering approach and those that almost solely leverage lexical information (as opposed to higher-level syntactic information) in assigning a score to a given response. This paper aims to introduce the NLP community to the largest corpus currently available for short-answer scoring, provide an overview of methods used in the shared task using this data, and explore the extent to which more syntactically-informed features can contribute to the short answer scoring task in a way that avoids the question-specific manual effort of the knowledge engineering approach.\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2014-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1194",
        "title": "Latent Semantic Word Sense Disambiguation Using Global Co-occurrence Information",
        "authors": [
            "Minoru Sasaki"
        ],
        "abstract": "In this paper, I propose a novel word sense disambiguation method based on the global co-occurrence information using NMF. When I calculate the dependency relation matrix, the existing method tends to produce very sparse co-occurrence matrix from a small training set. Therefore, the NMF algorithm sometimes does not converge to desired solutions. To obtain a large number of co-occurrence relations, I propose to use co-occurrence frequencies of dependency relations between word features in the whole training set. This enables us to solve data sparseness problem and induce more effective latent features. To evaluate the efficiency of the method of word sense disambiguation, I make some experiments to compare with the result of the two baseline methods. The results of the experiments show this method is effective for word sense disambiguation in comparison with the all baseline methods. Moreover, the proposed method is effective for obtaining a stable effect by analyzing the global co-occurrence information.\n    ",
        "submission_date": "2014-03-05T00:00:00",
        "last_modified_date": "2014-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1314",
        "title": "Authorship detection of SMS messages using unigrams",
        "authors": [
            "R. G. Ragel",
            "P. Herath",
            "U. Senanayake"
        ],
        "abstract": "SMS messaging is a popular media of communication. Because of its popularity and privacy, it could be used for many illegal purposes. Additionally, since they are part of the day to day life, SMSes can be used as evidence for many legal disputes. Since a cellular phone might be accessible to people close to the owner, it is important to establish the fact that the sender of the message is indeed the owner of the phone. For this purpose, the straight forward solutions seem to be the use of popular stylometric methods. However, in comparison with the data used for stylometry in the literature, SMSes have unusual characteristics making it hard or impossible to apply these methods in a conventional way. Our target is to come up with a method of authorship detection of SMS messages that could still give a usable accuracy. We argue that, considering the methods of author attribution, the best method that could be applied to SMS messages is an n-gram method. To prove our point, we checked two different methods of distribution comparison with varying number of training and testing data. We specifically try to compare how well our algorithms work under less amount of testing data and large number of candidate authors (which we believe to be the real world scenario) against controlled tests with less number of authors and selected SMSes with large number of words. To counter the lack of information in an SMS message, we propose the method of stacking together few SMSes.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1349",
        "title": "Learning Soft Linear Constraints with Application to Citation Field Extraction",
        "authors": [
            "Sam Anzaroot",
            "Alexandre Passos",
            "David Belanger",
            "Andrew McCallum"
        ],
        "abstract": "Accurately segmenting a citation string into fields for authors, titles, etc. is a challenging task because the output typically obeys various global constraints. Previous work has shown that modeling soft constraints, where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance. On the other hand, for imposing hard constraints, dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference. We extend the technique to perform prediction subject to soft constraints. Moreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training. This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1773",
        "title": "Finding Eyewitness Tweets During Crises",
        "authors": [
            "Fred Morstatter",
            "Nichola Lubold",
            "Heather Pon-Barry",
            "J\u00fcrgen Pfeffer",
            "Huan Liu"
        ],
        "abstract": "Disaster response agencies have started to incorporate social media as a source of fast-breaking information to understand the needs of people affected by the many crises that occur around the world. These agencies look for tweets from within the region affected by the crisis to get the latest updates of the status of the affected region. However only 1% of all tweets are geotagged with explicit location information. First responders lose valuable information because they cannot assess the origin of many of the tweets they collect. In this work we seek to identify non-geotagged tweets that originate from within the crisis region. Towards this, we address three questions: (1) is there a difference between the language of tweets originating within a crisis region and tweets originating outside the region, (2) what are the linguistic patterns that can be used to differentiate within-region and outside-region tweets, and (3) for non-geotagged tweets, can we automatically identify those originating within the crisis region in real-time?\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2004",
        "title": "Natural Language Feature Selection via Cooccurrence",
        "authors": [
            "Michael Stewart"
        ],
        "abstract": "Specificity is important for extracting collocations, keyphrases, multi-word and index terms [Newman et al. 2012]. It is also useful for tagging, ontology construction [Ryu and Choi 2006], and automatic summarization of documents [Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency and inverse-document frequency (TF-IDF) are typically used to do this, but fail to take advantage of the semantic relationships between terms [Church and Gale 1995]. The result is that general idiomatic terms are mistaken for specific terms. We demonstrate use of relational data for estimation of term specificity. The specificity of a term can be learned from its distribution of relations with other terms. This technique is useful for identifying relevant words or terms for other natural language processing tasks.\n    ",
        "submission_date": "2014-03-08T00:00:00",
        "last_modified_date": "2014-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2124",
        "title": "Generating Music from Literature",
        "authors": [
            "Hannah Davis",
            "Saif M. Mohammad"
        ],
        "abstract": "We present a system, TransProse, that automatically generates musical pieces from text. TransProse uses known relations between elements of music such as tempo and scale, and the emotions they evoke. Further, it uses a novel mechanism to determine sequences of notes that capture the emotional activity in the text. The work has applications in information visualization, in creating audio-visual e-books, and in developing music apps.\n    ",
        "submission_date": "2014-03-10T00:00:00",
        "last_modified_date": "2014-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2152",
        "title": "Parsing using a grammar of word association vectors",
        "authors": [
            "Robert John Freeman"
        ],
        "abstract": "This paper was was first drafted in 2001 as a formalization of the system described in U.S. patent U.S. 7,392,174. It describes a system for implementing a parser based on a kind of cross-product over vectors of contextually similar words. It is being published now in response to nascent interest in vector combination models of syntax and semantics. The method used aggressive substitution of contextually similar words and word groups to enable product vectors to stay in the same space as their operands and make entire sentences comparable syntactically, and potentially semantically. The vectors generated had sufficient representational strength to generate parse trees at least comparable with contemporary symbolic parsers.\n    ",
        "submission_date": "2014-03-10T00:00:00",
        "last_modified_date": "2014-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2837",
        "title": "HPS: a hierarchical Persian stemming method",
        "authors": [
            "Ayshe Rashidi",
            "Mina Zolfy Lighvan"
        ],
        "abstract": "In this paper, a novel hierarchical Persian stemming approach based on the Part-Of-Speech of the word in a sentence is presented. The implemented stemmer includes hash tables and several deterministic finite automata in its different levels of hierarchy for removing the prefixes and suffixes of the words. We had two intentions in using hash tables in our method. The first one is that the DFA don't support some special words, so hash table can partly solve the addressed problem. the second goal is to speed up the implemented stemmer with omitting the time that deterministic finite automata need. Because of the hierarchical organization, this method is fast and flexible enough. Our experiments on test sets from Hamshahri collection and security news (",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3142",
        "title": "ARSENAL: Automatic Requirements Specification Extraction from Natural Language",
        "authors": [
            "Shalini Ghosh",
            "Daniel Elenius",
            "Wenchao Li",
            "Patrick Lincoln",
            "Natarajan Shankar",
            "Wilfried Steiner"
        ],
        "abstract": "Requirements are informal and semi-formal descriptions of the expected behavior of a complex system from the viewpoints of its stakeholders (customers, users, operators, designers, and engineers). However, for the purpose of design, testing, and verification for critical systems, we can transform requirements into formal models that can be analyzed automatically. ARSENAL is a framework and methodology for systematically transforming natural language (NL) requirements into analyzable formal models and logic specifications. These models can be analyzed for consistency and implementability. The ARSENAL methodology is specialized to individual domains, but the approach is general enough to be adapted to new domains.\n    ",
        "submission_date": "2014-03-13T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3351",
        "title": "Semantic Unification A sheaf theoretic approach to natural language",
        "authors": [
            "Samson Abramsky",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "Language is contextual and sheaf theory provides a high level mathematical framework to model contextuality. We show how sheaf theory can model the contextual nature of natural language and how gluing can be used to provide a global semantics for a discourse by putting together the local logical semantics of each sentence within the discourse. We introduce a presheaf structure corresponding to a basic form of Discourse Representation Structures. Within this setting, we formulate a notion of semantic unification --- gluing meanings of parts of a discourse into a coherent whole --- as a form of sheaf-theoretic gluing. We illustrate this idea with a number of examples where it can used to represent resolutions of anaphoric references. We also discuss multivalued gluing, described using a distributions functor, which can be used to represent situations where multiple gluings are possible, and where we may need to rank them using quantitative measures.\n",
        "submission_date": "2014-03-13T00:00:00",
        "last_modified_date": "2014-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3668",
        "title": "Language Heedless of Logic - Philosophy Mindful of What? Failures of Distributive and Absorption Laws",
        "authors": [
            "Arthur Merin"
        ],
        "abstract": "Much of philosophical logic and all of philosophy of language make empirical claims about the vernacular natural language. They presume semantics under which `and' and `or' are related by the dually paired distributive and absorption laws. However, at least one of each pair of laws fails in the vernacular. `Implicature'-based auxiliary theories associated with the programme of H.P. Grice do not prove remedial. Conceivable alternatives that might replace the familiar logics as descriptive instruments are briefly noted: (i) substructural logics and (ii) meaning composition in linear algebras over the reals, occasionally constrained by norms of classical logic. Alternative (ii) locates the problem in violations of one of the idempotent laws. Reasons for a lack of curiosity about elementary and easily testable implications of the received theory are considered. The concept of `reflective equilibrium' is critically examined for its role in reconciling normative desiderata and descriptive commitments.\n    ",
        "submission_date": "2014-03-14T00:00:00",
        "last_modified_date": "2014-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4024",
        "title": "Measuring Global Similarity between Texts",
        "authors": [
            "Uli Fahrenberg",
            "Fabrizio Biondi",
            "Kevin Corre",
            "Cyrille Jegourel",
            "Simon Kongsh\u00f8j",
            "Axel Legay"
        ],
        "abstract": "We propose a new similarity measure between texts which, contrary to the current state-of-the-art approaches, takes a global view of the texts to be compared. We have implemented a tool to compute our textual distance and conducted experiments on several corpuses of texts. The experiments show that our methods can reliably identify different global types of texts.\n    ",
        "submission_date": "2014-03-17T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4467",
        "title": "A hybrid formalism to parse Sign Languages",
        "authors": [
            "R\u00e9mi Dubot",
            "Christophe Collet"
        ],
        "abstract": "Sign Language (SL) linguistic is dependent on the expensive task of annotating. Some automation is already available for low-level information (eg. body part tracking) and the lexical level has shown significant progresses. The syntactic level lacks annotated corpora as well as complete and consistent models. This article presents a solution for the automatic annotation of SL syntactic elements. It exposes a formalism able to represent both constituency-based and dependency-based models. The first enable the representation the structures one may want to annotate, the second aims at fulfilling the holes of the first. A parser is presented and used to conduct two experiments on the solution. One experiment is on a real corpus, the other is on a synthetic corpus.\n    ",
        "submission_date": "2014-03-18T00:00:00",
        "last_modified_date": "2014-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4473",
        "title": "Sign Language Gibberish for syntactic parsing evaluation",
        "authors": [
            "R\u00e9mi Dubot",
            "Christophe Collet"
        ],
        "abstract": "Sign Language (SL) automatic processing slowly progresses bottom-up. The field has seen proposition to handle the video signal, to recognize and synthesize sublexical and lexical units. It starts to see the development of supra-lexical processing. But the recognition, at this level, lacks data. The syntax of SL appears very specific as it uses massively the multiplicity of articulators and its access to the spatial dimensions. Therefore new parsing techniques are developed. However these need to be evaluated. The shortage on real data restrains the corpus-based models to small sizes. We propose here a solution to produce data-sets for the evaluation of parsers on the specific properties of SL. The article first describes the general model used to generates dependency grammars and the phrase generation from these lasts. It then discusses the limits of approach. The solution shows to be of particular interest to evaluate the scalability of the techniques on big models.\n    ",
        "submission_date": "2014-03-18T00:00:00",
        "last_modified_date": "2014-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4759",
        "title": "Spelling Error Trends and Patterns in Sindhi",
        "authors": [
            "Zeeshan Bhatti",
            "Imdad Ali Ismaili",
            "Asad Ali Shaikh",
            "Waseem Javaid"
        ],
        "abstract": "Statistical error Correction technique is the most accurate and widely used approach today, but for a language like Sindhi which is a low resourced language the trained corpora's are not available, so the statistical techniques are not possible at all. Instead a useful alternative would be to exploit various spelling error trends in Sindhi by using a Rule based approach. For designing such technique an essential prerequisite would be to study the various error patterns in a language. This pa per presents various studies of spelling error trends and their types in Sindhi Language. The research shows that the error trends common to all languages are also encountered in Sindhi but their do exist some error patters that are catered specifically to a Sindhi language.\n    ",
        "submission_date": "2014-03-19T00:00:00",
        "last_modified_date": "2014-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4887",
        "title": "Using Entropy Estimates for DAG-Based Ontologies",
        "authors": [
            "Andrew Warren",
            "Joao Setubal"
        ],
        "abstract": "Motivation: Entropy measurements on hierarchical structures have been used in methods for information retrieval and natural language modeling. Here we explore its application to semantic similarity. By finding shared ontology terms, semantic similarity can be established between annotated genes. A common procedure for establishing semantic similarity is to calculate the descriptiveness (information content) of ontology terms and use these values to determine the similarity of annotations. Most often information content is calculated for an ontology term by analyzing its frequency in an annotation corpus. The inherent problems in using these values to model functional similarity motivates our work. Summary: We present a novel calculation for establishing the entropy of a DAG-based ontology, which can be used in an alternative method for establishing the information content of its terms. We also compare our IC metric to two others using semantic and sequence similarity.\n    ",
        "submission_date": "2014-03-19T00:00:00",
        "last_modified_date": "2017-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4928",
        "title": "Clinical TempEval",
        "authors": [
            "Steven Bethard",
            "Leon Derczynski",
            "James Pustejovsky",
            "Marc Verhagen"
        ],
        "abstract": "We describe the Clinical TempEval task which is currently in preparation for the SemEval-2015 evaluation exercise. This task involves identifying and describing events, times and the relations between them in clinical text. Six discrete subtasks are included, focusing on recognising mentions of times and events, describing those mentions for both entity types, identifying the relation between an event and the document creation time, and identifying narrative container relations.\n    ",
        "submission_date": "2014-03-19T00:00:00",
        "last_modified_date": "2014-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5596",
        "title": "A Lemma Based Evaluator for Semitic Language Text Summarization Systems",
        "authors": [
            "Tarek El-Shishtawy",
            "Fatma El-Ghannam"
        ],
        "abstract": "Matching texts in highly inflected languages such as Arabic by simple stemming strategy is unlikely to perform well. In this paper, we present a strategy for automatic text matching technique for for inflectional languages, using Arabic as the test case. The system is an extension of ROUGE test in which texts are matched on token's lemma level. The experimental results show an enhancement of detecting similarities between different sentences having same semantics but written in different lexical forms..\n    ",
        "submission_date": "2014-03-22T00:00:00",
        "last_modified_date": "2014-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6023",
        "title": "Ensemble Detection of Single & Multiple Events at Sentence-Level",
        "authors": [
            "Lu\u00eds Marujo",
            "Anatole Gershman",
            "Jaime Carbonell",
            "Jo\u00e3o P. Neto",
            "David Martins de Matos"
        ],
        "abstract": "Event classification at sentence level is an important Information Extraction task with applications in several NLP, IR, and personalization systems. Multi-label binary relevance (BR) are the state-of-art methods. In this work, we explored new multi-label methods known for capturing relations between event types. These new methods, such as the ensemble Chain of Classifiers, improve the F1 on average across the 6 labels by 2.8% over the Binary Relevance. The low occurrence of multi-label sentences motivated the reduction of the hard imbalanced multi-label classification problem with low number of occurrences of multiple labels per instance to an more tractable imbalanced multiclass problem with better results (+ 4.6%). We report the results of adding new features, such as sentiment strength, rhetorical signals, domain-id (source-id and date), and key-phrases in both single-label and multi-label event classification scenarios.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6381",
        "title": "An efficiency dependency parser using hybrid approach for tamil language",
        "authors": [
            "K. Sureka",
            "K.G. Srinivasagan",
            "S. Suganthi"
        ],
        "abstract": "Natural language processing is a prompt research area across the country. Parsing is one of the very crucial tool in language analysis system which aims to forecast the structural relationship among the words in a given sentence. Many researchers have already developed so many language tools but the accuracy is not meet out the human expectation level, thus the research is still exists. Machine translation is one of the major application area under Natural Language Processing. While translation between one language to another language, the structure identification of a sentence play a key role. This paper introduces the hybrid way to solve the identification of relationship among the given words in a sentence. In existing system is implemented using rule based approach, which is not suited in huge amount of data. The machine learning approaches is suitable for handle larger amount of data and also to get better accuracy via learning and training the system. The proposed approach takes a Tamil sentence as an input and produce the result of a dependency relation as a tree like structure using hybrid approach. This proposed tool is very helpful for researchers and act as an odd-on improve the quality of existing approaches.\n    ",
        "submission_date": "2014-03-21T00:00:00",
        "last_modified_date": "2014-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6392",
        "title": "Implementation of an Automatic Sign Language Lexical Annotation Framework based on Propositional Dynamic Logic",
        "authors": [
            "Arturo Curiel",
            "Christophe Collet"
        ],
        "abstract": "In this paper, we present the implementation of an automatic Sign Language (SL) sign annotation framework based on a formal logic, the Propositional Dynamic Logic (PDL). Our system relies heavily on the use of a specific variant of PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets us describe SL signs as formulae and corpora videos as labeled transition systems (LTSs). Here, we intend to show how a generic annotation system can be constructed upon these underlying theoretical principles, regardless of the tracking technologies available or the input format of corpora. With this in mind, we generated a development framework that adapts the system to specific use cases. Furthermore, we present some results obtained by our application when adapted to one distinct case, 2D corpora analysis with pre-processed tracking information. We also present some insights on how such a technology can be used to analyze 3D real-time data, captured with a depth device.\n    ",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2014-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6636",
        "title": "Sign Language Lexical Recognition With Propositional Dynamic Logic",
        "authors": [
            "Arturo Curiel",
            "Christophe Collet"
        ],
        "abstract": "This paper explores the use of Propositional Dynamic Logic (PDL) as a suitable formal framework for describing Sign Language (SL), the language of deaf people, in the context of natural language processing. SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages.\n    ",
        "submission_date": "2014-03-26T00:00:00",
        "last_modified_date": "2014-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7335",
        "title": "Emotion Analysis Platform on Chinese Microblog",
        "authors": [
            "Duyu Tang",
            "Bing Qin",
            "Ting Liu",
            "Qiuhui Shi"
        ],
        "abstract": "Weibo, as the largest social media service in China, has billions of messages generated every day. The huge number of messages contain rich sentimental information. In order to analyze the emotional changes in accordance with time and space, this paper presents an Emotion Analysis Platform (EAP), which explores the emotional distribution of each province, so that can monitor the global pulse of each province in China. The massive data of Weibo and the real-time requirements make the building of EAP challenging. In order to solve the above problems, emoticons, emotion lexicon and emotion-shifting rules are adopted in EAP to analyze the emotion of each tweet. In order to verify the effectiveness of the platform, case study on the Sichuan earthquake is done, and the analysis result of the platform accords with the fact. In order to analyze from quantity, we manually annotate a test set and conduct experiment on it. The experimental results show that the macro-Precision of EAP reaches 80% and the EAP works effectively.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7455",
        "title": "Hybrid Approach to English-Hindi Name Entity Transliteration",
        "authors": [
            "Shruti Mathur",
            "Varun Prakash Saxena"
        ],
        "abstract": "Machine translation (MT) research in Indian languages is still in its infancy. Not much work has been done in proper transliteration of name entities in this domain. In this paper we address this issue. We have used English-Hindi language pair for our experiments and have used a hybrid approach. At first we have processed English words using a rule based approach which extracts individual phonemes from the words and then we have applied statistical approach which converts the English into its equivalent Hindi phoneme and in turn the corresponding Hindi word. Through this approach we have attained 83.40% accuracy.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1847",
        "title": "Evaluation and Ranking of Machine Translated Output in Hindi Language using Precision and Recall Oriented Metrics",
        "authors": [
            "Aditi Kalyani",
            "Hemant Kumud",
            "Shashi Pal Singh",
            "Ajai Kumar",
            "Hemant Darbari"
        ],
        "abstract": "Evaluation plays a crucial role in development of Machine translation systems. In order to judge the quality of an existing MT system i.e. if the translated output is of human translation quality or not, various automatic metrics exist. We here present the implementation results of different metrics when used on Hindi language along with their comparisons, illustrating how effective are these metrics on languages like Hindi (free word order language).\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1872",
        "title": "Int\u00e9gration des donn\u00e9es d'un lexique syntaxique dans un analyseur syntaxique probabiliste",
        "authors": [
            "Anthony Sigogne",
            "Matthieu Constant",
            "Eric Laporte"
        ],
        "abstract": "This article reports the evaluation of the integration of data from a syntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic parser. We show that by changing the set of labels for verbs and predicational nouns, we can improve the performance on French of a non-lexicalized probabilistic parser.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1890",
        "title": "Polish and English wordnets -- statistical analysis of interconnected networks",
        "authors": [
            "Maksymilian Bujok",
            "Piotr Fronczak",
            "Agata Fronczak"
        ],
        "abstract": "Wordnets are semantic networks containing nouns, verbs, adjectives, and adverbs organized according to linguistic principles, by means of semantic relations. In this work, we adopt a complex network perspective to perform a comparative analysis of the English and Polish wordnets. We determine their similarities and show that the networks exhibit some of the typical characteristics observed in other real-world networks. We analyse interlingual relations between both wordnets and deliberate over the problem of mapping the Polish lexicon onto the English one.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1982",
        "title": "Aspect-Based Opinion Extraction from Customer reviews",
        "authors": [
            "Amani K Samha",
            "Yuefeng Li",
            "Jinglan Zhang"
        ],
        "abstract": "Text is the main method of communicating information in the digital age. Messages, blogs, news articles, reviews, and opinionated information abound on the Internet. People commonly purchase products online and post their opinions about purchased items. This feedback is displayed publicly to assist others with their purchasing decisions, creating the need for a mechanism with which to extract and summarize useful information for enhancing the decision-making process. Our contribution is to improve the accuracy of extraction by combining different techniques from three major areas, named Data Mining, Natural Language Processing techniques and Ontologies. The proposed framework sequentially mines products aspects and users opinions, groups representative aspects by similarity, and generates an output summary. This paper focuses on the task of extracting product aspects and users opinions by extracting all possible aspects and opinions from reviews using natural language, ontology, and frequent (tag) sets. The proposed framework, when compared with an existing baseline model, yielded promising results.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2071",
        "title": "Extracting a bilingual semantic grammar from FrameNet-annotated corpora",
        "authors": [
            "Dana Dann\u00e9lls",
            "Normunds Gr\u016bz\u012btis"
        ],
        "abstract": "We present the creation of an English-Swedish FrameNet-based grammar in Grammatical Framework. The aim of this research is to make existing framenets computationally accessible for multilingual natural language applications via a common semantic grammar API, and to facilitate the porting of such grammar to other languages. In this paper, we describe the abstract syntax of the semantic grammar while focusing on its automatic extraction possibilities. We have extracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley FrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The abstract syntax defines 769 frame-specific valence patterns that cover 77.8% examples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames. As a side result, we provide a unified method for comparing semantic and syntactic valence patterns across framenets.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2188",
        "title": "A Convolutional Neural Network for Modelling Sentences",
        "authors": [
            "Nal Kalchbrenner",
            "Edward Grefenstette",
            "Phil Blunsom"
        ],
        "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2878",
        "title": "Overview of Stemming Algorithms for Indian and Non-Indian Languages",
        "authors": [
            "Dalwadi Bijal",
            "Suthar Sanket"
        ],
        "abstract": "Stemming is a pre-processing step in Text Mining applications as well as a very common requirement of Natural Language processing functions. Stemming is the process for reducing inflected words to their stem. The main purpose of stemming is to reduce different grammatical forms / word forms of a word like its noun, adjective, verb, adverb etc. to its root form. Stemming is widely uses in Information Retrieval system and reduces the size of index files. We can say that the goal of stemming is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. In this paper we have discussed different stemming algorithm for non-Indian and Indian language, methods of stemming, accuracy and errors.\n    ",
        "submission_date": "2014-04-10T00:00:00",
        "last_modified_date": "2014-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2997",
        "title": "Automatic Detection of Reuses and Citations in Literary Texts",
        "authors": [
            "Jean-Gabriel Ganascia",
            "Pierre Glaudes",
            "Andrea Del Lungo"
        ],
        "abstract": "For more than forty years now, modern theories of literature (Compagnon, 1979) insist on the role of paraphrases, rewritings, citations, reciprocal borrowings and mutual contributions of any kinds. The notions of intertextuality, transtextuality, hypertextuality/hypotextuality, were introduced in the seventies and eighties to approach these phenomena. The careful analysis of these references is of particular interest in evaluating the distance that the creator voluntarily introduces with his/her masters. Phoebus is collaborative project that makes computer scientists from the University Pierre and Marie Curie (LIP6-UPMC) collaborate with the literary teams of Paris-Sorbonne University with the aim to develop efficient tools for literary studies that take advantage of modern computer science techniques. In this context, we have developed a piece of software that automatically detects and explores networks of textual reuses in classical literature. This paper describes the principles on which is based this program, the significant results that have already been obtained and the perspectives for the near future.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3233",
        "title": "Pagination: It's what you say, not how long it takes to say it",
        "authors": [
            "Joshua Hailpern",
            "Niranjan Damera Venkata",
            "Marina Danilevsky"
        ],
        "abstract": "Pagination - the process of determining where to break an article across pages in a multi-article layout is a common layout challenge for most commercially printed newspapers and magazines. To date, no one has created an algorithm that determines a minimal pagination break point based on the content of the article. Existing approaches for automatic multi-article layout focus exclusively on maximizing content (number of articles) and optimizing aesthetic presentation (e.g., spacing between articles). However, disregarding the semantic information within the article can lead to overly aggressive cutting, thereby eliminating key content and potentially confusing the reader, or setting too generous of a break point, thereby leaving in superfluous content and making automatic layout more difficult. This is one of the remaining challenges on the path from manual layouts to fully automated processes that still ensure article content quality. In this work, we present a new approach to calculating a document minimal break point for the task of pagination. Our approach uses a statistical language model to predict minimal break points based on the semantic content of an article. We then compare 4 novel candidate approaches, and 4 baselines (currently in use by layout algorithms). Results from this experiment show that one of our approaches strongly outperforms the baselines and alternatives. Results from a second study suggest that humans are not able to agree on a single \"best\" break point. Therefore, this work shows that a semantic-based lower bound break point prediction is necessary for ideal automated document synthesis within a real-world context.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3377",
        "title": "A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing",
        "authors": [
            "Rene Pickhardt",
            "Thomas Gottron",
            "Martin K\u00f6rner",
            "Paul Georg Wagner",
            "Till Speicher",
            "Steffen Staab"
        ],
        "abstract": "We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity.\n    ",
        "submission_date": "2014-04-13T00:00:00",
        "last_modified_date": "2014-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3759",
        "title": "Meta-evaluation of comparability metrics using parallel corpora",
        "authors": [
            "Bogdan Babych",
            "Anthony Hartley"
        ],
        "abstract": "Metrics for measuring the comparability of corpora or texts need to be developed and evaluated systematically. Applications based on a corpus, such as training Statistical MT systems in specialised narrow domains, require finding a reasonable balance between the size of the corpus and its consistency, with controlled and benchmarked levels of comparability for any newly added sections. In this article we propose a method that can meta-evaluate comparability metrics by calculating monolingual comparability scores separately on the 'source' and 'target' sides of parallel corpora. The range of scores on the source side is then correlated (using Pearson's r coefficient) with the range of 'target' scores; the higher the correlation - the more reliable is the metric. The intuition is that a good metric should yield the same distance between different domains in different languages. Our method gives consistent results for the same metrics on different data sets, which indicates that it is reliable and can be used for metric comparison or for optimising settings of parametrised metrics.\n    ",
        "submission_date": "2014-04-14T00:00:00",
        "last_modified_date": "2014-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3925",
        "title": "Complexity of Grammar Induction for Quantum Types",
        "authors": [
            "Antonin Delpeuch"
        ],
        "abstract": "Most categorical models of meaning use a functor from the syntactic category to the semantic category. When semantic information is available, the problem of grammar induction can therefore be defined as finding preimages of the semantic types under this forgetful functor, lifting the information flow from the semantic level to a valid reduction at the syntactic level. We study the complexity of grammar induction, and show that for a variety of type systems, including pivotal and compact closed categories, the grammar induction problem is NP-complete. Our approach could be extended to linguistic type systems such as autonomous or bi-closed categories.\n    ",
        "submission_date": "2014-04-13T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3992",
        "title": "Assessing the Quality of MT Systems for Hindi to English Translation",
        "authors": [
            "Aditi Kalyani",
            "Hemant Kumud",
            "Shashi Pal Singh",
            "Ajai Kumar"
        ],
        "abstract": "Evaluation plays a vital role in checking the quality of MT output. It is done either manually or automatically. Manual evaluation is very time consuming and subjective, hence use of automatic metrics is done most of the times. This paper evaluates the translation quality of different MT Engines for Hindi-English (Hindi data is provided as input and English is obtained as output) using various automatic metrics like BLEU, METEOR etc. Further the comparison automatic evaluation results with Human ranking have also been given.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4314",
        "title": "An Empirical Comparison of Parsing Methods for Stanford Dependencies",
        "authors": [
            "Lingpeng Kong",
            "Noah A. Smith"
        ],
        "abstract": "Stanford typed dependencies are a widely desired representation of natural language sentences, but parsing is one of the major computational bottlenecks in text analysis systems. In light of the evolving definition of the Stanford dependencies and developments in statistical dependency parsing algorithms, this paper revisits the question of Cer et al. (2010): what is the tradeoff between accuracy and speed in obtaining Stanford dependencies in particular? We also explore the effects of input representations on this tradeoff: part-of-speech tags, the novel use of an alternative dependency representation as input, and distributional representaions of words. We find that direct dependency parsing is a more viable solution than it was found to be in the past. An accompanying software release can be found at: ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4326",
        "title": "Open Question Answering with Weakly Supervised Embedding Models",
        "authors": [
            "Antoine Bordes",
            "Jason Weston",
            "Nicolas Usunier"
        ],
        "abstract": "Building computers able to answer questions on any subject is a long standing goal of artificial intelligence. Promising progress has recently been achieved by methods that learn to map questions to logical forms or database queries. Such approaches can be effective but at the cost of either large amounts of human-labeled data or by defining lexicons and grammars tailored by practitioners. In this paper, we instead take the radical approach of learning to map questions to vectorial feature representations. By mapping answers into the same space one can query any knowledge base independent of its schema, without requiring any grammar or lexicon. Our method is trained with a new optimization procedure combining stochastic gradient descent followed by a fine-tuning step using the weak supervision provided by blending automatically and collaboratively generated resources. We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex, the only existing method able to be trained on similar weakly labeled data.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4572",
        "title": "The First Parallel Multilingual Corpus of Persian: Toward a Persian BLARK",
        "authors": [
            "Behrang Qasemizadeh",
            "Saeed Rahimi",
            "Behrooz Mahmoodi Bakhtiari"
        ],
        "abstract": "In this article, we have introduced the first parallel corpus of Persian with more than 10 other European languages. This article describes primary steps toward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now, we have proposed morphosyntactic specification of Persian based on EAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article introduces Persian Language, with emphasis on its orthography and morphosyntactic features, then a new Part-of-Speech categorization and orthography for Persian in digital environments is proposed. Finally, the corpus and related statistic will be analyzed.\n    ",
        "submission_date": "2014-04-17T00:00:00",
        "last_modified_date": "2014-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4641",
        "title": "Multilingual Models for Compositional Distributed Semantics",
        "authors": [
            "Karl Moritz Hermann",
            "Phil Blunsom"
        ],
        "abstract": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data.\n    ",
        "submission_date": "2014-04-17T00:00:00",
        "last_modified_date": "2014-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4714",
        "title": "Radical-Enhanced Chinese Character Embedding",
        "authors": [
            "Yaming Sun",
            "Lei Lin",
            "Duyu Tang",
            "Nan Yang",
            "Zhenzhou Ji",
            "Xiaolong Wang"
        ],
        "abstract": "We present a method to leverage radical for learning Chinese character embedding. Radical is a semantic and phonetic component of Chinese character. It plays an important role as characters with the same radical usually have similar semantic meaning and grammatical usage. However, existing Chinese processing algorithms typically regard word or character as the basic unit but ignore the crucial radical information. In this paper, we fill this gap by leveraging radical for learning continuous representation of Chinese character. We develop a dedicated neural architecture to effectively learn character embedding and apply it on Chinese character similarity judgement and Chinese word segmentation. Experiment results show that our radical-enhanced method outperforms existing embedding learning algorithms on both tasks.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4740",
        "title": "Challenges in Persian Electronic Text Analysis",
        "authors": [
            "Behrang QasemiZadeh",
            "Saeed Rahimi",
            "Mehdi Safaee Ghalati"
        ],
        "abstract": "Farsi, also known as Persian, is the official language of Iran and Tajikistan and one of the two main languages spoken in Afghanistan. Farsi enjoys a unified Arabic script as its writing system. In this paper we briefly introduce the writing standards of Farsi and highlight problems one would face when analyzing Farsi electronic texts, especially during development of Farsi corpora regarding to transcription and encoding of Farsi e-texts. The pointes mentioned may sounds easy but they are crucial when developing and processing written corpora of Farsi.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5278",
        "title": "The Frobenius anatomy of word meanings I: subject and object relative pronouns",
        "authors": [
            "Mehrnoosh Sadrzadeh",
            "Stephen Clark",
            "Bob Coecke"
        ],
        "abstract": "This paper develops a compositional vector-based semantics of subject and object relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. We develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics.\n    ",
        "submission_date": "2014-04-21T00:00:00",
        "last_modified_date": "2014-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5357",
        "title": "Morphological Analysis of the Bishnupriya Manipuri Language using Finite State Transducers",
        "authors": [
            "Nayan Jyoti Kalita",
            "Navanath Saharia",
            "Smriti Kumar Sinha"
        ],
        "abstract": "In this work we present a morphological analysis of Bishnupriya Manipuri language, an Indo-Aryan language spoken in the north eastern India. As of now, there is no computational work available for the language. Finite state morphology is one of the successful approaches applied in a wide variety of languages over the year. Therefore we adapted the finite state approach to analyse morphology of the Bishnupriya Manipuri language.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5367",
        "title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution",
        "authors": [
            "Alexandre Passos",
            "Vineet Kumar",
            "Andrew McCallum"
        ],
        "abstract": "Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons. Recently neural network-based language models have been explored, as they as a byproduct generate highly informative vector representations for words, known as word embeddings. In this paper we present two contributions: a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embeddings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for CoNLL 2003---significantly better than any previous system trained on public data, and matching a system employing massive private industrial query-log data.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5585",
        "title": "A Structural Query System for Han Characters",
        "authors": [
            "Matthew Skala"
        ],
        "abstract": "The IDSgrep structural query system for Han character dictionaries is presented. This system includes a data model and syntax for describing the spatial structure of Han characters using Extended Ideographic Description Sequences (EIDSes) based on the Unicode IDS syntax; a language for querying EIDS databases, designed to suit the needs of font developers and foreign language learners; a bit vector index inspired by Bloom filters for faster query operations; a freely available implementation; and format translation from popular third-party IDS and XML character databases. Experimental results are included, with a comparison to other software used for similar applications.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6312",
        "title": "Reconstructing Native Language Typology from Foreign Language Usage",
        "authors": [
            "Yevgeni Berzak",
            "Roi Reichart",
            "Boris Katz"
        ],
        "abstract": "Linguists and psychologists have long been studying cross-linguistic transfer, the influence of native language properties on linguistic performance in a foreign language. In this work we provide empirical evidence for this process in the form of a strong correlation between language similarities derived from structural features in English as Second Language (ESL) texts and equivalent similarities obtained from the typological features of the native languages. We leverage this finding to recover native language typological similarity structure directly from ESL text, and perform prediction of typological features in an unsupervised fashion with respect to the target languages. Our method achieves 72.2% accuracy on the typology prediction task, a result that is highly competitive with equivalent methods that rely on typological resources.\n    ",
        "submission_date": "2014-04-25T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6491",
        "title": "An Account of Opinion Implicatures",
        "authors": [
            "Janyce Wiebe",
            "Lingjia Deng"
        ],
        "abstract": "While previous sentiment analysis research has concentrated on the interpretation of explicitly stated opinions and attitudes, this work initiates the computational study of a type of opinion implicature (i.e., opinion-oriented inference) in text. This paper described a rule-based framework for representing and analyzing opinion implicatures which we hope will contribute to deeper automatic interpretation of subjective language. In the course of understanding implicatures, the system recognizes implicit sentiments (and beliefs) toward various events and entities in the sentence, often attributed to different sources (holders) and of mixed polarities; thus, it produces a richer interpretation than is typical in opinion analysis.\n    ",
        "submission_date": "2014-04-23T00:00:00",
        "last_modified_date": "2014-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7296",
        "title": "A Deep Architecture for Semantic Parsing",
        "authors": [
            "Edward Grefenstette",
            "Phil Blunsom",
            "Nando de Freitas",
            "Karl Moritz Hermann"
        ],
        "abstract": "Many successful approaches to semantic parsing build on top of the syntactic analysis of text, and make use of distributional representations or statistical models to match parses to ontology-specific queries. This paper presents a novel deep learning architecture which provides a semantic parsing system through the union of two neural models of language semantics. It allows for the generation of ontology-specific queries from natural language statements and questions without the need for parsing, which makes it especially suitable to grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resource-poor languages.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7362",
        "title": "Concise comparative summaries (CCS) of large text corpora with a human experiment",
        "authors": [
            "Jinzhu Jia",
            "Luke Miratrix",
            "Bin Yu",
            "Brian Gawalt",
            "Laurent El Ghaoui",
            "Luke Barnesmoore",
            "Sophie Clavier"
        ],
        "abstract": "In this paper we propose a general framework for topic-specific summarization of large text corpora and illustrate how it can be used for the analysis of news databases. Our framework, concise comparative summarization (CCS), is built on sparse classification methods. CCS is a lightweight and flexible tool that offers a compromise between simple word frequency based methods currently in wide use and more heavyweight, model-intensive methods such as latent Dirichlet allocation (LDA). We argue that sparse methods have much to offer for text analysis and hope CCS opens the door for a new branch of research in this important field. For a particular topic of interest (e.g., China or energy), CSS automatically labels documents as being either on- or off-topic (usually via keyword search), and then uses sparse classification methods to predict these labels with the high-dimensional counts of all the other words and phrases in the documents. The resulting small set of phrases found as predictive are then harvested as the summary. To validate our tool, we, using news articles from the New York Times international section, designed and conducted a human survey to compare the different summarizers with human understanding. We demonstrate our approach with two case studies, a media analysis of the framing of \"Egypt\" in the New York Times throughout the Arab Spring and an informal comparison of the New York Times' and Wall Street Journal's coverage of \"energy.\" Overall, we find that the Lasso with $L^2$ normalization can be effectively and usefully used to summarize large corpora, regardless of document size.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0049",
        "title": "Exemplar Dynamics Models of the Stability of Phonological Categories",
        "authors": [
            "P. F. Tupper"
        ],
        "abstract": "We develop a model for the stability and maintenance of phonological categories. Examples of phonological categories are vowel sounds such as \"i\" and \"e\". We model such categories as consisting of collections of labeled exemplars that language users store in their memory. Each exemplar is a detailed memory of an instance of the linguistic entity in question. Starting from an exemplar-level model we derive integro-differential equations for the long-term evolution of the density of exemplars in different portions of phonetic space. Using these latter equations we investigate under what conditions two phonological categories merge or not. Our main conclusion is that for the preservation of distinct phonological categories, it is necessary that anomalous speech tokens of a given category are discarded, and not merely stored in memory as an exemplar of another category.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0145",
        "title": "Contextual Semantic Parsing using Crowdsourced Spatial Descriptions",
        "authors": [
            "Kais Dukes"
        ],
        "abstract": "We describe a contextual parser for the Robot Commands Treebank, a new crowdsourced resource. In contrast to previous semantic parsers that select the most-probable parse, we consider the different problem of parsing using additional situational context to disambiguate between different readings of a sentence. We show that multiple semantic analyses can be searched using dynamic programming via interaction with a spatial planner, to guide the parsing process. We are able to parse sentences in near linear-time by ruling out analyses early on that are incompatible with spatial context. We report a 34% upper bound on accuracy, as our planner correctly processes spatial context for 3,394 out of 10,000 sentences. However, our parser achieves a 96.53% exact-match score for parsing within the subset of sentences recognized by the planner, compared to 82.14% for a non-contextual parser.\n    ",
        "submission_date": "2014-05-01T00:00:00",
        "last_modified_date": "2014-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0603",
        "title": "Extracting Family Relationship Networks from Novels",
        "authors": [
            "Aibek Makazhanov",
            "Denilson Barbosa",
            "Grzegorz Kondrak"
        ],
        "abstract": "We present an approach to the extraction of family relations from literary narrative, which incorporates a technique for utterance attribution proposed recently by Elson and McKeown (2010). In our work this technique is used in combination with the detection of vocatives - the explicit forms of address used by the characters in a novel. We take advantage of the fact that certain vocatives indicate family relations between speakers. The extracted relations are then propagated using a set of rules. We report the results of the application of our method to Jane Austen's Pride and Prejudice.\n    ",
        "submission_date": "2014-05-03T00:00:00",
        "last_modified_date": "2014-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0616",
        "title": "Automated Attribution and Intertextual Analysis",
        "authors": [
            "James Brofos",
            "Ajay Kannan",
            "Rui Shu"
        ],
        "abstract": "In this work, we employ quantitative methods from the realm of statistics and machine learning to develop novel methodologies for author attribution and textual analysis. In particular, we develop techniques and software suitable for applications to Classical study, and we illustrate the efficacy of our approach in several interesting open questions in the field. We apply our numerical analysis techniques to questions of authorship attribution in the case of the Greek tragedian Euripides, to instances of intertextuality and influence in the poetry of the Roman statesman Seneca the Younger, and to cases of \"interpolated\" text with respect to the histories of Livy.\n    ",
        "submission_date": "2014-05-03T00:00:00",
        "last_modified_date": "2014-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0701",
        "title": "\"Translation can't change a name\": Using Multilingual Data for Named Entity Recognition",
        "authors": [
            "Manaal Faruqui"
        ],
        "abstract": "Named Entities (NEs) are often written with no orthographic changes across different languages that share a common alphabet. We show that this can be leveraged so as to improve named entity recognition (NER) by using unsupervised word clusters from secondary languages as features in state-of-the-art discriminative NER systems. We observe significant increases in performance, finding that person and location identification is particularly improved, and that phylogenetically close languages provide more valuable features than more distant languages.\n    ",
        "submission_date": "2014-05-04T00:00:00",
        "last_modified_date": "2014-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0947",
        "title": "Learning Bilingual Word Representations by Marginalizing Alignments",
        "authors": [
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Karl Moritz Hermann",
            "Phil Blunsom"
        ],
        "abstract": "We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1346",
        "title": "Automatic Method Of Domain Ontology Construction based on Characteristics of Corpora POS-Analysis",
        "authors": [
            "Olena Orobinska"
        ],
        "abstract": "It is now widely recognized that ontologies, are one of the fundamental cornerstones of knowledge-based systems. What is lacking, however, is a currently accepted strategy of how to build ontology; what kinds of the resources and techniques are indispensables to optimize the expenses and the time on the one hand and the amplitude, the completeness, the robustness of en ontology on the other hand. The paper offers a semi-automatic ontology construction method from text corpora in the domain of radiological protection. This method is composed from next steps: 1) text annotation with part-of-speech tags; 2) revelation of the significant linguistic structures and forming the templates; 3) search of text fragments corresponding to these templates; 4) basic ontology instantiation process\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1359",
        "title": "Latent semantics of action verbs reflect phonetic parameters of intensity and emotional content",
        "authors": [
            "Michael Kai Petersen"
        ],
        "abstract": "Conjuring up our thoughts, language reflects statistical patterns of word co-occurrences which in turn come to describe how we perceive the world. Whether counting how frequently nouns and verbs combine in Google search queries, or extracting eigenvectors from term document matrices made up of Wikipedia lines and Shakespeare plots, the resulting latent semantics capture not only the associative links which form concepts, but also spatial dimensions embedded within the surface structure of language. As both the shape and movements of objects have been found to be associated with phonetic contrasts already in toddlers, this study explores whether articulatory and acoustic parameters may likewise differentiate the latent semantics of action verbs. Selecting 3 x 20 emotion, face, and hand related verbs known to activate premotor areas in the brain, their mutual cosine similarities were computed using latent semantic analysis LSA, and the resulting adjacency matrices were compared based on two different large scale text corpora; HAWIK and TASA. Applying hierarchical clustering to identify common structures across the two text corpora, the verbs largely divide into combined mouth and hand movements versus emotional expressions. Transforming the verbs into their constituent phonemes, the clustered small and large size movements appear differentiated by front versus back vowels corresponding to increasing levels of arousal. Whereas the clustered emotional verbs seem characterized by sequences of close versus open jaw produced phonemes, generating up- or downwards shifts in formant frequencies that may influence their perceived valence. Suggesting, that the latent semantics of action verbs reflect parameters of intensity and emotional polarity that appear correlated with the articulatory contrasts and acoustic characteristics of phonemes\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1406",
        "title": "D-Bees: A Novel Method Inspired by Bee Colony Optimization for Solving Word Sense Disambiguation",
        "authors": [
            "Sallam Abualhaija",
            "Karl-Heinz Zimmermann"
        ],
        "abstract": "Word sense disambiguation (WSD) is a problem in the field of computational linguistics given as finding the intended sense of a word (or a set of words) when it is activated within a certain context. WSD was recently addressed as a combinatorial optimization problem in which the goal is to find a sequence of senses that maximize the semantic relatedness among the target words. In this article, a novel algorithm for solving the WSD problem called D-Bees is proposed which is inspired by bee colony optimization (BCO)where artificial bee agents collaborate to solve the problem. The D-Bees algorithm is evaluated on a standard dataset (SemEval 2007 coarse-grained English all-words task corpus)and is compared to simulated annealing, genetic algorithms, and two ant colony optimization techniques (ACO). It will be observed that the BCO and ACO approaches are on par.\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1439",
        "title": "A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication",
        "authors": [
            "Chenhao Tan",
            "Lillian Lee"
        ],
        "abstract": "The strength with which a statement is made can have a significant impact on the audience. For example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings. It is thus important to understand the effects of statement strength. A first step is to be able to distinguish between strong and weak statements. However, even this problem is understudied, partly due to a lack of data. Since strength is inherently relative, revisions of texts that make claims are a natural source of data on strength differences. In this paper, we introduce a corpus of sentence-level revisions from academic writing. We also describe insights gained from our annotation efforts for this task.\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1605",
        "title": "DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News",
        "authors": [
            "Jacopo Staiano",
            "Marco Guerini"
        ],
        "abstract": "While many lexica annotated with words polarity are available for sentiment analysis, very few tackle the harder task of emotion analysis and are usually quite limited in coverage. In this paper, we present a novel approach for extracting - in a totally automated way - a high-coverage and high-precision lexicon of roughly 37 thousand terms annotated with emotion scores, called DepecheMood. Our approach exploits in an original way 'crowd-sourced' affective annotation implicitly provided by readers of news articles from ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2014-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1893",
        "title": "Initial Comparison of Linguistic Networks Measures for Parallel Texts",
        "authors": [
            "Kristina Ban",
            "Ana Me\u0161trovi\u0107",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107"
        ],
        "abstract": "This paper presents preliminary results of Croatian syllable networks analysis. Syllable network is a network in which nodes are syllables and links between them are constructed according to their connections within words. In this paper we analyze networks of syllables generated from texts collected from the Croatian Wikipedia and Blogs. As a main tool we use complex network analysis methods which provide mechanisms that can reveal new patterns in a language structure. We aim to show that syllable networks have much higher clustering coefficient in comparison to Erd\u00f6s-Renyi random networks. The results indicate that Croatian syllable networks exhibit certain properties of a small world networks. Furthermore, we compared Croatian syllable networks with Portuguese and Chinese syllable networks and we showed that they have similar properties.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1924",
        "title": "An Expert System for Automatic Reading of A Text Written in Standard Arabic",
        "authors": [
            "Tebbi Hanane",
            "Azzoune Hamid"
        ],
        "abstract": "In this work we present our expert system of Automatic reading or speech synthesis based on a text written in Standard Arabic, our work is carried out in two great stages: the creation of the sound data base, and the transformation of the written text into speech (Text To Speech TTS). This transformation is done firstly by a Phonetic Orthographical Transcription (POT) of any written Standard Arabic text with the aim of transforming it into his corresponding phonetics sequence, and secondly by the generation of the voice signal which corresponds to the chain transcribed. We spread out the different of conception of the system, as well as the results obtained compared to others works studied to realize TTS based on Standard Arabic.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2434",
        "title": "Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation",
        "authors": [
            "Chen Lijiang"
        ],
        "abstract": "Minimum error rate training (MERT) is a widely used training procedure for statistical machine translation. A general problem of this approach is that the search space is easy to converge to a local optimum and the acquired weight set is not in accord with the real distribution of feature functions. This paper introduces coordinate system selection (RSS) into the search algorithm for MERT. Contrary to previous approaches in which every dimension only corresponds to one independent feature function, we create several coordinate systems by moving one of the dimensions to a new direction. The basic idea is quite simple but critical that the training procedure of MERT should be based on a coordinate system formed by search directions but not directly on feature functions. Experiments show that by selecting coordinate systems with tuning set results, better results can be obtained without any other language knowledge.\n    ",
        "submission_date": "2014-05-10T00:00:00",
        "last_modified_date": "2014-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2702",
        "title": "Comparison of the language networks from literature and blogs",
        "authors": [
            "Sabina \u0160i\u0161ovi\u0107",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107",
            "Ana Me\u0161trovi\u0107"
        ],
        "abstract": "In this paper we present the comparison of the linguistic networks from literature and blog texts. The linguistic networks are constructed from texts as directed and weighted co-occurrence networks of words. Words are nodes and links are established between two nodes if they are directly co-occurring within the sentence. The comparison of the networks structure is performed at global level (network) in terms of: average node degree, average shortest path length, diameter, clustering coefficient, density and number of components. Furthermore, we perform analysis on the local level (node) by comparing the rank plots of in and out degree, strength and selectivity. The selectivity-based results point out that there are differences between the structure of the networks constructed from literature and blogs.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2874",
        "title": "A Study of Entanglement in a Categorical Framework of Natural Language",
        "authors": [
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "In both quantum mechanics and corpus linguistics based on vector spaces, the notion of entanglement provides a means for the various subsystems to communicate with each other. In this paper we examine a number of implementations of the categorical framework of Coecke, Sadrzadeh and Clark (2010) for natural language, from an entanglement perspective. Specifically, our goal is to better understand in what way the level of entanglement of the relational tensors (or the lack of it) affects the compositional structures in practical situations. Our findings reveal that a number of proposals for verb construction lead to almost separable tensors, a fact that considerably simplifies the interactions between the words. We examine the ramifications of this fact, and we show that the use of Frobenius algebras mitigates the potential problems to a great extent. Finally, we briefly examine a machine learning method that creates verb tensors exhibiting a sufficient level of entanglement.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3033",
        "title": "Phonetic based SoundEx & ShapeEx algorithm for Sindhi Spell Checker System",
        "authors": [
            "Zeeshan Bhatti",
            "Ahmad Waqas",
            "Imdad Ali Ismaili",
            "Dil Nawaz Hakro",
            "Waseem Javaid Soomro"
        ],
        "abstract": "This paper presents a novel combinational phonetic algorithm for Sindhi Language, to be used in developing Sindhi Spell Checker which has yet not been developed prior to this work. The compound textual forms and glyphs of Sindhi language presents a substantial challenge for developing Sindhi spell checker system and generating similar suggestion list for misspelled words. In order to implement such a system, phonetic based Sindhi language rules and patterns must be considered into account for increasing the accuracy and efficiency. The proposed system is developed with a blend between Phonetic based SoundEx algorithm and ShapeEx algorithm for pattern or glyph matching, generating accurate and efficient suggestion list for incorrect or misspelled Sindhi words. A table of phonetically similar sounding Sindhi characters for SoundEx algorithm is also generated along with another table containing similar glyph or shape based character groups for ShapeEx algorithm. Both these are first ever attempt of any such type of categorization and representation for Sindhi Language.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3282",
        "title": "How to Ask for a Favor: A Case Study on the Success of Altruistic Requests",
        "authors": [
            "Tim Althoff",
            "Cristian Danescu-Niculescu-Mizil",
            "Dan Jurafsky"
        ],
        "abstract": "Requests are at the core of many social media systems such as question & answer sites and online philanthropy communities. While the success of such requests is critical to the success of the community, the factors that lead community members to satisfy a request are largely unknown. Success of a request depends on factors like who is asking, how they are asking, when are they asking, and most critically what is being requested, ranging from small favors to substantial monetary donations. We present a case study of altruistic requests in an online community where all requests ask for the very same contribution and do not offer anything tangible in return, allowing us to disentangle what is requested from textual and social factors. Drawing from social psychology literature, we extract high-level social features from text that operationalize social relations between recipient and donor and demonstrate that these extracted relations are predictive of success. More specifically, we find that clearly communicating need through the narrative is essential and that that linguistic indications of gratitude, evidentiality, and generalized reciprocity, as well as high status of the asker further increase the likelihood of success. Building on this understanding, we develop a model that can predict the success of unseen requests, significantly improving over several baselines. We link these findings to research in psychology on helping behavior, providing a basis for further analysis of success in social media systems.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3515",
        "title": "Temporal Analysis of Language through Neural Language Models",
        "authors": [
            "Yoon Kim",
            "Yi-I Chiu",
            "Kentaro Hanaki",
            "Darshan Hegde",
            "Slav Petrov"
        ],
        "abstract": "We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as \"cell\" and \"gay\" as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3518",
        "title": "Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme for Sentiment Analysis and Text Classification",
        "authors": [
            "Yoon Kim",
            "Owen Zhang"
        ],
        "abstract": "We provide a simple but novel supervised weighting scheme for adjusting term frequency in tf-idf for sentiment analysis and text classification. We compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks. The method is robust and works well on both snippets and longer documents.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3772",
        "title": "INAUT, a Controlled Language for the French Coast Pilot Books Instructions nautiques",
        "authors": [
            "Yannis Haralambous",
            "Julie Sauvage-Vincent",
            "John Puentes"
        ],
        "abstract": "We describe INAUT, a controlled natural language dedicated to collaborative update of a knowledge base on maritime navigation and to automatic generation of coast pilot books (Instructions nautiques) of the French National Hydrographic and Oceanographic Service SHOM. INAUT is based on French language and abundantly uses georeferenced entities. After describing the structure of the overall system, giving details on the language and on its generation, and discussing the three major applications of INAUT (document production, interaction with ENCs and collaborative updates of the knowledge base), we conclude with future extensions and open problems.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3786",
        "title": "Complex Networks Measures for Differentiation between Normal and Shuffled Croatian Texts",
        "authors": [
            "Domagoj Margan",
            "Ana Me\u0161trovi\u0107",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107"
        ],
        "abstract": "This paper studies the properties of the Croatian texts via complex networks. We present network properties of normal and shuffled Croatian texts for different shuffling principles: on the sentence level and on the text level. In both experiments we preserved the vocabulary size, word and sentence frequency distributions. Additionally, in the first shuffling approach we preserved the sentence structure of the text and the number of words per sentence. Obtained results showed that degree rank distributions exhibit no substantial deviation in shuffled networks, and strength rank distributions are preserved due to the same word frequencies. Therefore, standard approach to study the structure of linguistic co-occurrence networks showed no clear difference among the topologies of normal and shuffled texts. Finally, we showed that the in- and out- selectivity values from shuffled texts are constantly below selectivity values calculated from normal texts. Our results corroborate that the node selectivity measure can capture structural differences between original and shuffled Croatian texts.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3925",
        "title": "M\u00e9thodes pour la repr\u00e9sentation informatis\u00e9e de donn\u00e9es lexicales / Methoden der Speicherung lexikalischer Daten",
        "authors": [
            "Laurent Romary",
            "Andreas Witt"
        ],
        "abstract": "In recent years, new developments in the area of lexicography have altered not only the management, processing and publishing of lexicographical data, but also created new types of products such as electronic dictionaries and thesauri. These expand the range of possible uses of lexical data and support users with more flexibility, for instance in assisting human translation. In this article, we give a short and easy-to-understand introduction to the problematic nature of the storage, display and interpretation of lexical data. We then describe the main methods and specifications used to build and represent lexical data. This paper is targeted for the following groups of people: linguists, lexicographers, IT specialists, computer linguists and all others who wish to learn more about the modelling, representation and visualization of lexical knowledge. This paper is written in two languages: French and German.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4053",
        "title": "Distributed Representations of Sentences and Documents",
        "authors": [
            "Quoc V. Le",
            "Tomas Mikolov"
        ],
        "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4097",
        "title": "A preliminary study of Croatian Language Syllable Networks",
        "authors": [
            "Kristina Ban",
            "Ivan Ivaki\u0107",
            "Ana Me\u0161trovi\u0107"
        ],
        "abstract": "This paper presents preliminary results of Croatian syllable networks analysis. Syllable network is a network in which nodes are syllables and links between them are constructed according to their connections within words. In this paper we analyze networks of syllables generated from texts collected from the Croatian Wikipedia and Blogs. As a main tool we use complex network analysis methods which provide mechanisms that can reveal new patterns in a language structure. We aim to show that syllable networks have much higher clustering coefficient in comparison to Erd\u00f6s-Renyi random networks. The results indicate that Croatian syllable networks exhibit certain properties of a small world networks. Furthermore, we compared Croatian syllable networks with Portuguese and Chinese syllable networks and we showed that they have similar properties.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4248",
        "title": "Les math\u00e9matiques de la langue : l'approche formelle de Montague",
        "authors": [
            "Yannis Haralambous"
        ],
        "abstract": "We present a natural language modelization method which is strongely relying on mathematics. This method, called \"Formal Semantics,\" has been initiated by the American linguist Richard M. Montague in the 1970's. It uses mathematical tools such as formal languages and grammars, first-order logic, type theory and $\\lambda$-calculus. Our goal is to have the reader discover both Montagovian formal semantics and the mathematical tools that he used in his method.\n",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4273",
        "title": "Compositional Morphology for Word Representations and Language Modelling",
        "authors": [
            "Jan A. Botha",
            "Phil Blunsom"
        ],
        "abstract": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4364",
        "title": "Thematically Reinforced Explicit Semantic Analysis",
        "authors": [
            "Yannis Haralambous",
            "Vitaly Klyuev"
        ],
        "abstract": "We present an extended, thematically reinforced version of Gabrilovich and Markovitch's Explicit Semantic Analysis (ESA), where we obtain thematic information through the category structure of Wikipedia. For this we first define a notion of categorical tfidf which measures the relevance of terms in categories. Using this measure as a weight we calculate a maximal spanning tree of the Wikipedia corpus considered as a directed graph of pages and categories. This tree provides us with a unique path of \"most related categories\" between each page and the top of the hierarchy. We reinforce tfidf of words in a page by aggregating it with categorical tfidfs of the nodes of these paths, and define a thematically reinforced ESA semantic relatedness measure which is more robust than standard ESA and less sensitive to noise caused by out-of-context words. We apply our method to the French Wikipedia corpus, evaluate it through a text classification on a 37.5 MB corpus of 20 French newsgroups and obtain a precision increase of 9-10% compared with standard ESA.\n    ",
        "submission_date": "2014-05-17T00:00:00",
        "last_modified_date": "2014-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4392",
        "title": "That's sick dude!: Automatic identification of word sense change across different timescales",
        "authors": [
            "Sunny Mitra",
            "Ritwik Mitra",
            "Martin Riedl",
            "Chris Biemann",
            "Animesh Mukherjee",
            "Pawan Goyal"
        ],
        "abstract": "In this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books. We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points. Subsequently, we compare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a particular sense has died. We conduct a thorough evaluation of the proposed methodology both manually as well as through comparison with WordNet. Manual evaluation indicates that the algorithm could correctly identify 60.4% birth cases from a set of 48 randomly picked samples and 57% split/join cases from a set of 21 randomly picked samples. Remarkably, in 44% cases the birth of a novel sense is attested by WordNet, while in 46% cases and 43% cases split and join are respectively confirmed by WordNet. Our approach can be applied for lexicography, as well as for applications like word sense disambiguation or semantic search.\n    ",
        "submission_date": "2014-05-17T00:00:00",
        "last_modified_date": "2014-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4433",
        "title": "Preliminary Report on the Structure of Croatian Linguistic Co-occurrence Networks",
        "authors": [
            "Domagoj Margan",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107",
            "Ana Me\u0161trovi\u0107"
        ],
        "abstract": "In this article, we investigate the structure of Croatian linguistic co-occurrence networks. We examine the change of network structure properties by systematically varying the co-occurrence window sizes, the corpus sizes and removing stopwords. In a co-occurrence window of size $n$ we establish a link between the current word and $n-1$ subsequent words. The results point out that the increase of the co-occurrence window size is followed by a decrease in diameter, average path shortening and expectedly condensing the average clustering coefficient. The same can be noticed for the removal of the stopwords. Finally, since the size of texts is reflected in the network properties, our results suggest that the corpus influence can be reduced by increasing the co-occurrence window size.\n    ",
        "submission_date": "2014-05-17T00:00:00",
        "last_modified_date": "2014-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4599",
        "title": "Modelling Data Dispersion Degree in Automatic Robust Estimation for Multivariate Gaussian Mixture Models with an Application to Noisy Speech Processing",
        "authors": [
            "Dalei Wu",
            "Haiqing Wu"
        ],
        "abstract": "The trimming scheme with a prefixed cutoff portion is known as a method of improving the robustness of statistical models such as multivariate Gaussian mixture models (MG- MMs) in small scale tests by alleviating the impacts of outliers. However, when this method is applied to real- world data, such as noisy speech processing, it is hard to know the optimal cut-off portion to remove the outliers and sometimes removes useful data samples as well. In this paper, we propose a new method based on measuring the dispersion degree (DD) of the training data to avoid this problem, so as to realise automatic robust estimation for MGMMs. The DD model is studied by using two different measures. For each one, we theoretically prove that the DD of the data samples in a context of MGMMs approximately obeys a specific (chi or chi-square) distribution. The proposed method is evaluated on a real-world application with a moderately-sized speaker recognition task. Experiments show that the proposed method can significantly improve the robustness of the conventional training method of GMMs for speaker recognition.\n    ",
        "submission_date": "2014-05-19T00:00:00",
        "last_modified_date": "2014-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5202",
        "title": "Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference Resolution",
        "authors": [
            "Altaf Rahman",
            "Vincent Ng"
        ],
        "abstract": "Traditional learning-based coreference resolvers operate by training the mention-pair model for determining whether two mentions are coreferent or not. Though conceptually simple and easy to understand, the mention-pair model is linguistically rather unappealing and lags far behind the heuristic-based coreference models proposed in the pre-statistical NLP era in terms of sophistication. Two independent lines of recent research have attempted to improve the mention-pair model, one by acquiring the mention-ranking model to rank preceding mentions for a given anaphor, and the other by training the entity-mention model to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution, which combines the strengths of the mention-ranking model and the entity-mention model, and is therefore theoretically more appealing than both of these models. In addition, we seek to improve cluster rankers via two extensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity by jointly modeling anaphoricity determination and coreference resolution. Experimental results on the ACE data sets demonstrate the superior performance of cluster rankers to competing approaches as well as the effectiveness of our two extensions.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5208",
        "title": "A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing",
        "authors": [
            "Alexander M. Rush",
            "Michael Collins"
        ],
        "abstract": "Dual decomposition, and more generally Lagrangian relaxation, is a classical method for combinatorial optimization; it has recently been applied to several inference problems in natural language processing (NLP). This tutorial gives an overview of the technique. We describe example algorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the NLP literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5474",
        "title": "New Perspectives in Sinographic Language Processing Through the Use of Character Structure",
        "authors": [
            "Yannis Haralambous"
        ],
        "abstract": "Chinese characters have a complex and hierarchical graphical structure carrying both semantic and phonetic information. We use this structure to enhance the text model and obtain better results in standard NLP operations. First of all, to tackle the problem of graphical variation we define allographic classes of characters. Next, the relation of inclusion of a subcharacter in a characters, provides us with a directed graph of allographic classes. We provide this graph with two weights: semanticity (semantic relation between subcharacter and character) and phoneticity (phonetic relation) and calculate \"most semantic subcharacter paths\" for each character. Finally, adding the information contained in these paths to unigrams we claim to increase the efficiency of text mining methods. We evaluate our method on a text classification task on two corpora (Chinese and Japanese) of a total of 18 million characters and get an improvement of 3% on an already high baseline of 89.6% precision, obtained by a linear SVM classifier. Other possible applications and perspectives of the system are discussed.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5654",
        "title": "Machine Translation Model based on Non-parallel Corpus and Semi-supervised Transductive Learning",
        "authors": [
            "Lijiang Chen"
        ],
        "abstract": "Although the parallel corpus has an irreplaceable role in machine translation, its scale and coverage is still beyond the actual needs. Non-parallel corpus resources on the web have an inestimable potential value in machine translation and other natural language processing tasks. This article proposes a semi-supervised transductive learning method for expanding the training corpus in statistical machine translation system by extracting parallel sentences from the non-parallel corpus. This method only requires a small amount of labeled corpus and a large unlabeled corpus to build a high-performance classifier, especially for when there is short of labeled corpus. The experimental results show that by combining the non-parallel corpus alignment and the semi-supervised transductive learning method, we can more effectively use their respective strengths to improve the performance of machine translation system.\n    ",
        "submission_date": "2014-05-22T00:00:00",
        "last_modified_date": "2014-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5674",
        "title": "Mot\u00e0Mot project: conversion of a French-Khmer published dictionary for building a multilingual lexical system",
        "authors": [
            "Mathieu Mangeot"
        ],
        "abstract": "Economic issues related to the information processing techniques are very important. The development of such technologies is a major asset for developing countries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia and Thailand. The MotAMot project aims to computerize an under-resourced language: Khmer, spoken mainly in Cambodia. The main goal of the project is the development of a multilingual lexical system targeted for Khmer. The macrostructure is a pivot one with each word sense of each language linked to a pivot axi. The microstructure comes from a simplification of the explanatory and combinatory dictionary. The lexical system has been initialized with data coming mainly from the conversion of the French-Khmer bilingual dictionary of Denis Richer from Word to XML format. The French part was completed with pronunciation and parts-of-speech coming from the FeM French-english-Malay dictionary. The Khmer headwords noted in IPA in the Richer dictionary were converted to Khmer writing with OpenFST, a finite state transducer tool. The resulting resource is available online for lookup, editing, download and remote programming via a REST API on a Jibiki platform.\n    ",
        "submission_date": "2014-05-22T00:00:00",
        "last_modified_date": "2014-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5893",
        "title": "Computerization of African languages-French dictionaries",
        "authors": [
            "Chantal Enguehard",
            "Mathieu Mangeot"
        ],
        "abstract": "This paper relates work done during the DiLAF project. It consists in converting 5 bilingual African language-French dictionaries originally in Word format into XML following the LMF model. The languages processed are Bambara, Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced languages concerning Natural Language Processing tools. Once converted, the dictionaries are available online on the Jibiki platform for lookup and modification. The DiLAF project is first presented. A description of each dictionary follows. Then, the conversion methodology from .doc format to XML files is presented. A specific point on the usage of Unicode follows. Then, each step of the conversion into XML and LMF is detailed. The last part presents the Jibiki lexical resources management platform used for the project.\n    ",
        "submission_date": "2014-05-22T00:00:00",
        "last_modified_date": "2014-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6068",
        "title": "Building of Networks of Natural Hierarchies of Terms Based on Analysis of Texts Corpora",
        "authors": [
            "Dmitry Lande"
        ],
        "abstract": "The technique of building of networks of hierarchies of terms based on the analysis of chosen text corpora is offered. The technique is based on the methodology of horizontal visibility graphs. Constructed and investigated language network, formed on the basis of electronic preprints arXiv on topics of information retrieval.\n    ",
        "submission_date": "2014-05-23T00:00:00",
        "last_modified_date": "2014-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6103",
        "title": "Evaluating the fully automatic multi-language translation of the Swiss avalanche bulletin",
        "authors": [
            "Kurt Winkler",
            "Tobias Kuhn",
            "Martin Volk"
        ],
        "abstract": "The Swiss avalanche bulletin is produced twice a day in four languages. Due to the lack of time available for manual translation, a fully automated translation system is employed, based on a catalogue of predefined phrases and predetermined rules of how these phrases can be combined to produce sentences. The system is able to automatically translate such sentences from German into the target languages French, Italian and English without subsequent proofreading or correction. Our catalogue of phrases is limited to a small sublanguage. The reduction of daily translation costs is expected to offset the initial development costs within a few years. After being operational for two winter seasons, we assess here the quality of the produced texts based on an evaluation where participants rate real danger descriptions from both origins, the catalogue of phrases versus the manually written and translated texts. With a mean recognition rate of 55%, users can hardly distinguish between the two types of texts, and give similar ratings with respect to their language quality. Overall, the output from the catalogue system can be considered virtually equivalent to a text written by avalanche forecasters and then manually translated by professional translators. Furthermore, forecasters declared that all relevant situations were captured by the system with sufficient accuracy and within the limited time available.\n    ",
        "submission_date": "2014-05-23T00:00:00",
        "last_modified_date": "2014-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6164",
        "title": "Generating Natural Language Descriptions from OWL Ontologies: the NaturalOWL System",
        "authors": [
            "Ion Androutsopoulos",
            "Gerasimos Lampouras",
            "Dimitrios Galanis"
        ],
        "abstract": "We present NaturalOWL, a natural language generation system that produces texts describing individuals or classes of OWL ontologies. Unlike simpler OWL verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. With a system like NaturalOWL, one can publish information in OWL on the Web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. We discuss the processing stages of NaturalOWL, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. We also present trials showing that when the domain-dependent llinguistic resources are available, NaturalOWL produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6293",
        "title": "Cross-Language Personal Name Mapping",
        "authors": [
            "Ahmed H. Yousef"
        ],
        "abstract": "Name matching between multiple natural languages is an important step in cross-enterprise integration applications and data mining. It is difficult to decide whether or not two syntactic values (names) from two heterogeneous data sources are alternative designation of the same semantic entity (person), this process becomes more difficult with Arabic language due to several factors including spelling and pronunciation variation, dialects and special vowel and consonant distinction and other linguistic characteristics. This paper proposes a new framework for name matching between the Arabic language and other languages. The framework uses a dictionary based on a new proposed version of the Soundex algorithm to encapsulate the recognition of special features of Arabic names. The framework proposes a new proximity matching algorithm to suit the high importance of order sensitivity in Arabic name matching. New performance evaluation metrics are proposed as well. The framework is implemented and verified empirically in several case studies demonstrating substantial improvements compared to other well-known techniques found in literature.\n    ",
        "submission_date": "2014-05-24T00:00:00",
        "last_modified_date": "2014-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6682",
        "title": "Optimality Theory as a Framework for Lexical Acquisition",
        "authors": [
            "Thierry Poibeau"
        ],
        "abstract": "This paper re-investigates a lexical acquisition system initially developed for ",
        "submission_date": "2014-05-26T00:00:00",
        "last_modified_date": "2014-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7397",
        "title": "An HMM Based Named Entity Recognition System for Indian Languages: The JU System at ICON 2013",
        "authors": [
            "Vivekananda Gayen",
            "Kamal Sarkar"
        ],
        "abstract": "This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named Entity Recognition. We submitted runs for Bengali, English, Hindi, Marathi, Punjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model has been used to implement our system. The system has been trained and tested on the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of 0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali, English, Hindi, Marathi, Punjabi, Tamil and Telugu respectively.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7711",
        "title": "Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language",
        "authors": [
            "David L. Chen",
            "Joohyun Kim",
            "Raymond J. Mooney"
        ],
        "abstract": "We present a novel framework for learning to interpret and generate language using only perceptual context as supervision.  We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge.  Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace.  The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing.  Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7713",
        "title": "Using Local Alignments for Relation Recognition",
        "authors": [
            "Sophia Katrenko",
            "Pieter Adriaans",
            "Maarten van Someren"
        ],
        "abstract": "This paper discusses the problem of marrying structural similarity with semantic relatedness for Information Extraction from text. Aiming at accurate recognition of relations, we introduce local alignment kernels and explore various possibilities of using them for this task. We give a definition of a local alignment (LA) kernel based on the Smith-Waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences. We show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge. Our experiments suggest that the LA kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin. Additional series of experiments have been conducted on the data sets of seven general relation types, where the performance of the LA kernel is comparable to the current state-of-the-art results.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7908",
        "title": "Semantic Composition and Decomposition: From Recognition to Generation",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text. Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word. We take a distributional approach to semantics, in which a word is represented by a context vector. Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem. For simplicity, we focus on noun-modifier bigrams and noun unigrams. A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram (\"red salmon\"), generate a noun unigram that is synonymous with the given bigram (\"sockeye\"). A test for semantic decomposition is, given a context vector for a noun unigram (\"snifter\"), generate a noun-modifier bigram that is synonymous with the given unigram (\"brandy glass\"). With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram. We generate ranked lists of potential solutions in two passes. A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list. We evaluate the candidate solutions by comparing them to WordNet synonym sets. For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time. For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0032",
        "title": "Comparing and Combining Sentiment Analysis Methods",
        "authors": [
            "Pollyanna Gon\u00e7alves",
            "Matheus Ara\u00fajo",
            "Fabr\u00edcio Benevenuto",
            "Meeyoung Cha"
        ],
        "abstract": "Several messages express opinions about events, products, and services, political views or even their author's emotional state and mood. Sentiment analysis has been used in several applications including analysis of the repercussions of events in social networks, analysis of opinions about products and services, and simply to better understand aspects of social communication in Online Social Networks (OSNs). There are multiple methods for measuring sentiments, including lexical-based approaches and supervised machine learning methods. Despite the wide use and popularity of some methods, it is unclear which method is better for identifying the polarity (i.e., positive or negative) of a message as the current literature does not provide a method of comparison among existing methods. Such a comparison is crucial for understanding the potential limitations, advantages, and disadvantages of popular methods in analyzing the content of OSNs messages. Our study aims at filling this gap by presenting comparisons of eight popular sentiment analysis methods in terms of coverage (i.e., the fraction of messages whose sentiment is identified) and agreement (i.e., the fraction of identified sentiments that are in tune with ground truth). We develop a new method that combines existing approaches, providing the best coverage results and competitive agreement. We also present a free Web service called iFeel, which provides an open API for accessing and comparing results across different sentiment methods for a given text.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0079",
        "title": "Bridging the gap between Legal Practitioners and Knowledge Engineers using semi-formal KR",
        "authors": [
            "Shashishekar Ramakrishna",
            "Adrian Paschke"
        ],
        "abstract": "The use of Structured English as a computation independent knowledge representation format for non-technical users in business rules representation has been proposed in OMGs Semantics and Business Vocabulary Representation (SBVR). In the legal domain we face a similar problem. Formal representation languages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2 ontologies etc.) support the technical knowledge engineer and the automated reasoning. But, they can be hardly used directly by the legal domain experts who do not have a computer science background. In this paper we adapt the SBVR Structured English approach for the legal domain and implement a proof-of-concept, called KR4IPLaw, which enables legal domain experts to represent their knowledge in Structured English in a computational independent and hence, for them, more usable way. The benefit of this approach is that the underlying pre-defined semantics of the Structured English approach makes transformations into formal languages such as OASIS LegalRuleML and OWL2 ontologies possible. We exemplify our approach in the domain of patent law.\n    ",
        "submission_date": "2014-05-31T00:00:00",
        "last_modified_date": "2014-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1078",
        "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "authors": [
            "Kyunghyun Cho",
            "Bart van Merrienboer",
            "Caglar Gulcehre",
            "Dzmitry Bahdanau",
            "Fethi Bougares",
            "Holger Schwenk",
            "Yoshua Bengio"
        ],
        "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.\n    ",
        "submission_date": "2014-06-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1203",
        "title": "A Semantic Approach to Summarization",
        "authors": [
            "Divyanshu Bhartiya",
            "Ashudeep Singh"
        ],
        "abstract": "Sentence extraction based summarization methods has some limitations as it doesn't go into the semantics of the document. Also, it lacks the capability of sentence generation which is intuitive to humans. Here we present a novel method to summarize text documents taking the process to semantic levels with the use of WordNet and other resources, and using a technique for sentence generation. We involve semantic role labeling to get the semantic representation of text and use of segmentation to form clusters of the related pieces of text. Picking out the centroids and sentence generation completes the task. We evaluate our system against human composed summaries and also present an evaluation done by humans to measure the quality attributes of our summaries.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1234",
        "title": "A Geometric Method to Obtain the Generation Probability of a Sentence",
        "authors": [
            "Chen Lijiang"
        ],
        "abstract": "\"How to generate a sentence\" is the most critical and difficult problem in all the natural language processing technologies. In this paper, we present a new approach to explain the generation process of a sentence from the perspective of mathematics. Our method is based on the premise that in our brain a sentence is a part of a word network which is formed by many word nodes. Experiments show that the probability of the entire sentence can be obtained by the probabilities of single words and the probabilities of the co-occurrence of word pairs, which indicate that human use the synthesis method to generate a sentence.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1241",
        "title": "The Best Templates Match Technique For Example Based Machine Translation",
        "authors": [
            "T. El-Shishtawy",
            "A. El-Sammak"
        ],
        "abstract": "It has been proved that large scale realistic Knowledge Based Machine Translation applications require acquisition of huge knowledge about language and about the world. This knowledge is encoded in computational grammars, lexicons and domain models. Another approach which avoids the need for collecting and analyzing massive knowledge, is the Example Based approach, which is the topic of this paper. We show through the paper that using Example Based in its native form is not suitable for translating into Arabic. Therefore a modification to the basic approach is presented to improve the accuracy of the translation process. The basic idea of the new approach is to improve the technique by which template-based approaches select the appropriate templates.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1280",
        "title": "Basis Identification for Automatic Creation of Pronunciation Lexicon for Proper Names",
        "authors": [
            "Sunil Kumar Kopparapu",
            "M Laxminarayana"
        ],
        "abstract": "Development of a proper names pronunciation lexicon is usually a manual effort which can not be avoided. Grapheme to phoneme (G2P) conversion modules, in literature, are usually rule based and work best for non-proper names in a particular language. Proper names are foreign to a G2P module. We follow an optimization approach to enable automatic construction of proper names pronunciation lexicon. The idea is to construct a small orthogonal set of words (basis) which can span the set of names in a given database. We propose two algorithms for the construction of this basis. The transcription lexicon of all the proper names in a database can be produced by the manual transcription of only the small set of basis words. We first construct a cost function and show that the minimization of the cost function results in a basis. We derive conditions for convergence of this cost function and validate them experimentally on a very large proper name database. Experiments show the transcription can be achieved by transcribing a set of small number of basis words. The algorithms proposed are generic and independent of language; however performance is better if the proper names have same origin, namely, same language or geographical region.\n    ",
        "submission_date": "2014-06-05T00:00:00",
        "last_modified_date": "2014-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1827",
        "title": "Recursive Neural Networks Can Learn Logical Semantics",
        "authors": [
            "Samuel R. Bowman",
            "Christopher Potts",
            "Christopher D. Manning"
        ],
        "abstract": "Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the fixed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models---plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)---can correctly learn to identify logical relationships such as entailment and contradiction using these representations. In our first set of experiments, we generate artificial data from a logical grammar and use it to evaluate the models' ability to learn to handle basic relational reasoning, recursive structures, and quantification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1870",
        "title": "Toward verbalizing ontologies in isiZulu",
        "authors": [
            "C. Maria Keet",
            "Langa Khumalo"
        ],
        "abstract": "IsiZulu is one of the eleven official languages of South Africa and roughly half the population can speak it. It is the first (home) language for over 10 million people in South Africa. Only a few computational resources exist for isiZulu and its related Nguni languages, yet the imperative for tool development exists. We focus on natural language generation, and the grammar options and preferences in particular, which will inform verbalization of knowledge representation languages and could contribute to machine translation. The verbalization pattern specification shows that the grammar rules are elaborate and there are several options of which one may have preference. We devised verbalization patterns for subsumption, basic disjointness, existential and universal quantification, and conjunction. This was evaluated in a survey among linguists and non-linguists. Some differences between linguists and non-linguists can be observed, with the former much more in agreement, and preferences depend on the overall structure of the sentence, such as singular for subsumption and plural in other cases.\n    ",
        "submission_date": "2014-06-07T00:00:00",
        "last_modified_date": "2014-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1953",
        "title": "Automatic Extraction of Protein Interaction in Literature",
        "authors": [
            "Peilei Liu",
            "Ting Wang"
        ],
        "abstract": "Protein-protein interaction extraction is the key precondition of the construction of protein knowledge network, and it is very important for the research in the biomedicine. This paper extracted directional protein-protein interaction from the biological text, using the SVM-based method. Experiments were evaluated on the LLL05 corpus with good results. The results show that dependency features are import for the protein-protein interaction extraction and features related to the interaction word are effective for the interaction direction judgment. At last, we analyzed the effects of different features and planed for the next step.\n    ",
        "submission_date": "2014-06-08T00:00:00",
        "last_modified_date": "2014-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2035",
        "title": "Learning Word Representations with Hierarchical Sparse Coding",
        "authors": [
            "Dani Yogatama",
            "Manaal Faruqui",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks---word similarity ranking, analogies, sentence completion, and sentiment analysis---demonstrate that the method outperforms or is competitive with state-of-the-art methods. Our word representations are available at \\url{",
        "submission_date": "2014-06-08T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2204",
        "title": "How Easy is it to Learn a Controlled Natural Language for Building a Knowledge Base?",
        "authors": [
            "Sandra Williams",
            "Richard Power",
            "Allan Third"
        ],
        "abstract": "Recent developments in controlled natural language editors for knowledge engineering (KE) have given rise to expectations that they will make KE tasks more accessible and perhaps even enable non-engineers to build knowledge bases. This exploratory research focussed on novices and experts in knowledge engineering during their attempts to learn a controlled natural language (CNL) known as OWL Simplified English and use it to build a small knowledge base. Participants' behaviours during the task were observed through eye-tracking and screen recordings. This was an attempt at a more ambitious user study than in previous research because we used a naturally occurring text as the source of domain knowledge, and left them without guidance on which information to select, or how to encode it. We have identified a number of skills (competencies) required for this difficult task and key problems that authors face.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2400",
        "title": "Controlled Natural Language Generation from a Multilingual FrameNet-based Grammar",
        "authors": [
            "Dana Dann\u00e9lls",
            "Normunds Gr\u016bz\u012btis"
        ],
        "abstract": "This paper presents a currently bilingual but potentially multilingual FrameNet-based grammar library implemented in Grammatical Framework. The contribution of this paper is two-fold. First, it offers a methodological approach to automatically generate the grammar based on semantico-syntactic valence patterns extracted from FrameNet-annotated corpora. Second, it provides a proof of concept for two use cases illustrating how the acquired multilingual grammar can be exploited in different CNL applications in the domains of arts and tourism.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2538",
        "title": "FrameNet CNL: a Knowledge Representation and Information Extraction Language",
        "authors": [
            "Guntis Barzdins"
        ],
        "abstract": "The paper presents a FrameNet-based information extraction and knowledge representation framework, called FrameNet-CNL. The framework is used on natural language documents and represents the extracted knowledge in a tailor-made Frame-ontology from which unambiguous FrameNet-CNL paraphrase text can be generated automatically in multiple languages. This approach brings together the fields of information extraction and CNL, because a source text can be considered belonging to FrameNet-CNL, if information extraction parser produces the correct knowledge representation as a result. We describe a state-of-the-art information extraction parser used by a national news agency and speculate that FrameNet-CNL eventually could shape the natural language subset used for writing the newswire articles.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2903",
        "title": "A Brief State of the Art for Ontology Authoring",
        "authors": [
            "Hazem Safwat",
            "Brian Davis"
        ],
        "abstract": "One of the main challenges for building the Semantic web is Ontology Authoring. Controlled Natural Languages CNLs offer a user friendly means for non-experts to author ontologies. This paper provides a snapshot of the state-of-the-art for the core CNLs for ontology authoring and reviews their respective evaluations.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3287",
        "title": "A Clustering Analysis of Tweet Length and its Relation to Sentiment",
        "authors": [
            "Matthew Mayo"
        ],
        "abstract": "Sentiment analysis of Twitter data is performed. The researcher has made the following contributions via this paper: (1) an innovative method for deriving sentiment score dictionaries using an existing sentiment dictionary as seed words is explored, and (2) an analysis of clustered tweet sentiment scores based on tweet length is performed.\n    ",
        "submission_date": "2014-06-12T00:00:00",
        "last_modified_date": "2015-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3460",
        "title": "Are Style Guides Controlled Languages? The Case of Koenig & Bauer AG",
        "authors": [
            "Karolina Suchowolec"
        ],
        "abstract": "Controlled natural languages for industrial application are often regarded as a response to the challenges of translation and multilingual communication. This paper presents a quite different approach taken by Koenig & Bauer AG, where the main goal was the improvement of the authoring process for technical documentation. Most importantly, this paper explores the notion of a controlled language and demonstrates how style guides can emerge from non-linguistic considerations. Moreover, it shows the transition from loose language recommendations into precise and prescriptive rules and investigates whether such rules can be regarded as a full-fledged controlled language.\n    ",
        "submission_date": "2014-06-13T00:00:00",
        "last_modified_date": "2014-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3676",
        "title": "Question Answering with Subgraph Embeddings",
        "authors": [
            "Antoine Bordes",
            "Sumit Chopra",
            "Jason Weston"
        ],
        "abstract": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a competitive benchmark of the literature.\n    ",
        "submission_date": "2014-06-14T00:00:00",
        "last_modified_date": "2014-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3714",
        "title": "Mining of product reviews at aspect level",
        "authors": [
            "Richa Sharma",
            "Shweta Nigam",
            "Rekha Jain"
        ],
        "abstract": "Todays world is a world of Internet, almost all work can be done with the help of it, from simple mobile phone recharge to biggest business deals can be done with the help of this technology. People spent their most of the times on surfing on the Web it becomes a new source of entertainment, education, communication, shopping etc. Users not only use these websites but also give their feedback and suggestions that will be useful for other users. In this way a large amount of reviews of users are collected on the Web that needs to be explored, analyse and organized for better decision making. Opinion Mining or Sentiment Analysis is a Natural Language Processing and Information Extraction task that identifies the users views or opinions explained in the form of positive, negative or neutral comments and quotes underlying the text. Aspect based opinion mining is one of the level of Opinion mining that determines the aspect of the given reviews and classify the review for each feature. In this paper an aspect based opinion mining system is proposed to classify the reviews as positive, negative and neutral for each feature. Negation is also handled in the proposed system. Experimental results using reviews of products show the effectiveness of the system.\n    ",
        "submission_date": "2014-06-14T00:00:00",
        "last_modified_date": "2014-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3830",
        "title": "Modelling, Visualising and Summarising Documents with a Single Convolutional Neural Network",
        "authors": [
            "Misha Denil",
            "Alban Demiraj",
            "Nal Kalchbrenner",
            "Phil Blunsom",
            "Nando de Freitas"
        ],
        "abstract": "Capturing the compositional process which maps the meaning of words to that of documents is a central challenge for researchers in Natural Language Processing and Information Retrieval. We introduce a model that is able to represent the meaning of documents by embedding them in a low dimensional vector space, while preserving distinctions of word and sentence order crucial for capturing nuanced semantics. Our model is based on an extended Dynamic Convolution Neural Network, which learns convolution filters at both the sentence and document level, hierarchically learning to capture and compose low level lexical features into high level semantic concepts. We demonstrate the effectiveness of this model on a range of document modelling tasks, achieving strong results with no feature engineering and with a more compact model. Inspired by recent advances in visualising deep convolution networks for computer vision, we present a novel visualisation technique for our document networks which not only provides insight into their learning process, but also can be interpreted to produce a compelling automatic summarisation system for texts.\n    ",
        "submission_date": "2014-06-15T00:00:00",
        "last_modified_date": "2014-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3969",
        "title": "Translation Of Telugu-Marathi and Vice-Versa using Rule Based Machine Translation",
        "authors": [
            "Siddhartha Ghosh",
            "Sujata Thamke",
            "Kalyani U.R.S"
        ],
        "abstract": "In todays digital world automated Machine Translation of one language to another has covered a long way to achieve different kinds of success stories. Whereas Babel Fish supports a good number of foreign languages and only Hindi from Indian languages, the Google Translator takes care of about 10 Indian languages. Though most of the Automated Machine Translation Systems are doing well but handling Indian languages needs a major care while handling the local proverbs/ idioms. Most of the Machine Translation system follows the direct translation approach while translating one Indian language to other. Our research at KMIT R&D Lab found that handling the local proverbs/idioms is not given enough attention by the earlier research work. This paper focuses on two of the majorly spoken Indian languages Marathi and Telugu, and translation between them. Handling proverbs and idioms of both the languages have been given a special care, and the research outcome shows a significant achievement in this direction.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3976",
        "title": "Handling non-compositionality in multilingual CNLs",
        "authors": [
            "Ramona Enache",
            "Inari Listenmaa",
            "Prasanth Kolachina"
        ],
        "abstract": "In this paper, we describe methods for handling multilingual non-compositional constructions in the framework of GF. We specifically look at methods to detect and extract non-compositional phrases from parallel texts and propose methods to handle such constructions in GF grammars. We expect that the methods to handle non-compositional constructions will enrich CNLs by providing more flexibility in the design of controlled languages. We look at two specific use cases of non-compositional constructions: a general-purpose method to detect and extract multilingual multiword expressions and a procedure to identify nominal compounds in German. We evaluate our procedure for multiword expressions by performing a qualitative analysis of the results. For the experiments on nominal compounds, we incorporate the detected compounds in a full SMT pipeline and evaluate the impact of our method in machine translation process.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3987",
        "title": "Towards an Error Correction Memory to Enhance Technical Texts Authoring in LELIE",
        "authors": [
            "Juyeon Kang",
            "Patrick Saint Dizier"
        ],
        "abstract": "In this paper, we investigate and experiment the notion of error correction memory applied to error correction in technical texts. The main purpose is to induce relatively generic correction patterns associated with more contextual correction recommendations, based on previously memorized and analyzed corrections. The notion of error correction memory is developed within the framework of the LELIE project and illustrated on the case of fuzzy lexical items, which is a major problem in technical texts.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4057",
        "title": "Embedded Controlled Languages",
        "authors": [
            "Aarne Ranta"
        ],
        "abstract": "Inspired by embedded programming languages, an embedded CNL (controlled natural language) is a proper fragment of an entire natural language (its host language), but it has a parser that recognizes the entire host language. This makes it possible to process out-of-CNL input and give useful feedback to users, instead of just reporting syntax errors. This extended abstract explains the main concepts of embedded CNL implementation in GF (Grammatical Framework), with examples from machine translation and some other ongoing work.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4211",
        "title": "Mapping the Economic Crisis: Some Preliminary Investigations",
        "authors": [
            "Pierre Bourreau",
            "Thierry Poibeau"
        ],
        "abstract": "In this paper we describe our contribution to the PoliInformatics 2014 Challenge on the 2007-2008 financial crisis. We propose a state of the art technique to extract information from texts and provide different representations, giving first a static overview of the domain and then a dynamic representation of its main evolutions. We show that this strategy provides a practical solution to some recent theories in social sciences that are facing a lack of methods and tools to automatically extract information from natural language texts.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4469",
        "title": "Authorship Attribution through Function Word Adjacency Networks",
        "authors": [
            "Santiago Segarra",
            "Mark Eisen",
            "Alejandro Ribeiro"
        ],
        "abstract": "A method for authorship attribution based on function word adjacency networks (WANs) is introduced. Function words are parts of speech that express grammatical relationships between other words but do not carry lexical meaning on their own. In the WANs in this paper, nodes are function words and directed edges stand in for the likelihood of finding the sink word in the ordered vicinity of the source word. WANs of different authors can be interpreted as transition probabilities of a Markov chain and are therefore compared in terms of their relative entropies. Optimal selection of WAN parameters is studied and attribution accuracy is benchmarked across a diverse pool of authors and varying text lengths. This analysis shows that, since function words are independent of content, their use tends to be specific to an author and that the relational data captured by function WANs is a good summary of stylometric fingerprints. Attribution accuracy is observed to exceed the one achieved by methods that rely on word frequencies alone. Further combining WANs with methods that rely on word frequencies alone, results in larger attribution accuracy, indicating that both sources of information encode different aspects of authorial styles.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4690",
        "title": "The Frobenius anatomy of word meanings II: possessive relative pronouns",
        "authors": [
            "Mehrnoosh Sadrzadeh",
            "Stephen Clark",
            "Bob Coecke"
        ],
        "abstract": "Within the categorical compositional distributional model of meaning, we provide semantic interpretations for the subject and object roles of the possessive relative pronoun `whose'. This is done in terms of Frobenius algebras over compact closed categories. These algebras and their diagrammatic language expose how meanings of words in relative clauses interact with each other. We show how our interpretation is related to Montague-style semantics and provide a truth-theoretic interpretation. We also show how vector spaces provide a concrete interpretation and provide preliminary corpus-based experimental evidence. In a prequel to this paper, we used similar methods and dealt with the case of subject and object relative pronouns.\n    ",
        "submission_date": "2014-06-18T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4710",
        "title": "Typed Hilbert Epsilon Operators and the Semantics of Determiner Phrases (Invited Lecture)",
        "authors": [
            "Christian Retor\u00e9"
        ],
        "abstract": "The semantics of determiner phrases, be they definite de- scriptions, indefinite descriptions or quantified noun phrases, is often as- sumed to be a fully solved question: common nouns are properties, and determiners are generalised quantifiers that apply to two predicates: the property corresponding to the common noun and the one corresponding to the verb phrase. We first present a criticism of this standard view. Firstly, the semantics of determiners does not follow the syntactical structure of the sentence. Secondly the standard interpretation of the indefinite article cannot ac- count for nominal sentences. Thirdly, the standard view misses the linguis- tic asymmetry between the two properties of a generalised quantifier. In the sequel, we propose a treatment of determiners and quantifiers as Hilbert terms in a richly typed system that we initially developed for lexical semantics, using a many sorted logic for semantical representations. We present this semantical framework called the Montagovian generative lexicon and show how these terms better match the syntactical structure and avoid the aforementioned problems of the standard approach. Hilbert terms rather differ from choice functions in that there is one polymorphic operator and not one operator per formula. They also open an intriguing connection between the logic for meaning assembly, the typed lambda calculus handling compositionality and the many-sorted logic for semantical representations. Furthermore epsilon terms naturally introduce type-judgements and confirm the claim that type judgment are a form of presupposition.\n    ",
        "submission_date": "2014-06-18T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4824",
        "title": "What is India speaking: The \"Hinglish\" invasion",
        "authors": [
            "Rana D. Parshad",
            "Vineeta Chand",
            "Neha Sinha",
            "Nitu Kumari"
        ],
        "abstract": "While language competition models of diachronic language shift are increasingly sophisticated, drawing on sociolinguistic components like variable language prestige, distance from language centers and intermediate bilingual transitionary populations, in one significant way they fall short. They fail to consider contact-based outcomes resulting in mixed language practices, e.g. outcome scenarios such as creoles or unmarked code switching as an emergent communicative norm. On these lines something very interesting is uncovered in India, where traditionally there have been monolingual Hindi speakers and Hindi/English bilinguals, but virtually no monolingual English speakers. While the Indian census data reports a sharp increase in the proportion of Hindi/English bilinguals, we argue that the number of Hindi/English bilinguals in India is inaccurate, given a new class of urban individuals speaking a mixed lect of Hindi and English, popularly known as \"Hinglish\". Based on predator-prey, sociolinguistic theories, salient local ecological factors and the rural-urban divide in India, we propose a new mathematical model of interacting monolingual Hindi speakers, Hindi/English bilinguals and Hinglish speakers. The model yields globally asymptotic stable states of coexistence, as well as bilingual extinction. To validate our model, sociolinguistic data from different Indian classes are contrasted with census reports: We see that purported urban Hindi/English bilinguals are unable to maintain fluent Hindi speech and instead produce Hinglish, whereas rural speakers evidence monolingual Hindi. Thus we present evidence for the first time where an unrecognized mixed lect involving English but not \"English\", has possibly taken over a sizeable faction of a large global population.\n    ",
        "submission_date": "2014-06-12T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5181",
        "title": "Zipf's law holds for phrases, not words",
        "authors": [
            "Jake Ryland Williams",
            "Paul R. Lessard",
            "Suma Desu",
            "Eric Clark",
            "James P. Bagrow",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "With Zipf's law being originally and most famously observed for word frequency, it is surprisingly limited in its applicability to human language, holding over no more than three to four orders of magnitude before hitting a clear break in scaling. Here, building on the simple observation that phrases of one or more words comprise the most coherent units of meaning in language, we show empirically that Zipf's law for phrases extends over as many as nine orders of rank magnitude. In doing so, we develop a principled and scalable statistical mechanical method of random text partitioning, which opens up a rich frontier of rigorous text analysis via a rank ordering of mixed length phrases.\n    ",
        "submission_date": "2014-06-19T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5598",
        "title": "A survey on phrase structure learning methods for text classification",
        "authors": [
            "Reshma Prasad",
            "Mary Priya Sebastian"
        ],
        "abstract": "Text classification is a task of automatic classification of text into one of the predefined categories. The problem of text classification has been widely studied in different communities like natural language processing, data mining and information retrieval. Text classification is an important constituent in many information management tasks like topic identification, spam filtering, email routing, language identification, genre classification, readability assessment etc. The performance of text classification improves notably when phrase patterns are used. The use of phrase patterns helps in capturing non-local behaviours and thus helps in the improvement of text classification task. Phrase structure extraction is the first step to continue with the phrase pattern identification. In this survey, detailed study of phrase structure learning methods have been carried out. This will enable future work in several NLP tasks, which uses syntactic information from phrase structure like grammar checkers, question answering, information extraction, machine translation, text classification. The paper also provides different levels of classification and detailed comparison of the phrase structure learning methods.\n    ",
        "submission_date": "2014-06-21T00:00:00",
        "last_modified_date": "2014-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5691",
        "title": "A CNL for Contract-Oriented Diagrams",
        "authors": [
            "John J. Camilleri",
            "Gabriele Paganelli",
            "Gerardo Schneider"
        ],
        "abstract": "We present a first step towards a framework for defining and manipulating normative documents or contracts described as Contract-Oriented (C-O) Diagrams. These diagrams provide a visual representation for such texts, giving the possibility to express a signatory's obligations, permissions and prohibitions, with or without timing constraints, as well as the penalties resulting from the non-fulfilment of a contract. This work presents a CNL for verbalising C-O Diagrams, a web-based tool allowing editing in this CNL, and another for visualising and manipulating the diagrams interactively. We then show how these proof-of-concept tools can be used by applying them to a small example.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2014-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6101",
        "title": "Improved Frame Level Features and SVM Supervectors Approach for the Recogniton of Emotional States from Speech: Application to categorical and dimensional states",
        "authors": [
            "Imen Trabelsi",
            "Dorra Ben Ayed",
            "Noureddine Ellouze"
        ],
        "abstract": "The purpose of speech emotion recognition system is to classify speakers utterances into different emotional states such as disgust, boredom, sadness, neutral and happiness. Speech features that are commonly used in speech emotion recognition rely on global utterance level prosodic features. In our work, we evaluate the impact of frame level feature extraction. The speech samples are from Berlin emotional database and the features extracted from these utterances are energy, different variant of mel frequency cepstrum coefficients, velocity and acceleration features.\n    ",
        "submission_date": "2014-06-23T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6312",
        "title": "Scalable Topical Phrase Mining from Text Corpora",
        "authors": [
            "Ahmed El-Kishky",
            "Yanglei Song",
            "Chi Wang",
            "Clare Voss",
            "Jiawei Han"
        ],
        "abstract": "While most topic modeling algorithms model text corpora with unigrams, human interpretation often relies on inherent grouping of terms into phrases. As such, we consider the problem of discovering topical phrases of mixed lengths. Existing work either performs post processing to the inference results of unigram-based topic models, or utilizes complex n-gram-discovery topic models. These methods generally produce low-quality topical phrases or suffer from poor scalability on even moderately-sized datasets. We propose a different approach that is both computationally efficient and effective. Our solution combines a novel phrase mining framework to segment a document into single and multi-word phrases, and a new topic model that operates on the induced document partition. Our approach discovers high quality topical phrases with negligible extra cost to the bag-of-words topic model in a variety of datasets including research publication titles, abstracts, reviews, and news articles.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6844",
        "title": "FrameNet Resource Grammar Library for GF",
        "authors": [
            "Normunds Gruzitis",
            "Peteris Paikens",
            "Guntis Barzdins"
        ],
        "abstract": "In this paper we present an ongoing research investigating the possibility and potential of integrating frame semantics, particularly FrameNet, in the Grammatical Framework (GF) application grammar development. An important component of GF is its Resource Grammar Library (RGL) that encapsulates the low-level linguistic knowledge about morphology and syntax of currently more than 20 languages facilitating rapid development of multilingual applications. In the ideal case, porting a GF application grammar to a new language would only require introducing the domain lexicon - translation equivalents that are interlinked via common abstract terms. While it is possible for a highly restricted CNL, developing and porting a less restricted CNL requires above average linguistic knowledge about the particular language, and above average GF experience. Specifying a lexicon is mostly straightforward in the case of nouns (incl. multi-word units), however, verbs are the most complex category (in terms of both inflectional paradigms and argument structure), and adding them to a GF application grammar is not a straightforward task. In this paper we are focusing on verbs, investigating the possibility of creating a multilingual FrameNet-based GF library. We propose an extension to the current RGL, allowing GF application developers to define clauses on the semantic level, thus leaving the language-specific syntactic mapping to this extension. We demonstrate our approach by reengineering the MOLTO Phrasebook application grammar.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7314",
        "title": "On the Use of Different Feature Extraction Methods for Linear and Non Linear kernels",
        "authors": [
            "Imen Trabelsi",
            "Dorra Ben Ayed"
        ],
        "abstract": "The speech feature extraction has been a key focus in robust speech recognition research; it significantly affects the recognition performance. In this paper, we first study a set of different features extraction methods such as linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC) and perceptual linear prediction (PLP) with several features normalization techniques like rasta filtering and cepstral mean subtraction (CMS). Based on this, a comparative evaluation of these features is performed on the task of text independent speaker identification using a combination between gaussian mixture models (GMM) and linear and non-linear kernels based on support vector machine (SVM).\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7483",
        "title": "Jabalin: a Comprehensive Computational Model of Modern Standard Arabic Verbal Morphology Based on Traditional Arabic Prosody",
        "authors": [
            "Alicia Gonzalez Martinez",
            "Susana Lopez Hervas",
            "Doaa Samy",
            "Carlos G. Arques",
            "Antonio Moreno Sandoval"
        ],
        "abstract": "The computational handling of Modern Standard Arabic is a challenge in the field of natural language processing due to its highly rich morphology. However, several authors have pointed out that the Arabic morphological system is in fact extremely regular. The existing Arabic morphological analyzers have exploited this regularity to variable extent, yet we believe there is still some scope for improvement. Taking inspiration in traditional Arabic prosody, we have designed and implemented a compact and simple morphological system which in our opinion takes further advantage of the regularities encountered in the Arabic morphological system. The output of the system is a large-scale lexicon of inflected forms that has subsequently been used to create an Online Interface for a morphological analyzer of Arabic verbs. The Jabalin Online Interface is available at ",
        "submission_date": "2014-06-29T00:00:00",
        "last_modified_date": "2014-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7806",
        "title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition",
        "authors": [
            "Andrew L. Maas",
            "Peng Qi",
            "Ziang Xie",
            "Awni Y. Hannun",
            "Christopher T. Lengerich",
            "Daniel Jurafsky",
            "Andrew Y. Ng"
        ],
        "abstract": "Deep neural networks (DNNs) are now a central component of nearly all state-of-the-art speech recognition systems. Building neural network acoustic models requires several design decisions including network architecture, size, and training loss function. This paper offers an empirical investigation on which aspects of DNN acoustic model design are most important for speech recognition system performance. We report DNN classifier performance and final speech recognizer word error rates, and compare DNNs using several metrics to quantify factors influencing differences in task performance. Our first set of experiments use the standard Switchboard benchmark corpus, which contains approximately 300 hours of conversational telephone speech. We compare standard DNNs to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. We additionally build systems on a corpus of 2,100 hours of training data by combining the Switchboard and Fisher corpora. This larger corpus allows us to more thoroughly examine performance of large DNN models -- with up to ten times more parameters than those typically used in speech recognition systems. Our results suggest that a relatively simple DNN architecture and optimization technique produces strong results. These findings, along with previous work, help establish a set of best practices for building DNN hybrid speech recognition systems with maximum likelihood training. Our experiments in DNN optimization additionally serve as a case study for training DNNs with discriminative loss functions for speech tasks, as well as DNN classifiers more generally.\n    ",
        "submission_date": "2014-06-30T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1605",
        "title": "Les noms propres se traduisent-ils ? \u00c9tude d'un corpus multilingue",
        "authors": [
            "\u00c9meline Lecuit",
            "Denis Maurel",
            "Dusko Vitas"
        ],
        "abstract": "In this paper, we tackle the problem of the translation of proper names. We introduce our hypothesis according to which proper names can be translated more often than most people seem to think. Then, we describe the construction of a parallel multilingual corpus used to illustrate our point. We eventually evaluate both the advantages and limits of this corpus in our study.\n    ",
        "submission_date": "2014-07-07T00:00:00",
        "last_modified_date": "2014-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1640",
        "title": "WordRep: A Benchmark for Research on Learning Word Representations",
        "authors": [
            "Bin Gao",
            "Jiang Bian",
            "Tie-Yan Liu"
        ],
        "abstract": "WordRep is a benchmark collection for the research on learning distributed word representations (or word embeddings), released by Microsoft Research. In this paper, we describe the details of the WordRep collection and show how to use it in different types of machine learning research related to word embedding. Specifically, we describe how the evaluation tasks in WordRep are selected, how the data are sampled, and how the evaluation tool is built. We then compare several state-of-the-art word representations on WordRep, report their evaluation performance, and make discussions on the results. After that, we discuss new potential research topics that can be supported by WordRep, in addition to algorithm comparison. We hope that this paper can help people gain deeper understanding of WordRep, and enable more interesting research on learning distributed word representations and related topics.\n    ",
        "submission_date": "2014-07-07T00:00:00",
        "last_modified_date": "2014-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1687",
        "title": "KNET: A General Framework for Learning Word Embedding using Morphological Knowledge",
        "authors": [
            "Qing Cui",
            "Bin Gao",
            "Jiang Bian",
            "Siyu Qiu",
            "Tie-Yan Liu"
        ],
        "abstract": "Neural network techniques are widely applied to obtain high-quality distributed representations of words, i.e., word embeddings, to address text mining, information retrieval, and natural language processing tasks. Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words. However, it is challenging to handle unseen words or rare words with insufficient context. In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both contextual information and morphological word similarity built based on morphological knowledge to learn word embeddings. Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.\n    ",
        "submission_date": "2014-07-07T00:00:00",
        "last_modified_date": "2014-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1933",
        "title": "Lexpresso: a Controlled Natural Language",
        "authors": [
            "Adam Saulwick"
        ],
        "abstract": "This paper presents an overview of `Lexpresso', a Controlled Natural Language developed at the Defence Science & Technology Organisation as a bidirectional natural language interface to a high-level information fusion system. The paper describes Lexpresso's main features including lexical coverage, expressiveness and range of linguistic syntactic and semantic structures. It also touches on its tight integration with a formal semantic formalism and tentatively classifies it against the PENS system.\n    ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2014-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1976",
        "title": "Inter-Rater Agreement Study on Readability Assessment in Bengali",
        "authors": [
            "Shanta Phani",
            "Shibamouli Lahiri",
            "Arindam Biswas"
        ],
        "abstract": "An inter-rater agreement study is performed for readability assessment in Bengali. A 1-7 rating scale was used to indicate different levels of readability. We obtained moderate to fair agreement among seven independent annotators on 30 text passages written by four eminent Bengali authors. As a by product of our study, we obtained a readability-annotated ground truth dataset in Bengali. .\n    ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2014-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2019",
        "title": "Assamese-English Bilingual Machine Translation",
        "authors": [
            "Kalyanee Kanchan Baruah",
            "Pranjal Das",
            "Abdul Hannan",
            "Shikhar Kr. Sarma"
        ],
        "abstract": "Machine translation is the process of translating text from one language to another. In this paper, Statistical Machine Translation is done on Assamese and English language by taking their respective parallel corpus. A statistical phrase based translation toolkit Moses is used here. To develop the language model and to align the words we used two another tools IRSTLM, GIZA respectively. BLEU score is used to check our translation system performance, how good it is. A difference in BLEU scores is obtained while translating sentences from Assamese to English and vice-versa. Since Indian languages are morphologically very rich hence translation is relatively harder from English to Assamese resulting in a low BLEU score. A statistical transliteration system is also introduced with our translation system to deal basically with proper nouns, OOV (out of vocabulary) words which are not present in our corpus.\n    ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2014-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2694",
        "title": "Quality Estimation Of Machine Translation Outputs Through Stemming",
        "authors": [
            "Pooja Gupta",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Machine Translation is the challenging problem for Indian languages. Every day we can see some machine translators being developed, but getting a high quality automatic translation is still a very distant dream . The correct translated sentence for Hindi language is rarely found. In this paper, we are emphasizing on English-Hindi language pair, so in order to preserve the correct MT output we present a ranking system, which employs some machine learning techniques and morphological features. In ranking no human intervention is required. We have also validated our results by comparing it with human ranking.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2918",
        "title": "A Survey of Named Entity Recognition in Assamese and other Indian Languages",
        "authors": [
            "Gitimoni Talukdar",
            "Pranjal Protim Borah",
            "Arup Baruah"
        ],
        "abstract": "Named Entity Recognition is always important when dealing with major Natural Language Processing tasks such as information extraction, question-answering, machine translation, document summarization etc so in this paper we put forward a survey of Named Entities in Indian Languages with particular reference to Assamese. There are various rule-based and machine learning approaches available for Named Entity Recognition. At the very first of the paper we give an idea of the available approaches for Named Entity Recognition and then we discuss about the related research in this field. Assamese like other Indian languages is agglutinative and suffers from lack of appropriate resources as Named Entity Recognition requires large data sets, gazetteer list, dictionary etc and some useful feature like capitalization as found in English cannot be found in Assamese. Apart from this we also describe some of the issues faced in Assamese while doing Named Entity Recognition.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2989",
        "title": "Hidden Markov Model Based Part of Speech Tagger for Sinhala Language",
        "authors": [
            "A.J.P.M.P. Jayaweera",
            "N.G.J. Dias"
        ],
        "abstract": "In this paper we present a fundamental lexical semantics of Sinhala language and a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala language. In any Natural Language processing task, Part of Speech is a very vital topic, which involves analysing of the construction, behaviour and the dynamics of the language, which the knowledge could utilized in computational linguistics analysis and automation applications. Though Sinhala is a morphologically rich and agglutinative language, in which words are inflected with various grammatical features, tagging is very essential for further analysis of the language. Our research is based on statistical based approach, in which the tagging process is done by computing the tag sequence probability and the word-likelihood probability from the given corpus, where the linguistic knowledge is automatically extracted from the annotated corpus. The current tagger could reach more than 90% of accuracy for known words.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3636",
        "title": "Toward Network-based Keyword Extraction from Multitopic Web Documents",
        "authors": [
            "Sabina \u0160i\u0161ovi\u0107",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107",
            "Ana Me\u0161trovi\u0107"
        ],
        "abstract": "In this paper we analyse the selectivity measure calculated from the complex network in the task of the automatic keyword extraction. Texts, collected from different web sources (portals, forums), are represented as directed and weighted co-occurrence complex networks of words. Words are nodes and links are established between two nodes if they are directly co-occurring within the sentence. We test different centrality measures for ranking nodes - keyword candidates. The promising results are achieved using the selectivity measure. Then we propose an approach which enables extracting word pairs according to the values of the in/out selectivity and weight measures combined with filtering.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3751",
        "title": "Benchmarking Named Entity Disambiguation approaches for Streaming Graphs",
        "authors": [
            "Sutanay Choudhury",
            "Chase Dowling"
        ],
        "abstract": "Named Entity Disambiaguation (NED) is a central task for applications dealing with natural language text. Assume that we have a graph based knowledge base (subsequently referred as Knowledge Graph) where nodes represent various real world entities such as people, location, organization and concepts. Given data sources such as social media streams and web pages Entity Linking is the task of mapping named entities that are extracted from the data to those present in the Knowledge Graph. This is an inherently difficult task due to several reasons. Almost all these data sources are generated without any formal ontology; the unstructured nature of the input, limited context and the ambiguity involved when multiple entities are mapped to the same name make this a hard task. This report looks at two state of the art systems employing two distinctive approaches: graph based Accurate Online Disambiguation of Entities (AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs a statistical inference approach. We compare both approaches using the data set and queries provided by the Knowledge Base Population (KBP) track at 2011 NIST Text Analytics Conference (TAC). This report begins with an overview of the respective approaches, followed by detailed description of the experimental setup. It concludes with our findings from the benchmarking exercise.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4723",
        "title": "Toward Selectivity Based Keyword Extraction for Croatian News",
        "authors": [
            "Slobodan Beliga",
            "Ana Me\u0161trovi\u0107",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107"
        ],
        "abstract": "Preliminary report on network based keyword extraction for Croatian is an unsupervised method for keyword extraction from the complex network. We build our approach with a new network measure the node selectivity, motivated by the research of the graph based centrality approaches. The node selectivity is defined as the average weight distribution on the links of the single node. We extract nodes (keyword candidates) based on the selectivity value. Furthermore, we expand extracted nodes to word-tuples ranked with the highest in/out selectivity values. Selectivity based extraction does not require linguistic knowledge while it is purely derived from statistical and structural information en-compassed in the source text which is reflected into the structure of the network. Obtained sets are evaluated on a manually annotated keywords: for the set of extracted keyword candidates average F1 score is 24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates average F1 score is 25,9% and average F2 score is 24,47%.\n    ",
        "submission_date": "2014-07-17T00:00:00",
        "last_modified_date": "2014-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6027",
        "title": "Modeling languages from graph networks",
        "authors": [
            "Alberto Besana",
            "Cristina Mart\u00ednez"
        ],
        "abstract": "We model and compute the probability distribution of the letters in random generated words in a language by using the theory of set partitions, Young tableaux and graph theoretical representation methods. This has been of interest for several application areas such as network systems, bioinformatics, internet search, data mining and computacional linguistics.\n    ",
        "submission_date": "2014-07-22T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6099",
        "title": "Autonomous requirements specification processing using natural language processing",
        "authors": [
            "S.G. Macdonell",
            "K. Min",
            "A.M. Connor"
        ],
        "abstract": "We describe our ongoing research that centres on the application of natural language processing (NLP) to software engineering and systems development activities. In particular, this paper addresses the use of NLP in the requirements analysis and systems design processes. We have developed a prototype toolset that can assist the systems analyst or software engineer to select and verify terms relevant to a project. In this paper we describe the processes employed by the system to extract and classify objects of interest from requirements documents. These processes are illustrated using a small example.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6853",
        "title": "Substitute Based SCODE Word Embeddings in Supervised NLP Tasks",
        "authors": [
            "Volkan Cirik",
            "Deniz Yuret"
        ],
        "abstract": "We analyze a word embedding method in supervised tasks. It maps words on a sphere such that words co-occurring in similar contexts lie closely. The similarity of contexts is measured by the distribution of substitutes that can fill them. We compared word embeddings, including more recent representations, in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine our framework in multilingual dependency parsing as well. The results show that the proposed method achieves as good as or better results compared to the other word embeddings in the tasks we investigate. It achieves state-of-the-art results in multilingual dependency parsing. Word embeddings in 7 languages are available for public use.\n    ",
        "submission_date": "2014-07-25T00:00:00",
        "last_modified_date": "2014-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6872",
        "title": "Interpretable Low-Rank Document Representations with Label-Dependent Sparsity Patterns",
        "authors": [
            "Ivan Ivek"
        ],
        "abstract": "In context of document classification, where in a corpus of documents their label tags are readily known, an opportunity lies in utilizing label information to learn document representation spaces with better discriminative properties. To this end, in this paper application of a Variational Bayesian Supervised Nonnegative Matrix Factorization (supervised vbNMF) with label-driven sparsity structure of coefficients is proposed for learning of discriminative nonsubtractive latent semantic components occuring in TF-IDF document representations. Constraints are such that the components pursued are made to be frequently occuring in a small set of labels only, making it possible to yield document representations with distinctive label-specific sparse activation patterns. A simple measure of quality of this kind of sparsity structure, dubbed inter-label sparsity, is introduced and experimentally brought into tight connection with classification performance. Representing a great practical convenience, inter-label sparsity is shown to be easily controlled in supervised vbNMF by a single parameter.\n    ",
        "submission_date": "2014-07-25T00:00:00",
        "last_modified_date": "2014-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7169",
        "title": "Principles and Parameters: a coding theory perspective",
        "authors": [
            "Matilde Marcolli"
        ],
        "abstract": "We propose an approach to Longobardi's parametric comparison method (PCM) via the theory of error-correcting codes. One associates to a collection of languages to be analyzed with the PCM a binary (or ternary) code with one code words for each language in the family and each word consisting of the binary values of the syntactic parameters of the language, with the ternary case allowing for an additional parameter state that takes into account phenomena of entailment of parameters. The code parameters of the resulting code can be compared with some classical bounds in coding theory: the asymptotic bound, the Gilbert-Varshamov bound, etc. The position of the code parameters with respect to some of these bounds provides quantitative information on the variability of syntactic parameters within and across historical-linguistic families. While computations carried out for languages belonging to the same family yield codes below the GV curve, comparisons across different historical families can give examples of isolated codes lying above the asymptotic bound.\n    ",
        "submission_date": "2014-07-26T00:00:00",
        "last_modified_date": "2014-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8215",
        "title": "Two-pass Discourse Segmentation with Pairing and Global Features",
        "authors": [
            "Vanessa Wei Feng",
            "Graeme Hirst"
        ],
        "abstract": "Previous attempts at RST-style discourse segmentation typically adopt features centered on a single token to predict whether to insert a boundary before that token. In contrast, we develop a discourse segmenter utilizing a set of pairing features, which are centered on a pair of adjacent tokens in the sentence, by equally taking into account the information from both tokens. Moreover, we propose a novel set of global features, which encode characteristics of the segmentation as a whole, once we have an initial segmentation. We show that both the pairing and global features are useful on their own, and their combination achieved an $F_1$ of 92.6% of identifying in-sentence discourse boundaries, which is a 17.8% error-rate reduction over the state-of-the-art performance, approaching 95% of human performance. In addition, similar improvement is observed across different classification frameworks.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0016",
        "title": "Architecture of a Web-based Predictive Editor for Controlled Natural Language Processing",
        "authors": [
            "Stephen Guy",
            "Rolf Schwitter"
        ],
        "abstract": "In this paper, we describe the architecture of a web-based predictive text editor being developed for the controlled natural language PENG$^{ASP)$. This controlled language can be used to write non-monotonic specifications that have the same expressive power as Answer Set Programs. In order to support the writing process of these specifications, the predictive text editor communicates asynchronously with the controlled natural language processor that generates lookahead categories and additional auxiliary information for the author of a specification text. The text editor can display multiple sets of lookahead categories simultaneously for different possible sentence completions, anaphoric expressions, and supports the addition of new content words to the lexicon.\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0782",
        "title": "Targetable Named Entity Recognition in Social Media",
        "authors": [
            "Sandeep Ashwini",
            "Jinho D. Choi"
        ],
        "abstract": "We present a novel approach for recognizing what we call targetable named entities; that is, named entities in a targeted set (e.g, movies, books, TV shows). Unlike many other NER systems that need to retrain their statistical models as new entities arrive, our approach does not require such retraining, which makes it more adaptable for types of entities that are frequently updated. For this preliminary study, we focus on one entity type, movie title, using data collected from Twitter. Our system is tested on two evaluation sets, one including only entities corresponding to movies in our training set, and the other excluding any of those entities. Our final model shows F1-scores of 76.19% and 78.70% on these evaluation sets, which gives strong evidence that our approach is completely unbiased to any par- ticular set of entities found during training.\n    ",
        "submission_date": "2014-08-04T00:00:00",
        "last_modified_date": "2014-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1031",
        "title": "Text to Multi-level MindMaps: A Novel Method for Hierarchical Visual Abstraction of Natural Language Text",
        "authors": [
            "Mohamed Elhoseiny",
            "Ahmed Elgammal"
        ],
        "abstract": "MindMapping is a well-known technique used in note taking, which encourages learning and studying. MindMapping has been manually adopted to help present knowledge and concepts in a visual form. Unfortunately, there is no reliable automated approach to generate MindMaps from Natural Language text. This work firstly introduces MindMap Multilevel Visualization concept which is to jointly visualize and summarize textual information. The visualization is achieved pictorially across multiple levels using semantic information (i.e. ontology), while the summarization is achieved by the information in the highest levels as they represent abstract information in the text. This work also presents the first automated approach that takes a text input and generates a MindMap visualization out of it. The approach could visualize text documents in multilevel MindMaps, in which a high-level MindMap node could be expanded into child MindMaps. \\ignore{ As far as we know, this is the first work that view MindMapping as a new approach to jointly summarize and visualize textual information.} The proposed method involves understanding of the input text and converting it into intermediate Detailed Meaning Representation (DMR). The DMR is then visualized with two modes; Single level or Multiple levels, which is convenient for larger text. The generated MindMaps from both approaches were evaluated based on Human Subject experiments performed on Amazon Mechanical Turk with various parameter settings.\n    ",
        "submission_date": "2014-08-01T00:00:00",
        "last_modified_date": "2014-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1774",
        "title": "Beyond description. Comment on \"Approaching human language with complex networks\" by Cong & Liu",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Comment on \"Approaching human language with complex networks\" by Cong & Liu\n    ",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2014-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1928",
        "title": "Microtask crowdsourcing for disease mention annotation in PubMed abstracts",
        "authors": [
            "Benjamin M Good",
            "Max Nanis",
            "Andrew I. Su"
        ],
        "abstract": "Identifying concepts and relationships in biomedical text enables knowledge to be applied in computational analyses. Many biological natural language process (BioNLP) projects attempt to address this challenge, but the state of the art in BioNLP still leaves much room for improvement. Progress in BioNLP research depends on large, annotated corpora for evaluating information extraction systems and training machine learning models. Traditionally, such corpora are created by small numbers of expert annotators often working over extended periods of time. Recent studies have shown that workers on microtask crowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in aggregate, generate high-quality annotations of biomedical text. Here, we investigated the use of the AMT in capturing disease mentions in PubMed abstracts. We used the NCBI Disease corpus as a gold standard for refining and benchmarking our crowdsourcing protocol. After several iterations, we arrived at a protocol that reproduced the annotations of the 593 documents in the training set of this gold standard with an overall F measure of 0.872 (precision 0.862, recall 0.883). The output can also be tuned to optimize for precision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when precision = 0.436). Each document was examined by 15 workers, and their annotations were merged based on a simple voting method. In total 145 workers combined to complete all 593 documents in the span of 1 week at a cost of $.06 per abstract per worker. The quality of the annotations, as judged with the F measure, increases with the number of workers assigned to each task such that the system can be tuned to balance cost against quality. These results demonstrate that microtask crowdsourcing can be a valuable tool for generating well-annotated corpora in BioNLP.\n    ",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2014-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1985",
        "title": "A model of grassroots changes in linguistic systems",
        "authors": [
            "Janet B. Pierrehumbert",
            "Forrest Stonedahl",
            "Robert Daland"
        ],
        "abstract": "Linguistic norms emerge in human communities because people imitate each other. A shared linguistic system provides people with the benefits of shared knowledge and coordinated planning. Once norms are in place, why would they ever change? This question, echoing broad questions in the theory of social dynamics, has particular force in relation to language. By definition, an innovator is in the minority when the innovation first occurs. In some areas of social dynamics, important minorities can strongly influence the majority through their power, fame, or use of broadcast media. But most linguistic changes are grassroots developments that originate with ordinary people. Here, we develop a novel model of communicative behavior in communities, and identify a mechanism for arbitrary innovations by ordinary people to have a good chance of being widely adopted.\n",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2014-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2359",
        "title": "Gap-weighted subsequences for automatic cognate identification and phylogenetic inference",
        "authors": [
            "Taraka Rama"
        ],
        "abstract": "In this paper, we describe the problem of cognate identification and its relation to phylogenetic inference. We introduce subsequence based features for discriminating cognates from non-cognates. We show that subsequence based features perform better than the state-of-the-art string similarity measures for the purpose of cognate identification. We use the cognate judgments for the purpose of phylogenetic inference and observe that these classifiers infer a tree which is close to the gold standard tree. The contribution of this paper is the use of subsequence features for cognate identification and to employ the cognate judgments for phylogenetic inference.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2014-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2466",
        "title": "Controlled Natural Language Processing as Answer Set Programming: an Experiment",
        "authors": [
            "Rolf Schwitter"
        ],
        "abstract": "Most controlled natural languages (CNLs) are processed with the help of a pipeline architecture that relies on different software components. We investigate in this paper in an experimental way how well answer set programming (ASP) is suited as a unifying framework for parsing a CNL, deriving a formal representation for the resulting syntax trees, and for reasoning with that representation. We start from a list of input tokens in ASP notation and show how this input can be transformed into a syntax tree using an ASP grammar and then into reified ASP rules in form of a set of facts. These facts are then processed by an ASP meta-interpreter that allows us to infer new knowledge.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2014-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2873",
        "title": "First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs",
        "authors": [
            "Awni Y. Hannun",
            "Andrew L. Maas",
            "Daniel Jurafsky",
            "Andrew Y. Ng"
        ],
        "abstract": "We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting transcript text from audio. This paper extends this approach in two ways. First, we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. Second, we propose and evaluate a modified prefix-search decoding algorithm. This approach to decoding enables first-pass speech recognition with a language model, completely unaided by the cumbersome infrastructure of HMM-based systems. Experiments on the Wall Street Journal corpus demonstrate fairly competitive word error rates, and the importance of bi-directional network recurrence.\n    ",
        "submission_date": "2014-08-12T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3153",
        "title": "Detection is the central problem in real-word spelling correction",
        "authors": [
            "L. Amber Wilcox-O'Hearn"
        ],
        "abstract": "Real-word spelling correction differs from non-word spelling correction in its aims and its challenges. Here we show that the central problem in real-word spelling correction is detection. Methods from non-word spelling correction, which focus instead on selection among candidate corrections, do not address detection adequately, because detection is either assumed in advance or heavily constrained. As we demonstrate in this paper, merely discriminating between the intended word and a random close variation of it within the context of a sentence is a task that can be performed with high accuracy using straightforward models. Trigram models are sufficient in almost all cases. The difficulty comes when every word in the sentence is a potential error, with a large set of possible candidate corrections. Despite their strengths, trigram models cannot reliably find true errors without introducing many more, at least not when used in the obvious sequential way without added structure. The detection task exposes weakness not visible in the selection task.\n    ",
        "submission_date": "2014-08-13T00:00:00",
        "last_modified_date": "2014-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3456",
        "title": "SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation",
        "authors": [
            "Felix Hill",
            "Roi Reichart",
            "Anna Korhonen"
        ],
        "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness, so that pairs of entities that are associated but not actually similar [Freud, psychology] have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.\n    ",
        "submission_date": "2014-08-15T00:00:00",
        "last_modified_date": "2014-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3731",
        "title": "Unsupervised Keyword Extraction from Polish Legal Texts",
        "authors": [
            "Micha\u0142 Jungiewicz",
            "Micha\u0142 \u0141opuszy\u0144ski"
        ],
        "abstract": "In this work, we present an application of the recently proposed unsupervised keyword extraction algorithm RAKE to a corpus of Polish legal texts from the field of public procurement. RAKE is essentially a language and domain independent method. Its only language-specific input is a stoplist containing a set of non-content words. The performance of the method heavily depends on the choice of such a stoplist, which should be domain adopted. Therefore, we complement RAKE algorithm with an automatic approach to selecting non-content words, which is based on the statistical properties of term distribution.\n    ",
        "submission_date": "2014-08-16T00:00:00",
        "last_modified_date": "2014-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3934",
        "title": "On Detecting Messaging Abuse in Short Text Messages using Linguistic and Behavioral patterns",
        "authors": [
            "Alejandro Mosquera",
            "Lamine Aouad",
            "Slawomir Grzonkowski",
            "Dylan Morss"
        ],
        "abstract": "The use of short text messages in social media and instant messaging has become a popular communication channel during the last years. This rising popularity has caused an increment in messaging threats such as spam, phishing or malware as well as other threats. The processing of these short text message threats could pose additional challenges such as the presence of lexical variants, SMS-like contractions or advanced obfuscations which can degrade the performance of traditional filtering solutions. By using a real-world SMS data set from a large telecommunications operator from the US and a social media corpus, in this paper we analyze the effectiveness of machine learning filters based on linguistic and behavioral patterns in order to detect short text spam and abusive users in the network. We have also explored different ways to deal with short text message challenges such as tokenization and entity detection by using text normalization and substring clustering techniques. The obtained results show the validity of the proposed solution by enhancing baseline approaches.\n    ",
        "submission_date": "2014-08-18T00:00:00",
        "last_modified_date": "2014-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4753",
        "title": "Be Careful When Assuming the Obvious: Commentary on \"The placement of the head that minimizes online memory: a complex systems approach\"",
        "authors": [
            "Phillip M. Alday"
        ],
        "abstract": "Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronic and diachronic nature of word order based on the assumption that memory costs are a never decreasing function of distance and a few very general linguistic assumptions. However, even these minimal and seemingly obvious assumptions are not as safe as they appear in light of recent typological and psycholinguistic evidence. The interaction of word order and memory has further depths to be explored.\n    ",
        "submission_date": "2014-08-20T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5882",
        "title": "Convolutional Neural Networks for Sentence Classification",
        "authors": [
            "Yoon Kim"
        ],
        "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.\n    ",
        "submission_date": "2014-08-25T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6179",
        "title": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings",
        "authors": [
            "Dmitrijs Milajevs",
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh",
            "Matthew Purver"
        ],
        "abstract": "We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts, in a number of compositional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the larger-scale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks.\n    ",
        "submission_date": "2014-08-26T00:00:00",
        "last_modified_date": "2014-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6181",
        "title": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning",
        "authors": [
            "Dimitri Kartsaklis",
            "Nal Kalchbrenner",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "This paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition. In contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression. The results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is model-independent.\n    ",
        "submission_date": "2014-08-26T00:00:00",
        "last_modified_date": "2014-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6746",
        "title": "Non-Standard Words as Features for Text Categorization",
        "authors": [
            "Slobodan Beliga",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107"
        ],
        "abstract": "This paper presents categorization of Croatian texts using Non-Standard Words (NSW) as features. Non-Standard Words are: numbers, dates, acronyms, abbreviations, currency, etc. NSWs in Croatian language are determined according to Croatian NSW taxonomy. For the purpose of this research, 390 text documents were collected and formed the SKIPEZ collection with 6 classes: official, literary, informative, popular, educational and scientific. Text categorization experiment was conducted on three different representations of the SKIPEZ collection: in the first representation, the frequencies of NSWs are used as features; in the second representation, the statistic measures of NSWs (variance, coefficient of variation, standard deviation, etc.) are used as features; while the third representation combines the first two feature sets. Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms were used in text categorization experiments. The best categorization results are achieved using the first feature set (NSW frequencies) with the categorization accuracy of 87%. This suggests that the NSWs should be considered as features in highly inflectional languages, such as Croatian. NSW based features reduce the dimensionality of the feature space without standard lemmatization procedures, and therefore the bag-of-NSWs should be considered for further Croatian texts categorization experiments.\n    ",
        "submission_date": "2014-08-28T00:00:00",
        "last_modified_date": "2014-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6788",
        "title": "Strongly Incremental Repair Detection",
        "authors": [
            "Julian Hough",
            "Matthew Purver"
        ],
        "abstract": "We present STIR (STrongly Incremental Repair detection), a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency. STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs. Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead. We evaluate its performance using incremental metrics and propose new repair processing evaluation standards.\n    ",
        "submission_date": "2014-08-28T00:00:00",
        "last_modified_date": "2014-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0314",
        "title": "Empirical Evaluation of Tree distances for Parser Evaluation",
        "authors": [
            "Taraka Rama"
        ],
        "abstract": "In this empirical study, I compare various tree distance measures -- originally developed in computational biology for the purpose of tree comparison -- for the purpose of parser evaluation. I will control for the parser setting by comparing the automatically generated parse trees from the state-of-the-art parser Charniak, 2000) with the gold-standard parse trees. The article describes two different tree distance measures (RF and QD) along with its variants (GRF and GQD) for the purpose of parser evaluation. The article will argue that RF measure captures similar information as the standard EvalB metric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha, 1989) applied by Tsarfaty et al. (2011). Finally, the article also provides empirical evidence by reporting high correlations between the different tree distances and EvalB metric's scores.\n    ",
        "submission_date": "2014-09-01T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0473",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "authors": [
            "Dzmitry Bahdanau",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ],
        "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.\n    ",
        "submission_date": "2014-09-01T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1257",
        "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation",
        "authors": [
            "Jean Pouget-Abadie",
            "Dzmitry Bahdanau",
            "Bart van Merrienboer",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ],
        "abstract": "The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1259",
        "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
        "authors": [
            "Kyunghyun Cho",
            "Bart van Merrienboer",
            "Dzmitry Bahdanau",
            "Yoshua Bengio"
        ],
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1612",
        "title": "Semantic clustering of Russian web search results: possibilities and problems",
        "authors": [
            "Andrey Kutuzov"
        ],
        "abstract": "The paper deals with word sense induction from lexical co-occurrence graphs. We construct such graphs on large Russian corpora and then apply this data to cluster ",
        "submission_date": "2014-09-04T00:00:00",
        "last_modified_date": "2014-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2073",
        "title": "An NLP Assistant for Clide",
        "authors": [
            "Tobias Kortkamp"
        ],
        "abstract": "This report describes an NLP assistant for the collaborative development environment Clide, that supports the development of NLP applications by providing easy access to some common NLP data structures. The assistant visualizes text fragments and their dependencies by displaying the semantic graph of a sentence, the coreference chain of a paragraph and mined triples that are extracted from a paragraph's semantic graphs and linked using its coreference chain. Using this information and a logic programming library, we create an NLP database which is used by a series of queries to mine the triples. The algorithm is tested by translating a natural language text describing a graph to an actual graph that is shown as an annotation in the text editor.\n    ",
        "submission_date": "2014-09-07T00:00:00",
        "last_modified_date": "2014-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2195",
        "title": "Analyzing the Language of Food on Social Media",
        "authors": [
            "Daniel Fried",
            "Mihai Surdeanu",
            "Stephen Kobourov",
            "Melanie Hingle",
            "Dane Bell"
        ],
        "abstract": "We investigate the predictive power behind the language of food on social media. We collect a corpus of over three million food-related posts from Twitter and demonstrate that many latent population characteristics can be directly predicted from this data: overweight rate, diabetes rate, political leaning, and home geographical location of authors. For all tasks, our language-based models significantly outperform the majority-class baselines. Performance is further improved with more complex natural language processing, such as topic modeling. We analyze which textual features have most predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics. Lastly, we design and implement an online system for real-time query and visualization of the dataset. Visualization tools, such as geo-referenced heatmaps, semantics-preserving wordclouds and temporal histograms, allow us to discover more complex, global patterns mirrored in the language of food.\n    ",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2014-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2433",
        "title": "Approximating solution structure of the Weighted Sentence Alignment problem",
        "authors": [
            "Antonina Kolokolova",
            "Renesa Nizamee"
        ],
        "abstract": "We study the complexity of approximating solution structure of the bijective weighted sentence alignment problem of DeNero and Klein (2008). In particular, we consider the complexity of finding an alignment that has a significant overlap with an optimal alignment. We discuss ways of representing the solution for the general weighted sentence alignment as well as phrases-to-words alignment problem, and show that computing a string which agrees with the optimal sentence partition on more than half (plus an arbitrarily small polynomial fraction) positions for the phrases-to-words alignment is NP-hard. For the general weighted sentence alignment we obtain such bound from the agreement on a little over 2/3 of the bits. Additionally, we generalize the Hamming distance approximation of a solution structure to approximating it with respect to the edit distance metric, obtaining similar lower bounds.\n    ",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2014-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3005",
        "title": "A Study of Association Measures and their Combination for Arabic MWT Extraction",
        "authors": [
            "Abdelkader El Mahdaouy",
            "Sa\u00efd EL Alaoui Ouatik",
            "Eric Gaussier"
        ],
        "abstract": "Automatic Multi-Word Term (MWT) extraction is a very important issue to many applications, such as information retrieval, question answering, and text categorization. Although many methods have been used for MWT extraction in English and other European languages, few studies have been applied to Arabic. In this paper, we propose a novel, hybrid method which combines linguistic and statistical approaches for Arabic Multi-Word Term extraction. The main contribution of our method is to consider contextual information and both termhood and unithood for association measures at the statistical filtering step. In addition, our technique takes into account the problem of MWT variation in the linguistic filtering step. The performance of the proposed statistical measure (NLC-value) is evaluated using an Arabic environment corpus by comparing it with some existing competitors. Experimental results show that our NLC-value measure outperforms the other ones in term of precision for both bi-grams and tri-grams.\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2014-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3215",
        "title": "Sequence to Sequence Learning with Neural Networks",
        "authors": [
            "Ilya Sutskever",
            "Oriol Vinyals",
            "Quoc V. Le"
        ],
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2014-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3512",
        "title": "Word Sense Disambiguation using WSD specific Wordnet of Polysemy Words",
        "authors": [
            "Udaya Raj Dhungana",
            "Subarna Shakya",
            "Kabita Baral",
            "Bharat Sharma"
        ],
        "abstract": "This paper presents a new model of WordNet that is used to disambiguate the correct sense of polysemy word based on the clue words. The related words for each sense of a polysemy word as well as single sense word are referred to as the clue words. The conventional WordNet organizes nouns, verbs, adjectives and adverbs together into sets of synonyms called synsets each expressing a different concept. In contrast to the structure of WordNet, we developed a new model of WordNet that organizes the different senses of polysemy words as well as the single sense words based on the clue words. These clue words for each sense of a polysemy word as well as for single sense word are used to disambiguate the correct meaning of the polysemy word in the given context using knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word can be a noun, verb, adjective or adverb.\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2014-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3813",
        "title": "Incorporating Semi-supervised Features into Discontinuous Easy-First Constituent Parsing",
        "authors": [
            "Yannick Versley"
        ],
        "abstract": "This paper describes adaptations for EaFi, a parser for easy-first parsing of discontinuous constituents, to adapt it to multiple languages as well as make use of the unlabeled data that was provided as part of the SPMRL shared task 2014.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3870",
        "title": "Text mixing shapes the anatomy of rank-frequency distributions: A modern Zipfian mechanics for natural language",
        "authors": [
            "Jake Ryland Williams",
            "James P. Bagrow",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "Natural languages are full of rules and exceptions. One of the most famous quantitative rules is Zipf's law which states that the frequency of occurrence of a word is approximately inversely proportional to its rank. Though this `law' of ranks has been found to hold across disparate texts and forms of data, analyses of increasingly large corpora over the last 15 years have revealed the existence of two scaling regimes. These regimes have thus far been explained by a hypothesis suggesting a separability of languages into core and non-core lexica. Here, we present and defend an alternative hypothesis, that the two scaling regimes result from the act of aggregating texts. We observe that text mixing leads to an effective decay of word introduction, which we show provides accurate predictions of the location and severity of breaks in scaling. Upon examining large corpora from 10 languages in the Project Gutenberg eBooks collection (eBooks), we find emphatic empirical support for the universality of our claim.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3881",
        "title": "An Approach to Reducing Annotation Costs for BioNLP",
        "authors": [
            "Michael Bloodgood",
            "K. Vijay-Shanker"
        ],
        "abstract": "There is a broad range of BioNLP tasks for which active learning (AL) can significantly reduce annotation costs and a specific AL algorithm we have developed is particularly effective in reducing annotation costs for these tasks. We have previously developed an AL algorithm called ClosestInitPA that works best with tasks that have the following characteristics: redundancy in training material, burdensome annotation costs, Support Vector Machines (SVMs) work well for the task, and imbalanced datasets (i.e. when set up as a binary classification problem, one class is substantially rarer than the other). Many BioNLP tasks have these characteristics and thus our AL algorithm is a natural approach to apply to BioNLP tasks.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3942",
        "title": "Polarity detection movie reviews in hindi language",
        "authors": [
            "Richa Sharma",
            "Shweta Nigam",
            "Rekha Jain"
        ],
        "abstract": "Nowadays peoples are actively involved in giving comments and reviews on social networking websites and other websites like shopping websites, news websites etc. large number of people everyday share their opinion on the web, results is a large number of user data is collected .users also find it trivial task to read all the reviews and then reached into the decision. It would be better if these reviews are classified into some category so that the user finds it easier to read. Opinion Mining or Sentiment Analysis is a natural language processing task that mines information from various text forms such as reviews, news, and blogs and classify them on the basis of their polarity as positive, negative or neutral. But, from the last few years, user content in Hindi language is also increasing at a rapid rate on the Web. So it is very important to perform opinion mining in Hindi language as well. In this paper a Hindi language opinion mining system is proposed. The system classifies the reviews as positive, negative and neutral for Hindi language. Negation is also handled in the proposed system. Experimental results using reviews of movies show the effectiveness of the system\n    ",
        "submission_date": "2014-09-13T00:00:00",
        "last_modified_date": "2014-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4169",
        "title": "An Algorithm Based on Empirical Methods, for Text-to-Tuneful-Speech Synthesis of Sanskrit Verse",
        "authors": [
            "Rama N.",
            "Meenakshi Lakshmanan"
        ],
        "abstract": "The rendering of Sanskrit poetry from text to speech is a problem that has not been solved before. One reason may be the complications in the language itself. We present unique algorithms based on extensive empirical analysis, to synthesize speech from a given text input of Sanskrit verses. Using a pre-recorded audio units database which is itself tremendously reduced in size compared to the colossal size that would otherwise be required, the algorithms work on producing the best possible, tunefully rendered chanting of the given verse. His would enable the visually impaired and those with reading disabilities to easily access the contents of Sanskrit verses otherwise available only in writing.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4354",
        "title": "A Binary Schema and Computational Algorithms to Process Vowel-based Euphonic Conjunctions for Word Searches",
        "authors": [
            "S. V. Kasmir Raja",
            "V. Rajitha",
            "Meenakshi Lakshmanan"
        ],
        "abstract": "Comprehensively searching for words in Sanskrit E-text is a non-trivial problem because words could change their forms in different contexts. One such context is sandhi or euphonic conjunctions, which cause a word to change owing to the presence of adjacent letters or words. The change wrought by these possible conjunctions can be so significant in Sanskrit that a simple search for the word in its given form alone can significantly reduce the success level of the search. This work presents a representational schema that represents letters in a binary format and reduces Paninian rules of euphonic conjunctions to simple bit set-unset operations. The work presents an efficient algorithm to process vowel-based sandhis using this schema. It further presents another algorithm that uses the sandhi processor to generate the possible transformed word forms of a given word to use in a comprehensive word search.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4364",
        "title": "Computational Algorithms Based on the Paninian System to Process Euphonic Conjunctions for Word Searches",
        "authors": [
            "S. V. Kasmir Raja",
            "V. Rajitha",
            "Meenakshi Lakshmanan"
        ],
        "abstract": "Searching for words in Sanskrit E-text is a problem that is accompanied by complexities introduced by features of Sanskrit such as euphonic conjunctions or sandhis. A word could occur in an E-text in a transformed form owing to the operation of rules of sandhi. Simple word search would not yield these transformed forms of the word. Further, there is no search engine in the literature that can comprehensively search for words in Sanskrit E-texts taking euphonic conjunctions into account. This work presents an optimal binary representational schema for letters of the Sanskrit alphabet along with algorithms to efficiently process the sandhi rules of Sanskrit grammar. The work further presents an algorithm that uses the sandhi processing algorithm to perform a comprehensive word search on E-text.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4504",
        "title": "Voting for Deceptive Opinion Spam Detection",
        "authors": [
            "Tao Wang",
            "Hua Zhu"
        ],
        "abstract": "Consumers' purchase decisions are increasingly influenced by user-generated online reviews. Accordingly, there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic, to deceive the readers. Existing approaches mainly focus on developing automatic supervised learning based methods to help users identify deceptive opinion spams.\n",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2014-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4614",
        "title": "Lexical Normalisation of Twitter Data",
        "authors": [
            "Bilal Ahmed"
        ],
        "abstract": "Twitter with over 500 million users globally, generates over 100,000 tweets per minute . The 140 character limit per tweet, perhaps unintentionally, encourages users to use shorthand notations and to strip spellings to their bare minimum \"syllables\" or elisions e.g. \"srsly\". The analysis of twitter messages which typically contain misspellings, elisions, and grammatical errors, poses a challenge to established Natural Language Processing (NLP) tools which are generally designed with the assumption that the data conforms to the basic grammatical structure commonly used in English language. In order to make sense of Twitter messages it is necessary to first transform them into a canonical form, consistent with the dictionary or grammar. This process, performed at the level of individual tokens (\"words\"), is called lexical normalisation. This paper investigates various techniques for lexical normalisation of Twitter data and presents the findings as the techniques are applied to process raw data from Twitter.\n    ",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4714",
        "title": "Modeling the average shortest path length in growth of word-adjacency networks",
        "authors": [
            "Andrzej Kulig",
            "Stanislaw Drozdz",
            "Jaroslaw Kwapien",
            "Pawel Oswiecimka"
        ],
        "abstract": "We investigate properties of evolving linguistic networks defined by the word-adjacency relation. Such networks belong to the category of networks with accelerated growth but their shortest path length appears to reveal the network size dependence of different functional form than the ones known so far. We thus compare the networks created from literary texts with their artificial substitutes based on different variants of the Dorogovtsev-Mendes model and observe that none of them is able to properly simulate the novel asymptotics of the shortest path length. Then, we identify the local chain-like linear growth induced by grammar and style as a missing element in this model and extend it by incorporating such effects. It is in this way that a satisfactory agreement with the empirical result is obtained.\n    ",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2015-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5502",
        "title": "Using crowdsourcing system for creating site-specific statistical machine translation engine",
        "authors": [
            "Alexander Kalinin",
            "George Savchenko"
        ],
        "abstract": "A crowdsourcing translation approach is an effective tool for globalization of site content, but it is also an important source of parallel linguistic data. For the given site, processed with a crowdsourcing system, a sentence-aligned corpus can be fetched, which covers a very narrow domain of terminology and language patterns - a site-specific domain. These data can be used for training and estimation of site-specific statistical machine translation engine\n    ",
        "submission_date": "2014-09-19T00:00:00",
        "last_modified_date": "2014-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7085",
        "title": "Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach",
        "authors": [
            "Kathryn Baker",
            "Michael Bloodgood",
            "Chris Callison-Burch",
            "Bonnie J. Dorr",
            "Nathaniel W. Filardo",
            "Lori Levin",
            "Scott Miller",
            "Christine Piatko"
        ],
        "abstract": "We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality---and further demonstrates that large gains can be achieved for low-resource languages with different word order than English.\n    ",
        "submission_date": "2014-09-24T00:00:00",
        "last_modified_date": "2014-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7275",
        "title": "The meaning-frequency law in Zipfian optimization models of communication",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "According to Zipf's meaning-frequency law, words that are more frequent tend to have more meanings. Here it is shown that a linear dependency between the frequency of a form and its number of meanings is found in a family of models of Zipf's law for word frequencies. This is evidence for a weak version of the meaning-frequency law. Interestingly, that weak law (a) is not an inevitable of property of the assumptions of the family and (b) is found at least in the narrow regime where those models exhibit Zipf's law for word frequencies.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7386",
        "title": "Performance of Stanford and Minipar Parser on Biomedical Texts",
        "authors": [
            "Rushdi Shams"
        ],
        "abstract": "In this paper, the performance of two dependency parsers, namely Stanford and Minipar, on biomedical texts has been reported. The performance of te parsers to assignm dependencies between two biomedical concepts that are already proved to be connected is not satisfying. Both Stanford and Minipar, being statistical parsers, fail to assign dependency relation between two connected concepts if they are distant by at least one clause. Minipar's performance, in terms of precision, recall and the F-score of the attachment score (e.g., correctly identified head in a dependency), to parse biomedical text is also measured taking the Stanford's as a gold standard. The results suggest that Minipar is not suitable yet to parse biomedical texts. In addition, a qualitative investigation reveals that the difference between working principles of the parsers also play a vital role for Minipar's degraded performance.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7591",
        "title": "Topic Similarity Networks: Visual Analytics for Large Document Sets",
        "authors": [
            "Arun S. Maiya",
            "Robert M. Rolfe"
        ],
        "abstract": "We investigate ways in which to improve the interpretability of LDA topic models by better analyzing and visualizing their outputs. We focus on examining what we refer to as topic similarity networks: graphs in which nodes represent latent topics in text collections and links represent similarity among topics. We describe efficient and effective approaches to both building and labeling such networks. Visualizations of topic models based on these networks are shown to be a powerful means of exploring, characterizing, and summarizing large collections of unstructured text documents. They help to \"tease out\" non-obvious connections among different sets of documents and provide insights into how topics form larger themes. We demonstrate the efficacy and practicality of these approaches through two case studies: 1) NSF grants for basic research spanning a 14 year period and 2) the entire English portion of Wikipedia.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2014-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7612",
        "title": "Semi-supervised Classification for Natural Language Processing",
        "authors": [
            "Rushdi Shams"
        ],
        "abstract": "Semi-supervised classification is an interesting idea where classification models are learned from both labeled and unlabeled data. It has several advantages over supervised classification in natural language processing domain. For instance, supervised classification exploits only labeled data that are expensive, often difficult to get, inadequate in quantity, and require human experts for annotation. On the other hand, unlabeled data are inexpensive and abundant. Despite the fact that many factors limit the wide-spread use of semi-supervised classification, it has become popular since its level of performance is empirically as good as supervised classification. This study explores the possibilities and achievements as well as complexity and limitations of semi-supervised classification for several natural langue processing tasks like parsing, biomedical information processing, text classification, and summarization.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7619",
        "title": "Generating Conceptual Metaphors from Proposition Stores",
        "authors": [
            "Ekaterina Ovchinnikova",
            "Vladimir Zaytsev",
            "Suzanne Wertheim",
            "Ross Israel"
        ],
        "abstract": "Contemporary research on computational processing of linguistic metaphors is divided into two main branches: metaphor recognition and metaphor interpretation. We take a different line of research and present an automated method for generating conceptual metaphors from linguistic data. Given the generated conceptual metaphors, we find corresponding linguistic metaphors in corpora. In this paper, we describe our approach and its evaluation using English and Russian data.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7985",
        "title": "The Utility of Text: The Case of Amicus Briefs and the Supreme Court",
        "authors": [
            "Yanchuan Sim",
            "Bryan Routledge",
            "Noah A. Smith"
        ],
        "abstract": "We explore the idea that authoring a piece of text is an act of maximizing one's expected utility. To make this idea concrete, we consider the societally important decisions of the Supreme Court of the United States. Extensive past work in quantitative political science provides a framework for empirically modeling the decisions of justices and how they relate to text. We incorporate into such a model texts authored by amici curiae (\"friends of the court\" separate from the litigants) who seek to weigh in on the decision, then explicitly model their goals in a random utility model. We demonstrate the benefits of this approach in improved vote prediction and the ability to perform counterfactual analysis.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8008",
        "title": "CRF-based Named Entity Recognition @ICON 2013",
        "authors": [
            "Arjun Das",
            "Utpal Garain"
        ],
        "abstract": "This paper describes performance of CRF based systems for Named Entity Recognition (NER) in Indian language as a part of ICON 2013 shared task. In this task we have considered a set of language independent features for all the languages. Only for English a language specific feature, i.e. capitalization, has been added. Next the use of gazetteer is explored for Bengali, Hindi and English. The gazetteers are built from Wikipedia and other sources. Test results show that the system achieves the highest F measure of 88% for English and the lowest F measure of 69% for both Tamil and Telugu. Note that for the least performing two languages no gazetteer was used. NER in Bengali and Hindi finds accuracy (F measure) of 87% and 79%, respectively.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2014-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8558",
        "title": "A Deep Learning Approach to Data-driven Parameterizations for Statistical Parametric Speech Synthesis",
        "authors": [
            "Prasanna Kumar Muthukumar",
            "Alan W. Black"
        ],
        "abstract": "Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral coefficients as the vocal tract parameterization of the speech signal. Mel Cepstral coefficients were never intended to work in a parametric speech synthesis framework, but as yet, there has been little success in creating a better parameterization that is more suited to synthesis. In this paper, we use deep learning algorithms to investigate a data-driven parameterization technique that is designed for the specific requirements of synthesis. We create an invertible, low-dimensional, noise-robust encoding of the Mel Log Spectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is then unwrapped and used as the initialization for a Multi-Layer Perceptron (MLP). The MLP is fine-tuned by training it to reconstruct the input at the output layer. This MLP is then split down the middle to form encoding and decoding networks. These networks produce a parameterization of the Mel Log Spectrum that is intended to better fulfill the requirements of synthesis. Results are reported for experiments conducted using this resulting parameterization with the ClusterGen speech synthesizer.\n    ",
        "submission_date": "2014-09-30T00:00:00",
        "last_modified_date": "2014-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8581",
        "title": "Improving the Performance of English-Tamil Statistical Machine Translation System using Source-Side Pre-Processing",
        "authors": [
            "M. Anand Kumar",
            "V. Dhanalakshmi",
            "K. P. Soman",
            "V. Sharmiladevi"
        ],
        "abstract": "Machine Translation is one of the major oldest and the most active research area in Natural Language Processing. Currently, Statistical Machine Translation (SMT) dominates the Machine Translation research. Statistical Machine Translation is an approach to Machine Translation which uses models to learn translation patterns directly from data, and generalize them to translate a new unseen text. The SMT approach is largely language independent, i.e. the models can be applied to any language pair. Statistical Machine Translation (SMT) attempts to generate translations using statistical methods based on bilingual text corpora. Where such corpora are available, excellent results can be attained translating similar texts, but such corpora are still not available for many language pairs. Statistical Machine Translation systems, in general, have difficulty in handling the morphology on the source or the target side especially for morphologically rich languages. Errors in morphology or syntax in the target language can have severe consequences on meaning of the sentence. They change the grammatical function of words or the understanding of the sentence through the incorrect tense information in verb. Baseline SMT also known as Phrase Based Statistical Machine Translation (PBSMT) system does not use any linguistic information and it only operates on surface word form. Recent researches shown that adding linguistic information helps to improve the accuracy of the translation with less amount of bilingual corpora. Adding linguistic information can be done using the Factored Statistical Machine Translation system through pre-processing steps. This paper investigates about how English side pre-processing is used to improve the accuracy of English-Tamil SMT system.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2014-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0286",
        "title": "LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible",
        "authors": [
            "Dirk Roorda",
            "Gino Kalkman",
            "Martijn Naaijer",
            "Andreas van Cranenburgh"
        ],
        "abstract": "The Linguistic Annotation Framework (LAF) provides a general, extensible stand-off markup system for corpora. This paper discusses LAF-Fabric, a new tool to analyse LAF resources in general with an extension to process the Hebrew Bible in particular. We first walk through the history of the Hebrew Bible as text database in decennium-wide steps. Then we describe how LAF-Fabric may serve as an analysis tool for this corpus. Finally, we describe three analytic projects/workflows that benefit from the new LAF representation:\n",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0291",
        "title": "A Morphological Analyzer for Japanese Nouns, Verbs and Adjectives",
        "authors": [
            "Yanchuan Sim"
        ],
        "abstract": "We present an open source morphological analyzer for Japanese nouns, verbs and adjectives. The system builds upon the morphological analyzing capabilities of MeCab to incorporate finer details of classification such as politeness, tense, mood and voice attributes. We implemented our analyzer in the form of a finite state transducer using the open source finite state compiler FOMA toolkit. The source code and tool is available at ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0718",
        "title": "Not All Neural Embeddings are Born Equal",
        "authors": [
            "Felix Hill",
            "KyungHyun Cho",
            "Sebastien Jean",
            "Coline Devin",
            "Yoshua Bengio"
        ],
        "abstract": "Neural language models learn word representations that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models. We show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and/or syntactic role. The findings suggest that, while monolingual models learn information about how concepts are related, neural-translation models better capture their true ontological status.\n    ",
        "submission_date": "2014-10-02T00:00:00",
        "last_modified_date": "2014-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1080",
        "title": "Generating abbreviations using Google Books library",
        "authors": [
            "Valery D. Solovyev",
            "Vladimir V. Bochkarev"
        ],
        "abstract": "The article describes the original method of creating a dictionary of abbreviations based on the Google Books Ngram Corpus. The dictionary of abbreviations is designed for Russian, yet as its methodology is universal it can be applied to any language. The dictionary can be used to define the function of the period during text segmentation in various applied systems of text processing. The article describes difficulties encountered in the process of its construction as well as the ways to overcome them. A model of evaluating a probability of first and second type errors (extraction accuracy and fullness) is constructed. Certain statistical data for the use of abbreviations are provided.\n    ",
        "submission_date": "2014-10-04T00:00:00",
        "last_modified_date": "2014-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1135",
        "title": "Corpora Preparation and Stopword List Generation for Arabic data in Social Network",
        "authors": [
            "Walaa Medhat",
            "Ahmed H. Yousef",
            "Hoda Korashy"
        ],
        "abstract": "This paper proposes a methodology to prepare corpora in Arabic language from online social network (OSN) and review site for Sentiment Analysis (SA) task. The paper also proposes a methodology for generating a stopword list from the prepared corpora. The aim of the paper is to investigate the effect of removing stopwords on the SA task. The problem is that the stopwords lists generated before were on Modern Standard Arabic (MSA) which is not the common language used in OSN. We have generated a stopword list of Egyptian dialect and a corpus-based list to be used with the OSN corpora. We compare the efficiency of text classification when using the generated lists along with previously generated lists of MSA and combining the Egyptian dialect list with the MSA list. The text classification was performed using Na\u00efve Bayes and Decision Tree classifiers and two feature selection approaches, unigrams and bigram. The experiments show that the general lists containing the Egyptian dialects words give better performance than using lists of MSA stopwords only.\n    ",
        "submission_date": "2014-10-05T00:00:00",
        "last_modified_date": "2014-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2045",
        "title": "Supervised learning Methods for Bangla Web Document Categorization",
        "authors": [
            "Ashis Kumar Mandal",
            "Rikta Sen"
        ],
        "abstract": "This paper explores the use of machine learning approaches, or more specifically, four supervised learning Methods, namely Decision Tree(C 4.5), K-Nearest Neighbour (KNN), Na\u00efve Bays (NB), and Support Vector Machine (SVM) for categorization of Bangla web documents. This is a task of automatically sorting a set of documents into categories from a predefined set. Whereas a wide range of methods have been applied to English text categorization, relatively few studies have been conducted on Bangla language text categorization. Hence, we attempt to analyze the efficiency of those four methods for categorization of Bangla documents. In order to validate, Bangla corpus from various websites has been developed and used as examples for the experiment. For Bangla, empirical results support that all four methods produce satisfactory performance with SVM attaining good result in terms of high dimensional and relatively noisy document feature vectors.\n    ",
        "submission_date": "2014-10-08T00:00:00",
        "last_modified_date": "2014-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2082",
        "title": "Contrastive Unsupervised Word Alignment with Non-Local Features",
        "authors": [
            "Yang Liu",
            "Maosong Sun"
        ],
        "abstract": "Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over state-of-the-art unsupervised word alignment methods.\n    ",
        "submission_date": "2014-10-08T00:00:00",
        "last_modified_date": "2014-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2149",
        "title": "Language-based Examples in the Statistics Classroom",
        "authors": [
            "Roger Bilisoly"
        ],
        "abstract": "Statistics pedagogy values using a variety of examples. Thanks to text resources on the Web, and since statistical packages have the ability to analyze string data, it is now easy to use language-based examples in a statistics class. Three such examples are discussed here. First, many types of wordplay (e.g., crosswords and hangman) involve finding words with letters that satisfy a certain pattern. Second, linguistics has shown that idiomatic pairs of words often appear together more frequently than chance. For example, in the Brown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.) Third, a pangram contains all the letters of the alphabet at least once. These are searched for in Charles Dickens' A Christmas Carol, and their lengths are compared to the expected value given by the unequal probability coupon collector's problem as well as simulations.\n    ",
        "submission_date": "2014-10-05T00:00:00",
        "last_modified_date": "2014-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2479",
        "title": "Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy and Reverberant Environments",
        "authors": [
            "Andreas Schwarz",
            "Christian Huemmer",
            "Roland Maas",
            "Walter Kellermann"
        ],
        "abstract": "We propose a spatial diffuseness feature for deep neural network (DNN)-based automatic speech recognition to improve recognition accuracy in reverberant and noisy environments. The feature is computed in real-time from multiple microphone signals without requiring knowledge or estimation of the direction of arrival, and represents the relative amount of diffuse noise in each time and frequency bin. It is shown that using the diffuseness feature as an additional input to a DNN-based acoustic model leads to a reduced word error rate for the REVERB challenge corpus, both compared to logmelspec features extracted from noisy signals, and features enhanced by spectral subtraction.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2646",
        "title": "Hybrid approaches for automatic vowelization of Arabic texts",
        "authors": [
            "Mohamed Bebah",
            "Chennoufi Amine",
            "Mazroui Azzeddine",
            "Lakhouaja Abdelhak"
        ],
        "abstract": "Hybrid approaches for automatic vowelization of Arabic texts are presented in this article. The process is made up of two modules. In the first one, a morphological analysis of the text words is performed using the open source morphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out of context, are its different possible vowelizations. The integration of this Analyzer in our vowelization system required the addition of a lexical database containing the most frequent words in Arabic language. Using a statistical approach based on two hidden Markov models (HMM), the second module aims to eliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic words are the observed states and the vowelized words are the hidden states. The observed states of the second HMM are identical to those of the first, but the hidden states are the lists of possible diacritics of the word without its Arabic letters. Our system uses Viterbi algorithm to select the optimal path among the solutions proposed by Al Khalil Morpho Sys. Our approach opens an important way to improve the performance of automatic vowelization of Arabic texts for other uses in automatic natural language processing.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2014-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2871",
        "title": "An Ontology for Comprehensive Tutoring of Euphonic Conjunctions of Sanskrit Grammar",
        "authors": [
            "S. V. Kasmir Raja",
            "V. Rajitha",
            "Meenakshi Lakshmanan"
        ],
        "abstract": "Euphonic conjunctions (sandhis) form a very important aspect of Sanskrit morphology and phonology. The traditional and modern methods of studying about euphonic conjunctions in Sanskrit follow different methodologies. The former involves a rigorous study of the Paninian system embodied in Panini's Ashtadhyayi, while the latter usually involves the study of a few important sandhi rules with the use of examples. The former is not suitable for beginners, and the latter, not sufficient to gain a comprehensive understanding of the operation of sandhi rules. This is so since there are not only numerous sandhi rules and exceptions, but also complex precedence rules involved. The need for a new ontology for sandhi-tutoring was hence felt. This work presents a comprehensive ontology designed to enable a student-user to learn in stages all about euphonic conjunctions and the relevant aphorisms of Sanskrit grammar and to test and evaluate the progress of the student-user. The ontology forms the basis of a multimedia sandhi tutor that was given to different categories of users including Sanskrit scholars for extensive and rigorous testing.\n    ",
        "submission_date": "2014-10-10T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3460",
        "title": "Sentiment Analysis based on User Tag for Traditional Chinese Medicine in Weibo",
        "authors": [
            "Junhui Shen",
            "Peiyan Zhu",
            "Rui Fan",
            "Wei Tan"
        ],
        "abstract": "With the acceptance of Western culture and science, Traditional Chinese Medicine (TCM) has become a controversial issue in China. So, it's important to study the public's sentiment and opinion on TCM. The rapid development of online social network, such as twitter, make it convenient and efficient to sample hundreds of millions of people for the aforementioned sentiment study. To the best of our knowledge, the present work is the first attempt that applies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-like microblogging service in China). In our work, firstly we collect tweets topic about TCM from Sina Weibo, and label the tweets as supporting TCM and opposing TCM automatically based on user tag. Then, a support vector machine classifier has been built to predict the sentiment of TCM tweets without labels. Finally, we present a method to adjust the classifier result. The performance of F-measure attained with our method is 97%.\n    ",
        "submission_date": "2014-10-13T00:00:00",
        "last_modified_date": "2014-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3791",
        "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition",
        "authors": [
            "Rami Al-Rfou",
            "Vivek Kulkarni",
            "Bryan Perozzi",
            "Steven Skiena"
        ],
        "abstract": "The increasing diversity of languages used on the web introduces a new level of complexity to Information Retrieval (IR) systems. We can no longer assume that textual content is written in one language or even the same language family. In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein - using only language agnostic techniques, while achieving competitive performance.\n",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4176",
        "title": "Learning Distributed Word Representations for Natural Logic Reasoning",
        "authors": [
            "Samuel R. Bowman",
            "Christopher Potts",
            "Christopher D. Manning"
        ],
        "abstract": "Natural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations, which have proven valuable in a wide range of sophisticated language tasks. However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical reasoning captured by natural logic. We address this question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks. Our experiments evaluate the models' ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph. The overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4281",
        "title": "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition",
        "authors": [
            "Xiangang Li",
            "Xihong Wu"
        ],
        "abstract": "Long short-term memory (LSTM) based acoustic modeling methods have recently been shown to give state-of-the-art performance on some speech recognition tasks. To achieve a further performance improvement, in this research, deep extensions on LSTM are investigated considering that deep hierarchical model has turned out to be more efficient than a shallow one. Motivated by previous research on constructing deep recurrent neural networks (RNNs), alternative deep LSTM architectures are proposed and empirically evaluated on a large vocabulary conversational telephone speech recognition task. Meanwhile, regarding to multi-GPU devices, the training process for LSTM networks is introduced and discussed. Experimental results demonstrate that the deep LSTM networks benefit from the depth and yield the state-of-the-art performance on this task.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2015-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4445",
        "title": "Patterns in the English Language: Phonological Networks, Percolation and Assembly Models",
        "authors": [
            "Massimo Stella",
            "Markus Brede"
        ],
        "abstract": "In this paper we provide a quantitative framework for the study of phonological networks (PNs) for the English language by carrying out principled comparisons to null models, either based on site percolation, randomization techniques, or network growth models. In contrast to previous work, we mainly focus on null models that reproduce lower order characteristics of the empirical data. We find that artificial networks matching connectivity properties of the English PN are exceedingly rare: this leads to the hypothesis that the word repertoire might have been assembled over time by preferentially introducing new words which are small modifications of old words. Our null models are able to explain the \"power-law-like\" part of the degree distributions and generally retrieve qualitative features of the PN such as high clustering, high assortativity coefficient, and small-world characteristics. However, the detailed comparison to expectations from null models also points out significant differences, suggesting the presence of additional constraints in word assembly. Key constraints we identify are the avoidance of large degrees, the avoidance of triadic closure, and the avoidance of large non-percolating clusters.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4639",
        "title": "Dependent Types for Pragmatics",
        "authors": [
            "Darryl McAdams",
            "Jonathan Sterling"
        ],
        "abstract": "This paper proposes the use of dependent types for pragmatic phenomena such as pronoun binding and presupposition resolution as a type-theoretic alternative to formalisms such as Discourse Representation Theory and Dynamic Semantics.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4863",
        "title": "Arabic Language Text Classification Using Dependency Syntax-Based Feature Selection",
        "authors": [
            "Yannis Haralambous",
            "Yassir Elidrissi",
            "Philippe Lenca"
        ],
        "abstract": "We study the performance of Arabic text classification combining various techniques: (a) tfidf vs. dependency syntax, for feature selection and weighting; (b) class association rules vs. support vector machines, for classification. The Arabic text is used in two forms: rootified and lightly stemmed. The results we obtain show that lightly stemmed text leads to better performance than rootified text; that class association rules are better suited for small feature sets obtained by dependency syntax constraints; and, finally, that support vector machines are better suited for large feature sets based on morphological feature selection criteria.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2014-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4868",
        "title": "A Modality Lexicon and its use in Automatic Tagging",
        "authors": [
            "Kathryn Baker",
            "Michael Bloodgood",
            "Bonnie J. Dorr",
            "Nathaniel W. Filardo",
            "Lori Levin",
            "Christine Piatko"
        ],
        "abstract": "This paper describes our resource-building results for an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation. Specifically, we describe the construction of a modality annotation scheme, a modality lexicon, and two automated modality taggers that were built using the lexicon and annotation scheme. Our annotation scheme is based on identifying three components of modality: a trigger, a target and a holder. We describe how our modality lexicon was produced semi-automatically, expanding from an initial hand-selected list of modality trigger words and phrases. The resulting expanded modality lexicon is being made publicly available. We demonstrate that one tagger---a structure-based tagger---results in precision around 86% (depending on genre) for tagging of a standard LDC data set. In a machine translation application, using the structure-based tagger to annotate English modalities on an English-Urdu training corpus improved the translation quality score for Urdu by 0.3 Bleu points in the face of sparse training data.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2014-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4966",
        "title": "The Visualization of Change in Word Meaning over Time using Temporal Word Embeddings",
        "authors": [
            "Chiraag Lala",
            "Shay B. Cohen"
        ],
        "abstract": "We describe a visualization tool that can be used to view the change in meaning of words over time. The tool makes use of existing (static) word embedding datasets together with a timestamped $n$-gram corpus to create {\\em temporal} word embeddings.\n    ",
        "submission_date": "2014-10-18T00:00:00",
        "last_modified_date": "2014-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5485",
        "title": "A stronger null hypothesis for crossing dependencies",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "The syntactic structure of a sentence can be modeled as a tree where vertices are words and edges indicate syntactic dependencies between words. It is well-known that those edges normally do not cross when drawn over the sentence. Here a new null hypothesis for the number of edge crossings of a sentence is presented. That null hypothesis takes into account the length of the pair of edges that may cross and predicts the relative number of crossings in random trees with a small error, suggesting that a ban of crossings or a principle of minimization of crossings are not needed in general to explain the origins of non-crossing dependencies. Our work paves the way for more powerful null hypotheses to investigate the origins of non-crossing dependencies in nature.\n    ",
        "submission_date": "2014-10-20T00:00:00",
        "last_modified_date": "2014-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5491",
        "title": "Using Mechanical Turk to Build Machine Translation Evaluation Sets",
        "authors": [
            "Michael Bloodgood",
            "Chris Callison-Burch"
        ],
        "abstract": "Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT test sets cheaply. We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield.\n    ",
        "submission_date": "2014-10-20T00:00:00",
        "last_modified_date": "2014-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5877",
        "title": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation",
        "authors": [
            "Michael Bloodgood",
            "Chris Callison-Burch"
        ],
        "abstract": "We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.\n    ",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2014-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6830",
        "title": "Clustering Words by Projection Entropy",
        "authors": [
            "I\u015f\u0131k Bar\u0131\u015f Fidaner",
            "Ali Taylan Cemgil"
        ],
        "abstract": "We apply entropy agglomeration (EA), a recently introduced algorithm, to cluster the words of a literary text. EA is a greedy agglomerative procedure that minimizes projection entropy (PE), a function that can quantify the segmentedness of an element set. To apply it, the text is reduced to a feature allocation, a combinatorial object to represent the word occurences in the text's paragraphs. The experiment results demonstrate that EA, despite its reduction and simplicity, is useful in capturing significant relationships among the words in the text. This procedure was implemented in Python and published as a free software: REBUS.\n    ",
        "submission_date": "2014-10-24T00:00:00",
        "last_modified_date": "2014-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7182",
        "title": "Analysis of Named Entity Recognition and Linking for Tweets",
        "authors": [
            "Leon Derczynski",
            "Diana Maynard",
            "Giuseppe Rizzo",
            "Marieke van Erp",
            "Genevieve Gorrell",
            "Rapha\u00ebl Troncy",
            "Johann Petrak",
            "Kalina Bontcheva"
        ],
        "abstract": "Applying natural language processing for mining and intelligent information access to tweets (a form of microblog) is a challenging, emerging research area. Unlike carefully authored news text and other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Information extraction from tweets is typically performed in a pipeline, comprising consecutive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). In this work, we describe a new Twitter entity disambiguation dataset, and conduct an empirical analysis of named entity recognition and disambiguation, investigating how robust a number of state-of-the-art systems are on such noisy texts, what the main sources of error are, and which problems should be further investigated to improve the state of the art.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7382",
        "title": "Modified Mel Filter Bank to Compute MFCC of Subsampled Speech",
        "authors": [
            "Kiran Kumar Bhuvanagiri",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used speech features in most speech and speaker recognition applications. In this work, we propose a modified Mel filter bank to extract MFCCs from subsampled speech. We also propose a stronger metric which effectively captures the correlation between MFCCs of original speech and MFCC of resampled speech. It is found that the proposed method of filter bank construction performs distinguishably well and gives recognition performance on resampled speech close to recognition accuracies on original speech.\n    ",
        "submission_date": "2014-10-25T00:00:00",
        "last_modified_date": "2014-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7787",
        "title": "Correcting Errors in Digital Lexicographic Resources Using a Dictionary Manipulation Language",
        "authors": [
            "David Zajic",
            "Michael Maxwell",
            "David Doermann",
            "Paul Rodrigues",
            "Michael Bloodgood"
        ],
        "abstract": "We describe a paradigm for combining manual and automatic error correction of noisy structured lexicographic data. Modifications to the structure and underlying text of the lexicographic data are expressed in a simple, interpreted programming language. Dictionary Manipulation Language (DML) commands identify nodes by unique identifiers, and manipulations are performed using simple commands such as create, move, set text, etc. Corrected lexicons are produced by applying sequences of DML commands to the source version of the lexicon. DML commands can be written manually to repair one-off errors or generated automatically to correct recurring problems. We discuss advantages of the paradigm for the task of editing digital bilingual dictionaries.\n    ",
        "submission_date": "2014-10-28T00:00:00",
        "last_modified_date": "2014-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8149",
        "title": "Detecting Structural Irregularity in Electronic Dictionaries Using Language Modeling",
        "authors": [
            "Paul Rodrigues",
            "David Zajic",
            "David Doermann",
            "Michael Bloodgood",
            "Peng Ye"
        ],
        "abstract": "Dictionaries are often developed using tools that save to Extensible Markup Language (XML)-based standards. These standards often allow high-level repeating elements to represent lexical entries, and utilize descendants of these repeating elements to represent the structure within each lexical entry, in the form of an XML tree. In many cases, dictionaries are published that have errors and inconsistencies that are expensive to find manually. This paper discusses a method for dictionary writers to quickly audit structural regularity across entries in a dictionary by using statistical language modeling. The approach learns the patterns of XML nodes that could occur within an XML tree, and then calculates the probability of each XML tree in the dictionary against these patterns to look for entries that diverge from the norm.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8206",
        "title": "Addressing the Rare Word Problem in Neural Machine Translation",
        "authors": [
            "Minh-Thang Luong",
            "Ilya Sutskever",
            "Quoc V. Le",
            "Oriol Vinyals",
            "Wojciech Zaremba"
        ],
        "abstract": "Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8498",
        "title": "Training for Fast Sequential Prediction Using Dynamic Feature Selection",
        "authors": [
            "Emma Strubell",
            "Luke Vilnis",
            "Andrew McCallum"
        ],
        "abstract": "We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. We present experiments in left-to-right part-of-speech tagging on WSJ, demonstrating that we can preserve accuracy above 97% with over a five-fold reduction in run-time.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8553",
        "title": "A random forest system combination approach for error detection in digital dictionaries",
        "authors": [
            "Michael Bloodgood",
            "Peng Ye",
            "Paul Rodrigues",
            "David Zajic",
            "David Doermann"
        ],
        "abstract": "When digitizing a print bilingual dictionary, whether via optical character recognition or manual entry, it is inevitable that errors are introduced into the electronic version that is created. We investigate automating the process of detecting errors in an XML representation of a digitized print dictionary using a hybrid approach that combines rule-based, feature-based, and language model-based methods. We investigate combining methods and show that using random forests is a promising approach. We find that in isolation, unsupervised methods rival the performance of supervised methods. Random forests typically require training data so we investigate how we can apply random forests to combine individual base methods that are themselves unsupervised without requiring large amounts of training data. Experiments reveal empirically that a relatively small amount of data is sufficient and can potentially be further reduced through specific selection criteria.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8581",
        "title": "Semi-Automatic Construction of a Domain Ontology for Wind Energy Using Wikipedia Articles",
        "authors": [
            "Dilek K\u00fc\u00e7\u00fck",
            "Yusuf Arslan"
        ],
        "abstract": "Domain ontologies are important information sources for knowledge-based systems. Yet, building domain ontologies from scratch is known to be a very labor-intensive process. In this study, we present our semi-automatic approach to building an ontology for the domain of wind energy which is an important type of renewable energy with a growing share in electricity generation all over the world. Related Wikipedia articles are first processed in an automated manner to determine the basic concepts of the domain together with their properties and next the concepts, properties, and relationships are organized to arrive at the ultimate ontology. We also provide pointers to other engineering ontologies which could be utilized together with the proposed wind energy ontology in addition to its prospective application areas. The current study is significant as, to the best of our knowledge, it proposes the first considerably wide-coverage ontology for the wind energy domain and the ontology is built through a semi-automatic process which makes use of the related Web resources, thereby reducing the overall cost of the ontology building process.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8668",
        "title": "Experiments to Improve Named Entity Recognition on Turkish Tweets",
        "authors": [
            "Dilek K\u00fc\u00e7\u00fck",
            "Ralf Steinberger"
        ],
        "abstract": "Social media texts are significant information sources for several application areas including trend analysis, event monitoring, and opinion mining. Unfortunately, existing solutions for tasks such as named entity recognition that perform well on formal texts usually perform poorly when applied to social media texts. In this paper, we report on experiments that have the purpose of improving named entity recognition on Turkish tweets, using two different annotated data sets. In these experiments, starting with a baseline named entity recognition system, we adapt its recognition rules and resources to better fit Twitter language by relaxing its capitalization constraint and by diacritics-based expansion of its lexical resources, and we employ a simplistic normalization scheme on tweets to observe the effects of these on the overall named entity recognition performance on Turkish tweets. The evaluation results of the system with these different settings are provided with discussions of these results.\n    ",
        "submission_date": "2014-10-31T00:00:00",
        "last_modified_date": "2014-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8783",
        "title": "Supervised learning model for parsing Arabic language",
        "authors": [
            "Nabil Khoufi",
            "Chafik Aloulou",
            "Lamia Hadrich Belguith"
        ],
        "abstract": "Parsing the Arabic language is a difficult task given the specificities of this language and given the scarcity of digital resources (grammars and annotated corpora). In this paper, we suggest a method for Arabic parsing based on supervised machine learning. We used the SVMs algorithm to select the syntactic labels of the sentence. Furthermore, we evaluated our parser following the cross validation method by using the Penn Arabic Treebank. The obtained results are very encouraging.\n    ",
        "submission_date": "2014-10-31T00:00:00",
        "last_modified_date": "2014-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0007",
        "title": "Rapid Adaptation of POS Tagging for Domain Specific Uses",
        "authors": [
            "John E. Miller",
            "Michael Bloodgood",
            "Manabu Torii",
            "K. Vijay-Shanker"
        ],
        "abstract": "Part-of-speech (POS) tagging is a fundamental component for performing natural language tasks such as parsing, information extraction, and question answering. When POS taggers are trained in one domain and applied in significantly different domains, their performance can degrade dramatically. We present a methodology for rapid adaptation of POS taggers to new domains. Our technique is unsupervised in that a manually annotated corpus for the new domain is not necessary. We use suffix information gathered from large amounts of raw text as well as orthographic information to increase the lexical coverage. We present an experiment in the Biological domain where our POS tagger achieves results comparable to POS taggers specifically trained to this domain.\n    ",
        "submission_date": "2014-10-31T00:00:00",
        "last_modified_date": "2014-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0129",
        "title": "The Latent Structure of Dictionaries",
        "authors": [
            "Philippe Vincent-Lamarre",
            "Alexandre Blondin Mass\u00e9",
            "Marcos Lopes",
            "M\u00e9lanie Lord",
            "Odile Marcotte",
            "Stevan Harnad"
        ],
        "abstract": "How many words (and which ones) are sufficient to define all other words? When dictionaries are analyzed as directed graphs with links from defining words to defined words, they reveal a latent structure. Recursively removing all words that are reachable by definition but that do not define any further words reduces the dictionary to a Kernel of about 10%. This is still not the smallest number of words that can define all the rest. About 75% of the Kernel turns out to be its Core, a Strongly Connected Subset of words with a definitional path to and from any pair of its words and no word's definition depending on a word outside the set. But the Core cannot define all the rest of the dictionary. The 25% of the Kernel surrounding the Core consists of small strongly connected subsets of words: the Satellites. The size of the smallest set of words that can define all the rest (the graph's Minimum Feedback Vertex Set or MinSet) is about 1% of the dictionary, 15% of the Kernel, and half-Core, half-Satellite. But every dictionary has a huge number of MinSets. The Core words are learned earlier, more frequent, and less concrete than the Satellites, which in turn are learned earlier and more frequent but more concrete than the rest of the Dictionary. In principle, only one MinSet's words would need to be grounded through the sensorimotor capacity to recognize and categorize their referents. In a dual-code sensorimotor-symbolic model of the mental lexicon, the symbolic code could do all the rest via re-combinatory definition.\n    ",
        "submission_date": "2014-11-01T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0588",
        "title": "On Detecting Noun-Adjective Agreement Errors in Bulgarian Language Using GATE",
        "authors": [
            "Nadezhda Borisova",
            "Grigor Iliev",
            "Elena Karashtranova"
        ],
        "abstract": "In this article, we describe an approach for automatic detection of noun-adjective agreement errors in Bulgarian texts by explaining the necessary steps required to develop a simple Java-based language processing application. For this purpose, we use the GATE language processing framework, which is capable of analyzing texts in Bulgarian language and can be embedded in software applications, accessed through a set of Java APIs. In our example application we also demonstrate how to use the functionality of GATE to perform regular expressions over annotations for detecting agreement errors in simple noun phrases formed by two words - attributive adjective and a noun, where the attributive adjective precedes the noun. The provided code samples can also be used as a starting point for implementing natural language processing functionalities in software applications related to language processing tasks like detection, annotation and retrieval of word groups meeting a specific set of criteria.\n    ",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2014-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0778",
        "title": "Detecting Suicidal Ideation in Chinese Microblogs with Psychological Lexicons",
        "authors": [
            "Xiaolei Huang",
            "Lei Zhang",
            "Tianli Liu",
            "David Chiu",
            "Tingshao Zhu",
            "Xin Li"
        ],
        "abstract": "Suicide is among the leading causes of death in China. However, technical approaches toward preventing suicide are challenging and remaining under development. Recently, several actual suicidal cases were preceded by users who posted microblogs with suicidal ideation to Sina Weibo, a Chinese social media network akin to Twitter. It would therefore be desirable to detect suicidal ideations from microblogs in real-time, and immediately alert appropriate support groups, which may lead to successful prevention. In this paper, we propose a real-time suicidal ideation detection system deployed over Weibo, using machine learning and known psychological techniques. Currently, we have identified 53 known suicidal cases who posted suicide notes on Weibo prior to their ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0895",
        "title": "Tied Probabilistic Linear Discriminant Analysis for Speech Recognition",
        "authors": [
            "Liang Lu",
            "Steve Renals"
        ],
        "abstract": "Acoustic models using probabilistic linear discriminant analysis (PLDA) capture the correlations within feature vectors using subspaces which do not vastly expand the model. This allows high dimensional and correlated feature spaces to be used, without requiring the estimation of multiple high dimension covariance matrices. In this letter we extend the recently presented PLDA mixture model for speech recognition through a tied PLDA approach, which is better able to control the model size to avoid overfitting. We carried out experiments using the Switchboard corpus, with both mel frequency cepstral coefficient features and bottleneck feature derived from a deep neural network. Reductions in word error rate were obtained by using tied PLDA, compared with the PLDA mixture model, subspace Gaussian mixture models, and deep neural networks.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2328",
        "title": "Modeling Word Relatedness in Latent Dirichlet Allocation",
        "authors": [
            "Xun Wang"
        ],
        "abstract": "Standard LDA model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. To address this problem, in this paper, we propose a model called Word Related Latent Dirichlet Allocation (WR-LDA) by incorporating word correlation into LDA topic models. This leads to new capabilities that standard LDA model does not have such as estimating infrequently occurring words or multi-language topic modeling. Experimental results demonstrate the effectiveness of our model compared with standard LDA.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2645",
        "title": "Non-crossing dependencies: least effort, not grammar",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "The use of null hypotheses (in a statistical sense) is common in hard sciences but not in theoretical linguistics. Here the null hypothesis that the low frequency of syntactic dependency crossings is expected by an arbitrary ordering of words is rejected. It is shown that this would require star dependency structures, which are both unrealistic and too restrictive. The hypothesis of the limited resources of the human brain is revisited. Stronger null hypotheses taking into account actual dependency lengths for the likelihood of crossings are presented. Those hypotheses suggests that crossings are likely to reduce when dependencies are shortened. A hypothesis based on pressure to reduce dependency lengths is more parsimonious than a principle of minimization of crossings or a grammatical ban that is totally dissociated from the general and non-linguistic principle of economy.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2738",
        "title": "word2vec Parameter Learning Explained",
        "authors": [
            "Xin Rong"
        ],
        "abstract": "The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models.\n",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2016-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3146",
        "title": "Distributed Representations for Compositional Semantics",
        "authors": [
            "Karl Moritz Hermann"
        ],
        "abstract": "The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP.\n",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2014-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3315",
        "title": "Statistically Significant Detection of Linguistic Change",
        "authors": [
            "Vivek Kulkarni",
            "Rami Al-Rfou",
            "Bryan Perozzi",
            "Steven Skiena"
        ],
        "abstract": "We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning and usage of words. Such linguistic shifts are especially prevalent on the Internet, where the rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis approach constructs property time series of word usage, and then uses statistically sound change point detection algorithms to identify significant linguistic shifts.\n",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2014-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3561",
        "title": "A Text to Speech (TTS) System with English to Punjabi Conversion",
        "authors": [
            "Prabhsimran Singh",
            "Amritpal Singh"
        ],
        "abstract": "The paper aims to show how an application can be developed that converts the English language into the Punjabi Language, and the same application can convert the Text to Speech(TTS) i.e. pronounce the text. This application can be really beneficial for those with special needs.\n    ",
        "submission_date": "2014-11-13T00:00:00",
        "last_modified_date": "2014-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4072",
        "title": "Learning Multi-Relational Semantics Using Neural-Embedding Models",
        "authors": [
            "Bishan Yang",
            "Wen-tau Yih",
            "Xiaodong He",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "In this paper we present a unified framework for modeling multi-relational representations, scoring, and learning, and conduct an empirical study of several recent multi-relational embedding models under the framework. We investigate the different choices of relation operators based on linear and bilinear transformations, and also the effects of entity representations by incorporating unsupervised vectors pre-trained on extra textual resources. Our results show several interesting findings, enabling the design of a simple embedding model that achieves the new state-of-the-art performance on a popular knowledge base completion task evaluated on Freebase.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4109",
        "title": "Resolution of Difficult Pronouns Using the ROSS Method",
        "authors": [
            "Glenn R. Hofford"
        ],
        "abstract": "A new natural language understanding method for disambiguation of difficult pronouns is described. Difficult pronouns are those pronouns for which a level of world or domain knowledge is needed in order to perform anaphoral or other types of resolution. Resolution of difficult pronouns may in some cases require a prior step involving the application of inference to a situation that is represented by the natural language text. A general method is described: it performs entity resolution and pronoun resolution. An extension to the general pronoun resolution method performs inference as an embedded commonsense reasoning method. The general method and the embedded method utilize features of the ROSS representational scheme; in particular the methods use ROSS ontology classes and the ROSS situation model. The overall method is a working solution that solves the following Winograd schemas: a) trophy and suitcase, b) person lifts person, c) person pays detective, and d) councilmen and demonstrators.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4114",
        "title": "Definition of Visual Speech Element and Research on a Method of Extracting Feature Vector for Korean Lip-Reading",
        "authors": [
            "Ha Jong Won",
            "Li Gwang Chol",
            "Kim Hyok Chol",
            "Li Kum Song"
        ],
        "abstract": "In this paper, we defined the viseme (visual speech element) and described about the method of extracting visual feature vector. We defined the 10 visemes based on vowel by analyzing of Korean utterance and proposed the method of extracting the 20-dimensional visual feature vector, combination of static features and dynamic features. Lastly, we took an experiment in recognizing words based on 3-viseme HMM and evaluated the efficiency.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4116",
        "title": "Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning",
        "authors": [
            "Jianpeng Cheng",
            "Dimitri Kartsaklis",
            "Edward Grefenstette"
        ],
        "abstract": "This paper aims to explore the effect of prior disambiguation on neural network- based compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4166",
        "title": "Retrofitting Word Vectors to Semantic Lexicons",
        "authors": [
            "Manaal Faruqui",
            "Jesse Dodge",
            "Sujay K. Jauhar",
            "Chris Dyer",
            "Eduard Hovy",
            "Noah A. Smith"
        ],
        "abstract": "Vector space word representations are learned from distributional information of words in large corpora. Although such statistics are semantically informative, they disregard the valuable information that is contained in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into the word vector training algorithms.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2015-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4455",
        "title": "Errata: Distant Supervision for Relation Extraction with Matrix Completion",
        "authors": [
            "Miao Fan",
            "Deli Zhao",
            "Qiang Zhou",
            "Zhiyuan Liu",
            "Thomas Fang Zheng",
            "Edward Y. Chang"
        ],
        "abstract": "The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4472",
        "title": "Opinion mining of text documents written in Macedonian language",
        "authors": [
            "Andrej Gajduk",
            "Ljupco Kocarev"
        ],
        "abstract": "The ability to extract public opinion from web portals such as review sites, social networks and blogs will enable companies and individuals to form a view, an attitude and make decisions without having to do lengthy and costly researches and surveys. In this paper machine learning techniques are used for determining the polarity of forum posts on kajgana which are written in Macedonian language. The posts are classified as being positive, negative or neutral. We test different feature metrics and classifiers and provide detailed evaluation of their participation in improving the overall performance on a manually generated dataset. By achieving 92% accuracy, we show that the performance of systems for automated opinion mining is comparable to a human evaluator, thus making it a viable option for text data analysis. Finally, we present a few statistics derived from the forum posts using the developed system.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4614",
        "title": "Using graph transformation algorithms to generate natural language equivalents of icons expressing medical concepts",
        "authors": [
            "Pascal Vaillant",
            "Jean-Baptiste Lamy"
        ],
        "abstract": "A graphical language addresses the need to communicate medical information in a synthetic way. Medical concepts are expressed by icons conveying fast visual information about patients' current state or about the known effects of drugs. In order to increase the visual language's acceptance and usability, a natural language generation interface is currently developed. In this context, this paper describes the use of an informatics method ---graph transformation--- to prepare data consisting of concepts in an OWL-DL ontology for use in a natural language generation component. The OWL concept may be considered as a star-shaped graph with a central node. The method transforms it into a graph representing the deep semantic structure of a natural language phrase. This work may be of future use in other contexts where ontology concepts have to be mapped to half-formalized natural language expressions.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2014-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4618",
        "title": "Relations World: A Possibilistic Graphical Model",
        "authors": [
            "Christopher J.C. Burges",
            "Erin Renshaw",
            "Andrzej Pastusiak"
        ],
        "abstract": "We explore the idea of using a \"possibilistic graphical model\" as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user's family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4960",
        "title": "Network Motifs Analysis of Croatian Literature",
        "authors": [
            "Hana Rizvi\u0107",
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107",
            "Ana Me\u0161trovi\u0107"
        ],
        "abstract": "In this paper we analyse network motifs in the co-occurrence directed networks constructed from five different texts (four books and one portal) in the Croatian language. After preparing the data and network construction, we perform the network motif analysis. We analyse the motif frequencies and Z-scores in the five networks. We present the triad significance profile for five datasets. Furthermore, we compare our results with the existing results for the linguistic networks. Firstly, we show that the triad significance profile for the Croatian language is very similar with the other languages and all the networks belong to the same family of networks. However, there are certain differences between the Croatian language and other analysed languages. We conclude that this is due to the free word-order of the Croatian language.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5379",
        "title": "Type-Driven Incremental Semantic Parsing with Polymorphism",
        "authors": [
            "Kai Zhao",
            "Liang Huang"
        ],
        "abstract": "Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GeoQuery, Jobs and Atis domains.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5595",
        "title": "Linking GloVe with word2vec",
        "authors": [
            "Tianze Shi",
            "Zhiyuan Liu"
        ],
        "abstract": "The Global Vectors for word representation (GloVe), introduced by Jeffrey Pennington et al. is reported to be an efficient and effective method for learning vector representations of words. State-of-the-art performance is also provided by skip-gram with negative-sampling (SGNS) implemented in the word2vec tool. In this note, we explain the similarities between the training objectives of the two models, and show that the objective of SGNS is similar to the objective of a specialized form of GloVe, though their cost functions are defined differently.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5732",
        "title": "A Joint Probabilistic Classification Model of Relevant and Irrelevant Sentences in Mathematical Word Problems",
        "authors": [
            "Suleyman Cetintas",
            "Luo Si",
            "Yan Ping Xin",
            "Dake Zhang",
            "Joo Young Park",
            "Ron Tzur"
        ],
        "abstract": "Estimating the difficulty level of math word problems is an important task for many educational applications. Identification of relevant and irrelevant sentences in math word problems is an important step for calculating the difficulty levels of such problems. This paper addresses a novel application of text categorization to identify two types of sentences in mathematical word problems, namely relevant and irrelevant sentences. A novel joint probabilistic classification model is proposed to estimate the joint probability of classification decisions for all sentences of a math word problem by utilizing the correlation among all sentences along with the correlation between the question sentence and other sentences, and sentence text. The proposed model is compared with i) a SVM classifier which makes independent classification decisions for individual sentences by only using the sentence text and ii) a novel SVM classifier that considers the correlation between the question sentence and other sentences along with the sentence text. An extensive set of experiments demonstrates the effectiveness of the joint probabilistic classification model for identifying relevant and irrelevant sentences as well as the novel SVM classifier that utilizes the correlation between the question sentence and other sentences. Furthermore, empirical results and analysis show that i) it is highly beneficial not to remove stopwords and ii) utilizing part of speech tagging does not make a significant improvement although it has been shown to be effective for the related task of math word problem type classification.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5796",
        "title": "Pre-processing of Domain Ontology Graph Generation System in Punjabi",
        "authors": [
            "Rajveer Kaur",
            "Saurabh Sharma"
        ],
        "abstract": "This paper describes pre-processing phase of ontology graph generation system from Punjabi text documents of different domains. This research paper focuses on pre-processing of Punjabi text documents. Pre-processing is structured representation of the input text. Pre-processing of ontology graph generation includes allowing input restrictions to the text, removal of special symbols and punctuation marks, removal of duplicate terms, removal of stop words, extract terms by matching input terms with dictionary and gazetteer lists terms.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6699",
        "title": "One Vector is Not Enough: Entity-Augmented Distributional Semantics for Discourse Relations",
        "authors": [
            "Yangfeng Ji",
            "Jacob Eisenstein"
        ],
        "abstract": "Discourse relations bind smaller linguistic units into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked arguments. A more subtle challenge is that it is not enough to represent the meaning of each argument of a discourse relation, because the relation may depend on links between lower-level components, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted from the distributional representations of the arguments, and also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6718",
        "title": "LABR: A Large Scale Arabic Sentiment Analysis Benchmark",
        "authors": [
            "Mahmoud Nabil",
            "Mohamed Aly",
            "Amir Atiya"
        ],
        "abstract": "We introduce LABR, the largest sentiment analysis dataset to-date for the Arabic language. It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars. We investigate the properties of the dataset, and present its statistics. We explore using the dataset for two tasks: (1) sentiment polarity classification; and (2) ratings classification. Moreover, we provide standard splits of the dataset into training, validation and testing, for both polarity and ratings classification, in both balanced and unbalanced settings. We extend our previous work by performing a comprehensive analysis on the dataset. In particular, we perform an extended survey of the different classifiers typically used for the sentiment polarity classification problem. We also construct a sentiment lexicon from the dataset that contains both single and compound sentiment words and we explore its effectiveness. We make the dataset and experimental details publicly available.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7820",
        "title": "Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic Models and Encyclopedic Knowledge",
        "authors": [
            "Vivi Nastase",
            "Angela Fahrni"
        ],
        "abstract": "We present a method for coarse-grained cross-lingual alignment of comparable texts: segments consisting of contiguous paragraphs that discuss the same theme (e.g. history, economy) are aligned based on induced multilingual topics. The method combines three ideas: a two-level LDA model that filters out words that do not convey themes, an HMM that models the ordering of themes in the collection of documents, and language-independent concept annotations to serve as a cross-language bridge and to strengthen the connection between paragraphs in the same segment through concept relations. The method is evaluated on English and French data previously used for monolingual alignment. The results show state-of-the-art performance in both monolingual and cross-lingual settings.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7942",
        "title": "Using Sentence Plausibility to Learn the Semantics of Transitive Verbs",
        "authors": [
            "Tamara Polajnar",
            "Laura Rimell",
            "Stephen Clark"
        ],
        "abstract": "The functional approach to compositional distributional semantics considers transitive verbs to be linear maps that transform the distributional vectors representing nouns into a vector representing a sentence. We conduct an initial investigation that uses a matrix consisting of the parameters of a logistic regression classifier trained on a plausibility task as a transitive verb function. We compare our method to a commonly used corpus-based method for constructing a verb matrix and find that the plausibility training may be more effective for disambiguation tasks.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0696",
        "title": "Understanding confounding effects in linguistic coordination: an information-theoretic approach",
        "authors": [
            "Shuyang Gao",
            "Greg Ver Steeg",
            "Aram Galstyan"
        ],
        "abstract": "We suggest an information-theoretic approach for measuring stylistic coordination in dialogues. The proposed measure has a simple predictive interpretation and can account for various confounding factors through proper conditioning. We revisit some of the previous studies that reported strong signatures of stylistic accommodation, and find that a significant part of the observed coordination can be attributed to a simple confounding effect - length coordination. Specifically, longer utterances tend to be followed by longer responses, which gives rise to spurious correlations in the other stylistic features. We propose a test to distinguish correlations in length due to contextual factors (topic of conversation, user verbosity, etc.) and turn-by-turn coordination. We also suggest a test to identify whether stylistic coordination persists even after accounting for length coordination and contextual factors.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2015-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0751",
        "title": "Tiered Clustering to Improve Lexical Entailment",
        "authors": [
            "John Wieting"
        ],
        "abstract": "Many tasks in Natural Language Processing involve recognizing lexical entailment. Two different approaches to this problem have been proposed recently that are quite different from each other. The first is an asymmetric similarity measure designed to give high scores when the contexts of the narrower term in the entailment are a subset of those of the broader term. The second is a supervised approach where a classifier is learned to predict entailment given a concatenated latent vector representation of the word. Both of these approaches are vector space models that use a single context vector as a representation of the word. In this work, I study the effects of clustering words into senses and using these multiple context vectors to infer entailment using extensions of these two algorithms. I find that this approach offers some improvement to these entailment algorithms.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0879",
        "title": "Watsonsim: Overview of a Question Answering Engine",
        "authors": [
            "Sean Gallagher",
            "Wlodek Zadrozny",
            "Walid Shalaby",
            "Adarsh Avadhani"
        ],
        "abstract": "The objective of the project is to design and run a system similar to Watson, designed to answer Jeopardy questions. In the course of a semester, we developed an open source question answering system using the Indri, Lucene, Bing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka among additional modules. By the end of the semester, we achieved 18% accuracy on Jeopardy questions, and work has not stopped since then.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1058",
        "title": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks",
        "authors": [
            "Rie Johnson",
            "Tong Zhang"
        ],
        "abstract": "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1215",
        "title": "Mary Astell's words in A Serious Proposal to the Ladies (part I), a lexicographic inquiry with NooJ",
        "authors": [
            "H\u00e9l\u00e8ne Pignot",
            "Odile Piton"
        ],
        "abstract": "In the following article we elected to study with NooJ the lexis of a 17 th century text, Mary Astell's seminal essay, A Serious Proposal to the Ladies, part I, published in 1694. We first focused on the semantics to see how Astell builds her vindication of the female sex, which words she uses to sensitise women to their alienated condition and promote their education. Then we studied the morphology of the lexemes (which is different from contemporary English) used by the author, thanks to the NooJ tools we have devised for this purpose. NooJ has great functionalities for lexicographic work. Its commands and graphs prove to be most efficient in the spotting of archaic words or variants in spelling. Introduction In our previous articles, we have studied the singularities of 17 th century English within the framework of a diachronic analysis thanks to syntactical and morphological graphs and thanks to the dictionaries we have compiled from a corpus that may be expanded overtime. Our early work was based on a limited corpus of English travel literature to Greece in the 17 th century. This article deals with a late seventeenth century text written by a woman philosopher and essayist, Mary Astell (1666--1731), considered as one of the first English feminists. Astell wrote her essay at a time in English history when women were \"the weaker vessel\" and their main business in life was to charm and please men by their looks and submissiveness. In this essay we will see how NooJ can help us analyse Astell's rhetoric (what point of view does she adopt, does she speak in her own name, in the name of all women, what is her representation of men and women and their relationships in the text, what are the goals of education?). Then we will turn our attention to the morphology of words in the text and use NooJ commands and graphs to carry out a lexicographic inquiry into Astell's lexemes.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1342",
        "title": "A perspective on the advancement of natural language processing tasks via topological analysis of complex networks",
        "authors": [
            "Diego R. Amancio"
        ],
        "abstract": "Comment on \"Approaching human language with complex networks\" by Cong and Liu (Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1632",
        "title": "Deep Learning for Answer Sentence Selection",
        "authors": [
            "Lei Yu",
            "Karl Moritz Hermann",
            "Phil Blunsom",
            "Stephen Pulman"
        ],
        "abstract": "Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2014-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1820",
        "title": "Context-Dependent Fine-Grained Entity Type Tagging",
        "authors": [
            "Dan Gillick",
            "Nevena Lazic",
            "Kuzman Ganchev",
            "Jesse Kirchner",
            "David Huynh"
        ],
        "abstract": "Entity type tagging is the task of assigning category labels to each mention of an entity in a document. While standard systems focus on a small set of types, recent work (Ling and Weld, 2012) suggests that using a large fine-grained label set can lead to dramatic improvements in downstream tasks. In the absence of labeled training data, existing fine-grained tagging systems obtain examples automatically, using resolved entities and their types extracted from a knowledge base. However, since the appropriate type often depends on context (e.g. Washington could be tagged either as city or government), this procedure can result in spurious labels, leading to poorer generalization. We propose the task of context-dependent fine type tagging, where the set of acceptable labels for a mention is restricted to only those deducible from the local context (e.g. sentence or document). We introduce new resources for this task: 12,017 mentions annotated with their context-dependent fine types, and we provide baseline experimental results on this data.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1841",
        "title": "Exemplar Dynamics and Sound Merger in Language",
        "authors": [
            "P. F. Tupper"
        ],
        "abstract": "We develop a model of phonological contrast in natural language. Specifically, the model describes the maintenance of contrast between different words in a language, and the elimination of such contrast when sounds in the words merge. An example of such a contrast is that provided by the two vowel sounds 'i' and 'e', which distinguish pairs of words such as 'pin' and 'pen' in most dialects of English. We model language users' knowledge of the pronunciation of a word as consisting of collections of labeled exemplars stored in memory. Each exemplar is a detailed memory of a particular utterance of the word in question. In our model an exemplar is represented by one or two phonetic variables along with a weight indicating how strong the memory of the utterance is. Starting from an exemplar-level model we derive integro-differential equations for the evolution of exemplar density fields in phonetic space. Using these latter equations we investigate under what conditions two sounds merge, thus eliminating the contrast. Our main conclusion is that for the preservation of phonological contrast, it is necessary that anomalous utterances of a given word are discarded, and not merely stored in memory as an exemplar of another word.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1866",
        "title": "Integer-Programming Ensemble of Temporal-Relations Classifiers",
        "authors": [
            "Catherine Kerr",
            "Terri Hoare",
            "Paula Carroll",
            "Jakub Marecek"
        ],
        "abstract": "The extraction and understanding of temporal events and their relations are major challenges in natural language processing. Processing text on a sentence-by-sentence or expression-by-expression basis often fails, in part due to the challenge of capturing the global consistency of the text. We present an ensemble method, which reconciles the outputs of multiple classifiers of temporal expressions across the text using integer programming. Computational experiments show that the ensemble improves upon the best individual results from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and SemEval-2016 Task 12 (Clinical TempEval).\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2018-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2007",
        "title": "On Using Very Large Target Vocabulary for Neural Machine Translation",
        "authors": [
            "S\u00e9bastien Jean",
            "Kyunghyun Cho",
            "Roland Memisevic",
            "Yoshua Bengio"
        ],
        "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English->German translation and almost as high performance as state-of-the-art English->French translation system.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2197",
        "title": "Practice in Synonym Extraction at Large Scale",
        "authors": [
            "Liangliang Cao",
            "Chang Wang"
        ],
        "abstract": "Synonym extraction is an important task in natural language processing and often used as a submodule in query expansion, question answering and other applications. Automatic synonym extractor is highly preferred for large scale applications. Previous studies in synonym extraction are most limited to small scale datasets. In this paper, we build a large dataset with 3.4 million synonym/non-synonym pairs to capture the challenges in real world scenarios. We proposed (1) a new cost function to accommodate the unbalanced learning problem, and (2) a feature learning based deep neural network to model the complicated relationships in synonym pairs. We compare several different approaches based on SVMs and neural networks, and find out a novel feature learning based neural network outperforms the methods with hand-assigned features. Specifically, the best performance of our model surpasses the SVM baseline with a significant 97\\% relative improvement.\n    ",
        "submission_date": "2014-12-06T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2378",
        "title": "Learning Word Representations from Relational Graphs",
        "authors": [
            "Danushka Bollegala",
            "Takanori Maehara",
            "Yuichi Yoshida",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "Attributes of words and relations between two words are central to numerous tasks in Artificial Intelligence such as knowledge representation, similarity measurement, and analogy detection. Often when two words share one or more attributes in common, they are connected by some semantic relations. On the other hand, if there are numerous semantic relations between two words, we can expect some of the attributes of one of the words to be inherited by the other. Motivated by this close connection between attributes and relations, given a relational graph in which words are inter- connected via numerous semantic relations, we propose a method to learn a latent representation for the individual words. The proposed method considers not only the co-occurrences of words as done by existing approaches for word representation learning, but also the semantic relations in which two words co-occur. To evaluate the accuracy of the word representations learnt using the proposed method, we use the learnt word representations to solve semantic word analogy problems. Our experimental results show that it is possible to learn better word representations by using semantic semantics between words.\n    ",
        "submission_date": "2014-12-07T00:00:00",
        "last_modified_date": "2014-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2442",
        "title": "Rediscovering the Alphabet - On the Innate Universal Grammar",
        "authors": [
            "M. Yahia Kaadan",
            "Asaad Kaadan"
        ],
        "abstract": "Universal Grammar (UG) theory has been one of the most important research topics in linguistics since introduced five decades ago. UG specifies the restricted set of languages learnable by human brain, and thus, many researchers believe in its biological roots. Numerous empirical studies of neurobiological and cognitive functions of the human brain, and of many natural languages, have been conducted to unveil some aspects of UG. This, however, resulted in different and sometimes contradicting theories that do not indicate a universally unique grammar. In this research, we tackle the UG problem from an entirely different perspective. We search for the Unique Universal Grammar (UUG) that facilitates communication and knowledge transfer, the sole purpose of a language. We formulate this UG and show that it is unique, intrinsic, and cosmic, rather than humanistic. Initial analysis on a widespread natural language already showed some positive results.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2812",
        "title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error Minimization Framework",
        "authors": [
            "Ivan Titov",
            "Ehsan Khoddam"
        ],
        "abstract": "We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2821",
        "title": "Zipf's Law and the Frequency of Characters or Words of Oracles",
        "authors": [
            "Xiuli Wang"
        ],
        "abstract": "The article discusses the frequency of characters of Oracle,concluding that the frequency and the rank of a word or character is fit to Zipf-Mandelboit Law or Zipf's law with three parameters,and figuring out the parameters based on the frequency,and pointing out that what some researchers of Oracle call the assembling on the two ends is just a description by their impression about the Oracle data.\n    ",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3336",
        "title": "Statistical Patterns in Written Language",
        "authors": [
            "Dami\u00e1n H. Zanette"
        ],
        "abstract": "Quantitative linguistics has been allowed, in the last few decades, within the admittedly blurry boundaries of the field of complex systems. A growing host of applied mathematicians and statistical physicists devote their efforts to disclose regularities, correlations, patterns, and structural properties of language streams, using techniques borrowed from statistics and information theory. Overall, results can still be categorized as modest, but the prospects are promising: medium- and long-range features in the organization of human language -which are beyond the scope of traditional linguistics- have already emerged from this kind of analysis and continue to be reported, contributing a new perspective to our understanding of this most complex communication system. This short book is intended to review some of these recent contributions.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2017-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4021",
        "title": "A Robust Transformation-Based Learning Approach Using Ripple Down Rules for Part-of-Speech Tagging",
        "authors": [
            "Dat Quoc Nguyen",
            "Dai Quoc Nguyen",
            "Dang Duc Pham",
            "Son Bao Pham"
        ],
        "abstract": "In this paper, we propose a new approach to construct a system of transformation rules for the Part-of-Speech (POS) tagging task. Our approach is based on an incremental knowledge acquisition method where rules are stored in an exception structure and new rules are only added to correct the errors of existing rules; thus allowing systematic control of the interaction between the rules. Experimental results on 13 languages show that our approach is fast in terms of training time and tagging speed. Furthermore, our approach obtains very competitive accuracy in comparison to state-of-the-art POS and morphological taggers.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2015-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4160",
        "title": "Ripple Down Rules for Question Answering",
        "authors": [
            "Dat Quoc Nguyen",
            "Dai Quoc Nguyen",
            "Son Bao Pham"
        ],
        "abstract": "Recent years have witnessed a new trend of building ontology-based question answering systems. These systems use semantic web information to produce more precise answers to users' queries. However, these systems are mostly designed for English. In this paper, we introduce an ontology-based question answering system named KbQAS which, to the best of our knowledge, is the first one made for Vietnamese. KbQAS employs our question analysis approach that systematically constructs a knowledge base of grammar rules to convert each input question into an intermediate representation element. KbQAS then takes the intermediate representation element with respect to a target ontology and applies concept-matching techniques to return an answer. On a wide range of Vietnamese questions, experimental results show that the performance of KbQAS is promising with accuracies of 84.1% and 82.4% for analyzing input questions and retrieving output answers, respectively. Furthermore, our question analysis approach can easily be applied to new domains and new languages, thus saving time and human effort.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4369",
        "title": "Incorporating Both Distributional and Relational Semantics in Word Representations",
        "authors": [
            "Daniel Fried",
            "Kevin Duh"
        ],
        "abstract": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.\n    ",
        "submission_date": "2014-12-14T00:00:00",
        "last_modified_date": "2015-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4385",
        "title": "Unsupervised Domain Adaptation with Feature Embeddings",
        "authors": [
            "Yi Yang",
            "Jacob Eisenstein"
        ],
        "abstract": "Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches often require the specification of \"pivot features\" that generalize across domains, which are selected by task-specific heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems.\n    ",
        "submission_date": "2014-12-14T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4616",
        "title": "A Broadcast News Corpus for Evaluation and Tuning of German LVCSR Systems",
        "authors": [
            "Felix Weninger",
            "Bj\u00f6rn Schuller",
            "Florian Eyben",
            "Martin W\u00f6llmer",
            "Gerhard Rigoll"
        ],
        "abstract": "Transcription of broadcast news is an interesting and challenging application for large-vocabulary continuous speech recognition (LVCSR). We present in detail the structure of a manually segmented and annotated corpus including over 160 hours of German broadcast news, and propose it as an evaluation framework of LVCSR systems. We show our own experimental results on the corpus, achieved with a state-of-the-art LVCSR decoder, measuring the effect of different feature sets and decoding parameters, and thereby demonstrate that real-time decoding of our test set is feasible on a desktop PC at 9.2% word error rate.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4682",
        "title": "Rule-based Emotion Detection on Social Media: Putting Tweets on Plutchik's Wheel",
        "authors": [
            "Erik Tromp",
            "Mykola Pechenizkiy"
        ],
        "abstract": "We study sentiment analysis beyond the typical granularity of polarity and instead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as an extension to the Rule-Based Emission Model algorithm to deduce such emotions from human-written messages. We evaluate our approach on two different datasets and compare its performance with the current state-of-the-art techniques for emotion detection, including a recursive auto-encoder. The results of the experimental study suggest that RBEM-Emo is a promising approach advancing the current state-of-the-art in emotion detection.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4846",
        "title": "Scaling laws in human speech, decreasing emergence of new words and a generalized model",
        "authors": [
            "Ruokuang Lin",
            "Qianli D.Y. Ma",
            "Chunhua Bian"
        ],
        "abstract": "Human language, as a typical complex system, its organization and evolution is an attractive topic for both physical and cultural researchers. In this paper, we present the first exhaustive analysis of the text organization of human speech. Two important results are that: (i) the construction and organization of spoken language can be characterized as Zipf's law and Heaps' law, as observed in written texts; (ii) word frequency vs. rank distribution and the growth of distinct words with the increase of text length shows significant differences between book and speech. In speech word frequency distribution are more concentrated on higher frequency words, and the emergence of new words decreases much rapidly when the content length grows. Based on these observations, a new generalized model is proposed to explain these complex dynamical behaviors and the differences between speech and book.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4930",
        "title": "Rehabilitation of Count-based Models for Word Vector Representations",
        "authors": [
            "R\u00e9mi Lebret",
            "Ronan Collobert"
        ],
        "abstract": "Recent works on word representations mostly rely on predictive models. Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear. Such models have succeeded in capturing word similarties as well as semantic and syntactic regularities. Instead, we aim at reviving interest in a model based on counts. We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurence statistics of large text corpora. We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation. Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases. This becomes a clear advantage compared to predictive models which must train these new words.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5212",
        "title": "Application of Topic Models to Judgments from Public Procurement Domain",
        "authors": [
            "Micha\u0142 \u0141opuszy\u0144ski"
        ],
        "abstract": "In this work, automatic analysis of themes contained in a large corpora of judgments from public procurement domain is performed. The employed technique is unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed, to use LDA in conjunction with recently developed method of unsupervised keyword extraction. Such an approach improves the interpretability of the automatically obtained topics and allows for better computational performance. The described analysis illustrates a potential of the method in detecting recurring themes and discovering temporal trends in lodged contract appeals. These results may be in future applied to improve information retrieval from repositories of legal texts or as auxiliary material for legal analyses carried out by human experts.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5335",
        "title": "Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews",
        "authors": [
            "Gr\u00e9goire Mesnil",
            "Tomas Mikolov",
            "Marc'Aurelio Ranzato",
            "Yoshua Bengio"
        ],
        "abstract": "Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classification problem. We compare several ma- chine learning approaches to this problem, and combine them to achieve the best possible results. We show how to use for this task the standard generative lan- guage models, which are slightly complementary to the state of the art techniques. We achieve strong results on a well-known dataset of IMDB movie reviews. Our results are easily reproducible, as we publish also the code needed to repeat the experiments. This should simplify further advance of the state of the art, as other researchers can combine their techniques with ours with little effort.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5404",
        "title": "Word Network Topic Model: A Simple but General Solution for Short and Imbalanced Texts",
        "authors": [
            "Yuan Zuo",
            "Jichang Zhao",
            "Ke Xu"
        ],
        "abstract": "The short text has been the prevalent format for information of Internet in recent decades, especially with the development of online social media, whose millions of users generate a vast number of short messages everyday. Although sophisticated signals delivered by the short text make it a promising source for topic modeling, its extreme sparsity and imbalance brings unprecedented challenges to conventional topic models like LDA and its variants. Aiming at presenting a simple but general solution for topic modeling in short texts, we present a word co-occurrence network based model named WNTM to tackle the sparsity and imbalance simultaneously. Different from previous approaches, WNTM models the distribution over topics for each word instead of learning topics for each document, which successfully enhance the semantic density of data space without importing too much time or space complexity. Meanwhile, the rich contextual information preserved in the word-word space also guarantees its sensitivity in identifying rare topics with convincing quality. Furthermore, employing the same Gibbs sampling with LDA makes WNTM easily to be extended to various application scenarios. Extensive validations on both short and normal texts testify the outperformance of WNTM as compared to baseline methods. And finally we also demonstrate its potential in precisely discovering newly emerging topics or unexpected events in Weibo at pretty early stages.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5477",
        "title": "Computational Model to Generate Case-Inflected Forms of Masculine Nouns for Word Search in Sanskrit E-Text",
        "authors": [
            "S V Kasmir Raja",
            "V Rajitha",
            "Lakshmanan Meenakshi"
        ],
        "abstract": "The problem of word search in Sanskrit is inseparable from complexities that include those caused by euphonic conjunctions and case-inflections. The case-inflectional forms of a noun normally number 24 owing to the fact that in Sanskrit there are eight cases and three numbers-singular, dual and plural. The traditional method of generating these inflectional forms is rather elaborate owing to the fact that there are differences in the forms generated between even very similar words and there are subtle nuances involved. Further, it would be a cumbersome exercise to generate and search for 24 forms of a word during a word search in a large text, using the currently available case-inflectional form generators. This study presents a new approach to generating case-inflectional forms that is simpler to compute. Further, an optimized model that is sufficient for generating only those word forms that are required in a word search and is more than 80% efficient compared to the complete case-inflectional forms generator, is presented in this study for the first time.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5567",
        "title": "Deep Speech: Scaling up end-to-end speech recognition",
        "authors": [
            "Awni Hannun",
            "Carl Case",
            "Jared Casper",
            "Bryan Catanzaro",
            "Greg Diamos",
            "Erich Elsen",
            "Ryan Prenger",
            "Sanjeev Satheesh",
            "Shubho Sengupta",
            "Adam Coates",
            "Andrew Y. Ng"
        ],
        "abstract": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5659",
        "title": "Effective sampling for large-scale automated writing evaluation systems",
        "authors": [
            "Nicholas Dronen",
            "Peter W. Foltz",
            "Kyle Habermehl"
        ],
        "abstract": "Automated writing evaluation (AWE) has been shown to be an effective mechanism for quickly providing feedback to students. It has already seen wide adoption in enterprise-scale applications and is starting to be adopted in large-scale contexts. Training an AWE model has historically required a single batch of several hundred writing examples and human scores for each of them. This requirement limits large-scale adoption of AWE since human-scoring essays is costly. Here we evaluate algorithms for ensuring that AWE models are consistently trained using the most informative essays. Our results show how to minimize training set sizes while maximizing predictive performance, thereby reducing cost without unduly sacrificing accuracy. We conclude with a discussion of how to integrate this approach into large-scale AWE systems.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5673",
        "title": "Entity-Augmented Distributional Semantics for Discourse Relations",
        "authors": [
            "Yangfeng Ji",
            "Jacob Eisenstein"
        ],
        "abstract": "Discourse relations bind smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5836",
        "title": "Incorporating Both Distributional and Relational Semantics in Word Representations",
        "authors": [
            "Daniel Fried",
            "Kevin Duh"
        ],
        "abstract": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2015-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6045",
        "title": "A Simple and Efficient Method To Generate Word Sense Representations",
        "authors": [
            "Luis Nieto Pi\u00f1a",
            "Richard Johansson"
        ],
        "abstract": "  Distributed representations of words have boosted the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not acknowledging the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6264",
        "title": "Supertagging: Introduction, learning, and application",
        "authors": [
            "Taraka Rama K"
        ],
        "abstract": "Supertagging is an approach originally developed by Bangalore and Joshi (1999) to improve the parsing efficiency. In the beginning, the scholars used small training datasets and somewhat na\u00efve smoothing techniques to learn the probability distributions of supertags. Since its inception, the applicability of Supertags has been explored for TAG (tree-adjoining grammar) formalism as well as other related yet, different formalisms such as CCG. This article will try to summarize the various chapters, relevant to statistical parsing, from the most recent edited book volume (Bangalore and Joshi, 2010). The chapters were selected so as to blend the learning of supertags, its integration into full-scale parsing, and in semantic parsing.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6277",
        "title": "N-gram-Based Low-Dimensional Representation for Document Classification",
        "authors": [
            "R\u00e9mi Lebret",
            "Ronan Collobert"
        ],
        "abstract": "The bag-of-words (BOW) model is the common approach for classifying documents, where words are used as feature for training a classifier. This generally involves a huge number of features. Some techniques, such as Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to summarize documents in a lower dimension with the least semantic information loss. Some semantic information is nevertheless always lost, since only words are considered. Instead, we aim at using information coming from n-grams to overcome this limitation, while remaining in a low-dimension space. Many approaches, such as the Skip-gram model, provide good word vector representations very quickly. We propose to average these representations to obtain representations of n-grams. All n-grams are thus embedded in a same semantic space. A K-means clustering can then group them into semantic concepts. The number of features is therefore dramatically reduced and documents can be represented as bag of semantic concepts. We show that this model outperforms LSA and LDA on a sentiment classification task, and yields similar results than a traditional BOW-model with far less features.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6334",
        "title": "Leveraging Monolingual Data for Crosslingual Compositional Word Representations",
        "authors": [
            "Hubert Soyer",
            "Pontus Stenetorp",
            "Akiko Aizawa"
        ],
        "abstract": "In this work, we present a novel neural network based architecture for inducing compositional crosslingual word representations. Unlike previously proposed methods, our method fulfills the following three criteria; it constrains the word-level representations to be compositional, it is capable of leveraging both bilingual and monolingual data, and it is scalable to large vocabularies and large quantities of data. The key component of our approach is what we refer to as a monolingual inclusion criterion, that exploits the observation that phrases are more closely semantically related to their sub-phrases than to other randomly sampled phrases. We evaluate our method on a well-established crosslingual document classification task and achieve results that are either comparable, or greatly improve upon previous state-of-the-art methods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for the English to German and German to English sub-tasks respectively. The former advances the state of the art by 0.9% points of accuracy, the latter is an absolute improvement upon the previous state of the art by 7.7% points of accuracy and an improvement of 33.0% in error reduction.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6418",
        "title": "Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations",
        "authors": [
            "Ivan Titov",
            "Ehsan Khoddam"
        ],
        "abstract": "In this work, we propose a new method to integrate two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6448",
        "title": "Embedding Word Similarity with Neural Machine Translation",
        "authors": [
            "Felix Hill",
            "Kyunghyun Cho",
            "Sebastien Jean",
            "Coline Devin",
            "Yoshua Bengio"
        ],
        "abstract": "Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6568",
        "title": "Improving zero-shot learning by mitigating the hubness problem",
        "authors": [
            "Georgiana Dinu",
            "Angeliki Lazaridou",
            "Marco Baroni"
        ],
        "abstract": "The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6575",
        "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases",
        "authors": [
            "Bishan Yang",
            "Wen-tau Yih",
            "Xiaodong He",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6616",
        "title": "Outperforming Word2Vec on Analogy Tasks with Random Projections",
        "authors": [
            "Abram Demski",
            "Volkan Ustun",
            "Paul Rosenbloom",
            "Cody Kommers"
        ],
        "abstract": "We present a distributed vector representation based on a simplification of the BEAGLE system, designed in the context of the Sigma cognitive architecture. Our method does not require gradient-based training of neural networks, matrix decompositions as with LSA, or convolutions as with BEAGLE. All that is involved is a sum of random vectors and their pointwise products. Despite the simplicity of this technique, it gives state-of-the-art results on analogy problems, in most cases better than Word2Vec. To explain this success, we interpret it as a dimension reduction via random projection.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6623",
        "title": "Word Representations via Gaussian Embedding",
        "authors": [
            "Luke Vilnis",
            "Andrew McCallum"
        ],
        "abstract": "Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6815",
        "title": "Extraction of Salient Sentences from Labelled Documents",
        "authors": [
            "Misha Denil",
            "Alban Demiraj",
            "Nando de Freitas"
        ],
        "abstract": "We present a hierarchical convolutional document model with an architecture designed to support introspection of the document structure. Using this model, we show how to use visualisation techniques from the computer vision literature to identify and extract topic-relevant sentences.\n",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7004",
        "title": "Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison",
        "authors": [
            "Pranava Swaroop Madhyastha",
            "Xavier Carreras",
            "Ariadna Quattoni"
        ],
        "abstract": "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7026",
        "title": "Language Recognition using Random Indexing",
        "authors": [
            "Aditya Joshi",
            "Johan Halseth",
            "Pentti Kanerva"
        ],
        "abstract": "Random Indexing is a simple implementation of Random Projections with a wide range of applications. It can solve a variety of problems with good accuracy without introducing much complexity. Here we use it for identifying the language of text samples. We present a novel method of generating language representation vectors using letter blocks. Further, we show that the method is easily implemented and requires little computational power and space. Experiments on a number of model parameters illustrate certain properties about high dimensional sparse vector representations of data. Proof of statistically relevant language vectors are shown through the extremely high success of various language recognition tasks. On a difficult data set of 21,000 short sentences from 21 different languages, our model performs a language recognition task and achieves 97.8% accuracy, comparable to state-of-the-art methods.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7063",
        "title": "Diverse Embedding Neural Network Language Models",
        "authors": [
            "Kartik Audhkhasi",
            "Abhinav Sethy",
            "Bhuvana Ramabhadran"
        ],
        "abstract": "We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7119",
        "title": "Pragmatic Neural Language Modelling in Machine Translation",
        "authors": [
            "Paul Baltescu",
            "Phil Blunsom"
        ],
        "abstract": "This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difficult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the trade-offs between neural models and back-off n-gram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7180",
        "title": "Bayesian Optimisation for Machine Translation",
        "authors": [
            "Yishu Miao",
            "Ziyu Wang",
            "Phil Blunsom"
        ],
        "abstract": "This paper presents novel Bayesian optimisation algorithms for minimum error rate training of statistical machine translation systems. We explore two classes of algorithms for efficiently exploring the translation space, with the first based on N-best lists and the second based on a hypergraph representation that compactly represents an exponential number of translation options. Our algorithms exhibit faster convergence and are capable of obtaining lower error rates than the existing translation model specific approaches, all within a generic Bayesian optimisation framework. Further more, we also introduce a random embedding algorithm to scale our approach to sparse high dimensional feature sets.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2014-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7186",
        "title": "Reply to the commentary \"Be careful when assuming the obvious\", by P. Alday",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Here we respond to some comments by Alday concerning headedness in linguistic theory and the validity of the assumptions of a mathematical model for word order. For brevity, we focus only on two assumptions: the unit of measurement of dependency length and the monotonicity of the cost of a dependency as a function of its length. We also revise the implicit psychological bias in Alday's comments. Notwithstanding, Alday is indicating the path for linguistic research with his unusual concerns about parsimony from multiple dimensions.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7415",
        "title": "A prototype Malayalam to Sign Language Automatic Translator",
        "authors": [
            "Jestin Joy",
            "Kannan Balakrishnan"
        ],
        "abstract": "Sign language, which is a medium of communication for deaf people, uses manual communication and body language to convey meaning, as opposed to using sound. This paper presents a prototype Malayalam text to sign language translation system. The proposed system takes Malayalam text as input and generates corresponding Sign Language. Output animation is rendered using a computer generated model. This system will help to disseminate information to the deaf people in public utility places like railways, banks, hospitals etc. This will also act as an educational tool in learning Sign Language.\n    ",
        "submission_date": "2014-12-23T00:00:00",
        "last_modified_date": "2015-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7449",
        "title": "Grammar as a Foreign Language",
        "authors": [
            "Oriol Vinyals",
            "Lukasz Kaiser",
            "Terry Koo",
            "Slav Petrov",
            "Ilya Sutskever",
            "Geoffrey Hinton"
        ],
        "abstract": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.\n    ",
        "submission_date": "2014-12-23T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8010",
        "title": "Construction of Vietnamese SentiWordNet by using Vietnamese Dictionary",
        "authors": [
            "Xuan-Son Vu",
            "Seong-Bae Park"
        ],
        "abstract": "SentiWordNet is an important lexical resource supporting sentiment analysis in opinion mining applications. In this paper, we propose a novel approach to construct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated from WordNet in which each synset has numerical scores to indicate its opinion polarities. Many previous studies obtained these scores by applying a machine learning method to WordNet. However, Vietnamese WordNet is not available unfortunately by the time of this paper. Therefore, we propose a method to construct VSWN from a Vietnamese dictionary, not from WordNet. We show the effectiveness of the proposed method by generating a VSWN with 39,561 synsets automatically. The method is experimentally tested with 266 synsets with aspect of positivity and negativity. It attains a competitive result compared with English SentiWordNet that is 0.066 and 0.052 differences for positivity and negativity sets respectively.\n    ",
        "submission_date": "2014-12-27T00:00:00",
        "last_modified_date": "2014-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8079",
        "title": "Persian Sentiment Analyzer: A Framework based on a Novel Feature Selection Method",
        "authors": [
            "Ayoub Bagheri",
            "Mohamad Saraee"
        ],
        "abstract": "In the recent decade, with the enormous growth of digital content in internet and databases, sentiment analysis has received more and more attention between information retrieval and natural language processing researchers. Sentiment analysis aims to use automated tools to detect subjective information from reviews. One of the main challenges in sentiment analysis is feature selection. Feature selection is widely used as the first stage of analysis and classification tasks to reduce the dimension of problem, and improve speed by the elimination of irrelevant and redundant features. Up to now as there are few researches conducted on feature selection in sentiment analysis, there are very rare works for Persian sentiment analysis. This paper considers the problem of sentiment classification using different feature selection methods for online customer reviews in Persian language. Three of the challenges of Persian text are using of a wide variety of declensional suffixes, different word spacing and many informal or colloquial words. In this paper we study these challenges by proposing a model for sentiment classification of Persian review documents. The proposed model is based on lemmatization and feature selection and is employed Naive Bayes algorithm for classification. We evaluate the performance of the model on a manually gathered collection of cellphone reviews, where the results show the effectiveness of the proposed approaches.\n    ",
        "submission_date": "2014-12-27T00:00:00",
        "last_modified_date": "2014-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8319",
        "title": "Quantifying origin and character of long-range correlations in narrative texts",
        "authors": [
            "Stanis\u0142aw Dro\u017cd\u017c",
            "Pawe\u0142 O\u015bwi\u0119cimka",
            "Andrzej Kulig",
            "Jaros\u0142aw Kwapie\u0144",
            "Katarzyna Bazarnik",
            "Iwona Grabska-Gradzi\u0144ska",
            "Jan Rybicki",
            "Marek Stanuszek"
        ],
        "abstract": "In natural language using short sentences is considered efficient for communication. However, a text composed exclusively of such sentences looks technical and reads boring. A text composed of long ones, on the other hand, demands significantly more effort for comprehension. Studying characteristics of the sentence length variability (SLV) in a large corpus of world-famous literary texts shows that an appealing and aesthetic optimum appears somewhere in between and involves selfsimilar, cascade-like alternation of various lengths sentences. A related quantitative observation is that the power spectra S(f) of thus characterized SLV universally develop a convincing `1/f^beta' scaling with the average exponent beta =~ 1/2, close to what has been identified before in musical compositions or in the brain waves. An overwhelming majority of the studied texts simply obeys such fractal attributes but especially spectacular in this respect are hypertext-like, \"stream of consciousness\" novels. In addition, they appear to develop structures characteristic of irreducibly interwoven sets of fractals called multifractals. Scaling of S(f) in the present context implies existence of the long-range correlations in texts and appearance of multifractality indicates that they carry even a nonlinear component. A distinct role of the full stops in inducing the long-range correlations in texts is evidenced by the fact that the above quantitative characteristics on the long-range correlations manifest themselves in variation of the full stops recurrence times along texts, thus in SLV, but to a much lesser degree in the recurrence times of the most frequent words. In this latter case the nonlinear correlations, thus multifractality, disappear even completely for all the texts considered. Treated as one extra word, the full stops at the same time appear to obey the Zipfian rank-frequency distribution, however.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8419",
        "title": "Simple Image Description Generator via a Linear Phrase-Based Approach",
        "authors": [
            "Remi Lebret",
            "Pedro O. Pinheiro",
            "Ronan Collobert"
        ],
        "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results on the recently release Microsoft COCO dataset.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8504",
        "title": "Probing the topological properties of complex networks modeling short written texts",
        "authors": [
            "Diego R. Amancio"
        ],
        "abstract": "In recent years, graph theory has been widely employed to probe several language properties. More specifically, the so-called word adjacency model has been proven useful for tackling several practical problems, especially those relying on textual stylistic analysis. The most common approach to treat texts as networks has simply considered either large pieces of texts or entire books. This approach has certainly worked well -- many informative discoveries have been made this way -- but it raises an uncomfortable question: could there be important topological patterns in small pieces of texts? To address this problem, the topological properties of subtexts sampled from entire books was probed. Statistical analyzes performed on a dataset comprising 50 novels revealed that most of the traditional topological measurements are stable for short subtexts. When the performance of the authorship recognition task was analyzed, it was found that a proper sampling yields a discriminability similar to the one found with full texts. Surprisingly, the support vector machine classification based on the characterization of short texts outperformed the one performed with entire books. These findings suggest that a local topological analysis of large documents might improve its global characterization. Most importantly, it was verified, as a proof of principle, that short texts can be analyzed with the methods and concepts of complex networks. As a consequence, the techniques described here can be extended in a straightforward fashion to analyze texts as time-varying complex networks.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2014-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1486",
        "title": "Design & Development of the Graphical User Interface for Sindhi Language",
        "authors": [
            "Imdad Ali Ismaili",
            "Zeeshan Bhatti",
            "Azhar Ali Shah"
        ],
        "abstract": "This paper describes the design and implementation of a Unicode-based GUISL (Graphical User Interface for Sindhi Language). The idea is to provide a software platform to the people of Sindh as well as Sindhi diasporas living across the globe to make use of computing for basic tasks such as editing, composition, formatting, and printing of documents in Sindhi by using GUISL. The implementation of the GUISL has been done in the Java technology to make the system platform independent. The paper describes several design issues of Sindhi GUI in the context of existing software tools and technologies and explains how mapping and concatenation techniques have been employed to achieve the cursive shape of Sindhi script.\n    ",
        "submission_date": "2014-01-07T00:00:00",
        "last_modified_date": "2014-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2258",
        "title": "Assessing Wikipedia-Based Cross-Language Retrieval Models",
        "authors": [
            "Benjamin Roth"
        ],
        "abstract": "This work compares concept models for cross-language retrieval: First, we adapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents. Experiments with different weighting schemes show that a weighting method favoring documents of similar length in both language sides gives best results. Considering that both monolingual and multilingual Latent Dirichlet Allocation (LDA) behave alike when applied for such documents, we use a training corpus built on Wikipedia where all documents are length-normalized and obtain improvements over previously reported scores for LDA. Another focus of our work is on model combination. For this end we include Explicit Semantic Analysis (ESA) in the experiments. We observe that ESA is not competitive with LDA in a query based retrieval task on CLEF 2000 data. The combination of machine translation with concept models increased performance by 21.1% map in comparison to machine translation alone. Machine translation relies on parallel corpora, which may not be available for many language pairs. We further explore how much cross-lingual information can be carried over by a specific information source in Wikipedia, namely linked text. The best results are obtained using a language modeling approach, entirely without information from parallel corpora. The need for smoothing raises interesting questions on soundness and efficiency. Link models capture only a certain kind of information and suggest weighting schemes to emphasize particular words. For a combined model, another interesting question is therefore how to integrate different weighting schemes. Using a very simple combination scheme, we obtain results that compare favorably to previously reported results on the CLEF 2000 dataset.\n    ",
        "submission_date": "2014-01-10T00:00:00",
        "last_modified_date": "2014-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2618",
        "title": "Sentiment Analysis Using Collaborated Opinion Mining",
        "authors": [
            "Deepali Virmani",
            "Vikrant Malhotra",
            "Ridhi Tyagi"
        ],
        "abstract": "Opinion mining and Sentiment analysis have emerged as a field of study since the widespread of World Wide Web and internet. Opinion refers to extraction of those lines or phrase in the raw and huge data which express an opinion. Sentiment analysis on the other hand identifies the polarity of the opinion being extracted. In this paper we propose the sentiment analysis in collaboration with opinion extraction, summarization, and tracking the records of the students. The paper modifies the existing algorithm in order to obtain the collaborated opinion about the students. The resultant opinion is represented as very high, high, moderate, low and very low. The paper is based on a case study where teachers give their remarks about the students and by applying the proposed sentiment analysis algorithm the opinion is extracted and represented.\n    ",
        "submission_date": "2014-01-12T00:00:00",
        "last_modified_date": "2014-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2851",
        "title": "Statistical Analysis based Hypothesis Testing Method in Biological Knowledge Discovery",
        "authors": [
            "Md. Naseef-Ur-Rahman Chowdhury",
            "Suvankar Paul",
            "Kazi Zakia Sultana"
        ],
        "abstract": "The correlation and interactions among different biological entities comprise the biological system. Although already revealed interactions contribute to the understanding of different existing systems, researchers face many questions everyday regarding inter-relationships among entities. Their queries have potential role in exploring new relations which may open up a new area of investigation. In this paper, we introduce a text mining based method for answering the biological queries in terms of statistical computation such that researchers can come up with new knowledge discovery. It facilitates user to submit their query in natural linguistic form which can be treated as hypothesis. Our proposed approach analyzes the hypothesis and measures the p-value of the hypothesis with respect to the existing literature. Based on the measured value, the system either accepts or rejects the hypothesis from statistical point of view. Moreover, even it does not find any direct relationship among the entities of the hypothesis, it presents a network to give an integral overview of all the entities through which the entities might be related. This is also congenial for the researchers to widen their view and thus think of new hypothesis for further investigation. It assists researcher to get a quantitative evaluation of their assumptions such that they can reach a logical conclusion and thus aids in relevant re-searches of biological knowledge discovery. The system also provides the researchers a graphical interactive interface to submit their hypothesis for assessment in a more convenient way.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3488",
        "title": "Content Modeling Using Latent Permutations",
        "authors": [
            "Harr Chen",
            "S.R.K. Branavan",
            "Regina Barzilay",
            "David R. Karger"
        ],
        "abstract": "We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3510",
        "title": "Improving Performance Of English-Hindi Cross Language Information Retrieval Using Transliteration Of Query Terms",
        "authors": [
            "Saurabh Varshney",
            "Jyoti Bajpai"
        ],
        "abstract": "The main issue in Cross Language Information Retrieval (CLIR) is the poor performance of retrieval in terms of average precision when compared to monolingual retrieval performance. The main reasons behind poor performance of CLIR are mismatching of query terms, lexical ambiguity and un-translated query terms. The existing problems of CLIR are needed to be addressed in order to increase the performance of the CLIR system. In this paper, we are putting our effort to solve the given problem by proposed an algorithm for improving the performance of English-Hindi CLIR system. We used all possible combination of Hindi translated query using transliteration of English query terms and choosing the best query among them for retrieval of documents. The experiment is performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets. The experimental result show that the proposed approach gives better performance of English-Hindi CLIR system and also helps in overcoming existing problems and outperforms the existing English-Hindi CLIR system in terms of average precision.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3908",
        "title": "Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity",
        "authors": [
            "Ricardo Ribeiro",
            "David Martins de Matos"
        ],
        "abstract": "In automatic summarization, centrality-as-relevance means that the most important content of an information source, or a collection of information sources, corresponds to the most central passages, considering a representation where such notion makes sense (graph, spatial, etc.). We assess the main paradigms, and introduce a new centrality-based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content. Geometric proximity is used to compute semantic relatedness. Centrality (relevance) is determined by considering the whole input source (and not only local information), and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized. The method consists in creating, for each passage of the input source, a support set consisting only of the most semantically related passages. Then, the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets. This model produces extractive summaries that are generic, and language- and domain-independent. Thorough automatic evaluation shows that the method achieves state-of-the-art performance, both in written text, and automatically transcribed speech summarization, including when compared to considerably more complex approaches.\n\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4603",
        "title": "Semantic Similarity Measures Applied to an Ontology for Human-Like Interaction",
        "authors": [
            "Esperanza Albacete",
            "Javier Calle",
            "Elena Castro",
            "Dolores Cuadra"
        ],
        "abstract": "The focus of this paper is the calculation of similarity between two concepts from an ontology for a Human-Like Interaction system. In order to facilitate this calculation, a similarity function is proposed based on five dimensions (sort, compositional, essential, restrictive and descriptive) constituting the structure of ontological knowledge. The paper includes a proposal for computing a similarity function for each dimension of knowledge. Later on, the similarity values obtained are weighted and aggregated to obtain a global similarity measure. In order to calculate those weights associated to each dimension, four training methods have been proposed. The training methods differ in the element to fit: the user, concepts or pairs of concepts, and a hybrid approach. For evaluating the proposal, the knowledge base was fed from WordNet and extended by using a knowledge editing toolkit (Cognos). The evaluation of the proposal is carried out through the comparison of system responses with those given by human test subjects, both providing a measure of the soundness of the procedure and revealing ways in which the proposal may be improved.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4634",
        "title": "The Capacity of String-Replication Systems",
        "authors": [
            "Farzad Farnoud",
            "Moshe Schwartz",
            "Jehoshua Bruck"
        ],
        "abstract": "It is known that the majority of the human genome consists of repeated sequences. Furthermore, it is believed that a significant part of the rest of the genome also originated from repeated sequences and has mutated to its current form. In this paper, we investigate the possibility of constructing an exponentially large number of sequences from a short initial sequence and simple replication rules, including those resembling genomic replication processes. In other words, our goal is to find out the capacity, or the expressive power, of these string-replication systems. Our results include exact capacities, and bounds on the capacities, of four fundamental string-replication systems.\n    ",
        "submission_date": "2014-01-19T00:00:00",
        "last_modified_date": "2014-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4994",
        "title": "A Review of Verbal and Non-Verbal Human-Robot Interactive Communication",
        "authors": [
            "Nikolaos Mavridis"
        ],
        "abstract": "In this paper, an overview of human-robot interactive communication is presented, covering verbal as well as non-verbal aspects of human-robot interaction. Following a historical introduction, and motivation towards fluid human-robot communication, ten desiderata are proposed, which provide an organizational axis both of recent as well as of future research on human-robot communication. Then, the ten desiderata are examined in detail, culminating to a unifying discussion, and a forward-looking conclusion.\n    ",
        "submission_date": "2014-01-20T00:00:00",
        "last_modified_date": "2014-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5389",
        "title": "Which Clustering Do You Want? Inducing Your Ideal Clustering with Minimal Feedback",
        "authors": [
            "Sajib Dasgupta",
            "Vincent Ng"
        ],
        "abstract": "While traditional research on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the authors mood, gender, age, or sentiment. Without knowing the users intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires. To address the problem of clustering documents along the user-desired dimension, previous work has focused on learning a similarity metric from data manually annotated with the users intention or having a human construct a feature space in an interactive manner during the clustering process. With the goal of reducing reliance on human knowledge for fine-tuning the similarity function or selecting the relevant features required by these approaches, we propose a novel active clustering algorithm, which allows a user to easily select the dimension along which she wants to cluster the documents by inspecting only a small number of words. We demonstrate the viability of our algorithm on a variety of commonly-used sentiment datasets.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6169",
        "title": "Parsimonious Topic Models with Salient Word Discovery",
        "authors": [
            "Hossein Soleimani",
            "David J. Miller"
        ],
        "abstract": "We propose a parsimonious topic model for text corpora. In related models such as Latent Dirichlet Allocation (LDA), all words are modeled topic-specifically, even though many words occur with similar frequencies across different topics. Our modeling determines salient words for each topic, which have topic-specific probabilities, with the rest explained by a universal shared model. Further, in LDA all topics are in principle present in every document. By contrast our model gives sparse topic representation, determining the (small) subset of relevant topics for each document. We derive a Bayesian Information Criterion (BIC), balancing model complexity and goodness of fit. Here, interestingly, we identify an effective sample size and corresponding penalty specific to each parameter type in our model. We minimize BIC to jointly determine our entire model -- the topic-specific words, document-specific topics, all model parameter values, {\\it and} the total number of topics -- in a wholly unsupervised fashion. Results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels, compared to LDA and to a model designed to incorporate sparsity.\n    ",
        "submission_date": "2014-01-22T00:00:00",
        "last_modified_date": "2014-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6427",
        "title": "Towards Unsupervised Learning of Temporal Relations between Events",
        "authors": [
            "Seyed Abolghasem Mirroshandel",
            "Gholamreza Ghassem-Sani"
        ],
        "abstract": "Automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as Question Answering, Information Extraction, and Summarization. Since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of \"one type of temporal relation per discourse, it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. The second proposed method for temporal relation extraction is based on the expectation maximization (EM) algorithm. Within EM, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. We think that the experimental results of our EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6574",
        "title": "Category theory, logic and formal linguistics: some connections, old and new",
        "authors": [
            "Jean Gillibert",
            "Christian Retor\u00e9"
        ],
        "abstract": "We seize the opportunity of the publication of selected papers from the \\emph{Logic, categories, semantics} workshop in the \\emph{Journal of Applied Logic} to survey some current trends in logic, namely intuitionistic and linear type theories, that interweave categorical, geometrical and computational considerations. We thereafter present how these rich logical frameworks can model the way language conveys meaning.\n    ",
        "submission_date": "2014-01-25T00:00:00",
        "last_modified_date": "2014-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6984",
        "title": "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN",
        "authors": [
            "Yajie Miao"
        ],
        "abstract": "The Kaldi toolkit is becoming popular for constructing automated speech recognition (ASR) systems. Meanwhile, in recent years, deep neural networks (DNNs) have shown state-of-the-art performance on various ASR tasks. This document describes our open-source recipes to implement fully-fledged DNN acoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning toolkit developed under the Theano environment. Using these recipes, we can build up multiple systems including DNN hybrid systems, convolutional neural network (CNN) systems and bottleneck feature systems. These recipes are directly based on the Kaldi Switchboard 110-hour setup. However, adapting them to new datasets is easy to achieve.\n    ",
        "submission_date": "2014-01-27T00:00:00",
        "last_modified_date": "2014-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0556",
        "title": "Generating Extractive Summaries of Scientific Paradigms",
        "authors": [
            "Vahed Qazvinian",
            "Dragomir R. Radev",
            "Saif M. Mohammad",
            "Bonnie Dorr",
            "David Zajic",
            "Michael Whidby",
            "Taesun Moon"
        ],
        "abstract": "Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature.  We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments  to summarize a set of papers, which cover the same scientific topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1128",
        "title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition",
        "authors": [
            "Ha\u015fim Sak",
            "Andrew Senior",
            "Fran\u00e7oise Beaufays"
        ],
        "abstract": "Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.\n    ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1668",
        "title": "Evaluation of YTEX and MetaMap for clinical concept recognition",
        "authors": [
            "John David Osborne",
            "Binod Gyawali",
            "Thamar Solorio"
        ],
        "abstract": "We used MetaMap and YTEX as a basis for the construc- tion of two separate systems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], the recognition of clinical concepts. No modifications were directly made to these systems, but output concepts were filtered using stop concepts, stop concept text and UMLS semantic type. Con- cept boundaries were also adjusted using a small collection of rules to increase precision on the strict task. Overall MetaMap had better per- formance than YTEX on the strict task, primarily due to a 20% perfor- mance improvement in precision. In the relaxed task YTEX had better performance in both precision and recall giving it an overall F-Score 4.6% higher than MetaMap on the test data. Our results also indicated a 1.3% higher accuracy for YTEX in UMLS CUI mapping.\n    ",
        "submission_date": "2014-02-07T00:00:00",
        "last_modified_date": "2014-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1939",
        "title": "Maximum Entropy, Word-Frequency, Chinese Characters, and Multiple Meanings",
        "authors": [
            "Xiao-Yong Yan",
            "Petter Minnhagen"
        ],
        "abstract": "The word-frequency distribution of a text written by an author is well accounted for by a maximum entropy distribution, the RGF (random group formation)-prediction. The RGF-distribution is completely determined by the a priori values of the total number of words in the text (M), the number of distinct words (N) and the number of repetitions of the most common word (k_max). It is here shown that this maximum entropy prediction also describes a text written in Chinese characters. In particular it is shown that although the same Chinese text written in words and Chinese characters have quite differently shaped distributions, they are nevertheless both well predicted by their respective three a priori characteristic values. It is pointed out that this is analogous to the change in the shape of the distribution when translating a given text to another language. Another consequence of the RGF-prediction is that taking a part of a long text will change the input parameters (M, N, k_max) and consequently also the shape of the frequency distribution. This is explicitly confirmed for texts written in Chinese characters. Since the RGF-prediction has no system-specific information beyond the three a priori values (M, N, k_max), any specific language characteristic has to be sought in systematic deviations from the RGF-prediction and the measured frequencies. One such systematic deviation is identified and, through a statistical information theoretical argument and an extended RGF-model, it is proposed that this deviation is caused by multiple meanings of Chinese characters. The effect is stronger for Chinese characters than for Chinese words. The relation between Zipf's law, the Simon-model for texts and the present results are discussed.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2015-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2427",
        "title": "An evaluation of keyword extraction from online communication for the characterisation of social relations",
        "authors": [
            "Jan Hauffa",
            "Tobias Lichtenberg",
            "Georg Groh"
        ],
        "abstract": "The set of interpersonal relationships on a social network service or a similar online community is usually highly heterogenous. The concept of tie strength captures only one aspect of this heterogeneity. Since the unstructured text content of online communication artefacts is a salient source of information about a social relationship, we investigate the utility of keywords extracted from the message body as a representation of the relationship's characteristics as reflected by the conversation topics. Keyword extraction is performed using standard natural language processing methods. Communication data and human assessments of the extracted keywords are obtained from Facebook users via a custom application. The overall positive quality assessment provides evidence that the keywords indeed convey relevant information about the relationship.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2562",
        "title": "\u00c9tude cognitive des processus de construction d'une requ\u00eate dans un syst\u00e8me de gestion de connaissances m\u00e9dicales",
        "authors": [
            "Nathalie Chaignaud",
            "Val\u00e9rie Delavigne",
            "Maryvonne Holzem",
            "Jean-Philippe Kotowicz",
            "Alain Loisel"
        ],
        "abstract": "This article presents the Cogni-CISMeF project, which aims at improving medical information search in the CISMeF system (Catalog and Index of French-language health resources) by including a conversational agent to interact with the user in natural language. To study the cognitive processes involved during the information search, a bottom-up methodology was adopted. Experimentation has been set up to obtain human dialogs between a user (playing the role of patient) dealing with medical information search and a CISMeF expert refining the request. The analysis of these dialogs underlined the use of discursive evidence: vocabulary, reformulation, implicit or explicit expression of user intentions, conversational sequences, etc. A model of artificial agent is proposed. It leads the user in its information search by proposing to him examples, assistance and choices. This model was implemented and integrated in the CISMeF system. \n----\nCet article d\u00e9crit le projet Cogni-CISMeF qui propose un module de dialogue Homme-Machine \u00e0 int\u00e9grer dans le syst\u00e8me d'indexation de connaissances m\u00e9dicales CISMeF (Catalogue et Index des Sites M\u00e9dicaux Francophones). Nous avons adopt\u00e9 une d\u00e9marche de mod\u00e9lisation cognitive en proc\u00e9dant \u00e0 un recueil de corpus de dialogues entre un utilisateur (jouant le r\u00f4le d'un patient) d\u00e9sirant une information m\u00e9dicale et un expert CISMeF af inant cette demande pour construire la requ\u00eate. Nous avons analys\u00e9 la structure des dialogues ainsi obtenus et avons \u00e9tudi\u00e9 un certain nombre d'indices discursifs : vocabulaire employ\u00e9, marques de reformulation, commentaires m\u00e9ta et \u00e9pilinguistiques, expression implicite ou explicite des intentions de l'utilisateur, encha\u00eenement conversationnel, etc. De cette analyse, nous avons construit un mod\u00e8le d'agent artificiel dot\u00e9 de capacit\u00e9s cognitives capables d'aider l'utilisateur dans sa t\u00e2che de recherche d'information. Ce mod\u00e8le a \u00e9t\u00e9 impl\u00e9ment\u00e9 et int\u00e9gr\u00e9 dans le syst\u00e8me CISMeF.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3891",
        "title": "Performance Evaluation of Machine Learning Classifiers in Sentiment Mining",
        "authors": [
            "Vinodhini G Chandrasekaran RM"
        ],
        "abstract": "In recent years, the use of machine learning classifiers is of great value in solving a variety of problems in text classification. Sentiment mining is a kind of text classification in which, messages are classified according to sentiment orientation such as positive or negative. This paper extends the idea of evaluating the performance of various classifiers to show their effectiveness in sentiment mining of online product reviews. The product reviews are collected from Amazon reviews. To evaluate the performance of classifiers various evaluation methods like random sampling, linear sampling and bootstrap sampling are used. Our results shows that support vector machine with bootstrap sampling method outperforms others classifiers and sampling methods in terms of misclassification rate.\n    ",
        "submission_date": "2014-02-17T00:00:00",
        "last_modified_date": "2014-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4259",
        "title": "Extracting Networks of Characters and Places from Written Works with CHAPLIN",
        "authors": [
            "Roberto Marazzato",
            "Amelia Carolina Sparavigna"
        ],
        "abstract": "We are proposing a tool able to gather information on social networks from narrative texts. Its name is CHAPLIN, CHAracters and PLaces Interaction Network, implemented in ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4802",
        "title": "Ambiguity in language networks",
        "authors": [
            "Ricard V. Sol\u00e9",
            "Lu\u00eds F. Seoane"
        ],
        "abstract": "Human language defines the most complex outcomes of evolution. The emergence of such an elaborated form of communication allowed humans to create extremely structured societies and manage symbols at different levels including, among others, semantics. All linguistic levels have to deal with an astronomic combinatorial potential that stems from the recursive nature of languages. This recursiveness is indeed a key defining trait. However, not all words are equally combined nor frequent. In breaking the symmetry between less and more often used and between less and more meaning-bearing units, universal scaling laws arise. Such laws, common to all human languages, appear on different stages from word inventories to networks of interacting words. Among these seemingly universal traits exhibited by language networks, ambiguity appears to be a specially relevant component. Ambiguity is avoided in most computational approaches to language processing, and yet it seems to be a crucial element of language architecture. Here we review the evidence both from language network architecture and from theoretical reasonings based on a least effort argument. Ambiguity is shown to play an essential role in providing a source of language efficiency, and is likely to be an inevitable byproduct of network growth.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6010",
        "title": "Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social Media",
        "authors": [
            "Linhong Zhu",
            "Aram Galstyan",
            "James Cheng",
            "Kristina Lerman"
        ],
        "abstract": "The growing popularity of social media (e.g, Twitter) allows users to easily share information with each other and influence others by expressing their own sentiments on various subjects. In this work, we propose an unsupervised \\emph{tri-clustering} framework, which analyzes both user-level and tweet-level sentiments through co-clustering of a tripartite graph. A compelling feature of the proposed framework is that the quality of sentiment clustering of tweets, users, and features can be mutually improved by joint clustering. We further investigate the evolution of user-level sentiments and latent feature vectors in an online framework and devise an efficient online algorithm to sequentially update the clustering of tweets, users and features with newly arrived data. The online framework not only provides better quality of both dynamic user-level and tweet-level sentiment analysis, but also improves the computational and storage efficiency. We verified the effectiveness and efficiency of the proposed approaches on the November 2012 California ballot Twitter data.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6238",
        "title": "Improving Collaborative Filtering based Recommenders using Topic Modelling",
        "authors": [
            "Jobin Wilson",
            "Santanu Chaudhury",
            "Brejesh Lall",
            "Prateek Kapadia"
        ],
        "abstract": "Standard Collaborative Filtering (CF) algorithms make use of interactions between users and items in the form of implicit or explicit ratings alone for generating recommendations. Similarity among users or items is calculated purely based on rating overlap in this case,without considering explicit properties of users or items involved, limiting their applicability in domains with very sparse rating spaces. In many domains such as movies, news or electronic commerce recommenders, considerable contextual data in text form describing item properties is available along with the rating data, which could be utilized to improve recommendation ",
        "submission_date": "2014-02-25T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6690",
        "title": "Why Are You More Engaged? Predicting Social Engagement from Word Use",
        "authors": [
            "Jalal Mahmud",
            "Jilin Chen",
            "Jeffrey Nichols"
        ],
        "abstract": "We present a study to analyze how word use can predict social engagement behaviors such as replies and retweets in Twitter. We compute psycholinguistic category scores from word usage, and investigate how people with different scores exhibited different reply and retweet behaviors on Twitter. We also found psycholinguistic categories that show significant correlations with such social engagement behaviors. In addition, we have built predictive models of replies and retweets from such psycholinguistic category based features. Our experiments using a real world dataset collected from Twitter validates that such predictions can be done with reasonable accuracy.\n    ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6764",
        "title": "A method to identify potential ambiguous Malay words through Ambiguity Attributes mapping: An exploratory Study",
        "authors": [
            "Hazlina Haron",
            "Abdul Azim Abd. Ghani"
        ],
        "abstract": "We describe here a methodology to identify a list of ambiguous Malay words that are commonly being used in Malay documentations such as Requirement Specification. We compiled several relevant and appropriate requirement quality attributes and sentence rules from previous literatures and adopt it to come out with a set of ambiguity attributes that most suit Malay words. The extracted Malay ambiguous words (potential) are then being mapped onto the constructed ambiguity attributes to confirm their vagueness. The list is then verified by Malay linguist experts. This paper aims to identify a list of potential ambiguous words in Malay as an attempt to assist writers to avoid using the vague words while documenting Malay Requirement Specification as well as to any other related Malay documentation. The result of this study is a list of 120 potential ambiguous Malay words that could act as guidelines in writing Malay sentences\n    ",
        "submission_date": "2014-02-27T00:00:00",
        "last_modified_date": "2014-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6792",
        "title": "Information Evolution in Social Networks",
        "authors": [
            "Lada A. Adamic",
            "Thomas M. Lento",
            "Eytan Adar",
            "Pauline C. Ng"
        ],
        "abstract": "Social networks readily transmit information, albeit with less than perfect fidelity. We present a large-scale measurement of this imperfect information copying mechanism by examining the dissemination and evolution of thousands of memes, collectively replicated hundreds of millions of times in the online social network Facebook. The information undergoes an evolutionary process that exhibits several regularities. A meme's mutation rate characterizes the population distribution of its variants, in accordance with the Yule process. Variants further apart in the diffusion cascade have greater edit distance, as would be expected in an iterative, imperfect replication process. Some text sequences can confer a replicative advantage; these sequences are abundant and transfer \"laterally\" between different memes. Subpopulations of the social network can preferentially transmit a specific variant of a meme if the variant matches their beliefs or culture. Understanding the mechanism driving change in diffusing information has important implications for how we interpret and harness the information that reaches us through our social networks.\n    ",
        "submission_date": "2014-02-27T00:00:00",
        "last_modified_date": "2014-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0541",
        "title": "Representing, reasoning and answering questions about biological pathways - various applications",
        "authors": [
            "Saadat Anwar"
        ],
        "abstract": "Biological organisms are composed of numerous interconnected biochemical processes. Diseases occur when normal functionality of these processes is disrupted. Thus, understanding these biochemical processes and their interrelationships is a primary task in biomedical research and a prerequisite for diagnosing diseases, and drug development. Scientists studying these processes have identified various pathways responsible for drug metabolism, and signal transduction, etc.\n",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1252",
        "title": "Inducing Language Networks from Continuous Space Word Representations",
        "authors": [
            "Bryan Perozzi",
            "Rami Al-Rfou",
            "Vivek Kulkarni",
            "Steven Skiena"
        ],
        "abstract": "Recent advancements in unsupervised feature learning have developed powerful latent representations of words. However, it is still not clear what makes one representation better than another and how we can learn the ideal representation. Understanding the structure of latent spaces attained is key to any future advancement in unsupervised learning. In this work, we introduce a new view of continuous space word representations as language networks. We explore two techniques to create language networks from learned features by inducing them for two popular word representation methods and examining the properties of their resulting networks. We find that the induced networks differ from other methods of creating language networks, and that they contain meaningful community structure.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1310",
        "title": "AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based Assignments",
        "authors": [
            "M.A.C. Jiffriya",
            "M.A.C. Akmal Jahan",
            "R.G. Ragel",
            "S. Deegalla"
        ],
        "abstract": "Plagiarism is one of the growing issues in academia and is always a concern in Universities and other academic institutions. The situation is becoming even worse with the availability of ample resources on the web. This paper focuses on creating an effective and fast tool for plagiarism detection for text based electronic assignments. Our plagiarism detection tool named AntiPlag is developed using the tri-gram sequence matching technique. Three sets of text based assignments were tested by AntiPlag and the results were compared against an existing commercial plagiarism detection tool. AntiPlag showed better results in terms of false positives compared to the commercial tool due to the pre-processing steps performed in AntiPlag. In addition, to improve the detection latency, AntiPlag applies a data clustering technique making it four times faster than the commercial tool considered. AntiPlag could be used to isolate plagiarized text based assignments from non-plagiarised assignments easily. Therefore, we present AntiPlag, a fast and effective tool for plagiarism detection on text based electronic assignments.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1451",
        "title": "Real-Time Classification of Twitter Trends",
        "authors": [
            "Arkaitz Zubiaga",
            "Damiano Spina",
            "Raquel Mart\u00ednez",
            "V\u00edctor Fresno"
        ],
        "abstract": "Social media users give rise to social trends as they share about common interests, which can be triggered by different reasons. In this work, we explore the types of triggers that spark trends on Twitter, introducing a typology with following four types: 'news', 'ongoing events', 'memes', and 'commemoratives'. While previous research has analyzed trending topics in a long term, we look at the earliest tweets that produce a trend, with the aim of categorizing trends early on. This would allow to provide a filtered subset of trends to end users. We analyze and experiment with a set of straightforward language-independent features based on the social spread of trends to categorize them into the introduced typology. Our method provides an efficient way to accurately categorize trending topics without need of external data, enabling news organizations to discover breaking news in real-time, or to quickly identify viral memes that might enrich marketing decisions, among others. The analysis of social features also reveals patterns associated with each type of trend, such as tweets about ongoing events being shorter as many were likely sent from mobile devices, or memes having more retweets originating from a few trend-setters.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1618",
        "title": "Design a Persian Automated Plagiarism Detector (AMZPPD)",
        "authors": [
            "Maryam Mahmoodi",
            "Mohammad Mahmoodi Varnamkhasti"
        ],
        "abstract": "Currently there are lots of plagiarism detection approaches. But few of them implemented and adapted for Persian languages. In this paper, our work on designing and implementation of a plagiarism detection system based on pre-processing and NLP technics will be described. And the results of testing on a corpus will be presented.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2345",
        "title": "Home Location Identification of Twitter Users",
        "authors": [
            "Jalal Mahmud",
            "Jeffrey Nichols",
            "Clemens Drews"
        ],
        "abstract": "We present a new algorithm for inferring the home location of Twitter users at different granularities, including city, state, time zone or geographic region, using the content of users tweets and their tweeting behavior. Unlike existing approaches, our algorithm uses an ensemble of statistical and heuristic classifiers to predict locations and makes use of a geographic gazetteer dictionary to identify place-name entities. We find that a hierarchical classification approach, where time zone, state or geographic region is predicted first and city is predicted next, can improve prediction accuracy. We have also analyzed movement variations of Twitter users, built a classifier to predict whether a user was travelling in a certain period of time and use that to further improve the location detection accuracy. Experimental evidence suggests that our algorithm works well in practice and outperforms the best existing algorithms for predicting the home location of Twitter users.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3185",
        "title": "Sentiment Analysis by Using Fuzzy Logic",
        "authors": [
            "Md. Ansarul Haque"
        ],
        "abstract": "How could a product or service is reasonably evaluated by anyone in the shortest time? A million dollar question but it is having a simple answer: Sentiment analysis. Sentiment analysis is consumers review on products and services which helps both the producers and consumers (stakeholders) to take effective and efficient decision within a shortest period of time. Producers can have better knowledge of their products and services through the sentiment analysis (ex. positive and negative comments or consumers likes and dislikes) which will help them to know their products status (ex. product limitations or market status). Consumers can have better knowledge of their interested products and services through the sentiment analysis (ex. positive and negative comments or consumers likes and dislikes) which will help them to know their deserving products status (ex. product limitations or market status). For more specification of the sentiment values, fuzzy logic could be introduced. Therefore, sentiment analysis with the help of fuzzy logic (deals with reasoning and gives closer views to the exact sentiment values) will help the producers or consumers or any interested person for taking the effective decision according to their product or service interest.\n    ",
        "submission_date": "2014-03-13T00:00:00",
        "last_modified_date": "2014-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3460",
        "title": "Scalable and Robust Construction of Topical Hierarchies",
        "authors": [
            "Chi Wang",
            "Xueqing Liu",
            "Yanglei Song",
            "Jiawei Han"
        ],
        "abstract": "Automated generation of high-quality topical hierarchies for a text collection is a dream problem in knowledge engineering with many valuable applications. In this paper a scalable and robust algorithm is proposed for constructing a hierarchy of topics from a text collection. We divide and conquer the problem using a top-down recursive framework, based on a tensor orthogonal decomposition technique. We solve a critical challenge to perform scalable inference for our newly designed hierarchical topic model. Experiments with various real-world datasets illustrate its ability to generate robust, high-quality hierarchies efficiently. Our method reduces the time of construction by several orders of magnitude, and its robust feature renders it possible for users to interactively revise the hierarchy.\n    ",
        "submission_date": "2014-03-13T00:00:00",
        "last_modified_date": "2014-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4362",
        "title": "Concept Based vs. Pseudo Relevance Feedback Performance Evaluation for Information Retrieval System",
        "authors": [
            "Mohammed El Amine Abderrahim"
        ],
        "abstract": "This article evaluates the performance of two techniques for query reformulation in a system for information retrieval, namely, the concept based and the pseudo relevance feedback reformulation. The experiments performed on a corpus of Arabic text have allowed us to compare the contribution of these two reformulation techniques in improving the performance of an information retrieval system for Arabic texts.\n    ",
        "submission_date": "2014-03-18T00:00:00",
        "last_modified_date": "2014-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6173",
        "title": "Coherent Multi-Sentence Video Description with Variable Level of Detail",
        "authors": [
            "Anna Senina",
            "Marcus Rohrbach",
            "Wei Qiu",
            "Annemarie Friedrich",
            "Sikandar Amin",
            "Mykhaylo Andriluka",
            "Manfred Pinkal",
            "Bernt Schiele"
        ],
        "abstract": "Humans can easily describe what they see in a coherent way and at varying level of detail. However, existing approaches for automatic video description are mainly focused on single sentence generation and produce descriptions at a fixed level of detail. In this paper, we address both of these limitations: for a variable level of detail we produce coherent multi-sentence descriptions of complex videos. We follow a two-step approach where we first learn to predict a semantic representation (SR) from video and then generate natural language descriptions from the SR. To produce consistent multi-sentence descriptions, we model across-sentence consistency at the level of the SR by enforcing a consistent topic. We also contribute both to the visual recognition of objects proposing a hand-centric approach as well as to the robust generation of sentences using a word lattice. Human judges rate our multi-sentence descriptions as more readable, correct, and relevant than related work. To understand the difference between more detailed and shorter descriptions, we collect and analyze a video description corpus of three levels of detail.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6397",
        "title": "Evaluating topic coherence measures",
        "authors": [
            "Frank Rosner",
            "Alexander Hinneburg",
            "Michael R\u00f6der",
            "Martin Nettling",
            "Andreas Both"
        ],
        "abstract": "Topic models extract representative word sets - called topics - from word counts in documents without requiring any semantic annotations. Topics are not guaranteed to be well interpretable, therefore, coherence measures have been proposed to distinguish between good and bad topics. Studies of topic coherence so far are limited to measures that score pairs of individual words. For the first time, we include coherence measures from scientific philosophy that score pairs of more complex word subsets and apply them to topic scoring.\n    ",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2014-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0850",
        "title": "Application of Ontologies in Identifying Requirements Patterns in Use Cases",
        "authors": [
            "Rui Couto",
            "Ant\u00f3nio Nestor Ribeiro",
            "Jos\u00e9 Creissac Campos"
        ],
        "abstract": "Use case specifications have successfully been used for requirements description.  They allow  joining, in the same modeling space, the expectations of the stakeholders as well as the needs of the software engineer and analyst involved in the process. While use cases are not meant to describe a system's implementation, by formalizing their description we are able to extract implementation relevant information from them. More specifically, we are interested in identifying requirements patterns (common requirements with typical implementation solutions) in support for a requirements based software development approach. In the paper we propose the transformation of Use Case descriptions expressed in a Controlled Natural Language into an ontology expressed in the Web Ontology Language (OWL). OWL's query engines can then be used to identify requirements patterns expressed as queries over the ontology. We describe a tool that we have developed to support the approach and provide an example of usage.\n    ",
        "submission_date": "2014-04-03T00:00:00",
        "last_modified_date": "2014-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1521",
        "title": "Exploring the power of GPU's for training Polyglot language models",
        "authors": [
            "Vivek Kulkarni",
            "Rami Al-Rfou'",
            "Bryan Perozzi",
            "Steven Skiena"
        ],
        "abstract": "One of the major research trends currently is the evolution of heterogeneous parallel computing. GP-GPU computing is being widely used and several applications have been designed to exploit the massive parallelism that GP-GPU's have to offer. While GPU's have always been widely used in areas of computer vision for image processing, little has been done to investigate whether the massive parallelism provided by GP-GPU's can be utilized effectively for Natural Language Processing(NLP) tasks. In this work, we investigate and explore the power of GP-GPU's in the task of learning language models. More specifically, we investigate the performance of training Polyglot language models using deep belief neural networks. We evaluate the performance of training the model on the GPU and present optimizations that boost the performance on the ",
        "submission_date": "2014-04-05T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3026",
        "title": "On the Ground Validation of Online Diagnosis with Twitter and Medical Records",
        "authors": [
            "Todd Bodnar",
            "Victoria C Barclay",
            "Nilam Ram",
            "Conrad S Tucker",
            "Marcel Salath\u00e9"
        ],
        "abstract": "Social media has been considered as a data source for tracking disease. However, most analyses are based on models that prioritize strong correlation with population-level disease rates over determining whether or not specific individual users are actually sick. Taking a different approach, we develop a novel system for social-media based disease detection at the individual level using a sample of professionally diagnosed individuals. Specifically, we develop a system for making an accurate influenza diagnosis based on an individual's publicly available Twitter data. We find that about half (17/35 = 48.57%) of the users in our sample that were sick explicitly discuss their disease on Twitter. By developing a meta classifier that combines text analysis, anomaly detection, and social network analysis, we are able to diagnose an individual with greater than 99% accuracy even if she does not discuss her health.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3610",
        "title": "Targeting HIV-related Medication Side Effects and Sentiment Using Twitter Data",
        "authors": [
            "Cosme Adrover",
            "Todd Bodnar",
            "Marcel Salathe"
        ],
        "abstract": "We present a descriptive analysis of Twitter data. Our study focuses on extracting the main side effects associated with HIV treatments. The crux of our work was the identification of personal tweets referring to HIV. We summarize our results in an infographic aimed at the general public. In addition, we present a measure of user sentiment based on hand-rated tweets.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3959",
        "title": "Is it morally acceptable for a system to lie to persuade me?",
        "authors": [
            "Marco Guerini",
            "Fabio Pianesi",
            "Oliviero Stock"
        ],
        "abstract": "Given the fast rise of increasingly autonomous artificial agents and robots, a key acceptability criterion will be the possible moral implications of their actions. In particular, intelligent persuasive systems (systems designed to influence humans via communication) constitute a highly sensitive topic because of their intrinsically social nature. Still, ethical studies in this area are rare and tend to focus on the output of the required action. Instead, this work focuses on the persuasive acts themselves (e.g. \"is it morally acceptable that a machine lies or appeals to the emotions of a person to persuade her, even if for a good end?\"). Exploiting a behavioral approach, based on human assessment of moral dilemmas -- i.e. without any prior assumption of underlying ethical theories -- this paper reports on a set of experiments. These experiments address the type of persuader (human or machine), the strategies adopted (purely argumentative, appeal to positive emotions, appeal to negative emotions, lie) and the circumstances. Findings display no differences due to the agent, mild acceptability for persuasion and reveal that truth-conditional reasoning (i.e. argument validity) is a significant dimension affecting subjects' judgment. Some implications for the design of intelligent persuasive systems are discussed.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4606",
        "title": "How Many Topics? Stability Analysis for Topic Models",
        "authors": [
            "Derek Greene",
            "Derek O'Callaghan",
            "P\u00e1draig Cunningham"
        ],
        "abstract": "Topic modeling refers to the task of discovering the underlying thematic structure in a text corpus, where the output is commonly presented as a report of the top terms appearing in each topic. Despite the diversity of topic modeling algorithms that have been proposed, a common challenge in successfully applying these techniques is the selection of an appropriate number of topics for a given corpus. Choosing too few topics will produce results that are overly broad, while choosing too many will result in the \"over-clustering\" of a corpus into many small, highly-similar topics. In this paper, we propose a term-centric stability analysis strategy to address this issue, the idea being that a model with an appropriate number of topics will be more robust to perturbations in the data. Using a topic modeling approach based on matrix factorization, evaluations performed on a range of corpora show that this strategy can successfully guide the model selection process.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4935",
        "title": "Opinion Mining In Hindi Language: A Survey",
        "authors": [
            "Richa Sharma",
            "Shweta Nigam",
            "Rekha Jain"
        ],
        "abstract": "Opinions are very important in the life of human beings. These Opinions helped the humans to carry out the decisions. As the impact of the Web is increasing day by day, Web documents can be seen as a new source of opinion for human beings. Web contains a huge amount of information generated by the users through blogs, forum entries, and social networking websites and so on To analyze this large amount of information it is required to develop a method that automatically classifies the information available on the Web. This domain is called Sentiment Analysis and Opinion Mining. Opinion Mining or Sentiment Analysis is a natural language processing task that mine information from various text forms such as reviews, news, and blogs and classify them on the basis of their polarity as positive, negative or neutral. But, from the last few years, enormous increase has been seen in Hindi language on the Web. Research in opinion mining mostly carried out in English language but it is very important to perform the opinion mining in Hindi language also as large amount of information in Hindi is also available on the Web. This paper gives an overview of the work that has been done Hindi language.\n    ",
        "submission_date": "2014-04-19T00:00:00",
        "last_modified_date": "2014-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5372",
        "title": "Linking Geographic Vocabularies through WordNet",
        "authors": [
            "Andrea Ballatore",
            "Michela Bertolotto",
            "David C. Wilson"
        ],
        "abstract": "The linked open data (LOD) paradigm has emerged as a promising approach to structuring and sharing geospatial information. One of the major obstacles to this vision lies in the difficulties found in the automatic integration between heterogeneous vocabularies and ontologies that provides the semantic backbone of the growing constellation of open geo-knowledge bases. In this article, we show how to utilize WordNet as a semantic hub to increase the integration of LOD. With this purpose in mind, we devise Voc2WordNet, an unsupervised mapping technique between a given vocabulary and WordNet, combining intensional and extensional aspects of the geographic terms. Voc2WordNet is evaluated against a sample of human-generated alignments with the OpenStreetMap (OSM) Semantic Network, a crowdsourced geospatial resource, and the GeoNames ontology, the vocabulary of a large digital gazetteer. These empirical results indicate that the approach can obtain high precision and recall.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0546",
        "title": "Kaggle LSHTC4 Winning Solution",
        "authors": [
            "Antti Puurula",
            "Jesse Read",
            "Albert Bifet"
        ],
        "abstract": "Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores. This document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package. A package omitting precomputed result files is also distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies.\n    ",
        "submission_date": "2014-05-03T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0941",
        "title": "Towards a Benchmark of Natural Language Arguments",
        "authors": [
            "Elena Cabrio",
            "Serena Villata"
        ],
        "abstract": "The connections among natural language processing and argumentation theory are becoming stronger in the latest years, with a growing amount of works going in this direction, in different scenarios and applying heterogeneous techniques. In this paper, we present two datasets we built to cope with the combination of the Textual Entailment framework and bipolar abstract argumentation. In our approach, such datasets are used to automatically identify through a Textual Entailment system the relations among the arguments (i.e., attack, support), and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1379",
        "title": "Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device",
        "authors": [
            "Ramin Pichevar",
            "Jason Wung",
            "Daniele Giacobello",
            "Joshua Atkins"
        ],
        "abstract": "This paper addresses the challenging scenario for the distant-talking control of a music playback device, a common portable speaker with four small loudspeakers in close proximity to one microphone. The user controls the device through voice, where the speech-to-music ratio can be as low as -30 dB during music playback. We propose a speech enhancement front-end that relies on known robust methods for echo cancellation, double-talk detection, and noise suppression, as well as a novel adaptive quasi-binary mask that is well suited for speech recognition. The optimization of the system is then formulated as a large scale nonlinear programming problem where the recognition rate is maximized and the optimal values for the system parameters are found through a genetic algorithm. We validate our methodology by testing over the TIMIT database for different music playback levels and noise types. Finally, we show that the proposed front-end allows a natural interaction with the device for limited-vocabulary voice commands.\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1438",
        "title": "The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter",
        "authors": [
            "Chenhao Tan",
            "Lillian Lee",
            "Bo Pang"
        ],
        "abstract": "Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craft the message. Does it actually matter? While there has been extensive prior work looking into predicting popularity of social-media content, the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic. To control for these confounding factors, we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording. Given such pairs, we ask: which version attracts more retweets? This turns out to be a more difficult task than predicting popular topics. Still, humans can answer this question better than chance (but far from perfectly), and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data.\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2048",
        "title": "Learning Alternative Name Spellings",
        "authors": [
            "Jeffrey Sukharev",
            "Leonid Zhukov",
            "Alexandrin Popescul"
        ],
        "abstract": "Name matching is a key component of systems for entity resolution or record linkage. Alternative spellings of the same names are a com- mon occurrence in many applications. We use the largest collection of genealogy person records in the world together with user search query logs to build name matching models. The procedure for building a crowd-sourced training set is outlined together with the presentation of our method. We cast the problem of learning alternative spellings as a machine translation problem at the character level. We use in- formation retrieval evaluation methodology to show that this method substantially outperforms on our data a number of standard well known phonetic and string similarity methods in terms of precision and re- call. Additionally, we rigorously compare the performance of standard methods when compared with each other. Our result can lead to a significant practical impact in entity resolution applications.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2014-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2386",
        "title": "Predicting Central Topics in a Blog Corpus from a Networks Perspective",
        "authors": [
            "Srayan Datta"
        ],
        "abstract": "In today's content-centric Internet, blogs are becoming increasingly popular and important from a data analysis perspective. According to Wikipedia, there were over 156 million public blogs on the Internet as of February 2011. Blogs are a reflection of our contemporary society. The contents of different blog posts are important from social, psychological, economical and political perspectives. Discovery of important topics in the blogosphere is an area which still needs much exploring. We try to come up with a procedure using probabilistic topic modeling and network centrality measures which identifies the central topics in a blog corpus.\n    ",
        "submission_date": "2014-05-10T00:00:00",
        "last_modified_date": "2014-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2584",
        "title": "Sentiment Analysis: A Survey",
        "authors": [
            "Rahul Tejwani"
        ],
        "abstract": "Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. Mining opinions expressed in the user generated content is a challenging yet practically very useful problem. This survey would cover various approaches and methodology used in Sentiment Analysis and Opinion Mining in general. The focus would be on Internet text like, Product review, tweets and other social media.\n    ",
        "submission_date": "2014-05-11T00:00:00",
        "last_modified_date": "2014-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3272",
        "title": "Fast and Fuzzy Private Set Intersection",
        "authors": [
            "Nicholas Kersting"
        ],
        "abstract": "Private Set Intersection (PSI) is usually implemented as a sequence of encryption rounds between pairs of users, whereas the present work implements PSI in a simpler fashion: each set only needs to be encrypted once, after which each pair of users need only one ordinary set comparison. This is typically orders of magnitude faster than ordinary PSI at the cost of some ``fuzziness\" in the matching, which may nonetheless be tolerable or even desirable. This is demonstrated in the case where the sets consist of English words processed with WordNet.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3539",
        "title": "Pattern Recognition in Narrative: Tracking Emotional Expression in Context",
        "authors": [
            "Fionn Murtagh",
            "Adam Ganz"
        ],
        "abstract": "Using geometric data analysis, our objective is the analysis of narrative, with narrative of emotion being the focus in this work. The following two principles for analysis of emotion inform our work. Firstly, emotion is revealed not as a quality in its own right but rather through interaction. We study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the 3-way relationship of Emma, Charles and Rodolphe in the novel {\\em Madame Bovary}. Secondly, emotion, that is expression of states of mind of subjects, is formed and evolves within the narrative that expresses external events and (personal, social, physical) context. In addition to the analysis methodology with key aspects that are innovative, the input data used is crucial. We use, firstly, dialogue, and secondly, broad and general description that incorporates dialogue. In a follow-on study, we apply our unsupervised narrative mapping to data streams with very low emotional expression. We map the narrative of Twitter streams. Thus we demonstrate map analysis of general narratives.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4918",
        "title": "Fighting Authorship Linkability with Crowdsourcing",
        "authors": [
            "Mishari Almishari",
            "Ekin Oguz",
            "Gene Tsudik"
        ],
        "abstract": "Massive amounts of contributed content -- including traditional literature, blogs, music, videos, reviews and tweets -- are available on the Internet today, with authors numbering in many millions. Textual information, such as product or service reviews, is an important and increasingly popular type of content that is being used as a foundation of many trendy community-based reviewing sites, such as TripAdvisor and Yelp. Some recent results have shown that, due partly to their specialized/topical nature, sets of reviews authored by the same person are readily linkable based on simple stylometric features. In practice, this means that individuals who author more than a few reviews under different accounts (whether within one site or across multiple sites) can be linked, which represents a significant loss of privacy.\n",
        "submission_date": "2014-05-19T00:00:00",
        "last_modified_date": "2014-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5447",
        "title": "Learning to Exploit Different Translation Resources for Cross Language Information Retrieval",
        "authors": [
            "Hosein Azarbonyad",
            "Azadeh Shakery",
            "Heshaam Faili"
        ],
        "abstract": "One of the important factors that affects the performance of Cross Language Information Retrieval(CLIR)is the quality of translations being employed in CLIR. In order to improve the quality of translations, it is important to exploit available resources efficiently. Employing different translation resources with different characteristics has many challenges. In this paper, we propose a method for exploiting available translation resources simultaneously. This method employs Learning to Rank(LTR) for exploiting different translation resources. To apply LTR methods for query translation, we define different translation relation based features in addition to context based features. We use the contextual information contained in translation resources for extracting context based ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6667",
        "title": "Inferring gender of a Twitter user using celebrities it follows",
        "authors": [
            "Puneet Singh Ludu"
        ],
        "abstract": "This paper addresses the task of user gender classification in social media, with an application to Twitter. The approach automatically predicts gender by leveraging observable information such as the tweet behavior, linguistic content of the user's Twitter feed and the celebrities followed by the user. This paper first evaluates linguistic content based features using LIWC dictionary and popular neighborhood features using Wikipedia and Freebase. Then augments both features which yielded a significant increase in the accuracy for gender prediction. Results show that rich linguistic features combined with popular neighborhood prove valuables and promising for additional user classification needs.\n    ",
        "submission_date": "2014-05-26T00:00:00",
        "last_modified_date": "2014-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6678",
        "title": "Hybrid Type-Logical Grammars, First-Order Linear Logic and the Descriptive Inadequacy of Lambda Grammars",
        "authors": [
            "Richard Moot"
        ],
        "abstract": "In this article we show that hybrid type-logical grammars are a fragment of first-order linear logic. This embedding result has several important consequences: it not only provides a simple new proof theory for the calculus, thereby clarifying the proof-theoretic foundations of hybrid type-logical grammars, but, since the translation is simple and direct, it also provides several new parsing strategies for hybrid type-logical grammars. Second, NP-completeness of hybrid type-logical grammars follows immediately. The main embedding result also sheds new light on problems with lambda grammars/abstract categorial grammars and shows lambda grammars/abstract categorial grammars suffer from problems of over-generation and from problems at the syntax-semantics interface unlike any other categorial grammar.\n    ",
        "submission_date": "2014-05-26T00:00:00",
        "last_modified_date": "2014-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7519",
        "title": "Aspect Based Sentiment Analysis to Extract Meticulous Opinion Value",
        "authors": [
            "Deepali Virmani",
            "Vikrant Malhotra",
            "Ridhi Tyagi"
        ],
        "abstract": "Opinion Mining and Sentiment Analysis is a process of identifying opinions in large unstructured/structured data and then analysing polarity of those opinions. Opinion mining and sentiment analysis have found vast application in analysing online ratings, analysing product based reviews, e-governance, and managing hostile content over the internet. This paper proposes an algorithm to implement aspect level sentiment analysis. The algorithm takes input from the remarks submitted by various teachers of a student. An aspect tree is formed which has various levels and weights are assigned to each branch to identify level of aspect. Aspect value is calculated by the algorithm by means of the proposed aspect tree. Dictionary based method is implemented to evaluate the polarity of the remark. The algorithm returns the aspect value clubbed with opinion value and sentiment value which helps in concluding the summarized value of remark.\n    ",
        "submission_date": "2014-05-29T00:00:00",
        "last_modified_date": "2014-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7975",
        "title": "Multi-layered graph-based multi-document summarization model",
        "authors": [
            "Ercan Canhasi"
        ],
        "abstract": "Multi-document summarization is a process of automatic generation of a compressed version of the given collection of documents. Recently, the graph-based models and ranking algorithms have been actively investigated by the extractive document summarization community. While most work to date focuses on homogeneous connecteness of sentences and heterogeneous connecteness of documents and sentences (e.g. sentence similarity weighted by document importance), in this paper we present a novel 3-layered graph model that emphasizes not only sentence and document level relations but also the influence of under sentence level relations (e.g. a part of sentence similarity).\n    ",
        "submission_date": "2014-05-17T00:00:00",
        "last_modified_date": "2014-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1143",
        "title": "Identifying Duplicate and Contradictory Information in Wikipedia",
        "authors": [
            "Sarah Weissman",
            "Samet Ayhan",
            "Joshua Bradley",
            "Jimmy Lin"
        ],
        "abstract": "Our study identifies sentences in Wikipedia articles that are either identical or highly similar by applying techniques for near-duplicate detection of web pages. This is accomplished with a MapReduce implementation of minhash to identify clusters of sentences with high Jaccard similarity. We show that these clusters can be categorized into six different types, two of which are particularly interesting: identical sentences quantify the extent to which content in Wikipedia is copied and pasted, and near-duplicate sentences that state contradictory facts point to quality issues in Wikipedia.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1765",
        "title": "Linguistic Analysis of Requirements of a Space Project and their Conformity with the Recommendations Proposed by a Controlled Natural Language",
        "authors": [
            "Anne Condamines",
            "Maxime Warnier"
        ],
        "abstract": "The long term aim of the project carried out by the French National Space Agency (CNES) is to design a writing guide based on the real and regular writing of requirements. As a first step in the project, this paper proposes a lin-guistic analysis of requirements written in French by CNES engineers. The aim is to determine to what extent they conform to two rules laid down in INCOSE, a recent guide for writing requirements. Although CNES engineers are not obliged to follow any Controlled Natural Language in their writing of requirements, we believe that language regularities are likely to emerge from this task, mainly due to the writers' experience. The issue is approached using natural language processing tools to identify sentences that do not comply with INCOSE rules. We further review these sentences to understand why the recommendations cannot (or should not) always be applied when specifying large-scale projects.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2014-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2022",
        "title": "Two-dimensional Sentiment Analysis of text",
        "authors": [
            "Rahul Tejwani"
        ],
        "abstract": "Sentiment Analysis aims to get the underlying viewpoint of the text, which could be anything that holds a subjective opinion, such as an online review, Movie rating, Comments on Blog posts etc. This paper presents a novel approach that classify text in two-dimensional Emotional space, based on the sentiments of the author. The approach uses existing lexical resources to extract feature set, which is trained using Supervised Learning techniques.\n    ",
        "submission_date": "2014-06-08T00:00:00",
        "last_modified_date": "2014-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2096",
        "title": "RuleCNL: A Controlled Natural Language for Business Rule Specifications",
        "authors": [
            "Paul Brillant Feuto Njonko",
            "Sylviane Cardey",
            "Peter Greenfield",
            "Walid El Abed"
        ],
        "abstract": "Business rules represent the primary means by which companies define their business, perform their actions in order to reach their objectives. Thus, they need to be expressed unambiguously to avoid inconsistencies between business stakeholders and formally in order to be machine-processed. A promising solution is the use of a controlled natural language (CNL) which is a good mediator between natural and formal languages. This paper presents RuleCNL, which is a CNL for defining business rules. Its core feature is the alignment of the business rule definition with the business vocabulary which ensures traceability and consistency with the business domain. The RuleCNL tool provides editors that assist end-users in the writing process and automatic mappings into the Semantics of Business Vocabulary and Business Rules (SBVR) standard. SBVR is grounded in first order logic and includes constructs called semantic formulations that structure the meaning of rules.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2298",
        "title": "Explaining Violation Traces with Finite State Natural Language Generation Models",
        "authors": [
            "Gordon J. Pace",
            "Michael Rosner"
        ],
        "abstract": "An essential element of any verification technique is that of identifying and communicating to the user, system behaviour which leads to a deviation from the expected behaviour. Such behaviours are typically made available as long traces of system actions which would benefit from a natural language explanation of the trace and especially in the context of business logic level specifications. In this paper we present a natural language generation model which can be used to explain such traces. A key idea is that the explanation language is a CNL that is, formally speaking, regular language susceptible transformations that can be expressed with finite state machinery. At the same time it admits various forms of abstraction and simplification which contribute to the naturalness of explanations that are communicated to the user.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2710",
        "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations",
        "authors": [
            "Ryan Kiros",
            "Richard S. Zemel",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2880",
        "title": "POS Tagging and its Applications for Mathematics",
        "authors": [
            "Ulf Sch\u00f6neberg",
            "Wolfram Sperber"
        ],
        "abstract": "Content analysis of scientific publications is a nontrivial task, but a useful and important one for scientific information services. In the Gutenberg era it was a domain of human experts; in the digital age many machine-based methods, e.g., graph analysis tools and machine-learning techniques, have been developed for it. Natural Language Processing (NLP) is a powerful machine-learning approach to semiautomatic speech and language processing, which is also applicable to mathematics. The well established methods of NLP have to be adjusted for the special needs of mathematics, in particular for handling mathematical formulae. We demonstrate a mathematics-aware part of speech tagger and give a short overview about our adaptation of NLP methods for mathematical publications. We show the use of the tools developed for key phrase extraction and classification in the database zbMATH.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2963",
        "title": "A machine-compiled macroevolutionary history of Phanerozoic life",
        "authors": [
            "Shanan E. Peters",
            "Ce Zhang",
            "Miron Livny",
            "Christopher R\u00e9"
        ],
        "abstract": "Many aspects of macroevolutionary theory and our understanding of biotic responses to global environmental change derive from literature-based compilations of palaeontological data. Existing manually assembled databases are, however, incomplete and difficult to assess and enhance. Here, we develop and validate the quality of a machine reading system, PaleoDeepDive, that automatically locates and extracts data from heterogeneous text, tables, and figures in publications. PaleoDeepDive performs comparably to humans in complex data extraction and inference tasks and generates congruent synthetic macroevolutionary results. Unlike traditional databases, PaleoDeepDive produces a probabilistic database that systematically improves as information is added. We also show that the system can readily accommodate sophisticated data types, such as morphological data in biological illustrations and associated textual descriptions. Our machine reading approach to scientific data integration and synthesis brings within reach many questions that are currently underdetermined and does so in ways that may stimulate entirely new modes of inquiry.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3855",
        "title": "Human language reveals a universal positivity bias",
        "authors": [
            "Peter Sheridan Dodds",
            "Eric M. Clark",
            "Suma Desu",
            "Morgan R. Frank",
            "Andrew J. Reagan",
            "Jake Ryland Williams",
            "Lewis Mitchell",
            "Kameron Decker Harris",
            "Isabel M. Kloumann",
            "James P. Bagrow",
            "Karine Megerdoomian",
            "Matthew T. McMahon",
            "Brian F. Tivnan",
            "Christopher M. Danforth"
        ],
        "abstract": "Using human evaluation of 100,000 words spread across 24 corpora in 10 languages diverse in origin and culture, we present evidence of a deep imprint of human sociality in language, observing that (1) the words of natural human language possess a universal positivity bias; (2) the estimated emotional content of words is consistent between languages under translation; and (3) this positivity bias is strongly independent of frequency of word usage. Alongside these general regularities, we describe inter-language variations in the emotional spectrum of languages which allow us to rank corpora. We also show how our word evaluations can be used to construct physical-like instruments for both real-time and offline measurement of the emotional content of large-scale texts.\n    ",
        "submission_date": "2014-06-15T00:00:00",
        "last_modified_date": "2014-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3915",
        "title": "A Bengali HMM Based Speech Synthesis System",
        "authors": [
            "Sankar Mukherjee",
            "Shyamal Kumar Das Mandal"
        ],
        "abstract": "The paper presents the capability of an HMM-based TTS system to produce Bengali speech. In this synthesis method, trajectories of speech parameters are generated from the trained Hidden Markov Models. A final speech waveform is synthesized from those speech parameters. In our experiments, spectral properties were represented by Mel Cepstrum Coefficients. Both the training and synthesis issues are investigated in this paper using annotated Bengali speech database. Experimental evaluation depicts that the developed text-to-speech system is capable of producing adequately natural speech in terms of intelligibility and intonation for Bengali.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4441",
        "title": "Scaling laws and fluctuations in the statistics of word frequencies",
        "authors": [
            "Martin Gerlach",
            "Eduardo G. Altmann"
        ],
        "abstract": "In this paper we combine statistical analysis of large text databases and simple stochastic models to explain the appearance of scaling laws in the statistics of word frequencies. Besides the sublinear scaling of the vocabulary size with database size (Heaps' law), here we report a new scaling of the fluctuations around this average (fluctuation scaling analysis). We explain both scaling laws by modeling the usage of words by simple stochastic processes in which the overall distribution of word-frequencies is fat tailed (Zipf's law) and the frequency of a single word is subject to fluctuations across documents (as in topic models). In this framework, the mean and the variance of the vocabulary size can be expressed as quenched averages, implying that: i) the inhomogeneous dissemination of words cause a reduction of the average vocabulary size in comparison to the homogeneous case, and ii) correlations in the co-occurrence of words lead to an increase in the variance and the vocabulary size becomes a non-self-averaging quantity. We address the implications of these observations to the measurement of lexical richness. We test our results in three large text databases (Google-ngram, Enlgish Wikipedia, and a collection of scientific articles).\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4498",
        "title": "Extracting information from S-curves of language change",
        "authors": [
            "Fakhteh Ghanbarnejad",
            "Martin Gerlach",
            "Jose M. Miotto",
            "Eduardo G. Altmann"
        ],
        "abstract": "It is well accepted that adoption of innovations are described by S-curves (slow start, accelerating period, and slow end). In this paper, we analyze how much information on the dynamics of innovation spreading can be obtained from a quantitative description of S-curves. We focus on the adoption of linguistic innovations for which detailed databases of written texts from the last 200 years allow for an unprecedented statistical precision. Combining data analysis with simulations of simple models (e.g., the Bass dynamics on complex networks) we identify signatures of endogenous and exogenous factors in the S-curves of adoption. We propose a measure to quantify the strength of these factors and three different methods to estimate it from S-curves. We obtain cases in which the exogenous factors are dominant (in the adoption of German orthographic reforms and of one irregular verb) and cases in which endogenous factors are dominant (in the adoption of conventions for romanization of Russian names and in the regularization of most studied verbs). These results show that the shape of S-curve is not universal and contains information on the adoption mechanism. (published at \"J. R. Soc. Interface, vol. 11, no. 101, (2014) 1044\"; DOI: ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5679",
        "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
        "authors": [
            "Andrej Karpathy",
            "Armand Joulin",
            "Li Fei-Fei"
        ],
        "abstract": "We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2014-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5824",
        "title": "VideoSET: Video Summary Evaluation through Text",
        "authors": [
            "Serena Yeung",
            "Alireza Fathi",
            "Li Fei-Fei"
        ],
        "abstract": "In this paper we present VideoSET, a method for Video Summary Evaluation through Text that can evaluate how well a video summary is able to retain the semantic information contained in its original video. We observe that semantics is most easily expressed in words, and develop a text-based approach for the evaluation. Given a video summary, a text representation of the video summary is first generated, and an NLP-based metric is then used to measure its semantic distance to ground-truth text summaries written by humans. We show that our technique has higher agreement with human judgment than pixel-based distance metrics. We also release text annotations and ground-truth text summaries for a number of publicly available video datasets, for use by the computer vision community.\n    ",
        "submission_date": "2014-06-23T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7558",
        "title": "Human Communication Systems Evolve by Cultural Selection",
        "authors": [
            "Nicolas Fay",
            "Monica Tamariz",
            "T Mark Ellison",
            "Dale Barr"
        ],
        "abstract": "Human communication systems, such as language, evolve culturally; their components undergo reproduction and variation. However, a role for selection in cultural evolutionary dynamics is less clear. Often neutral evolution (also known as 'drift') models, are used to explain the evolution of human communication systems, and cultural evolution more generally. Under this account, cultural change is unbiased: for instance, vocabulary, baby names and pottery designs have been found to spread through random copying.\n",
        "submission_date": "2014-06-29T00:00:00",
        "last_modified_date": "2014-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0167",
        "title": "Mathematical Language Processing Project",
        "authors": [
            "Robert Pagael",
            "Moritz Schubotz"
        ],
        "abstract": "In natural language, words and phrases themselves imply the semantics. In contrast, the meaning of identifiers in mathematical formulae is undefined. Thus scientists must study the context to decode the meaning. The Mathematical Language Processing (MLP) project aims to support that process. In this paper, we compare two approaches to discover identifier-definition tuples. At first we use a simple pattern matching approach. Second, we present the MLP approach that uses part-of-speech tag based distances as well as sentence positions to calculate identifier-definition probabilities. The evaluation of our prototypical system, applied on the Wikipedia text corpus, shows that our approach augments the user experience substantially. While hovering the identifiers in the formula, tool-tips with the most probable definitions occur. Tests with random samples show that the displayed definitions provide a good match with the actual meaning of the identifiers.\n    ",
        "submission_date": "2014-07-01T00:00:00",
        "last_modified_date": "2014-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1165",
        "title": "Recognition of Isolated Words using Zernike and MFCC features for Audio Visual Speech Recognition",
        "authors": [
            "Prashant Bordea",
            "Amarsinh Varpeb",
            "Ramesh Manzac",
            "Pravin Yannawara"
        ],
        "abstract": "Automatic Speech Recognition (ASR) by machine is an attractive research topic in signal processing domain and has attracted many researchers to contribute in this area. In recent year, there have been many advances in automatic speech reading system with the inclusion of audio and visual speech features to recognize words under noisy conditions. The objective of audio-visual speech recognition system is to improve recognition accuracy. In this paper we computed visual features using Zernike moments and audio feature using Mel Frequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary of Independent Standard Words) dataset which contains collection of isolated set of city names of 10 speakers. The visual features were normalized and dimension of features set was reduced by Principal Component Analysis (PCA) in order to recognize the isolated word utterance on PCA ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4610",
        "title": "Understanding Zipf's law of word frequencies through sample-space collapse in sentence formation",
        "authors": [
            "Stefan Thurner",
            "Rudolf Hanel",
            "Bo Liu",
            "Bernat Corominas-Murtra"
        ],
        "abstract": "The formation of sentences is a highly structured and history-dependent process. The probability of using a specific word in a sentence strongly depends on the 'history' of word-usage earlier in that sentence. We study a simple history-dependent model of text generation assuming that the sample-space of word usage reduces along sentence formation, on average. We first show that the model explains the approximate Zipf law found in word frequencies as a direct consequence of sample-space reduction. We then empirically quantify the amount of sample-space reduction in the sentences of ten famous English books, by analysis of corresponding word-transition tables that capture which words can follow any given word in a text. We find a highly nested structure in these transition tables and show that this `nestedness' is tightly related to the power law exponents of the observed word frequency distributions. With the proposed model it is possible to understand that the nestedness of a text can be the origin of the actual scaling exponent, and that deviations from the exact Zipf law can be understood by variations of the degree of nestedness on a book-by-book basis. On a theoretical level we are able to show that in case of weak nesting, Zipf's law breaks down in a fast transition. Unlike previous attempts to understand Zipf's law in language the sample-space reducing model is not based on assumptions of multiplicative, preferential, or self-organised critical mechanisms behind language formation, but simply used the empirically quantifiable parameter 'nestedness' to understand the statistics of word frequencies.\n    ",
        "submission_date": "2014-07-17T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6439",
        "title": "Feature Engineering for Knowledge Base Construction",
        "authors": [
            "Christopher R\u00e9",
            "Amir Abbas Sadeghian",
            "Zifei Shan",
            "Jaeho Shin",
            "Feiran Wang",
            "Sen Wu",
            "Ce Zhang"
        ],
        "abstract": "Knowledge base construction (KBC) is the process of populating a knowledge base, i.e., a relational database together with inference rules, with information extracted from documents and structured sources. KBC blurs the distinction between two traditional database problems, information extraction and information integration. For the last several years, our group has been building knowledge bases with scientific collaborators. Using our approach, we have built knowledge bases that have comparable and sometimes better quality than those constructed by human volunteers. In contrast to these knowledge bases, which took experts a decade or more human years to construct, many of our projects are constructed by a single graduate student.\n",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6639",
        "title": "How the Voynich Manuscript was created",
        "authors": [
            "Torsten Timm"
        ],
        "abstract": "The Voynich manuscript is a medieval book written in an unknown script. This paper studies the relation between similarly spelled words in the Voynich manuscript. By means of a detailed analysis of similar spelled words it was possible to reveal the text generation method used for the Voynich manuscript.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7094",
        "title": "Crowdsourcing Dialect Characterization through Twitter",
        "authors": [
            "Bruno Gon\u00e7alves",
            "David S\u00e1nchez"
        ],
        "abstract": "We perform a large-scale analysis of language diatopic variation using geotagged microblogging datasets. By collecting all Twitter messages written in Spanish over more than two years, we build a corpus from which a carefully selected list of concepts allows us to characterize Spanish varieties on a global scale. A cluster analysis proves the existence of well defined macroregions sharing common lexical properties. Remarkably enough, we find that Spanish language is split into two superdialects, namely, an urban speech used across major American and Spanish citites and a diverse form that encompasses rural areas and small towns. The latter can be further clustered into smaller varieties with a stronger regional character.\n    ",
        "submission_date": "2014-07-26T00:00:00",
        "last_modified_date": "2014-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7357",
        "title": "Text Classification Using Association Rules, Dependency Pruning and Hyperonymization",
        "authors": [
            "Yannis Haralambous",
            "Philippe Lenca"
        ],
        "abstract": "We present new methods for pruning and enhancing item- sets for text classification via association rule mining. Pruning methods are based on dependency syntax and enhancing methods are based on replacing words by their hyperonyms of various orders. We discuss the impact of these methods, compared to pruning based on tfidf rank of words.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7736",
        "title": "A Latent Space Analysis of Editor Lifecycles in Wikipedia",
        "authors": [
            "Xiangju Qin",
            "Derek Greene",
            "P\u00e1draig Cunningham"
        ],
        "abstract": "Collaborations such as Wikipedia are a key part of the value of the modern Internet. At the same time there is concern that these collaborations are threatened by high levels of member turnover. In this paper we borrow ideas from topic analysis to editor activity on Wikipedia over time into a latent space that offers an insight into the evolving patterns of editor behavior. This latent space representation reveals a number of different categories of editor (e.g. content experts, social networkers) and we show that it does provide a signal that predicts an editor's departure from the community. We also show that long term editors gradually diversify their participation by shifting edit preference from one or two namespaces to multiple namespaces and experience relatively soft evolution in their editor profiles, while short term editors generally distribute their contribution randomly among the namespaces and experience considerably fluctuated evolution in their editor profiles.\n    ",
        "submission_date": "2014-07-29T00:00:00",
        "last_modified_date": "2014-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8322",
        "title": "Zipf's law for word frequencies: word forms versus lemmas in long texts",
        "authors": [
            "Alvaro Corral",
            "Gemma Boleda",
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Zipf's law is a fundamental paradigm in the statistics of written and spoken natural language as well as in other communication systems. We raise the question of the elementary units for which Zipf's law should hold in the most natural way, studying its validity for plain word forms and for the corresponding lemma forms. In order to have as homogeneous sources as possible, we analyze some of the longest literary texts ever written, comprising four different languages, with different levels of morphological complexity. In all cases Zipf's law is fulfilled, in the sense that a power-law distribution of word or lemma frequencies is valid for several orders of magnitude. We investigate the extent to which the word-lemma transformation preserves two parameters of Zipf's law: the exponent and the low-frequency cut-off. We are not able to demonstrate a strict invariance of the tail, as for a few texts both exponents deviate significantly, but we conclude that the exponents are very similar, despite the remarkable transformation that going from words to lemmas represents, considerably affecting all ranges of frequencies. In contrast, the low-frequency cut-offs are less stable.\n    ",
        "submission_date": "2014-07-31T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0985",
        "title": "Speech earthquakes: scaling and universality in human voice",
        "authors": [
            "Jordi Luque",
            "Bartolo Luque",
            "Lucas Lacasa"
        ],
        "abstract": "Speech is a distinctive complex feature of human capabilities. In order to understand the physics underlying speech production, in this work we empirically analyse the statistics of large human speech datasets ranging several languages. We first show that during speech the energy is unevenly released and power-law distributed, reporting a universal robust Gutenberg-Richter-like law in speech. We further show that such earthquakes in speech show temporal correlations, as the interevent statistics are again power-law distributed. Since this feature takes place in the intra-phoneme range, we conjecture that the responsible for this complex phenomenon is not cognitive, but it resides on the physiological speech production mechanism. Moreover, we show that these waiting time distributions are scale invariant under a renormalisation group transformation, suggesting that the process of speech generation is indeed operating close to a critical point. These results are put in contrast with current paradigms in speech processing, which point towards low dimensional deterministic chaos as the origin of nonlinear traits in speech fluctuations. As these latter fluctuations are indeed the aspects that humanize synthetic speech, these findings may have an impact in future speech synthesis technologies. Results are robust and independent of the communication language or the number of speakers, pointing towards an universal pattern and yet another hint of complexity in human speech.\n    ",
        "submission_date": "2014-08-05T00:00:00",
        "last_modified_date": "2014-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2430",
        "title": "Optimizing Component Combination in a Multi-Indexing Paragraph Retrieval System",
        "authors": [
            "Boris Iolis",
            "Gianluca Bontempi"
        ],
        "abstract": "We demonstrate a method to optimize the combination of distinct components in a paragraph retrieval system. Our system makes use of several indices, query generators and filters, each of them potentially contributing to the quality of the returned list of results. The components are combined with a weighed sum, and we optimize the weights using a heuristic optimization algorithm. This allows us to maximize the quality of our results, but also to determine which components are most valuable in our system. We evaluate our approach on the paragraph selection task of a Question Answering dataset.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2014-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2699",
        "title": "Internal and external dynamics in language: Evidence from verb regularity in a historical corpus of English",
        "authors": [
            "Christine F. Cuskley",
            "Martina Pugliese",
            "Claudio Castellano",
            "Francesca Colaiori",
            "Vittorio Loreto",
            "Francesca Tria"
        ],
        "abstract": "Human languages are rule governed, but almost invariably these rules have exceptions in the form of irregularities. Since rules in language are efficient and productive, the persistence of irregularity is an anomaly. How does irregularity linger in the face of internal (endogenous) and external (exogenous) pressures to conform to a rule? Here we address this problem by taking a detailed look at simple past tense verbs in the Corpus of Historical American English. The data show that the language is open, with many new verbs entering. At the same time, existing verbs might tend to regularize or irregularize as a consequence of internal dynamics, but overall, the amount of irregularity sustained by the language stays roughly constant over time. Despite continuous vocabulary growth, and presumably, an attendant increase in expressive power, there is no corresponding growth in irregularity. We analyze the set of irregulars, showing they may adhere to a set of minority rules, allowing for increased stability of irregularity over time. These findings contribute to the debate on how language systems become rule governed, and how and why they sustain exceptions to rules, providing insight into the interplay between the emergence and maintenance of rules and exceptions in language.\n    ",
        "submission_date": "2014-08-12T00:00:00",
        "last_modified_date": "2014-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3829",
        "title": "Opinion mining of movie reviews at document level",
        "authors": [
            "Richa Sharma",
            "Shweta Nigam",
            "Rekha Jain"
        ],
        "abstract": "The whole world is changed rapidly and using the current technologies Internet becomes an essential need for everyone. Web is used in every field. Most of the people use web for a common purpose like online shopping, chatting etc. During an online shopping large number of reviews/opinions are given by the users that reflect whether the product is good or bad. These reviews need to be explored, analyse and organized for better decision making. Opinion Mining is a natural language processing task that deals with finding orientation of opinion in a piece of text with respect to a topic. In this paper a document based opinion mining system is proposed that classify the documents as positive, negative and neutral. Negation is also handled in the proposed system. Experimental results using reviews of movies show the effectiveness of the system.\n    ",
        "submission_date": "2014-08-17T00:00:00",
        "last_modified_date": "2014-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4245",
        "title": "Towards crowdsourcing and cooperation in linguistic resources",
        "authors": [
            "Dmitry Ustalov"
        ],
        "abstract": "Linguistic resources can be populated with data through the use of such approaches as crowdsourcing and gamification when motivated people are involved. However, current crowdsourcing genre taxonomies lack the concept of cooperation, which is the principal element of modern video games and may potentially drive the annotators' interest. This survey on crowdsourcing taxonomies and cooperation in linguistic resources provides recommendations on using cooperation in existent genres of crowdsourcing and an evidence of the efficiency of cooperation using a popular Russian linguistic resource created through crowdsourcing as an example.\n    ",
        "submission_date": "2014-08-19T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5403",
        "title": "Neural Mechanism of Language",
        "authors": [
            "Peilei Liu",
            "Ting Wang"
        ],
        "abstract": "This paper is based on our previous work on neural coding. It is a self-organized model supported by existing evidences. Firstly, we briefly introduce this model in this paper, and then we explain the neural mechanism of language and reasoning with it. Moreover, we find that the position of an area determines its importance. Specifically, language relevant areas are in the capital position of the cortical kingdom. Therefore they are closely related with autonomous consciousness and working memories. In essence, language is a miniature of the real world. Briefly, this paper would like to bridge the gap between molecule mechanism of neurons and advanced functions such as language and reasoning.\n    ",
        "submission_date": "2014-08-22T00:00:00",
        "last_modified_date": "2014-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5427",
        "title": "A Case Study in Text Mining: Interpreting Twitter Data From World Cup Tweets",
        "authors": [
            "Daniel Godfrey",
            "Caley Johns",
            "Carl Meyer",
            "Shaina Race",
            "Carol Sadek"
        ],
        "abstract": "Cluster analysis is a field of data analysis that extracts underlying patterns in data. One application of cluster analysis is in text-mining, the analysis of large collections of text to find similarities between documents. We used a collection of about 30,000 tweets extracted from Twitter just before the World Cup started. A common problem with real world text data is the presence of linguistic noise. In our case it would be extraneous tweets that are unrelated to dominant themes. To combat this problem, we created an algorithm that combined the DBSCAN algorithm and a consensus matrix. This way we are left with the tweets that are related to those dominant themes. We then used cluster analysis to find those topics that the tweets describe. We clustered the tweets using k-means, a commonly used clustering algorithm, and Non-Negative Matrix Factorization (NMF) and compared the results. The two algorithms gave similar results, but NMF proved to be faster and provided more easily interpreted results. We explored our results using two visualization tools, Gephi and Wordle.\n    ",
        "submission_date": "2014-08-21T00:00:00",
        "last_modified_date": "2014-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6418",
        "title": "Video In Sentences Out",
        "authors": [
            "Andrei Barbu",
            "Alexander Bridge",
            "Zachary Burchill",
            "Dan Coroian",
            "Sven Dickinson",
            "Sanja Fidler",
            "Aaron Michaux",
            "Sam Mussman",
            "Siddharth Narayanaswamy",
            "Dhaval Salvi",
            "Lara Schmidt",
            "Jiangnan Shangguan",
            "Jeffrey Mark Siskind",
            "Jarrell Waggoner",
            "Song Wang",
            "Jinlian Wei",
            "Yifan Yin",
            "Zhiqi Zhang"
        ],
        "abstract": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the trackto-role assignments, and changing body posture.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6762",
        "title": "Chatbot for admissions",
        "authors": [
            "Nikolaos Polatidis"
        ],
        "abstract": "The communication of potential students with a university department is performed manually and it is a very time consuming procedure. The opportunity to communicate with on a one-to-one basis is highly valued. However with many hundreds of applications each year, one-to-one conversations are not feasible in most cases. The communication will require a member of academic staff to expend several hours to find suitable answers and contact each student. It would be useful to reduce his costs and time.\n",
        "submission_date": "2014-08-28T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6988",
        "title": "An Information Retrieval Approach to Short Text Conversation",
        "authors": [
            "Zongcheng Ji",
            "Zhengdong Lu",
            "Hang Li"
        ],
        "abstract": "Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather \"intelligently\", when combined with a huge repository of conversation data from social media.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2014-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0915",
        "title": "An Approach for Text Steganography Based on Markov Chains",
        "authors": [
            "H. Hernan Moraldo"
        ],
        "abstract": "A text steganography method based on Markov chains is introduced, together with a reference implementation. This method allows for information hiding in texts that are automatically generated following a given Markov model. Other Markov - based systems of this kind rely on big simplifications of the language model to work, which produces less natural looking and more easily detectable texts. The method described here is designed to generate texts within a good approximation of the original language model provided.\n    ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1744",
        "title": "Structure of a media co-occurrence network",
        "authors": [
            "V.A. Traag",
            "R. Reinanda",
            "G. van Klinken"
        ],
        "abstract": "Social networks have been of much interest in recent years. We here focus on a network structure derived from co-occurrences of people in traditional newspaper media. We find three clear deviations from what can be expected in a random graph. First, the average degree in the empirical network is much lower than expected, and the average weight of a link much higher than expected. Secondly, high degree nodes attract disproportionately much weight. Thirdly, relatively much of the weight seems to concentrate between high degree nodes. We believe this can be explained by the fact that most people tend to co-occur repeatedly with the same people. We create a model that replicates these observations qualitatively based on two self-reinforcing processes: (1) more frequently occurring persons are more likely to occur again; and (2) if two people co-occur frequently, they are more likely to co-occur again. This suggest that the media tends to focus on people that are already in the news, and that they reinforce existing co-occurrences.\n    ",
        "submission_date": "2014-09-05T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2450",
        "title": "Exploiting Social Network Structure for Person-to-Person Sentiment Analysis",
        "authors": [
            "Robert West",
            "Hristo S. Paskov",
            "Jure Leskovec",
            "Christopher Potts"
        ],
        "abstract": "Person-to-person evaluations are prevalent in all kinds of discourse and important for establishing reputations, building social bonds, and shaping public opinion. Such evaluations can be analyzed separately using signed social networks and textual sentiment analysis, but this misses the rich interactions between language and social context. To capture such interactions, we develop a model that predicts individual A's opinion of individual B by synthesizing information from the signed social network in which A and B are embedded with sentiment analysis of the evaluative texts relating A to B. We prove that this problem is NP-hard but can be relaxed to an efficiently solvable hinge-loss Markov random field, and we show that this implementation outperforms text-only and network-only versions in two very different datasets involving community-level decision-making: the Wikipedia Requests for Adminship corpus and the Convote U.S. Congressional speech corpus.\n    ",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2014-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2944",
        "title": "Collaborative Deep Learning for Recommender Systems",
        "authors": [
            "Hao Wang",
            "Naiyan Wang",
            "Dit-Yan Yeung"
        ],
        "abstract": "Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art.\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2993",
        "title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model",
        "authors": [
            "Jian Tang",
            "Ming Zhang",
            "Qiaozhu Mei"
        ],
        "abstract": "It has always been a burden to the users of statistical topic models to predetermine the right number of topics, which is a key parameter of most topic models. Conventionally, automatic selection of this parameter is done through either statistical model selection (e.g., cross-validation, AIC, or BIC) or Bayesian nonparametric models (e.g., hierarchical Dirichlet process). These methods either rely on repeated runs of the inference algorithm to search through a large range of parameter values which does not suit the mining of big data, or replace this parameter with alternative parameters that are less intuitive and still hard to be determined. In this paper, we explore to \"eliminate\" this parameter from a new perspective. We first present a nonparametric treatment of the PLSA model named nonparametric probabilistic latent semantic analysis (nPLSA). The inference procedure of nPLSA allows for the exploration and comparison of different numbers of topics within a single execution, yet remains as simple as that of PLSA. This is achieved by substituting the parameter of the number of topics with an alternative parameter that is the minimal goodness of fit of a document. We show that the new parameter can be further eliminated by two parameter-free treatments: either by monitoring the diversity among the discovered topics or by a weak supervision from users in the form of an exemplar topic. The parameter-free topic model finds the appropriate number of topics when the diversity among the discovered topics is maximized, or when the granularity of the discovered topics matches the exemplar topic. Experiments on both synthetic and real data prove that the parameter-free topic model extracts topics with a comparable quality comparing to classical topic models with \"manual transmission\". The quality of the topics outperforms those extracted through classical Bayesian nonparametric models.\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2014-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4617",
        "title": "The Role of Emotions in Propagating Brands in Social Networks",
        "authors": [
            "Ronald Hochreiter",
            "Christoph Waldhauser"
        ],
        "abstract": "A key aspect of word of mouth marketing are emotions. Emotions in texts help propagating messages in conventional advertising. In word of mouth scenarios, emotions help to engage consumers and incite to propagate the message further. While the function of emotions in offline marketing in general and word of mouth marketing in particular is rather well understood, online marketing can only offer a limited view on the function of emotions. In this contribution we seek to close this gap. We therefore investigate how emotions function in social media. To do so, we collected more than 30,000 brand marketing messages from the Google+ social networking site. Using state of the art computational linguistics classifiers, we compute the sentiment of these messages. Starting out with Poisson regression-based baseline models, we seek to replicate earlier findings using this large data set. We extend upon earlier research by computing multi-level mixed effects models that compare the function of emotions across different industries. We find that while the well known notion of activating emotions propagating messages holds in general for our data as well. But there are significant differences between the observed industries.\n    ",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2014-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4835",
        "title": "Taking into Account the Differences between Actively and Passively Acquired Data: The Case of Active Learning with Support Vector Machines for Imbalanced Datasets",
        "authors": [
            "Michael Bloodgood",
            "K. Vijay-Shanker"
        ],
        "abstract": "Actively sampled data can have very different characteristics than passively sampled data. Therefore, it's promising to investigate using different inference procedures during AL than are used during passive learning (PL). This general idea is explored in detail for the focused case of AL with cost-weighted SVMs for imbalanced data, a situation that arises for many HLT tasks. The key idea behind the proposed InitPA method for addressing imbalance is to base cost models during AL on an estimate of overall corpus imbalance computed via a small unbiased sample rather than the imbalance in the labeled training data, which is the leading method used during PL.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5165",
        "title": "A Method for Stopping Active Learning Based on Stabilizing Predictions and the Need for User-Adjustable Stopping",
        "authors": [
            "Michael Bloodgood",
            "K. Vijay-Shanker"
        ],
        "abstract": "A survey of existing methods for stopping active learning (AL) reveals the needs for methods that are: more widely applicable; more aggressive in saving annotations; and more stable across changing datasets. A new method for stopping AL based on stabilizing predictions is presented that addresses these needs. Furthermore, stopping methods are required to handle a broad range of different annotation/performance tradeoff valuations. Despite this, the existing body of work is dominated by conservative methods with little (if any) attention paid to providing users with control over the behavior of stopping methods. The proposed method is shown to fill a gap in the level of aggressiveness available for stopping AL and supports providing users with control over stopping behavior.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5623",
        "title": "Interactive Visual Exploration of Topic Models using Graphs",
        "authors": [
            "Samuel R\u00f6nnqvist",
            "Xiaolu Wang",
            "Peter Sarlin"
        ],
        "abstract": "Probabilistic topic modeling is a popular and powerful family of tools for uncovering thematic structure in large sets of unstructured text documents. While much attention has been directed towards the modeling algorithms and their various extensions, comparatively few studies have concerned how to present or visualize topic models in meaningful ways. In this paper, we present a novel design that uses graphs to visually communicate topic structure and meaning. By connecting topic nodes via descriptive keyterms, the graph representation reveals topic similarities, topic meaning and shared, ambiguous keyterms. At the same time, the graph can be used for information retrieval purposes, to find documents by topic or topic subsets. To exemplify the utility of the design, we illustrate its use for organizing and exploring corpora of financial patents.\n    ",
        "submission_date": "2014-09-19T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7336",
        "title": "Does network complexity help organize Babel's library?",
        "authors": [
            "Juan Pablo C\u00e1rdenas",
            "Iv\u00e1n Gonz\u00e1lez",
            "Gerardo Vidal",
            "Miguel Fuentes"
        ],
        "abstract": "In this work, we study properties of texts from the perspective of complex network theory. Words in given texts are linked by co-occurrence and transformed into networks, and we observe that these display topological properties common to other complex systems. However, there are some properties that seem to be exclusive to texts; many of these properties depend on the frequency of words in the text, while others seem to be strictly determined by the grammar. Precisely, these properties allow for a categorization of texts as either with a sense and others encoded or senseless.\n    ",
        "submission_date": "2014-09-23T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8152",
        "title": "Controversy and Sentiment in Online News",
        "authors": [
            "Yelena Mejova",
            "Amy X. Zhang",
            "Nicholas Diakopoulos",
            "Carlos Castillo"
        ],
        "abstract": "How do news sources tackle controversial issues? In this work, we take a data-driven approach to understand how controversy interplays with emotional expression and biased language in the news. We begin by introducing a new dataset of controversial and non-controversial terms collected using crowdsourcing. Then, focusing on 15 major U.S. news outlets, we compare millions of articles discussing controversial and non-controversial issues over a span of 7 months. We find that in general, when it comes to controversial issues, the use of negative affect and biased language is prevalent, while the use of strong emotion is tempered. We also observe many differences across news sources. Using these findings, we show that we can indicate to what extent an issue is controversial, by comparing it with other issues in terms of how they are portrayed across different media.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2014-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8309",
        "title": "Arabic Spelling Correction using Supervised Learning",
        "authors": [
            "Youssef Hassan",
            "Mohamed Aly",
            "Amir Atiya"
        ],
        "abstract": "In this work, we address the problem of spelling correction in the Arabic language utilizing the new corpus provided by QALB (Qatar Arabic Language Bank) project which is an annotated corpus of sentences with errors and their corrections. The corpus contains edit, add before, split, merge, add after, move and other error types. We are concerned with the first four error types as they contribute more than 90% of the spelling errors in the corpus. The proposed system has many models to address each error type on its own and then integrating all the models to provide an efficient and robust system that achieves an overall recall of 0.59, precision of 0.58 and F1 score of 0.58 including all the error types on the development set. Our system participated in the QALB 2014 shared task \"Automatic Arabic Error Correction\" and achieved an F1 score of 0.6, earning the sixth place out of nine participants.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2014-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8484",
        "title": "An agent-driven semantical identifier using radial basis neural networks and reinforcement learning",
        "authors": [
            "Christian Napoli",
            "Giuseppe Pappalardo",
            "Emiliano Tramontana"
        ],
        "abstract": "Due to the huge availability of documents in digital form, and the deception possibility raise bound to the essence of digital documents and the way they are spread, the authorship attribution problem has constantly increased its relevance. Nowadays, authorship attribution,for both information retrieval and analysis, has gained great importance in the context of security, trust and copyright preservation. This work proposes an innovative multi-agent driven machine learning technique that has been developed for authorship attribution. By means of a preprocessing for word-grouping and time-period related analysis of the common lexicon, we determine a bias reference level for the recurrence frequency of the words within analysed texts, and then train a Radial Basis Neural Networks (RBPNN)-based classifier to identify the correct author. The main advantage of the proposed approach lies in the generality of the semantic analysis, which can be applied to different contexts and lexical domains, without requiring any modification. Moreover, the proposed system is able to incorporate an external input, meant to tune the classifier, and then self-adjust by means of continuous learning reinforcement.\n    ",
        "submission_date": "2014-09-30T00:00:00",
        "last_modified_date": "2014-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0210",
        "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0316",
        "title": "Using social network graph analysis for interest detection",
        "authors": [
            "Brian Lee Yung Rowe"
        ],
        "abstract": "A person's interests exist as an internal state and are difficult to define. Since only external actions are observable, a proxy must be used that represents someone's interests. Techniques like collaborative filtering, behavioral targeting, and hashtag analysis implicitly model an individual's interests. I argue that these models are limited to shallow, temporary interests, which do not reflect people's deeper interests or passions. I propose an alternative model of interests that takes advantage of a user's social graph. The basic principle is that people only follow those that interest them, so the social graph is an effective and robust proxy for people's interests.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1090",
        "title": "Explain Images with Multimodal Recurrent Neural Networks",
        "authors": [
            "Junhua Mao",
            "Wei Xu",
            "Yi Yang",
            "Jiang Wang",
            "Alan L. Yuille"
        ],
        "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.\n    ",
        "submission_date": "2014-10-04T00:00:00",
        "last_modified_date": "2014-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2265",
        "title": "A Scalable, Lexicon Based Technique for Sentiment Analysis",
        "authors": [
            "Chetan Kaushik",
            "Atul Mishra"
        ],
        "abstract": "Rapid increase in the volume of sentiment rich social media on the web has resulted in an increased interest among researchers regarding Sentimental Analysis and opinion mining. However, with so much social media available on the web, sentiment analysis is now considered as a big data task. Hence the conventional sentiment analysis approaches fails to efficiently handle the vast amount of sentiment data available now a days. The main focus of the research was to find such a technique that can efficiently perform sentiment analysis on big data sets. A technique that can categorize the text as positive, negative and neutral in a fast and accurate manner. In the research, sentiment analysis was performed on a large data set of tweets using Hadoop and the performance of the technique was measured in form of speed and accuracy. The experimental results shows that the technique exhibits very good efficiency in handling big sentiment data sets.\n    ",
        "submission_date": "2014-10-08T00:00:00",
        "last_modified_date": "2014-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2455",
        "title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments",
        "authors": [
            "Stephan Gouws",
            "Yoshua Bengio",
            "Greg Corrado"
        ],
        "abstract": "We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2686",
        "title": "Polarization Measurement of High Dimensional Social Media Messages With Support Vector Machine Algorithm Using Mapreduce",
        "authors": [
            "Ferhat \u00d6zg\u00fcr \u00c7atak"
        ],
        "abstract": "In this article, we propose a new Support Vector Machine (SVM) training algorithm based on distributed MapReduce technique. In literature, there are a lots of research that shows us SVM has highest generalization property among classification algorithms used in machine learning area. Also, SVM classifier model is not affected by correlations of the features. But SVM uses quadratic optimization techniques in its training phase. The SVM algorithm is formulated as quadratic optimization problem. Quadratic optimization problem has $O(m^3)$ time and $O(m^2)$ space complexity, where m is the training set size. The computation time of SVM training is quadratic in the number of training instances. In this reason, SVM is not a suitable classification algorithm for large scale dataset classification. To solve this training problem we developed a new distributed MapReduce method developed. Accordingly, (i) SVM algorithm is trained in distributed dataset individually; (ii) then merge all support vectors of classifier model in every trained node; and (iii) iterate these two steps until the classifier model converges to the optimal classifier function. In the implementation phase, large scale social media dataset is presented in TFxIDF matrix. The matrix is used for sentiment analysis to get polarization value. Two and three class models are created for classification method. Confusion matrices of each classification model are presented in tables. Social media messages corpus consists of 108 public and 66 private universities messages in Turkey. Twitter is used for source of corpus. Twitter user messages are collected using Twitter Streaming API. Results are shown in graphics and tables.\n    ",
        "submission_date": "2014-10-10T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2910",
        "title": "Riesz Logic",
        "authors": [
            "Daoud Clarke"
        ],
        "abstract": "We introduce Riesz Logic, whose models are abelian lattice ordered groups, which generalise Riesz spaces (vector lattices), and show soundness and completeness. Our motivation is to provide a logic for distributional semantics of natural language, where words are typically represented as elements of a vector space whose dimensions correspond to contexts in which words may occur. This basis provides a lattice ordering on the space, and this ordering may be interpreted as \"distributional entailment\". Several axioms of Riesz Logic are familiar from Basic Fuzzy Logic, and we show how the models of these two logics may be related; Riesz Logic may thus be considered a new fuzzy logic. In addition to applications in natural language processing, there is potential for applying the theory to neuro-fuzzy systems.\n    ",
        "submission_date": "2014-10-10T00:00:00",
        "last_modified_date": "2014-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3916",
        "title": "Memory Networks",
        "authors": [
            "Jason Weston",
            "Sumit Chopra",
            "Antoine Bordes"
        ],
        "abstract": "We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4510",
        "title": "Graph-Sparse LDA: A Topic Model with Structured Sparsity",
        "authors": [
            "Finale Doshi-Velez",
            "Byron Wallace",
            "Ryan Adams"
        ],
        "abstract": "Originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. The goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. In other cases, the content of the topic itself may be of intrinsic scientific interest.\n",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5078",
        "title": "Learning Vague Concepts for the Semantic Web",
        "authors": [
            "Paolo Pareti",
            "Ewan Klein"
        ],
        "abstract": "Ontologies can be a powerful tool for structuring knowledge, and they are currently the subject of extensive research. Updating the contents of an ontology or improving its interoperability with other ontologies is an important but difficult process. In this paper, we focus on the presence of vague concepts, which are pervasive in natural language, within the framework of formal ontologies. We will adopt a framework in which vagueness is captured via numerical restrictions that can be automatically adjusted. Since updating vague concepts, either through ontology alignment or ontology evolution, can lead to inconsistent sets of axioms, we define and implement a method to detecting and repairing such inconsistencies in a local fashion.\n    ",
        "submission_date": "2014-10-19T00:00:00",
        "last_modified_date": "2014-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6903",
        "title": "Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech",
        "authors": [
            "Laxmi Narayana M.",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used speech features in most speech and speaker recognition applications. In this paper, we study the effect of resampling a speech signal on these speech features. We first derive a relationship between the MFCC param- eters of the resampled speech and the MFCC parameters of the original speech. We propose six methods of calculating the MFCC parameters of downsampled speech by transforming the Mel filter bank used to com- pute MFCC of the original speech. We then experimentally compute the MFCC parameters of the down sampled speech using the proposed meth- ods and compute the Pearson coefficient between the MFCC parameters of the downsampled speech and that of the original speech to identify the most effective choice of Mel-filter band that enables the computed MFCC of the resampled speech to be as close as possible to the original speech sample MFCC.\n    ",
        "submission_date": "2014-10-25T00:00:00",
        "last_modified_date": "2014-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8027",
        "title": "Towards a Visual Turing Challenge",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "As language and visual understanding by machines progresses rapidly, we are observing an increasing interest in holistic architectures that tightly interlink both modalities in a joint learning and inference process. This trend has allowed the community to progress towards more challenging and open tasks and refueled the hope at achieving the old AI dream of building machines that could pass a turing test in open domains. In order to steadily make progress towards this goal, we realize that quantifying performance becomes increasingly difficult. Therefore we ask how we can precisely define such challenges and how we can evaluate different algorithms on this open tasks? In this paper, we summarize and discuss such challenges as well as try to give answers where appropriate options are available in the literature. We exemplify some of the solutions on a recently presented dataset of question-answering task based on real-world indoor images that establishes a visual turing challenge. Finally, we argue despite the success of unique ground-truth annotation, we likely have to step away from carefully curated dataset and rather rely on 'social consensus' as the main driving force to create suitable benchmarks. Providing coverage in this inherently ambiguous output space is an emerging challenge that we face in order to make quantifiable progress in this area.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8326",
        "title": "Towards Learning Object Affordance Priors from Technical Texts",
        "authors": [
            "Nicholas H. Kirk"
        ],
        "abstract": "Everyday activities performed by artificial assistants can potentially be executed naively and dangerously given their lack of common sense knowledge. This paper presents conceptual work towards obtaining prior knowledge on the usual modality (passive or active) of any given entity, and their affordance estimates, by extracting high-confidence ability modality semantic relations (X can Y relationship) from non-figurative texts, by analyzing co-occurrence of grammatical instances of subjects and verbs, and verbs and objects. The discussion includes an outline of the concept, potential and limitations, and possible feature and learning framework adoption.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8749",
        "title": "What a Nasty day: Exploring Mood-Weather Relationship from Twitter",
        "authors": [
            "Jiwei Li",
            "Xun Wang",
            "Eduard Hovy"
        ],
        "abstract": "While it has long been believed in psychology that weather somehow influences human's mood, the debates have been going on for decades about how they are correlated. In this paper, we try to study this long-lasting topic by harnessing a new source of data compared from traditional psychological researches: Twitter. We analyze 2 years' twitter data collected by twitter API which amounts to $10\\%$ of all postings and try to reveal the correlations between multiple dimensional structure of human mood with meteorological effects. Some of our findings confirm existing hypotheses, while others contradict them. We are hopeful that our approach, along with the new data source, can shed on the long-going debates on weather-mood correlation.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8808",
        "title": "A Semantic Web of Know-How: Linked Data for Community-Centric Tasks",
        "authors": [
            "Paolo Pareti",
            "Ewan Klein",
            "Adam Barker"
        ],
        "abstract": "This paper proposes a novel framework for representing community know-how on the Semantic Web. Procedural knowledge generated by web communities typically takes the form of natural language instructions or videos and is largely unstructured. The absence of semantic structure impedes the deployment of many useful applications, in particular the ability to discover and integrate know-how automatically. We discuss the characteristics of community know-how and argue that existing knowledge representation frameworks fail to represent it adequately. We present a novel framework for representing the semantic structure of community know-how and demonstrate the feasibility of our approach by providing a concrete implementation which includes a method for automatically acquiring procedural knowledge for real-world tasks.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0861",
        "title": "Using Linguistic Features to Estimate Suicide Probability of Chinese Microblog Users",
        "authors": [
            "Lei Zhang",
            "Xiaolei Huang",
            "Tianli Liu",
            "Zhenxiang Chen",
            "Tingshao Zhu"
        ],
        "abstract": "If people with high risk of suicide can be identified through social media like microblog, it is possible to implement an active intervention system to save their lives. Based on this motivation, the current study administered the Suicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is a leading microblog service provider in China. Two NLP (Natural Language Processing) methods, the Chinese edition of Linguistic Inquiry and Word Count (LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extract linguistic features from the Sina Weibo data. We trained predicting models by machine learning algorithm based on these two types of features, to estimate suicide probability based on linguistic features. The experiment results indicate that LDA can find topics that relate to suicide probability, and improve the performance of prediction. Our study adds value in prediction of suicidal probability of social network users with their behaviors.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1006",
        "title": "A Probabilistic Translation Method for Dictionary-based Cross-lingual Information Retrieval in Agglutinative Languages",
        "authors": [
            "Javid Dadashkarimi",
            "Azadeh Shakery",
            "Heshaam Faili"
        ],
        "abstract": "Translation ambiguity, out of vocabulary words and missing some translations in bilingual dictionaries make dictionary-based Cross-language Information Retrieval (CLIR) a challenging task. Moreover, in agglutinative languages which do not have reliable stemmers, missing various lexical formations in bilingual dictionaries degrades CLIR performance. This paper aims to introduce a probabilistic translation model to solve the ambiguity problem, and also to provide most likely formations of a dictionary candidate. We propose Minimum Edit Support Candidates (MESC) method that exploits a monolingual corpus and a bilingual dictionary to translate users' native language queries to documents' language. Our experiments show that the proposed method outperforms state-of-the-art dictionary-based English-Persian CLIR.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1147",
        "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction",
        "authors": [
            "Waleed Ammar",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observable data using a feature-rich conditional random field. Then a reconstruction of the input is (re)generated, conditional on the latent structure, using models for which maximum likelihood estimation has a closed-form. Our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. We show competitive results with instantiations of the model for two canonical NLP tasks: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1243",
        "title": "Using Twitter to predict football outcomes",
        "authors": [
            "Stylianos Kampakis",
            "Andreas Adamides"
        ],
        "abstract": "Twitter has been proven to be a notable source for predictive modelling on various domains such as the stock market, the dissemination of diseases or sports outcomes. However, such a study has not been conducted in football (soccer) so far. The purpose of this research was to study whether data mined from Twitter can be used for this purpose. We built a set of predictive models for the outcome of football games of the English Premier League for a 3 month period based on tweets and we studied whether these models can overcome predictive models which use only historical data and simple football statistics. Moreover, combined models are constructed using both Twitter and historical data. The final results indicate that data mined from Twitter can indeed be a useful source for predicting games in the Premier League. The final Twitter-based model performs significantly better than chance when measured by Cohen's kappa and is comparable to the model that uses simple statistics and historical data. Combining both models raises the performance higher than it was achieved by each individual model. Thereby, this study provides evidence that Twitter derived features can indeed provide useful information for the prediction of football (soccer) outcomes.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1999",
        "title": "Azhary: An Arabic Lexical Ontology",
        "authors": [
            "Hossam Ishkewy",
            "Hany Harb",
            "Hassan Farahat"
        ],
        "abstract": "Arabic language is the most spoken languages in the Semitic languages group, and one of the most common languages in the world spoken by more than 422 million. It is also of paramount importance to Muslims, it is a sacred language of the Islamic Holly Book (Quran) and prayer (and other acts of worship) in Islam is performed only by mastering some of Arabic words. Arabic is also a major ritual language of a number of Christian churches in the Arab world and it is also used in writing several intellectual and religious Jewish books in the Middle Ages. Despite this, there is no semantic Arabic lexicon which researchers can depend on. In this paper we introduce Azhary as a lexical ontology for the Arabic language. It groups Arabic words into sets of synonyms called synsets, and records a number of relationships between words such as synonym, antonym, hypernym, hyponym, meronym, holonym and association relations. The ontology contains 26,195 words organized in 13,328 synsets. It has been developed and contrasted against AWN which is the most common available Arabic lexical ontology.\n    ",
        "submission_date": "2014-11-07T00:00:00",
        "last_modified_date": "2014-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2539",
        "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
        "authors": [
            "Ryan Kiros",
            "Ruslan Salakhutdinov",
            "Richard S. Zemel"
        ],
        "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2674",
        "title": "The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation",
        "authors": [
            "Fangjian Guo",
            "Charles Blundell",
            "Hanna Wallach",
            "Katherine Heller"
        ],
        "abstract": "We present the Bayesian Echo Chamber, a new Bayesian generative model for social interaction data. By modeling the evolution of people's language usage over time, this model discovers latent influence relationships between them. Unlike previous work on inferring influence, which has primarily focused on simple temporal dynamics evidenced via turn-taking behavior, our model captures more nuanced influence relationships, evidenced via linguistic accommodation patterns in interaction content. The model, which is based on a discrete analog of the multivariate Hawkes process, permits a fully Bayesian inference algorithm. We validate our model's ability to discover latent influence patterns using transcripts of arguments heard by the US Supreme Court and the movie \"12 Angry Men.\" We showcase our model's capabilities by using it to infer latent influence patterns from Federal Open Market Committee meeting transcripts, demonstrating state-of-the-art performance at uncovering social dynamics in group discussions.\n    ",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2679",
        "title": "Inferring User Preferences by Probabilistic Logical Reasoning over Social Networks",
        "authors": [
            "Jiwei Li",
            "Alan Ritter",
            "Dan Jurafsky"
        ],
        "abstract": "We propose a framework for inferring the latent attitudes or preferences of users by performing probabilistic first-order logical reasoning over the social network graph. Our method answers questions about Twitter users like {\\em Does this user like sushi?} or {\\em Is this user a New York Knicks fan?} by building a probabilistic model that reasons over user attributes (the user's location or gender) and the social network (the user's friends and spouse), via inferences like homophily (I am more likely to like sushi if spouse or friends like sushi, I am more likely to like the Knicks if I live in New York). The algorithm uses distant supervision, semi-supervised data harvesting and vector space models to extract user attributes (e.g. spouse, education, location) and preferences (likes and dislikes) from text. The extracted propositions are then fed into a probabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft Logic). Our experiments show that probabilistic logical reasoning significantly improves the performance on attribute and relation extraction, and also achieves an F-score of 0.791 at predicting a users likes or dislikes, significantly better than two strong baselines.\n    ",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2014-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3827",
        "title": "Autonomization of Monoidal Categories",
        "authors": [
            "Antonin Delpeuch"
        ],
        "abstract": "We show that contrary to common belief in the DisCoCat community, a monoidal category is all that is needed to define a categorical compositional model of  natural language. This relies on a construction which freely adds adjoints to a monoidal category. In the case of distributional semantics, this broadens the range of available models, to include non-linear maps and cartesian products for instance. We illustrate the applications of this principle to various  distributional models of meaning.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2020-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4194",
        "title": "ROSS User's Guide and Reference Manual (Version 1.0)",
        "authors": [
            "Glenn R. Hofford"
        ],
        "abstract": "The ROSS method is a new approach in the area of knowledge representation that is useful for many artificial intelligence and natural language understanding representation and reasoning tasks. (ROSS stands for \"Representation\", \"Ontology\", \"Structure\", \"Star\" language). ROSS is a physical symbol-based representational scheme. ROSS provides a complex model for the declarative representation of physical structure and for the representation of processes and causality. From the metaphysical perspective, the ROSS view of external reality involves a 4D model, wherein discrete single-time-point unit-sized locations with states are the basis for all objects, processes and aspects that can be modeled. ROSS includes a language called \"Star\" for the specification of ontology classes. The ROSS method also includes a formal scheme called the \"instance model\". Instance models are used in the area of natural language meaning representation to represent situations. This document is an in-depth specification of the ROSS method.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4825",
        "title": "Cognitive Systems and Question Answering",
        "authors": [
            "Ulrich Furbach",
            "Claudia Schon",
            "Frieder Stolzenburg"
        ],
        "abstract": "This paper briefly characterizes the field of cognitive computing. As an exemplification, the field of natural language question answering is introduced together with its specific challenges. A possibility to master these challenges is illustrated by a detailed presentation of the LogAnswer system, which is a successful representative of the field of natural language question answering.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4925",
        "title": "Linguistic Descriptions for Automatic Generation of Textual Short-Term Weather Forecasts on Real Prediction Data",
        "authors": [
            "A. Ramos-Soto",
            "A. Bugar\u00edn",
            "S. Barro",
            "J. Taboada"
        ],
        "abstract": "We present in this paper an application which automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data together with a natural language generation (NLG) system. The application, named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the natural language generation system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation GALiWeather will be released as a real service offering custom forecasts for a wide public.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4952",
        "title": "From Captions to Visual Concepts and Back",
        "authors": [
            "Hao Fang",
            "Saurabh Gupta",
            "Forrest Iandola",
            "Rupesh Srivastava",
            "Li Deng",
            "Piotr Doll\u00e1r",
            "Jianfeng Gao",
            "Xiaodong He",
            "Margaret Mitchell",
            "John C. Platt",
            "C. Lawrence Zitnick",
            "Geoffrey Zweig"
        ],
        "abstract": "  This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5654",
        "title": "Learning a Recurrent Visual Representation for Image Caption Generation",
        "authors": [
            "Xinlei Chen",
            "C. Lawrence Zitnick"
        ],
        "abstract": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over $19.8\\%$ of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5726",
        "title": "CIDEr: Consensus-based Image Description Evaluation",
        "authors": [
            "Ramakrishna Vedantam",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "abstract": "Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric (CIDEr) that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1454",
        "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation",
        "authors": [
            "Noam Shazeer",
            "Joris Pelemans",
            "Ciprian Chelba"
        ],
        "abstract": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM $n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as $n$-gram LMs do.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2015-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2486",
        "title": "Optimization models of natural communication",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "A family of information theoretic models of communication was introduced more than a decade ago to explain the origins of Zipf's law for word frequencies. The family is a based on a combination of two information theoretic principles: maximization of mutual information between forms and meanings and minimization of form entropy. The family also sheds light on the origins of three other patterns: the principle of contrast, a related vocabulary learning bias and the meaning-frequency law. Here two important components of the family, namely the information theoretic principles and the energy function that combines them linearly, are reviewed from the perspective of psycholinguistics, language learning, information theory and synergetic linguistics. The minimization of this linear function is linked to the problem of compression of standard information theory and might be tuned by self-organization.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2487",
        "title": "Word learning under infinite uncertainty",
        "authors": [
            "Richard A. Blythe",
            "Andrew D. M. Smith",
            "Kenny Smith"
        ],
        "abstract": "Language learners must learn the meanings of many thousands of words, despite those words occurring in complex environments in which infinitely many meanings might be inferred by the learner as a word's true meaning. This problem of infinite referential uncertainty is often attributed to Willard Van Orman Quine. We provide a mathematical formalisation of an ideal cross-situational learner attempting to learn under infinite referential uncertainty, and identify conditions under which word learning is possible. As Quine's intuitions suggest, learning under infinite uncertainty is in fact possible, provided that learners have some means of ranking candidate word meanings in terms of their plausibility; furthermore, our analysis shows that this ranking could in fact be exceedingly weak, implying that constraints which allow learners to infer the plausibility of candidate word meanings could themselves be weak. This approach lifts the burden of explanation from `smart' word learning constraints in learners, and suggests a programme of research into weak, unreliable, probabilistic constraints on the inference of word meaning in real word learners.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3714",
        "title": "Feature Weight Tuning for Recursive Neural Networks",
        "authors": [
            "Jiwei Li"
        ],
        "abstract": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \"weight tuning\" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4314",
        "title": "Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus",
        "authors": [
            "Joseph Chee Chang",
            "Chu-Cheng Lin"
        ],
        "abstract": "Mixed language data is one of the difficult yet less explored domains of natural language processing. Most research in fields like machine translation or sentiment analysis assume monolingual input. However, people who are capable of using more than one language often communicate using multiple languages at the same time. Sociolinguists believe this \"code-switching\" phenomenon to be socially motivated. For example, to express solidarity or to establish authority. Most past work depend on external tools or resources, such as part-of-speech tagging, dictionary look-up, or named-entity recognizers to extract rich features for training machine learning models. In this paper, we train recurrent neural networks with only raw features, and use word embedding to automatically learn meaningful representations. Using the same mixed-language Twitter corpus, our system is able to outperform the best SVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in accuracy, or by 17% in error rate reduction.\n    ",
        "submission_date": "2014-12-14T00:00:00",
        "last_modified_date": "2014-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4401",
        "title": "Tools for Terminology Processing",
        "authors": [
            "C. Enguehard",
            "B. Daille",
            "E. Morin"
        ],
        "abstract": "Automatic terminology processing appeared 10 years ago when electronic corpora became widely available. Such processing may be statistically or linguistically based and produces terminology resources that can be used in a number of applications : indexing, information retrieval, technology watch, etc. We present the tools that have been developed in the IRIN Institute. They all take as input texts (or collection of texts) and reflect different states of terminology processing: term acquisition, term recognition and term structuring.\n    ",
        "submission_date": "2014-12-14T00:00:00",
        "last_modified_date": "2014-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4729",
        "title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks",
        "authors": [
            "Subhashini Venugopalan",
            "Huijuan Xu",
            "Jeff Donahue",
            "Marcus Rohrbach",
            "Raymond Mooney",
            "Kate Saenko"
        ],
        "abstract": "Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5448",
        "title": "Extended Recommendation Framework: Generating the Text of a User Review as a Personalized Summary",
        "authors": [
            "Micka\u00ebl Poussevin",
            "Vincent Guigue",
            "Patrick Gallinari"
        ],
        "abstract": "We propose to augment rating based recommender systems by providing the user with additional information which might help him in his choice or in the understanding of the recommendation. We consider here as a new task, the generation of personalized reviews associated to items. We use an extractive summary formulation for generating these reviews. We also show that the two information sources, ratings and items could be used both for estimating ratings and for generating summaries, leading to improved performance for each system compared to the use of a single source. Besides these two contributions, we show how a personalized polarity classifier can integrate the rating and textual aspects. Overall, the proposed system offers the user three personalized hints for a recommendation: rating, text and polarity. We evaluate these three components on two datasets using appropriate measures for each task.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6069",
        "title": "Annotation as a New Paradigm in Research Archiving",
        "authors": [
            "Dirk Roorda",
            "Charles van den Heuvel"
        ],
        "abstract": "We outline a paradigm to preserve results of digital scholarship, whether they are query results, feature values, or topic assignments. This paradigm is characterized by using annotations as multifunctional carriers and making them portable. The testing grounds we have chosen are two significant enterprises, one in the history of science, and one in Hebrew scholarship. The first one (CKCC) focuses on the results of a project where a Dutch consortium of universities, research institutes, and cultural heritage institutions experimented for 4 years with language techniques and topic modeling methods with the aim to analyze the emergence of scholarly debates. The data: a complex set of about 20.000 letters. The second one (DTHB) is a multi-year effort to express the linguistic features of the Hebrew bible in a text database, which is still growing in detail and sophistication. Versions of this database are packaged in commercial bible study software. We state that the results of these forms of scholarship require new knowledge management and archive practices. Only when researchers can build efficiently on each other's (intermediate) results, they can achieve the aggregations of quality data by which new questions can be answered, and hidden patterns visualized. Archives are required to find a balance between preserving authoritative versions of sources and supporting collaborative efforts in digital scholarship. Annotations are promising vehicles for preserving and reusing research results. Keywords annotation, portability, archiving, queries, features, topics, keywords, Republic of Letters, Hebrew text databases.\n    ",
        "submission_date": "2014-10-07T00:00:00",
        "last_modified_date": "2014-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6211",
        "title": "Multiple Authors Detection: A Quantitative Analysis of Dream of the Red Chamber",
        "authors": [
            "Xianfeng Hu",
            "Yang Wang",
            "Qiang Wu"
        ],
        "abstract": "Inspired by the authorship controversy of Dream of the Red Chamber and the application of machine learning in the study of literary stylometry, we develop a rigorous new method for the mathematical analysis of authorship by testing for a so-called chrono-divide in writing styles. Our method incorporates some of the latest advances in the study of authorship attribution, particularly techniques from support vector machines. By introducing the notion of relative frequency as a feature ranking metric our method proves to be highly effective and robust.\n",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6577",
        "title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks",
        "authors": [
            "Ozan \u0130rsoy",
            "Claire Cardie"
        ],
        "abstract": "We present the multiplicative recurrent neural network as a general model for compositional meaning in language, and evaluate it on the task of fine-grained sentiment analysis. We establish a connection to the previously investigated matrix-space models for compositionality, and show they are special cases of the multiplicative recurrent net. Our experiments show that these models perform comparably or better than Elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus. Furthermore, they yield comparable results to structural deep models on the recently published Stanford Sentiment Treebank without the need for generating parse trees.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6632",
        "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)",
        "authors": [
            "Junhua Mao",
            "Wei Xu",
            "Yi Yang",
            "Jiang Wang",
            "Zhiheng Huang",
            "Alan Yuille"
        ],
        "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6645",
        "title": "Weakly Supervised Multi-Embeddings Learning of Acoustic Models",
        "authors": [
            "Gabriel Synnaeve",
            "Emmanuel Dupoux"
        ],
        "abstract": "We trained a Siamese network with multi-task same/different information on a speech dataset, and found that it was possible to share a network for both tasks without a loss in performance. The first task was to discriminate between two same or different words, and the second was to discriminate between two same or different talkers.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6650",
        "title": "Incremental Adaptation Strategies for Neural Network Language Models",
        "authors": [
            "Aram Ter-Sarkisov",
            "Holger Schwenk",
            "Loic Barrault",
            "Fethi Bougares"
        ],
        "abstract": "It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without overfitting the small adaptation data.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6881",
        "title": "On Learning Vector Representations in Hierarchical Label Spaces",
        "authors": [
            "Jinseok Nam",
            "Johannes F\u00fcrnkranz"
        ],
        "abstract": "An important problem in multi-label classification is to capture label patterns or underlying structures that have an impact on such patterns. This paper addresses one such problem, namely how to exploit hierarchical structures over labels. We present a novel method to learn vector representations of a label space given a hierarchy of labels and label co-occurrence patterns. Our experimental results demonstrate qualitatively that the proposed method is able to learn regularities among labels by exploiting a label hierarchy as well as label co-occurrences. It highlights the importance of the hierarchical information in order to obtain regularities which facilitate analogical reasoning over a label space. We also experimentally illustrate the dependency of the learned representations on the label hierarchy.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7028",
        "title": "Joint RNN-Based Greedy Parsing and Word Composition",
        "authors": [
            "Jo\u00ebl Legrand",
            "Ronan Collobert"
        ],
        "abstract": "This paper introduces a greedy parser based on neural networks, which leverages a new compositional sub-tree representation. The greedy parser and the compositional procedure are jointly trained, and tightly depends on each-other. The composition procedure outputs a vector representation which summarizes syntactically (parsing tags) and semantically (words) sub-trees. Composition and tagging is achieved over continuous (word or tag) representations, and recurrent neural networks. We reach F1 performance on par with well-known existing parsers, while having the advantage of speed, thanks to the greedy nature of the parser. We provide a fully functional implementation of the method described in this paper.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7091",
        "title": "Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets",
        "authors": [
            "Pascal Vincent",
            "Alexandre de Br\u00e9bisson",
            "Xavier Bouthillier"
        ],
        "abstract": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d^2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7110",
        "title": "Learning linearly separable features for speech recognition using convolutional neural networks",
        "authors": [
            "Dimitri Palaz",
            "Mathew Magimai Doss",
            "Ronan Collobert"
        ],
        "abstract": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7782",
        "title": "Plagiarism Detection on Electronic Text based Assignments using Vector Space Model (ICIAfS14)",
        "authors": [
            "MAC Jiffriya",
            "MAC Akmal Jahan",
            "Roshan G. Ragel"
        ],
        "abstract": "Plagiarism is known as illegal use of others' part of work or whole work as one's own in any field such as art, poetry, literature, cinema, research and other creative forms of study. Plagiarism is one of the important issues in academic and research fields and giving more concern in academic systems. The situation is even worse with the availability of ample resources on the web. This paper focuses on an effective plagiarism detection tool on identifying suitable intra-corpal plagiarism detection for text based assignments by comparing unigram, bigram, trigram of vector space model with cosine similarity measure. Manually evaluated, labelled dataset was tested using unigram, bigram and trigram vector. Even though trigram vector consumes comparatively more time, it shows better results with the labelled data. In addition, the selected trigram vector space model with cosine similarity measure is compared with tri-gram sequence matching technique with Jaccard measure. In the results, cosine similarity score shows slightly higher values than the other. Because, it focuses on giving more weight for terms that do not frequently exist in the dataset and cosine similarity measure using trigram technique is more preferable than the other. Therefore, we present our new tool and it could be used as an effective tool to evaluate text based electronic assignments and minimize the plagiarism among students.\n    ",
        "submission_date": "2014-12-25T00:00:00",
        "last_modified_date": "2014-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8102",
        "title": "Proceedings of the 11th workshop on Quantum Physics and Logic",
        "authors": [
            "Bob Coecke",
            "Ichiro Hasuo",
            "Prakash Panangaden"
        ],
        "abstract": "This volume contains the proceedings of the 11th International Workshop on Quantum Physics and Logic (QPL 2014), which was held from the 4th to the 6th of June, 2014, at Kyoto University, Japan.\n",
        "submission_date": "2014-12-28T00:00:00",
        "last_modified_date": "2014-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8527",
        "title": "From Logical to Distributional Models",
        "authors": [
            "Anne Preller"
        ],
        "abstract": "The paper relates two variants of semantic models for natural language, logical functional models and compositional distributional vector space models, by transferring the logic and reasoning from the logical to the distributional models.\n",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    }
]