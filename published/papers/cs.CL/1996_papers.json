[
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601001",
        "title": "Automatic Inference of DATR Theories",
        "authors": [
            "Petra Barg"
        ],
        "abstract": "  This paper presents an approach for the automatic acquisition of linguistic knowledge from unstructured data. The acquired knowledge is represented in the lexical knowledge representation language DATR. A set of transformation rules that establish inheritance relationships and a default-inference algorithm make up the basis components of the system. Since the overall approach is not restricted to a special domain, the heuristic inference strategy uses criteria to evaluate the quality of a DATR theory, where different domains may require different criteria. The system is applied to the linguistic learning task of German noun inflection.\n    ",
        "submission_date": "1996-01-04T00:00:00",
        "last_modified_date": "1996-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601002",
        "title": "Generic rules and non-constituent coordination",
        "authors": [
            "Julio Gonzalo",
            "Teresa Solias"
        ],
        "abstract": "  We present a metagrammatical formalism, {\\em generic rules}, to give a default interpretation to grammar rules. Our formalism introduces a process of {\\em dynamic binding} interfacing the level of pure grammatical knowledge representation and the parsing level. We present an approach to non-constituent coordination within categorial grammars, and reformulate it as a generic rule. This reformulation is context-free parsable and reduces drastically the search space associated to the parsing task for such phenomena.\n    ",
        "submission_date": "1996-01-09T00:00:00",
        "last_modified_date": "1996-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601003",
        "title": "Report of the Study Group on Assessment and Evaluation",
        "authors": [
            "Richard Crouch",
            "Robert Gaizauskas",
            "Klaus Netter"
        ],
        "abstract": "  This is an interim report discussing possible guidelines for the assessment and evaluation of projects developing speech and language systems. It was prepared at the request of the European Commission DG XIII by an ad hoc study group, and is now being made available in the form in which it was submitted to the Commission. However, the report is not an official European Commission document, and does not reflect European Commission policy, official or otherwise.\n",
        "submission_date": "1996-01-18T00:00:00",
        "last_modified_date": "1996-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601004",
        "title": "Similarity between Words Computed by Spreading Activation on an English Dictionary",
        "authors": [
            "Hideki Kozima",
            "Teiji Furugori"
        ],
        "abstract": "  This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis. The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary, LDOCE (Longman Dictionary of Contemporary English). Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary, and indirectly the similarity of all the other words in LDOCE. The similarity represents the strength of lexical cohesion or semantic relation, and also provides valuable information about similarity and coherence of texts.\n    ",
        "submission_date": "1996-01-23T00:00:00",
        "last_modified_date": "1996-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601005",
        "title": "Text Segmentation Based on Similarity between Words",
        "authors": [
            "Hideki Kozima"
        ],
        "abstract": "  This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis.\n    ",
        "submission_date": "1996-01-23T00:00:00",
        "last_modified_date": "1996-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601006",
        "title": "Possessive Pronouns as Determiners in Japanese-to-English Machine Translation",
        "authors": [
            "Francis Bond",
            "Kentaro Ogura",
            "Satoru Ikehara"
        ],
        "abstract": "  Possessive pronouns are used as determiners in English when no equivalent would be used in a Japanese sentence with the same meaning. This paper proposes a heuristic method of generating such possessive pronouns even when there is no equivalent in the Japanese. The method uses information about the use of possessive pronouns in English treated as a lexical property of nouns, in addition to contextual information about noun phrase referentiality and the subject and main verb of the sentence that the noun phrase appears in. The proposed method has been implemented in NTT Communication Science Laboratories' Japanese-to-English machine translation system ALT-J/E. In a test set of 6,200 sentences, the proposed method increased the number of noun phrases with appropriate possessive pronouns generated, by 263 to 609, at the cost of generating 83 noun phrases with inappropriate possessive pronouns.\n    ",
        "submission_date": "1996-01-23T00:00:00",
        "last_modified_date": "1996-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601007",
        "title": "Context-Sensitive Measurement of Word Distance by Adaptive Scaling of a Semantic Space",
        "authors": [
            "Hideki Kozima",
            "Akira Ito"
        ],
        "abstract": "  The paper proposes a computationally feasible method for measuring context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multi-dimensional vector which is obtained from an English dictionary through a principal component analysis. Given a word set C which specifies a context for measuring word distance, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, distance between words in V becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text.\n    ",
        "submission_date": "1996-01-23T00:00:00",
        "last_modified_date": "1996-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601008",
        "title": "Noun Phrase Reference in Japanese-to-English Machine Translation",
        "authors": [
            "Francis Bond",
            "Kentaro Ogura",
            "Tsukasa Kawaoka"
        ],
        "abstract": "  This paper shows the necessity of distinguishing different referential uses of noun phrases in machine translation. We argue that differentiating between the generic, referential and ascriptive uses of noun phrases is the minimum necessary to generate articles and number correctly when translating from Japanese to English. Heuristics for determining these differences are proposed for a Japanese-to-English machine translation system. Finally the results of using the proposed heuristics are shown to have raised the percentage of noun phrases generated with correct use of articles and number in the Japanese-to-English machine translation system ALT-J/E from 65% to 77%.\n    ",
        "submission_date": "1996-01-23T00:00:00",
        "last_modified_date": "1996-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601009",
        "title": "A General Architecture for Language Engineering (GATE) - a new approach to Language Engineering R&D",
        "authors": [
            "Hamish Cunningham",
            "Robert J. Gaizauskas",
            "Yorick Wilks"
        ],
        "abstract": "  This report argues for the provision of a common software infrastructure for NLP systems. Current trends in Language Engineering research are reviewed as motivation for this infrastructure, and relevant recent work discussed. A freely-available system called GATE is described which builds on this work.\n    ",
        "submission_date": "1996-01-23T00:00:00",
        "last_modified_date": "1996-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601010",
        "title": "Parsing with Typed Feature Structures",
        "authors": [
            "Shuly Wintner",
            "Nissim Francez"
        ],
        "abstract": "  In this paper we provide for parsing with respect to grammars expressed in a general TFS-based formalism, a restriction of ALE. Our motivation being the design of an abstract (WAM-like) machine for the formalism, we consider parsing as a computational process and use it as an operational semantics to guide the design of the control structures for the abstract machine.\n",
        "submission_date": "1996-01-31T00:00:00",
        "last_modified_date": "1996-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9601011",
        "title": "Parsing with Typed Feature Structures",
        "authors": [
            "Shuly Wintner",
            "Nissim Francez"
        ],
        "abstract": "  In this paper we provide for parsing with respect to grammars expressed in a general TFS-based formalism, a restriction of ALE. Our motivation being the design of an abstract (WAM-like) machine for the formalism, we consider parsing as a computational process and use it as an operational semantics to guide the design of the control structures for the abstract machine.\n",
        "submission_date": "1996-01-31T00:00:00",
        "last_modified_date": "1996-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9602001",
        "title": "How Part-of-Speech Tags Affect Text Retrieval and Filtering Performance",
        "authors": [
            "Robert M. Losee"
        ],
        "abstract": "  Natural language processing (NLP) applied to information retrieval (IR) and filtering problems may assign part-of-speech tags to terms and, more generally, modify queries and documents. Analytic models can predict the performance of a text filtering system as it incorporates changes suggested by NLP, allowing us to make precise statements about the average effect of NLP operations on IR. Here we provide a model of retrieval and tagging that allows us to both compute the performance change due to syntactic parsing and to allow us to understand what factors affect performance and how. In addition to a prediction of performance with tags, upper and lower bounds for retrieval performance are derived, giving the best and worst effects of including part-of-speech tags. Empirical grounds for selecting sets of tags are considered.\n    ",
        "submission_date": "1996-02-08T00:00:00",
        "last_modified_date": "1996-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9602002",
        "title": "Situations and Computation: An Overview of Recent Research",
        "authors": [
            "Erkan TIN",
            "Varol AKMAN"
        ],
        "abstract": "  Serious thinking about the computational aspects of situation theory is just starting. There have been some recent proposals in this direction (viz. PROSIT and ASTL), with varying degrees of divergence from the ontology of the theory. We believe that a programming environment incorporating bona fide situation-theoretic constructs is needed and describe our very recent BABY-SIT implementation. A detailed critical account of PROSIT and ASTL is also offered in order to compare our system with these pioneering and influential frameworks.\n    ",
        "submission_date": "1996-02-15T00:00:00",
        "last_modified_date": "1996-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9602003",
        "title": "Text Windows and Phrases Differing by Discipline, Location in Document, and Syntactic Structure",
        "authors": [
            "Robert M. Losee"
        ],
        "abstract": "  Knowledge of window style, content, location and grammatical structure may be used to classify documents as originating within a particular discipline or may be used to place a document on a theory versus practice spectrum. This distinction is also studied here using the type-token ratio to differentiate between sublanguages. The statistical significance of windows is computed, based on the the presence of terms in titles, abstracts, citations, and section headers, as well as binary independent (BI) and inverse document frequency (IDF) weightings. The characteristics of windows are studied by examining their within window density (WWD) and the S concentration (SC), the concentration of terms from various document fields (e.g. title, abstract) in the fulltext. The rate of window occurrences from the beginning to the end of document fulltext differs between academic fields. Different syntactic structures in sublanguages are examined, and their use is considered for discriminating between specific academic disciplines and, more generally, between theory versus practice or knowledge versus applications oriented documents.\n    ",
        "submission_date": "1996-02-18T00:00:00",
        "last_modified_date": "1996-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9602004",
        "title": "Assessing agreement on classification tasks: the kappa statistic",
        "authors": [
            "Jean Carletta"
        ],
        "abstract": "  Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as a field adopting techniques from content analysis.\n    ",
        "submission_date": "1996-02-27T00:00:00",
        "last_modified_date": "1996-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9603001",
        "title": "Speech Recognition by Composition of Weighted Finite Automata",
        "authors": [
            "Fernando C. N. Pereira",
            "Michael D. Riley"
        ],
        "abstract": "  We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.\n    ",
        "submission_date": "1996-03-07T00:00:00",
        "last_modified_date": "1996-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9603002",
        "title": "Finite-State Approximation of Phrase-Structure Grammars",
        "authors": [
            "Fernando C. N. Pereira",
            "Rebecca N. Wright"
        ],
        "abstract": "  Phrase-structure grammars are effective models for important syntactic and semantic aspects of natural languages, but can be computationally too demanding for use as language models in real-time speech recognition. Therefore, finite-state models are used instead, even though they lack expressive power. To reconcile those two alternatives, we designed an algorithm to compute finite-state approximations of context-free grammars and context-free-equivalent augmented phrase-structure grammars. The approximation is exact for certain context-free grammars generating regular languages, including all left-linear and right-linear context-free grammars. The algorithm has been used to build finite-state language models for limited-domain speech recognition tasks.\n    ",
        "submission_date": "1996-03-08T00:00:00",
        "last_modified_date": "1996-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9603003",
        "title": "Attempto Controlled English (ACE)",
        "authors": [
            "Norbert E. Fuchs",
            "Rolf Schwitter"
        ],
        "abstract": "  Attempto Controlled English (ACE) allows domain specialists to interactively formulate requirements specifications in domain concepts. ACE can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage. The Attempto system translates specification texts in ACE into discourse representation structures and optionally into Prolog. Translated specification texts are incrementally added to a knowledge base. This knowledge base can be queried in ACE for verification, and it can be executed for simulation, prototyping and validation of the specification.\n    ",
        "submission_date": "1996-03-13T00:00:00",
        "last_modified_date": "1996-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9603004",
        "title": "Attempto - From Specifications in Controlled Natural Language towards Executable Specifications",
        "authors": [
            "Rolf Schwitter",
            "Norbert E. Fuchs"
        ],
        "abstract": "  Deriving formal specifications from informal requirements is difficult since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge the conceptual gap we propose controlled natural language as a textual view on formal specifications in logic. The specification language Attempto Controlled English (ACE) is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage. The Attempto system translates specifications in ACE into discourse representation structures and into Prolog. The resulting knowledge base can be queried in ACE for verification, and it can be executed for simulation, prototyping and validation of the specification.\n    ",
        "submission_date": "1996-03-12T00:00:00",
        "last_modified_date": "1996-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9603005",
        "title": "Integrated speech and morphological processing in a connectionist continuous speech understanding for Korean",
        "authors": [
            "Geunbae Lee",
            "Jong-Hyeok Lee"
        ],
        "abstract": "  A new tightly coupled speech and natural language integration model is presented for a TDNN-based continuous possibly large vocabulary speech recognition system for Korean. Unlike popular n-best techniques developed for integrating mainly HMM-based speech recognition and natural language processing in a {\\em word level}, which is obviously inadequate for morphologically complex agglutinative languages, our model constructs a spoken language system based on a {\\em morpheme-level} speech and language integration. With this integration scheme, the spoken Korean processing engine (SKOPE) is designed and implemented using a TDNN-based diphone recognition module integrated with a Viterbi-based lexical decoding and symbolic phonological/morphological co-analysis. Our experiment results show that the speaker-dependent continuous {\\em eojeol} (Korean word) recognition and integrated morphological analysis can be achieved with over 80.6% success rate directly from speech inputs for the middle-level vocabularies.\n    ",
        "submission_date": "1996-03-18T00:00:00",
        "last_modified_date": "1996-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9603006",
        "title": "Extraction of V-N-Collocations from Text Corpora: A Feasibility Study for German",
        "authors": [
            "Elisabeth Breidt"
        ],
        "abstract": "  The usefulness of a statistical approach suggested by Church et al. (1991) is evaluated for the extraction of verb-noun (V-N) collocations from German text corpora. Some problematic issues of that method arising from properties of the German language are discussed and various modifications of the method are considered that might improve extraction results for German. The precision and recall of all variant methods is evaluated for V-N collocations containing support verbs, and the consequences for further work on the extraction of collocations from German corpora are discussed.\n",
        "submission_date": "1996-03-18T00:00:00",
        "last_modified_date": "1996-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604001",
        "title": "Combining Hand-crafted Rules and Unsupervised Learning in Constraint-based Morphological Disambiguation",
        "authors": [
            "Kemal Oflazer",
            "Gokhan Tur"
        ],
        "abstract": "  This paper presents a constraint-based morphological disambiguation approach that is applicable languages with complex morphology--specifically agglutinative languages with productive inflectional and derivational morphological phenomena. In certain respects, our approach has been motivated by Brill's recent work, but with the observation that his transformational approach is not directly applicable to languages like Turkish. Our system combines corpus independent hand-crafted constraint rules, constraint rules that are learned via unsupervised learning from a training corpus, and additional statistical information from the corpus to be morphologically disambiguated. The hand-crafted rules are linguistically motivated and tuned to improve precision without sacrificing recall. The unsupervised learning process produces two sets of rules: (i) choose rules which choose morphological parses of a lexical item satisfying constraint effectively discarding other parses, and (ii) delete rules, which delete parses satisfying a constraint. Our approach also uses a novel approach to unknown word processing by employing a secondary morphological processor which recovers any relevant inflectional and derivational information from a lexical item whose root is unknown. With this approach, well below 1 percent of the tokens remains as unknown in the texts we have experimented with. Our results indicate that by combining these hand-crafted,statistical and learned information sources, we can attain a recall of 96 to 97 percent with a corresponding precision of 93 to 94 percent, and ambiguity of 1.02 to 1.03 parses per token.\n    ",
        "submission_date": "1996-04-11T00:00:00",
        "last_modified_date": "1996-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604002",
        "title": "A Constraint-based Case Frame Lexicon",
        "authors": [
            "Kemal Oflazer",
            "Okan Yilmaz"
        ],
        "abstract": "  We present a constraint-based case frame lexicon architecture for bi-directional mapping between a syntactic case frame and a semantic frame. The lexicon uses a semantic sense as the basic unit and employs a multi-tiered constraint structure for the resolution of syntactic information into the appropriate senses and/or idiomatic usage. Valency changing transformations such as morphologically marked passivized or causativized forms are handled via lexical rules that manipulate case frames templates. The system has been implemented in a typed-feature system and applied to Turkish.\n    ",
        "submission_date": "1996-04-11T00:00:00",
        "last_modified_date": "1996-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604003",
        "title": "Error-tolerant Tree Matching",
        "authors": [
            "Kemal Oflazer"
        ],
        "abstract": "  This paper presents an efficient algorithm for retrieving from a database of trees, all trees that match a given query tree approximately, that is, within a certain error tolerance. It has natural language processing applications in searching for matches in example-based translation systems, and retrieval from lexical databases containing entries of complex feature structures. The algorithm has been implemented on SparcStations, and for large randomly generated synthetic tree databases (some having tens of thousands of trees) it can associatively search for trees with a small error, in a matter of tenths of a second to few seconds.\n    ",
        "submission_date": "1996-04-11T00:00:00",
        "last_modified_date": "1996-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604004",
        "title": "Apportioning Development Effort in a Probabilistic LR Parsing System through Evaluation",
        "authors": [
            "John Carroll",
            "Ted Briscoe"
        ],
        "abstract": "  We describe an implemented system for robust domain-independent syntactic parsing of English, using a unification-based grammar of part-of-speech and punctuation labels coupled with a probabilistic LR parser. We present evaluations of the system's performance along several different dimensions; these enable us to assess the contribution that each individual part is making to the success of the system as a whole, and thus prioritise the effort to be devoted to its further enhancement. Currently, the system is able to parse around 80% of sentences in a substantial corpus of general text containing a number of distinct genres. On a random sample of 250 such sentences the system has a mean crossing bracket rate of 0.71 and recall and precision of 83% and 84% respectively when evaluated against manually-disambiguated analyses.\n    ",
        "submission_date": "1996-04-12T00:00:00",
        "last_modified_date": "1996-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604005",
        "title": "Better Language Models with Model Merging",
        "authors": [
            "Thorsten Brants"
        ],
        "abstract": "  This paper investigates model merging, a technique for deriving Markov models from text or speech corpora. Models are derived by starting with a large and specific model and by successively combining states to build smaller and more general models. We present methods to reduce the time complexity of the algorithm and report on experiments on deriving language models for a speech recognition task. The experiments show the advantage of model merging over the standard bigram approach. The merged model assigns a lower perplexity to the test set and uses considerably fewer states.\n    ",
        "submission_date": "1996-04-17T00:00:00",
        "last_modified_date": "1996-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604006",
        "title": "The Role of the Gricean Maxims in the Generation of Referring Expressions",
        "authors": [
            "Robert Dale",
            "Ehud Reiter"
        ],
        "abstract": "  Grice's maxims of conversation [Grice 1975] are framed as directives to be followed by a speaker of the language. This paper argues that, when considered from the point of view of natural language generation, such a characterisation is rather misleading, and that the desired behaviour falls out quite naturally if we view language generation as a goal-oriented process. We argue this position with particular regard to the generation of referring expressions.\n    ",
        "submission_date": "1996-04-18T00:00:00",
        "last_modified_date": "1996-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604007",
        "title": "Collocational Grammar",
        "authors": [
            "Robert John Freeman"
        ],
        "abstract": "  A perspective of statistical language models which emphasizes their collocational aspect is advocated. It is suggested that strings be generalized in terms of classes of relationships instead of classes of objects. The single most important characteristic of such a model is a mechanism for comparing patterns. When patterns are fully generalized a natural definition of syntactic class emerges as a subset of relational class. These collocational syntactic classes should be an unambiguous partition of traditional syntactic classes.\n    ",
        "submission_date": "1996-04-19T00:00:00",
        "last_modified_date": "1996-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604008",
        "title": "Efficient Algorithms for Parsing the DOP Model",
        "authors": [
            "Joshua Goodman"
        ],
        "abstract": "  Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.\n    ",
        "submission_date": "1996-04-22T00:00:00",
        "last_modified_date": "1996-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604009",
        "title": "Another Facet of LIG Parsing",
        "authors": [
            "Pierre Boullier"
        ],
        "abstract": "  In this paper we present a new parsing algorithm for linear indexed grammars (LIGs) in the same spirit as the one described in (Vijay-Shanker and Weir, 1993) for tree adjoining grammars. For a LIG $L$ and an input string $x$ of length $n$, we build a non ambiguous context-free grammar whose sentences are all (and exclusively) valid derivation sequences in $L$ which lead to $x$. We show that this grammar can be built in ${\\cal O}(n^6)$ time and that individual parses can be extracted in linear time with the size of the extracted parse tree. Though this ${\\cal O}(n^6)$ upper bound does not improve over previous results, the average case behaves much better. Moreover, practical parsing times can be decreased by some statically performed computations.\n    ",
        "submission_date": "1996-04-23T00:00:00",
        "last_modified_date": "1996-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604010",
        "title": "Off-line Constraint Propagation for Efficient HPSG Processing",
        "authors": [
            "Walt Detmar Meurers",
            "Guido Minnen"
        ],
        "abstract": "  We investigate the use of a technique developed in the constraint programming community called constraint propagation to automatically make a HPSG theory more specific at those places where linguistically motivated underspecification would lead to inefficient processing. We discuss two concrete HPSG examples showing how off-line constraint propagation helps improve processing efficiency.\n    ",
        "submission_date": "1996-04-23T00:00:00",
        "last_modified_date": "1996-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604011",
        "title": "Multi-level post-processing for Korean character recognition using morphological analysis and linguistic evaluation",
        "authors": [
            "Geunbae Lee",
            "Jong-Hyeok Lee",
            "JinHee Yoo"
        ],
        "abstract": "  Most of the post-processing methods for character recognition rely on contextual information of character and word-fragment levels. However, due to linguistic characteristics of Korean, such low-level information alone is not sufficient for high-quality character-recognition applications, and we need much higher-level contextual information to improve the recognition results. This paper presents a domain independent post-processing technique that utilizes multi-level morphological, syntactic, and semantic information as well as character-level information. The proposed post-processing system performs three-level processing: candidate character-set selection, candidate eojeol (Korean word) generation through morphological analysis, and final single eojeol-sequence selection by linguistic evaluation. All the required linguistic information and probabilities are automatically acquired from a statistical corpus analysis. Experimental results demonstrate the effectiveness of our method, yielding error correction rate of 80.46%, and improved recognition rate of 95.53% from before-post-processing rate 71.2% for single best-solution selection.\n    ",
        "submission_date": "1996-04-24T00:00:00",
        "last_modified_date": "1996-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604012",
        "title": "SemHe: A Generalised Two-Level System",
        "authors": [
            "George Anton Kiraz"
        ],
        "abstract": "  This paper presents a generalised two-level implementation which can handle linear and non-linear morphological operations. An algorithm for the interpretation of multi-tape two-level rules is described. In addition, a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac.\n    ",
        "submission_date": "1996-04-24T00:00:00",
        "last_modified_date": "1996-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604013",
        "title": "Syntactic Analyses for Parallel Grammars: Auxiliaries and Genitive NPs",
        "authors": [
            "Miriaam Butt",
            "Christian Fortman",
            "Christian Rohrer"
        ],
        "abstract": "  This paper focuses on two disparate aspects of German syntax from the perspective of parallel grammar development. As part of a cooperative project, we present an innovative approach to auxiliaries and multiple genitive NPs in German. The LFG-based implementation presented here avoids unnessary structural complexity in the representation of auxiliaries by challenging the traditional analysis of auxiliaries as raising verbs. The approach developed for multiple genitive NPs provides a more abstract, language independent representation of genitives associated with nominalized verbs. Taken together, the two approaches represent a step towards providing uniformly applicable treatments for differing languages, thus lightening the burden for machine translation.\n    ",
        "submission_date": "1996-04-24T00:00:00",
        "last_modified_date": "1996-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604014",
        "title": "The importance of being lazy -- using lazy evaluation to process queries to HPSG grammars",
        "authors": [
            "Thilo G\u00f6tz",
            "Walt Detmar Meurers"
        ],
        "abstract": "  Linguistic theories formulated in the architecture of {\\sc hpsg} can be very precise and explicit since {\\sc hpsg} provides a formally well-defined setup. However, when querying a faithful implementation of such an explicit theory, the large data structures specified can make it hard to see the relevant aspects of the reply given by the system. Furthermore, the system spends much time applying constraints which can never fail just to be able to enumerate specific answers. In this paper we want to describe lazy evaluation as the result of an off-line compilation technique. This method of evaluation can be used to answer queries to an {\\sc hpsg} system so that only the relevant aspects are checked and output.\n    ",
        "submission_date": "1996-04-25T00:00:00",
        "last_modified_date": "1996-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604015",
        "title": "Computing Prosodic Morphology",
        "authors": [
            "George Anton Kiraz"
        ],
        "abstract": "  This paper establishes a framework under which various aspects of prosodic morphology, such as templatic morphology and infixation, can be handled under two-level theory using an implemented multi-tape two-level model. The paper provides a new computational analysis of root-and-pattern morphology based on prosody.\n    ",
        "submission_date": "1996-04-25T00:00:00",
        "last_modified_date": "1996-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604016",
        "title": "Processing Metonymy: a Domain-Model Heuristic Graph Traversal Approach",
        "authors": [
            "Jacques Bouaud",
            "Bruno Bachimont",
            "Pierre Zweigenbaum"
        ],
        "abstract": "  We address here the treatment of metonymic expressions from a knowledge representation perspective, that is, in the context of a text understanding system which aims to build a conceptual representation from texts according to a domain model expressed in a knowledge representation formalism.\n",
        "submission_date": "1996-04-26T00:00:00",
        "last_modified_date": "1996-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604017",
        "title": "Fast Parsing using Pruning and Grammar Specialization",
        "authors": [
            "Manny Rayner",
            "David Carter"
        ],
        "abstract": "  We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning. These methods together give an order of magnitude increase in speed, and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work. Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application.\n    ",
        "submission_date": "1996-04-26T00:00:00",
        "last_modified_date": "1996-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604018",
        "title": "The Measure of a Model",
        "authors": [
            "Rebecca Bruce",
            "Janyce Wiebe",
            "Ted Pedersen"
        ],
        "abstract": "  This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set. These determinants are the appropriateness, for the test set, of the results of (1) feature selection, (2) formulation of the parametric form of the model, and (3) parameter estimation. These are part of any model formulation procedure, even if not broken out as separate steps, so the tradeoffs explored in this paper are relevant to a wide variety of methods. The measures are demonstrated in a large experiment, in which they are used to analyze the results of roughly 300 classifiers that perform word-sense disambiguation.\n    ",
        "submission_date": "1996-04-28T00:00:00",
        "last_modified_date": "1996-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604019",
        "title": "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "authors": [
            "Guido Minnen"
        ],
        "abstract": "  Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar. The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness. Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest.\n    ",
        "submission_date": "1996-04-29T00:00:00",
        "last_modified_date": "1996-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604020",
        "title": "Translating into Free Word Order Languages",
        "authors": [
            "Beryl Hoffman"
        ],
        "abstract": "  In this paper, I discuss machine translation of English text into Turkish, a relatively ``free'' word order language. I present algorithms that determine the topic and the focus of each target sentence (using salience (Centering Theory), old vs. new information, and contrastiveness in the discourse model) in order to generate the contextually appropriate word orders in the target language.\n    ",
        "submission_date": "1996-04-29T00:00:00",
        "last_modified_date": "1996-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604021",
        "title": "Extended Dependency Structures and their Formal Interpretation",
        "authors": [
            "Marc Dymetman",
            "Max Copperman"
        ],
        "abstract": "  We describe two ``semantically-oriented'' dependency-structure formalisms, U-forms and S-forms. U-forms have been previously used in machine translation as interlingual representations, but without being provided with a formal interpretation. S-forms, which we introduce in this paper, are a scoped version of U-forms, and we define a compositional semantics mechanism for them. Two types of semantic composition are basic: complement incorporation and modifier incorporation. Binding of variables is done at the time of incorporation, permitting much flexibility in composition order and a simple account of the semantic effects of permuting several incorporations.\n    ",
        "submission_date": "1996-04-29T00:00:00",
        "last_modified_date": "1996-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604022",
        "title": "Unsupervised Learning of Word-Category Guessing Rules",
        "authors": [
            "Andrei Mikheev"
        ],
        "abstract": "  Words unknown to the lexicon present a substantial problem to part-of-speech tagging. In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words. Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus: prefix morphological rules, suffix morphological rules and ending-guessing rules. The learning was performed on the Brown Corpus data and rule-sets, with a highly competitive performance, were produced and compared with the state-of-the-art.\n    ",
        "submission_date": "1996-04-30T00:00:00",
        "last_modified_date": "1996-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604023",
        "title": "A Model-Theoretic Framework for Theories of Syntax",
        "authors": [
            "James Rogers"
        ],
        "abstract": "  A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism---to express syntactic theories purely in terms of the properties of the class of structures they license. By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties, this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization, allowing disparate theories to be compared on the basis of those properties. We discuss $\\LKP$, a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive---supporting direct statement of most linguistically significant syntactic properties---but having well-defined strong generative capacity---languages are definable in $\\LKP$ iff they are strongly context-free. We draw examples from the realms of GPSG and GB.\n    ",
        "submission_date": "1996-04-30T00:00:00",
        "last_modified_date": "1996-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604024",
        "title": "Connectivity in Bag Generation",
        "authors": [
            "Arturo Trujillo",
            "Simon Berry"
        ],
        "abstract": "  This paper presents a pruning technique which can be used to reduce the number of paths searched in rule-based bag generators of the type proposed by \\cite{poznanskietal95} and \\cite{popowich95}. Pruning the search space in these generators is important given the computational cost of bag generation. The technique relies on a connectivity constraint between the semantic indices associated with each lexical sign in a bag. Testing the algorithm on a range of sentences shows reductions in the generation time and the number of edges constructed.\n    ",
        "submission_date": "1996-04-30T00:00:00",
        "last_modified_date": "1996-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604025",
        "title": "Learning Part-of-Speech Guessing Rules from Lexicon: Extension to Non-Concatenative Operations",
        "authors": [
            "Andrei Mikheev"
        ],
        "abstract": "  One of the problems in part-of-speech tagging of real-word texts is that of unknown to the lexicon words. In Mikheev (ACL-96 ",
        "submission_date": "1996-04-30T00:00:00",
        "last_modified_date": "1996-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9604026",
        "title": "Towards a Workbench for Acquisition of Domain Knowledge from Natural Language",
        "authors": [
            "Andrei Mikheev",
            "Steven Finch"
        ],
        "abstract": "  In this paper we describe an architecture and functionality of main components of a workbench for an acquisition of domain knowledge from large text corpora. The workbench supports an incremental process of corpus analysis starting from a rough automatic extraction and organization of lexico-semantic regularities and ending with a computer supported analysis of extracted data and a semi-automatic refinement of obtained hypotheses. For doing this the workbench employs methods from computational linguistics, information retrieval and knowledge engineering. Although the workbench is currently under implementation some of its components are already implemented and their performance is illustrated with samples from engineering for a medical domain.\n    ",
        "submission_date": "1996-04-30T00:00:00",
        "last_modified_date": "1996-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605001",
        "title": "Compiling a Partition-Based Two-Level Formalism",
        "authors": [
            "Edmund Grimley-Evans",
            "George Anton Kiraz",
            "Stephen G. Pulman"
        ],
        "abstract": "  This paper describes an algorithm for the compilation of a two (or more) level orthographic or phonological rule notation into finite state transducers. The notation is an alternative to the standard one deriving from Koskenniemi's work: it is believed to have some practical descriptive advantages, and is quite widely used, but has a different interpretation. Efficient interpreters exist for the notation, but until now it has not been clear how to compile to equivalent automata in a transparent way. The present paper shows how to do this, using some of the conceptual tools provided by Kaplan and Kay's regular relations calculus.\n    ",
        "submission_date": "1996-05-02T00:00:00",
        "last_modified_date": "1996-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605002",
        "title": "Building Natural-Language Generation Systems",
        "authors": [
            "Ehud Reiter"
        ],
        "abstract": "  This is a very short paper that briefly discusses some of the tasks that NLG systems perform. It is of no research interest, but I have occasionally found it useful as a way of introducing NLG to potential project collaborators who know nothing about the field.\n    ",
        "submission_date": "1996-05-02T00:00:00",
        "last_modified_date": "1996-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605003",
        "title": "Yet Another Paper about Partial Verb Phrase Fronting in German",
        "authors": [
            "Stefan M\u00fcller"
        ],
        "abstract": "  I describe a very simple HPSG analysis for partial verb phrase fronting. I will argue that the presented account is more adequate than others made during the past years because it allows the description of constituents in fronted positions with their modifier remaining in the non-fronted part of the sentence. A problem with ill-formed signs that are admitted by all HPSG accounts for partial verb phrase fronting known so far will be explained and a solution will be suggested that uses the difference between combinatoric relations of signs and their representation in word order domains.\n    ",
        "submission_date": "1996-05-02T00:00:00",
        "last_modified_date": "1996-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605004",
        "title": "Higher-Order Coloured Unification and Natural Language Semantics",
        "authors": [
            "Claire Gardent",
            "Michael Kohlhase"
        ],
        "abstract": "  In this paper, we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, non semantic information. In particular, it provides the general theory for the Primary Occurrence Restriction which (Dalrymple, Shieber and Pereira, 1991)'s analysis called for.\n    ",
        "submission_date": "1996-05-02T00:00:00",
        "last_modified_date": "1996-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605005",
        "title": "Focus and Higher-Order Unification",
        "authors": [
            "Claire Gardent",
            "Michael Kohlhase"
        ],
        "abstract": "  Pulman has shown that Higher--Order Unification (HOU) can be used to model the interpretation of focus. In this paper, we extend the unification--based approach to cases which are often seen as a test--bed for focus theory: utterances with multiple focus operators and second occurrence expressions. We then show that the resulting analysis favourably compares with two prominent theories of focus (namely, Rooth's Alternative Semantics and Krifka's Structured Meanings theory) in that it correctly generates interpretations which these alternative theories cannot yield. Finally, we discuss the formal properties of the approach and argue that even though HOU need not terminate, for the class of unification--problems dealt with in this paper, HOU avoids this shortcoming and is in fact computationally tractable.\n    ",
        "submission_date": "1996-05-02T00:00:00",
        "last_modified_date": "1996-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605006",
        "title": "Active Constraints for a Direct Interpretation of HPSG",
        "authors": [
            "Philippe Blache",
            "Jean-Louis Paquelin"
        ],
        "abstract": "  In this paper, we characterize the properties of a direct interpretation of HPSG and present the advantages of this approach. High-level programming languages constitute in this perspective an efficient solution: we show how a multi-paradigm approach, containing in particular constraint logic programming, offers mechanims close to that of the theory and preserves its fundamental properties. We take the example of LIFE and describe the implementation of the main HPSG mechanisms.\n    ",
        "submission_date": "1996-05-03T00:00:00",
        "last_modified_date": "1996-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605007",
        "title": "Resolving Anaphors in Embedded Sentences",
        "authors": [
            "Saliha Azzam"
        ],
        "abstract": "  We propose an algorithm to resolve anaphors, tackling mainly the problem of intrasentential antecedents. We base our methodology on the fact that such antecedents are likely to occur in embedded sentences. Sidner's focusing mechanism is used as the basic algorithm in a more complete approach. The proposed algorithm has been tested and implemented as a part of a conceptual analyser, mainly to process pronouns. Details of an evaluation are given.\n    ",
        "submission_date": "1996-05-04T00:00:00",
        "last_modified_date": "1996-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605008",
        "title": "Tactical Generation in a Free Constituent Order Language",
        "authors": [
            "Dilek Zeynep Hakkani",
            "Kemal Oflazer",
            "Ilyas Cicekli"
        ],
        "abstract": "  This paper describes tactical generation in Turkish, a free constituent order language, in which the order of the constituents may change according to the information structure of the sentences to be generated. In the absence of any information regarding the information structure of a sentence (i.e., topic, focus, background, etc.), the constituents of the sentence obey a default order, but the order is almost freely changeable, depending on the constraints of the text flow or discourse. We have used a recursively structured finite state machine for handling the changes in constituent order, implemented as a right-linear grammar backbone. Our implementation environment is the GenKit system, developed at Carnegie Mellon University--Center for Machine Translation. Morphological realization has been implemented using an external morphological analysis/generation component which performs concrete morpheme selection and handles morphographemic processes.\n    ",
        "submission_date": "1996-05-05T00:00:00",
        "last_modified_date": "1996-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605009",
        "title": "Learning similarity-based word sense disambiguation from sparse data",
        "authors": [
            "Yael Karov",
            "Shimon Edelman"
        ],
        "abstract": "  We describe a method for automatic word sense disambiguation using a text corpus and a machine-readable dictionary (MRD). The method is based on word similarity and context similarity measures. Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words. The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD. A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context. Experiments show that this method performs well, and can learn even from very sparse training data.\n    ",
        "submission_date": "1996-05-05T00:00:00",
        "last_modified_date": "1996-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605010",
        "title": "Best-First Surface Realization",
        "authors": [
            "Stephan Busemann"
        ],
        "abstract": "  Current work in surface realization concentrates on the use of general, abstract algorithms that interpret large, reversible grammars. Only little attention has been paid so far to the many small and simple applications that require coverage of a small sublanguage at different degrees of sophistication. The system TG/2 described in this paper can be smoothly integrated with deep generation processes, it integrates canned text, templates, and context-free rules into a single formalism, it allows for both textual and tabular output, and it can be parameterized according to linguistic preferences. These features are based on suitably restricted production system techniques and on a generic backtracking regime.\n    ",
        "submission_date": "1996-05-06T00:00:00",
        "last_modified_date": "1996-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605011",
        "title": "Counting Coordination Categorially",
        "authors": [
            "Crit Cremers",
            "Maarten Hijzelendoorn"
        ],
        "abstract": "  This paper presents a way of reducing the complexity of parsing free coordination. It lives on the Coordinative Count Invariant, a property of derivable sequences in occurrence-sensitive categorial grammar. This invariant can be exploited to cut down deterministically the search space for coordinated sentences to minimal fractions. The invariant is based on inequalities, which is shown to be the best one can get in the presence of coordination without proper parsing. It is implemented in a categorial parser for Dutch. Some results of applying the invariant to the parsing of coordination in this parser are presented.\n    ",
        "submission_date": "1996-05-06T00:00:00",
        "last_modified_date": "1996-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605012",
        "title": "A New Statistical Parser Based on Bigram Lexical Dependencies",
        "authors": [
            "Michael Collins"
        ],
        "abstract": "  This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95, Jelinek et al 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.\n    ",
        "submission_date": "1996-05-06T00:00:00",
        "last_modified_date": "1996-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605013",
        "title": "Learning Dependencies between Case Frame Slots",
        "authors": [
            "Hang Li",
            "Naoki Abe"
        ],
        "abstract": "  We address the problem of automatically acquiring case frame patterns (selectional patterns) from large corpus data. In particular, we propose a method of learning dependencies between case frame slots. We view the problem of learning case frame patterns as that of learning multi-dimensional discrete joint distributions, where random variables represent case slots. We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables. Since the number of parameters in a multi-dimensional joint distribution is exponential, it is infeasible to accurately estimate them in practice. To overcome this difficulty, we settle with approximating the target joint distribution by the product of low order component distributions, based on corpus data. In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task. Our experimental results indicate that for certain classes of verbs, the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies.\n    ",
        "submission_date": "1996-05-12T00:00:00",
        "last_modified_date": "1996-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605014",
        "title": "Clustering Words with the MDL Principle",
        "authors": [
            "Hang Li",
            "Naoki Abe"
        ],
        "abstract": "  We address the problem of automatically constructing a thesaurus (hierarchically clustering words) based on corpus data. We view the problem of clustering words as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs, and propose an estimation algorithm using simulated annealing with an energy function based on the Minimum Description Length (MDL) Principle. We empirically compared the performance of our method based on the MDL Principle against that of one based on the Maximum Likelihood Estimator, and found that the former outperforms the latter. We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus. Our experimental results indicate that we can improve accuracy in disambiguation by using such a thesaurus.\n    ",
        "submission_date": "1996-05-12T00:00:00",
        "last_modified_date": "1996-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605015",
        "title": "Adapting the Core Language Engine to French and Spanish",
        "authors": [
            "Manny Rayner",
            "David Carter",
            "Pierrette Bouillon"
        ],
        "abstract": "  We describe how substantial domain-independent language-processing systems for French and Spanish were quickly developed by manually adapting an existing English-language system, the SRI Core Language Engine. We explain the adaptation process in detail, and argue that it provides a fairly general recipe for converting a grammar-based system for English into a corresponding one for a Romance language.\n    ",
        "submission_date": "1996-05-10T00:00:00",
        "last_modified_date": "1996-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605016",
        "title": "Parsing for Semidirectional Lambek Grammar is NP-Complete",
        "authors": [
            "Jochen Doerre"
        ],
        "abstract": "  We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call {\\em semidirectional}. In semidirectional Lambek calculus $\\SDL$ there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent's left-hand side, thus permitting non-peripheral extraction. $\\SDL$ grammars are able to generate each context-free language and more than that. We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem.\n    ",
        "submission_date": "1996-05-10T00:00:00",
        "last_modified_date": "1996-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605017",
        "title": "A Chart Generator for Shake and Bake Machine Translation",
        "authors": [
            "Fred Popowich"
        ],
        "abstract": "  A generation algorithm based on an active chart parsing algorithm is introduced which can be used in conjunction with a Shake and Bake machine translation system. A concise Prolog implementation of the algorithm is provided, and some performance comparisons with a shift-reduce based algorithm are given which show the chart generator is much more efficient for generating all possible sentences from an input specification.\n    ",
        "submission_date": "1996-05-10T00:00:00",
        "last_modified_date": "1996-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605018",
        "title": "Efficient Tabular LR Parsing",
        "authors": [
            "Mark-Jan Nederhof",
            "Giorgio Satta"
        ],
        "abstract": "  We give a new treatment of tabular LR parsing, which is an alternative to Tomita's generalized LR algorithm. The advantage is twofold. Firstly, our treatment is conceptually more attractive because it uses simpler concepts, such as grammar transformations and standard tabulation techniques also know as chart parsing. Secondly, the static and dynamic complexity of parsing, both in space and time, is significantly reduced.\n    ",
        "submission_date": "1996-05-13T00:00:00",
        "last_modified_date": "1996-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605019",
        "title": "Noun-Phrase Analysis in Unrestricted Text for Information Retrieval",
        "authors": [
            "David A. Evans",
            "Chengxiang Zhai"
        ],
        "abstract": "  Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text. This paper reports on the application of a few simple, yet robust and efficient noun-phrase analysis techniques to create better indexing phrases for information retrieval. In particular, we describe a hybrid approach to the extraction of meaningful (continuous or discontinuous) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics. Results of experiments show that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system. The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction.\n    ",
        "submission_date": "1996-05-13T00:00:00",
        "last_modified_date": "1996-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605020",
        "title": "Where Defaults Don't Help: the Case of the German Plural System",
        "authors": [
            "Ramin Charles Nakisa",
            "Ulrike Hahn"
        ],
        "abstract": "  The German plural system has become a focal point for conflicting theories of language, both linguistic and cognitive. We present simulation results with three simple classifiers - an ordinary nearest neighbour algorithm, Nosofsky's `Generalized Context Model' (GCM) and a standard, three-layer backprop network - predicting the plural class from a phonological representation of the singular in German. Though these are absolutely `minimal' models, in terms of architecture and input information, they nevertheless do remarkably well. The nearest neighbour predicts the correct plural class with an accuracy of 72% for a set of 24,640 nouns from the CELEX database. With a subset of 8,598 (non-compound) nouns, the nearest neighbour, the GCM and the network score 71.0%, 75.0% and 83.5%, respectively, on novel items. Furthermore, they outperform a hybrid, `pattern-associator + default rule', model, as proposed by Marcus et al. (1995), on this data set.\n    ",
        "submission_date": "1996-05-13T00:00:00",
        "last_modified_date": "1996-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605021",
        "title": "Functional Centering",
        "authors": [
            "Michael Strube",
            "Udo Hahn"
        ],
        "abstract": "  Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering.\n    ",
        "submission_date": "1996-05-14T00:00:00",
        "last_modified_date": "1996-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605022",
        "title": "Processing Complex Sentences in the Centering Framework",
        "authors": [
            "Michael Strube"
        ],
        "abstract": "  We extend the centering model for the resolution of intra-sentential anaphora and specify how to handle complex sentences. An empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence.\n    ",
        "submission_date": "1996-05-14T00:00:00",
        "last_modified_date": "1996-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605023",
        "title": "A Simple Transformation for Offline-Parsable Grammars and its Termination Properties",
        "authors": [
            "Marc Dymetman"
        ],
        "abstract": "  We present, in easily reproducible terms, a simple transformation for offline-parsable grammars which results in a provably terminating parsing program directly top-down interpretable in Prolog. The transformation consists in two steps: (1) removal of empty-productions, followed by: (2) left-recursion elimination. It is related both to left-corner parsing (where the grammar is compiled, rather than interpreted through a parsing program, and with the advantage of guaranteed termination in the presence of empty productions) and to the Generalized Greibach Normal Form for DCGs (with the advantage of implementation simplicity).\n    ",
        "submission_date": "1996-05-14T00:00:00",
        "last_modified_date": "1996-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605024",
        "title": "Using Terminological Knowledge Representation Languages to Manage Linguistic Resources",
        "authors": [
            "Pamela W. Jordan"
        ],
        "abstract": "  I examine how terminological languages can be used to manage linguistic data during NL research and development. In particular, I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions, identify the need for new verb classes, and identify appropriate linguistic hypotheses for a new verb's behavior.\n    ",
        "submission_date": "1996-05-14T00:00:00",
        "last_modified_date": "1996-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605025",
        "title": "A Conceptual Reasoning Approach to Textual Ellipsis",
        "authors": [
            "Udo Hahn",
            "Katja Markert",
            "Michael Strube"
        ],
        "abstract": "  We present a hybrid text understanding methodology for the resolution of textual ellipsis. It integrates conceptual criteria (based on the well-formedness and conceptual strength of role chains in a terminological knowledge base) and functional constraints reflecting the utterances' information structure (based on the distinction between context-bound and unbound discourse elements). The methodological framework for text ellipsis resolution is the centering model that has been adapted to these constraints.\n    ",
        "submission_date": "1996-05-15T00:00:00",
        "last_modified_date": "1996-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605026",
        "title": "Trading off Completeness for Efficiency --- The \\textsc{ParseTalk} Performance Grammar Approach to Real-World Text Parsing",
        "authors": [
            "Peter Neuhaus",
            "Udo Hahn"
        ],
        "abstract": "  We argue for a performance-based design of natural language grammars and their associated parsers in order to meet the constraints posed by real-world natural language understanding. This approach incorporates declarative and procedural knowledge about language and language use within an object-oriented specification framework. We discuss several message passing protocols for real-world text parsing and provide reasons for sacrificing completeness of the parse in favor of efficiency.\n    ",
        "submission_date": "1996-05-15T00:00:00",
        "last_modified_date": "1996-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605027",
        "title": "Restricted Parallelism in Object-Oriented Lexical Parsing",
        "authors": [
            "Peter Neuhaus",
            "Udo Hahn"
        ],
        "abstract": "  We present an approach to parallel natural language parsing which is based on a concurrent, object-oriented model of computation. A depth-first, yet incomplete parsing algorithm for a dependency grammar is specified and several restrictions on the degree of its parallelization are discussed.\n    ",
        "submission_date": "1996-05-15T00:00:00",
        "last_modified_date": "1996-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605028",
        "title": "Towards Understanding Spontaneous Speech: Word Accuracy vs. Concept Accuracy",
        "authors": [
            "M. Boros",
            "W. Eckert",
            "F. Gallwitz",
            "G. Goerz",
            "G. Hanrieder",
            "H. Niemann"
        ],
        "abstract": "  In this paper we describe an approach to automatic evaluation of both the speech recognition and understanding capabilities of a spoken dialogue system for train time table information. We use word accuracy for recognition and concept accuracy for understanding performance judgement. Both measures are calculated by comparing these modules' output with a correct reference answer. We report evaluation results for a spontaneous speech corpus with about 10000 utterances. We observed a nearly linear relationship between word accuracy and concept accuracy.\n    ",
        "submission_date": "1996-05-15T00:00:00",
        "last_modified_date": "1996-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605029",
        "title": "Learning Word Association Norms Using Tree Cut Pair Models",
        "authors": [
            "Naoki Abe",
            "Hang Li"
        ],
        "abstract": "  We consider the problem of learning co-occurrence information between two word categories, or more in general between two discrete random variables taking values in a hierarchically classified domain. In particular, we consider the problem of learning the `association norm' defined by A(x,y)=p(x, y)/(p(x)*p(y)), where p(x, y) is the joint distribution for x and y and p(x) and p(y) are marginal distributions induced by p(x, y). We formulate this problem as a sub-task of learning the conditional distribution p(x|y), by exploiting the identity p(x|y) = A(x,y)*p(x). We propose a two-step estimation method based on the MDL principle, which works as follows: It first estimates p(x) as p1 using MDL, and then estimates p(x|y) for a fixed y by applying MDL on the hypothesis class of {A * p1 | A \\in B} for some given class B of representations for association norm. The estimation of A is therefore obtained as a side-effect of a near optimal estimation of p(x|y). We then apply this general framework to the problem of acquiring case-frame patterns. We assume that both p(x) and A(x, y) for given y are representable by a model based on a classification that exists within an existing thesaurus tree as a `cut,' and hence p(x|y) is represented as the product of a pair of `tree cut models.' We then devise an efficient algorithm that implements our general strategy. We tested our method by using it to actually acquire case-frame patterns and conducted disambiguation experiments using the acquired knowledge. The experimental results show that our method improves upon existing methods.\n    ",
        "submission_date": "1996-05-16T00:00:00",
        "last_modified_date": "1996-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605030",
        "title": "Incremental Centering and Center Ambiguity",
        "authors": [
            "Udo Hahn",
            "Michael Strube"
        ],
        "abstract": "  In this paper, we present a model of anaphor resolution within the framework of the centering model. The consideration of an incremental processing mode introduces the need to manage structural ambiguity at the center level. Hence, the centering framework is further refined to account for local and global parsing ambiguities which propagate up to the level of center representations, yielding moderately adapted data structures for the centering algorithm.\n    ",
        "submission_date": "1996-05-16T00:00:00",
        "last_modified_date": "1996-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605031",
        "title": "Efficient Algorithms for Parsing the DOP Model? A Reply to Joshua Goodman",
        "authors": [
            "Rens Bod"
        ],
        "abstract": "  This note is a reply to Joshua Goodman's paper \"Efficient Algorithms for Parsing the DOP Model\" (Goodman, 1996; ",
        "submission_date": "1996-05-24T00:00:00",
        "last_modified_date": "1996-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605032",
        "title": "Synchronous Models of Language",
        "authors": [
            "Owen Rambow",
            "Giorgio Satta"
        ],
        "abstract": "  In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.\n    ",
        "submission_date": "1996-05-27T00:00:00",
        "last_modified_date": "1996-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605033",
        "title": "Notes on LR Parser Design",
        "authors": [
            "Christer Samuelsson"
        ],
        "abstract": "  The design of an LR parser based on interleaving the atomic symbol processing of a context-free backbone grammar with the full constraints of the underlying unification grammar is described. The parser employs a set of reduced constraints derived from the unification grammar in the LR parsing step. Gap threading is simulated to reduce the applicability of empty productions.\n    ",
        "submission_date": "1996-05-29T00:00:00",
        "last_modified_date": "1996-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605034",
        "title": "Handling Sparse Data by Successive Abstraction",
        "authors": [
            "Christer Samuelsson"
        ],
        "abstract": "  A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles. It has been tested on a part-of-speech tagging task and outperformed (deleted) interpolation with context-independent weights, even when the latter used a globally optimal parameter setting determined a posteriori.\n    ",
        "submission_date": "1996-05-29T00:00:00",
        "last_modified_date": "1996-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605035",
        "title": "Example-Based Optimization of Surface-Generation Tables",
        "authors": [
            "Christer Samuelsson"
        ],
        "abstract": "  A method is given that \"inverts\" a logic grammar and displays it from the point of view of the logical form, rather than from that of the word string. LR-compiling techniques are used to allow a recursive-descent generation algorithm to perform \"functor merging\" much in the same way as an LR parser performs prefix merging. This is an improvement on the semantic-head-driven generator that results in a much smaller search space. The amount of semantic lookahead can be varied, and appropriate tradeoff points between table size and resulting nondeterminism can be found automatically. This can be done by removing all spurious nondeterminism for input sufficiently close to the examples of a training corpus, and large portions of it for other input, while preserving completeness.\n    ",
        "submission_date": "1996-05-29T00:00:00",
        "last_modified_date": "1996-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605036",
        "title": "Parsing Algorithms and Metrics",
        "authors": [
            "Joshua Goodman"
        ],
        "abstract": "  Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree. By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved. We present two new algorithms: the ``Labelled Recall Algorithm,'' which maximizes the expected Labelled Recall Rate, and the ``Bracketed Recall Algorithm,'' which maximizes the Bracketed Recall Rate. Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize.\n    ",
        "submission_date": "1996-05-30T00:00:00",
        "last_modified_date": "1996-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605037",
        "title": "Combining Trigram-based and Feature-based Methods for Context-Sensitive Spelling Correction",
        "authors": [
            "Andrew R. Golding",
            "Yves Schabes"
        ],
        "abstract": "  This paper addresses the problem of correcting spelling errors that result in valid, though unintended words (such as ``peace'' and ``piece'', or ``quiet'' and ``quite'') and also the problem of correcting particular word usage errors (such as ``amount'' and ``number'', or ``among'' and ``between''). Such corrections require contextual information and are not handled by conventional spelling programs such as Unix `spell'. First, we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context. This method uses a small number of parameters compared to previous methods based on word trigrams. However, it is effectively unable to distinguish among words that have the same part of speech. For this case, an alternative feature-based method called Bayes performs better; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints. A hybrid method called Tribayes is then introduced that combines the best of the previous two methods. The improvement in performance of Tribayes over its components is verified experimentally. Tribayes is also compared with the grammar checker in Microsoft Word, and is found to have substantially higher performance.\n    ",
        "submission_date": "1996-05-31T00:00:00",
        "last_modified_date": "1996-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9605038",
        "title": "Efficient Normal-Form Parsing for Combinatory Categorial Grammar",
        "authors": [
            "Jason Eisner"
        ],
        "abstract": "  Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses. Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input. This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique. The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated.\n    ",
        "submission_date": "1996-06-02T00:00:00",
        "last_modified_date": "1996-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606001",
        "title": "A Bayesian hybrid method for context-sensitive spelling correction",
        "authors": [
            "Andrew R. Golding"
        ],
        "abstract": "  Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical ``atmosphere'' (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.\n    ",
        "submission_date": "1996-06-03T00:00:00",
        "last_modified_date": "1996-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606002",
        "title": "Clustered Language Models with Context-Equivalent States",
        "authors": [
            "J.P. Ueberla",
            "I.R. Gransden"
        ],
        "abstract": "  In this paper, a hierarchical context definition is added to an existing clustering algorithm in order to increase its robustness. The resulting algorithm, which clusters contexts and events separately, is used to experiment with different ways of defining the context a language model takes into account. The contexts range from standard bigram and trigram contexts to part of speech five-grams. Although none of the models can compete directly with a backoff trigram, they give up to 9\\% improvement in perplexity when interpolated with a trigram. Moreover, the modified version of the algorithm leads to a performance increase over the original version of up to 12\\%.\n    ",
        "submission_date": "1996-06-04T00:00:00",
        "last_modified_date": "1996-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606003",
        "title": "Morphological Cues for Lexical Semantics",
        "authors": [
            "Marc Light"
        ],
        "abstract": "  Most natural language processing tasks require lexical semantic information. Automated acquisition of this information would thus increase the robustness and portability of NLP systems. This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information. One advantage of this method, and of other methods that rely only on surface characteristics of language, is that the necessary input is currently available.\n    ",
        "submission_date": "1996-06-04T00:00:00",
        "last_modified_date": "1996-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606004",
        "title": "Classification in Feature-based Default Inheritance Hierarchies",
        "authors": [
            "Marc Light"
        ],
        "abstract": "  Increasingly, inheritance hierarchies are being used to reduce redundancy in natural language processing lexicons. Systems that utilize inheritance hierarchies need to be able to insert words under the optimal set of classes in these hierarchies. In this paper, we formalize this problem for feature-based default inheritance hierarchies. Since the problem turns out to be NP-complete, we present an approximation algorithm for it. We show that this algorithm is efficient and that it performs well with respect to a number of standard problems for default inheritance. A prototype implementation has been tested on lexical hierarchies and it has produced encouraging results. The work presented here is also relevant to other types of default hierarchies.\n    ",
        "submission_date": "1996-06-04T00:00:00",
        "last_modified_date": "1996-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606005",
        "title": "Part-of-Speech-Tagging using morphological information",
        "authors": [
            "Bernd Ludwig"
        ],
        "abstract": "  This paper presents the results of an experiment to decide the question of authenticity of the supposedly spurious Rhesus - a attic tragedy sometimes credited to Euripides. The experiment involves use of statistics in order to test whether significant deviations in the distribution of word categories between Rhesus and the other works of Euripides can or cannot be found. To count frequencies of word categories in the corpus, a part-of-speech-tagger for Greek has been implemented. Some special techniques for reducing the problem of sparse data are used resulting in an accuracy of ca. 96.6%.\n    ",
        "submission_date": "1996-06-04T00:00:00",
        "last_modified_date": "1996-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606006",
        "title": "Coordination in Tree Adjoining Grammars: Formalization and Implementation",
        "authors": [
            "Anoop Sarkar",
            "Aravind Joshi"
        ],
        "abstract": "  In this paper we show that an account for coordination can be constructed using the derivation structures in a lexicalized Tree Adjoining Grammar (LTAG). We present a notion of derivation in LTAGs that preserves the notion of fixed constituency in the LTAG lexicon while providing the flexibility needed for coordination phenomena. We also discuss the construction of a practical parser for LTAGs that can handle coordination including cases of non-constituent coordination.\n    ",
        "submission_date": "1996-06-07T00:00:00",
        "last_modified_date": "1996-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606007",
        "title": "Word Sense Disambiguation using Conceptual Density",
        "authors": [
            "Eneko Agirre",
            "German Rigau"
        ],
        "abstract": "  This paper presents a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus.\n    ",
        "submission_date": "1996-06-07T00:00:00",
        "last_modified_date": "1996-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606008",
        "title": "Coordination as a Direct Process",
        "authors": [
            "Augusta Mela",
            "Christophe Fouquere"
        ],
        "abstract": "  We propose a treatment of coordination based on the concepts of functor, argument and subcategorization. Its formalization comprises two parts which are conceptually independent. On one hand, we have extended the feature structure unification to disjunctive and set values in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements. On the other hand, we have considered the conjunction {\\em et (and)} as the head of the coordinate structure, so that coordinate structures stem simply from the subcategorization specifications of {\\em et} and the general schemata of a head saturation. Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended.\n    ",
        "submission_date": "1996-06-08T00:00:00",
        "last_modified_date": "1996-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606009",
        "title": "Modularizing Contexted Constraints",
        "authors": [
            "John Griffith"
        ],
        "abstract": "  This paper describes a method for compiling a constraint-based grammar into a potentially more efficient form for processing. This method takes dependent disjunctions within a constraint formula and factors them into non-interacting groups whenever possible by determining their independence. When a group of dependent disjunctions is split into smaller groups, an exponential amount of redundant information is reduced. At runtime, this means that an exponential amount of processing can be saved as well. Since the performance of an algorithm for processing constraints with dependent disjunctions is highly determined by its input, the transformation presented in this paper should prove beneficial for all such algorithms.\n    ",
        "submission_date": "1996-06-08T00:00:00",
        "last_modified_date": "1996-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606010",
        "title": "An Information Structural Approach to Spoken Language Generation",
        "authors": [
            "Scott Prevost"
        ],
        "abstract": "  This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation. A two-tiered information structure representation is used in the high-level content planning and sentence planning stages of generation to produce efficient, coherent speech that makes certain discourse relationships, such as explicit contrasts, appropriately salient. The system is able to produce appropriate intonational patterns that cannot be generated by other systems which rely solely on word class and given/new distinctions.\n    ",
        "submission_date": "1996-06-10T00:00:00",
        "last_modified_date": "1996-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606011",
        "title": "An Empirical Study of Smoothing Techniques for Language Modeling",
        "authors": [
            "Stanley F. Chen",
            "Joshua T. Goodman"
        ],
        "abstract": "  We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.\n    ",
        "submission_date": "1996-06-11T00:00:00",
        "last_modified_date": "1996-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606012",
        "title": "An Efficient Inductive Unsupervised Semantic Tagger",
        "authors": [
            "K T Lua"
        ],
        "abstract": "  We report our development of a simple but fast and efficient inductive unsupervised semantic tagger for Chinese words. A POS hand-tagged corpus of 348,000 words is used. The corpus is being tagged in two steps. First, possible semantic tags are selected from a semantic dictionary(Tong Yi Ci Ci Lin), the POS and the conditional probability of semantic from POS, i.e., P(S|P). The final semantic tag is then assigned by considering the semantic tags before and after the current word and the semantic-word conditional probability P(S|W) derived from the first step. Semantic bigram probabilities P(S|S) are used in the second step. Final manual checking shows that this simple but efficient algorithm has a hit rate of 91%. The tagger tags 142 words per second, using a 120 MHz Pentium running FOXPRO. It runs about 2.3 times faster than a Viterbi tagger.\n    ",
        "submission_date": "1996-06-11T00:00:00",
        "last_modified_date": "1996-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606013",
        "title": "Relating Turing's Formula and Zipf's Law",
        "authors": [
            "Christer Samuelsson"
        ],
        "abstract": "  An asymptote is derived from Turing's local reestimation formula for population frequencies, and a local reestimation formula is derived from Zipf's law for the asymptotic behavior of population frequencies. The two are shown to be qualitatively different asymptotically, but nevertheless to be instances of a common class of reestimation-formula-asymptote pairs, in which they constitute the upper and lower bounds of the convergence region of the cumulative of the frequency function, as rank tends to infinity. The results demonstrate that Turing's formula is qualitatively different from the various extensions to Zipf's law, and suggest that it smooths the frequency estimates towards a geometric distribution.\n    ",
        "submission_date": "1996-06-11T00:00:00",
        "last_modified_date": "1996-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606014",
        "title": "Building Probabilistic Models for Natural Language",
        "authors": [
            "Stanley F. Chen"
        ],
        "abstract": "  In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.\n    ",
        "submission_date": "1996-06-11T00:00:00",
        "last_modified_date": "1996-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606015",
        "title": "Stabilizing the Richardson Algorithm by Controlling Chaos",
        "authors": [
            "Song He"
        ],
        "abstract": "  By viewing the operations of the Richardson purification algorithm as a discrete time dynamical process, we propose a method to overcome the instability of the algorithm by controlling chaos. We present theoretical analysis and numerical results on the behavior and performance of the stabilized algorithm.\n    ",
        "submission_date": "1996-06-11T00:00:00",
        "last_modified_date": "1996-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606016",
        "title": "A Probabilistic Disambiguation Method Based on Psycholinguistic Principles",
        "authors": [
            "Hang Li"
        ],
        "abstract": "  We address the problem of structural disambiguation in syntactic parsing. In psycholinguistics, a number of principles of disambiguation have been proposed, notably the Lexical Preference Rule (LPR), the Right Association Principle (RAP), and the Attach Low and Parallel Principle (ALPP) (an extension of RAP). We argue that in order to improve disambiguation results it is necessary to implement these principles on the basis of a probabilistic methodology. We define a `three-word probability' for implementing LPR, and a `length probability' for implementing RAP and ALPP. Furthermore, we adopt the `back-off' method to combine these two types of probabilities. Our experimental results indicate our method to be effective, attaining an accuracy of 89.2%.\n    ",
        "submission_date": "1996-06-13T00:00:00",
        "last_modified_date": "1996-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606017",
        "title": "With raised eyebrows or the eyebrows raised ? A Neural Network Approach to Grammar Checking for Definiteness",
        "authors": [
            "Gabriele Scheler"
        ],
        "abstract": "  In this paper, we use a feature model of the semantics of plural determiners to present an approach to grammar checking for definiteness. Using neural network techniques, a semantics -- morphological category mapping was learned. We then applied a textual encoding technique to the 125 occurences of the relevant category in a 10 000 word narrative text and learned a surface -- semantics mapping. By applying the learned generation function to the newly generated representations, we achieved a correct category assignment in many cases (87 %). These results are considerably better than a direct surface categorization approach (54 %), with a baseline (always guessing the dominant category) of 60 %. It is discussed, how these results could be used in multilingual NLP applications.\n    ",
        "submission_date": "1996-06-14T00:00:00",
        "last_modified_date": "1996-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606018",
        "title": "Compilation of Weighted Finite-State Transducers from Decision Trees",
        "authors": [
            "Richard Sproat",
            "Michael Riley"
        ],
        "abstract": "  We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996).\n    ",
        "submission_date": "1996-06-14T00:00:00",
        "last_modified_date": "1996-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606019",
        "title": "Computational Complexity of Probabilistic Disambiguation by means of Tree-Grammars",
        "authors": [
            "Khalil Sima'an"
        ],
        "abstract": "  This paper studies the computational complexity of disambiguation under probabilistic tree-grammars and context-free grammars. It presents a proof that the following problems are NP-hard: computing the Most Probable Parse (MPP) from a sentence or from a word-graph, and computing the Most Probable Sentence (MPS) from a word-graph. The NP-hardness of computing the MPS from a word-graph also holds for Stochastic Context-Free Grammars. Consequently, the existence of deterministic polynomial-time algorithms for solving these disambiguation problems is a highly improbable event.\n    ",
        "submission_date": "1996-06-17T00:00:00",
        "last_modified_date": "1996-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606020",
        "title": "Computing Optimal Descriptions for Optimality Theory Grammars with Context-Free Position Structures",
        "authors": [
            "Bruce Tesar"
        ],
        "abstract": "  This paper describes an algorithm for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures. This algorithm extends Tesar's dynamic programming approach [Tesar 1994][Tesar 1995] to computing optimal structural descriptions from regular to context-free structures. The generalization to context-free structures creates several complications, all of which are overcome without compromising the core dynamic programming approach. The resulting algorithm has a time complexity cubic in the length of the input, and is applicable to grammars with universal constraints that exhibit context-free locality.\n    ",
        "submission_date": "1996-06-17T00:00:00",
        "last_modified_date": "1996-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606021",
        "title": "An Iterative Algorithm to Build Chinese Language Models",
        "authors": [
            "Xiaoqiang Luo",
            "Salim Roukos"
        ],
        "abstract": "  We present an iterative procedure to build a Chinese language model (LM). We segment Chinese text into words based on a word-based Chinese language model. However, the construction of a Chinese LM itself requires word boundaries. To get out of the chicken-and-egg problem, we propose an iterative procedure that alternates two operations: segmenting text into words and building an LM. Starting with an initial segmented corpus and an LM based upon it, we use a Viterbi-liek algorithm to segment another set of data. Then, we build an LM based on the second set and use the resulting LM to segment again the first corpus. The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors. Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation, but discovers unseen words surprisingly well. The resulting word-based LM has a perplexity of 188 for a general Chinese corpus.\n    ",
        "submission_date": "1996-06-17T00:00:00",
        "last_modified_date": "1996-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606022",
        "title": "Two Questions about Data-Oriented Parsing",
        "authors": [
            "Rens Bod"
        ],
        "abstract": "  In this paper I present ongoing work on the data-oriented parsing (DOP) model. In previous work, DOP was tested on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank, achieving excellent test results. This left, however, two important questions unanswered: (1) how does DOP perform if tested on unedited data, and (2) how can DOP be used for parsing word strings that contain unknown words? This paper addresses these questions. We show that parse results on unedited data are worse than on cleaned-up data, although very competitive if compared to other models. As to the parsing of word strings, we show that the hardness of the problem does not so much depend on unknown words, but on previously unseen lexical categories of known words. We give a novel method for parsing these words by estimating the probabilities of unknown subtrees. The method is of general interest since it shows that good performance can be obtained without the use of a part-of-speech tagger. To the best of our knowledge, our method outperforms other statistical parsers tested on Penn Treebank word strings.\n    ",
        "submission_date": "1996-06-17T00:00:00",
        "last_modified_date": "1996-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606023",
        "title": "A Robust System for Natural Spoken Dialogue",
        "authors": [
            "James F. Allen",
            "Bradford W. Miller",
            "Eric K. Ringger",
            "Teresa Sikorski"
        ],
        "abstract": "  This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains. It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors. Robustness is achieved by a combination of statistical error post-correction, syntactically- and semantically-driven robust parsing, and extensive use of the dialogue context. We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training.\n    ",
        "submission_date": "1996-06-18T00:00:00",
        "last_modified_date": "1996-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606024",
        "title": "A Data-Oriented Approach to Semantic Interpretation",
        "authors": [
            "Rens Bod",
            "Remko Bonnema",
            "Remko Scha"
        ],
        "abstract": "  In Data-Oriented Parsing (DOP), an annotated language corpus is used as a stochastic grammar. The most probable analysis of a new input sentence is constructed by combining sub-analyses from the corpus in the most probable way. This approach has been succesfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Treebank. If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence. The present paper explains this semantic interpretation method, and summarizes the results of a preliminary experiment. Semantic annotations were added to the syntactic annotations of most of the sentences of the ATIS corpus. A data-oriented semantic interpretation algorithm was succesfully tested on this semantically enriched corpus.\n    ",
        "submission_date": "1996-06-18T00:00:00",
        "last_modified_date": "1996-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606025",
        "title": "Two Sources of Control over the Generation of Software Instructions",
        "authors": [
            "Anthony Hartley",
            "Cecile Paris"
        ],
        "abstract": "  This paper presents an analysis conducted on a corpus of software instructions in French in order to establish whether task structure elements (the procedural representation of the users' tasks) are alone sufficient to control the grammatical resources of a text generator. We show that the construct of genre provides a useful additional source of control enabling us to resolve undetermined cases.\n    ",
        "submission_date": "1996-06-19T00:00:00",
        "last_modified_date": "1996-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606026",
        "title": "An Efficient Compiler for Weighted Rewrite Rules",
        "authors": [
            "Mehryar Mohri",
            "Richard Sproat"
        ],
        "abstract": "  Context-dependent rewrite rules are used in many areas of natural language and speech processing. Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs). We describe a new algorithm for compiling rewrite rules into FSTs. We show the algorithm to be simpler and more efficient than existing algorithms. Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights. We have extended the algorithm to allow for this.\n    ",
        "submission_date": "1996-06-20T00:00:00",
        "last_modified_date": "1996-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606027",
        "title": "Linguistic Structure as Composition and Perturbation",
        "authors": [
            "Carl de Marcken"
        ],
        "abstract": "  This paper discusses the problem of learning language from unprocessed text and speech signals, concentrating on the problem of learning a lexicon. In particular, it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters. The power of this representation is demonstrated by several examples in text segmentation and compression, acquisition of a lexicon from raw speech, and the acquisition of mappings between text and artificial representations of meaning.\n    ",
        "submission_date": "1996-06-21T00:00:00",
        "last_modified_date": "1996-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606028",
        "title": "Maximizing Top-down Constraints for Unification-based Systems",
        "authors": [
            "Noriko Tomuro"
        ],
        "abstract": "  A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems. However, due to the nontermination of parsing with left-recursive grammars, top-down constraints must be weakened. In this paper, a general method of maximizing top-down constraints is proposed. The method provides a procedure to dynamically compute *restrictor*, a minimum set of features involved in an infinite loop for every propagation path; thus top-down constraints are maximally propagated.\n    ",
        "submission_date": "1996-06-20T00:00:00",
        "last_modified_date": "1996-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606029",
        "title": "Directed Replacement",
        "authors": [
            "Lauri Karttunen"
        ],
        "abstract": "  This paper introduces to the finite-state calculus a family of directed replace operators. In contrast to the simple replace expression, UPPER -> LOWER, defined in Karttunen (ACL-95), the new directed version, UPPER @-> LOWER, yields an unambiguous transducer if the lower language consists of a single string. It transduces the input string from left to right, making only the longest possible replacement at each point.\n",
        "submission_date": "1996-06-23T00:00:00",
        "last_modified_date": "1996-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606030",
        "title": "Minimizing Manual Annotation Cost In Supervised Training From Corpora",
        "authors": [
            "Sean P. Engelson",
            "Ido Dagan"
        ],
        "abstract": "  Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora. This paper investigates methods for reducing annotation cost by {\\it sample selection}. In this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage. This avoids redundantly annotating examples that contribute little new information. This paper extends our previous work on {\\it committee-based sample selection} for probabilistic classifiers. We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-of-speech tagging. We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger.\n    ",
        "submission_date": "1996-06-24T00:00:00",
        "last_modified_date": "1996-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606031",
        "title": "Research on Architectures for Integrated Speech/Language Systems in Verbmobil",
        "authors": [
            "G\u00fcnther G\u00f6rz",
            "Marcus Kesseler",
            "J\u00f6rg Spilker",
            "Hans Weber"
        ],
        "abstract": "  The German joint research project Verbmobil (VM) aims at the development of a speech to speech translation system. This paper reports on research done in our group which belongs to Verbmobil's subproject on system architectures (TP15). Our specific research areas are the construction of parsers for spontaneous speech, investigations in the parallelization of parsing and to contribute to the development of a flexible communication architecture with distributed control.\n    ",
        "submission_date": "1996-06-25T00:00:00",
        "last_modified_date": "1996-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9606032",
        "title": "Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach",
        "authors": [
            "Hwee Tou Ng",
            "Hian Beng Lee"
        ],
        "abstract": "  In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named {\\sc Lexas}, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. {\\sc Lexas} achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of {\\sc WordNet}.\n    ",
        "submission_date": "1996-06-29T00:00:00",
        "last_modified_date": "1996-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607001",
        "title": "GramCheck: A Grammar and Style Checker",
        "authors": [
            "Flora Ram\u00edrez Bustamante",
            "Fernando S\u00e1nchez Le\u00f3n"
        ],
        "abstract": "  This paper presents a grammar and style checker demonstrator for Spanish and Greek native writers developed within the project GramCheck. Besides a brief grammar error typology for Spanish, a linguistically motivated approach to detection and diagnosis is presented, based on the generalized use of PROLOG extensions to highly typed unification-based grammars. The demonstrator, currently including full coverage for agreement errors and certain head-argument relation issues, also provides correction by means of an analysis-transfer-synthesis cycle. Finally, future extensions to the current system are discussed.\n    ",
        "submission_date": "1996-07-01T00:00:00",
        "last_modified_date": "1996-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607002",
        "title": "Inducing Constraint Grammars",
        "authors": [
            "Christer Samuelsson",
            "Pasi Tapanainen",
            "Atro Voutilainen"
        ],
        "abstract": "  Constraint Grammar rules are induced from corpora. A simple scheme based on local information, i.e., on lexical biases and next-neighbour contexts, extended through the use of barriers, reached 87.3 percent precision (1.12 tags/word) at 98.2 percent recall. The results compare favourably with other methods that are used for similar tasks although they are by no means as good as the results achieved using the original hand-written rules developed over several years time.\n    ",
        "submission_date": "1996-07-01T00:00:00",
        "last_modified_date": "1996-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607003",
        "title": "Domain and Language Independent Feature Extraction for Statistical Text Categorization",
        "authors": [
            "Thomas Bayer",
            "Ingrid Renz",
            "Michael Stein",
            "Ulrich Kressel"
        ],
        "abstract": "  A generic system for text categorization is presented which uses a representative text corpus to adapt the processing steps: feature extraction, dimension reduction, and classification. Feature extraction automatically learns features from the corpus by reducing actual word forms using statistical information of the corpus and general linguistic knowledge. The dimension of feature vector is then reduced by linear transformation keeping the essential information. The classification principle is a minimum least square approach based on polynomials. The described system can be readily adapted to new domains or new languages. In application, the system is reliable, fast, and processes completely automatically. It is shown that the text categorizer works successfully both on text generated by document image analysis - DIA and on ground truth data.\n    ",
        "submission_date": "1996-07-02T00:00:00",
        "last_modified_date": "1996-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607004",
        "title": "Integrating Syntactic and Prosodic Information for the Efficient Detection of Empty Categories",
        "authors": [
            "Anton Batliner",
            "Anke Feldhaus",
            "Stefan Geissler",
            "Andreas Kiessling",
            "Tibor Kiss",
            "Ralf Kompe",
            "Elmar Noeth"
        ],
        "abstract": "  We describe a number of experiments that demonstrate the usefulness of prosodic information for a processing module which parses spoken utterances with a feature-based grammar employing empty categories. We show that by requiring certain prosodic properties from those positions in the input where the presence of an empty category has to be hypothesized, a derivation can be accomplished more efficiently. The approach has been implemented in the machine translation project VERBMOBIL and results in a significant reduction of the work-load for the parser.\n    ",
        "submission_date": "1996-07-02T00:00:00",
        "last_modified_date": "1996-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607005",
        "title": "Head Automata and Bilingual Tiling: Translation with Minimal Representations",
        "authors": [
            "Hiyan Alshawi"
        ],
        "abstract": "  We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases. The model is suitable for incremental application of lexical associations in a dynamic programming search for optimal dependency tree derivations. We also present a model and algorithm for machine translation involving optimal ``tiling'' of a dependency tree with entries of a costed bilingual lexicon. Experimental results are reported comparing methods for assigning cost functions to these models. We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation.\n    ",
        "submission_date": "1996-07-03T00:00:00",
        "last_modified_date": "1996-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607006",
        "title": "Head Automata for Speech Translation",
        "authors": [
            "Hiyan Alshawi"
        ],
        "abstract": "  This paper presents statistical language and translation models based on collections of small finite state machines we call ``head automata''. The models are intended to capture the lexical sensitivity of N-gram models and direct statistical translation models, while at the same time taking account of the hierarchical phrasal structure of language. Two types of head automata are defined: relational head automata suitable for translation by transfer of dependency trees, and head transducers suitable for direct recursive lexical translation.\n    ",
        "submission_date": "1996-07-04T00:00:00",
        "last_modified_date": "1996-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607007",
        "title": "Parallel Replacement in Finite State Calculus",
        "authors": [
            "Andre Kempe",
            "Lauri Karttunen"
        ],
        "abstract": "  This paper extends the calculus of regular expressions with new types of replacement expressions that enhance the expressiveness of the simple replace operator defined in Karttunen (1995). Parallel replacement allows multiple replacements to apply simultaneously to the same input without interfering with each other. We also allow a replacement to be constrained by any number of alternative contexts. With these enhancements, the general replacement expressions are more versatile than two-level rules for the description of complex morphological alternations.\n    ",
        "submission_date": "1996-07-05T00:00:00",
        "last_modified_date": "1996-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607008",
        "title": "From Submit to Submitted via Submission: On Lexical Rules in Large-Scale Lexicon Acquisition",
        "authors": [
            "Evelyne Viegas",
            "Boyan Onyshkevych",
            "Victor Raskin",
            "Sergei Nirenburg"
        ],
        "abstract": "  This paper deals with the discovery, representation, and use of lexical rules (LRs) during large-scale semi-automatic computational lexicon acquisition. The analysis is based on a set of LRs implemented and tested on the basis of Spanish and English business- and finance-related corpora. We show that, though the use of LRs is justified, they do not come cost-free. Semi-automatic output checking is required, even with blocking and preemtion procedures built in. Nevertheless, large-scope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition. We also argue that the place of LRs in the computational process is a complex issue.\n    ",
        "submission_date": "1996-07-08T00:00:00",
        "last_modified_date": "1996-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607009",
        "title": "Semantic-based Transfer",
        "authors": [
            "Michael Dorna",
            "Martin C. Emele"
        ],
        "abstract": "  This article presents a new semantic-based transfer approach developed and applied within the Verbmobil Machine Translation project. We give an overview of the declarative transfer formalism together with its procedural realization. Our approach is discussed and compared with several other approaches from the MT literature.\n    ",
        "submission_date": "1996-07-09T00:00:00",
        "last_modified_date": "1996-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607010",
        "title": "Efficient Implementation of a Semantic-based Transfer Approach",
        "authors": [
            "Michael Dorna",
            "Martin C. Emele"
        ],
        "abstract": "  This article gives an overview of a new semantic-based transfer approach developed and applied within the Verbmobil Machine Translation project. We present the declarative transfer formalism and discuss its implementation.\n    ",
        "submission_date": "1996-07-09T00:00:00",
        "last_modified_date": "1996-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607011",
        "title": "Pattern-Based Context-Free Grammars for Machine Translation",
        "authors": [
            "Koichi Takeda"
        ],
        "abstract": "  This paper proposes the use of ``pattern-based'' context-free grammars as a basis for building machine translation (MT) systems, which are now being adopted as personal tools by a broad range of users in the cyberspace society. We discuss major requirements for such tools, including easy customization for diverse domains, the efficiency of the translation algorithm, and scalability (incremental improvement in translation quality through user interaction), and describe how our approach meets these requirements.\n    ",
        "submission_date": "1996-07-11T00:00:00",
        "last_modified_date": "1996-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607012",
        "title": "MBT: A Memory-Based Part of Speech Tagger-Generator",
        "authors": [
            "Walter Daelemans",
            "Jakub Zavrel",
            "Peter Berck",
            "Steven Gillis"
        ],
        "abstract": "  We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using {\\em IGTree}, a tree-based formalism for indexing and searching huge case bases.} The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed.\n    ",
        "submission_date": "1996-07-11T00:00:00",
        "last_modified_date": "1996-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607013",
        "title": "Unsupervised Discovery of Phonological Categories through Supervised Learning of Morphological Rules",
        "authors": [
            "Walter Daelemans",
            "Peter Berck",
            "Steven Gillis"
        ],
        "abstract": "  We describe a case study in the application of {\\em symbolic machine learning} techniques for the discovery of linguistic rules and categories. A supervised rule induction algorithm is used to learn to predict the correct diminutive suffix given the phonological representation of Dutch nouns. The system produces rules which are comparable to rules proposed by linguists. Furthermore, in the process of learning this morphological task, the phonemes used are grouped into phonologically relevant categories. We discuss the relevance of our method for linguistics and language technology.\n    ",
        "submission_date": "1996-07-11T00:00:00",
        "last_modified_date": "1996-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607014",
        "title": "A Corpus Study of Negative Imperatives in Natural Language Instructions",
        "authors": [
            "Keith Vander Linden",
            "Barbara Di Eugenio"
        ],
        "abstract": "  In this paper, we define the notion of a preventative expression and discuss a corpus study of such expressions in instructional text. We discuss our coding schema, which takes into account both form and function features, and present measures of inter-coder reliability for those features. We then discuss the correlations that exist between the function and the form features.\n    ",
        "submission_date": "1996-07-12T00:00:00",
        "last_modified_date": "1996-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607015",
        "title": "Learning Micro-Planning Rules for Preventative Expressions",
        "authors": [
            "Keith Vander Linden",
            "; Barbara Di Eugenio"
        ],
        "abstract": "  Building text planning resources by hand is time-consuming and difficult. Certainly, a number of planning architectures and their accompanying plan libraries have been implemented, but while the architectures themselves may be reused in a new domain, the library of plans typically cannot. One way to address this problem is to use machine learning techniques to automate the derivation of planning resources for new domains. In this paper, we apply this technique to build micro-planning rules for preventative expressions in instructional text.\n    ",
        "submission_date": "1996-07-12T00:00:00",
        "last_modified_date": "1996-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607016",
        "title": "Beyond Word N-Grams",
        "authors": [
            "Fernando C. N. Pereira",
            "Yoram Singer",
            "Naftali Tishby"
        ],
        "abstract": "  We describe, analyze, and evaluate experimentally a new probabilistic model for word-sequence prediction in natural language based on prediction suffix trees (PSTs). By using efficient data structures, we extend the notion of PST to unbounded vocabularies. We also show how to use a Bayesian approach based on recursive priors over all possible PSTs to efficiently maintain tree mixtures. These mixtures have provably and practically better performance than almost any single model. We evaluate the model on several corpora. The low perplexity achieved by relatively small PST mixture models suggests that they may be an advantageous alternative, both theoretically and practically, to the widely used n-gram models.\n    ",
        "submission_date": "1996-07-13T00:00:00",
        "last_modified_date": "1996-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607017",
        "title": "Natural Language Processing: Structure and Complexity",
        "authors": [
            "Wlodek Zadrozny"
        ],
        "abstract": "  We introduce a method for analyzing the complexity of natural language processing tasks, and for predicting the difficulty new NLP tasks.\n",
        "submission_date": "1996-07-13T00:00:00",
        "last_modified_date": "1996-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607018",
        "title": "TSNLP - Test Suites for Natural Language Processing",
        "authors": [
            "Sabine Lehmann",
            "Stephan Oepen",
            "Sylvie Regnier-Prost",
            "Klaus Netter",
            "Veronika Lux",
            "Judith Klein",
            "Kirsten Falkedal",
            "Frederik Fouvry",
            "Dominique Estival",
            "Eva Dauphin",
            "Herve Compagnion",
            "Judith Baur",
            "Judith Baur",
            "Lorna Balkan",
            "Doug Arnold"
        ],
        "abstract": "  The TSNLP project has investigated various aspects of the construction, maintenance and application of systematic test suites as diagnostic and evaluation tools for NLP applications. The paper summarizes the motivation and main results of the project: besides the solid methodological foundation, TSNLP has produced substantial multi-purpose and multi-user test suites for three European languages together with a set of specialized tools that facilitate the construction, extension, maintenance, retrieval, and customization of the test data. As TSNLP results, including the data and technology, are made publicly available, the project presents a valuable linguistic resourc e that has the potential of providing a wide-spread pre-standard diagnostic and evaluation tool for both developers and users of NLP applications.\n    ",
        "submission_date": "1996-07-15T00:00:00",
        "last_modified_date": "1996-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607019",
        "title": "Mental State Adjectives: the Perspective of Generative Lexicon",
        "authors": [
            "Pierrette Bouillon"
        ],
        "abstract": "  This paper focusses on mental state adjectives and offers a unified analysis in the theory of Generative Lexicon (Pustejovsky, 1991, 1995). We show that, instead of enumerating the various syntactic constructions they enter into, with the different senses which arise, it is possible to give them a rich typed semantic representation which will explain both their semantic and syntactic polymorphism.\n    ",
        "submission_date": "1996-07-15T00:00:00",
        "last_modified_date": "1996-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607020",
        "title": "A Divide-and-Conquer Strategy for Parsing",
        "authors": [
            "Peh Li Shiuan",
            "Christopher Ting Hian Ann"
        ],
        "abstract": "  In this paper, we propose a novel strategy which is designed to enhance the accuracy of the parser by simplifying complex sentences before parsing. This approach involves the separate parsing of the constituent sub-sentences within a complex sentence. To achieve that, the divide-and-conquer strategy first disambiguates the roles of the link words in the sentence and segments the sentence based on these roles. The separate parse trees of the segmented sub-sentences and the noun phrases within them are then synthesized to form the final parse. To evaluate the effects of this strategy on parsing, we compare the original performance of a dependency parser with the performance when it is enhanced with the divide-and-conquer strategy. When tested on 600 sentences of the IPSM'95 data sets, the enhanced parser saw a considerable error reduction of 21.2% in its accuracy.\n    ",
        "submission_date": "1996-07-16T00:00:00",
        "last_modified_date": "1996-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607021",
        "title": "Morphological Analysis as Classification: an Inductive-Learning Approach",
        "authors": [
            "Antal van den Bosch",
            "Walter Daelemans",
            "Ton Weijters"
        ],
        "abstract": "  Morphological analysis is an important subtask in text-to-speech conversion, hyphenation, and other language engineering tasks. The traditional approach to performing morphological analysis is to combine a morpheme lexicon, sets of (linguistic) rules, and heuristics to find a most probable analysis. In contrast we present an inductive learning approach in which morphological analysis is reformulated as a segmentation task. We report on a number of experiments in which five inductive learning algorithms are applied to three variations of the task of morphological analysis. Results show (i) that the generalisation performance of the algorithms is good, and (ii) that the lazy learning algorithm IB1-IG performs best on all three tasks. We conclude that lazy learning of morphological analysis as a classification task is indeed a viable approach; moreover, it has the strong advantages over the traditional approach of avoiding the knowledge-acquisition bottleneck, being fast and deterministic in learning and processing, and being language-independent.\n    ",
        "submission_date": "1996-07-16T00:00:00",
        "last_modified_date": "1996-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607022",
        "title": "A Machine Learning Approach to the Classification of Dialogue Utterances",
        "authors": [
            "Toine Andernach"
        ],
        "abstract": "  The purpose of this paper is to present a method for automatic classification of dialogue utterances and the results of applying that method to a corpus. Superficial features of a set of training utterances (which we will call cues) are taken as the basis for finding relevant utterance classes and for extracting rules for assigning these classes to new utterances. Each cue is assumed to partially contribute to the communicative function of an utterance. Instead of relying on subjective judgments for the tasks of finding classes and rules, we opt for using machine learning techniques to guarantee objectivity.\n    ",
        "submission_date": "1996-07-16T00:00:00",
        "last_modified_date": "1996-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607023",
        "title": "Phonological modeling for continuous speech recognition in Korean",
        "authors": [
            "WonIl Lee",
            "Geunbae Lee",
            "Jong-Hyeok Lee"
        ],
        "abstract": "  A new scheme to represent phonological changes during continuous speech recognition is suggested. A phonological tag coupled with its morphological tag is designed to represent the conditions of Korean phonological changes. A pairwise language model of these morphological and phonological tags is implemented in Korean speech recognition system. Performance of the model is verified through the TDNN-based speech recognition experiments.\n    ",
        "submission_date": "1996-07-18T00:00:00",
        "last_modified_date": "1996-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607024",
        "title": "Applying Winnow to Context-Sensitive Spelling Correction",
        "authors": [
            "Andrew R. Golding",
            "Dan Roth"
        ],
        "abstract": "  Multiplicative weight-updating algorithms such as Winnow have been studied extensively in the COLT literature, but only recently have people started to use them in applications. In this paper, we apply a Winnow-based algorithm to a task in natural language: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting {\\it to\\/} for {\\it too}, {\\it casual\\/} for {\\it causal}, and so on. Previous approaches to this problem have been statistics-based; we compare Winnow to one of the more successful such approaches, which uses Bayesian classifiers. We find that: (1)~When the standard (heavily-pruned) set of features is used to describe problem instances, Winnow performs comparably to the Bayesian method; (2)~When the full (unpruned) set of features is used, Winnow is able to exploit the new features and convincingly outperform Bayes; and (3)~When a test set is encountered that is dissimilar to the training set, Winnow is better than Bayes at adapting to the unfamiliar test set, using a strategy we will present for combining learning on the training set with unsupervised learning on the (noisy) test set.\n    ",
        "submission_date": "1996-07-19T00:00:00",
        "last_modified_date": "1996-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607025",
        "title": "New Methods, Current Trends and Software Infrastructure for NLP",
        "authors": [
            "Hamish Cunningham",
            "Yorick Wilks",
            "Robert J. Gaizauskas"
        ],
        "abstract": "  The increasing use of `new methods' in NLP, which the NeMLaP conference series exemplifies, occurs in the context of a wider shift in the nature and concerns of the discipline. This paper begins with a short review of this context and significant trends in the field. The review motivates and leads to a set of requirements for support software of general utility for NLP research and development workers. A freely-available system designed to meet these requirements is described (called GATE - a General Architecture for Text Engineering). Information Extraction (IE), in the sense defined by the Message Understanding Conferences (ARPA \\cite{Arp95}), is an NLP application in which many of the new methods have found a home (Hobbs \\cite{Hob93}; Jacobs ed. \\cite{Jac92}). An IE system based on GATE is also available for research purposes, and this is described. Lastly we review related work.\n    ",
        "submission_date": "1996-07-23T00:00:00",
        "last_modified_date": "1996-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607026",
        "title": "Building Knowledge Bases for the Generation of Software Documentation",
        "authors": [
            "Cecile Paris",
            "Keith Vander Linden"
        ],
        "abstract": "  Automated text generation requires a underlying knowledge base from which to generate, which is often difficult to produce. Software documentation is one domain in which parts of this knowledge base may be derived automatically. In this paper, we describe \\drafter, an authoring support tool for generating user-centred software documentation, and in particular, we describe how parts of its required knowledge base can be obtained automatically.\n    ",
        "submission_date": "1996-07-25T00:00:00",
        "last_modified_date": "1996-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607027",
        "title": "Learning Translation Rules From A Bilingual Corpus",
        "authors": [
            "Ilyas Cicekli",
            "H. Altay Guvenir"
        ],
        "abstract": "  This paper proposes a mechanism for learning pattern correspondences between two languages from a corpus of translated sentence pairs. The proposed mechanism uses analogical reasoning between two translations. Given a pair of translations, the similar parts of the sentences in the source language must correspond the similar parts of the sentences in the target language. Similarly, the different parts should correspond to the respective parts in the translated sentences. The correspondences between the similarities, and also differences are learned in the form of translation rules. The system is tested on a small training dataset and produced promising results for further investigation.\n    ",
        "submission_date": "1996-07-26T00:00:00",
        "last_modified_date": "1996-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607028",
        "title": "The Grammar of Sense: Is word-sense tagging much more than part-of-speech tagging?",
        "authors": [
            "Yorick Wilks",
            "Mark Stevenson"
        ],
        "abstract": "  This squib claims that Large-scale Automatic Sense Tagging of text (LAST) can be done at a high-level of accuracy and with far less complexity and computational effort than has been believed until now. Moreover, it can be done for all open class words, and not just carefully selected opposed pairs as in some recent work. We describe two experiments: one exploring the amount of information relevant to sense disambiguation which is contained in the part-of-speech field of entries in Longman Dictionary of Contemporary English (LDOCE). Another, more practical, experiment attempts sense disambiguation of all open class words in a text assigning LDOCE homographs as sense tags using only part-of-speech information. We report that 92% of open class words can be successfully tagged in this way. We plan to extend this work and to implement an improved large-scale tagger, a description of which is included here.\n    ",
        "submission_date": "1996-07-26T00:00:00",
        "last_modified_date": "1996-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607029",
        "title": "Design and Implementation of a Tactical Generator for Turkish, a Free Constituent Order Language",
        "authors": [
            "Dilek Zeynep Hakkani"
        ],
        "abstract": "  This thesis describes a tactical generator for Turkish, a free constituent order language, in which the order of the constituents may change according to the information structure of the sentences to be generated. In the absence of any information regarding the information structure of a sentence (i.e., topic, focus, background, etc.), the constituents of the sentence obey a default order, but the order is almost freely changeable, depending on the constraints of the text flow or discourse. We have used a recursively structured finite state machine for handling the changes in constituent order, implemented as a right-linear grammar backbone. Our implementation environment is the GenKit system, developed at Carnegie Mellon University--Center for Machine Translation. Morphological realization has been implemented using an external morphological analysis/generation component which performs concrete morpheme selection and handles morphographemic processes.\n    ",
        "submission_date": "1996-07-30T00:00:00",
        "last_modified_date": "1996-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607030",
        "title": "Using Multiple Sources of Information for Constraint-Based Morphological Disambiguation",
        "authors": [
            "Gokhan Tur"
        ],
        "abstract": "  This thesis presents a constraint-based morphological disambiguation approach that is applicable to languages with complex morphology--specifically agglutinative languages with productive inflectional and derivational morphological phenomena. For morphologically complex languages like Turkish, automatic morphological disambiguation involves selecting for each token morphological parse(s), with the right set of inflectional and derivational markers. Our system combines corpus independent hand-crafted constraint rules, constraint rules that are learned via unsupervised learning from a training corpus, and additional statistical information obtained from the corpus to be morphologically disambiguated. The hand-crafted rules are linguistically motivated and tuned to improve precision without sacrificing recall. In certain respects, our approach has been motivated by Brill's recent work, but with the observation that his transformational approach is not directly applicable to languages like Turkish. Our approach also uses a novel approach to unknown word processing by employing a secondary morphological processor which recovers any relevant inflectional and derivational information from a lexical item whose root is unknown. With this approach, well below 1% of the tokens remains as unknown in the texts we have experimented with. Our results indicate that by combining these hand-crafted, statistical and learned information sources, we can attain a recall of 96 to 97% with a corresponding precision of 93 to 94%, and ambiguity of 1.02 to 1.03 parses per token.\n    ",
        "submission_date": "1996-07-30T00:00:00",
        "last_modified_date": "1996-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607031",
        "title": "Compositional Semantics in Verbmobil",
        "authors": [
            "Johan Bos",
            "Bj\u00f6rn Gamb\u00e4ck",
            "Christian Lieske",
            "Yoshiki Mori",
            "Manfred Pinkal",
            "Karsten Worm"
        ],
        "abstract": "  The paper discusses how compositional semantics is implemented in the Verbmobil speech-to-speech translation system using LUD, a description language for underspecified discourse representation structures. The description language and its formal interpretation in DRT are described as well as its implementation together with the architecture of the system's entire syntactic-semantic processing module. We show that a linguistically sound theory and formalism can be properly implemented in a system with (near) real-time requirements.\n    ",
        "submission_date": "1996-07-30T00:00:00",
        "last_modified_date": "1996-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607032",
        "title": "A Lexical Semantic Database for Verbmobil",
        "authors": [
            "Johannes Heinecke",
            "Karsten L. Worm"
        ],
        "abstract": "  This paper describes the development and use of a lexical semantic database for the Verbmobil speech-to-speech machine translation system. The motivation is to provide a common information source for the distributed development of the semantics, transfer and semantic evaluation modules and to store lexical semantic information application-independently.\n",
        "submission_date": "1996-07-30T00:00:00",
        "last_modified_date": "1996-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607033",
        "title": "Multiple Discourse Relations on the Sentential Level in Japanese",
        "authors": [
            "Yoshiki Mori"
        ],
        "abstract": "  In the German government (BMBF) funded project Verbmobil, a semantic formalism Language for Underspecified Discourse Representation Structures (LUD) is used which describes several DRSs and allows for underspecification. Dealing with Japanese poses challenging problems. In this paper, a treatment of multiple discourse relation constructions on the sentential level is shown, which are common in Japanese but cause a problem for the formalism,. The problem is to distinguish discourse relations which take the widest scope compared with other scope-taking elements on the one hand and to have them underspecified among each other on the other hand. We also state a semantic constraint on the resolution of multiple discourse relations which seems to prevail over the syntactic c-command constraint.\n    ",
        "submission_date": "1996-07-30T00:00:00",
        "last_modified_date": "1996-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607034",
        "title": "Using textual clues to improve metaphor processing",
        "authors": [
            "St\u00e9phane Ferrari"
        ],
        "abstract": "  In this paper, we propose a textual clue approach to help metaphor detection, in order to improve the semantic processing of this figure. The previous works in the domain studied the semantic regularities only, overlooking an obvious set of regularities. A corpus-based analysis shows the existence of surface regularities related to metaphors. These clues can be characterized by syntactic structures and lexical markers. We present an object oriented model for representing the textual clues that were found. This representation is designed to help the choice of a semantic processing, in terms of possible non-literal meanings. A prototype implementing this model is currently under development, within an incremental approach allowing step-by-step evaluations. \\footnote{This work takes part in a research project sponsored by the AUPELF-UREF (Francophone Agency For Education and Research)}\n    ",
        "submission_date": "1996-07-30T00:00:00",
        "last_modified_date": "1996-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607035",
        "title": "Completeness of Compositional Translation for Context-Free Grammars",
        "authors": [
            "Willem-Olaf Huijsen"
        ],
        "abstract": "  A machine translation system is said to be *complete* if all expressions that are correct according to the source-language grammar can be translated into the target language. This paper addresses the completeness issue for compositional machine translation in general, and for compositional machine translation of context-free grammars in particular. Conditions that guarantee translation completeness of context-free grammars are presented.\n    ",
        "submission_date": "1996-07-31T00:00:00",
        "last_modified_date": "1996-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607036",
        "title": "Connected Text Recognition Using Layered HMMs and Token Passing",
        "authors": [
            "Peter Ingels"
        ],
        "abstract": "  We present a novel approach to lexical error recovery on textual input. An advanced robust tokenizer has been implemented that can not only correct spelling mistakes, but also recover from segmentation errors. Apart from the orthographic considerations taken, the tokenizer also makes use of linguistic expectations extracted from a training corpus. The idea is to arrange Hidden Markov Models (HMM) in multiple layers where the HMMs in each layer are responsible for different aspects of the processing of the input. We report on experimental evaluations with alternative probabilistic language models to guide the lexical error recovery process.\n    ",
        "submission_date": "1996-07-31T00:00:00",
        "last_modified_date": "1996-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9607037",
        "title": "Automatic Construction of Clean Broad-Coverage Translation Lexicons",
        "authors": [
            "I. Dan Melamed"
        ],
        "abstract": "  Word-level translational equivalences can be extracted from parallel texts by surprisingly simple statistical techniques. However, these techniques are easily fooled by {\\em indirect associations} --- pairs of unrelated words whose statistical properties resemble those of mutual translations. Indirect associations pollute the resulting translation lexicons, drastically reducing their precision. This paper presents an iterative lexicon cleaning method. On each iteration, most of the remaining incorrect lexicon entries are filtered out, without significant degradation in recall. This lexicon cleaning technique can produce translation lexicons with recall and precision both exceeding 90\\%, as well as dictionary-sized translation lexicons that are over 99\\% correct.\n    ",
        "submission_date": "1996-08-01T00:00:00",
        "last_modified_date": "1996-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608001",
        "title": "Storage of Natural Language Sentences in a Hopfield Network",
        "authors": [
            "Nigel Collier"
        ],
        "abstract": "  This paper look at how the Hopfield neural network can be used to store and recall patterns constructed from natural language sentences. As a pattern recognition and storage tool, the Hopfield neural network has received much attention. This attention however has been mainly in the field of statistical physics due to the model's simple abstraction of spin glass systems. A discussion is made of the differences, shown as bias and correlation, between natural language sentence patterns and the randomly generated ones used in previous experiments. Results are given for numerical simulations which show the auto-associative competence of the network when trained with natural language patterns.\n    ",
        "submission_date": "1996-08-02T00:00:00",
        "last_modified_date": "1996-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608002",
        "title": "Controlling Functional Uncertainty",
        "authors": [
            "Rolf Backofen"
        ],
        "abstract": "  There have been two different methods for checking the satisfiability of feature descriptions that use the functional uncertainty device, namely~\\cite{Kaplan:88CO} and \\cite{Backofen:94JSC}. Although only the one in \\cite{Backofen:94JSC} solves the satisfiability problem completely, both methods have their merits. But it may happen that in one single description, there are parts where the first method is more appropriate, and other parts where the second should be applied. In this paper, we present a common framework that allows one to combine both methods. This is done by presenting a set of rules for simplifying feature descriptions. The different methods are described as different controls on this rule set, where a control specifies in which order the different rules must be applied.\n    ",
        "submission_date": "1996-08-06T00:00:00",
        "last_modified_date": "1996-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608003",
        "title": "Stylistic Variation in an Information Retrieval Experiment",
        "authors": [
            "Jussi Karlgren"
        ],
        "abstract": "  Texts exhibit considerable stylistic variation. This paper reports an experiment where a corpus of documents (N= 75 000) is analyzed using various simple stylistic metrics. A subset (n = 1000) of the corpus has been previously assessed to be relevant for answering given information retrieval queries. The experiment shows that this subset differs significantly from the rest of the corpus in terms of the stylistic metrics studied.\n    ",
        "submission_date": "1996-08-08T00:00:00",
        "last_modified_date": "1996-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608004",
        "title": "Patterns of Language - A Population Model for Language Structure",
        "authors": [
            "Robert John Freeman"
        ],
        "abstract": "  A key problem in the description of language structure is to explain its contradictory properties of specificity and generality, the contrasting poles of formulaic prescription and generative productivity. I argue that this is possible if we accept analogy and similarity as the basic mechanisms of structural definition. As a specific example I discuss how it would be possible to use analogy to define a generative model of syntactic structure.\n    ",
        "submission_date": "1996-08-09T00:00:00",
        "last_modified_date": "1996-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608005",
        "title": "CLEARS - An Education and Research Tool for Computational Semantics",
        "authors": [
            "David Milward",
            "Karsten Konrad",
            "Holger Maier",
            "Manfred Pinkal"
        ],
        "abstract": "  The CLEARS (Computational Linguistics Education and Research for Semantics) tool provides a graphical interface allowing interactive construction of semantic representations in a variety of different formalisms, and using several construction methods. CLEARS was developed as part of the FraCaS project which was designed to encourage convergence between different semantic formalisms, such as Montague-Grammar, DRT, and Situation Semantics. The CLEARS system is freely available on the WWW from ",
        "submission_date": "1996-08-13T00:00:00",
        "last_modified_date": "1996-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608006",
        "title": "Grapheme-to-Phoneme Conversion using Multiple Unbounded Overlapping Chunks",
        "authors": [
            "Francois Yvon"
        ],
        "abstract": "  We present in this paper an original extension of two data-driven algorithms for the transcription of a sequence of graphemes into the corresponding sequence of phonemes. In particular, our approach generalizes the algorithm originally proposed by Dedina and Nusbaum (D&N) (1991), which had originally been promoted as a model of the human ability to pronounce unknown words by analogy to familiar lexical items. We will show that DN's algorithm performs comparatively poorly when evaluated on a realistic test set, and that our extension allows us to improve substantially the performance of the analogy-based model. We will also suggest that both algorithms can be reformulated in a much more general framework, which allows us to anticipate other useful extensions. However, considering the inability to define in these models important notions like lexical neighborhood, we conclude that both approaches fail to offer a proper model of the analogical processes involved in reading aloud.\n    ",
        "submission_date": "1996-08-14T00:00:00",
        "last_modified_date": "1996-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608007",
        "title": "Centering in Italian",
        "authors": [
            "Barbara Di Eugenio"
        ],
        "abstract": "  This paper explores the correlation between centering and different forms of pronominal reference in Italian, in particular zeros and overt pronouns in subject position. Such correlations, that I had proposed in earlier work (COLING 90), are verified through the analysis of a corpus of naturally occurring texts. In the process, I extend my previous analysis in several ways, for example by taking possessives and subordinates into account. I also provide a more detailed analysis of the \"continue\" transition: more specifically, I show that pronouns are used in a markedly different way in a \"continue\" preceded by another \"continue\" or by a \"shift\", and in a \"continue\" preceded by a \"retain\".\n    ",
        "submission_date": "1996-08-14T00:00:00",
        "last_modified_date": "1996-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608008",
        "title": "The discourse functions of Italian subjects: a centering approach",
        "authors": [
            "Barbara Di Eugenio"
        ],
        "abstract": "  This paper examines the discourse functions that different types of subjects perform in Italian within the centering framework. I build on my previous work (COLING90) that accounted for the alternation of null and strong pronouns in subject position. I extend my previous analysis in several ways: for example, I refine the notion of {\\sc continue} and discuss the centering functions of full NPs.\n    ",
        "submission_date": "1996-08-14T00:00:00",
        "last_modified_date": "1996-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608009",
        "title": "Centering theory and the Italian pronominal system",
        "authors": [
            "Barbara Di Eugenio"
        ],
        "abstract": "  In this paper, I give an account of some phenomena of pronominalization in Italian in terms of centering theory. After a general introduction to the Italian pronominal system, I will review centering, and then show how the original rules have to be extended or modified. Finally, I will show that centering does not account for two phenomena: first, the functional role of an utterance may override the predictions of centering; second, a null subject can be used to refer to a whole discourse segment.\n    ",
        "submission_date": "1996-08-14T00:00:00",
        "last_modified_date": "1996-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608010",
        "title": "Fishing for Exactness",
        "authors": [
            "Ted Pedersen"
        ],
        "abstract": "  Statistical methods for automatically identifying dependent word pairs (i.e. dependent bigrams) in a corpus of natural language text have traditionally been performed using asymptotic tests of significance. This paper suggests that Fisher's exact test is a more appropriate test due to the skewed and sparse data samples typical of this problem. Both theoretical and experimental comparisons between Fisher's exact test and a variety of asymptotic tests (the t-test, Pearson's chi-square test, and Likelihood-ratio chi-square test) are presented. These comparisons show that Fisher's exact test is more reliable in identifying dependent word pairs. The usefulness of Fisher's exact test extends to other problems in statistical natural language processing as skewed and sparse data appears to be the rule in natural language. The experiment presented in this paper was performed using PROC FREQ of the SAS System.\n    ",
        "submission_date": "1996-08-16T00:00:00",
        "last_modified_date": "1996-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608011",
        "title": "Punctuation in Quoted Speech",
        "authors": [
            "Christine Doran"
        ],
        "abstract": "  Quoted speech is often set off by punctuation marks, in particular quotation marks. Thus, it might seem that the quotation marks would be extremely useful in identifying these structures in texts. Unfortunately, the situation is not quite so clear. In this work, I will argue that quotation marks are not adequate for either identifying or constraining the syntax of quoted speech. More useful information comes from the presence of a quoting verb, which is either a verb of saying or a punctual verb, and the presence of other punctuation marks, usually commas. Using a lexicalized grammar, we can license most quoting clauses as text adjuncts. A distinction will be made not between direct and indirect quoted speech, but rather between adjunct and non-adjunct quoting clauses.\n    ",
        "submission_date": "1996-08-16T00:00:00",
        "last_modified_date": "1996-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608012",
        "title": "Multilingual Text Analysis for Text-to-Speech Synthesis",
        "authors": [
            "Richard Sproat"
        ],
        "abstract": "  We present a model of text analysis for text-to-speech (TTS) synthesis based on (weighted) finite-state transducers, which serves as the text-analysis module of the multilingual Bell Labs TTS system. The transducers are constructed using a lexical toolkit that allows declarative descriptions of lexicons, morphological rules, numeral-expansion rules, and phonological rules, inter alia. To date, the model has been applied to eight languages: Spanish, Italian, Romanian, French, German, Russian, Mandarin and Japanese.\n    ",
        "submission_date": "1996-08-19T00:00:00",
        "last_modified_date": "1996-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608013",
        "title": "A Word Grammar of Turkish with Morphophonemic Rules",
        "authors": [
            "S. Murat Oztaner"
        ],
        "abstract": "  In this thesis, morphological description of Turkish is encoded using the two-level model. This description is made up of the phonological component that contains the two-level morphophonemic rules, and the lexicon component which lists the lexical items and encodes the morphotactic constraints. The word grammar is expressed in tabular form. It includes the verbal and the nominal paradigm. Vowel and consonant harmony, epenthesis, reduplication, etc. are described in detail and coded in two-level notation. Loan-word phonology is modelled separately.\n",
        "submission_date": "1996-08-20T00:00:00",
        "last_modified_date": "1996-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608014",
        "title": "Classifiers in Japanese-to-English Machine Translation",
        "authors": [
            "Francis Bond",
            "Kentaro Ogura",
            "Satoru Ikehara"
        ],
        "abstract": "  This paper proposes an analysis of classifiers into four major types: UNIT, METRIC, GROUP and SPECIES, based on properties of both Japanese and English. The analysis makes possible a uniform and straightforward treatment of noun phrases headed by classifiers in Japanese-to-English machine translation, and has been implemented in the MT system ALT-J/E. Although the analysis is based on the characteristics of, and differences between, Japanese and English, it is shown to be also applicable to the unrelated language Thai.\n    ",
        "submission_date": "1996-08-21T00:00:00",
        "last_modified_date": "1996-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608015",
        "title": "Morphological Productivity in the Lexicon",
        "authors": [
            "Onur Sehitoglu",
            "Cem Bozsahin"
        ],
        "abstract": "  In this paper we outline a lexical organization for Turkish that makes use of lexical rules for inflections, derivations, and lexical category changes to control the proliferation of lexical entries. Lexical rules handle changes in grammatical roles, enforce type constraints, and control the mapping of subcategorization frames in valency-changing operations. A lexical inheritance hierarchy facilitates the enforcement of type constraints. Semantic compositions in inflections and derivations are constrained by the properties of the terms and predicates.\n",
        "submission_date": "1996-08-23T00:00:00",
        "last_modified_date": "1996-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608016",
        "title": "A Sign-Based Phrase Structure Grammar for Turkish",
        "authors": [
            "Onur Tolga Sehitoglu"
        ],
        "abstract": "  This study analyses Turkish syntax from an informational point of view. Sign based linguistic representation and principles of HPSG (Head-driven Phrase Structure Grammar) theory are adapted to Turkish. The basic informational elements are nested and inherently sorted feature structures called signs.\n",
        "submission_date": "1996-08-26T00:00:00",
        "last_modified_date": "1996-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608017",
        "title": "Automatic Alignment of English-Chinese Bilingual Texts of CNS News",
        "authors": [
            "Donghua Xu",
            "Chew Lim Tan"
        ],
        "abstract": "  In this paper we address a method to align English-Chinese bilingual news reports from China News Service, combining both lexical and satistical approaches. Because of the sentential structure differences between English and Chinese, matching at the sentence level as in many other works may result in frequent matching of several sentences en masse. In view of this, the current work also attempts to create shorter alignment pairs by permitting finer matching between clauses from both texts if possible. The current method is based on statiscal correlation between sentence or clause length of both texts and at the same time uses obvious anchors such as numbers and place names appearing frequently in the news reports as lexcial cues.\n    ",
        "submission_date": "1996-08-27T00:00:00",
        "last_modified_date": "1996-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608018",
        "title": "Algorithms for Speech Recognition and Language Processing",
        "authors": [
            "Mehryar Mohri",
            "Michael Riley",
            "Richard Sproat"
        ],
        "abstract": "  Speech processing requires very efficient methods and algorithms. Finite-state transducers have been shown recently both to constitute a very useful abstract model and to lead to highly efficient time and space algorithms in this field. We present these methods and algorithms and illustrate them in the case of speech recognition. In addition to classical techniques, we describe many new algorithms such as minimization, global and local on-the-fly determinization of weighted automata, and efficient composition of transducers. These methods are currently used in large vocabulary speech recognition systems. We then show how the same formalism and algorithms can be used in text-to-speech applications and related areas of language processing such as morphology, syntax, and local grammars, in a very efficient way. The tutorial is self-contained and requires no specific computational or linguistic knowledge other than classical results.\n    ",
        "submission_date": "1996-08-27T00:00:00",
        "last_modified_date": "1996-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608019",
        "title": "Using sentence connectors for evaluating MT output",
        "authors": [
            "Eric M. Visser",
            "Masaru Fuji"
        ],
        "abstract": "  This paper elaborates on the design of a machine translation evaluation method that aims to determine to what degree the meaning of an original text is preserved in translation, without looking into the grammatical correctness of its constituent sentences. The basic idea is to have a human evaluator take the sentences of the translated text and, for each of these sentences, determine the semantic relationship that exists between it and the sentence immediately preceding it. In order to minimise evaluator dependence, relations between sentences are expressed in terms of the conjuncts that can connect them, rather than through explicit categories. For an n-sentence text this results in a list of n-1 sentence-to-sentence relationships, which we call the text's connectivity profile. This can then be compared to the connectivity profile of the original text, and the degree of correspondence between the two would be a measure for the quality of the translation.\n",
        "submission_date": "1996-08-29T00:00:00",
        "last_modified_date": "1996-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608020",
        "title": "Phonetic Ambiguity : Approaches, Touchstones, Pitfalls and New Approaches",
        "authors": [
            "Patrick Juola"
        ],
        "abstract": "  Phonetic ambiguity and confusibility are bugbears for any form of bottom-up or data-driven approach to language processing. The question of when an input is ``close enough'' to a target word pervades the entire problem spaces of speech recognition, synthesis, language acquisition, speech compression, and language representation, but the variety of representations that have been applied are demonstrably inadequate to at least some aspects of the problem. This paper reviews this inadequacy by examining several touchstone models in phonetic ambiguity and relating them to the problems they were designed to solve. An good solution would be, among other things, efficient, accurate, precise, and universally applicable to representation of words, ideally usable as a ``phonetic distance'' metric for direct measurement of the ``distance'' between word or utterance pairs. None of the proposed models can provide a complete solution to the problem; in general, there is no algorithmic theory of phonetic distance. It is unclear whether this is a weakness of our representational technology or a more fundamental difficulty with the problem statement.\n    ",
        "submission_date": "1996-08-29T00:00:00",
        "last_modified_date": "1996-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9608021",
        "title": "Isolated-Word Confusion Metrics and the PGPfone Alphabet",
        "authors": [
            "Patrick Juola"
        ],
        "abstract": "  Although the confusion of individual phonemes and features have been studied and analyzed since (Miller and Nicely, 1955), there has been little work done on extending this to a predictive theory of word-level confusions. The PGPfone alphabet is a good touchstone problem for developing such word-level confusion metrics. This paper presents some difficulties incurred, along with their proposed solutions, in the extension of phonetic confusion results to a theoretical whole-word phonetic distance metric. The proposed solutions have been used, in conjunction with a set of selection filters, in a genetic algorithm to automatically generate appropriate word lists for a radio alphabet. This work illustrates some principles and pitfalls that should be addressed in any numeric theory of isolated word perception.\n    ",
        "submission_date": "1996-08-29T00:00:00",
        "last_modified_date": "1996-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609001",
        "title": "Corrections and Higher-Order Unification",
        "authors": [
            "Claire Gardent",
            "Michael Kohlhase",
            "Noor van Neusen"
        ],
        "abstract": "  We propose an analysis of corrections which models some of the requirements corrections place on context. We then show that this analysis naturally extends to the interaction of corrections with pronominal anaphora on the one hand, and (in)definiteness on the other. The analysis builds on previous unification--based approaches to NL semantics and relies on Higher--Order Unification with Equivalences, a form of unification which takes into account not only syntactic beta-eta-identity but also denotational equivalence.\n    ",
        "submission_date": "1996-09-02T00:00:00",
        "last_modified_date": "1996-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609002",
        "title": "Inferring Acceptance and Rejection in Dialogue by Default Rules of Inference",
        "authors": [
            "Marilyn A. Walker"
        ],
        "abstract": "  This paper discusses the processes by which conversants in a dialogue can infer whether their assertions and proposals have been accepted or rejected by their conversational partners. It expands on previous work by showing that logical consistency is a necessary indicator of acceptance, but that it is not sufficient, and that logical inconsistency is sufficient as an indicator of rejection, but it is not necessary. I show how conversants can use information structure and prosody as well as logical reasoning in distinguishing between acceptances and logically consistent rejections, and relate this work to previous work on implicature and default reasoning by introducing three new classes of rejection: {\\sc implicature rejections}, {\\sc epistemic rejections} and {\\sc deliberation rejections}. I show how these rejections are inferred as a result of default inferences, which, by other analyses, would have been blocked by the context. In order to account for these facts, I propose a model of the common ground that allows these default inferences to go through, and show how the model, originally proposed to account for the various forms of acceptance, can also model all types of rejection.\n    ",
        "submission_date": "1996-09-07T00:00:00",
        "last_modified_date": "1996-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609003",
        "title": "Cue Phrase Classification Using Machine Learning",
        "authors": [
            "Diane J. Litman"
        ],
        "abstract": "  Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods.\n    ",
        "submission_date": "1996-09-09T00:00:00",
        "last_modified_date": "1996-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609004",
        "title": "A Principled Framework for Constructing Natural Language Interfaces To Temporal Databases",
        "authors": [
            "Ion Androutsopoulos"
        ],
        "abstract": "  Most existing natural language interfaces to databases (NLIDBs) were designed to be used with ``snapshot'' database systems, that provide very limited facilities for manipulating time-dependent data. Consequently, most NLIDBs also provide very limited support for the notion of time. The database community is becoming increasingly interested in _temporal_ database systems. These are intended to store and manipulate in a principled manner information not only about the present, but also about the past and future.\n",
        "submission_date": "1996-09-23T00:00:00",
        "last_modified_date": "1996-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609005",
        "title": "Centering in Japanese Discourse",
        "authors": [
            "Marilyn Walker",
            "Masayo Iida",
            "Sharon Cote"
        ],
        "abstract": "  In this paper we propose a computational treatment of the resolution of zero pronouns in Japanese discourse, using an adaptation of the centering algorithm. We are able to factor language-specific dependencies into one parameter of the centering algorithm. Previous analyses have stipulated that a zero pronoun and its cospecifier must share a grammatical function property such as {\\sc Subject} or {\\sc NonSubject}. We show that this property-sharing stipulation is unneeded. In addition we propose the notion of {\\sc topic ambiguity} within the centering framework, which predicts some ambiguities that occur in Japanese discourse. This analysis has implications for the design of language-independent discourse modules for Natural Language systems. The centering algorithm has been implemented in an HPSG Natural Language system with both English and Japanese grammars.\n    ",
        "submission_date": "1996-09-24T00:00:00",
        "last_modified_date": "1996-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609006",
        "title": "Japanese Discourse and the Process of Centering",
        "authors": [
            "Marilyn Walker",
            "Masayo Iida",
            "Sharon Cote"
        ],
        "abstract": "  This paper has three aims: (1) to generalize a computational account of the discourse process called {\\sc centering}, (2) to apply this account to discourse processing in Japanese so that it can be used in computational systems for machine translation or language understanding, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation. We argue that while discourse interpretation is an inferential process, syntactic cues constrain this process, and demonstrate this argument with respect to the interpretation of {\\sc zeros}, unexpressed arguments of the verb, in Japanese. The syntactic cues in Japanese discourse that we investigate are the morphological markers for grammatical {\\sc topic}, the postposition {\\it wa}, as well as those for grammatical functions such as {\\sc subject}, {\\em ga}, {\\sc object}, {\\em o} and {\\sc object2}, {\\em ni}. In addition, we investigate the role of speaker's {\\sc empathy}, which is the viewpoint from which an event is described. This is syntactically indicated through the use of verbal compounding, i.e. the auxiliary use of verbs such as {\\it kureta, kita}. Our results are based on a survey of native speakers of their interpretation of short discourses, consisting of minimal pairs, varied by one of the above factors. We demonstrate that these syntactic cues do indeed affect the interpretation of {\\sc zeros}, but that having previously been the {\\sc topic} and being realized as a {\\sc zero} also contributes to the salience of a discourse entity. We propose a discourse rule of {\\sc zero topic assignment}, and show that {\\sc centering} provides constraints on when a {\\sc zero} can be interpreted as the {\\sc zero topic}.\n    ",
        "submission_date": "1996-09-24T00:00:00",
        "last_modified_date": "1996-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609007",
        "title": "Discourse Coherence and Shifting Centers in Japanese Texts",
        "authors": [
            "Masayo Iida"
        ],
        "abstract": "  In languages such as Japanese, the use of {\\it zeros}, unexpressed arguments of the verb, in utterances that shift the topic involves a risk that the meaning intended by the speaker may not be transparent to the hearer. However, this potentially undesirable conversational strategy often occurs in the course of naturally-occurring discourse. In this chapter, I report on an empirical study of 250 utterances with {\\it zeros} in 20 Japanese newspaper articles. Each utterance is analyzed in terms of centering transitions and the form in which centers are realized by referring expressions. I also examine lexical subcategorization information, and tense and aspect in order to test the hypothesis that the speaker expects the hearer to use this information in determining global discourse structure. I explain the occurrence of {\\it zeros} in {\\sc retain} and {\\sc rough-shift} centering transitions, by claiming that a {\\it zero} can only be used in these cases when the shift of centers is supported by contextual information such as lexical semantics, tense and aspect, and agreement features. I then propose an algorithm by which centering can incorporate these observations to integrate centering with global discourse structure, and thus enhance its ability for non-local pronoun resolution.\n    ",
        "submission_date": "1996-09-24T00:00:00",
        "last_modified_date": "1996-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609008",
        "title": "Designing Statistical Language Learners: Experiments on Noun Compounds",
        "authors": [
            "Mark Lauer"
        ],
        "abstract": "  The goal of this thesis is to advance the exploration of the statistical language learning design space. In pursuit of that goal, the thesis makes two main theoretical contributions: (i) it identifies a new class of designs by specifying an architecture for natural language analysis in which probabilities are given to semantic forms rather than to more superficial linguistic elements; and (ii) it explores the development of a mathematical theory to predict the expected accuracy of statistical language learning systems in terms of the volume of data used to train them.\n",
        "submission_date": "1996-09-25T00:00:00",
        "last_modified_date": "1996-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609009",
        "title": "A Geometric Approach to Mapping Bitext Correspondence",
        "authors": [
            "I. Dan Melamed"
        ],
        "abstract": "  The first step in most corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation. Several automatic methods for this task have been proposed in recent years. Yet even the best of these methods can err by several typeset pages. The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm. SIMR's errors are smaller than those of the previous front-runner by more than a factor of 4. Its robustness has enabled new commercial-quality applications. The greedy nature of the algorithm makes it independent of memory resources. Unlike other bitext mapping algorithms, SIMR allows crossing correspondences to account for word order differences. Its output can be converted quickly and easily into a sentence alignment. SIMR's output has been used to align over 200 megabytes of the Canadian Hansards for publication by the Linguistic Data Consortium.\n    ",
        "submission_date": "1996-09-28T00:00:00",
        "last_modified_date": "1996-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9609010",
        "title": "Automatic Detection of Omissions in Translations",
        "authors": [
            "I. Dan Melamed"
        ],
        "abstract": "  ADOMIT is an algorithm for Automatic Detection of OMIssions in Translations. The algorithm relies solely on geometric analysis of bitext maps and uses no linguistic information. This property allows it to deal equally well with omissions that do not correspond to linguistic units, such as might result from word-processing mishaps. ADOMIT has proven itself by discovering many errors in a hand-constructed gold standard for evaluating bitext mapping algorithms. Quantitative evaluation on simulated omissions showed that, even with today's poor bitext mapping technology, ADOMIT is a valuable quality control tool for translators and translation bureaus.\n    ",
        "submission_date": "1996-09-28T00:00:00",
        "last_modified_date": "1996-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9610001",
        "title": "Death and Lightness: Using a Demographic Model to Find Support Verbs",
        "authors": [
            "Mark Dras",
            "Mike Johnson"
        ],
        "abstract": "  Some verbs have a particular kind of binary ambiguity: they can carry their normal, full meaning, or they can be merely acting as a prop for the nominal object. It has been suggested that there is a detectable pattern in the relationship between a verb acting as a prop (a \\term{support verb}) and the noun it supports.\n",
        "submission_date": "1996-10-02T00:00:00",
        "last_modified_date": "1996-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9610002",
        "title": "Gathering Statistics to Aspectually Classify Sentences with a Genetic Algorithm",
        "authors": [
            "Eric V. Siegel",
            "Kathleen R. McKeown"
        ],
        "abstract": "  This paper presents a method for large corpus analysis to semantically classify an entire clause. In particular, we use cooccurrence statistics among similar clauses to determine the aspectual class of an input clause. The process examines linguistic features of clauses that are relevant to aspectual classification. A genetic algorithm determines what combinations of linguistic features to use for this task.\n    ",
        "submission_date": "1996-10-21T00:00:00",
        "last_modified_date": "1996-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9610003",
        "title": "Stochastic Attribute-Value Grammars",
        "authors": [
            "Steven Abney"
        ],
        "abstract": "  Probabilistic analogues of regular and context-free grammars are well-known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define a correct parameter-estimation algorithm.\n",
        "submission_date": "1996-10-23T00:00:00",
        "last_modified_date": "1996-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9610004",
        "title": "A Faster Structured-Tag Word-Classification Method",
        "authors": [
            "Min Zhang"
        ],
        "abstract": "  Several methods have been proposed for processing a corpus to induce a tagset for the sub-language represented by the corpus. This paper examines a structured-tag word classification method introduced by McMahon (1994) and discussed further by McMahon & Smith (1995) in ",
        "submission_date": "1996-10-25T00:00:00",
        "last_modified_date": "1996-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9610005",
        "title": "Learning string edit distance",
        "authors": [
            "Eric Sven Ristad",
            "Peter N. Yianilos"
        ],
        "abstract": "  In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. In this report, we provide a stochastic model for string edit distance. Our stochastic model allows us to learn a string edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string edit distance with one fourth the error rate of the untrained Levenshtein distance. Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes.\n",
        "submission_date": "1996-10-29T00:00:00",
        "last_modified_date": "1997-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9610006",
        "title": "A Morphology-System and Part-of-Speech Tagger for German",
        "authors": [
            "Wolfgang Lezius",
            "Reinhard Rapp",
            "Manfred Wettler"
        ],
        "abstract": "  This paper presents an integrated tool for German morphology and statistical part-of-speech tagging which aims at making some well established methods widely available. The software is very user friendly, runs on any PC and can be downloaded as a complete package (including lexicon and documentation) from the World Wide Web. Compared with the performance of other tagging systems the tagger produces similar results.\n    ",
        "submission_date": "1996-10-30T00:00:00",
        "last_modified_date": "1996-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9611001",
        "title": "OT SIMPLE - a construction-kit approach to Optimality Theory implementation",
        "authors": [
            "Markus Walther"
        ],
        "abstract": "  This paper details a simple approach to the implementation of Optimality Theory (OT, Prince and Smolensky 1993) on a computer, in part reusing standard system software. In a nutshell, OT's GENerating source is implemented as a BinProlog program interpreting a context-free specification of a GEN structural grammar according to a user-supplied input form. The resulting set of textually flattened candidate tree representations is passed to the CONstraint stage. Constraints are implemented by finite-state transducers specified as `sed' stream editor scripts that typically map ill-formed portions of the candidate to violation marks. EVALuation of candidates reduces to simple sorting: the violation-mark-annotated output leaving CON is fed into `sort', which orders candidates on the basis of the violation vector column of each line, thereby bringing the optimal candidate to the top. This approach gave rise to OT SIMPLE, the first freely available software tool for the OT framework to provide generic facilities for both GEN and CONstraint definition. Its practical applicability is demonstrated by modelling the OT analysis of apparent subtractive pluralization in Upper Hessian presented in Golston and Wiese (1996).\n    ",
        "submission_date": "1996-11-12T00:00:00",
        "last_modified_date": "1996-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9611002",
        "title": "Unsupervised Language Acquisition",
        "authors": [
            "Carl de Marcken"
        ],
        "abstract": "  This thesis presents a computational theory of unsupervised language acquisition, precisely defining procedures for learning language from ordinary spoken or written utterances, with no explicit help from a teacher. The theory is based heavily on concepts borrowed from machine learning and statistical estimation. In particular, learning takes place by fitting a stochastic, generative model of language to the evidence. Much of the thesis is devoted to explaining conditions that must hold for this general learning strategy to arrive at linguistically desirable grammars. The thesis introduces a variety of technical innovations, among them a common representation for evidence and grammars, and a learning strategy that separates the ``content'' of linguistic parameters from their representation. Algorithms based on it suffer from few of the search problems that have plagued other computational approaches to language acquisition.\n",
        "submission_date": "1996-11-12T00:00:00",
        "last_modified_date": "1996-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9611003",
        "title": "Data-Oriented Language Processing. An Overview",
        "authors": [
            "Rens Bod",
            "Remko Scha"
        ],
        "abstract": "  During the last few years, a new approach to language processing has started to emerge, which has become known under various labels such as \"data-oriented parsing\", \"corpus-based interpretation\", and \"tree-bank grammar\" (cf. van den Berg et al. 1994; Bod 1992-96; Bod et al. 1996a/b; Bonnema 1996; Charniak 1996a/b; Goodman 1996; Kaplan 1996; Rajman 1995a/b; Scha 1990-92; Sekine & Grishman 1995; Sima'an et al. 1994; Sima'an 1995-96; Tugwell 1995). This approach, which we will call \"data-oriented processing\" or \"DOP\", embodies the assumption that human language perception and production works with representations of concrete past language experiences, rather than with abstract linguistic rules. The models that instantiate this approach therefore maintain large corpora of linguistic representations of previously occurring utterances. When processing a new input utterance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one.\n",
        "submission_date": "1996-11-14T00:00:00",
        "last_modified_date": "1996-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9611004",
        "title": "Nonuniform Markov models",
        "authors": [
            "Eric Sven Ristad",
            "Robert G. Thomas"
        ],
        "abstract": "  A statistical language model assigns probability to strings of arbitrary length. Unfortunately, it is not possible to gather reliable statistics on strings of arbitrary length from a finite corpus. Therefore, a statistical language model must decide that each symbol in a string depends on at most a small, finite number of other symbols in the string. In this report we propose a new way to model conditional independence in Markov models. The central feature of our nonuniform Markov model is that it makes predictions of varying lengths using contexts of varying lengths. Experiments on the Wall Street Journal reveal that the nonuniform model performs slightly better than the classic interpolated Markov model. This result is somewhat remarkable because both models contain identical numbers of parameters whose values are estimated in a similar manner. The only difference between the two models is how they combine the statistics of longer and shorter strings.\n",
        "submission_date": "1996-11-16T00:00:00",
        "last_modified_date": "1996-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9611005",
        "title": "Integrating HMM-Based Speech Recognition With Direct Manipulation In A Multimodal Korean Natural Language Interface",
        "authors": [
            "Geunbae Lee",
            "Jong-Hyeok Lee",
            "Sangeok Kim"
        ],
        "abstract": "  This paper presents a HMM-based speech recognition engine and its integration into direct manipulation interfaces for Korean document editor. Speech recognition can reduce typical tedious and repetitive actions which are inevitable in standard GUIs (graphic user interfaces). Our system consists of general speech recognition engine called ABrain {Auditory Brain} and speech commandable document editor called SHE {Simple Hearing Editor}. ABrain is a phoneme-based speech recognition engine which shows up to 97% of discrete command recognition rate. SHE is a EuroBridge widget-based document editor that supports speech commands as well as direct manipulation interfaces.\n    ",
        "submission_date": "1996-11-18T00:00:00",
        "last_modified_date": "1996-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9611006",
        "title": "A Framework for Natural Language Interfaces to Temporal Databases",
        "authors": [
            "I. Androutsopoulos",
            "G.D. Ritchie",
            "P. Thanisch"
        ],
        "abstract": "  Over the past thirty years, there has been considerable progress in the design of natural language interfaces to databases. Most of this work has concerned snapshot databases, in which there are only limited facilities for manipulating time-varying information. The database community is becoming increasingly interested in temporal databases, databases with special support for time-dependent entries. We have developed a framework for constructing natural language interfaces to temporal databases, drawing on research on temporal phenomena within logic and linguistics. The central part of our framework is a logic-like formal language, called TOP, which can capture the semantics of a wide range of English sentences. We have implemented an HPSG-based sentence analyser that converts a large set of English queries involving time into TOP formulae, and have formulated a provably correct procedure for translating TOP expressions into queries in the TSQL2 temporal database language. In this way we have established a sound route from English to a general-purpose temporal database language.\n    ",
        "submission_date": "1996-11-25T00:00:00",
        "last_modified_date": "1996-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9612001",
        "title": "Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning",
        "authors": [
            "Raymond J. Mooney"
        ],
        "abstract": "  This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context. The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques. The specific problem tested involves disambiguating six senses of the word ``line'' using the words in the current and proceeding sentence as context. The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference. We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems.\n    ",
        "submission_date": "1996-12-09T00:00:00",
        "last_modified_date": "1996-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9612002",
        "title": "Specialized Language Models using Dialogue Predictions",
        "authors": [
            "Cosmin Popovici",
            "Paolo Baggia"
        ],
        "abstract": "  This paper analyses language modeling in spoken dialogue systems for accessing a database. The use of several language models obtained by exploiting dialogue predictions gives better results than the use of a single model for the whole dialogue interaction. For this reason several models have been created, each one for a specific system question, such as the request or the confirmation of a parameter.\n",
        "submission_date": "1996-12-12T00:00:00",
        "last_modified_date": "1996-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9612003",
        "title": "Metrics for Evaluating Dialogue Strategies in a Spoken Language System",
        "authors": [
            "Morena Danieli",
            "Elisabetta Gerbino"
        ],
        "abstract": "  In this paper, we describe a set of metrics for the evaluation of different dialogue management strategies in an implemented real-time spoken language system. The set of metrics we propose offers useful insights in evaluating how particular choices in the dialogue management can affect the overall quality of the man-machine dialogue. The evaluation makes use of established metrics: the transaction success, the contextual appropriateness of system answers, the calculation of normal and correction turns in a dialogue. We also define a new metric, the implicit recovery, which allows to measure the ability of a dialogue manager to deal with errors by different levels of analysis. We report evaluation data from several experiments, and we compare two different approaches to dialogue repair strategies using the set of metrics we argue for.\n    ",
        "submission_date": "1996-12-17T00:00:00",
        "last_modified_date": "1996-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9612004",
        "title": "Dialogos: a Robust System for Human-Machine Spoken Dialogue on the Telephone",
        "authors": [
            "Dario Albesano",
            "Paolo Baggia",
            "Morena Danieli",
            "Roberto Gemello",
            "Elisabetta Gerbino",
            "Claudio Rullent"
        ],
        "abstract": "  This paper presents Dialogos, a real-time system for human-machine spoken dialogue on the telephone in task-oriented domains. The system has been tested in a large trial with inexperienced users and it has proved robust enough to allow spontaneous interactions both to users which get good recognition performance and to the ones which get lower scores. The robust behavior of the system has been achieved by combining the use of specific language models during the recognition phase of analysis, the tolerance toward spontaneous speech phenomena, the activity of a robust parser, and the use of pragmatic-based dialogue knowledge. This integration of the different modules allows to deal with partial or total breakdowns of the different levels of analysis. We report the field trial data of the system and the evaluation results of the overall system and of the submodules.\n    ",
        "submission_date": "1996-12-20T00:00:00",
        "last_modified_date": "1996-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9612005",
        "title": "Maximum Entropy Modeling Toolkit",
        "authors": [
            "Eric Sven Ristad"
        ],
        "abstract": "  The Maximum Entropy Modeling Toolkit supports parameter estimation and prediction for statistical language models in the maximum entropy framework. The maximum entropy framework provides a constructive method for obtaining the unique conditional distribution p*(y|x) that satisfies a set of linear constraints and maximizes the conditional entropy H(p|f) with respect to the empirical distribution f(x). The maximum entropy distribution p*(y|x) also has a unique parametric representation in the class of exponential models, as m(y|x) = r(y|x)/Z(x) where the numerator m(y|x) = prod_i alpha_i^g_i(x,y) is a product of exponential weights, with alpha_i = exp(lambda_i), and the denominator Z(x) = sum_y r(y|x) is required to satisfy the axioms of probability.\n",
        "submission_date": "1996-12-31T00:00:00",
        "last_modified_date": "1996-12-31T00:00:00"
    }
]