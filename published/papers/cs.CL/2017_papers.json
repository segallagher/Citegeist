[
    {
        "url": "https://arxiv.org/abs/1701.00066",
        "title": "A POS Tagger for Code Mixed Indian Social Media Text - ICON-2016 NLP Tools Contest Entry from Surukam",
        "authors": [
            "Sree Harsha Ramesh",
            "Raveena R Kumar"
        ],
        "abstract": "Building Part-of-Speech (POS) taggers for code-mixed Indian languages is a particularly challenging problem in computational linguistics due to a dearth of accurately annotated training corpora. ICON, as part of its NLP tools contest has organized this challenge as a shared task for the second consecutive year to improve the state-of-the-art. This paper describes the POS tagger built at Surukam to predict the coarse-grained and fine-grained POS tags for three language pairs - Bengali-English, Telugu-English and Hindi-English, with the text spanning three popular social media platforms - Facebook, WhatsApp and Twitter. We employed Conditional Random Fields as the sequence tagging algorithm and used a library called sklearn-crfsuite - a thin wrapper around CRFsuite for training our model. Among the features we used include - character n-grams, language information and patterns for emoji, number, punctuation and web-address. Our submissions in the constrained environment,i.e., without making any use of monolingual POS taggers or the like, obtained an overall average F1-score of 76.45%, which is comparable to the 2015 winning score of 76.79%.\n    ",
        "submission_date": "2016-12-31T00:00:00",
        "last_modified_date": "2016-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00138",
        "title": "Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization",
        "authors": [
            "Jun Suzuki",
            "Masaaki Nagata"
        ],
        "abstract": "This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark.\n    ",
        "submission_date": "2016-12-31T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00145",
        "title": "Expanding Subjective Lexicons for Social Media Mining with Embedding Subspaces",
        "authors": [
            "Silvio Amir",
            "R\u00e1mon Astudillo",
            "Wang Ling",
            "Paula C. Carvalho",
            "M\u00e1rio J. Silva"
        ],
        "abstract": "Recent approaches for sentiment lexicon induction have capitalized on pre-trained word embeddings that capture latent semantic properties. However, embeddings obtained by optimizing performance of a given task (e.g. predicting contextual words) are sub-optimal for other applications. In this paper, we address this problem by exploiting task-specific representations, induced via embedding sub-space projection. This allows us to expand lexicons describing multiple semantic properties. For each property, our model jointly learns suitable representations and the concomitant predictor. Experiments conducted over multiple subjective lexicons, show that our model outperforms previous work and other baselines; even in low training data regimes. Furthermore, lexicon-based sentiment classifiers built on top of our lexicons outperform similar resources and yield performances comparable to those of supervised models.\n    ",
        "submission_date": "2016-12-31T00:00:00",
        "last_modified_date": "2017-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00168",
        "title": "Social Media Argumentation Mining: The Quest for Deliberateness in Raucousness",
        "authors": [
            "Jan \u0160najder"
        ],
        "abstract": "Argumentation mining from social media content has attracted increasing attention. The task is both challenging and rewarding. The informal nature of user-generated content makes the task dauntingly difficult. On the other hand, the insights that could be gained by a large-scale analysis of social media argumentation make it a very worthwhile task. In this position paper I discuss the motivation for social media argumentation mining, as well as the tasks and challenges involved.\n    ",
        "submission_date": "2016-12-31T00:00:00",
        "last_modified_date": "2016-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00188",
        "title": "Aspect-augmented Adversarial Networks for Domain Adaptation",
        "authors": [
            "Yuan Zhang",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ],
        "abstract": "We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.\n    ",
        "submission_date": "2017-01-01T00:00:00",
        "last_modified_date": "2017-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00504",
        "title": "Stance detection in online discussions",
        "authors": [
            "Peter Krejzl",
            "Barbora Hourov\u00e1",
            "Josef Steinberger"
        ],
        "abstract": "This paper describes our system created to detect stance in online discussions. The goal is to identify whether the author of a comment is in favor of the given target or against. Our approach is based on a maximum entropy classifier, which uses surface-level, sentiment and domain-specific features. The system was originally developed to detect stance in English tweets. We adapted it to process Czech news commentaries.\n    ",
        "submission_date": "2017-01-02T00:00:00",
        "last_modified_date": "2017-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00562",
        "title": "End-to-End Attention based Text-Dependent Speaker Verification",
        "authors": [
            "Shi-Xiong Zhang",
            "Zhuo Chen",
            "Yong Zhao",
            "Jinyu Li",
            "Yifan Gong"
        ],
        "abstract": "A new type of End-to-End system for text-dependent speaker verification is presented in this paper. Previously, using the phonetically discriminative/speaker discriminative DNNs as feature extractors for speaker verification has shown promising results. The extracted frame-level (DNN bottleneck, posterior or d-vector) features are equally weighted and aggregated to compute an utterance-level speaker representation (d-vector or i-vector). In this work we use speaker discriminative CNNs to extract the noise-robust frame-level features. These features are smartly combined to form an utterance-level speaker vector through an attention mechanism. The proposed attention model takes the speaker discriminative information and the phonetic information to learn the weights. The whole system, including the CNN and attention model, is joint optimized using an end-to-end criterion. The training algorithm imitates exactly the evaluation process --- directly mapping a test utterance and a few target speaker utterances into a single verification score. The algorithm can automatically select the most similar impostor for each target speaker to train the network. We demonstrated the effectiveness of the proposed end-to-end system on Windows $10$ \"Hey Cortana\" speaker verification task.\n    ",
        "submission_date": "2017-01-03T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00576",
        "title": "Shortcut Sequence Tagging",
        "authors": [
            "Huijia Wu",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "Deep stacked RNNs are usually hard to train. Adding shortcut connections across different layers is a common way to ease the training of stacked networks. However, extra shortcuts make the recurrent step more complicated. To simply the stacked architecture, we propose a framework called shortcut block, which is a marriage of the gating mechanism and shortcuts, while discarding the self-connected part in LSTM cell. We present extensive empirical experiments showing that this design makes training easy and improves generalization. We propose various shortcut block topologies and compositions to explore its effectiveness. Based on this architecture, we obtain a 6% relatively improvement over the state-of-the-art on CCGbank supertagging dataset. We also get comparable results on POS tagging task.\n    ",
        "submission_date": "2017-01-03T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00728",
        "title": "On (Commercial) Benefits of Automatic Text Summarization Systems in the News Domain: A Case of Media Monitoring and Media Response Analysis",
        "authors": [
            "Pashutan Modaresi",
            "Philipp Gross",
            "Siavash Sefidrodi",
            "Mirja Eckhof",
            "Stefan Conrad"
        ],
        "abstract": "In this work, we present the results of a systematic study to investigate the (commercial) benefits of automatic text summarization systems in a real world scenario. More specifically, we define a use case in the context of media monitoring and media response analysis and claim that even using a simple query-based extractive approach can dramatically save the processing time of the employees without significantly reducing the quality of their work.\n    ",
        "submission_date": "2017-01-03T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00798",
        "title": "Fuzzy Based Implicit Sentiment Analysis on Quantitative Sentences",
        "authors": [
            "Amir Hossein Yazdavar",
            "Monireh Ebrahimi",
            "Naomie Salim"
        ],
        "abstract": "With the rapid growth of social media on the web, emotional polarity computation has become a flourishing frontier in the text mining community. However, it is challenging to understand the latest trends and summarize the state or general opinions about products due to the big diversity and size of social media data and this creates the need of automated and real time opinion extraction and mining. On the other hand, the bulk of current research has been devoted to study the subjective sentences which contain opinion keywords and limited work has been reported for objective statements that imply sentiment. In this paper, fuzzy based knowledge engineering model has been developed for sentiment classification of special group of such sentences including the change or deviation from desired range or value. Drug reviews are the rich source of such statements. Therefore, in this research, some experiments were carried out on patient's reviews on several different cholesterol lowering drugs to determine their sentiment polarity. The main conclusion through this study is, in order to increase the accuracy level of existing drug opinion mining systems, objective sentences which imply opinion should be taken into account. Our experimental results demonstrate that our proposed model obtains over 72 percent F1 value.\n    ",
        "submission_date": "2017-01-03T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00851",
        "title": "Unsupervised neural and Bayesian models for zero-resource speech processing",
        "authors": [
            "Herman Kamper"
        ],
        "abstract": "In settings where only unlabelled speech data is available, zero-resource speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. There are two central problems in zero-resource speech processing: (i) finding frame-level feature representations which make it easier to discriminate between linguistic units (phones or words), and (ii) segmenting and clustering unlabelled speech into meaningful units. In this thesis, we argue that a combination of top-down and bottom-up modelling is advantageous in tackling these two problems.\n",
        "submission_date": "2017-01-03T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00874",
        "title": "Neural Probabilistic Model for Non-projective MST Parsing",
        "authors": [
            "Xuezhe Ma",
            "Eduard Hovy"
        ],
        "abstract": "In this paper, we propose a probabilistic parsing model, which defines a proper conditional probability distribution over non-projective dependency trees for a given sentence, using neural representations as inputs. The neural network architecture is based on bi-directional LSTM-CNNs which benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM and CNN. On top of the neural network, we introduce a probabilistic structured layer, defining a conditional log-linear model over non-projective trees. We evaluate our model on 17 different datasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree Theorem (Tutte, 1984), the partition functions and marginals can be computed efficiently, leading to a straight-forward end-to-end model training procedure via back-propagation. Our parser achieves state-of-the-art parsing performance on nine datasets.\n    ",
        "submission_date": "2017-01-04T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00946",
        "title": "Joint Semantic Synthesis and Morphological Analysis of the Derived Word",
        "authors": [
            "Ryan Cotterell",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word's meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituents segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DerivBase (Zeller et al., 2013) data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3% and 5%. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist's notion of morphological productivity.\n    ",
        "submission_date": "2017-01-04T00:00:00",
        "last_modified_date": "2018-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01126",
        "title": "Textual Entailment with Structured Attentions and Composition",
        "authors": [
            "Kai Zhao",
            "Liang Huang",
            "Mingbo Ma"
        ],
        "abstract": "Deep learning techniques are increasingly popular in the textual entailment task, overcoming the fragility of traditional discrete models with hard alignments and logics. In particular, the recently proposed attention models (Rockt\u00e4schel et al., 2015; Wang and Jiang, 2015) achieves state-of-the-art accuracy by computing soft word alignments between the premise and hypothesis sentences. However, there remains a major limitation: this line of work completely ignores syntax and recursion, which is helpful in many traditional efforts. We show that it is beneficial to extend the attention model to tree nodes between premise and hypothesis. More importantly, this subtree-level attention reveals information about entailment relation. We study the recursive composition of this subtree-level entailment relation, which can be viewed as a soft version of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy.\n    ",
        "submission_date": "2017-01-04T00:00:00",
        "last_modified_date": "2017-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01505",
        "title": "Crime Topic Modeling",
        "authors": [
            "Da Kuang",
            "P. Jeffrey Brantingham",
            "Andrea L. Bertozzi"
        ],
        "abstract": "The classification of crime into discrete categories entails a massive loss of information. Crimes emerge out of a complex mix of behaviors and situations, yet most of these details cannot be captured by singular crime type labels. This information loss impacts our ability to not only understand the causes of crime, but also how to develop optimal crime prevention strategies. We apply machine learning methods to short narrative text descriptions accompanying crime records with the goal of discovering ecologically more meaningful latent crime classes. We term these latent classes \"crime topics\" in reference to text-based topic modeling methods that produce them. We use topic distributions to measure clustering among formally recognized crime types. Crime topics replicate broad distinctions between violent and property crime, but also reveal nuances linked to target characteristics, situational conditions and the tools and methods of attack. Formal crime types are not discrete in topic space. Rather, crime types are distributed across a range of crime topics. Similarly, individual crime topics are distributed across a range of formal crime types. Key ecological groups include identity theft, shoplifting, burglary and theft, car crimes and vandalism, criminal threats and confidence crimes, and violent crimes. Though not a replacement for formal legal crime classifications, crime topics provide a unique window into the heterogeneous causal processes underlying crime.\n    ",
        "submission_date": "2017-01-05T00:00:00",
        "last_modified_date": "2018-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01565",
        "title": "Replication issues in syntax-based aspect extraction for opinion mining",
        "authors": [
            "Edison Marrese-Taylor",
            "Yutaka Matsuo"
        ],
        "abstract": "Reproducing experiments is an important instrument to validate previous work and build upon existing approaches. It has been tackled numerous times in different areas of science. In this paper, we introduce an empirical replicability study of three well-known algorithms for syntactic centric aspect-based opinion mining. We show that reproducing results continues to be a difficult endeavor, mainly due to the lack of details regarding preprocessing and parameter setting, as well as due to the absence of available implementations that clarify these details. We consider these are important threats to validity of the research on the field, specifically when compared to other problems in NLP where public datasets and code availability are critical validity components. We conclude by encouraging code-based research, which we think has a key role in helping researchers to understand the meaning of the state-of-the-art better and to generate continuous advances.\n    ",
        "submission_date": "2017-01-06T00:00:00",
        "last_modified_date": "2017-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01574",
        "title": "Real Multi-Sense or Pseudo Multi-Sense: An Approach to Improve Word Representation",
        "authors": [
            "Haoyue Shi",
            "Caihua Li",
            "Junfeng Hu"
        ],
        "abstract": "Previous researches have shown that learning multiple representations for polysemous words can improve the performance of word embeddings on many tasks. However, this leads to another problem. Several vectors of a word may actually point to the same meaning, namely pseudo multi-sense. In this paper, we introduce the concept of pseudo multi-sense, and then propose an algorithm to detect such cases. With the consideration of the detected pseudo multi-sense cases, we try to refine the existing word embeddings to eliminate the influence of pseudo multi-sense. Moreover, we apply our algorithm on previous released multi-sense word embeddings and tested it on artificial word similarity tasks and the analogy task. The result of the experiments shows that diminishing pseudo multi-sense can improve the quality of word representations. Thus, our method is actually an efficient way to reduce linguistic complexity.\n    ",
        "submission_date": "2017-01-06T00:00:00",
        "last_modified_date": "2017-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01614",
        "title": "Enumeration of Extractive Oracle Summaries",
        "authors": [
            "Tsutomu Hirao",
            "Masaaki Nishino",
            "Jun Suzuki",
            "Masaaki Nagata"
        ],
        "abstract": "To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the oracle summaries for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries.\n    ",
        "submission_date": "2017-01-06T00:00:00",
        "last_modified_date": "2017-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01623",
        "title": "Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages",
        "authors": [
            "Michael Sejr Schlichtkrull",
            "Anders S\u00f8gaard"
        ],
        "abstract": "In cross-lingual dependency annotation projection, information is often lost during transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25% averaged across 10 languages compared to the previous state of the art.\n    ",
        "submission_date": "2017-01-06T00:00:00",
        "last_modified_date": "2017-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01811",
        "title": "Structural Attention Neural Networks for improved sentiment analysis",
        "authors": [
            "Filippos Kokkinos",
            "Alexandros Potamianos"
        ],
        "abstract": "We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset.\n    ",
        "submission_date": "2017-01-07T00:00:00",
        "last_modified_date": "2017-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01854",
        "title": "Neural Machine Translation on Scarce-Resource Condition: A case-study on Persian-English",
        "authors": [
            "Mohaddeseh Bastan",
            "Shahram Khadivi",
            "Mohammad Mehdi Homayounpour"
        ],
        "abstract": "Neural Machine Translation (NMT) is a new approach for Machine Translation (MT), and due to its success, it has absorbed the attention of many researchers in the field. In this paper, we study NMT model on Persian-English language pairs, to analyze the model and investigate the appropriateness of the model for scarce-resourced scenarios, the situation that exists for Persian-centered translation systems. We adjust the model for the Persian language and find the best parameters and hyper parameters for two tasks: translation and transliteration. We also apply some preprocessing task on the Persian dataset which yields to increase for about one point in terms of BLEU score. Also, we have modified the loss function to enhance the word alignment of the model. This new loss function yields a total of 1.87 point improvements in terms of BLEU score in the translation quality.\n    ",
        "submission_date": "2017-01-07T00:00:00",
        "last_modified_date": "2017-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01908",
        "title": "Sentence-level dialects identification in the greater China region",
        "authors": [
            "Fan Xu",
            "Mingwen Wang",
            "Maoxi Li"
        ],
        "abstract": "Identifying the different varieties of the same language is more challenging than unrelated languages identification. In this paper, we propose an approach to discriminate language varieties or dialects of Mandarin Chinese for the Mainland China, Hong Kong, Taiwan, Macao, Malaysia and Singapore, a.k.a., the Greater China Region (GCR). When applied to the dialects identification of the GCR, we find that the commonly used character-level or word-level uni-gram feature is not very efficient since there exist several specific problems such as the ambiguity and context-dependent characteristic of words in the dialects of the GCR. To overcome these challenges, we use not only the general features like character-level n-gram, but also many new word-level features, including PMI-based and word alignment-based features. A series of evaluation results on both the news and open-domain dataset from Wikipedia show the effectiveness of the proposed approach.\n    ",
        "submission_date": "2017-01-08T00:00:00",
        "last_modified_date": "2017-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02025",
        "title": "Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities",
        "authors": [
            "Yadollah Yaghoobzadeh",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Entities are essential elements of natural language. In this paper, we present methods for learning multi-level representations of entities on three complementary levels: character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-the-art learning methods on each level and find large differences, e.g., for deep learning models, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level; for word2vec (Mikolov et al., 2013) on the word level; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of representation contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities.\n    ",
        "submission_date": "2017-01-08T00:00:00",
        "last_modified_date": "2017-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02073",
        "title": "Neural Personalized Response Generation as Domain Adaptation",
        "authors": [
            "Weinan Zhang",
            "Ting Liu",
            "Yifa Wang",
            "Qingfu Zhu"
        ],
        "abstract": "In this paper, we focus on the personalized response generation for conversational systems. Based on the sequence to sequence learning, especially the encoder-decoder framework, we propose a two-phase approach, namely initialization then adaptation, to model the responding style of human and then generate personalized responses. For evaluation, we propose a novel human aided method to evaluate the performance of the personalized response generation models by online real-time conversation and offline human judgement. Moreover, the lexical divergence of the responses generated by the 5 personalized models indicates that the proposed two-phase approach achieves good results on modeling the responding style of human and generating personalized responses for the conversational systems.\n    ",
        "submission_date": "2017-01-09T00:00:00",
        "last_modified_date": "2019-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02149",
        "title": "Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "This work studies comparatively two typical sentence matching tasks: textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final classifier. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single framework of alignment without considering the characteristics of specific tasks, which limits the framework's effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach.\n    ",
        "submission_date": "2017-01-09T00:00:00",
        "last_modified_date": "2017-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02185",
        "title": "Crowdsourcing Ground Truth for Medical Relation Extraction",
        "authors": [
            "Anca Dumitrache",
            "Lora Aroyo",
            "Chris Welty"
        ],
        "abstract": "Cognitive computing systems require human labeled data for evaluation, and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, that reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the $cause$ and $treat$ relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task.\n    ",
        "submission_date": "2017-01-09T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02477",
        "title": "Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic Speech Recognition",
        "authors": [
            "Abhinav Thanda",
            "Shankar M Venkatesan"
        ],
        "abstract": "Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\\% relative improvement in WER is reported at -3 SNR dB\n    ",
        "submission_date": "2017-01-10T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02481",
        "title": "Implicitly Incorporating Morphological Information into Word Embedding",
        "authors": [
            "Yang Xu",
            "Jiawei Liu"
        ],
        "abstract": "In this paper, we propose three novel models to enhance word embedding by implicitly using morphological information. Experiments on word similarity and syntactic analogy show that the implicit models are superior to traditional explicit ones. Our models outperform all state-of-the-art baselines and significantly improve the performance on both tasks. Moreover, our performance on the smallest corpus is similar to the performance of CBOW on the corpus which is five times the size of ours. Parameter analysis indicates that the implicit models can supplement semantic information during the word embedding training process.\n    ",
        "submission_date": "2017-01-10T00:00:00",
        "last_modified_date": "2017-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02593",
        "title": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling",
        "authors": [
            "Diego Marcheggiani",
            "Anton Frolov",
            "Ivan Titov"
        ],
        "abstract": "We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves competitive performance on English, even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the English CoNLL-2009 dataset. We also consider Chinese, Czech and Spanish where our approach also achieves competitive results. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e., syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on standard out-of-domain test sets.\n    ",
        "submission_date": "2017-01-10T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02720",
        "title": "Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks",
        "authors": [
            "Ying Zhang",
            "Mohammad Pezeshki",
            "Philemon Brakel",
            "Saizheng Zhang",
            "Cesar Laurent Yoshua Bengio",
            "Aaron Courville"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an end-to-end speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information.\n    ",
        "submission_date": "2017-01-10T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02795",
        "title": "Bidirectional American Sign Language to English Translation",
        "authors": [
            "Hardie Cate",
            "Zeshan Hussain"
        ],
        "abstract": "We outline a bidirectional translation system that converts sentences from American Sign Language (ASL) to English, and vice versa. To perform machine translation between ASL and English, we utilize a generative approach. Specifically, we employ an adjustment to the IBM word-alignment model 1 (IBM WAM1), where we define language models for English and ASL, as well as a translation model, and attempt to generate a translation that maximizes the posterior distribution defined by these models. Then, using these models, we are able to quantify the concepts of fluency and faithfulness of a translation between languages.\n    ",
        "submission_date": "2017-01-10T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02810",
        "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
        "authors": [
            "Guillaume Klein",
            "Yoon Kim",
            "Yuntian Deng",
            "Jean Senellart",
            "Alexander M. Rush"
        ],
        "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.\n    ",
        "submission_date": "2017-01-10T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02854",
        "title": "Towards Decoding as Continuous Optimization in Neural Machine Translation",
        "authors": [
            "Cong Duy Vu Hoang",
            "Gholamreza Haffari",
            "Trevor Cohn"
        ],
        "abstract": "We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We convert decoding - basically a discrete optimization problem - into a continuous optimization problem. The resulting constrained continuous optimisation problem is then tackled using gradient-based methods. Our powerful decoding framework enables decoding intractable models such as the intersection of left-to-right and right-to-left (bidirectional) as well as source-to-target and target-to-source (bilingual) NMT models. Our empirical results show that our decoding framework is effective, and leads to substantial improvements in translations generated from the intersected models where the typical greedy or beam search is not feasible. We also compare our framework against reranking, and analyse its advantages and disadvantages.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02877",
        "title": "Generalisation in Named Entity Recognition: A Quantitative Analysis",
        "authors": [
            "Isabelle Augenstein",
            "Leon Derczynski",
            "Kalina Bontcheva"
        ],
        "abstract": "Named Entity Recognition (NER) is a key NLP task, which is all the more challenging on Web and user-generated content with their diverse and continuously changing language. This paper aims to quantify how this diversity impacts state-of-the-art NER methods, by measuring named entity (NE) and context variability, feature sparsity, and their effects on precision and recall. In particular, our findings indicate that NER approaches struggle to generalise in diverse genres with limited training data. Unseen NEs, in particular, play an important role, which have a higher incidence in diverse genres such as social media than in more regular genres such as newswire. Coupled with a higher incidence of unseen features more generally and the lack of large training corpora, this leads to significantly lower F1 scores for diverse genres as compared to more regular ones. We also find that leading systems rely heavily on surface forms found in training data, having problems generalising beyond these, and offer explanations for this observation.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02901",
        "title": "A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions",
        "authors": [
            "Antonio Toral",
            "V\u00edctor M. S\u00e1nchez-Cartagena"
        ],
        "abstract": "We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02925",
        "title": "Question Analysis for Arabic Question Answering Systems",
        "authors": [
            "Waheeb Ahmed",
            "Dr. Anto P Babu"
        ],
        "abstract": "The first step of processing a question in Question Answering(QA) Systems is to carry out a detailed analysis of the question for the purpose of determining what it is asking for and how to perfectly approach answering it. Our Question analysis uses several techniques to analyze any question given in natural language: a Stanford POS Tagger & parser for Arabic language, a named entity recognizer, tokenizer,Stop-word removal, Question expansion, Question classification and Question focus extraction components. We employ numerous detection rules and trained classifier using features from this analysis to detect important elements of the question, including: 1) the portion of the question that is a referring to the answer (the focus); 2) different terms in the question that identify what type of entity is being asked for (the lexical answer types); 3) Question expansion ; 4) a process of classifying the question into one or more of several and different types; and We describe how these elements are identified and evaluate the effect of accurate detection on our question-answering system using the Mean Reciprocal Rank(MRR) accuracy measure.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02946",
        "title": "Cross-lingual RST Discourse Parsing",
        "authors": [
            "Chlo\u00e9 Braud",
            "Maximin Coavoux",
            "Anders S\u00f8gaard"
        ],
        "abstract": "Discourse parsing is an integral part of understanding information flow and argumentative structure in documents. Most previous research has focused on inducing and evaluating models from the English RST Discourse Treebank. However, discourse treebanks for other languages exist, including Spanish, German, Basque, Dutch and Brazilian Portuguese. The treebanks share the same underlying linguistic theory, but differ slightly in the way documents are annotated. In this paper, we present (a) a new discourse parser which is simpler, yet competitive (significantly better on 2/3 metrics) to state of the art for English, (b) a harmonization of discourse treebanks across languages, enabling us to present (c) what to the best of our knowledge are the first experiments on cross-lingual discourse parsing.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02962",
        "title": "Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network",
        "authors": [
            "Kim Anh Nguyen",
            "Sabine Schulte im Walde",
            "Ngoc Thang Vu"
        ],
        "abstract": "Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from classification experiments show that AntSynNET improves the performance over prior pattern-based methods.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03038",
        "title": "Decoding with Finite-State Transducers on GPUs",
        "authors": [
            "Arturo Argueta",
            "David Chiang"
        ],
        "abstract": "Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, part-of-speech tagging, chunking, named entity recognition, speech recognition, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of NLP. Although researchers have implemented GPU versions of basic graph algorithms, limited previous work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving decoding speedups of up to 5.2x over our serial implementation running on different computer architectures and 6093x over OpenFST.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03079",
        "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems",
        "authors": [
            "Chongyang Tao",
            "Lili Mou",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "abstract": "Open-domain human-computer conversation has been attracting increasing attention over the past few years. However, there does not exist a standard automatic evaluation metric for open-domain dialog systems; researchers usually resort to human annotation for model evaluation, which is time- and labor-intensive. In this paper, we propose RUBER, a Referenced metric and Unreferenced metric Blended Evaluation Routine, which evaluates a reply by taking into consideration both a groundtruth reply and a query (previous user-issued utterance). Our metric is learnable, but its training does not require labels of human satisfaction. Hence, RUBER is flexible and extensible to different datasets and languages. Experiments on both retrieval and generative dialog systems show that RUBER has a high correlation with human annotation.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03092",
        "title": "Job Detection in Twitter",
        "authors": [
            "Besat Kassaie"
        ],
        "abstract": "In this report, we propose a new application for twitter data called \\textit{job detection}. We identify people's job category based on their tweets. As a preliminary work, we limited our task to identify only IT workers from other job holders. We have used and compared both simple bag of words model and a document representation based on Skip-gram model. Our results show that the model based on Skip-gram, achieves a 76\\% precision and 82\\% recall.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03129",
        "title": "De-identification In practice",
        "authors": [
            "Besat Kassaie"
        ],
        "abstract": "We report our effort to identify the sensitive information, subset of data items listed by HIPAA (Health Insurance Portability and Accountability), from medical text using the recent advances in natural language processing and machine learning techniques. We represent the words with high dimensional continuous vectors learned by a variant of Word2Vec called Continous Bag Of Words (CBOW). We feed the word vectors into a simple neural network with a Long Short-Term Memory (LSTM) architecture. Without any attempts to extract manually crafted features and considering that our medical dataset is too small to be fed into neural network, we obtained promising results. The results thrilled us to think about the larger scale of the project with precise parameter tuning and other possible improvements.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03163",
        "title": "Parsing Universal Dependencies without training",
        "authors": [
            "H\u00e9ctor Mart\u00ednez Alonso",
            "\u017deljko Agi\u0107",
            "Barbara Plank",
            "Anders S\u00f8gaard"
        ],
        "abstract": "We propose UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03185",
        "title": "Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models",
        "authors": [
            "Louis Shao",
            "Stephan Gouws",
            "Denny Britz",
            "Anna Goldie",
            "Brian Strope",
            "Ray Kurzweil"
        ],
        "abstract": "Sequence-to-sequence models have been applied to the conversation response generation problem where the source sequence is the conversation history and the target sequence is the response. Unlike translation, conversation responding is inherently creative. The generation of long, informative, coherent, and diverse responses remains a hard task. In this work, we focus on the single turn setting. We add self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03214",
        "title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation",
        "authors": [
            "Chenhui Chu",
            "Raj Dabre",
            "Sadao Kurohashi"
        ],
        "abstract": "In this paper, we propose a novel domain adaptation method named \"mixed fine tuning\" for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings.\n    ",
        "submission_date": "2017-01-12T00:00:00",
        "last_modified_date": "2017-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03227",
        "title": "Prior matters: simple and general methods for evaluating and improving topic quality in topic modeling",
        "authors": [
            "Angela Fan",
            "Finale Doshi-Velez",
            "Luke Miratrix"
        ],
        "abstract": "Latent Dirichlet Allocation (LDA) models trained without stopword removal often produce topics with high posterior probabilities on uninformative words, obscuring the underlying corpus content. Even when canonical stopwords are manually removed, uninformative words common in that corpus will still dominate the most probable words in a topic. In this work, we first show how the standard topic quality measures of coherence and pointwise mutual information act counter-intuitively in the presence of common but irrelevant words, making it difficult to even quantitatively identify situations in which topics may be dominated by stopwords. We propose an additional topic quality metric that targets the stopword problem, and show that it, unlike the standard measures, correctly correlates with human judgements of quality. We also propose a simple-to-implement strategy for generating topics that are evaluated to be of much higher quality by both human assessment and our new metric. This approach, a collection of informative priors easily introduced into most LDA-style inference methods, automatically promotes terms with domain relevance and demotes domain-specific stop words. We demonstrate this approach's effectiveness in three very different domains: Department of Labor accident reports, online health forum posts, and NIPS abstracts. Overall we find that current practices thought to solve this problem do not do so adequately, and that our proposal offers a substantial improvement for those interested in interpreting their topics as objects in their own right.\n    ",
        "submission_date": "2017-01-12T00:00:00",
        "last_modified_date": "2017-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03231",
        "title": "Single-Pass, Adaptive Natural Language Filtering: Measuring Value in User Generated Comments on Large-Scale, Social Media News Forums",
        "authors": [
            "Manuel Amunategui"
        ],
        "abstract": "There are large amounts of insight and social discovery potential in mining crowd-sourced comments left on popular news forums like ",
        "submission_date": "2017-01-12T00:00:00",
        "last_modified_date": "2017-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03329",
        "title": "A Data-Oriented Model of Literary Language",
        "authors": [
            "Andreas van Cranenburgh",
            "Rens Bod"
        ],
        "abstract": "We consider the task of predicting how literary a text is, with a gold standard from human ratings. Aside from a standard bigram baseline, we apply rich syntactic tree fragments, mined from the training set, and a series of hand-picked features. Our model is the first to distinguish degrees of highly and less literary novels using a variety of lexical and syntactic features, and explains 76.0 % of the variation in literary ratings.\n    ",
        "submission_date": "2017-01-12T00:00:00",
        "last_modified_date": "2017-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03338",
        "title": "LanideNN: Multilingual Language Identification on Character Window",
        "authors": [
            "Tom Kocmi",
            "Ond\u0159ej Bojar"
        ],
        "abstract": "In language identification, a common first step in natural language processing, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on Bidirectional Recurrent Neural Networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the accuracy also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data.\n    ",
        "submission_date": "2017-01-12T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03434",
        "title": "SMARTies: Sentiment Models for Arabic Target Entities",
        "authors": [
            "Noura Farra",
            "Kathleen McKeown"
        ],
        "abstract": "We consider entity-level sentiment analysis in Arabic, a morphologically rich language with increasing resources. We present a system that is applied to complex posts written in response to Arabic newspaper articles. Our goal is to identify important entity \"targets\" within the post along with the polarity expressed about each target. We achieve significant improvements over multiple baselines, demonstrating that the use of specific morphological representations improves the performance of identifying both important targets and their sentiment, and that the use of distributional semantic clusters further boosts performances for these representations, especially when richer linguistic resources are not available.\n    ",
        "submission_date": "2017-01-12T00:00:00",
        "last_modified_date": "2017-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03492",
        "title": "Scalable, Trie-based Approximate Entity Extraction for Real-Time Financial Transaction Screening",
        "authors": [
            "Emrah Budur"
        ],
        "abstract": "Financial institutions have to screen their transactions to ensure that they are not affiliated with terrorism entities. Developing appropriate solutions to detect such affiliations precisely while avoiding any kind of interruption to large amount of legitimate transactions is essential. In this paper, we present building blocks of a scalable solution that may help financial institutions to build their own software to extract terrorism entities out of both structured and unstructured financial messages in real time and with approximate similarity matching approach.\n    ",
        "submission_date": "2017-01-12T00:00:00",
        "last_modified_date": "2017-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03578",
        "title": "Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network",
        "authors": [
            "Seunghyun Yoon",
            "Hyeongu Yun",
            "Yuna Kim",
            "Gyu-tae Park",
            "Kyomin Jung"
        ],
        "abstract": "In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects.\n    ",
        "submission_date": "2017-01-13T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03682",
        "title": "LIDE: Language Identification from Text Documents",
        "authors": [
            "Priyank Mathur",
            "Arkajyoti Misra",
            "Emrah Budur"
        ],
        "abstract": "The increase in the use of microblogging came along with the rapid growth on short linguistic data. On the other hand deep learning is considered to be the new frontier to extract meaningful information out of large amount of raw data in an automated manner. In this study, we engaged these two emerging fields to come up with a robust language identifier on demand, namely Language Identification Engine (LIDE). As a result, we achieved 95.12% accuracy in Discriminating between Similar Languages (DSL) Shared Task 2015 dataset, which is comparable to the maximum reported accuracy of 95.54% achieved so far.\n    ",
        "submission_date": "2017-01-13T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03849",
        "title": "Deep Neural Networks for Czech Multi-label Document Classification",
        "authors": [
            "Ladislav Lenc",
            "Pavel Kr\u00e1l"
        ],
        "abstract": "This paper is focused on automatic multi-label document classification of Czech text documents. The current approaches usually use some pre-processing which can have negative impact (loss of information, additional implementation work, etc). Therefore, we would like to omit it and use deep neural networks that learn from simple features. This choice was motivated by their successful usage in many other machine learning fields. Two different networks are compared: the first one is a standard multi-layer perceptron, while the second one is a popular convolutional network. The experiments on a Czech newspaper corpus show that both networks significantly outperform baseline method which uses a rich set of features with maximum entropy classifier. We have also shown that convolutional network gives the best results.\n    ",
        "submission_date": "2017-01-13T00:00:00",
        "last_modified_date": "2020-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03924",
        "title": "QCRI Machine Translation Systems for IWSLT 16",
        "authors": [
            "Nadir Durrani",
            "Fahim Dalvi",
            "Hassan Sajjad",
            "Stephan Vogel"
        ],
        "abstract": "This paper describes QCRI's machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic->English and English->Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic->English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs.\n    ",
        "submission_date": "2017-01-14T00:00:00",
        "last_modified_date": "2017-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04024",
        "title": "A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue",
        "authors": [
            "Mihail Eric",
            "Christopher D. Manning"
        ],
        "abstract": "Task-oriented dialogue focuses on conversational agents that participate in user-initiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an explicit representation by depending on a latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. We complement recent work by showing the effectiveness of simple sequence-to-sequence neural architectures with a copy mechanism. Our model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on DSTC2.\n    ",
        "submission_date": "2017-01-15T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04027",
        "title": "Neural Models for Sequence Chunking",
        "authors": [
            "Feifei Zhai",
            "Saloni Potdar",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-Outside-Beginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks.\n    ",
        "submission_date": "2017-01-15T00:00:00",
        "last_modified_date": "2017-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04056",
        "title": "Dialog Context Language Modeling with Recurrent Neural Networks",
        "authors": [
            "Bing Liu",
            "Ian Lane"
        ],
        "abstract": "In this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.\n    ",
        "submission_date": "2017-01-15T00:00:00",
        "last_modified_date": "2017-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04189",
        "title": "Deep Memory Networks for Attitude Identification",
        "authors": [
            "Cheng Li",
            "Xiaoxiao Guo",
            "Qiaozhu Mei"
        ],
        "abstract": "We consider the task of identifying attitudes towards a given set of entities from text. Conventionally, this task is decomposed into two separate subtasks: target detection that identifies whether each entity is mentioned in the text, either explicitly or implicitly, and polarity classification that classifies the exact sentiment towards an identified entity (the target) into positive, negative, or neutral.\n",
        "submission_date": "2017-01-16T00:00:00",
        "last_modified_date": "2017-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04290",
        "title": "Machine Translation Approaches and Survey for Indian Languages",
        "authors": [
            "Nadeem Jadoon Khan",
            "Waqas Anwar",
            "Nadir Durrani"
        ],
        "abstract": "In this study, we present an analysis regarding the performance of the state-of-art Phrase-based Statistical Machine Translation (SMT) on multiple Indian languages. We report baseline systems on several language pairs. The motivation of this study is to promote the development of SMT and linguistic resources for these language pairs, as the current state-of-the-art is quite bleak due to sparse data resources. The success of an SMT system is contingent on the availability of a large parallel corpus. Such data is necessary to reliably estimate translation probabilities. We report the performance of baseline systems translating from Indian languages (Bengali, Guajarati, Hindi, Malayalam, Punjabi, Tamil, Telugu and Urdu) into English with average 10% accurate results for all the language pairs.\n    ",
        "submission_date": "2017-01-16T00:00:00",
        "last_modified_date": "2017-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04313",
        "title": "End-to-End ASR-free Keyword Search from Speech",
        "authors": [
            "Kartik Audhkhasi",
            "Andrew Rosenberg",
            "Abhinav Sethy",
            "Bhuvana Ramabhadran",
            "Brian Kingsbury"
        ],
        "abstract": "End-to-end (E2E) systems have achieved competitive results compared to conventional hybrid hidden Markov model (HMM)-deep neural network based automatic speech recognition (ASR) systems. Such E2E systems are attractive due to the lack of dependence on alignments between input acoustic and output grapheme or HMM state sequence during training. This paper explores the design of an ASR-free end-to-end system for text query-based keyword search (KWS) from speech trained with minimal supervision. Our E2E KWS system consists of three sub-systems. The first sub-system is a recurrent neural network (RNN)-based acoustic auto-encoder trained to reconstruct the audio through a finite-dimensional representation. The second sub-system is a character-level RNN language model using embeddings learned from a convolutional neural network. Since the acoustic and text query embeddings occupy different representation spaces, they are input to a third feed-forward neural network that predicts whether the query occurs in the acoustic utterance or not. This E2E ASR-free KWS system performs respectably despite lacking a conventional ASR system and trains much faster.\n    ",
        "submission_date": "2017-01-13T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04653",
        "title": "Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods",
        "authors": [
            "Marzieh Saeidi",
            "Alessandro Venerandi",
            "Licia Capra",
            "Sebastian Riedel"
        ],
        "abstract": "In this paper, we investigate whether text from a Community Question Answering (QA) platform can be used to predict and describe real-world attributes. We experiment with predicting a wide range of 62 demographic attributes for neighbourhoods of London. We use the text from QA platform of Yahoo! Answers and compare our results to the ones obtained from Twitter microblogs. Outcomes show that the correlation between the predicted demographic attributes using text from Yahoo! Answers discussions and the observed demographic attributes can reach an average Pearson correlation coefficient of \\r{ho} = 0.54, slightly higher than the predictions obtained using Twitter data. Our qualitative analysis indicates that there is semantic relatedness between the highest correlated terms extracted from both datasets and their relative demographic attributes. Furthermore, the correlations highlight the different natures of the information contained in Yahoo! Answers and Twitter. While the former seems to offer a more encyclopedic content, the latter provides information related to the current sociocultural aspects or phenomena.\n    ",
        "submission_date": "2017-01-17T00:00:00",
        "last_modified_date": "2017-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.05011",
        "title": "Assessing User Expertise in Spoken Dialog System Interactions",
        "authors": [
            "Eug\u00e9nio Ribeiro",
            "Fernando Batista",
            "Isabel Trancoso",
            "Jos\u00e9 Lopes",
            "Ricardo Ribeiro",
            "David Martins de Matos"
        ],
        "abstract": "Identifying the level of expertise of its users is important for a system since it can lead to a better interaction through adaptation techniques. Furthermore, this information can be used in offline processes of root cause analysis. However, not much effort has been put into automatically identifying the level of expertise of an user, especially in dialog-based interactions. In this paper we present an approach based on a specific set of task related features. Based on the distribution of the features among the two classes - Novice and Expert - we used Random Forests as a classification approach. Furthermore, we used a Support Vector Machine classifier, in order to perform a result comparison. By applying these approaches on data from a real system, Let's Go, we obtained preliminary results that we consider positive, given the difficulty of the task and the lack of competing approaches for comparison.\n    ",
        "submission_date": "2017-01-18T00:00:00",
        "last_modified_date": "2017-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.05343",
        "title": "A Joint Framework for Argumentative Text Analysis Incorporating Domain Knowledge",
        "authors": [
            "Zhongyu Wei",
            "Chen Li",
            "Yang Liu"
        ],
        "abstract": "For argumentation mining, there are several sub-tasks such as argumentation component type classification, relation classification. Existing research tends to solve such sub-tasks separately, but ignore the close relation between them. In this paper, we present a joint framework incorporating logical relation between sub-tasks to improve the performance of argumentation structure generation. We design an objective function to combine the predictions from individual models for each sub-task and solve the problem with some constraints constructed from background knowledge. We evaluate our proposed model on two public corpora and the experiment results show that our model can outperform the baseline that uses a separate model significantly for each sub-task. Our model also shows advantages on component-related sub-tasks compared to a state-of-the-art joint model based on the evidence graph.\n    ",
        "submission_date": "2017-01-19T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.05574",
        "title": "Harnessing Cognitive Features for Sarcasm Detection",
        "authors": [
            "Abhijit Mishra",
            "Diptesh Kanojia",
            "Seema Nagar",
            "Kuntal Dey",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "In this paper, we propose a novel mechanism for enriching the feature vector, for the task of sarcasm detection, with cognitive features extracted from eye-movement patterns of human readers. Sarcasm detection has been a challenging research problem, and its importance for NLP applications such as review summarization, dialog systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the behaviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by his observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of F-score), over the performance of the best reported system.\n    ",
        "submission_date": "2017-01-19T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.05581",
        "title": "Leveraging Cognitive Features for Sentiment Analysis",
        "authors": [
            "Abhijit Mishra",
            "Diptesh Kanojia",
            "Seema Nagar",
            "Kuntal Dey",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers. Statistical classification using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features. We perform feature significance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment analyzers to handle complex constructs.\n    ",
        "submission_date": "2017-01-19T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.05625",
        "title": "CEVO: Comprehensive EVent Ontology Enhancing Cognitive Annotation",
        "authors": [
            "Saeedeh Shekarpour",
            "Faisal Alshargi",
            "Valerie Shalin",
            "Krishnaprasad Thirunarayan",
            "Amit P. Sheth"
        ],
        "abstract": "While the general analysis of named entities has received substantial research attention on unstructured as well as structured data, the analysis of relations among named entities has received limited focus. In fact, a review of the literature revealed a deficiency in research on the abstract conceptualization required to organize relations. We believe that such an abstract conceptualization can benefit various communities and applications such as natural language processing, information extraction, machine learning, and ontology engineering. In this paper, we present Comprehensive EVent Ontology (CEVO), built on Levin's conceptual hierarchy of English verbs that categorizes verbs with shared meaning, and syntactic behavior. We present the fundamental concepts and requirements for this ontology. Furthermore, we present three use cases employing the CEVO ontology on annotation tasks: (i) annotating relations in plain text, (ii) annotating ontological properties, and (iii) linking textual relations to ontological properties. These use-cases demonstrate the benefits of using CEVO for annotation: (i) annotating English verbs from an abstract conceptualization, (ii) playing the role of an upper ontology for organizing ontological properties, and (iii) facilitating the annotation of text relations using any underlying vocabulary. This resource is available at ",
        "submission_date": "2017-01-19T00:00:00",
        "last_modified_date": "2018-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.06247",
        "title": "A Multichannel Convolutional Neural Network For Cross-language Dialog State Tracking",
        "authors": [
            "Hongjie Shi",
            "Takashi Ushio",
            "Mitsuru Endo",
            "Katsuyoshi Yamagami",
            "Noriaki Horii"
        ],
        "abstract": "The fifth Dialog State Tracking Challenge (DSTC5) introduces a new cross-language dialog state tracking scenario, where the participants are asked to build their trackers based on the English training corpus, while evaluating them with the unlabeled Chinese corpus. Although the computer-generated translations for both English and Chinese corpus are provided in the dataset, these translations contain errors and careless use of them can easily hurt the performance of the built trackers. To address this problem, we propose a multichannel Convolutional Neural Networks (CNN) architecture, in which we treat English and Chinese language as different input channels of one single CNN model. In the evaluation of DSTC5, we found that such multichannel architecture can effectively improve the robustness against translation errors. Additionally, our method for DSTC5 is purely machine learning based and requires no prior knowledge about the target language. We consider this a desirable property for building a tracker in the cross-language context, as not every developer will be familiar with both languages.\n    ",
        "submission_date": "2017-01-23T00:00:00",
        "last_modified_date": "2017-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.06521",
        "title": "Incorporating Global Visual Features into Attention-Based Neural Machine Translation",
        "authors": [
            "Iacer Calixto",
            "Qun Liu",
            "Nick Campbell"
        ],
        "abstract": "We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. We utilise global image features extracted using a pre-trained convolutional neural network and incorporate them (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate how these different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.\n    ",
        "submission_date": "2017-01-23T00:00:00",
        "last_modified_date": "2017-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.06547",
        "title": "Adversarial Learning for Neural Dialogue Generation",
        "authors": [
            "Jiwei Li",
            "Will Monroe",
            "Tianlin Shi",
            "S\u00e9bastien Jean",
            "Alan Ritter",
            "Dan Jurafsky"
        ],
        "abstract": "In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.\n",
        "submission_date": "2017-01-23T00:00:00",
        "last_modified_date": "2017-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.06549",
        "title": "Learning to Decode for Future Success",
        "authors": [
            "Jiwei Li",
            "Will Monroe",
            "Dan Jurafsky"
        ],
        "abstract": "We introduce a simple, general strategy to manipulate the behavior of a neural decoder that enables it to generate outputs that have specific properties of interest (e.g., sequences of a pre-specified length). The model can be thought of as a simple version of the actor-critic model that uses an interpolation of the actor (the MLE-based token generation policy) and the critic (a value function that estimates the future values of the desired property) for decision making. We demonstrate that the approach is able to incorporate a variety of properties that cannot be handled by standard neural sequence decoders, such as sequence length and backward probability (probability of sources given targets), in addition to yielding consistent improvements in abstractive summarization and machine translation when the property to be optimized is BLEU or ROUGE scores.\n    ",
        "submission_date": "2017-01-23T00:00:00",
        "last_modified_date": "2017-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.07149",
        "title": "Hierarchical Recurrent Attention Network for Response Generation",
        "authors": [
            "Chen Xing",
            "Wei Wu",
            "Yu Wu",
            "Ming Zhou",
            "Yalou Huang",
            "Wei-Ying Ma"
        ],
        "abstract": "We study multi-turn response generation in chatbots where a response is generated according to a conversation context. Existing work has modeled the hierarchy of the context, but does not pay enough attention to the fact that words and utterances in the context are differentially important. As a result, they may lose important information in context and generate irrelevant responses. We propose a hierarchical recurrent attention network (HRAN) to model both aspects in a unified framework. In HRAN, a hierarchical attention mechanism attends to important parts within and among utterances with word level attention and utterance level attention respectively. With the word level attention, hidden vectors of a word level encoder are synthesized as utterance vectors and fed to an utterance level encoder to construct hidden representations of the context. The hidden vectors of the context are then processed by the utterance level attention and formed as context vectors for decoding the response. Empirical studies on both automatic evaluation and human judgment show that HRAN can significantly outperform state-of-the-art models for multi-turn response generation.\n    ",
        "submission_date": "2017-01-25T00:00:00",
        "last_modified_date": "2017-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.07481",
        "title": "Learning Word-Like Units from Joint Audio-Visual Analysis",
        "authors": [
            "David Harwath",
            "James R. Glass"
        ],
        "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word 'lighthouse' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.\n    ",
        "submission_date": "2017-01-25T00:00:00",
        "last_modified_date": "2017-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.07880",
        "title": "emLam -- a Hungarian Language Modeling baseline",
        "authors": [
            "D\u00e1vid M\u00e1rk Nemeskey"
        ],
        "abstract": "This paper aims to make up for the lack of documented baselines for Hungarian language modeling. Various approaches are evaluated on three publicly available Hungarian corpora. Perplexity values comparable to models of similar-sized English corpora are reported. A new, freely downloadable Hungar- ian benchmark corpus is introduced.\n    ",
        "submission_date": "2017-01-26T00:00:00",
        "last_modified_date": "2017-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08071",
        "title": "Emotion Recognition From Speech With Recurrent Neural Networks",
        "authors": [
            "Vladimir Chernykh",
            "Pavel Prikhodko"
        ],
        "abstract": "In this paper the task of emotion recognition from speech is considered. Proposed approach uses deep recurrent neural network trained on a sequence of acoustic features calculated over small speech intervals. At the same time special probabilistic-nature CTC loss function allows to consider long utterances containing both emotional and neutral parts. The effectiveness of such an approach is shown in two ways. Firstly, the comparison with recent advances in this field is carried out. Secondly, human performance on the same task is measured. Both criteria show the high quality of the proposed method.\n    ",
        "submission_date": "2017-01-27T00:00:00",
        "last_modified_date": "2018-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08118",
        "title": "Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis",
        "authors": [
            "Bj\u00f6rn Ross",
            "Michael Rist",
            "Guillermo Carbonell",
            "Benjamin Cabrera",
            "Nils Kurowsky",
            "Michael Wojatzki"
        ],
        "abstract": "Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.\n    ",
        "submission_date": "2017-01-27T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08198",
        "title": "Adversarial Evaluation of Dialogue Models",
        "authors": [
            "Anjuli Kannan",
            "Oriol Vinyals"
        ],
        "abstract": "The recent application of RNN encoder-decoder models has resulted in substantial progress in fully data-driven dialogue systems, but evaluation remains a challenge. An adversarial loss could be a way to directly evaluate the extent to which generated dialogue responses sound like they came from a human. This could reduce the need for human evaluation, while more directly evaluating on a generative task. In this work, we investigate this idea by training an RNN to discriminate a dialogue model's samples from human-generated samples. Although we find some evidence this setup could be viable, we also note that many issues remain in its practical application. We discuss both aspects and conclude that future work is warranted.\n    ",
        "submission_date": "2017-01-27T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08251",
        "title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation",
        "authors": [
            "Nasrin Mostafazadeh",
            "Chris Brockett",
            "Bill Dolan",
            "Michel Galley",
            "Jianfeng Gao",
            "Georgios P. Spithourakis",
            "Lucy Vanderwende"
        ],
        "abstract": "The popularity of image sharing on social media and the engagement it creates between users reflects the important role that visual context plays in everyday conversations. We present a novel task, Image-Grounded Conversations (IGC), in which natural-sounding conversations are generated about a shared image. To benchmark progress, we introduce a new multiple-reference dataset of crowd-sourced, event-centric conversations on images. IGC falls on the continuum between chit-chat and goal-directed conversation models, where visual grounding constrains the topic of conversation to event-driven utterances. Experiments with models trained on social media data show that the combination of visual and textual context enhances the quality of generated conversational turns. In human evaluation, the gap between human performance and that of both neural and retrieval architectures suggests that multi-modal IGC presents an interesting challenge for dialogue research.\n    ",
        "submission_date": "2017-01-28T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08303",
        "title": "Drug-Drug Interaction Extraction from Biomedical Text Using Long Short Term Memory Network",
        "authors": [
            "Sunil Kumar Sahu",
            "Ashish Anand"
        ],
        "abstract": "Simultaneous administration of multiple drugs can have synergistic or antagonistic effects as one drug can affect activities of other drugs. Synergistic effects lead to improved therapeutic outcomes, whereas, antagonistic effects can be life-threatening, may lead to increased healthcare cost, or may even cause death. Thus identification of unknown drug-drug interaction (DDI) is an important concern for efficient and effective healthcare. Although multiple resources for DDI exist, they are often unable to keep pace with rich amount of information available in fast growing biomedical texts. Most existing methods model DDI extraction from text as a classification problem and mainly rely on handcrafted features. Some of these features further depend on domain specific tools. Recently neural network models using latent features have been shown to give similar or better performance than the other existing models dependent on handcrafted features. In this paper, we present three models namely, {\\it B-LSTM}, {\\it AB-LSTM} and {\\it Joint AB-LSTM} based on long short-term memory (LSTM) network. All three models utilize word and position embedding as latent features and thus do not rely on explicit feature engineering. Further use of bidirectional long short-term memory (Bi-LSTM) networks allow implicit feature extraction from the whole sentence. The two models, {\\it AB-LSTM} and {\\it Joint AB-LSTM} also use attentive pooling in the output of Bi-LSTM layer to assign weights to features. Our experimental results on the SemEval-2013 DDI extraction dataset show that the {\\it Joint AB-LSTM} model outperforms all the existing methods, including those relying on handcrafted features. The other two proposed LSTM models also perform competitively with state-of-the-art methods.\n    ",
        "submission_date": "2017-01-28T00:00:00",
        "last_modified_date": "2017-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08339",
        "title": "Using English as Pivot to Extract Persian-Italian Parallel Sentences from Non-Parallel Corpora",
        "authors": [
            "Ebrahim Ansari",
            "M.H. Sadreddini",
            "Mostafa Sheikhalishahi",
            "Richard Wallace",
            "Fatemeh Alimardani"
        ],
        "abstract": "The effectiveness of a statistical machine translation system (SMT) is very dependent upon the amount of parallel corpus used in the training phase. For low-resource language pairs there are not enough parallel corpora to build an accurate SMT. In this paper, a novel approach is presented to extract bilingual Persian-Italian parallel sentences from a non-parallel (comparable) corpus. In this study, English is used as the pivot language to compute the matching scores between source and target sentences and candidate selection phase. Additionally, a new monolingual sentence similarity metric, Normalized Google Distance (NGD) is proposed to improve the matching process. Moreover, some extensions of the baseline system are applied to improve the quality of extracted sentences measured with BLEU. Experimental results show that using the new pivot based extraction can increase the quality of bilingual corpus significantly and consequently improves the performance of the Persian-Italian SMT system.\n    ",
        "submission_date": "2017-01-29T00:00:00",
        "last_modified_date": "2017-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08340",
        "title": "Extracting Bilingual Persian Italian Lexicon from Comparable Corpora Using Different Types of Seed Dictionaries",
        "authors": [
            "Ebrahim Ansari",
            "M.H. Sadreddini",
            "Lucio Grandinetti",
            "Mahsa Radinmehr",
            "Ziba Khosravan",
            "Mehdi Sheikhalishahi"
        ],
        "abstract": "Bilingual dictionaries are very important in various fields of natural language processing. In recent years, research on extracting new bilingual lexicons from non-parallel (comparable) corpora have been proposed. Almost all use a small existing dictionary or other resources to make an initial list called the \"seed dictionary\". In this paper, we discuss the use of different types of dictionaries as the initial starting list for creating a bilingual Persian-Italian lexicon from a comparable corpus. Our experiments apply state-of-the-art techniques on three different seed dictionaries; an existing dictionary, a dictionary created with pivot-based schema, and a dictionary extracted from a small Persian-Italian parallel text. The interesting challenge of our approach is to find a way to combine different dictionaries together in order to produce a better and more accurate lexicon. In order to combine seed dictionaries, we propose two different combination models and examine the effect of our novel combination models on various comparable corpora that have differing degrees of comparability. We conclude with a proposal for a new weighting system to improve the extracted lexicon. The experimental results produced by our implementation show the efficiency of our proposed models.\n    ",
        "submission_date": "2017-01-29T00:00:00",
        "last_modified_date": "2019-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08533",
        "title": "Graph-Based Semi-Supervised Conditional Random Fields For Spoken Language Understanding Using Unaligned Data",
        "authors": [
            "Mohammad Aliannejadi",
            "Masoud Kiaeeha",
            "Shahram Khadivi",
            "Saeed Shiry Ghidary"
        ],
        "abstract": "We experiment graph-based Semi-Supervised Learning (SSL) of Conditional Random Fields (CRF) for the application of Spoken Language Understanding (SLU) on unaligned data. The aligned labels for examples are obtained using IBM Model. We adapt a baseline semi-supervised CRF by defining new feature set and altering the label propagation algorithm. Our results demonstrate that our proposed approach significantly improves the performance of the supervised model by utilizing the knowledge gained from the graph.\n    ",
        "submission_date": "2017-01-30T00:00:00",
        "last_modified_date": "2017-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08655",
        "title": "Structural Analysis of Hindi Phonetics and A Method for Extraction of Phonetically Rich Sentences from a Very Large Hindi Text Corpus",
        "authors": [
            "Shrikant Malviya",
            "Rohit Mishra",
            "Uma Shanker Tiwary"
        ],
        "abstract": "Automatic speech recognition (ASR) and Text to speech (TTS) are two prominent area of research in human computer interaction nowadays. A set of phonetically rich sentences is in a matter of importance in order to develop these two interactive modules of HCI. Essentially, the set of phonetically rich sentences has to cover all possible phone units distributed uniformly. Selecting such a set from a big corpus with maintaining phonetic characteristic based similarity is still a challenging problem. The major objective of this paper is to devise a criteria in order to select a set of sentences encompassing all phonetic aspects of a corpus with size as minimum as possible. First, this paper presents a statistical analysis of Hindi phonetics by observing the structural characteristics. Further a two stage algorithm is proposed to extract phonetically rich sentences with a high variety of triphones from the EMILLE Hindi corpus. The algorithm consists of a distance measuring criteria to select a sentence in order to improve the triphone distribution. Moreover, a special preprocessing method is proposed to score each triphone in terms of inverse probability in order to fasten the algorithm. The results show that the approach efficiently build uniformly distributed phonetically-rich corpus with optimum number of sentences.\n    ",
        "submission_date": "2017-01-30T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08694",
        "title": "A Comparative Study on Different Types of Approaches to Bengali document Categorization",
        "authors": [
            "Md. Saiful Islam",
            "Fazla Elahi Md Jubayer",
            "Syed Ikhtiar Ahmed"
        ],
        "abstract": "Document categorization is a technique where the category of a document is determined. In this paper three well-known supervised learning techniques which are Support Vector Machine(SVM), Na\u00efve Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article.\n    ",
        "submission_date": "2017-01-27T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08702",
        "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language Model",
        "authors": [
            "Dipaloke Saha",
            "Md Saddam Hossain",
            "MD. Saiful Islam",
            "Sabir Ismail"
        ],
        "abstract": "In this paper, we describe a research method that generates Bangla word clusters on the basis of relating to meaning in language and contextual similarity. The importance of word clustering is in parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. In the history of word clustering, English and some other languages have already implemented some methods on word clustering efficiently. But due to lack of the resources, word clustering in Bangla has not been still implemented efficiently. Presently, its implementation is in the beginning stage. In some research of word clustering in English based on preceding and next five words of a key word they found an efficient result. Now, we are trying to implement the tri-gram, 4-gram and 5-gram model of word clustering for Bangla to observe which one is the best among them. We have started our research with quite a large corpus of approximate 1 lakh Bangla words. We are using a machine learning technique in this research. We will generate word clusters and analyze the clusters by testing some different threshold values.\n    ",
        "submission_date": "2017-01-27T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08711",
        "title": "Predicting Auction Price of Vehicle License Plate with Deep Recurrent Neural Network",
        "authors": [
            "Vinci Chow"
        ],
        "abstract": "In Chinese societies, superstition is of paramount importance, and vehicle license plates with desirable numbers can fetch very high prices in auctions. Unlike other valuable items, license plates are not allocated an estimated price before auction. I propose that the task of predicting plate prices can be viewed as a natural language processing (NLP) task, as the value depends on the meaning of each individual character on the plate and its semantics. I construct a deep recurrent neural network (RNN) to predict the prices of vehicle license plates in Hong Kong, based on the characters on a plate. I demonstrate the importance of having a deep network and of retraining. Evaluated on 13 years of historical auction prices, the deep RNN's predictions can explain over 80 percent of price variations, outperforming previous models by a significant margin. I also demonstrate how the model can be extended to become a search engine for plates and to provide estimates of the expected price distribution.\n    ",
        "submission_date": "2017-01-30T00:00:00",
        "last_modified_date": "2019-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.09123",
        "title": "Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features",
        "authors": [
            "Rodrigo Agerri",
            "German Rigau"
        ],
        "abstract": "We present a multilingual Named Entity Recognition approach based on a robust and general set of features across languages and datasets. Our system combines shallow local information with clustering semi-supervised features induced on large amounts of unlabeled text. Understanding via empirical experimentation how to effectively combine various types of clustering features allows us to seamlessly export our system to other datasets and languages. The result is a simple but highly competitive system which obtains state of the art results across five languages and twelve datasets. The results are reported on standard shared task evaluation data such as CoNLL for English, Spanish and Dutch. Furthermore, and despite the lack of linguistically motivated features, we also report best results for languages such as Basque and German. In addition, we demonstrate that our method also obtains very competitive results even when the amount of supervised data is cut by half, alleviating the dependency on manually annotated data. Finally, the results show that our emphasis on clustering features is crucial to develop robust out-of-domain models. The system and models are freely available to facilitate its use and guarantee the reproducibility of results.\n    ",
        "submission_date": "2017-01-31T00:00:00",
        "last_modified_date": "2017-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00167",
        "title": "SMPOST: Parts of Speech Tagger for Code-Mixed Indic Social Media Text",
        "authors": [
            "Deepak Gupta",
            "Shubham Tripathi",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Use of social media has grown dramatically during the last few years. Users follow informal languages in communicating through social media. The language of communication is often mixed in nature, where people transcribe their regional language with English and this technique is found to be extremely popular. Natural language processing (NLP) aims to infer the information from these text where Part-of-Speech (PoS) tagging plays an important role in getting the prosody of the written text. For the task of PoS tagging on Code-Mixed Indian Social Media Text, we develop a supervised system based on Conditional Random Field classifier. In order to tackle the problem effectively, we have focused on extracting rich linguistic features. We participate in three different language pairs, ie. English-Hindi, English-Bengali and English-Telugu on three different social media platforms, Twitter, Facebook & WhatsApp. The proposed system is able to successfully assign coarse as well as fine-grained PoS tag labels for a given a code-mixed sentence. Experiments show that our system is quite generic that shows encouraging performance levels on all the three language pairs in all the domains.\n    ",
        "submission_date": "2017-02-01T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00500",
        "title": "AMR-to-text Generation with Synchronous Node Replacement Grammar",
        "authors": [
            "Linfeng Song",
            "Xiaochang Peng",
            "Yue Zhang",
            "Zhiguo Wang",
            "Daniel Gildea"
        ],
        "abstract": "This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far.\n    ",
        "submission_date": "2017-02-01T00:00:00",
        "last_modified_date": "2017-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00700",
        "title": "Multilingual and Cross-lingual Timeline Extraction",
        "authors": [
            "Egoitz Laparra",
            "Rodrigo Agerri",
            "Itziar Aldabe",
            "German Rigau"
        ],
        "abstract": "In this paper we present an approach to extract ordered timelines of events, their participants, locations and times from a set of multilingual and cross-lingual data sources. Based on the assumption that event-related information can be recovered from different documents written in different languages, we extend the Cross-document Event Ordering task presented at SemEval 2015 by specifying two new tasks for, respectively, Multilingual and Cross-lingual Timeline Extraction. We then develop three deterministic algorithms for timeline extraction based on two main ideas. First, we address implicit temporal relations at document level since explicit time-anchors are too scarce to build a wide coverage timeline extraction system. Second, we leverage several multilingual resources to obtain a single, inter-operable, semantic representation of events across documents and across languages. The result is a highly competitive system that strongly outperforms the current state-of-the-art. Nonetheless, further analysis of the results reveals that linking the event mentions with their target entities and time-anchors remains a difficult challenge. The systems, resources and scorers are freely available to facilitate its use and guarantee the reproducibility of results.\n    ",
        "submission_date": "2017-02-02T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00716",
        "title": "Analysing Temporal Evolution of Interlingual Wikipedia Article Pairs",
        "authors": [
            "Simon Gottschalk",
            "Elena Demidova"
        ],
        "abstract": "Wikipedia articles representing an entity or a topic in different language editions evolve independently within the scope of the language-specific user communities. This can lead to different points of views reflected in the articles, as well as complementary and inconsistent information. An analysis of how the information is propagated across the Wikipedia language editions can provide important insights in the article evolution along the temporal and cultural dimensions and support quality control. To facilitate such analysis, we present MultiWiki - a novel web-based user interface that provides an overview of the similarities and differences across the article pairs originating from different language editions on a timeline. MultiWiki enables users to observe the changes in the interlingual article similarity over time and to perform a detailed visual comparison of the article snapshots at a particular time point.\n    ",
        "submission_date": "2017-02-02T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00764",
        "title": "Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey",
        "authors": [
            "Lorenzo Ferrone",
            "Fabio Massimo Zanzotto"
        ],
        "abstract": "Natural language is inherently a discrete symbolic representation of human knowledge. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: discrete symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and discrete symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols may certainly lead to radically new deep learning networks. In this paper we make a survey that aims to renew the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how discrete symbols are represented inside neural networks.\n    ",
        "submission_date": "2017-02-02T00:00:00",
        "last_modified_date": "2019-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00860",
        "title": "Topic Modeling the H\u00e0n di\u0103n Ancient Classics",
        "authors": [
            "Colin Allen",
            "Hongliang Luo",
            "Jaimie Murdock",
            "Jianghuai Pu",
            "Xiaohong Wang",
            "Yanjie Zhai",
            "Kun Zhao"
        ],
        "abstract": "Ancient Chinese texts present an area of enormous challenge and opportunity for humanities scholars interested in exploiting computational methods to assist in the development of new insights and interpretations of culturally significant materials. In this paper we describe a collaborative effort between Indiana University and Xi'an Jiaotong University to support exploration and interpretation of a digital corpus of over 18,000 ancient Chinese documents, which we refer to as the \"Handian\" ancient classics corpus (H\u00e0n di\u0103n g\u016d j\u00ed, i.e, the \"Han canon\" or \"Chinese classics\"). It contains classics of ancient Chinese philosophy, documents of historical and biographical significance, and literary works. We begin by describing the Digital Humanities context of this joint project, and the advances in humanities computing that made this project feasible. We describe the corpus and introduce our application of probabilistic topic modeling to this corpus, with attention to the particular challenges posed by modeling ancient Chinese documents. We give a specific example of how the software we have developed can be used to aid discovery and interpretation of themes in the corpus. We outline more advanced forms of computer-aided interpretation that are also made possible by the programming interface provided by our system, and the general implications of these methods for understanding the nature of meaning in these texts.\n    ",
        "submission_date": "2017-02-02T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00887",
        "title": "Structured Attention Networks",
        "authors": [
            "Yoon Kim",
            "Carl Denton",
            "Luong Hoang",
            "Alexander M. Rush"
        ],
        "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00992",
        "title": "Automatic Prediction of Discourse Connectives",
        "authors": [
            "Eric Malmi",
            "Daniele Pighin",
            "Sebastian Krause",
            "Mikhail Kozhevnikov"
        ],
        "abstract": "Accurate prediction of suitable discourse connectives (however, furthermore, etc.) is a key component of any system aimed at building coherent and fluent discourses from shorter sentences and passages. As an example, a dialog system might assemble a long and informative answer by sampling passages extracted from different documents retrieved from the Web. We formulate the task of discourse connective prediction and release a dataset of 2.9M sentence pairs separated by discourse connectives for this task. Then, we evaluate the hardness of the task for human raters, apply a recently proposed decomposable attention (DA) model to this task and observe that the automatic predictor has a higher F1 than human raters (32 vs. 30). Nevertheless, under specific conditions the raters still outperform the DA model, suggesting that there is headroom for future improvements.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2018-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01101",
        "title": "Multilingual Multi-modal Embeddings for Natural Language Processing",
        "authors": [
            "Iacer Calixto",
            "Qun Liu",
            "Nick Campbell"
        ],
        "abstract": "We propose a novel discriminative model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple languages to improve embedding quality. To that end, we introduce a modification of a pairwise contrastive estimation optimisation function as our training objective. We evaluate our embeddings on an image-sentence ranking (ISR), a semantic textual similarity (STS), and a neural machine translation (NMT) task. We find that the additional multilingual signals lead to improvements on both the ISR and STS tasks, and the discriminative cost can also be used in re-ranking $n$-best lists produced by NMT models, yielding strong improvements.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01147",
        "title": "Predicting Target Language CCG Supertags Improves Neural Machine Translation",
        "authors": [
            "Maria Nadejde",
            "Siva Reddy",
            "Rico Sennrich",
            "Tomasz Dwojak",
            "Marcin Junczys-Dowmunt",
            "Philipp Koehn",
            "Alexandra Birch"
        ],
        "abstract": "Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags in the decoder, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling target-syntax improves machine translation quality for German->English, a high-resource pair, and for Romanian->English, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German->English and 1.2 BLEU for Romanian->English.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01172",
        "title": "Insights into Entity Name Evolution on Wikipedia",
        "authors": [
            "Helge Holzmann",
            "Thomas Risse"
        ],
        "abstract": "Working with Web archives raises a number of issues caused by their temporal characteristics. Depending on the age of the content, additional knowledge might be needed to find and understand older texts. Especially facts about entities are subject to change. Most severe in terms of information retrieval are name changes. In order to find entities that have changed their name over time, search engines need to be aware of this evolution. We tackle this problem by analyzing Wikipedia in terms of entity evolutions mentioned in articles regardless the structural elements. We gathered statistics and automatically extracted minimum excerpts covering name changes by incorporating lists dedicated to that subject. In future work, these excerpts are going to be used to discover patterns and detect changes in other sources. In this work we investigate whether or not Wikipedia is a suitable source for extracting the required knowledge.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01176",
        "title": "Named Entity Evolution Analysis on Wikipedia",
        "authors": [
            "Helge Holzmann",
            "Thomas Risse"
        ],
        "abstract": "Accessing Web archives raises a number of issues caused by their temporal characteristics. Additional knowledge is needed to find and understand older texts. Especially entities mentioned in texts are subject to change. Most severe in terms of information retrieval are name changes. In order to find entities that have changed their name over time, search engines need to be aware of this evolution. We tackle this problem by analyzing Wikipedia in terms of entity evolutions mentioned in articles. We present statistical data on excerpts covering name changes, which will be used to discover similar text passages and extract evolution knowledge in future work.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01179",
        "title": "Extraction of Evolution Descriptions from the Web",
        "authors": [
            "Helge Holzmann",
            "Thomas Risse"
        ],
        "abstract": "The evolution of named entities affects exploration and retrieval tasks in digital libraries. An information retrieval system that is aware of name changes can actively support users in finding former occurrences of evolved entities. However, current structured knowledge bases, such as DBpedia or Freebase, do not provide enough information about evolutions, even though the data is available on their resources, like Wikipedia. Our \\emph{Evolution Base} prototype will demonstrate how excerpts describing name evolutions can be identified on these websites with a promising precision. The descriptions are classified by means of models that we trained based on a recent analysis of named entity evolutions on Wikipedia.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01187",
        "title": "Named Entity Evolution Recognition on the Blogosphere",
        "authors": [
            "Helge Holzmann",
            "Nina Tahmasebi",
            "Thomas Risse"
        ],
        "abstract": "Advancements in technology and culture lead to changes in our language. These changes create a gap between the language known by users and the language stored in digital archives. It affects user's possibility to firstly find content and secondly interpret that content. In previous work we introduced our approach for Named Entity Evolution Recognition~(NEER) in newspaper collections. Lately, increasing efforts in Web preservation lead to increased availability of Web archives covering longer time spans. However, language on the Web is more dynamic than in traditional media and many of the basic assumptions from the newspaper domain do not hold for Web data. In this paper we discuss the limitations of existing methodology for NEER. We approach these by adapting an existing NEER method to work on noisy data like the Web and the Blogosphere in particular. We develop novel filters that reduce the noise and make use of Semantic Web resources to obtain more information about terms. Our evaluation shows the potentials of the proposed approach.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01287",
        "title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation",
        "authors": [
            "Iacer Calixto",
            "Qun Liu",
            "Nick Campbell"
        ],
        "abstract": "We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.\n    ",
        "submission_date": "2017-02-04T00:00:00",
        "last_modified_date": "2017-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01360",
        "title": "An Empirical Evaluation of Zero Resource Acoustic Unit Discovery",
        "authors": [
            "Chunxi Liu",
            "Jinyi Yang",
            "Ming Sun",
            "Santosh Kesiraju",
            "Alena Rott",
            "Lucas Ondel",
            "Pegah Ghahremani",
            "Najim Dehak",
            "Lukas Burget",
            "Sanjeev Khudanpur"
        ],
        "abstract": "Acoustic unit discovery (AUD) is a process of automatically identifying a categorical acoustic unit inventory from speech and producing corresponding acoustic unit tokenizations. AUD provides an important avenue for unsupervised acoustic model training in a zero resource setting where expert-provided linguistic knowledge and transcribed speech are unavailable. Therefore, to further facilitate zero-resource AUD process, in this paper, we demonstrate acoustic feature representations can be significantly improved by (i) performing linear discriminant analysis (LDA) in an unsupervised self-trained fashion, and (ii) leveraging resources of other languages through building a multilingual bottleneck (BN) feature extractor to give effective cross-lingual generalization. Moreover, we perform comprehensive evaluations of AUD efficacy on multiple downstream speech applications, and their correlated performance suggests that AUD evaluations are feasible using different alternative language resources when only a subset of these evaluation resources can be available in typical zero resource applications.\n    ",
        "submission_date": "2017-02-05T00:00:00",
        "last_modified_date": "2017-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01417",
        "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations",
        "authors": [
            "Jiaqi Mu",
            "Suma Bhat",
            "Pramod Viswanath"
        ],
        "abstract": "Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a {\\em very simple}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations {\\em even stronger}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and { text classification}) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.\n    ",
        "submission_date": "2017-02-05T00:00:00",
        "last_modified_date": "2018-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01466",
        "title": "Prepositions in Context",
        "authors": [
            "Hongyu Gong",
            "Jiaqi Mu",
            "Suma Bhat",
            "Pramod Viswanath"
        ],
        "abstract": "Prepositions are highly polysemous, and their variegated senses encode significant semantic information. In this paper we match each preposition's complement and attachment and their interplay crucially to the geometry of the word vectors to the left and right of the preposition. Extracting such features from the vast number of instances of each preposition and clustering them makes for an efficient preposition sense disambigution (PSD) algorithm, which is comparable to and better than state-of-the-art on two benchmark datasets. Our reliance on no external linguistic resource allows us to scale the PSD algorithm to a large WikiCorpus and learn sense-specific preposition representations -- which we show to encode semantic relations and paraphrasing of verb particle compounds, via simple vector operations.\n    ",
        "submission_date": "2017-02-05T00:00:00",
        "last_modified_date": "2017-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01517",
        "title": "Opinion Recommendation using Neural Memory Model",
        "authors": [
            "Zhongqing Wang",
            "Yue Zhang"
        ],
        "abstract": "We present opinion recommendation, a novel task of jointly predicting a custom review with a rating score that a certain user would give to a certain product or service, given existing reviews and rating scores to the product or service by other users, and the reviews that the user has given to other products and services. A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning, which is the strength of neural models. We use a single neural network to model users and products, capturing their correlation and generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on ",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01569",
        "title": "Neural Semantic Parsing over Multiple Knowledge-bases",
        "authors": [
            "Jonathan Herzig",
            "Jonathan Berant"
        ],
        "abstract": "A fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form. In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets. We find that we can substantially improve parsing accuracy by training a single sequence-to-sequence model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-of-the-art performance on the Overnight dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters.\n    ",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01587",
        "title": "A Hybrid Approach For Hindi-English Machine Translation",
        "authors": [
            "Omkar Dhariya",
            "Shrikant Malviya",
            "Uma Shanker Tiwary"
        ],
        "abstract": "In this paper, an extended combined approach of phrase based statistical machine translation (SMT), example based MT (EBMT) and rule based MT (RBMT) is proposed to develop a novel hybrid data driven MT system capable of outperforming the baseline SMT, EBMT and RBMT systems from which it is derived. In short, the proposed hybrid MT process is guided by the rule based MT after getting a set of partial candidate translations provided by EBMT and SMT subsystems. Previous works have shown that EBMT systems are capable of outperforming the phrase-based SMT systems and RBMT approach has the strength of generating structurally and morphologically more accurate results. This hybrid approach increases the fluency, accuracy and grammatical precision which improve the quality of a machine translation system. A comparison of the proposed hybrid machine translation (HTM) model with renowned translators i.e. Google, BING and Babylonian is also presented which shows that the proposed model works better on sentences with ambiguity as well as comprised of idioms than others.\n    ",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01711",
        "title": "Q-WordNet PPV: Simple, Robust and (almost) Unsupervised Generation of Polarity Lexicons for Multiple Languages",
        "authors": [
            "I\u00f1aki San Vicente",
            "Rodrigo Agerri",
            "German Rigau"
        ],
        "abstract": "This paper presents a simple, robust and (almost) unsupervised dictionary-based method, qwn-ppv (Q-WordNet as Personalized PageRanking Vector) to automatically generate polarity lexicons. We show that qwn-ppv outperforms other automatically generated lexicons for the four extrinsic evaluations presented here. It also shows very competitive and robust results with respect to manually annotated ones. Results suggest that no single lexicon is best for every task and dataset and that the intrinsic evaluation of polarity lexicons is not a good performance indicator on a Sentiment Analysis task. The qwn-ppv method allows to easily create quality polarity lexicons whenever no domain-based annotated corpora are available for a given language.\n    ",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01714",
        "title": "DNN adaptation by automatic quality estimation of ASR hypotheses",
        "authors": [
            "Daniele Falavigna",
            "Marco Matassoni",
            "Shahab Jalalvand",
            "Matteo Negri",
            "Marco Turchi"
        ],
        "abstract": "In this paper we propose to exploit the automatic Quality Estimation (QE) of ASR hypotheses to perform the unsupervised adaptation of a deep neural network modeling acoustic probabilities. Our hypothesis is that significant improvements can be achieved by: i)automatically transcribing the evaluation data we are currently trying to recognise, and ii) selecting from it a subset of \"good quality\" instances based on the word error rate (WER) scores predicted by a QE component. To validate this hypothesis, we run several experiments on the evaluation data sets released for the CHiME-3 challenge. First, we operate in oracle conditions in which manual transcriptions of the evaluation data are available, thus allowing us to compute the \"true\" sentence WER. In this scenario, we perform the adaptation with variable amounts of data, which are characterised by different levels of quality. Then, we move to realistic conditions in which the manual transcriptions of the evaluation data are not available. In this case, the adaptation is performed on data selected according to the WER scores \"predicted\" by a QE component. Our results indicate that: i) QE predictions allow us to closely approximate the adaptation results obtained in oracle conditions, and ii) the overall ASR performance based on the proposed QE-driven adaptation method is significantly better than the strong, most recent, CHiME-3 baseline.\n    ",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01776",
        "title": "Multi-task memory networks for category-specific aspect and opinion terms co-extraction",
        "authors": [
            "Wenya Wang",
            "Sinno Jialin Pan",
            "Daniel Dahlmeier"
        ],
        "abstract": "In aspect-based sentiment analysis, most existing methods either focus on aspect/opinion terms extraction or aspect terms categorization. However, each task by itself only provides partial information to end users. To generate more detailed and structured opinion analysis, we propose a finer-grained problem, which we call category-specific aspect and opinion terms extraction. This problem involves the identification of aspect and opinion terms within each sentence, as well as the categorization of the identified terms. To this end, we propose an end-to-end multi-task attention model, where each task corresponds to aspect/opinion terms extraction for a specific category. Our model benefits from exploring the commonalities and relationships among different tasks to address the data sparsity issue. We demonstrate its state-of-the-art performance on three benchmark datasets.\n    ",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01802",
        "title": "Ensemble Distillation for Neural Machine Translation",
        "authors": [
            "Markus Freitag",
            "Yaser Al-Onaizan",
            "Baskaran Sankaran"
        ],
        "abstract": "Knowledge distillation describes a method for training a student network to perform better by learning from a stronger teacher network. Translating a sentence with an Neural Machine Translation (NMT) engine is time expensive and having a smaller model speeds up this process. We demonstrate how to transfer the translation quality of an ensemble and an oracle BLEU teacher network into a single NMT system. Further, we present translation improvements from a teacher network that has the same architecture and dimensions of the student network. As the training of the student model is still expensive, we introduce a data filtering method based on the knowledge of the teacher model that not only speeds up the training, but also leads to better translation quality. Our techniques need no code change and can be easily reproduced with any NMT architecture to speed up the decoding process.\n    ",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01806",
        "title": "Beam Search Strategies for Neural Machine Translation",
        "authors": [
            "Markus Freitag",
            "Yaser Al-Onaizan"
        ],
        "abstract": "The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to- right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the draw- back of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German-English and Chinese-English without losing any translation quality.\n    ",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01815",
        "title": "Living a discrete life in a continuous world: Reference with distributed representations",
        "authors": [
            "Gemma Boleda",
            "Sebastian Pad\u00f3",
            "Nghia The Pham",
            "Marco Baroni"
        ],
        "abstract": "Reference is a crucial property of language that allows us to connect linguistic expressions to the world. Modeling it requires handling both continuous and discrete aspects of meaning. Data-driven models excel at the former, but struggle with the latter, and the reverse is true for symbolic models.\n",
        "submission_date": "2017-02-06T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01829",
        "title": "Neural Discourse Structure for Text Categorization",
        "authors": [
            "Yangfeng Ji",
            "Noah Smith"
        ],
        "abstract": "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01841",
        "title": "The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task",
        "authors": [
            "Roy Schwartz",
            "Maarten Sap",
            "Ioannis Konstas",
            "Li Zilles",
            "Yejin Choi",
            "Noah A. Smith"
        ],
        "abstract": "A writer's style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatically affect the way people write.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01923",
        "title": "Comparative Study of CNN and RNN for Natural Language Processing",
        "authors": [
            "Wenpeng Yin",
            "Katharina Kann",
            "Mo Yu",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01925",
        "title": "Effects of Stop Words Elimination for Arabic Information Retrieval: A Comparative Study",
        "authors": [
            "Ibrahim Abu El-Khair"
        ],
        "abstract": "The effectiveness of three stop words lists for Arabic Information Retrieval---General Stoplist, Corpus-Based Stoplist, Combined Stoplist ---were investigated in this study. Three popular weighting schemes were examined: the inverse document frequency weight, probabilistic weighting, and statistical language modelling. The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance, and compare their effect on retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was used with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study, stoplists improved retrieval effectiveness especially when used with the BM25 weight. The overall performance of a general stoplist was better than the other two lists.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01932",
        "title": "A Knowledge-Grounded Neural Conversation Model",
        "authors": [
            "Marjan Ghazvininejad",
            "Chris Brockett",
            "Ming-Wei Chang",
            "Bill Dolan",
            "Jianfeng Gao",
            "Wen-tau Yih",
            "Michel Galley"
        ],
        "abstract": "Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external \"facts\", allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive Seq2Seq baseline. Human judges found that our outputs are significantly more informative.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2018-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01944",
        "title": "EliXa: A Modular and Flexible ABSA Platform",
        "authors": [
            "I\u00f1aki San Vicente",
            "Xabier Saralegi",
            "Rodrigo Agerri"
        ],
        "abstract": "This paper presents a supervised Aspect Based Sentiment Analysis (ABSA) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the Opinion Target Extraction (OTE) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classification (slot 3) is addressed by means of a multiclass SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01991",
        "title": "Representations of language in a model of visually grounded speech signal",
        "authors": [
            "Grzegorz Chrupa\u0142a",
            "Lieke Gelderloos",
            "Afra Alishahi"
        ],
        "abstract": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02052",
        "title": "Knowledge Adaptation: Teaching to Adapt",
        "authors": [
            "Sebastian Ruder",
            "Parsa Ghaffari",
            "John G. Breslin"
        ],
        "abstract": "Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics.\n",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02092",
        "title": "Characterisation of speech diversity using self-organising maps",
        "authors": [
            "Tom A. F. Anderson",
            "David M. W. Powers"
        ],
        "abstract": "We report investigations into speaker classification of larger quantities of unlabelled speech data using small sets of manually phonemically annotated speech. The Kohonen speech typewriter is a semi-supervised method comprised of self-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a 2D array of cells that learn vector representations of the data based on neighbourhoods. In this paper, we report a method to evaluate pronunciation using multilevel SOMs with /hVd/ single syllable utterances for the study of vowels, for Australian pronunciation.\n    ",
        "submission_date": "2017-01-23T00:00:00",
        "last_modified_date": "2017-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02098",
        "title": "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions",
        "authors": [
            "Emma Strubell",
            "Patrick Verga",
            "David Belanger",
            "Andrew McCallum"
        ],
        "abstract": "Today when many practitioners run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs. Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF). Though expressive and accurate, these models fail to fully exploit GPU parallelism, limiting their computational efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER: Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8x faster test time speeds.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02170",
        "title": "How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks",
        "authors": [
            "Stanis\u0142aw Jastrzebski",
            "Damian Le\u015bniak",
            "Wojciech Marian Czarnecki"
        ],
        "abstract": "Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning).\n",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02171",
        "title": "Question Answering through Transfer Learning from Large Fine-grained Supervision Data",
        "authors": [
            "Sewon Min",
            "Minjoon Seo",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2018-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02206",
        "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets",
        "authors": [
            "Zhilin Yang",
            "Junjie Hu",
            "Ruslan Salakhutdinov",
            "William W. Cohen"
        ],
        "abstract": "We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02211",
        "title": "Fixing the Infix: Unsupervised Discovery of Root-and-Pattern Morphology",
        "authors": [
            "Tarek Sakakini",
            "Suma Bhat",
            "Pramod Viswanath"
        ],
        "abstract": "We present an unsupervised and language-agnostic method for learning root-and-pattern morphology in Semitic languages. This form of morphology, abundant in Semitic languages, has not been handled in prior unsupervised approaches. We harness the syntactico-semantic information in distributed word representations to solve the long standing problem of root-and-pattern discovery in Semitic languages. Moreover, we construct an unsupervised root extractor based on the learned rules. We prove the validity of learned rules across Arabic, Hebrew, and Amharic, alongside showing that our root extractor compares favorably with a widely used, carefully engineered root extractor: ISRI.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02212",
        "title": "MORSE: Semantic-ally Drive-n MORpheme SEgment-er",
        "authors": [
            "Tarek Sakakini",
            "Suma Bhat",
            "Pramod Viswanath"
        ],
        "abstract": "We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results.\n    ",
        "submission_date": "2017-02-07T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02261",
        "title": "Social media mining for identification and exploration of health-related information from pregnant women",
        "authors": [
            "Pramod Bharadwaj Chandrashekar",
            "Arjun Magge",
            "Abeed Sarker",
            "Graciela Gonzalez"
        ],
        "abstract": "Widespread use of social media has led to the generation of substantial amounts of information about individuals, including health-related information. Social media provides the opportunity to study health-related information about selected population groups who may be of interest for a particular study. In this paper, we explore the possibility of utilizing social media to perform targeted data collection and analysis from a particular population group -- pregnant women. We hypothesize that we can use social media to identify cohorts of pregnant women and follow them over time to analyze crucial health-related information. To identify potentially pregnant women, we employ simple rule-based searches that attempt to detect pregnancy announcements with moderate precision. To further filter out false positives and noise, we employ a supervised classifier using a small number of hand-annotated data. We then collect their posts over time to create longitudinal health timelines and attempt to divide the timelines into different pregnancy trimesters. Finally, we assess the usefulness of the timelines by performing a preliminary analysis to estimate drug intake patterns of our cohort at different trimesters. Our rule-based cohort identification technique collected 53,820 users over thirty months from Twitter. Our pregnancy announcement classification technique achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user timelines. Analysis of the timelines revealed that pertinent health-related information, such as drug-intake and adverse reactions can be mined from the data. Our approach to using user timelines in this fashion has produced very encouraging results and can be employed for other important tasks where cohorts, for which health-related information may not be available from other sources, are required to be followed over time to derive population-based estimates.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02265",
        "title": "Neural Machine Translation with Source-Side Latent Graph Parsing",
        "authors": [
            "Kazuma Hashimoto",
            "Yoshimasa Tsuruoka"
        ],
        "abstract": "This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the parser is optimized according to the translation objective. In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our model can be further improved by pre-training it with a small amount of treebank annotations. Our final ensemble model significantly outperforms the previous best models on the standard English-to-Japanese translation dataset.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02363",
        "title": "Automatically Annotated Turkish Corpus for Named Entity Recognition and Text Categorization using Large-Scale Gazetteers",
        "authors": [
            "H. Bahadir Sahin",
            "Caglar Tirkaz",
            "Eray Yildiz",
            "Mustafa Tolga Eren",
            "Ozan Sonmez"
        ],
        "abstract": "Turkish Wikipedia Named-Entity Recognition and Text Categorization (TWNERTC) dataset is a collection of automatically categorized and annotated sentences obtained from Wikipedia. We constructed large-scale gazetteers by using a graph crawler algorithm to extract relevant entity and domain information from a semantic knowledge base, Freebase. The constructed gazetteers contains approximately 300K entities with thousands of fine-grained entity types under 77 different domains. Since automated processes are prone to ambiguity, we also introduce two new content specific noise reduction methodologies. Moreover, we map fine-grained entity types to the equivalent four coarse-grained types: person, loc, org, misc. Eventually, we construct six different dataset versions and evaluate the quality of annotations by comparing ground truths from human annotators. We make these datasets publicly available to support studies on Turkish named-entity recognition (NER) and text categorization (TC).\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02367",
        "title": "Iterative Multi-document Neural Attention for Multiple Answer Prediction",
        "authors": [
            "Claudio Greco",
            "Alessandro Suglia",
            "Pierpaolo Basile",
            "Gaetano Rossiello",
            "Giovanni Semeraro"
        ],
        "abstract": "People have information needs of varying complexity, which can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences. In a scenario in which the user profile can be considered as a question, intelligent agents able to answer questions can be used to find the most relevant answers for a given user. In this work we propose a novel model based on Artificial Neural Networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base. The model is evaluated on the factoid Question Answering and top-n recommendation tasks of the bAbI Movie Dialog dataset. After assessing the performance of the model on both tasks, we try to define the long-term goal of a conversational recommender system able to interact using natural language and to support users in their information seeking processes in a personalized way.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02390",
        "title": "A Hybrid Convolutional Variational Autoencoder for Text Generation",
        "authors": [
            "Stanislau Semeniuta",
            "Aliaksei Severyn",
            "Erhardt Barth"
        ],
        "abstract": "In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02426",
        "title": "Data Selection Strategies for Multi-Domain Sentiment Analysis",
        "authors": [
            "Sebastian Ruder",
            "Parsa Ghaffari",
            "John G. Breslin"
        ],
        "abstract": "Domain adaptation is important in sentiment analysis as sentiment-indicating words vary between domains. Recently, multi-domain adaptation has become more pervasive, but existing approaches train on all available source domains including dissimilar ones. However, the selection of appropriate training data is as important as the choice of algorithm. We undertake -- to our knowledge for the first time -- an extensive study of domain similarity metrics in the context of sentiment analysis and propose novel representations, metrics, and a new scope for data selection. We evaluate the proposed methods on two large-scale multi-domain adaptation settings on tweets and reviews and demonstrate that they consistently outperform strong random and balanced baselines, while our proposed selection strategy outperforms instance-level selection and yields the best score on a large reviews corpus.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02429",
        "title": "Trainable Greedy Decoding for Neural Machine Translation",
        "authors": [
            "Jiatao Gu",
            "Kyunghyun Cho",
            "Victor O.K. Li"
        ],
        "abstract": "Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02535",
        "title": "Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization",
        "authors": [
            "Ye Zhang",
            "Matthew Lease",
            "Byron C. Wallace"
        ],
        "abstract": "A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02540",
        "title": "Automatic Rule Extraction from Long Short Term Memory Networks",
        "authors": [
            "W. James Murdoch",
            "Arthur Szlam"
        ],
        "abstract": "Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02584",
        "title": "Predicting Audience's Laughter Using Convolutional Neural Network",
        "authors": [
            "Lei Chen",
            "Chong MIn Lee"
        ],
        "abstract": "For the purpose of automatically evaluating speakers' humor usage, we build a presentation corpus containing humorous utterances based on TED talks. Compared to previous data resources supporting humor recognition research, ours has several advantages, including (a) both positive and negative instances coming from a homogeneous data set, (b) containing a large number of speakers, and (c) being open. Focusing on using lexical cues for humor recognition, we systematically compare a newly emerging text classification method based on Convolutional Neural Networks (CNNs) with a well-established conventional method using linguistic knowledge. The advantages of the CNN method are both getting higher detection accuracies and being able to learn essential features automatically.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02640",
        "title": "Character-level Deep Conflation for Business Data Analytics",
        "authors": [
            "Zhe Gan",
            "P. D. Singh",
            "Ameet Joshi",
            "Xiaodong He",
            "Jianshu Chen",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "Connecting different text attributes associated with the same entity (conflation) is important in business data analytics since it could help merge two different tables in a database to provide a more comprehensive profile of an entity. However, the conflation task is challenging because two text strings that describe the same entity could be quite different from each other for reasons such as misspelling. It is therefore critical to develop a conflation model that is able to truly understand the semantic meaning of the strings and match them at the semantic level. To this end, we develop a character-level deep conflation model that encodes the input text strings from character level into finite dimension feature vectors, which are then used to compute the cosine similarity between the text strings. The model is trained in an end-to-end manner using back propagation and stochastic gradient descent to maximize the likelihood of the correct association. Specifically, we propose two variants of the deep conflation model, based on long-short-term memory (LSTM) recurrent neural network (RNN) and convolutional neural network (CNN), respectively. Both models perform well on a real-world business analytics dataset and significantly outperform the baseline bag-of-character (BoC) model.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02736",
        "title": "Challenges in Providing Automatic Affective Feedback in Instant Messaging Applications",
        "authors": [
            "Chieh-Yang Huang",
            "Ting-Hao",
            "Huang",
            "Lun-Wei Ku"
        ],
        "abstract": "Instant messaging is one of the major channels of computer mediated communication. However, humans are known to be very limited in understanding others' emotions via text-based communication. Aiming on introducing emotion sensing technologies to instant messaging, we developed EmotionPush, a system that automatically detects the emotions of the messages end-users received on Facebook Messenger and provides colored cues on their smartphones accordingly. We conducted a deployment study with 20 participants during a time span of two weeks. In this paper, we revealed five challenges, along with examples, that we observed in our study based on both user's feedback and chat logs, including (i)the continuum of emotions, (ii)multi-user conversations, (iii)different dynamics between different users, (iv)misclassification of emotions and (v)unconventional content. We believe this discussion will benefit the future exploration of affective computing for instant messaging, and also shed light on research of conversational emotion sensing.\n    ",
        "submission_date": "2017-02-09T00:00:00",
        "last_modified_date": "2017-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03033",
        "title": "Local System Voting Feature for Machine Translation System Combination",
        "authors": [
            "Markus Freitag",
            "Jan-Thorsten Peter",
            "Stephan Peitz",
            "Minwei Feng",
            "Hermann Ney"
        ],
        "abstract": "In this paper, we enhance the traditional confusion network system combination approach with an additional model trained by a neural network. This work is motivated by the fact that the commonly used binary system voting models only assign each input system a global weight which is responsible for the global impact of each input system on all translations. This prevents individual systems with low system weights from having influence on the system combination output, although in some situations this could be helpful. Further, words which have only been seen by one or few systems rarely have a chance of being present in the combined output. We train a local system voting model by a neural network which is based on the words themselves and the combinatorial occurrences of the different system outputs. This gives system combination the option to prefer other systems at different word positions even for the same sentence.\n    ",
        "submission_date": "2017-02-10T00:00:00",
        "last_modified_date": "2017-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03082",
        "title": "UsingWord Embedding for Cross-Language Plagiarism Detection",
        "authors": [
            "J. Ferrero",
            "F. Agnes",
            "L. Besacier",
            "D. Schwab"
        ],
        "abstract": "This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following: (a) we introduce new cross-language similarity detection methods based on distributed representation of words; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15% for English-French similarity detection at chunk level (88.5% at sentence level) on a very challenging corpus.\n    ",
        "submission_date": "2017-02-10T00:00:00",
        "last_modified_date": "2017-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03121",
        "title": "Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction",
        "authors": [
            "Ashutosh Modi",
            "Ivan Titov",
            "Vera Demberg",
            "Asad Sayeed",
            "Manfred Pinkal"
        ],
        "abstract": "Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content. Prediction also affects perception and might be a key to robustness in human language processing. In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect.\n    ",
        "submission_date": "2017-02-10T00:00:00",
        "last_modified_date": "2017-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03196",
        "title": "Universal Semantic Parsing",
        "authors": [
            "Siva Reddy",
            "Oscar T\u00e4ckstr\u00f6m",
            "Slav Petrov",
            "Mark Steedman",
            "Mirella Lapata"
        ],
        "abstract": "Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions. Our code and data can be downloaded at ",
        "submission_date": "2017-02-10T00:00:00",
        "last_modified_date": "2017-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03197",
        "title": "Arabic Language Sentiment Analysis on Health Services",
        "authors": [
            "Abdulaziz M. Alayba",
            "Vasile Palade",
            "Matthew England",
            "Rahat Iqbal"
        ],
        "abstract": "The social media network phenomenon leads to a massive amount of valuable data that is available online and easy to access. Many users share images, videos, comments, reviews, news and opinions on different social networks sites, with Twitter being one of the most popular ones. Data collected from Twitter is highly unstructured, and extracting useful information from tweets is a challenging task. Twitter has a huge number of Arabic users who mostly post and write their tweets using the Arabic language. While there has been a lot of research on sentiment analysis in English, the amount of researches and datasets in Arabic language is limited. This paper introduces an Arabic language dataset which is about opinions on health services and has been collected from Twitter. The paper will first detail the process of collecting the data from Twitter and also the process of filtering, pre-processing and annotating the Arabic text in order to build a big sentiment analysis dataset in Arabic. Several Machine Learning algorithms (Naive Bayes, Support Vector Machine and Logistic Regression) alongside Deep and Convolutional Neural Networks were utilized in our experiments of sentiment analysis on our health dataset.\n    ",
        "submission_date": "2017-02-10T00:00:00",
        "last_modified_date": "2017-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03305",
        "title": "Universal Dependencies to Logical Forms with Negation Scope",
        "authors": [
            "Federico Fancellu",
            "Siva Reddy",
            "Adam Lopez",
            "Bonnie Webber"
        ],
        "abstract": "Many language technology applications would benefit from the ability to represent negation and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a first-order logic representation with negation scope marked using Universal Dependencies. To do so, we enhance UDepLambda, a framework that converts dependency graphs to logical forms. The resulting UDepLambda$\\lnot$ is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers.\n    ",
        "submission_date": "2017-02-10T00:00:00",
        "last_modified_date": "2017-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03342",
        "title": "Learning Concept Embeddings for Efficient Bag-of-Concepts Densification",
        "authors": [
            "Walid Shalaby",
            "Wlodek Zadrozny"
        ],
        "abstract": "Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main ideas, objects, and the characteristics of these structures. The so called Bag of Concepts (BoC) representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. To address this problem, we propose two neural embedding models to learn continuous concept vectors. Once they are learned, we propose an efficient vector aggregation method to generate fully continuous BoC representations. We evaluate our concept embedding models on three tasks: 1) measuring entity semantic relatedness and ranking where we achieve 1.6% improvement in correlation scores, 2) dataless concept categorization where we achieve state-of-the-art performance and reduce the categorization error rate by more than 5% compared to five prior word and entity embedding models, and 3) dataless document classification where our models outperform the sparse BoC representations. In addition, by exploiting our efficient linear time vector aggregation method, we achieve better accuracy scores with much less concept dimensions compared to previous BoC densification methods which operate in polynomial time and require hundreds of dimensions in the BoC representation.\n    ",
        "submission_date": "2017-02-10T00:00:00",
        "last_modified_date": "2018-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03470",
        "title": "Vector Embedding of Wikipedia Concepts and Entities",
        "authors": [
            "Ehsan Sherkat",
            "Evangelos Milios"
        ],
        "abstract": "Using deep learning for different machine learning tasks such as image classification and word embedding has recently gained many attentions. Its appealing performance reported across specific Natural Language Processing (NLP) tasks in comparison with other approaches is the reason for its popularity. Word embedding is the task of mapping words or phrases to a low dimensional numerical vector. In this paper, we use deep learning to embed Wikipedia Concepts and Entities. The English version of Wikipedia contains more than five million pages, which suggest its capability to cover many English Entities, Phrases, and Concepts. Each Wikipedia page is considered as a concept. Some concepts correspond to entities, such as a person's name, an organization or a place. Contrary to word embedding, Wikipedia Concepts Embedding is not ambiguous, so there are different vectors for concepts with similar surface form but different mentions. We proposed several approaches and evaluated their performance based on Concept Analogy and Concept Similarity tasks. The results show that proposed approaches have the performance comparable and in some cases even higher than the state-of-the-art methods.\n    ",
        "submission_date": "2017-02-12T00:00:00",
        "last_modified_date": "2017-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03525",
        "title": "Learning to Parse and Translate Improves Neural Machine Translation",
        "authors": [
            "Akiko Eriguchi",
            "Yoshimasa Tsuruoka",
            "Kyunghyun Cho"
        ],
        "abstract": "There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.\n    ",
        "submission_date": "2017-02-12T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03654",
        "title": "A Morphology-aware Network for Morphological Disambiguation",
        "authors": [
            "Eray Yildiz",
            "Caglar Tirkaz",
            "H. Bahadir Sahin",
            "Mustafa Tolga Eren",
            "Ozan Sonmez"
        ],
        "abstract": "Agglutinative languages such as Turkish, Finnish and Hungarian require morphological disambiguation before further processing due to the complex morphology of words. A morphological disambiguator is used to select the correct morphological analysis of a word. Morphological disambiguation is important because it generally is one of the first steps of natural language processing and its performance affects subsequent analyses. In this paper, we propose a system that uses deep learning techniques for morphological disambiguation. Many of the state-of-the-art results in computer vision, speech recognition and natural language processing have been obtained through deep learning models. However, applying deep learning techniques to morphologically rich languages is not well studied. In this work, while we focus on Turkish morphological disambiguation we also present results for French and German in order to show that the proposed architecture achieves high accuracy with no language-specific feature engineering or additional resource. In the experiments, we achieve 84.12, 88.35 and 93.78 morphological disambiguation accuracy among the ambiguous words for Turkish, German and French respectively.\n    ",
        "submission_date": "2017-02-13T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03706",
        "title": "Multitask Learning with Deep Neural Networks for Community Question Answering",
        "authors": [
            "Daniele Bonadiman",
            "Antonio Uva",
            "Alessandro Moschitti"
        ],
        "abstract": "In this paper, we developed a deep neural network (DNN) that learns to solve simultaneously the three tasks of the cQA challenge proposed by the SemEval-2016 Task 3, i.e., question-comment similarity, question-question similarity and new question-comment similarity. The latter is the main task, which can exploit the previous two for achieving better results. Our DNN is trained jointly on all the three cQA tasks and learns to encode questions and comments into a single vector representation shared across the multiple tasks. The results on the official challenge test set show that our approach produces higher accuracy and faster convergence rates than the individual neural networks. Additionally, our method, which does not use any manual feature engineering, approaches the state of the art established with methods that make heavy use of it.\n    ",
        "submission_date": "2017-02-13T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03856",
        "title": "Towards speech-to-text translation without speech recognition",
        "authors": [
            "Sameer Bansal",
            "Herman Kamper",
            "Adam Lopez",
            "Sharon Goldwater"
        ],
        "abstract": "We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.\n    ",
        "submission_date": "2017-02-13T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03859",
        "title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax",
        "authors": [
            "Samuel L. Smith",
            "David H. P. Turban",
            "Steven Hamblin",
            "Nils Y. Hammerla"
        ],
        "abstract": "Usually bilingual word vectors are trained \"online\". Mikolov et al. showed they can also be found \"offline\", whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.\n    ",
        "submission_date": "2017-02-13T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03964",
        "title": "The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations",
        "authors": [
            "Lasha Abzianidze",
            "Johannes Bjerva",
            "Kilian Evang",
            "Hessel Haagsma",
            "Rik van Noord",
            "Pierre Ludmann",
            "Duc-Duy Nguyen",
            "Johan Bos"
        ],
        "abstract": "The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection: automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The semantic annotation consists of five main steps: (i) segmentation of the text in sentences and lexical items; (ii) syntactic parsing with Combinatory Categorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and (v) compositional semantic analysis based on Discourse Representation Theory. These steps are performed using statistical models trained in a semi-supervised manner. The employed annotation models are all language-neutral. Our first results are promising.\n    ",
        "submission_date": "2017-02-13T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04066",
        "title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction",
        "authors": [
            "Courtney Napoles",
            "Keisuke Sakaguchi",
            "Joel Tetreault"
        ],
        "abstract": "We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading GEC systems on this corpus, identifying specific areas in which they do well and how they can improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of GEC.\n    ",
        "submission_date": "2017-02-14T00:00:00",
        "last_modified_date": "2017-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04241",
        "title": "Detection of Slang Words in e-Data using semi-Supervised Learning",
        "authors": [
            "Alok Ranjan Pal",
            "Diganta Saha"
        ],
        "abstract": "The proposed algorithmic approach deals with finding the sense of a word in an electronic data. Now a day,in different communication mediums like internet, mobile services etc. people use few words, which are slang in nature. This approach detects those abusive words using supervised learning procedure. But in the real life scenario, the slang words are not used in complete word forms always. Most of the times, those words are used in different abbreviated forms like sounds alike forms, taboo morphemes etc. This proposed approach can detect those abbreviated forms also using semi supervised learning procedure. Using the synset and concept analysis of the text, the probability of a suspicious word to be a slang word is also evaluated.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04333",
        "title": "On the Relevance of Auditory-Based Gabor Features for Deep Learning in Automatic Speech Recognition",
        "authors": [
            "Angel Mario Castro Martinez",
            "Sri Harish Mallidi",
            "Bernd T. Meyer"
        ],
        "abstract": "Previous studies support the idea of merging auditory-based Gabor features with deep learning architectures to achieve robust automatic speech recognition, however, the cause behind the gain of such combination is still unknown. We believe these representations provide the deep learning decoder with more discriminable cues. Our aim with this paper is to validate this hypothesis by performing experiments with three different recognition tasks (Aurora 4, CHiME 2 and CHiME 3) and assess the discriminability of the information encoded by Gabor filterbank features. Additionally, to identify the contribution of low, medium and high temporal modulation frequencies subsets of the Gabor filterbank were used as features (dubbed LTM, MTM and HTM respectively). With temporal modulation frequencies between 16 and 25 Hz, HTM consistently outperformed the remaining ones in every condition, highlighting the robustness of these representations against channel distortions, low signal-to-noise ratios and acoustically challenging real-life scenarios with relative improvements from 11 to 56% against a Mel-filterbank-DNN baseline. To explain the results, a measure of similarity between phoneme classes from DNN activations is proposed and linked to their acoustic properties. We find this measure to be consistent with the observed error rates and highlight specific differences on phoneme level to pinpoint the benefit of the proposed features.\n    ",
        "submission_date": "2017-02-14T00:00:00",
        "last_modified_date": "2017-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04372",
        "title": "A case study on using speech-to-translation alignments for language documentation",
        "authors": [
            "Antonios Anastasopoulos",
            "David Chiang"
        ],
        "abstract": "For many low-resource or endangered languages, spoken language resources are more likely to be annotated with translations than with transcriptions. Recent work exploits such annotations to produce speech-to-translation alignments, without access to any text transcriptions. We investigate whether providing such information can aid in producing better (mismatched) crowdsourced transcriptions, which in turn could be valuable for training speech recognition systems, and show that they can indeed be beneficial through a small-scale case study as a proof-of-concept. We also present a simple phonetically aware string averaging technique that produces transcriptions of higher quality.\n    ",
        "submission_date": "2017-02-14T00:00:00",
        "last_modified_date": "2017-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04457",
        "title": "Automated Phrase Mining from Massive Text Corpora",
        "authors": [
            "Jingbo Shang",
            "Jialu Liu",
            "Meng Jiang",
            "Xiang Ren",
            "Clare R Voss",
            "Jiawei Han"
        ],
        "abstract": "As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a text corpus. Phrase mining is important in various tasks such as information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance on text corpora of new domains and genres without extra but expensive adaption. Recently, a few data-driven methods have been developed successfully for extraction of phrases from massive domain-specific text. However, none of the state-of-the-art models is fully automated because they require human experts for designing rules or labeling phrases.\n",
        "submission_date": "2017-02-15T00:00:00",
        "last_modified_date": "2017-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04488",
        "title": "Transfer Deep Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network",
        "authors": [
            "Jingjing Xu",
            "Xu Sun"
        ],
        "abstract": "Recent studies have shown effectiveness in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. We propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Furthermore, this paper achieves state-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets.\n    ",
        "submission_date": "2017-02-15T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04510",
        "title": "A Dependency-Based Neural Reordering Model for Statistical Machine Translation",
        "authors": [
            "Christian Hadiwinoto",
            "Hwee Tou Ng"
        ],
        "abstract": "In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-the-art statistical MT system that uses sparse dependency-based reordering features.\n    ",
        "submission_date": "2017-02-15T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04521",
        "title": "Frustratingly Short Attention Spans in Neural Language Modeling",
        "authors": [
            "Micha\u0142 Daniluk",
            "Tim Rockt\u00e4schel",
            "Johannes Welbl",
            "Sebastian Riedel"
        ],
        "abstract": "Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.\n    ",
        "submission_date": "2017-02-15T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04615",
        "title": "Automated Identification of Drug-Drug Interactions in Pediatric Congestive Heart Failure Patients",
        "authors": [
            "Daniel Miller"
        ],
        "abstract": "Congestive Heart Failure, or CHF, is a serious medical condition that can result in fluid buildup in the body as a result of a weak heart. When the heart can't pump enough blood to efficiently deliver nutrients and oxygen to the body, kidney function may be impaired, resulting in fluid retention. CHF patients require a broad drug regimen to maintain the delicate system balance, particularly between their heart and kidneys. These drugs include ACE inhibitors and Beta Blockers to control blood pressure, anticoagulants to prevent blood clots, and diuretics to reduce fluid overload. Many of these drugs may interact, and potential effects of these interactions must be weighed against their benefits. For this project, we consider a set of 44 drugs identified as specifically relevant for treating CHF by pediatric cardiologists at Lucile Packard Children's Hospital. This list was generated as part of our current work at the LPCH Heart Center. The goal of this project is to identify and evaluate potentially harmful drug-drug interactions (DDIs) within pediatric patients with Congestive Heart Failure. This identification will be done autonomously, so that it may continuously update by evaluating newly published literature.\n    ",
        "submission_date": "2017-02-11T00:00:00",
        "last_modified_date": "2017-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04770",
        "title": "Training Language Models Using Target-Propagation",
        "authors": [
            "Sam Wiseman",
            "Sumit Chopra",
            "Marc'Aurelio Ranzato",
            "Arthur Szlam",
            "Ruoyu Sun",
            "Soumith Chintala",
            "Nicolas Vasilache"
        ],
        "abstract": "While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.\n    ",
        "submission_date": "2017-02-15T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04811",
        "title": "Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study",
        "authors": [
            "John P. Lalor",
            "Hao Wu",
            "Tsendsuren Munkhdalai",
            "Hong Yu"
        ],
        "abstract": "Interpreting the performance of deep learning models beyond test set accuracy is challenging. Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally. We examine the impact of a test set question's difficulty to determine if there is a relationship between difficulty and performance. We model difficulty using well-studied psychometric methods on human response patterns. Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question's difficulty. As DNNs are trained with more data, easy examples are learned more quickly than hard examples.\n    ",
        "submission_date": "2017-02-15T00:00:00",
        "last_modified_date": "2018-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.04938",
        "title": "Fast and unsupervised methods for multilingual cognate clustering",
        "authors": [
            "Taraka Rama",
            "Johannes Wahle",
            "Pavel Sofroniev",
            "Gerhard J\u00e4ger"
        ],
        "abstract": "In this paper we explore the use of unsupervised methods for detecting cognates in multilingual word lists. We use online EM to train sound segment similarity weights for computing similarity between two words. We tested our online systems on geographically spread sixteen different language groups of the world and show that the Online PMI system (Pointwise Mutual Information) outperforms a HMM based system and two linguistically motivated systems: LexStat and ALINE. Our results suggest that a PMI system trained in an online fashion can be used by historical linguists for fast and accurate identification of cognates in not so well-studied language families.\n    ",
        "submission_date": "2017-02-16T00:00:00",
        "last_modified_date": "2017-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05053",
        "title": "Addressing the Data Sparsity Issue in Neural AMR Parsing",
        "authors": [
            "Xiaochang Peng",
            "Chuan Wang",
            "Daniel Gildea",
            "Nianwen Xue"
        ],
        "abstract": "Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources.\n    ",
        "submission_date": "2017-02-16T00:00:00",
        "last_modified_date": "2017-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05270",
        "title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision",
        "authors": [
            "Sandro Pezzelle",
            "Marco Marelli",
            "Raffaella Bernardi"
        ],
        "abstract": "People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a 'fuzzy' measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided.\n    ",
        "submission_date": "2017-02-17T00:00:00",
        "last_modified_date": "2017-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05398",
        "title": "Experiment Segmentation in Scientific Discourse as Clause-level Structured Prediction using Recurrent Neural Networks",
        "authors": [
            "Pradeep Dasigi",
            "Gully A.P.C. Burns",
            "Eduard Hovy",
            "Anita de Waard"
        ],
        "abstract": "We propose a deep learning model for identifying structure within experiment narratives in scientific literature. We take a sequence labeling approach to this problem, and label clauses within experiment narratives to identify the different parts of the experiment. Our dataset consists of paragraphs taken from open access PubMed papers labeled with rhetorical information as a result of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells that labels clauses. The clause representations are computed by combining word representations using a novel attention mechanism that involves a separate RNN. We compare this model against LSTMs where the input layer has simple or no attention and a feature rich CRF model. Furthermore, we describe how our work could be useful for information extraction from scientific literature.\n    ",
        "submission_date": "2017-02-17T00:00:00",
        "last_modified_date": "2017-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05531",
        "title": "Analysis and Optimization of fastText Linear Text Classifier",
        "authors": [
            "Vladimir Zolotov",
            "David Kung"
        ],
        "abstract": "The paper [1] shows that simple linear classifier can compete with complex deep learning algorithms in text classification applications. Combining bag of words (BoW) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower. We proved formally that fastText can be transformed into a simpler equivalent classifier, which unlike fastText does not have any hidden layer. We also proved that the necessary and sufficient dimensionality of the word vector embedding space is exactly the number of document classes. These results help constructing more optimal linear text classifiers with guaranteed maximum classification capabilities. The results are proven exactly by pure formal algebraic methods without attracting any empirical data.\n    ",
        "submission_date": "2017-02-17T00:00:00",
        "last_modified_date": "2017-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05624",
        "title": "Reproducing and learning new algebraic operations on word embeddings using genetic programming",
        "authors": [
            "Roberto Santana"
        ],
        "abstract": "Word-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming (GP). We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion running words, and assessing their accuracy in the same word analogy task.\n    ",
        "submission_date": "2017-02-18T00:00:00",
        "last_modified_date": "2017-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05638",
        "title": "A Stylometric Inquiry into Hyperpartisan and Fake News",
        "authors": [
            "Martin Potthast",
            "Johannes Kiesel",
            "Kevin Reinartz",
            "Janek Bevendorff",
            "Benno Stein"
        ],
        "abstract": "This paper reports on a writing style analysis of hyperpartisan (i.e., extremely one-sided) news in connection to fake news. It presents a large corpus of 1,627 articles that were manually fact-checked by professional journalists from BuzzFeed. The articles originated from 9 well-known political publishers, 3 each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing. In sum, the corpus contains 299 fake news, 97% of which originated from hyperpartisan publishers.\n",
        "submission_date": "2017-02-18T00:00:00",
        "last_modified_date": "2017-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05793",
        "title": "Harmonic Grammar, Optimality Theory, and Syntax Learnability: An Empirical Exploration of Czech Word Order",
        "authors": [
            "Ann Irvine",
            "Mark Dredze"
        ],
        "abstract": "This work presents a systematic theoretical and empirical comparison of the major algorithms that have been proposed for learning Harmonic and Optimality Theory grammars (HG and OT, respectively). By comparing learning algorithms, we are also able to compare the closely related OT and HG frameworks themselves. Experimental results show that the additional expressivity of the HG framework over OT affords performance gains in the task of predicting the surface word order of Czech sentences. We compare the perceptron with the classic Gradual Learning Algorithm (GLA), which learns OT grammars, as well as the popular Maximum Entropy model. In addition to showing that the perceptron is theoretically appealing, our work shows that the performance of the HG model it learns approaches that of the upper bound in prediction accuracy on a held out test set and that it is capable of accurately modeling observed variation.\n    ",
        "submission_date": "2017-02-19T00:00:00",
        "last_modified_date": "2017-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05821",
        "title": "Post-edit Analysis of Collective Biography Generation",
        "authors": [
            "Bo Han",
            "Will Radford",
            "Ana\u00efs Cadilhac",
            "Art Harol",
            "Andrew Chisholm",
            "Ben Hachey"
        ],
        "abstract": "Text generation is increasingly common but often requires manual post-editing where high precision is critical to end users. However, manual editing is expensive so we want to ensure this effort is focused on high-value tasks. And we want to maintain stylistic consistency, a particular challenge in crowd settings. We present a case study, analysing human post-editing in the context of a template-based biography generation system. An edit flow visualisation combined with manual characterisation of edits helps identify and prioritise work for improving end-to-end efficiency and accuracy.\n    ",
        "submission_date": "2017-02-20T00:00:00",
        "last_modified_date": "2017-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05962",
        "title": "Latent Variable Dialogue Models and their Diversity",
        "authors": [
            "Kris Cao",
            "Stephen Clark"
        ],
        "abstract": "We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the `boring output' issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.\n    ",
        "submission_date": "2017-02-20T00:00:00",
        "last_modified_date": "2017-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06027",
        "title": "Parent Oriented Teacher Selection Causes Language Diversity",
        "authors": [
            "Ibrahim Cimentepe",
            "Haluk O. Bingol"
        ],
        "abstract": "An evolutionary model for emergence of diversity in language is developed. We investigated the effects of two real life observations, namely, people prefer people that they communicate with well, and people interact with people that are physically close to each other. Clearly these groups are relatively small compared to the entire population. We restrict selection of the teachers from such small groups, called imitation sets, around parents. Then the child learns language from a teacher selected within the imitation set of her parent. As a result, there are subcommunities with their own languages developed. Within subcommunity comprehension is found to be high. The number of languages is related to the relative size of imitation set by a power law.\n    ",
        "submission_date": "2017-02-20T00:00:00",
        "last_modified_date": "2017-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06135",
        "title": "Enabling Multi-Source Neural Machine Translation By Concatenating Source Sentences In Multiple Languages",
        "authors": [
            "Raj Dabre",
            "Fabien Cromieres",
            "Sadao Kurohashi"
        ],
        "abstract": "In this paper, we explore a simple solution to \"Multi-Source Neural Machine Translation\" (MSNMT) which only relies on preprocessing a N-way multilingual corpus without modifying the Neural Machine Translation (NMT) architecture or training procedure. We simply concatenate the source sentences to form a single long multi-source input sentence while keeping the target side sentence as it is and train an NMT system using this preprocessed corpus. We evaluate our method in resource poor as well as resource rich settings and show its effectiveness (up to 4 BLEU using 2 source languages and up to 6 BLEU using 5 source languages). We also compare against existing methods for MSNMT and show that our solution gives competitive results despite its simplicity. We also provide some insights on how the NMT system leverages multilingual information in such a scenario by visualizing attention.\n    ",
        "submission_date": "2017-02-20T00:00:00",
        "last_modified_date": "2019-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06216",
        "title": "Filtering Tweets for Social Unrest",
        "authors": [
            "Alan Mishler",
            "Kevin Wonus",
            "Wendy Chambers",
            "Michael Bloodgood"
        ],
        "abstract": "Since the events of the Arab Spring, there has been increased interest in using social media to anticipate social unrest. While efforts have been made toward automated unrest prediction, we focus on filtering the vast volume of tweets to identify tweets relevant to unrest, which can be provided to downstream users for further analysis. We train a supervised classifier that is able to label Arabic language tweets as relevant to unrest with high reliability. We examine the relationship between training data size and performance and investigate ways to optimize the model building process while minimizing cost. We also explore how confidence thresholds can be set to achieve desired levels of performance.\n    ",
        "submission_date": "2017-02-20T00:00:00",
        "last_modified_date": "2017-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06235",
        "title": "Learning to generate one-sentence biographies from Wikidata",
        "authors": [
            "Andrew Chisholm",
            "Will Radford",
            "Ben Hachey"
        ],
        "abstract": "We investigate the generation of one-sentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06239",
        "title": "Reinforcement Learning Based Argument Component Detection",
        "authors": [
            "Yang Gao",
            "Hao Wang",
            "Chen Zhang",
            "Wei Wang"
        ],
        "abstract": "Argument component detection (ACD) is an important sub-task in argumentation mining. ACD aims at detecting and classifying different argument components in natural language texts. Historical annotations (HAs) are important features the human annotators consider when they manually perform the ACD task. However, HAs are largely ignored by existing automatic ACD techniques. Reinforcement learning (RL) has proven to be an effective method for using HAs in some natural language processing tasks. In this work, we propose a RL-based ACD technique, and evaluate its performance on two well-annotated corpora. Results suggest that, in terms of classification accuracy, HAs-augmented RL outperforms plain RL by at most 17.85%, and outperforms the state-of-the-art supervised learning algorithm by at most 11.94%.\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06336",
        "title": "Hybrid Dialog State Tracker with ASR Features",
        "authors": [
            "Miroslav Vodol\u00e1n",
            "Rudolf Kadlec",
            "Jan Kleindienst"
        ],
        "abstract": "This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slot-filling dialog systems. Our architecture is inspired by previously proposed neural-network-based belief-tracking systems. In addition, we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machine-learning based systems. For evaluation, we used the Dialog State Tracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new state-of-the-art result in three out of four categories within the DSTC2.\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06378",
        "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition",
        "authors": [
            "Liang Lu",
            "Lingpeng Kong",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling methods used for end-to-end training of speech recognition models. Both models define a transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a \"continuation\" of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder is used for feature extraction for both outputs. We find that this multitask objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model.\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2017-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06478",
        "title": "Syst\u00e8mes du LIA \u00e0 DEFT'13",
        "authors": [
            "Xavier Bost",
            "Ilaria Brunetti",
            "Luis Adri\u00e1n Cabrera-Diego",
            "Jean-Val\u00e8re Cossu",
            "Andr\u00e9a Linhares",
            "Mohamed Morchid",
            "Juan-Manuel Torres-Moreno",
            "Marc El-B\u00e8ze",
            "Richard Dufour"
        ],
        "abstract": "The 2013 D\u00e9fi de Fouille de Textes (DEFT) campaign is interested in two types of language analysis tasks, the document classification and the information extraction in the specialized domain of cuisine recipes. We present the systems that the LIA has used in DEFT 2013. Our systems show interesting results, even though the complexity of the proposed tasks.\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06589",
        "title": "Neural Multi-Step Reasoning for Question Answering on Semi-Structured Tables",
        "authors": [
            "Till Haug",
            "Octavian-Eugen Ganea",
            "Paulina Grnarova"
        ],
        "abstract": "Advances in natural language processing tasks have gained momentum in recent years due to the increasingly popular neural network methods. In this paper, we explore deep learning techniques for answering multi-step reasoning questions that operate on semi-structured tables. Challenges here arise from the level of logical compositionality expressed by questions, as well as the domain openness. Our approach is weakly supervised, trained on question-answer-table triples without requiring intermediate strong supervision. It performs two phases: first, machine understandable logical forms (programs) are generated from natural language questions following the work of [Pasupat and Liang, 2015]. Second, paraphrases of logical forms and questions are embedded in a jointly learned vector space using word and character convolutional neural networks. A neural scoring function is further used to rank and retrieve the most probable logical form (interpretation) of a question. Our best single model achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best ensemble of our models pushes the state-of-the-art score on this task to 38.7%, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of [Neelakantan et al., 2016].\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2018-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06594",
        "title": "On the Complexity of CCG Parsing",
        "authors": [
            "Marco Kuhlmann",
            "Giorgio Satta",
            "Peter Jonsson"
        ],
        "abstract": "We study the parsing complexity of Combinatory Categorial Grammar (CCG) in the formalism of Vijay-Shanker and Weir (1994). As our main result, we prove that any parsing algorithm for this formalism will take in the worst case exponential time when the size of the grammar, and not only the length of the input sentence, is included in the analysis. This sets the formalism of Vijay-Shanker and Weir (1994) apart from weakly equivalent formalisms such as Tree-Adjoining Grammar (TAG), for which parsing can be performed in time polynomial in the combined size of grammar and input sentence. Our results contribute to a refined understanding of the class of mildly context-sensitive grammars, and inform the search for new, mildly context-sensitive versions of CCG.\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2018-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06663",
        "title": "Guided Deep List: Automating the Generation of Epidemiological Line Lists from Open Sources",
        "authors": [
            "Saurav Ghosh",
            "Prithwish Chakraborty",
            "Bryan L. Lewis",
            "Maimuna S. Majumder",
            "Emily Cohn",
            "John S. Brownstein",
            "Madhav V. Marathe",
            "Naren Ramakrishnan"
        ],
        "abstract": "Real-time monitoring and responses to emerging public health threats rely on the availability of timely surveillance data. During the early stages of an epidemic, the ready availability of line lists with detailed tabular information about laboratory-confirmed cases can assist epidemiologists in making reliable inferences and forecasts. Such inferences are crucial to understand the epidemiology of a specific disease early enough to stop or control the outbreak. However, construction of such line lists requires considerable human supervision and therefore, difficult to generate in real-time. In this paper, we motivate Guided Deep List, the first tool for building automated line lists (in near real-time) from open source reports of emerging disease outbreaks. Specifically, we focus on deriving epidemiological characteristics of an emerging disease and the affected population from reports of illness. Guided Deep List uses distributed vector representations (ala word2vec) to discover a set of indicators for each line list feature. This discovery of indicators is followed by the use of dependency parsing based techniques for final extraction in tabular form. We evaluate the performance of Guided Deep List against a human annotated line list provided by HealthMap corresponding to MERS outbreaks in Saudi Arabia. We demonstrate that Guided Deep List extracts line list features with increased accuracy compared to a baseline method. We further show how these automatically extracted line list features can be used for making epidemiological inferences, such as inferring demographics and symptoms-to-hospitalization period of affected individuals.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06672",
        "title": "Calculating Probabilities Simplifies Word Learning",
        "authors": [
            "Aida Nematzadeh",
            "Barend Beekhuizen",
            "Shanshan Huang",
            "Suzanne Stevenson"
        ],
        "abstract": "Children can use the statistical regularities of their environment to learn word meanings, a mechanism known as cross-situational learning. We take a computational approach to investigate how the information present during each observation in a cross-situational framework can affect the overall acquisition of word meanings. We do so by formulating various in-the-moment learning mechanisms that are sensitive to different statistics of the environment, such as counts and conditional probabilities. Each mechanism introduces a unique source of competition or mutual exclusivity bias to the model; the mechanism that maximally uses the model's knowledge of word meanings performs the best. Moreover, the gap between this mechanism and others is amplified in more challenging learning scenarios, such as learning from few examples.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06675",
        "title": "Context-Aware Prediction of Derivational Word-forms",
        "authors": [
            "Ekaterina Vylomova",
            "Ryan Cotterell",
            "Timothy Baldwin",
            "Trevor Cohn"
        ],
        "abstract": "Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose the new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder--decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under a lexicon agnostic setting.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06696",
        "title": "One Representation per Word - Does it make Sense for Composition?",
        "authors": [
            "Thomas Kober",
            "Julie Weeds",
            "John Wilkie",
            "Jeremy Reffin",
            "David Weir"
        ],
        "abstract": "In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through composition alone. We evaluate the performance of off-the-shelf single-vector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as pointwise addition are able to recover sense specific information from a single-sense vector model remarkably well.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06703",
        "title": "Data Distillation for Controlling Specificity in Dialogue Generation",
        "authors": [
            "Jiwei Li",
            "Will Monroe",
            "Dan Jurafsky"
        ],
        "abstract": "People speak at different levels of specificity in different situations. Depending on their knowledge, interlocutors, mood, etc.} A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network--based conversational agent this ability. Our approach involves alternating between \\emph{data distillation} and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity.\n",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06709",
        "title": "Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings",
        "authors": [
            "Abhishek",
            "Ashish Anand",
            "Amit Awekar"
        ],
        "abstract": "Fine-grained entity type classification (FETC) is the task of classifying an entity mention to a broad set of types. Distant supervision paradigm is extensively used to generate training data for this task. However, generated training data assigns same set of labels to every mention of an entity without considering its local context. Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features. Our work overcomes both drawbacks. We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features. Our model treats training data as noisy and uses non-parametric variant of hinge loss function. Experiments show that the proposed model outperforms previous state-of-the-art methods on two publicly available datasets, namely FIGER (GOLD) and BBN with an average relative improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems. These approaches of transferring knowledge further improve the performance of respective models.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06733",
        "title": "Improving a Strong Neural Parser with Conjunction-Specific Features",
        "authors": [
            "Jessica Ficler",
            "Yoav Goldberg"
        ],
        "abstract": "While dependency parsers reach very high overall accuracy, some dependency relations are much harder than others. In particular, dependency parsers perform poorly in coordination construction (i.e., correctly attaching the \"conj\" relation). We extend a state-of-the-art dependency parser with conjunction-specific features, focusing on the similarity between the conjuncts head words. Training the extended parser yields an improvement in \"conj\" attachment as well as in overall dependency parsing accuracy on the Stanford dependency conversion of the Penn TreeBank.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06740",
        "title": "Improving Chinese SRL with Heterogeneous Annotations",
        "authors": [
            "Qiaolin Xia",
            "Baobao Chang",
            "Zhifang Sui"
        ],
        "abstract": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on single semantically annotated corpus. But the training data of single corpus is often limited. Meanwhile, there usually exists other semantically annotated corpora for Chinese SRL scattered across different annotation frameworks. Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In these papers, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model outperforms state-of-the-art methods.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06777",
        "title": "Dialectometric analysis of language variation in Twitter",
        "authors": [
            "Gonzalo Donoso",
            "David Sanchez"
        ],
        "abstract": "In the last few years, microblogging platforms such as Twitter have given rise to a deluge of textual data that can be used for the analysis of informal communication between millions of individuals. In this work, we propose an information-theoretic approach to geographic language variation using a corpus based on Twitter. We test our models with tens of concepts and their associated keywords detected in Spanish tweets geolocated in Spain. We employ dialectometric measures (cosine similarity and Jensen-Shannon divergence) to quantify the linguistic distance on the lexical level between cells created in a uniform grid over the map. This can be done for a single concept or in the general case taking into account an average of the considered variants. The latter permits an analysis of the dialects that naturally emerge from the data. Interestingly, our results reveal the existence of two dialect macrovarieties. The first group includes a region-specific speech spoken in small towns and rural areas whereas the second cluster encompasses cities that tend to use a more uniform variety. Since the results obtained with the two different metrics qualitatively agree, our work suggests that social media corpora can be efficiently used for dialectometric analyses.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06794",
        "title": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing",
        "authors": [
            "Minh Le",
            "Antske Fokkens"
        ],
        "abstract": "Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06875",
        "title": "Triaging Content Severity in Online Mental Health Forums",
        "authors": [
            "Arman Cohan",
            "Sydney Young",
            "Andrew Yates",
            "Nazli Goharian"
        ],
        "abstract": "Mental health forums are online communities where people express their issues and seek help from moderators and other users. In such forums, there are often posts with severe content indicating that the user is in acute distress and there is a risk of attempted self-harm. Moderators need to respond to these severe posts in a timely manner to prevent potential self-harm. However, the large volume of daily posted content makes it difficult for the moderators to locate and respond to these critical posts. We present a framework for triaging user content into four severity categories which are defined based on indications of self-harm ideation. Our models are based on a feature-rich classification framework which includes lexical, psycholinguistic, contextual and topic modeling features. Our approaches improve the state of the art in triaging the content severity in mental health forums by large margins (up to 17% improvement over the F-1 scores). Using the proposed model, we analyze the mental state of users and we show that overall, long-term users of the forum demonstrate a decreased severity of risk over time. Our analysis on the interaction of the moderators with the users further indicates that without an automatic way to identify critical content, it is indeed challenging for the moderators to provide timely response to the users in need.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06891",
        "title": "EVE: Explainable Vector Based Embedding Technique Using Wikipedia",
        "authors": [
            "M. Atif Qureshi",
            "Derek Greene"
        ],
        "abstract": "We present an unsupervised explainable word embedding technique, called EVE, which is built upon the structure of Wikipedia. The proposed model defines the dimensions of a semantic vector representing a word using human-readable labels, thereby it readily interpretable. Specifically, each vector is constructed using the Wikipedia category graph structure together with the Wikipedia article link structure. To test the effectiveness of the proposed word embedding model, we consider its usefulness in three fundamental tasks: 1) intruder detection - to evaluate its ability to identify a non-coherent vector from a list of coherent vectors, 2) ability to cluster - to evaluate its tendency to group related vectors together while keeping unrelated vectors in separate clusters, and 3) sorting relevant items first - to evaluate its ability to rank vectors (items) relevant to the query in the top order of the result. For each task, we also propose a strategy to generate a task-specific human-interpretable explanation from the model. These demonstrate the overall effectiveness of the explainable embeddings generated by EVE. Finally, we compare EVE with the Word2Vec, FastText, and GloVe embedding techniques across the three tasks, and report improvements over the state-of-the-art.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07015",
        "title": "Unsupervised Learning of Morphological Forests",
        "authors": [
            "Jiaming Luo",
            "Karthik Narasimhan",
            "Regina Barzilay"
        ],
        "abstract": "This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07046",
        "title": "Feature Generation for Robust Semantic Role Labeling",
        "authors": [
            "Travis Wolfe",
            "Mark Dredze",
            "Benjamin Van Durme"
        ],
        "abstract": "Hand-engineered feature sets are a well understood method for creating robust NLP models, but they require a lot of expertise and effort to create. In this work we describe how to automatically generate rich feature sets from simple units called featlets, requiring less engineering. Using information gain to guide the generation process, we train models which rival the state of the art on two standard Semantic Role Labeling datasets with almost no task or linguistic insight.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07071",
        "title": "Pronunciation recognition of English phonemes /\\textipa{@}/, /\u00e6/, /\\textipa{A}:/ and /\\textipa{2}/ using Formants and Mel Frequency Cepstral Coefficients",
        "authors": [
            "Keith Y. Patarroyo",
            "Vladimir Vargas-Calder\u00f3n"
        ],
        "abstract": "The Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /\\textipa{@}/, /\u00e6/, /\\textipa{A}:/ and /\\textipa{2}/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70\\% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 \\% was achieved and an accuracy of 71\\% when /\\textipa{@}/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does.\n    ",
        "submission_date": "2017-02-23T00:00:00",
        "last_modified_date": "2017-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07092",
        "title": "A Neural Attention Model for Categorizing Patient Safety Events",
        "authors": [
            "Arman Cohan",
            "Allan Fong",
            "Nazli Goharian",
            "Raj Ratwani"
        ],
        "abstract": "Medical errors are leading causes of death in the US and as such, prevention of these errors is paramount to promoting health care. Patient Safety Event reports are narratives describing potential adverse events to the patients and are important in identifying and preventing medical errors. We present a neural network architecture for identifying the type of safety events which is the first step in understanding these narratives. Our proposed model is based on a soft neural attention model to improve the effectiveness of encoding long sequences. Empirical results on two large-scale real-world datasets of patient safety reports demonstrate the effectiveness of our method with significant improvements over existing methods.\n    ",
        "submission_date": "2017-02-23T00:00:00",
        "last_modified_date": "2017-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07117",
        "title": "LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations",
        "authors": [
            "Jarvan Law",
            "Hankz Hankui Zhuo",
            "Junhua He",
            "Erhu Rong"
        ],
        "abstract": "Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks.\n    ",
        "submission_date": "2017-02-23T00:00:00",
        "last_modified_date": "2017-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07203",
        "title": "Utilizing Lexical Similarity between Related, Low-resource Languages for Pivot-based SMT",
        "authors": [
            "Anoop Kunchukuttan",
            "Maulik Shah",
            "Pradyot Prakash",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "We investigate pivot-based translation between related languages in a low resource, phrase-based SMT setting. We show that a subword-level pivot-based SMT model using a related pivot language is substantially better than word and morpheme-level pivot models. It is also highly competitive with the best direct translation model, which is encouraging as no direct source-target training corpus is used. We also show that combining multiple related language pivot models can rival a direct translation model. Thus, the use of subwords as translation units coupled with multiple related pivot languages can compensate for the lack of a direct parallel corpus.\n    ",
        "submission_date": "2017-02-23T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07285",
        "title": "Are Emojis Predictable?",
        "authors": [
            "Francesco Barbieri",
            "Miguel Ballesteros",
            "Horacio Saggion"
        ],
        "abstract": "Emojis are ideograms which are naturally combined with plain text to visually complement or condense the meaning of a message. Despite being widely used in social media, their underlying semantics have received little attention from a Natural Language Processing standpoint. In this paper, we investigate the relation between words and emojis, studying the novel task of predicting which emojis are evoked by text-based tweet messages. We train several models based on Long Short-Term Memory networks (LSTMs) in this task. Our experimental results show that our neural model outperforms two baselines as well as humans solving the same task, suggesting that computational models are able to better capture the underlying semantics of emojis.\n    ",
        "submission_date": "2017-02-23T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07324",
        "title": "Inherent Biases of Recurrent Neural Networks for Phonological Assimilation and Dissimilation",
        "authors": [
            "Amanda Doucette"
        ],
        "abstract": "A recurrent neural network model of phonological pattern learning is proposed. The model is a relatively simple neural network with one recurrent layer, and displays biases in learning that mimic observed biases in human learning. Single-feature patterns are learned faster than two-feature patterns, and vowel or consonant-only patterns are learned faster than patterns involving vowels and consonants, mimicking the results of laboratory learning experiments. In non-recurrent models, capturing these biases requires the use of alpha features or some other representation of repeated features, but with a recurrent neural network, these elaborations are not necessary.\n    ",
        "submission_date": "2017-02-23T00:00:00",
        "last_modified_date": "2017-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07495",
        "title": "Dirichlet-vMF Mixture Model",
        "authors": [
            "Shaohua Li"
        ],
        "abstract": "This document is about the multi-document Von-Mises-Fisher mixture model with a Dirichlet prior, referred to as VMFMix. VMFMix is analogous to Latent Dirichlet Allocation (LDA) in that they can capture the co-occurrence patterns acorss multiple documents. The difference is that in VMFMix, the topic-word distribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix is used to derive topic embeddings, i.e., representative vectors, from multiple sets of embedding vectors. An efficient Variational Expectation-Maximization inference algorithm is derived. The performance of VMFMix on two document classification tasks is reported, with some preliminary analysis.\n    ",
        "submission_date": "2017-02-24T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07507",
        "title": "Use Generalized Representations, But Do Not Forget Surface Features",
        "authors": [
            "Nafise Sadat Moosavi",
            "Michael Strube"
        ],
        "abstract": "Only a year ago, all state-of-the-art coreference resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using word embeddings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving coreference resolution.\n    ",
        "submission_date": "2017-02-24T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07680",
        "title": "Consistent Alignment of Word Embedding Models",
        "authors": [
            "Cem Safak Sahin",
            "Rajmonda S. Caceres",
            "Brandon Oselio",
            "William M. Campbell"
        ],
        "abstract": "Word embedding models offer continuous vector representations that can capture rich contextual semantics based on their word co-occurrence patterns. While these word vectors can provide very effective features used in many NLP tasks such as clustering similar words and inferring learning relationships, many challenges and open research questions remain. In this paper, we propose a solution that aligns variations of the same model (or different models) in a joint low-dimensional latent space leveraging carefully generated synthetic data points. This generative process is inspired by the observation that a variety of linguistic relationships is captured by simple linear operations in embedded space. We demonstrate that our approach can lead to substantial improvements in recovering embeddings of local neighborhoods.\n    ",
        "submission_date": "2017-02-24T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07717",
        "title": "When confidence and competence collide: Effects on online decision-making discussions",
        "authors": [
            "Liye Fu",
            "Lillian Lee",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Group discussions are a way for individuals to exchange ideas and arguments in order to reach better decisions than they could on their own. One of the premises of productive discussions is that better solutions will prevail, and that the idea selection process is mediated by the (relative) competence of the individuals involved. However, since people may not know their actual competence on a new task, their behavior is influenced by their self-estimated competence --- that is, their confidence --- which can be misaligned with their actual competence.\n",
        "submission_date": "2017-02-24T00:00:00",
        "last_modified_date": "2017-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07793",
        "title": "Residual Convolutional CTC Networks for Automatic Speech Recognition",
        "authors": [
            "Yisen Wang",
            "Xuejiao Deng",
            "Songbai Pu",
            "Zhiheng Huang"
        ],
        "abstract": "Deep learning approaches have been widely used in Automatic Speech Recognition (ASR) and they have achieved a significant accuracy improvement. Especially, Convolutional Neural Networks (CNNs) have been revisited in ASR recently. However, most CNNs used in existing work have less than 10 layers which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and wide CNN architecture denoted as RCNN-CTC, which has residual connections and Connectionist Temporal Classification (CTC) loss function. RCNN-CTC is an end-to-end system which can exploit temporal and spectral structures of speech signals simultaneously. Furthermore, we introduce a CTC-based system combination, which is different from the conventional frame-wise senone-based one. The basic subsystems adopted in the combination are different types and thus mutually complementary to each other. Experimental results show that our proposed single system RCNN-CTC can achieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets, compared to several widely used neural network systems in ASR. In addition, the proposed system combination can offer a further error reduction on these two data sets, resulting in relative WER reductions of $14.91\\%$ and $6.52\\%$ on WSJ dev93 and Tencent Chat data sets respectively.\n    ",
        "submission_date": "2017-02-24T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07825",
        "title": "Deep Voice: Real-time Neural Text-to-Speech",
        "authors": [
            "Sercan O. Arik",
            "Mike Chrzanowski",
            "Adam Coates",
            "Gregory Diamos",
            "Andrew Gibiansky",
            "Yongguo Kang",
            "Xian Li",
            "John Miller",
            "Andrew Ng",
            "Jonathan Raiman",
            "Shubho Sengupta",
            "Mohammad Shoeybi"
        ],
        "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.\n    ",
        "submission_date": "2017-02-25T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07835",
        "title": "Critical Survey of the Freely Available Arabic Corpora",
        "authors": [
            "Wajdi Zaghouani"
        ],
        "abstract": "The availability of corpora is a major factor in building natural language processing applications. However, the costs of acquiring corpora can prevent some researchers from going further in their endeavours. The ease of access to freely available corpora is urgent needed in the NLP research community especially for language such as Arabic. Currently, there is not easy was to access to a comprehensive and updated list of freely available Arabic corpora. We present in this paper, the results of a recent survey conducted to identify the list of the freely available Arabic corpora and language resources. Our preliminary results showed an initial list of 66 sources. We presents our findings in the various categories studied and we provided the direct links to get the data when possible.\n    ",
        "submission_date": "2017-02-25T00:00:00",
        "last_modified_date": "2017-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07998",
        "title": "Detecting (Un)Important Content for Single-Document News Summarization",
        "authors": [
            "Yinfei Yang",
            "Forrest Sheng Bao",
            "Ani Nenkova"
        ],
        "abstract": "We present a robust approach for detecting intrinsic sentence importance in news, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the \"beginning of document\" heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline.\n    ",
        "submission_date": "2017-02-26T00:00:00",
        "last_modified_date": "2017-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08021",
        "title": "Friends and Enemies of Clinton and Trump: Using Context for Detecting Stance in Political Tweets",
        "authors": [
            "Mirko Lai",
            "Delia Iraz\u00fa Hern\u00e1ndez Far\u00edas",
            "Viviana Patti",
            "Paolo Rosso"
        ],
        "abstract": "Stance detection, the task of identifying the speaker's opinion towards a particular target, has attracted the attention of researchers. This paper describes a novel approach for detecting stance in Twitter. We define a set of features in order to consider the context surrounding a target of interest with the final aim of training a model for predicting the stance towards the mentioned targets. In particular, we are interested in investigating political debates in social media. For this reason we evaluated our approach focusing on two targets of the SemEval-2016 Task6 on Detecting stance in tweets, which are related to the political campaign for the 2016 U.S. presidential elections: Hillary Clinton vs. Donald Trump. For the sake of comparison with the state of the art, we evaluated our model against the dataset released in the SemEval-2016 Task 6 shared task competition. Our results outperform the best ones obtained by participating teams, and show that information about enemies and friends of politicians help in detecting stance towards them.\n    ",
        "submission_date": "2017-02-26T00:00:00",
        "last_modified_date": "2017-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08217",
        "title": "A case study on English-Malayalam Machine Translation",
        "authors": [
            "Sreelekha S",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "In this paper we present our work on a case study on Statistical Machine Translation (SMT) and Rule based machine translation (RBMT) for translation from English to Malayalam and Malayalam to English. One of the motivations of our study is to make a three way performance comparison, such as, a) SMT and RBMT b) English to Malayalam SMT and Malayalam to English SMT c) English to Malayalam RBMT and Malayalam to English RBMT. We describe the development of English to Malayalam and Malayalam to English baseline phrase based SMT system and the evaluation of its performance compared against the RBMT system. Based on our study the observations are: a) SMT systems outperform RBMT systems, b) In the case of SMT, English - Malayalam systems perform better than that of Malayalam - English systems, c) In the case RBMT, Malayalam to English systems are performing better than English to Malayalam systems. Based on our evaluations and detailed error analysis, we describe the requirements of incorporating morphological processing into the SMT to improve the accuracy of translation.\n    ",
        "submission_date": "2017-02-27T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08303",
        "title": "Identifying beneficial task relations for multi-task learning in deep neural networks",
        "authors": [
            "Joachim Bingel",
            "Anders S\u00f8gaard"
        ],
        "abstract": "Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.\n    ",
        "submission_date": "2017-02-27T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08388",
        "title": "Political Homophily in Independence Movements: Analysing and Classifying Social Media Users by National Identity",
        "authors": [
            "Arkaitz Zubiaga",
            "Bo Wang",
            "Maria Liakata",
            "Rob Procter"
        ],
        "abstract": "Social media and data mining are increasingly being used to analyse political and societal issues. Here we undertake the classification of social media users as supporting or opposing ongoing independence movements in their territories. Independence movements occur in territories whose citizens have conflicting national identities; users with opposing national identities will then support or oppose the sense of being part of an independent nation that differs from the officially recognised country. We describe a methodology that relies on users' self-reported location to build large-scale datasets for three territories -- Catalonia, the Basque Country and Scotland. An analysis of these datasets shows that homophily plays an important role in determining who people connect with, as users predominantly choose to follow and interact with others from the same national identity. We show that a classifier relying on users' follow networks can achieve accurate, language-independent classification performances ranging from 85% to 97% for the three territories.\n    ",
        "submission_date": "2017-02-27T00:00:00",
        "last_modified_date": "2018-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08450",
        "title": "A Knowledge-Based Approach to Word Sense Disambiguation by distributional selection and semantic features",
        "authors": [
            "Mokhtar Billami"
        ],
        "abstract": "Word sense disambiguation improves many Natural Language Processing (NLP) applications such as Information Retrieval, Information Extraction, Machine Translation, or Lexical Simplification. Roughly speaking, the aim is to choose for each word in a text its best sense. One of the most popular method estimates local semantic similarity relatedness between two word senses and then extends it to all words from text. The most direct method computes a rough score for every pair of word senses and chooses the lexical chain that has the best score (we can imagine the exponential complexity that returns this comprehensive approach). In this paper, we propose to use a combinatorial optimization metaheuristic for choosing the nearest neighbors obtained by distributional selection around the word to disambiguate. The test and the evaluation of our method concern a corpus written in French by means of the semantic network BabelNet. The obtained accuracy rate is 78 % on all names and verbs chosen for the evaluation.\n    ",
        "submission_date": "2017-02-27T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08451",
        "title": "Approches d'analyse distributionnelle pour am\u00e9liorer la d\u00e9sambigu\u00efsation s\u00e9mantique",
        "authors": [
            "Mokhtar Billami",
            "N\u00faria Gala"
        ],
        "abstract": "Word sense disambiguation (WSD) improves many Natural Language Processing (NLP) applications such as Information Retrieval, Machine Translation or Lexical Simplification. WSD is the ability of determining a word sense among different ones within a polysemic lexical unit taking into account the context. The most straightforward approach uses a semantic proximity measure between the word sense candidates of the target word and those of its context. Such a method very easily entails a combinatorial explosion. In this paper, we propose two methods based on distributional analysis which enable to reduce the exponential complexity without losing the coherence. We present a comparison between the selection of distributional neighbors and the linearly nearest neighbors. The figures obtained show that selecting distributional neighbors leads to better results.\n    ",
        "submission_date": "2017-02-27T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08563",
        "title": "Soft Label Memorization-Generalization for Natural Language Inference",
        "authors": [
            "John P. Lalor",
            "Hao Wu",
            "Hong Yu"
        ],
        "abstract": "Often when multiple labels are obtained for a training example it is assumed that there is an element of noise that must be accounted for. It has been shown that this disagreement can be considered signal instead of noise. In this work we investigate using soft labels for training data to improve generalization in machine learning models. However, using soft labels for training Deep Neural Networks (DNNs) is not practical due to the costs involved in obtaining multiple labels for large data sets. We propose soft label memorization-generalization (SLMG), a fine-tuning approach to using soft labels for training DNNs. We assume that differences in labels provided by human annotators represent ambiguity about the true label instead of noise. Experiments with SLMG demonstrate improved generalization performance on the Natural Language Inference (NLI) task. Our experiments show that by injecting a small percentage of soft label training data (0.03% of training set size) we can improve generalization performance over several baselines.\n    ",
        "submission_date": "2017-02-27T00:00:00",
        "last_modified_date": "2019-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08653",
        "title": "Scaffolding Networks: Incremental Learning and Teaching Through Questioning",
        "authors": [
            "Asli Celikyilmaz",
            "Li Deng",
            "Lihong Li",
            "Chong Wang"
        ],
        "abstract": "We introduce a new paradigm of learning for reasoning, understanding, and prediction, as well as the scaffolding network to implement this paradigm. The scaffolding network embodies an incremental learning approach that is formulated as a teacher-student network architecture to teach machines how to understand text and do reasoning. The key to our computational scaffolding approach is the interactions between the teacher and the student through sequential questioning. The student observes each sentence in the text incrementally, and it uses an attention-based neural net to discover and register the key information in relation to its current memory. Meanwhile, the teacher asks questions about the observed text, and the student network gets rewarded by correctly answering these questions. The entire network is updated continually using reinforcement learning. Our experimental results on synthetic and real datasets show that the scaffolding network not only outperforms state-of-the-art methods but also learns to do reasoning in a scalable way even with little human generated input.\n    ",
        "submission_date": "2017-02-28T00:00:00",
        "last_modified_date": "2017-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08866",
        "title": "Studying Positive Speech on Twitter",
        "authors": [
            "Marina Sokolova",
            "Vera Sazonova",
            "Kanyi Huang",
            "Rudraneel Chakraboty",
            "Stan Matwin"
        ],
        "abstract": "We present results of empirical studies on positive speech on Twitter. By positive speech we understand speech that works for the betterment of a given situation, in this case relations between different communities in a conflict-prone country. We worked with four Twitter data sets. Through semi-manual opinion mining, we found that positive speech accounted for < 1% of the data . In fully automated studies, we tested two approaches: unsupervised statistical analysis, and supervised text classification based on distributed word representation. We discuss benefits and challenges of those approaches and report empirical evidence obtained in the study.\n    ",
        "submission_date": "2017-02-24T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00089",
        "title": "A Joint Identification Approach for Argumentative Writing Revisions",
        "authors": [
            "Fan Zhang",
            "Diane Litman"
        ],
        "abstract": "Prior work on revision identification typically uses a pipeline method: revision extraction is first conducted to identify the locations of revisions and revision classification is then conducted on the identified revisions. Such a setting propagates the errors of the revision extraction step to the revision classification step. This paper proposes an approach that identifies the revision location and the revision type jointly to solve the issue of error propagation. It utilizes a sequence representation of revisions and conducts sequence labeling for revision identification. A mutation-based approach is utilized to update identification sequences. Results demonstrate that our proposed approach yields better performance on both revision location extraction and revision type classification compared to a pipeline baseline.\n    ",
        "submission_date": "2017-02-28T00:00:00",
        "last_modified_date": "2017-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00096",
        "title": "Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling",
        "authors": [
            "Hairong Liu",
            "Zhenyao Zhu",
            "Xiangang Li",
            "Sanjeev Satheesh"
        ],
        "abstract": "Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.\n    ",
        "submission_date": "2017-03-01T00:00:00",
        "last_modified_date": "2017-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00099",
        "title": "Learning Conversational Systems that Interleave Task and Non-Task Content",
        "authors": [
            "Zhou Yu",
            "Alan W Black",
            "Alexander I. Rudnicky"
        ],
        "abstract": "Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave non-task content (i.e. everyday social conversation) into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system.\n    ",
        "submission_date": "2017-03-01T00:00:00",
        "last_modified_date": "2017-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00317",
        "title": "Tracing Linguistic Relations in Winning and Losing Sides of Explicit Opposing Groups",
        "authors": [
            "Ceyda Sanli",
            "Anupam Mondal",
            "Erik Cambria"
        ],
        "abstract": "Linguistic relations in oral conversations present how opinions are constructed and developed in a restricted time. The relations bond ideas, arguments, thoughts, and feelings, re-shape them during a speech, and finally build knowledge out of all information provided in the conversation. Speakers share a common interest to discuss. It is expected that each speaker's reply includes duplicated forms of words from previous speakers. However, linguistic adaptation is observed and evolves in a more complex path than just transferring slightly modified versions of common concepts. A conversation aiming a benefit at the end shows an emergent cooperation inducing the adaptation. Not only cooperation, but also competition drives the adaptation or an opposite scenario and one can capture the dynamic process by tracking how the concepts are linguistically linked. To uncover salient complex dynamic events in verbal communications, we attempt to discover self-organized linguistic relations hidden in a conversation with explicitly stated winners and losers. We examine open access data of the United States Supreme Court. Our understanding is crucial in big data research to guide how transition states in opinion mining and decision-making should be modeled and how this required knowledge to guide the model should be pinpointed, by filtering large amount of data.\n    ",
        "submission_date": "2017-03-01T00:00:00",
        "last_modified_date": "2017-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00538",
        "title": "Unsupervised Ensemble Ranking of Terms in Electronic Health Record Notes Based on Their Importance to Patients",
        "authors": [
            "Jinying Chen",
            "Hong Yu"
        ],
        "abstract": "Background: Electronic health record (EHR) notes contain abundant medical jargon that can be difficult for patients to comprehend. One way to help patients is to reduce information overload and help them focus on medical terms that matter most to them.\n",
        "submission_date": "2017-03-01T00:00:00",
        "last_modified_date": "2017-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00565",
        "title": "Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ",
        "authors": [
            "Jason S. Kessler"
        ],
        "abstract": "Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics.\n    ",
        "submission_date": "2017-03-02T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00572",
        "title": "Structural Embedding of Syntactic Trees for Machine Comprehension",
        "authors": [
            "Rui Liu",
            "Junjie Hu",
            "Wei Wei",
            "Zi Yang",
            "Eric Nyberg"
        ],
        "abstract": "Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.\n    ",
        "submission_date": "2017-03-02T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00607",
        "title": "Dynamic Word Embeddings for Evolving Semantic Discovery",
        "authors": [
            "Zijun Yao",
            "Yifan Sun",
            "Weicong Ding",
            "Nikhil Rao",
            "Hui Xiong"
        ],
        "abstract": "Word evolution refers to the changing meanings and associations of words throughout time, as a byproduct of human language evolution. By studying word evolution, we can infer social trends and language constructs over different periods of human history. However, traditional techniques such as word representation learning do not adequately capture the evolving language structure and vocabulary. In this paper, we develop a dynamic statistical model to learn time-aware word vector representation. We propose a model that simultaneously learns time-aware embeddings and solves the resulting \"alignment problem\". This model is trained on a crawled NYTimes dataset. Additionally, we develop multiple intuitive evaluation strategies of temporal word embeddings. Our qualitative and quantitative tests indicate that our method not only reliably captures this evolution over time, but also consistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality.\n    ",
        "submission_date": "2017-03-02T00:00:00",
        "last_modified_date": "2018-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00782",
        "title": "Lock-Free Parallel Perceptron for Graph-based Dependency Parsing",
        "authors": [
            "Xu Sun",
            "Shuming Ma"
        ],
        "abstract": "Dependency parsing is an important NLP task. A popular approach for dependency parsing is structured perceptron. Still, graph-based dependency parsing has the time complexity of $O(n^3)$, and it suffers from slow training. To deal with this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can make full use of a multi-core computer which saves a lot of training time. Based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads, and with no loss at all in accuracy.\n    ",
        "submission_date": "2017-03-02T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00786",
        "title": "A Generic Online Parallel Learning Framework for Large Margin Models",
        "authors": [
            "Shuming Ma",
            "Xu Sun"
        ],
        "abstract": "To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent (SGD) instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy.\n    ",
        "submission_date": "2017-03-02T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00993",
        "title": "A Comparative Study of Word Embeddings for Reading Comprehension",
        "authors": [
            "Bhuwan Dhingra",
            "Hanxiao Liu",
            "Ruslan Salakhutdinov",
            "William W. Cohen"
        ],
        "abstract": "The focus of past machine learning research for Reading Comprehension tasks has been primarily on the design of novel deep learning architectures. Here we show that seemingly minor choices made on (1) the use of pre-trained word embeddings, and (2) the representation of out-of-vocabulary tokens at test time, can turn out to have a larger impact than architectural choices on the final performance. We systematically explore several options for these choices, and provide recommendations to researchers working in this area.\n    ",
        "submission_date": "2017-03-02T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01008",
        "title": "End-to-End Task-Completion Neural Dialogue Systems",
        "authors": [
            "Xiujun Li",
            "Yun-Nung Chen",
            "Lihong Li",
            "Jianfeng Gao",
            "Asli Celikyilmaz"
        ],
        "abstract": "One of the major drawbacks of modularized task-completion dialogue systems is that each module is trained individually, which presents several challenges. For example, downstream modules are affected by earlier modules, and the performance of the entire system is not robust to the accumulated errors. This paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues. Our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation, but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module.\n    ",
        "submission_date": "2017-03-03T00:00:00",
        "last_modified_date": "2018-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01024",
        "title": "Exponential Moving Average Model in Parallel Speech Recognition Training",
        "authors": [
            "Xu Tian",
            "Jun Zhang",
            "Zejun Ma",
            "Yi He",
            "Juan Wei"
        ],
        "abstract": "As training data rapid growth, large-scale parallel training with multi-GPUs cluster is widely applied in the neural network model learning ",
        "submission_date": "2017-03-03T00:00:00",
        "last_modified_date": "2017-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01485",
        "title": "Lexical Resources for Hindi Marathi MT",
        "authors": [
            "Sreelekha S",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "In this paper we describe some ways to utilize various lexical resources to improve the quality of statistical machine translation system. We have augmented the training corpus with various lexical resources such as IndoWordnet semantic relation set, function words, kridanta pairs and verb phrases etc. Our research on the usage of lexical resources mainly focused on two ways such as augmenting parallel corpus with more vocabulary and augmenting with various word forms. We have described case studies, evaluations and detailed error analysis for both Marathi to Hindi and Hindi to Marathi machine translation systems. From the evaluations we observed that, there is an incremental growth in the quality of machine translation as the usage of various lexical resources increases. Moreover usage of various lexical resources helps to improve the coverage and quality of machine translation where limited parallel corpus is available.\n    ",
        "submission_date": "2017-03-04T00:00:00",
        "last_modified_date": "2017-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01619",
        "title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial",
        "authors": [
            "Graham Neubig"
        ],
        "abstract": "This tutorial introduces a new and powerful set of techniques variously called \"neural machine translation\" or \"neural sequence-to-sequence models\". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice.\n    ",
        "submission_date": "2017-03-05T00:00:00",
        "last_modified_date": "2017-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01694",
        "title": "Word forms - not just their lengths- are optimized for efficient communication",
        "authors": [
            "Stephan C. Meylan",
            "Thomas L. Griffiths"
        ],
        "abstract": "The inverse relationship between the length of a word and the frequency of its use, first identified by G.K. Zipf in 1935, is a classic empirical law that holds across a wide range of human languages. We demonstrate that length is one aspect of a much more general property of words: how distinctive they are with respect to other words in a language. Distinctiveness plays a critical role in recognizing words in fluent speech, in that it reflects the strength of potential competitors when selecting the best candidate for an ambiguous signal. Phonological information content, a measure of a word's string probability under a statistical model of a language's sound or character sequences, concisely captures distinctiveness. Examining large-scale corpora from 13 languages, we find that distinctiveness significantly outperforms word length as a predictor of frequency. This finding provides evidence that listeners' processing constraints shape fine-grained aspects of word forms across languages.\n    ",
        "submission_date": "2017-03-06T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01720",
        "title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds",
        "authors": [
            "Ashwin K Vijayakumar",
            "Ramakrishna Vedantam",
            "Devi Parikh"
        ],
        "abstract": "To be able to interact better with humans, it is crucial for machines to understand sound - a primary modality of human perception. Previous works have used sound to learn embeddings for improved generic textual similarity assessment. In this work, we treat sound as a first-class citizen, studying downstream textual tasks which require aural grounding. To this end, we propose sound-word2vec - a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering foley sound effects (used in movies). Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.\n    ",
        "submission_date": "2017-03-06T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01726",
        "title": "A Novel Comprehensive Approach for Estimating Concept Semantic Similarity in WordNet",
        "authors": [
            "Xiao-gang Zhang",
            "Shou-qian Sun",
            "Ke-jun Zhang"
        ],
        "abstract": "Computation of semantic similarity between concepts is an important foundation for many research works. This paper focuses on IC computing methods and IC measures, which estimate the semantic similarities between concepts by exploiting the topological parameters of the taxonomy. Based on analyzing representative IC computing methods and typical semantic similarity measures, we propose a new hybrid IC computing method. Through adopting the parameter dhyp and lch, we utilize the new IC computing method and propose a novel comprehensive measure of semantic similarity between concepts. An experiment based on WordNet \"is a\" taxonomy has been designed to test representative measures and our measure on benchmark dataset R&G, and the results show that our measure can obviously improve the similarity accuracy. We evaluate the proposed approach by comparing the correlation coefficients between five measures and the artificial data. The results show that our proposal outperforms the previous measures.\n    ",
        "submission_date": "2017-03-06T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02019",
        "title": "Performing Stance Detection on Twitter Data using Computational Linguistics Techniques",
        "authors": [
            "Gourav G. Shenoy",
            "Erika H. Dsouza",
            "Sandra K\u00fcbler"
        ],
        "abstract": "As humans, we can often detect from a persons utterances if he or she is in favor of or against a given target entity (topic, product, another person, etc). But from the perspective of a computer, we need means to automatically deduce the stance of the tweeter, given just the tweet text. In this paper, we present our results of performing stance detection on twitter data using a supervised approach. We begin by extracting bag-of-words to perform classification using TIMBL, then try and optimize the features to improve stance detection accuracy, followed by extending the dataset with two sets of lexicons - arguing, and MPQA subjectivity; next we explore the MALT parser and construct features using its dependency triples, finally we perform analysis using Scikit-learn Random Forest implementation.\n    ",
        "submission_date": "2017-03-06T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02031",
        "title": "Random vector generation of a semantic space",
        "authors": [
            "Jean-Fran\u00e7ois Delpech",
            "Sabine Ploux"
        ],
        "abstract": "We show how random vectors and random projection can be implemented in the usual vector space model to construct a Euclidean semantic space from a French synonym dictionary. We evaluate theoretically the resulting noise and show the experimental distribution of the similarities of terms in a neighborhood according to the choice of parameters. We also show that the Schmidt orthogonalization process is applicable and can be used to separate homonyms with distinct semantic meanings. Neighboring terms are easily arranged into semantically significant clusters which are well suited to the generation of realistic lists of synonyms and to such applications as word selection for automatic text generation. This process, applicable to any language, can easily be extended to collocations, is extremely fast and can be updated in real time, whenever new synonyms are proposed.\n    ",
        "submission_date": "2017-03-05T00:00:00",
        "last_modified_date": "2017-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02136",
        "title": "English Conversational Telephone Speech Recognition by Humans and Machines",
        "authors": [
            "George Saon",
            "Gakuto Kurata",
            "Tom Sercu",
            "Kartik Audhkhasi",
            "Samuel Thomas",
            "Dimitrios Dimitriadis",
            "Xiaodong Cui",
            "Bhuvana Ramabhadran",
            "Michael Picheny",
            "Lynn-Li Lim",
            "Bergul Roomi",
            "Phil Hall"
        ],
        "abstract": "One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.\n    ",
        "submission_date": "2017-03-06T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02166",
        "title": "Building a Syllable Database to Solve the Problem of Khmer Word Segmentation",
        "authors": [
            "Nam Tran Van"
        ],
        "abstract": "Word segmentation is a basic problem in natural language processing. With the languages having the complex writing system like the Khmer language in Southern of Vietnam, this problem really very intractable, posing the significant challenges. Although there are some experts in Vietnam as well as international having deeply researched this problem, there are still no reasonable results meeting the demand, in particular, no treated thoroughly the ambiguous phenomenon, in the process of Khmer language processing so far. This paper present a solution based on the syllable division into component clusters using two syllable models proposed, thereby building a Khmer syllable database, is still not actually available. This method using a lexical database updated from the online Khmer dictionaries and some supported dictionaries serving role of training data and complementary linguistic characteristics. Each component cluster is labelled and located by the first and last letter to identify entirety a syllable. This approach is workable and the test results achieve high accuracy, eliminate the ambiguity, contribute to solving the problem of word segmentation and applying efficiency in Khmer language processing.\n    ",
        "submission_date": "2017-03-07T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02504",
        "title": "Leveraging Large Amounts of Weakly Supervised Data for Multi-Language Sentiment Classification",
        "authors": [
            "Jan Deriu",
            "Aurelien Lucchi",
            "Valeria De Luca",
            "Aliaksei Severyn",
            "Simon M\u00fcller",
            "Mark Cieliebak",
            "Thomas Hofmann",
            "Martin Jaggi"
        ],
        "abstract": "This paper presents a novel approach for multi-lingual sentiment classification in short texts. This is a challenging task as the amount of training data in languages other than English is very limited. Previously proposed multi-lingual approaches typically require to establish a correspondence to English for which powerful classifiers are already available. In contrast, our method does not require such supervision. We leverage large amounts of weakly-supervised data in various languages to train a multi-layer convolutional network and demonstrate the importance of using pre-training of such networks. We thoroughly evaluate our approach on various multi-lingual datasets, including the recent SemEval-2016 sentiment prediction benchmark (Task 4), where we achieved state-of-the-art performance. We also compare the performance of our model trained individually for each language to a variant trained for all languages at once. We show that the latter model reaches slightly worse - but still acceptable - performance when compared to the single language model, while benefiting from better generalization properties across languages.\n    ",
        "submission_date": "2017-03-07T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02507",
        "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features",
        "authors": [
            "Matteo Pagliardini",
            "Prakhar Gupta",
            "Martin Jaggi"
        ],
        "abstract": "The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.\n    ",
        "submission_date": "2017-03-07T00:00:00",
        "last_modified_date": "2018-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02517",
        "title": "Learning opacity in Stratal Maximum Entropy Grammar",
        "authors": [
            "Aleksei Nazarov",
            "Joe Pater"
        ],
        "abstract": "Opaque phonological patterns are sometimes claimed to be difficult to learn; specific hypotheses have been advanced about the relative difficulty of particular kinds of opaque processes (Kiparsky 1971, 1973), and the kind of data that will be helpful in learning an opaque pattern (Kiparsky 2000). In this paper, we present a computationally implemented learning theory for one grammatical theory of opacity: a Maximum Entropy version of Stratal OT (Berm\u00fadez-Otero 1999, Kiparsky 2000), and test it on simplified versions of opaque French tense-lax vowel alternations and the opaque interaction of diphthong raising and flapping in Canadian English. We find that the difficulty of opacity can be influenced by evidence for stratal affiliation: the Canadian English case is easier if the learner encounters application of raising outside the flapping context, or non-application of raising between words (i.e., <life> with a raised vowel; <lie for> with a non-raised vowel).\n    ",
        "submission_date": "2017-03-07T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02620",
        "title": "Linguistic Knowledge as Memory for Recurrent Neural Networks",
        "authors": [
            "Bhuwan Dhingra",
            "Zhilin Yang",
            "William W. Cohen",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs. We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text. We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task. Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document.\n    ",
        "submission_date": "2017-03-07T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02859",
        "title": "A World of Difference: Divergent Word Interpretations among People",
        "authors": [
            "Tianran Hu",
            "Ruihua Song",
            "Maya Abtahian",
            "Philip Ding",
            "Xing Xie",
            "Jiebo Luo"
        ],
        "abstract": "Divergent word usages reflect differences among people. In this paper, we present a novel angle for studying word usage divergence -- word interpretations. We propose an approach that quantifies semantic differences in interpretations among different groups of people. The effectiveness of our approach is validated by quantitative evaluations. Experiment results indicate that divergences in word interpretations exist. We further apply the approach to two well studied types of differences between people -- gender and region. The detected words with divergent interpretations reveal the unique features of specific groups of people. For gender, we discover that certain different interests, social attitudes, and characters between males and females are reflected in their divergent interpretations of many words. For region, we find that specific interpretations of certain words reveal the geographical and cultural features of different regions.\n    ",
        "submission_date": "2017-03-08T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02860",
        "title": "Spice up Your Chat: The Intentions and Sentiment Effects of Using Emoji",
        "authors": [
            "Tianran Hu",
            "Han Guo",
            "Hao Sun",
            "Thuy-vy Thi Nguyen",
            "Jiebo Luo"
        ],
        "abstract": "Emojis, as a new way of conveying nonverbal cues, are widely adopted in computer-mediated communications. In this paper, first from a message sender perspective, we focus on people's motives in using four types of emojis -- positive, neutral, negative, and non-facial. We compare the willingness levels of using these emoji types for seven typical intentions that people usually apply nonverbal cues for in communication. The results of extensive statistical hypothesis tests not only report the popularities of the intentions, but also uncover the subtle differences between emoji types in terms of intended uses. Second, from a perspective of message recipients, we further study the sentiment effects of emojis, as well as their duplications, on verbal messages. Different from previous studies in emoji sentiment, we study the sentiments of emojis and their contexts as a whole. The experiment results indicate that the powers of conveying sentiment are different between four emoji types, and the sentiment effects of emojis vary in the contexts of different valences.\n    ",
        "submission_date": "2017-03-08T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03091",
        "title": "Deep Learning applied to NLP",
        "authors": [
            "Marc Moreno Lopez",
            "Jugal Kalita"
        ],
        "abstract": "Convolutional Neural Network (CNNs) are typically associated with Computer Vision. CNNs are responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently CNNs have been applied to problems in Natural Language Processing and gotten some interesting results. In this paper, we will try to explain the basics of CNNs, its different variations and how they have been applied to NLP.\n    ",
        "submission_date": "2017-03-09T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03097",
        "title": "Information Extraction in Illicit Domains",
        "authors": [
            "Mayank Kejriwal",
            "Pedro Szekely"
        ],
        "abstract": "Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have `long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18\\% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment.\n    ",
        "submission_date": "2017-03-09T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03130",
        "title": "A Structured Self-attentive Sentence Embedding",
        "authors": [
            "Zhouhan Lin",
            "Minwei Feng",
            "Cicero Nogueira dos Santos",
            "Mo Yu",
            "Bing Xiang",
            "Bowen Zhou",
            "Yoshua Bengio"
        ],
        "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.\n    ",
        "submission_date": "2017-03-09T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03149",
        "title": "Detecting Sockpuppets in Deceptive Opinion Spam",
        "authors": [
            "Marjan Hosseinia",
            "Arjun Mukherjee"
        ],
        "abstract": "This paper explores the problem of sockpuppet detection in deceptive opinion spam using authorship attribution and verification approaches. Two methods are explored. The first is a feature subsampling scheme that uses the KL-Divergence on stylistic language models of an author to find discriminative features. The second is a transduction scheme, spy induction that leverages the diversity of authors in the unlabeled test set by sending a set of spies (positive samples) from the training set to retrieve hidden samples in the unlabeled test set using nearest and farthest neighbors. Experiments using ground truth sockpuppet data show the effectiveness of the proposed schemes.\n    ",
        "submission_date": "2017-03-09T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03200",
        "title": "Turkish PoS Tagging by Reducing Sparsity with Morpheme Tags in Small Datasets",
        "authors": [
            "Burcu Can",
            "Ahmet \u00dcst\u00fcn",
            "Murathan Kurfal\u0131"
        ],
        "abstract": "Sparsity is one of the major problems in natural language processing. The problem becomes even more severe in agglutinating languages that are highly prone to be inflected. We deal with sparsity in Turkish by adopting morphological features for part-of-speech tagging. We learn inflectional and derivational morpheme tags in Turkish by using conditional random fields (CRF) and we employ the morpheme tags in part-of-speech (PoS) tagging by using hidden Markov models (HMMs) to mitigate sparsity. Results show that using morpheme tags in PoS tagging helps alleviate the sparsity in emission probabilities. Our model outperforms other hidden Markov model based PoS tagging models for small training datasets in Turkish. We obtain an accuracy of 94.1% in morpheme tagging and 89.2% in PoS tagging on a 5K training dataset.\n    ",
        "submission_date": "2017-03-09T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03442",
        "title": "The cognitive roots of regularization in language",
        "authors": [
            "Vanessa Ferdinand",
            "Simon Kirby",
            "Kenny Smith"
        ],
        "abstract": "Regularization occurs when the output a learner produces is less variable than the linguistic data they observed. In an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias in cognition: a domain-general source based on cognitive load and a domain-specific source triggered by linguistic stimuli. Both of these factors modulate how frequency information is encoded and produced, but only the production-side modulations result in regularization (i.e. cause learners to eliminate variation from the observed input). We formalize the definition of regularization as the reduction of entropy and find that entropy measures are better at identifying regularization behavior than frequency-based analyses. Using our experimental data and a model of cultural transmission, we generate predictions for the amount of regularity that would develop in each experimental condition if the artificial language were transmitted over several generations of learners. Here we find that the effect of cognitive constraints can become more complex when put into the context of cultural evolution: although learning biases certainly carry information about the course of language evolution, we should not expect a one-to-one correspondence between the micro-level processes that regularize linguistic datasets and the macro-level evolution of linguistic regularity.\n    ",
        "submission_date": "2017-03-09T00:00:00",
        "last_modified_date": "2018-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03640",
        "title": "A Study of Metrics of Distance and Correlation Between Ranked Lists for Compositionality Detection",
        "authors": [
            "Christina Lioma",
            "Niels Dalum Hansen"
        ],
        "abstract": "Compositionality in language refers to how much the meaning of some phrase can be decomposed into the meaning of its constituents and the way these constituents are combined. Based on the premise that substitution by synonyms is meaning-preserving, compositionality can be approximated as the semantic similarity between a phrase and a version of that phrase where words have been replaced by their synonyms. Different ways of representing such phrases exist (e.g., vectors [1] or language models [2]), and the choice of representation affects the measurement of semantic similarity.\n",
        "submission_date": "2017-03-10T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03666",
        "title": "Comparison of SMT and RBMT; The Requirement of Hybridization for Marathi-Hindi MT",
        "authors": [
            "Sreelekha. S",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "We present in this paper our work on comparison between Statistical Machine Translation (SMT) and Rule-based machine translation for translation from Marathi to Hindi. Rule Based systems although robust take lots of time to build. On the other hand statistical machine translation systems are easier to create, maintain and improve upon. We describe the development of a basic Marathi-Hindi SMT system and evaluate its performance. Through a detailed error analysis, we, point out the relative strengths and weaknesses of both systems. Effectively, we shall see that even with a small amount of training corpus a statistical machine translation system has many advantages for high quality domain specific machine translation over that of a rule-based counterpart.\n    ",
        "submission_date": "2017-03-10T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03714",
        "title": "Applying the Wizard-of-Oz Technique to Multimodal Human-Robot Dialogue",
        "authors": [
            "Matthew Marge",
            "Claire Bonial",
            "Brendan Byrne",
            "Taylor Cassidy",
            "A. William Evans",
            "Susan G. Hill",
            "Clare Voss"
        ],
        "abstract": "Our overall program objective is to provide more natural ways for soldiers to interact and communicate with robots, much like how soldiers communicate with other soldiers today. We describe how the Wizard-of-Oz (WOz) method can be applied to multimodal human-robot dialogue in a collaborative exploration task. While the WOz method can help design robot behaviors, traditional approaches place the burden of decisions on a single wizard. In this work, we consider two wizards to stand in for robot navigation and dialogue management software components. The scenario used to elicit data is one in which a human-robot team is tasked with exploring an unknown environment: a human gives verbal instructions from a remote location and the robot follows them, clarifying possible misunderstandings as needed via dialogue. We found the division of labor between wizards to be workable, which holds promise for future software development.\n    ",
        "submission_date": "2017-03-10T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03771",
        "title": "Coping with Construals in Broad-Coverage Semantic Annotation of Adpositions",
        "authors": [
            "Jena D. Hwang",
            "Archna Bhatia",
            "Na-Rae Han",
            "Tim O'Gorman",
            "Vivek Srikumar",
            "Nathan Schneider"
        ],
        "abstract": "We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that a preposition's lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition's lexical function so they can be annotated at scale---supporting automatic, statistical processing of domain-general language---and sketch how this representation would inform a constructional analysis.\n    ",
        "submission_date": "2017-03-10T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03842",
        "title": "Effects of Limiting Memory Capacity on the Behaviour of Exemplar Dynamics",
        "authors": [
            "B. Goodman",
            "P. F. Tupper"
        ],
        "abstract": "Exemplar models are a popular class of models used to describe language change. Here we study how limiting the memory capacity of an individual in these models affects the system's behaviour. In particular we demonstrate the effect this change has on the extinction of categories. Previous work in exemplar dynamics has not addressed this question. In order to investigate this, we will inspect a simplified exemplar model. We will prove for the simplified model that all the sound categories but one will always become extinct, whether memory storage is limited or not. However, computer simulations show that changing the number of stored memories alters how fast categories become extinct.\n    ",
        "submission_date": "2017-03-10T00:00:00",
        "last_modified_date": "2017-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03906",
        "title": "Massive Exploration of Neural Machine Translation Architectures",
        "authors": [
            "Denny Britz",
            "Anna Goldie",
            "Minh-Thang Luong",
            "Quoc Le"
        ],
        "abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.\n    ",
        "submission_date": "2017-03-11T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03939",
        "title": "Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model)",
        "authors": [
            "Govardana Sachithanandam Ramachandran",
            "Ajay Sohmshetty"
        ],
        "abstract": "We examine Memory Networks for the task of question answering (QA), under common real world scenario where training examples are scarce and under weakly supervised scenario, that is only extrinsic labels are available for training. We propose extensions for the Dynamic Memory Network (DMN), specifically within the attention mechanism, we call the resulting Neural Architecture as Dynamic Memory Tensor Network (DMTN). Ultimately, we see that our proposed extensions results in over 80% improvement in the number of task passed against the baselined standard DMN and 20% more task passed compared to state-of-the-art End-to-End Memory Network for Facebook's single task weakly trained 1K bAbi dataset.\n    ",
        "submission_date": "2017-03-11T00:00:00",
        "last_modified_date": "2017-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04001",
        "title": "Language Use Matters: Analysis of the Linguistic Structure of Question Texts Can Characterize Answerability in Quora",
        "authors": [
            "Suman Kalyan Maity",
            "Aman Kharb",
            "Animesh Mukherjee"
        ],
        "abstract": "Quora is one of the most popular community Q&A sites of recent times. However, many question posts on this Q&A site often do not get answered. In this paper, we quantify various linguistic activities that discriminates an answered question from an unanswered one. Our central finding is that the way users use language while writing the question text can be a very effective means to characterize answerability. This characterization helps us to predict early if a question remaining unanswered for a specific time period t will eventually be answered or not and achieve an accuracy of 76.26% (t = 1 month) and 68.33% (t = 3 months). Notably, features representing the language use patterns of the users are most discriminative and alone account for an accuracy of 74.18%. We also compare our method with some of the similar works (Dror et al., Yang et al.) achieving a maximum improvement of ~39% in terms of accuracy.\n    ",
        "submission_date": "2017-03-11T00:00:00",
        "last_modified_date": "2017-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04009",
        "title": "Automated Hate Speech Detection and the Problem of Offensive Language",
        "authors": [
            "Thomas Davidson",
            "Dana Warmsley",
            "Michael Macy",
            "Ingmar Weber"
        ],
        "abstract": "A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.\n    ",
        "submission_date": "2017-03-11T00:00:00",
        "last_modified_date": "2017-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04178",
        "title": "Why we have switched from building full-fledged taxonomies to simply detecting hypernymy relations",
        "authors": [
            "Jose Camacho-Collados"
        ],
        "abstract": "The study of taxonomies and hypernymy relations has been extensive on the Natural Language Processing (NLP) literature. However, the evaluation of taxonomy learning approaches has been traditionally troublesome, as it mainly relies on ad-hoc experiments which are hardly reproducible and manually expensive. Partly because of this, current research has been lately focusing on the hypernymy detection task. In this paper we reflect on this trend, analyzing issues related to current evaluation procedures. Finally, we propose three potential avenues for future work so that is-a relations and resources based on them play a more important role in downstream NLP applications.\n    ",
        "submission_date": "2017-03-12T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04213",
        "title": "MetaPAD: Meta Pattern Discovery from Massive Text Corpora",
        "authors": [
            "Meng Jiang",
            "Jingbo Shang",
            "Taylor Cassidy",
            "Xiang Ren",
            "Lance M. Kaplan",
            "Timothy P. Hanratty",
            "Jiawei Han"
        ],
        "abstract": "Mining textual patterns in news, tweets, papers, and many other kinds of text corpora has been an active theme in text mining and NLP research. Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets---their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04330",
        "title": "Story Cloze Ending Selection Baselines and Data Examination",
        "authors": [
            "Todor Mihaylov",
            "Anette Frank"
        ],
        "abstract": "This paper describes two supervised baseline systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a). We first build a classifier using features based on word embeddings and semantic similarity computation. We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings. Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models. Our best model achieves an accuracy of 72.42, ranking 3rd in the official evaluation.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04357",
        "title": "Nematus: a Toolkit for Neural Machine Translation",
        "authors": [
            "Rico Sennrich",
            "Orhan Firat",
            "Kyunghyun Cho",
            "Alexandra Birch",
            "Barry Haddow",
            "Julian Hitschler",
            "Marcin Junczys-Dowmunt",
            "Samuel L\u00e4ubli",
            "Antonio Valerio Miceli Barone",
            "Jozef Mokry",
            "Maria N\u0103dejde"
        ],
        "abstract": "We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04417",
        "title": "El Lenguaje Natural como Lenguaje Formal",
        "authors": [
            "Franco M. Luque"
        ],
        "abstract": "Formal languages theory is useful for the study of natural language. In particular, it is of interest to study the adequacy of the grammatical formalisms to express syntactic phenomena present in natural language. First, it helps to draw hypothesis about the nature and complexity of the speaker-hearer linguistic competence, a fundamental question in linguistics and other cognitive sciences. Moreover, from an engineering point of view, it allows the knowledge of practical limitations of applications based on those formalisms. In this article I introduce the adequacy problem of grammatical formalisms for natural language, also introducing some formal language theory concepts required for this discussion. Then, I review the formalisms that have been proposed in history, and the arguments that have been given to support or reject their adequacy.\n",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04474",
        "title": "DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks",
        "authors": [
            "Lingpeng Kong",
            "Chris Alberti",
            "Daniel Andor",
            "Ivan Bogatyy",
            "David Weiss"
        ],
        "abstract": "In this work, we present a compact, modular framework for constructing novel recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and re-cursive tree-structured models. A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04481",
        "title": "Geometrical morphology",
        "authors": [
            "John Goldsmith",
            "Eric Rosen"
        ],
        "abstract": "We explore inflectional morphology as an example of the relationship of the discrete and the continuous in linguistics. The grammar requests a form of a lexeme by specifying a set of feature values, which corresponds to a corner M of a hypercube in feature value space. The morphology responds to that request by providing a morpheme, or a set of morphemes, whose vector sum is geometrically closest to the corner M. In short, the chosen morpheme $\\mu$ is the morpheme (or set of morphemes) that maximizes the inner product of $\\mu$ and M.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04489",
        "title": "Reinforcement Learning for Transition-Based Mention Detection",
        "authors": [
            "Georgiana Dinu",
            "Wael Hamza",
            "Radu Florian"
        ],
        "abstract": "This paper describes an application of reinforcement learning to the mention detection task. We define a novel action-based formulation for the mention detection task, in which a model can flexibly revise past labeling decisions by grouping together tokens and assigning partial mention labels. We devise a method to create mention-level episodes and we train a model by rewarding correctly labeled complete mentions, irrespective of the inner structure created. The model yields results which are on par with a competitive supervised counterpart while being more flexible in terms of achieving targeted behavior through reward modeling and generating internal mention structure, especially on longer mentions.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04617",
        "title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering",
        "authors": [
            "Junbei Zhang",
            "Xiaodan Zhu",
            "Qian Chen",
            "Lirong Dai",
            "Si Wei",
            "Hui Jiang"
        ],
        "abstract": "The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.\n    ",
        "submission_date": "2017-03-14T00:00:00",
        "last_modified_date": "2017-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04650",
        "title": "Joint Learning of Correlated Sequence Labelling Tasks Using Bidirectional Recurrent Neural Networks",
        "authors": [
            "Vardaan Pahuja",
            "Anirban Laha",
            "Shachar Mirkin",
            "Vikas Raykar",
            "Lili Kotlerman",
            "Guy Lev"
        ],
        "abstract": "The stream of words produced by Automatic Speech Recognition (ASR) systems is typically devoid of punctuations and formatting. Most natural language processing applications expect segmented and well-formatted texts as input, which is not available in ASR output. This paper proposes a novel technique of jointly modeling multiple correlated tasks such as punctuation and capitalization using bidirectional recurrent neural networks, which leads to improved performance for each of these tasks. This method could be extended for joint modeling of any other correlated sequence labeling tasks.\n    ",
        "submission_date": "2017-03-14T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04677",
        "title": "A computational investigation of sources of variability in sentence comprehension difficulty in aphasia",
        "authors": [
            "Paul M\u00e4tzig",
            "Shravan Vasishth",
            "Felix Engelmann",
            "David Caplan"
        ],
        "abstract": "We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis and Vasishth (2005) model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object rela- tives whose matrix clause contained either an NP or a reflexive, presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.\n    ",
        "submission_date": "2017-03-14T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04718",
        "title": "Extending Automatic Discourse Segmentation for Texts in Spanish to Catalan",
        "authors": [
            "Iria da Cunha",
            "Eric SanJuan",
            "Juan-Manuel Torres-Moreno",
            "Irene Castell\u00f3n"
        ],
        "abstract": "At present, automatic discourse analysis is a relevant research topic in the field of NLP. However, discourse is one of the phenomena most difficult to process. Although discourse parsers have been already developed for several languages, this tool does not exist for Catalan. In order to implement this kind of parser, the first step is to develop a discourse segmenter. In this article we present the first discourse segmenter for texts in Catalan. This segmenter is based on Rhetorical Structure Theory (RST) for Spanish, and uses lexical and syntactic information to translate rules valid for Spanish into rules for Catalan. We have evaluated the system by using a gold standard corpus including manually segmented texts and results are promising.\n    ",
        "submission_date": "2017-03-11T00:00:00",
        "last_modified_date": "2017-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04816",
        "title": "Making Neural QA as Simple as Possible but not Simpler",
        "authors": [
            "Dirk Weissenborn",
            "Georg Wiese",
            "Laura Seiffe"
        ],
        "abstract": "Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.\n    ",
        "submission_date": "2017-03-14T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04826",
        "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
        "authors": [
            "Diego Marcheggiani",
            "Ivan Titov"
        ],
        "abstract": "Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.\n    ",
        "submission_date": "2017-03-14T00:00:00",
        "last_modified_date": "2017-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04879",
        "title": "Sparse Named Entity Classification using Factorization Machines",
        "authors": [
            "Ai Hirata",
            "Mamoru Komachi"
        ],
        "abstract": "Named entity classification is the task of classifying text-based elements into various categories, including places, names, dates, times, and monetary values. A bottleneck in named entity classification, however, is the data problem of sparseness, because new named entities continually emerge, making it rather difficult to maintain a dictionary for named entity classification. Thus, in this paper, we address the problem of named entity classification using matrix factorization to overcome the problem of feature sparsity. Experimental results show that our proposed model, with fewer features and a smaller size, achieves competitive accuracy to state-of-the-art models.\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04887",
        "title": "Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets",
        "authors": [
            "Zhen Yang",
            "Wei Chen",
            "Feng Wang",
            "Bo Xu"
        ],
        "abstract": "This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences (i.e., the golden target sentences), And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2018-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04914",
        "title": "Ensemble of Neural Classifiers for Scoring Knowledge Base Triples",
        "authors": [
            "Ikuya Yamada",
            "Motoki Sato",
            "Hiroyuki Shindo"
        ],
        "abstract": "This paper describes our approach for the triple scoring task at the WSDM Cup 2017. The task required participants to assign a relevance score for each pair of entities and their types in a knowledge base in order to enhance the ranking results in entity retrieval tasks. We propose an approach wherein the outputs of multiple neural network classifiers are combined using a supervised machine learning model. The experimental results showed that our proposed method achieved the best performance in one out of three measures (i.e., Kendall's tau), and performed competitively in the other two measures (i.e., accuracy and average score difference).\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04929",
        "title": "SyntaxNet Models for the CoNLL 2017 Shared Task",
        "authors": [
            "Chris Alberti",
            "Daniel Andor",
            "Ivan Bogatyy",
            "Michael Collins",
            "Dan Gillick",
            "Lingpeng Kong",
            "Terry Koo",
            "Ji Ma",
            "Mark Omernick",
            "Slav Petrov",
            "Chayut Thanapirom",
            "Zora Tung",
            "David Weiss"
        ],
        "abstract": "We describe a baseline dependency parsing system for the CoNLL2017 Shared Task. This system, which we call \"ParseySaurus,\" uses the DRAGNN framework [Kong et al, 2017] to combine transition-based recurrent parsing and tagging with character-based word representations. On the v1.3 Universal Dependencies Treebanks, the new system outpeforms the publicly available, state-of-the-art \"Parsey's Cousins\" models by 3.47% absolute Labeled Accuracy Score (LAS) across 52 treebanks.\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05122",
        "title": "Is this word borrowed? An automatic approach to quantify the likeliness of borrowing in social media",
        "authors": [
            "Jasabanta Patro",
            "Bidisha Samanta",
            "Saurabh Singh",
            "Prithwish Mukherjee",
            "Monojit Choudhury",
            "Animesh Mukherjee"
        ],
        "abstract": "Code-mixing or code-switching are the effortless phenomena of natural switching between two or more languages in a single conversation. Use of a foreign word in a language; however, does not necessarily mean that the speaker is code-switching because often languages borrow lexical items from other languages. If a word is borrowed, it becomes a part of the lexicon of a language; whereas, during code-switching, the speaker is aware that the conversation involves foreign words or phrases. Identifying whether a foreign word used by a bilingual speaker is due to borrowing or code-switching is a fundamental importance to theories of multilingualism, and an essential prerequisite towards the development of language and speech technologies for multilingual communities. In this paper, we present a series of novel computational methods to identify the borrowed likeliness of a word, based on the social media signals. We first propose context based clustering method to sample a set of candidate words from the social media ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05260",
        "title": "InScript: Narrative texts annotated with script information",
        "authors": [
            "Ashutosh Modi",
            "Tatjana Anikina",
            "Simon Ostermann",
            "Manfred Pinkal"
        ],
        "abstract": "This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05320",
        "title": "Legal Question Answering using Ranking SVM and Deep Convolutional Neural Network",
        "authors": [
            "Phong-Khac Do",
            "Huy-Tien Nguyen",
            "Chien-Xuan Tran",
            "Minh-Tien Nguyen",
            "Minh-Le Nguyen"
        ],
        "abstract": "This paper presents a study of employing Ranking SVM and Convolutional Neural Network for two missions: legal information retrieval and question answering in the Competition on Legal Information Extraction/Entailment. For the first task, our proposed model used a triple of features (LSI, Manhattan, Jaccard), and is based on paragraph level instead of article level as in previous studies. In fact, each single-paragraph article corresponds to a particular paragraph in a huge multiple-paragraph article. For the legal question answering task, additional statistical features from information retrieval task integrated into Convolutional Neural Network contribute to higher accuracy.\n    ",
        "submission_date": "2017-03-16T00:00:00",
        "last_modified_date": "2017-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05390",
        "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting",
        "authors": [
            "Sercan O. Arik",
            "Markus Kliegl",
            "Rewon Child",
            "Joel Hestness",
            "Andrew Gibiansky",
            "Chris Fougner",
            "Ryan Prenger",
            "Adam Coates"
        ],
        "abstract": "Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05423",
        "title": "End-to-end optimization of goal-driven and visually grounded dialogue systems",
        "authors": [
            "Florian Strub",
            "Harm de Vries",
            "Jeremie Mary",
            "Bilal Piot",
            "Aaron Courville",
            "Olivier Pietquin"
        ],
        "abstract": "End-to-end design of dialogue systems has recently become a popular research topic thanks to powerful tools such as encoder-decoder architectures for sequence-to-sequence learning. Yet, most current approaches cast human-machine dialogue management as a supervised learning problem, aiming at predicting the next utterance of a participant given the full history of the dialogue. This vision is too simplistic to render the intrinsic planning problem inherent to dialogue as well as its grounded nature, making the context of a dialogue larger than the sole history. This is why only chit-chat and question answering tasks have been addressed so far using end-to-end architectures. In this paper, we introduce a Deep Reinforcement Learning method to optimize visually grounded task-oriented dialogues, based on the policy gradient algorithm. This approach is tested on a dataset of 120k dialogues collected through Mechanical Turk and provides encouraging results at solving both the problem of generating natural dialogues and the task of discovering a specific object in a complex picture.\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05465",
        "title": "Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model",
        "authors": [
            "Wenli Zhuang",
            "Ernie Chang"
        ],
        "abstract": "This paper describes a neural-network model which performed competitively (top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS) task. Our system employs an attention-based recurrent neural network model that optimizes the sentence similarity. In this paper, we describe our participation in the multilingual STS task which measures similarity across English, Spanish, and Arabic.\n    ",
        "submission_date": "2017-03-16T00:00:00",
        "last_modified_date": "2017-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05880",
        "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling",
        "authors": [
            "Wenpeng Li",
            "BinBin Zhang",
            "Lei Xie",
            "Dong Yu"
        ],
        "abstract": "Deep learning models (DLMs) are state-of-the-art techniques in speech recognition. However, training good DLMs can be time consuming especially for production-size models and corpora. Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them. In this paper we aim at filling this gap by comparing four popular parallel training algorithms in speech recognition, namely asynchronous stochastic gradient descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using feed-forward deep neural networks (DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms single-GPU SGD. ASGD can be used as a substitute in some cases.\n    ",
        "submission_date": "2017-03-17T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05916",
        "title": "Construction of a Japanese Word Similarity Dataset",
        "authors": [
            "Yuya Sakaizawa",
            "Mamoru Komachi"
        ],
        "abstract": "An evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.\n    ",
        "submission_date": "2017-03-17T00:00:00",
        "last_modified_date": "2018-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06345",
        "title": "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks",
        "authors": [
            "Zhilin Yang",
            "Ruslan Salakhutdinov",
            "William W. Cohen"
        ],
        "abstract": "Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering. However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained. These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.\n    ",
        "submission_date": "2017-03-18T00:00:00",
        "last_modified_date": "2017-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06501",
        "title": "M\u00e9todos de Otimiza\u00e7\u00e3o Combinat\u00f3ria Aplicados ao Problema de Compress\u00e3o MultiFrases",
        "authors": [
            "Elvys Linhares Pontes",
            "Thiago Gouveia da Silva",
            "Andr\u00e9a Carneiro Linhares",
            "Juan-Manuel Torres-Moreno",
            "St\u00e9phane Huet"
        ],
        "abstract": "The Internet has led to a dramatic increase in the amount of available information. In this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for Multi-Sentences Compression (MSC), method which reduces the sentence length without removing core information, the insertion of optimization methods requires further study to improve the performance of MSC. This article describes a method for MSC using Combinatorial Optimization and Graph Theory to generate more informative sentences while maintaining their grammaticality. An experiment led on a corpus of 40 clusters of sentences shows that our system has achieved a very good quality and is better than the state-of-the-art.\n    ",
        "submission_date": "2017-03-19T00:00:00",
        "last_modified_date": "2017-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06541",
        "title": "Native Language Identification using Stacked Generalization",
        "authors": [
            "Shervin Malmasi",
            "Mark Dras"
        ],
        "abstract": "Ensemble methods using multiple classifiers have proven to be the most successful approach for the task of Native Language Identification (NLI), achieving the current state of the art. However, a systematic examination of ensemble methods for NLI has yet to be conducted. Additionally, deeper ensemble architectures such as classifier stacking have not been closely evaluated. We present a set of experiments using three ensemble-based models, testing each with multiple configurations and algorithms. This includes a rigorous application of meta-classification models for NLI, achieving state-of-the-art results on three datasets from different languages. We also present the first use of statistical significance testing for comparing NLI systems, showing that our results are significantly better than the previous state of the art. We make available a collection of test set predictions to facilitate future statistical tests.\n    ",
        "submission_date": "2017-03-19T00:00:00",
        "last_modified_date": "2017-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.07055",
        "title": "Investigation of Language Understanding Impact for Reinforcement Learning Based Dialogue Systems",
        "authors": [
            "Xiujun Li",
            "Yun-Nung Chen",
            "Lihong Li",
            "Jianfeng Gao",
            "Asli Celikyilmaz"
        ],
        "abstract": "Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.\n    ",
        "submission_date": "2017-03-21T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.07090",
        "title": "Deep LSTM for Large Vocabulary Continuous Speech Recognition",
        "authors": [
            "Xu Tian",
            "Jun Zhang",
            "Zejun Ma",
            "Yi He",
            "Juan Wei",
            "Peihao Wu",
            "Wenchang Situ",
            "Shuai Li",
            "Yang Zhang"
        ],
        "abstract": "Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, are effective network for sequential task like speech recognition. Deeper LSTM models perform well on large vocabulary continuous speech recognition, because of their impressive learning ability. However, it is more difficult to train a deeper network. We introduce a training framework with layer-wise training and exponential moving average methods for deeper LSTM models. It is a competitive framework that LSTM models of more than 7 layers are successfully trained on Shenma voice search data in Mandarin and they outperform the deep LSTM models trained by conventional approach. Moreover, in order for online streaming speech recognition applications, the shallow model with low real time factor is distilled from the very deep model. The recognition accuracy have little loss in the distillation process. Therefore, the model trained with the proposed training framework reduces relative 14\\% character error rate, compared to original model which has the similar real-time capability. Furthermore, the novel transfer learning strategy with segmental Minimum Bayes-Risk is also introduced in the framework. The strategy makes it possible that training with only a small part of dataset could outperform full dataset training from the beginning.\n    ",
        "submission_date": "2017-03-21T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.07438",
        "title": "The NLTK FrameNet API: Designing for Discoverability with a Rich Linguistic Resource",
        "authors": [
            "Nathan Schneider",
            "Chuck Wooters"
        ],
        "abstract": "A new Python API, integrated within the NLTK suite, offers access to the FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.\n    ",
        "submission_date": "2017-03-21T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.07476",
        "title": "Topic Identification for Speech without ASR",
        "authors": [
            "Chunxi Liu",
            "Jan Trmal",
            "Matthew Wiesner",
            "Craig Harman",
            "Sanjeev Khudanpur"
        ],
        "abstract": "Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks.\n    ",
        "submission_date": "2017-03-22T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.07713",
        "title": "Hierarchical RNN with Static Sentence-Level Attention for Text-Based Speaker Change Detection",
        "authors": [
            "Zhao Meng",
            "Lili Mou",
            "Zhi Jin"
        ],
        "abstract": "Speaker change detection (SCD) is an important task in dialog modeling. Our paper addresses the problem of text-based SCD, which differs from existing audio-based studies and is useful in various scenarios, for example, processing dialog transcripts where speaker identities are missing (e.g., OpenSubtitle), and enhancing audio SCD with textual information. We formulate text-based SCD as a matching problem of utterances before and after a certain decision point; we propose a hierarchical recurrent neural network (RNN) with static sentence-level attention. Experimental results show that neural networks consistently achieve better performance than feature-based approaches, and that our attention-based model significantly outperforms non-attention neural networks.\n    ",
        "submission_date": "2017-03-22T00:00:00",
        "last_modified_date": "2018-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.07754",
        "title": "Direct Acoustics-to-Word Models for English Conversational Speech Recognition",
        "authors": [
            "Kartik Audhkhasi",
            "Bhuvana Ramabhadran",
            "George Saon",
            "Michael Picheny",
            "David Nahamoo"
        ],
        "abstract": "Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models.\n    ",
        "submission_date": "2017-03-22T00:00:00",
        "last_modified_date": "2017-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.07805",
        "title": "Supervised Typing of Big Graphs using Semantic Embeddings",
        "authors": [
            "Mayank Kejriwal",
            "Pedro Szekely"
        ],
        "abstract": "We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. The algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.\n    ",
        "submission_date": "2017-03-22T00:00:00",
        "last_modified_date": "2017-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08002",
        "title": "A network of deep neural networks for distant speech recognition",
        "authors": [
            "Mirco Ravanelli",
            "Philemon Brakel",
            "Maurizio Omologo",
            "Yoshua Bengio"
        ],
        "abstract": "Despite the remarkable progress recently made in distant speech recognition, state-of-the-art technology still suffers from a lack of robustness, especially when adverse acoustic conditions characterized by non-stationary noises and reverberation are met. A prominent limitation of current systems lies in the lack of matching and communication between the various technologies involved in the distant speech recognition process. The speech enhancement and speech recognition modules are, for instance, often trained independently. Moreover, the speech enhancement normally helps the speech recognizer, but the output of the latter is not commonly used, in turn, to improve the speech enhancement. To address both concerns, we propose a novel architecture based on a network of deep neural networks, where all the components are jointly trained and better cooperate with each other thanks to a full communication scheme between them. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework can overtake other competitive solutions, including recent joint training approaches.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08068",
        "title": "Sequential Recurrent Neural Networks for Language Modeling",
        "authors": [
            "Youssef Oualil",
            "Clayton Greenberg",
            "Mittul Singh",
            "Dietrich Klakow"
        ],
        "abstract": "Feedforward Neural Network (FNN)-based language models estimate the probability of the next word based on the history of the last N words, whereas Recurrent Neural Networks (RNN) perform the same task based only on the last word and some context information that cycles in the network. This paper presents a novel approach, which bridges the gap between these two categories of networks. In particular, we propose an architecture which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network. The context integration is performed using an additional word-dependent weight matrix that is also learned during the training. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08084",
        "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation",
        "authors": [
            "Jean-Benoit Delbrouck",
            "Stephane Dupont"
        ],
        "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08088",
        "title": "Rapid-Rate: A Framework for Semi-supervised Real-time Sentiment Trend Detection in Unstructured Big Data",
        "authors": [
            "Vineet John"
        ],
        "abstract": "Commercial establishments like restaurants, service centres and retailers have several sources of customer feedback about products and services, most of which need not be as structured as rated reviews provided by services like Yelp, or Amazon, in terms of sentiment conveyed. For instance, Amazon provides a fine-grained score on a numeric scale for product reviews. Some sources, however, like social media (Twitter, Facebook), mailing lists (Google Groups) and forums (Quora) contain text data that is much more voluminous, but unstructured and unlabelled. It might be in the best interests of a business establishment to assess the general sentiment towards their brand on these platforms as well. This text could be pipelined into a system with a built-in prediction model, with the objective of generating real-time graphs on opinion and sentiment trends. Although such tasks like the one described about have been explored with respect to document classification problems in the past, the implementation described in this paper, by virtue of learning a continuous function rather than a discrete one, offers a lot more depth of insight as compared to document classification approaches. This study aims to explore the validity of such a continuous function predicting model to quantify sentiment about an entity, without the additional overhead of manual labelling, and computational preprocessing & feature extraction. This research project also aims to design and implement a re-usable document regression pipeline as a framework, Rapid-Rate, that can be used to predict document scores in real-time.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08098",
        "title": "A survey of embedding models of entities and relationships for knowledge graph completion",
        "authors": [
            "Dat Quoc Nguyen"
        ],
        "abstract": "Knowledge graphs (KGs) of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge graphs are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the knowledge graph is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2020-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08120",
        "title": "Recurrent and Contextual Models for Visual Question Answering",
        "authors": [
            "Abhijit Sharang",
            "Eric Lau"
        ],
        "abstract": "We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to $64.6\\%$, while the ensemble of all models achieves the best accuracy of $66.67\\%$, within $0.5\\%$ of the current state-of-the-art for Visual7W.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08135",
        "title": "An embedded segmental K-means model for unsupervised segmentation and clustering of speech",
        "authors": [
            "Herman Kamper",
            "Karen Livescu",
            "Sharon Goldwater"
        ],
        "abstract": "Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08136",
        "title": "Visually grounded learning of keyword prediction from untranscribed speech",
        "authors": [
            "Herman Kamper",
            "Shane Settle",
            "Gregory Shakhnarovich",
            "Karen Livescu"
        ],
        "abstract": "During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance---acting as a spoken bag-of-words classifier---without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. \"man\" and \"person\", making it even more effective as a semantic keyword spotter.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08244",
        "title": "TokTrack: A Complete Token Provenance and Change Tracking Dataset for the English Wikipedia",
        "authors": [
            "Fabian Fl\u00f6ck",
            "Kenan Erdogan",
            "Maribel Acosta"
        ],
        "abstract": "We present a dataset that contains every instance of all tokens (~ words) ever written in undeleted, non-redirect English Wikipedia articles until October 2016, in total 13,545,349,787 instances. Each token is annotated with (i) the article revision it was originally created in, and (ii) lists with all the revisions in which the token was ever deleted and (potentially) re-added and re-deleted from its article, enabling a complete and straightforward tracking of its history. This data would be exceedingly hard to create by an average potential user as it is (i) very expensive to compute and as (ii) accurately tracking the history of each token in revisioned documents is a non-trivial task. Adapting a state-of-the-art algorithm, we have produced a dataset that allows for a range of analyses and metrics, already popular in research and going beyond, to be generated on complete-Wikipedia scale; ensuring quality and allowing researchers to forego expensive text-comparison computation, which so far has hindered scalable usage. We show how this data enables, on token-level, computation of provenance, measuring survival of content over time, very detailed conflict metrics, and fine-grained interactions of editors like partial reverts, re-additions and other metrics, in the process gaining several novel insights.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08471",
        "title": "Batch-normalized joint training for DNN-based distant speech recognition",
        "authors": [
            "Mirco Ravanelli",
            "Philemon Brakel",
            "Maurizio Omologo",
            "Yoshua Bengio"
        ],
        "abstract": "Improving distant speech recognition is a crucial step towards flexible human-machine interfaces. Current technology, however, still exhibits a lack of robustness, especially when adverse acoustic conditions are met. Despite the significant progress made in the last years on both speech enhancement and speech recognition, one potential limitation of state-of-the-art technology lies in composing modules that are not well matched because they are not trained jointly. To address this concern, a promising approach consists in concatenating a speech enhancement and a speech recognition deep neural network and to jointly update their parameters as if they were within a single bigger network. Unfortunately, joint training can be difficult because the output distribution of the speech enhancement system may change substantially during the optimization procedure. The speech recognition module would have to deal with an input distribution that is non-stationary and unnormalized. To mitigate this issue, we propose a joint training approach based on a fully batch-normalized architecture. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework significantly overtakes other competitive solutions, especially in challenging environments.\n    ",
        "submission_date": "2017-03-24T00:00:00",
        "last_modified_date": "2017-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08513",
        "title": "Interactive Natural Language Acquisition in a Multi-modal Recurrent Neural Architecture",
        "authors": [
            "Stefan Heinrich",
            "Stefan Wermter"
        ],
        "abstract": "For the complex human brain that enables us to communicate in natural language, we gathered good understandings of principles underlying language acquisition and processing, knowledge about socio-cultural conditions, and insights about activity patterns in the brain. However, we were not yet able to understand the behavioural and mechanistic characteristics for natural language and how mechanisms in the brain allow to acquire and process language. In bridging the insights from behavioural psychology and neuroscience, the goal of this paper is to contribute a computational understanding of appropriate characteristics that favour language acquisition. Accordingly, we provide concepts and refinements in cognitive modelling regarding principles and mechanisms in the brain and propose a neurocognitively plausible model for embodied language acquisition from real world interaction of a humanoid robot with its environment. In particular, the architecture consists of a continuous time recurrent neural network, where parts have different leakage characteristics and thus operate on multiple timescales for every modality and the association of the higher level nodes of all modalities into cell assemblies. The model is capable of learning language production grounded in both, temporal dynamic somatosensation and vision, and features hierarchical concept abstraction, concept decomposition, multi-modal integration, and self-organisation of latent representations.\n    ",
        "submission_date": "2017-03-24T00:00:00",
        "last_modified_date": "2018-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08537",
        "title": "Crowdsourcing Universal Part-Of-Speech Tags for Code-Switching",
        "authors": [
            "Victor Soto",
            "Julia Hirschberg"
        ],
        "abstract": "Code-switching is the phenomenon by which bilingual speakers switch between multiple languages during communication. The importance of developing language technologies for codeswitching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task.\n    ",
        "submission_date": "2017-03-24T00:00:00",
        "last_modified_date": "2017-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08581",
        "title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech",
        "authors": [
            "Ron J. Weiss",
            "Jan Chorowski",
            "Navdeep Jaitly",
            "Yonghui Wu",
            "Zhifeng Chen"
        ],
        "abstract": "We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points.\n    ",
        "submission_date": "2017-03-24T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08646",
        "title": "Simplifying the Bible and Wikipedia Using Statistical Machine Translation",
        "authors": [
            "Yohan Jo"
        ],
        "abstract": "I started this work with the hope of generating a text synthesizer (like a musical synthesizer) that can imitate certain linguistic styles. Most of the report focuses on text simplification using statistical machine translation (SMT) techniques. I applied MOSES to a parallel corpus of the Bible (King James Version and Easy-to-Read Version) and that of Wikipedia articles (normal and simplified). I report the importance of the three main components of SMT---phrase translation, language model, and recording---by changing their weights and comparing the resulting quality of simplified text in terms of METEOR and BLEU. Toward the end of the report will be presented some examples of text \"synthesized\" into the King James style.\n    ",
        "submission_date": "2017-03-25T00:00:00",
        "last_modified_date": "2017-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08701",
        "title": "Morphological Analysis for the Maltese Language: The Challenges of a Hybrid System",
        "authors": [
            "Claudia Borg",
            "Albert Gatt"
        ],
        "abstract": "Maltese is a morphologically rich language with a hybrid morphological system which features both concatenative and non-concatenative processes. This paper analyses the impact of this hybridity on the performance of machine learning techniques for morphological labelling and clustering. In particular, we analyse a dataset of morphologically related word clusters to evaluate the difference in results for concatenative and nonconcatenative clusters. We also describe research carried out in morphological labelling, with a particular focus on the verb category. Two evaluations were carried out, one using an unseen dataset, and another one using a gold standard dataset which was manually labelled. The gold standard dataset was split into concatenative and non-concatenative to analyse the difference in results between the two morphological systems.\n    ",
        "submission_date": "2017-03-25T00:00:00",
        "last_modified_date": "2017-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08705",
        "title": "Comparing Rule-Based and Deep Learning Models for Patient Phenotyping",
        "authors": [
            "Sebastian Gehrmann",
            "Franck Dernoncourt",
            "Yeran Li",
            "Eric T. Carlson",
            "Joy T. Wu",
            "Jonathan Welt",
            "John Foote Jr.",
            "Edward T. Moseley",
            "David W. Grant",
            "Patrick D. Tyler",
            "Leo Anthony Celi"
        ],
        "abstract": "Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.\n",
        "submission_date": "2017-03-25T00:00:00",
        "last_modified_date": "2017-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08748",
        "title": "LEPOR: An Augmented Machine Translation Evaluation Metric",
        "authors": [
            "Lifeng Han"
        ],
        "abstract": "Machine translation (MT) was developed as one of the hottest research topics in the natural language processing (NLP) literature. One important issue in MT is that how to evaluate the MT system reasonably and tell us whether the translation system makes an improvement or not. The traditional manual judgment methods are expensive, time-consuming, unrepeatable, and sometimes with low agreement. On the other hand, the popular automatic MT evaluation methods have some weaknesses. Firstly, they tend to perform well on the language pairs with English as the target language, but weak when English is used as source. Secondly, some methods rely on many additional linguistic features to achieve good performance, which makes the metric unable to replicate and apply to other language pairs easily. Thirdly, some popular metrics utilize incomprehensive factors, which result in low performance on some practical tasks. In this thesis, to address the existing problems, we design novel MT evaluation methods and investigate their performances on different languages. Firstly, we design augmented factors to yield highly accurate evaluation. Secondly, we design a tunable evaluation model where weighting of factors can be optimized according to the characteristics of languages. Thirdly, in the enhanced version of our methods, we design concise linguistic feature using part-of-speech (POS) to show that our methods can yield even higher performance when using some external linguistic resources. Finally, we introduce the practical performance of our metrics in the ACL-WMT workshop shared tasks, which show that the proposed methods are robust across different languages. In addition, we also present some novel work on quality estimation of MT without using reference translations including the usage of probability models of Na\u00efve Bayes (NB), support vector machine (SVM) classification algorithms, and CRFs.\n    ",
        "submission_date": "2017-03-26T00:00:00",
        "last_modified_date": "2022-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08864",
        "title": "Learning Simpler Language Models with the Differential State Framework",
        "authors": [
            "Alexander G. Ororbia II",
            "Tomas Mikolov",
            "David Reitter"
        ],
        "abstract": "Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks such as language modeling. Existing architectures that address the issue are often complex and costly to train. The Differential State Framework (DSF) is a simple and high-performing design that unifies previously introduced gated neural models. DSF models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. This requires hardly any more parameters than a classical, simple recurrent network. Within the DSF framework, a new architecture is presented, the Delta-RNN. In language modeling at the word and character levels, the Delta-RNN outperforms popular complex architectures, such as the Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU), and, when regularized, performs comparably to several state-of-the-art baselines. At the subword level, the Delta-RNN's performance is comparable to that of complex gated architectures.\n    ",
        "submission_date": "2017-03-26T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08885",
        "title": "Question Answering from Unstructured Text by Retrieval and Comprehension",
        "authors": [
            "Yusuke Watanabe",
            "Bhuwan Dhingra",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "Open domain Question Answering (QA) systems must interact with external knowledge sources, such as web pages, to find relevant information. Information sources like Wikipedia, however, are not well structured and difficult to utilize in comparison with Knowledge Bases (KBs). In this work we present a two-step approach to question answering from unstructured text, consisting of a retrieval step and a comprehension step. For comprehension, we present an RNN based attention model with a novel mixture mechanism for selecting answers from either retrieved articles or a fixed vocabulary. For retrieval we introduce a hand-crafted model and a neural model for ranking relevant articles. We achieve state-of-the-art performance on W IKI M OVIES dataset, reducing the error by 40%. Our experimental results further demonstrate the importance of each of the introduced components.\n    ",
        "submission_date": "2017-03-26T00:00:00",
        "last_modified_date": "2017-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09013",
        "title": "A Sentence Simplification System for Improving Relation Extraction",
        "authors": [
            "Christina Niklaus",
            "Bernhard Bermeitinger",
            "Siegfried Handschuh",
            "Andr\u00e9 Freitas"
        ],
        "abstract": "In this demo paper, we present a text simplification approach that is directed at improving the performance of state-of-the-art Open Relation Extraction (RE) systems. As syntactically complex sentences often pose a challenge for current Open RE approaches, we have developed a simplification framework that performs a pre-processing step by taking a single sentence as input and using a set of syntactic-based transformation rules to create a textual input that is easier to process for subsequently applied Open RE systems.\n    ",
        "submission_date": "2017-03-27T00:00:00",
        "last_modified_date": "2017-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09439",
        "title": "A practical approach to dialogue response generation in closed domains",
        "authors": [
            "Yichao Lu",
            "Phillip Keung",
            "Shaonan Zhang",
            "Jason Sun",
            "Vikas Bhardwaj"
        ],
        "abstract": "We describe a prototype dialogue response generation model for the customer service domain at Amazon. The model, which is trained in a weakly supervised fashion, measures the similarity between customer questions and agent answers using a dual encoder network, a Siamese-like neural network architecture. Answer templates are extracted from embeddings derived from past agent answers, without turn-by-turn annotations. Responses to customer inquiries are generated by selecting the best template from the final set of templates. We show that, in a closed domain like customer service, the selected templates cover $>$70\\% of past customer inquiries. Furthermore, the relevance of the model-selected templates is significantly higher than templates selected by a standard tf-idf baseline.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09527",
        "title": "Is This a Joke? Detecting Humor in Spanish Tweets",
        "authors": [
            "Santiago Castro",
            "Mat\u00edas Cubero",
            "Diego Garat",
            "Guillermo Moncecchi"
        ],
        "abstract": "While humor has been historically studied from a psychological, cognitive and linguistic standpoint, its study from a computational perspective is an area yet to be explored in Computational Linguistics. There exist some previous works, but a characterization of humor that allows its automatic recognition and generation is far from being specified. In this work we build a crowdsourced corpus of labeled tweets, annotated according to its humor value, letting the annotators subjectively decide which are humorous. A humor classifier for Spanish tweets is assembled based on supervised learning, reaching a precision of 84% and a recall of 69%.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09570",
        "title": "A Tidy Data Model for Natural Language Processing using cleanNLP",
        "authors": [
            "Taylor Arnold"
        ],
        "abstract": "The package cleanNLP provides a set of fast tools for converting a textual corpus into a set of normalized tables. The underlying natural language processing pipeline utilizes Stanford's CoreNLP library, exposing a number of annotation tasks for text written in English, French, German, and Spanish. Annotators include tokenization, part of speech tagging, named entity recognition, entity linking, sentiment analysis, dependency parsing, coreference resolution, and information extraction.\n    ",
        "submission_date": "2017-03-27T00:00:00",
        "last_modified_date": "2018-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09817",
        "title": "Learning Similarity Functions for Pronunciation Variations",
        "authors": [
            "Einat Naaman",
            "Yossi Adi",
            "Joseph Keshet"
        ],
        "abstract": "A significant source of errors in Automatic Speech Recognition (ASR) systems is due to pronunciation variations which occur in spontaneous and conversational speech. Usually ASR systems use a finite lexicon that provides one or more pronunciations for each word. In this paper, we focus on learning a similarity function between two pronunciations. The pronunciations can be the canonical and the surface pronunciations of the same word or they can be two surface pronunciations of different words. This task generalizes problems such as lexical access (the problem of learning the mapping between words and their possible pronunciations), and defining word neighborhoods. It can also be used to dynamically increase the size of the pronunciation lexicon, or in predicting ASR errors. We propose two methods, which are based on recurrent neural networks, to learn the similarity function. The first is based on binary classification, and the second is based on learning the ranking of the pronunciations. We demonstrate the efficiency of our approach on the task of lexical access using a subset of the Switchboard conversational speech corpus. Results suggest that on this task our methods are superior to previous methods which are based on graphical Bayesian methods.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09825",
        "title": "Semi-Supervised Affective Meaning Lexicon Expansion Using Semantic and Distributed Word Representations",
        "authors": [
            "Areej Alhothali",
            "Jesse Hoey"
        ],
        "abstract": "In this paper, we propose an extension to graph-based sentiment lexicon induction methods by incorporating distributed and semantic word representations in building the similarity graph to expand a three-dimensional sentiment lexicon. We also implemented and evaluated the label propagation using four different word representations and similarity metrics. Our comprehensive evaluation of the four approaches was performed on a single data set, demonstrating that all four methods can generate a significant number of new sentiment assignments with high accuracy. The highest correlations (tau=0.51) and the lowest error (mean absolute error < 1.1%), obtained by combining both the semantic and the distributional features, outperformed the distributional-based and semantic-based label-propagation models and approached a supervised algorithm.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09831",
        "title": "A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment",
        "authors": [
            "Haonan Yu",
            "Haichao Zhang",
            "Wei Xu"
        ],
        "abstract": "We tackle a task where an agent learns to navigate in a 2D maze-like environment called XWORLD. In each session, the agent perceives a sequence of raw-pixel frames, a natural language command issued by a teacher, and a set of rewards. The agent learns the teacher's language from scratch in a grounded and compositional manner, such that after training it is able to correctly execute zero-shot commands: 1) the combination of words in the command never appeared before, and/or 2) the command contains new object concepts that are learned from another task but never learned from navigation. Our deep framework for the agent is trained end to end: it learns simultaneously the visual representations of the environment, the syntax and semantics of the language, and the action module that outputs actions. The zero-shot learning capability of our framework results from its compositionality and modularity with parameter tying. We visualize the intermediate outputs of the framework, demonstrating that the agent truly understands how to solve the problem. We believe that our results provide some preliminary insights on how to train an agent with similar abilities in a 3D environment.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09902",
        "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation",
        "authors": [
            "Albert Gatt",
            "Emiel Krahmer"
        ],
        "abstract": "This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.\n    ",
        "submission_date": "2017-03-29T00:00:00",
        "last_modified_date": "2018-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10065",
        "title": "Hierarchical Classification for Spoken Arabic Dialect Identification using Prosody: Case of Algerian Dialects",
        "authors": [
            "Soumia Bougrine",
            "Hadda Cherroun",
            "Djelloul Ziadi"
        ],
        "abstract": "In daily communications, Arabs use local dialects which are hard to identify automatically using conventional classification methods. The dialect identification challenging task becomes more complicated when dealing with an under-resourced dialects belonging to a same county/region. In this paper, we start by analyzing statistically Algerian dialects in order to capture their specificities related to prosody information which are extracted at utterance level after a coarse-grained consonant/vowel segmentation. According to these analysis findings, we propose a Hierarchical classification approach for spoken Arabic algerian Dialect IDentification (HADID). It takes advantage from the fact that dialects have an inherent property of naturally structured into hierarchy. Within HADID, a top-down hierarchical classification is applied, in which we use Deep Neural Networks (DNNs) method to build a local classifier for every parent node into the hierarchy dialect structure. Our framework is implemented and evaluated on Algerian Arabic dialects corpus. Whereas, the hierarchy dialect structure is deduced from historic and linguistic knowledges. The results reveal that within {\\HD}, the best classifier is DNNs compared to Support Vector Machine. In addition, compared with a baseline Flat classification system, our HADID gives an improvement of 63.5% in term of precision. Furthermore, overall results evidence the suitability of our prosody-based HADID for speaker independent dialect identification while requiring less than 6s test utterances.\n    ",
        "submission_date": "2017-03-29T00:00:00",
        "last_modified_date": "2017-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10090",
        "title": "A Short Review of Ethical Challenges in Clinical Natural Language Processing",
        "authors": [
            "Simon \u0160uster",
            "St\u00e9phan Tulkens",
            "Walter Daelemans"
        ],
        "abstract": "Clinical NLP has an immense potential in contributing to how clinical practice will be revolutionized by the advent of large scale processing of clinical records. However, this potential has remained largely untapped due to slow progress primarily caused by strict data access policies for researchers. In this paper, we discuss the concern for privacy and the measures it entails. We also suggest sources of less sensitive data. Finally, we draw attention to biases that can compromise the validity of empirical research and lead to socially harmful applications.\n    ",
        "submission_date": "2017-03-29T00:00:00",
        "last_modified_date": "2017-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10135",
        "title": "Tacotron: Towards End-to-End Speech Synthesis",
        "authors": [
            "Yuxuan Wang",
            "RJ Skerry-Ryan",
            "Daisy Stanton",
            "Yonghui Wu",
            "Ron J. Weiss",
            "Navdeep Jaitly",
            "Zongheng Yang",
            "Ying Xiao",
            "Zhifeng Chen",
            "Samy Bengio",
            "Quoc Le",
            "Yannis Agiomyrgiannakis",
            "Rob Clark",
            "Rif A. Saurous"
        ],
        "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.\n    ",
        "submission_date": "2017-03-29T00:00:00",
        "last_modified_date": "2017-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10152",
        "title": "Automatic Argumentative-Zoning Using Word2vec",
        "authors": [
            "Haixia Liu"
        ],
        "abstract": "In comparison with document summarization on the articles from social media and newswire, argumentative zoning (AZ) is an important task in scientific paper analysis. Traditional methodology to carry on this task relies on feature engineering from different levels. In this paper, three models of generating sentence vectors for the task of sentence classification were explored and compared. The proposed approach builds sentence representations using learned embeddings based on neural network. The learned word embeddings formed a feature space, to which the examined sentence is mapped to. Those features are input into the classifiers for supervised classification. Using 10-cross-validation scheme, evaluation was conducted on the Argumentative-Zoning (AZ) annotated articles. The results showed that simply averaging the word vectors in a sentence works better than the paragraph to vector algorithm and by integrating specific cuewords into the loss function of the neural network can improve the classification performance. In comparison with the hand-crafted features, the word2vec method won for most of the categories. However, the hand-crafted features showed their strength on classifying some of the categories.\n    ",
        "submission_date": "2017-03-29T00:00:00",
        "last_modified_date": "2017-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10186",
        "title": "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding",
        "authors": [
            "Will Monroe",
            "Robert X.D. Hawkins",
            "Noah D. Goodman",
            "Christopher Potts"
        ],
        "abstract": "We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the classifiers from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases: when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.\n    ",
        "submission_date": "2017-03-29T00:00:00",
        "last_modified_date": "2017-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10252",
        "title": "Linguistic Matrix Theory",
        "authors": [
            "Dimitrios Kartsaklis",
            "Sanjaye Ramgoolam",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "Recent research in computational linguistics has developed algorithms which associate matrices with adjectives and verbs, based on the distribution of words in a corpus of text. These matrices are linear operators on a vector space of context words. They are used to construct the meaning of composite expressions from that of the elementary constituents, forming part of a compositional distributional approach to semantics. We propose a Matrix Theory approach to this data, based on permutation symmetry along with Gaussian weights and their perturbations. A simple Gaussian model is tested against word matrices created from a large corpus of text. We characterize the cubic and quartic departures from the model, which we propose, alongside the Gaussian parameters, as signatures for comparison of linguistic corpora. We propose that perturbed Gaussian models with permutation symmetry provide a promising framework for characterizing the nature of universality in the statistical properties of word matrices. The matrix theory framework developed here exploits the view of statistics as zero dimensional perturbative quantum field theory. It perceives language as a physical system realizing a universality class of matrix statistics characterized by permutation symmetry.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10661",
        "title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character Dataset",
        "authors": [
            "Mithun Biswas",
            "Rafiqul Islam",
            "Gautam Kumar Shom",
            "Md Shopon",
            "Nabeel Mohammed",
            "Sifat Momen",
            "Md Anowarul Abedin"
        ],
        "abstract": "Bangla handwriting recognition is becoming a very important issue nowadays. It is potentially a very important task specially for Bangla speaking population of Bangladesh and West Bengal. By keeping that in our mind we are introducing a comprehensive Bangla handwritten character dataset named BanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic characters and compound characters. This dataset was collected from multiple geographical location within Bangladesh and includes sample collected from a variety of aged groups. This dataset can also be used for other classification problems i.e: gender, age, district. This is the largest dataset on Bangla handwritten characters yet.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10698",
        "title": "Neutral evolution and turnover over centuries of English word popularity",
        "authors": [
            "Damian Ruck",
            "R. Alexander Bentley",
            "Alberto Acerbi",
            "Philip Garnett",
            "Daniel J. Hruschka"
        ],
        "abstract": "Here we test Neutral models against the evolution of English word frequency and vocabulary at the population scale, as recorded in annual word frequencies from three centuries of English language books. Against these data, we test both static and dynamic predictions of two neutral models, including the relation between corpus size and vocabulary size, frequency distributions, and turnover within those frequency distributions. Although a commonly used Neutral model fails to replicate all these emergent properties at once, we find that modified two-stage Neutral model does replicate the static and dynamic properties of the corpus data. This two-stage model is meant to represent a relatively small corpus (population) of English books, analogous to a `canon', sampled by an exponentially increasing corpus of books in the wider population of authors. More broadly, this mode -- a smaller neutral model within a larger neutral model -- could represent more broadly those situations where mass attention is focused on a small subset of the cultural variants.\n    ",
        "submission_date": "2017-03-30T00:00:00",
        "last_modified_date": "2017-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10722",
        "title": "Factorization tricks for LSTM networks",
        "authors": [
            "Oleksii Kuchaiev",
            "Boris Ginsburg"
        ],
        "abstract": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.\n    ",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2018-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10724",
        "title": "N-gram Language Modeling using Recurrent Neural Network Estimation",
        "authors": [
            "Ciprian Chelba",
            "Mohammad Norouzi",
            "Samy Bengio"
        ],
        "abstract": "We investigate the effective memory depth of RNN models by using them for $n$-gram language model (LM) smoothing.\n",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2017-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10772",
        "title": "Joining Hands: Exploiting Monolingual Treebanks for Parsing of Code-mixing Data",
        "authors": [
            "Irshad Ahmad Bhat",
            "Riyaz Ahmad Bhat",
            "Manish Shrivastava",
            "Dipti Misra Sharma"
        ],
        "abstract": "In this paper, we propose efficient and less resource-intensive strategies for parsing of code-mixed data. These strategies are not constrained by in-domain annotations, rather they leverage pre-existing monolingual annotated resources for training. We show that these methods can produce significantly better results as compared to an informed baseline. Besides, we also present a data set of 450 Hindi and English code-mixed tweets of Hindi multilingual speakers for evaluation. The data set is manually annotated with Universal Dependencies.\n    ",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2017-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10931",
        "title": "Sentence Simplification with Deep Reinforcement Learning",
        "authors": [
            "Xingxing Zhang",
            "Mirella Lapata"
        ],
        "abstract": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call {\\sc Dress} (as shorthand for {\\bf D}eep {\\bf RE}inforcement {\\bf S}entence {\\bf S}implification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.\n    ",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10960",
        "title": "Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders",
        "authors": [
            "Tiancheng Zhao",
            "Ran Zhao",
            "Maxine Eskenazi"
        ],
        "abstract": "While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved by introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.\n    ",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2017-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00016",
        "title": "Opinion Mining on Non-English Short Text",
        "authors": [
            "Esra Akbas"
        ],
        "abstract": "As the type and the number of such venues increase, automated analysis of sentiment on textual resources has become an essential data mining task. In this paper, we investigate the problem of mining opinions on the collection of informal short texts. Both positive and negative sentiment strength of texts are detected. We focus on a non-English language that has few resources for text mining. This approach would help enhance the sentiment analysis in languages where a list of opinionated words does not exist. We propose a new method projects the text into dense and low dimensional feature vectors according to the sentiment strength of the words. We detect the mixture of positive and negative sentiments on a multi-variant scale. Empirical evaluation of the proposed framework on Turkish tweets shows that our approach gets good results for opinion mining.\n    ",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2017-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00051",
        "title": "Reading Wikipedia to Answer Open-Domain Questions",
        "authors": [
            "Danqi Chen",
            "Adam Fisch",
            "Jason Weston",
            "Antoine Bordes"
        ],
        "abstract": "This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.\n    ",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2017-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00052",
        "title": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion",
        "authors": [
            "Katharina Kann",
            "Ryan Cotterell",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.\n    ",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2017-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00057",
        "title": "Frames: A Corpus for Adding Memory to Goal-Oriented Dialogue Systems",
        "authors": [
            "Layla El Asri",
            "Hannes Schulz",
            "Shikhar Sharma",
            "Jeremie Zumer",
            "Justin Harris",
            "Emery Fine",
            "Rahul Mehrotra",
            "Kaheer Suleman"
        ],
        "abstract": "This paper presents the Frames dataset (Frames is available at ",
        "submission_date": "2017-03-31T00:00:00",
        "last_modified_date": "2017-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00119",
        "title": "Psychological and Personality Profiles of Political Extremists",
        "authors": [
            "Meysam Alizadeh",
            "Ingmar Weber",
            "Claudio Cioffi-Revilla",
            "Santo Fortunato",
            "Michael Macy"
        ],
        "abstract": "Global recruitment into radical Islamic movements has spurred renewed interest in the appeal of political extremism. Is the appeal a rational response to material conditions or is it the expression of psychological and personality disorders associated with aggressive behavior, intolerance, conspiratorial imagination, and paranoia? Empirical answers using surveys have been limited by lack of access to extremist groups, while field studies have lacked psychological measures and failed to compare extremists with contrast groups. We revisit the debate over the appeal of extremism in the U.S. context by comparing publicly available Twitter messages written by over 355,000 political extremist followers with messages written by non-extremist U.S. users. Analysis of text-based psychological indicators supports the moral foundation theory which identifies emotion as a critical factor in determining political orientation of individuals. Extremist followers also differ from others in four of the Big Five personality traits.\n    ",
        "submission_date": "2017-04-01T00:00:00",
        "last_modified_date": "2017-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00177",
        "title": "Sentiment Analysis of Citations Using Word2vec",
        "authors": [
            "Haixia Liu"
        ],
        "abstract": "Citation sentiment analysis is an important task in scientific paper analysis. Existing machine learning techniques for citation sentiment analysis are focusing on labor-intensive feature engineering, which requires large annotated corpus. As an automatic feature extraction tool, word2vec has been successfully applied to sentiment analysis of short texts. In this work, I conducted empirical research with the question: how well does word2vec work on the sentiment analysis of citations? The proposed method constructed sentence vectors (sent2vec) by averaging the word embeddings, which were learned from Anthology Collections (ACL-Embeddings). I also investigated polarity-specific word embeddings (PS-Embeddings) for classifying positive and negative citations. The sentence vectors formed a feature space, to which the examined citation sentence was mapped to. Those features were input into classifiers (support vector machines) for supervised classification. Using 10-cross-validation scheme, evaluation was conducted on a set of annotated citations. The results showed that word embeddings are effective on classifying positive and negative citations. However, hand-crafted features performed better for the overall classification.\n    ",
        "submission_date": "2017-04-01T00:00:00",
        "last_modified_date": "2017-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00200",
        "title": "Towards Building Large Scale Multimodal Domain-Aware Conversation Systems",
        "authors": [
            "Amrita Saha",
            "Mitesh Khapra",
            "Karthik Sankaranarayanan"
        ],
        "abstract": "While multimodal conversation agents are gaining importance in several domains such as retail, travel etc., deep learning research in this area has been limited primarily due to the lack of availability of large-scale, open chatlogs. To overcome this bottleneck, in this paper we introduce the task of multimodal, domain-aware conversations, and propose the MMD benchmark dataset. This dataset was gathered by working in close coordination with large number of domain experts in the retail domain. These experts suggested various conversations flows and dialog states which are typically seen in multimodal conversations in the fashion domain. Keeping these flows and states in mind, we created a dataset consisting of over 150K conversation sessions between shoppers and sales agents, with the help of in-house annotators using a semi-automated manually intense iterative process. With this dataset, we propose 5 new sub-tasks for multimodal conversations along with their evaluation methodology. We also propose two multimodal neural models in the encode-attend-decode paradigm and demonstrate their performance on two of the sub-tasks, namely text response generation and best image response selection. These experiments serve to establish baseline performance and open new research directions for each of these sub-tasks. Further, for each of the sub-tasks, we present a `per-state evaluation' of 9 most significant dialog states, which would enable more focused research into understanding the challenges and complexities involved in each of these states.\n    ",
        "submission_date": "2017-04-01T00:00:00",
        "last_modified_date": "2018-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00217",
        "title": "Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification",
        "authors": [
            "Lianhui Qin",
            "Zhisong Zhang",
            "Hai Zhao",
            "Zhiting Hu",
            "Eric P. Xing"
        ],
        "abstract": "Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.\n    ",
        "submission_date": "2017-04-01T00:00:00",
        "last_modified_date": "2017-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00253",
        "title": "Building a Neural Machine Translation System Using Only Synthetic Parallel Data",
        "authors": [
            "Jaehong Park",
            "Jongyoon Song",
            "Sungroh Yoon"
        ],
        "abstract": "Recent works have shown that synthetic parallel data automatically generated by translation models can be effective for various neural machine translation (NMT) issues. In this study, we build NMT systems using only synthetic parallel data. As an efficient alternative to real parallel data, we also present a new type of synthetic parallel corpus. The proposed pseudo parallel data are distinct from previous works in that ground truth and synthetic examples are mixed on both sides of sentence pairs. Experiments on Czech-German and French-German translations demonstrate the efficacy of the proposed pseudo parallel corpus, which shows not only enhanced results for bidirectional translation tasks but also substantial improvement with the aid of a ground truth real parallel corpus.\n    ",
        "submission_date": "2017-04-02T00:00:00",
        "last_modified_date": "2017-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00380",
        "title": "Word-Alignment-Based Segment-Level Machine Translation Evaluation using Word Embeddings",
        "authors": [
            "Junki Matsuo",
            "Mamoru Komachi",
            "Katsuhito Sudoh"
        ],
        "abstract": "One of the most important problems in machine translation (MT) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference, especially at the segment level. We propose to use word embeddings to perform word alignment for segment-level MT evaluation. We performed experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with various translation datasets. Experimental results show that our proposed methods outperform previous word embeddings-based methods.\n    ",
        "submission_date": "2017-04-02T00:00:00",
        "last_modified_date": "2017-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00405",
        "title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling",
        "authors": [
            "Feng Qian",
            "Lei Sha",
            "Baobao Chang",
            "Lu-chen Liu",
            "Ming Zhang"
        ],
        "abstract": "As for semantic role labeling (SRL) task, when it comes to utilizing parsing information, both traditional methods and recent recurrent neural network (RNN) based methods use the feature engineering way. In this paper, we propose Syntax Aware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies according to dependency parsing information in order to model parsing information directly in an architecture engineering way instead of feature engineering way. We experimentally demonstrate that SA-LSTM gains more improvement from the model architecture. Furthermore, SA-LSTM outperforms the state-of-the-art on CPB 1.0 significantly according to Student t-test ($p<0.05$).\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00440",
        "title": "Combining Lexical and Syntactic Features for Detecting Content-dense Texts in News",
        "authors": [
            "Yinfei Yang",
            "Ani Nenkova"
        ],
        "abstract": "Content-dense news report important factual information about an event in direct, succinct manner. Information seeking applications such as information extraction, question answering and summarization normally assume all text they deal with is content-dense. Here we empirically test this assumption on news articles from the business, U.S. international relations, sports and science journalism domains. Our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector. We heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features. On manually annotated data, we compare the performance of domain-specific classifiers, trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together. Our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label. Domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer. Domain independent classifiers reproduce better naive crowdsourced judgements. Classification prediction is high across all conditions, around 80%.\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2017-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00514",
        "title": "Multi-Task Learning of Keyphrase Boundary Classification",
        "authors": [
            "Isabelle Augenstein",
            "Anders S\u00f8gaard"
        ],
        "abstract": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00552",
        "title": "A Transition-Based Directed Acyclic Graph Parser for UCCA",
        "authors": [
            "Daniel Hershcovich",
            "Omri Abend",
            "Ari Rappoport"
        ],
        "abstract": "We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2017-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00559",
        "title": "Neural Lattice-to-Sequence Models for Uncertain Inputs",
        "authors": [
            "Matthias Sperber",
            "Graham Neubig",
            "Jan Niehues",
            "Alex Waibel"
        ],
        "abstract": "The input to a neural sequence-to-sequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM's child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00656",
        "title": "Detection and Resolution of Rumours in Social Media: A Survey",
        "authors": [
            "Arkaitz Zubiaga",
            "Ahmet Aker",
            "Kalina Bontcheva",
            "Maria Liakata",
            "Rob Procter"
        ],
        "abstract": "Despite the increasing use of social media platforms for information and news gathering, its unmoderated nature often leads to the emergence and spread of rumours, i.e. pieces of information that are unverified at the time of posting. At the same time, the openness of social media platforms provides opportunities to study how users share and discuss rumours, and to explore how natural language processing and data mining techniques may be used to find ways of determining their veracity. In this survey we introduce and discuss two types of rumours that circulate on social media; long-standing rumours that circulate for long periods of time, and newly-emerging rumours spawned during fast-paced events such as breaking news, where reports are released piecemeal and often with an unverified status in their early stages. We provide an overview of research into social media rumours with the ultimate goal of developing a rumour classification system that consists of four components: rumour detection, rumour tracking, rumour stance classification and rumour veracity classification. We delve into the approaches presented in the scientific literature for the development of each of these four components. We summarise the efforts and achievements so far towards the development of rumour classification systems and conclude with suggestions for avenues for future research in social media mining for detection and resolution of rumours.\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2018-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00774",
        "title": "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality",
        "authors": [
            "Alexandre Salle",
            "Aline Villavicencio"
        ],
        "abstract": "Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, with significant increase of computational cost. Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve language model performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated Recurrent Units and Long Short-Term Memory.\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2018-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00849",
        "title": "Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks",
        "authors": [
            "Chin-Cheng Hsu",
            "Hsin-Te Hwang",
            "Yi-Chiao Wu",
            "Yu Tsao",
            "Hsin-Min Wang"
        ],
        "abstract": "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.\n    ",
        "submission_date": "2017-04-04T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00898",
        "title": "Interpretation of Semantic Tweet Representations",
        "authors": [
            "J Ganesh",
            "Manish Gupta",
            "Vasudeva Varma"
        ],
        "abstract": "Research in analysis of microblogging platforms is experiencing a renewed surge with a large number of works applying representation learning models for applications like sentiment analysis, semantic textual similarity computation, hashtag prediction, etc. Although the performance of the representation learning models has been better than the traditional baselines for such tasks, little is known about the elementary properties of a tweet encoded within these representations, or why particular representations work better for certain tasks. Our work presented here constitutes the first step in opening the black-box of vector embeddings for tweets. Traditional feature engineering methods for high-level applications have exploited various elementary properties of tweets. We believe that a tweet representation is effective for an application because it meticulously encodes the application-specific elementary properties of tweets. To understand the elementary properties encoded in a tweet representation, we evaluate the representations on the accuracy to which they can model each of those properties such as tweet length, presence of particular words, hashtags, mentions, capitalization, etc. Our systematic extensive study of nine supervised and four unsupervised tweet representations against most popular eight textual and five social elementary properties reveal that Bi-directional LSTMs (BLSTMs) and Skip-Thought Vectors (STV) best encode the textual and social properties of tweets respectively. FastText is the best model for low resource settings, providing very little degradation with reduction in embedding size. Finally, we draw interesting insights by correlating the model performance obtained for elementary property prediction tasks with the highlevel downstream applications.\n    ",
        "submission_date": "2017-04-04T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00924",
        "title": "Japanese Sentiment Classification using a Tree-Structured Long Short-Term Memory with Attention",
        "authors": [
            "Ryosuke Miyazaki",
            "Mamoru Komachi"
        ],
        "abstract": "Previous approaches to training syntax-based sentiment classification models required phrase-level annotated corpora, which are not readily available in many languages other than English. Thus, we propose the use of tree-structured Long Short-Term Memory with an attention mechanism that pays attention to each subtree of the parse tree. Experimental results indicate that our model achieves the state-of-the-art performance in a Japanese sentiment classification task.\n    ",
        "submission_date": "2017-04-04T00:00:00",
        "last_modified_date": "2018-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00939",
        "title": "Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines",
        "authors": [
            "Youness Mansar",
            "Lorenzo Gatti",
            "Sira Ferradans",
            "Marco Guerini",
            "Jacopo Staiano"
        ],
        "abstract": "In this paper, we describe a methodology to infer Bullish or Bearish sentiment towards companies/brands. More specifically, our approach leverages affective lexica and word embeddings in combination with convolutional neural networks to infer the sentiment of financial news headlines towards a target company. Such architecture was used and evaluated in the context of the SemEval 2017 challenge (task 5, subtask 2), in which it obtained the best performance.\n    ",
        "submission_date": "2017-04-04T00:00:00",
        "last_modified_date": "2017-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01074",
        "title": "Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory",
        "authors": [
            "Hao Zhou",
            "Minlie Huang",
            "Tianyang Zhang",
            "Xiaoyan Zhu",
            "Bing Liu"
        ],
        "abstract": "Perception and expression of emotion are key factors to the success of dialogue systems or conversational agents. However, this problem has not been studied in large-scale conversation generation so far. In this paper, we propose Emotional Chatting Machine (ECM) that can generate appropriate responses not only in content (relevant and grammatical) but also in emotion (emotionally consistent). To the best of our knowledge, this is the first work that addresses the emotion factor in large-scale conversation generation. ECM addresses the factor using three new mechanisms that respectively (1) models the high-level abstraction of emotion expressions by embedding emotion categories, (2) captures the change of implicit internal emotion states, and (3) uses explicit emotion expressions with an external emotion vocabulary. Experiments show that the proposed model can generate responses appropriate not only in content but also in emotion.\n    ",
        "submission_date": "2017-04-04T00:00:00",
        "last_modified_date": "2018-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01314",
        "title": "Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF",
        "authors": [
            "Yan Shao",
            "Christian Hardmeier",
            "J\u00f6rg Tiedemann",
            "Joakim Nivre"
        ],
        "abstract": "We present a character-based model for joint segmentation and POS tagging for Chinese. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and lower-than-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our model is accurate and robust across datasets in different sizes, genres and annotation schemes. We obtain state-of-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and POS tagging.\n    ",
        "submission_date": "2017-04-05T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01346",
        "title": "CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity",
        "authors": [
            "Jeremy Ferrero",
            "Frederic Agnes",
            "Laurent Besacier",
            "Didier Schwab"
        ],
        "abstract": "We present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of Spanish-English sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1st on track 4a with a correlation of 83.02% with human annotations.\n    ",
        "submission_date": "2017-04-05T00:00:00",
        "last_modified_date": "2017-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01419",
        "title": "Linear Ensembles of Word Embedding Models",
        "authors": [
            "Avo Murom\u00e4gi",
            "Kairit Sirts",
            "Sven Laur"
        ],
        "abstract": "This paper explores linear methods for combining several word embedding models into an ensemble. We construct the combined models using an iterative method based on either ordinary least squares regression or the solution to the orthogonal Procrustes problem.\n",
        "submission_date": "2017-04-05T00:00:00",
        "last_modified_date": "2017-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01523",
        "title": "MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional Neural Networks",
        "authors": [
            "Ji Young Lee",
            "Franck Dernoncourt",
            "Peter Szolovits"
        ],
        "abstract": "Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).\n    ",
        "submission_date": "2017-04-05T00:00:00",
        "last_modified_date": "2017-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01631",
        "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition",
        "authors": [
            "Shubham Toshniwal",
            "Hao Tang",
            "Liang Lu",
            "Karen Livescu"
        ],
        "abstract": "End-to-end training of deep learning-based models allows for implicit learning of intermediate representations based on the final task loss. However, the end-to-end approach ignores the useful domain knowledge encoded in explicit intermediate-level supervision. We hypothesize that using intermediate representations as auxiliary supervision at lower levels of deep networks may be a good way of combining the advantages of end-to-end training and more traditional pipeline approaches. We present experiments on conversational speech recognition where we use lower-level tasks, such as phoneme recognition, in a multitask training approach with an encoder-decoder model for direct character transcription. We compare multiple types of lower-level tasks and analyze the effects of the auxiliary tasks. Our results on the Switchboard corpus show that this approach improves recognition accuracy over a standard encoder-decoder model on the Eval2000 test set.\n    ",
        "submission_date": "2017-04-05T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01653",
        "title": "Automatic Measurement of Pre-aspiration",
        "authors": [
            "Yaniv Sheena",
            "M\u00ed\u0161a Hejn\u00e1",
            "Yossi Adi",
            "Joseph Keshet"
        ],
        "abstract": "Pre-aspiration is defined as the period of glottal friction occurring in sequences of vocalic/consonantal sonorants and phonetically voiceless obstruents. We propose two machine learning methods for automatic measurement of pre-aspiration duration: a feedforward neural network, which works at the frame level; and a structured prediction model, which relies on manually designed feature functions, and works at the segment level. The input for both algorithms is a speech signal of an arbitrary length containing a single obstruent, and the output is a pair of times which constitutes the pre-aspiration boundaries. We train both models on a set of manually annotated examples. Results suggest that the structured model is superior to the frame-based model as it yields higher accuracy in predicting the boundaries and generalizes to new speakers and new languages. Finally, we demonstrate the applicability of our structured prediction algorithm by replicating linguistic analysis of pre-aspiration in Aberystwyth English with high correlation.\n    ",
        "submission_date": "2017-04-05T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01691",
        "title": "Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction",
        "authors": [
            "Chunting Zhou",
            "Graham Neubig"
        ],
        "abstract": "Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.\n    ",
        "submission_date": "2017-04-06T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01696",
        "title": "A Syntactic Neural Model for General-Purpose Code Generation",
        "authors": [
            "Pengcheng Yin",
            "Graham Neubig"
        ],
        "abstract": "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.\n    ",
        "submission_date": "2017-04-06T00:00:00",
        "last_modified_date": "2017-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01748",
        "title": "MRA - Proof of Concept of a Multilingual Report Annotator Web Application",
        "authors": [
            "Lu\u00eds Campos",
            "Francisco Couto"
        ],
        "abstract": "MRA (Multilingual Report Annotator) is a web application that translates Radiology text and annotates it with RadLex terms. Its goal is to explore the solution of translating non-English Radiology reports as a way to solve the problem of most of the Text Mining tools being developed for English. In this brief paper we explain the language barrier problem and shortly describe the application. MRA can be found at ",
        "submission_date": "2017-04-06T00:00:00",
        "last_modified_date": "2017-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01792",
        "title": "Neural Question Generation from Text: A Preliminary Study",
        "authors": [
            "Qingyu Zhou",
            "Nan Yang",
            "Furu Wei",
            "Chuanqi Tan",
            "Hangbo Bao",
            "Ming Zhou"
        ],
        "abstract": "Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub-spans of the given passage. Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions. In this work, we propose to apply the neural encoder-decoder model to generate meaningful and diverse questions from natural language sentences. The encoder reads the input text and the answer position, to produce an answer-aware input representation, which is fed to the decoder to generate an answer focused question. We conduct a preliminary study on neural question generation from text with the SQuAD dataset, and the experiment results show that our method can produce fluent and diverse questions.\n    ",
        "submission_date": "2017-04-06T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01938",
        "title": "The Interplay of Semantics and Morphology in Word Embeddings",
        "authors": [
            "Oded Avraham",
            "Yoav Goldberg"
        ],
        "abstract": "We explore the ability of word embeddings to capture both semantic and morphological similarity, as affected by the different types of linguistic properties (surface form, lemma, morphological tag) used to compose the representation of each word. We train several models, where each uses a different subset of these properties to compose its representations. By evaluating the models on semantic and morphological measures, we reveal some useful insights on the relationship between semantics and morphology.\n    ",
        "submission_date": "2017-04-06T00:00:00",
        "last_modified_date": "2017-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01975",
        "title": "An Automated Text Categorization Framework based on Hyperparameter Optimization",
        "authors": [
            "Eric S. Tellez",
            "Daniela Moctezuma",
            "Sabino Miranda-J\u00edmenez",
            "Mario Graff"
        ],
        "abstract": "A great variety of text tasks such as topic or spam identification, user profiling, and sentiment analysis can be posed as a supervised learning problem and tackle using a text classifier. A text classifier consists of several subprocesses, some of them are general enough to be applied to any supervised learning problem, whereas others are specifically designed to tackle a particular task, using complex and computational expensive processes such as lemmatization, syntactic analysis, etc. Contrary to traditional approaches, we propose a minimalistic and wide system able to tackle text classification tasks independent of domain and language, namely microTC. It is composed by some easy to implement text transformations, text representations, and a supervised learning algorithm. These pieces produce a competitive classifier even in the domain of informally written text. We provide a detailed description of microTC along with an extensive experimental comparison with relevant state-of-the-art methods. mircoTC was compared on 30 different datasets. Regarding accuracy, microTC obtained the best performance in 20 datasets while achieves competitive results in the remaining 10. The compared datasets include several problems like topic and polarity classification, spam detection, user profiling and authorship attribution. Furthermore, it is important to state that our approach allows the usage of the technology even without knowledge of machine learning and natural language processing.\n    ",
        "submission_date": "2017-04-06T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02080",
        "title": "Conversation Modeling on Reddit using a Graph-Structured LSTM",
        "authors": [
            "Vicky Zayats",
            "Mari Ostendorf"
        ],
        "abstract": "This paper presents a novel approach for modeling threaded discussions on social media using a graph-structured bidirectional LSTM which represents both hierarchical and temporal conversation structure. In experiments with a task of predicting popularity of comments in Reddit discussions, the proposed model outperforms a node-independent architecture for different sets of input features. Analyses show a benefit to the model over the full course of the discussion, improving detection in both early and late stages. Further, the use of language cues with the bidirectional tree state updates helps with identifying controversial comments.\n    ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02090",
        "title": "Conceptualization Topic Modeling",
        "authors": [
            "Yi-Kun Tang",
            "Xian-Ling Mao",
            "Heyan Huang",
            "Guihua Wen"
        ],
        "abstract": "Recently, topic modeling has been widely used to discover the abstract topics in text corpora. Most of the existing topic models are based on the assumption of three-layer hierarchical Bayesian structure, i.e. each document is modeled as a probability distribution over topics, and each topic is a probability distribution over words. However, the assumption is not optimal. Intuitively, it's more reasonable to assume that each topic is a probability distribution over concepts, and then each concept is a probability distribution over words, i.e. adding a latent concept layer between topic layer and word layer in traditional three-layer assumption. In this paper, we verify the proposed assumption by incorporating the new assumption in two representative topic models, and obtain two novel topic models. Extensive experiments were conducted among the proposed models and corresponding baselines, and the results show that the proposed models significantly outperform the baselines in terms of case study and perplexity, which means the new assumption is more reasonable than traditional one.\n    ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02134",
        "title": "Adposition and Case Supersenses v2.6: Guidelines for English",
        "authors": [
            "Nathan Schneider",
            "Jena D. Hwang",
            "Vivek Srikumar",
            "Archna Bhatia",
            "Na-Rae Han",
            "Tim O'Gorman",
            "Sarah R. Moeller",
            "Omri Abend",
            "Adi Shalev",
            "Austin Blodgett",
            "Jakob Prange"
        ],
        "abstract": "This document offers a detailed linguistic description of SNACS (Semantic Network of Adposition and Case Supersenses; Schneider et al., 2018), an inventory of 52 semantic labels (\"supersenses\") that characterize the use of adpositions and case markers at a somewhat coarse level of granularity, as demonstrated in the STREUSLE corpus (",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2022-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02156",
        "title": "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing",
        "authors": [
            "Rik van Noord",
            "Johan Bos"
        ],
        "abstract": "We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.\n    ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02215",
        "title": "EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for kEyphrase ClassificaTION",
        "authors": [
            "Steffen Eger",
            "Erik-L\u00e2n Do Dinh",
            "Ilia Kuznetsov",
            "Masoud Kiaeeha",
            "Iryna Gurevych"
        ],
        "abstract": "This paper describes our approach to the SemEval 2017 Task 10: \"Extracting Keyphrases and Relations from Scientific Publications\", specifically to Subtask (B): \"Classification of identified keyphrases\". We explored three different deep learning approaches: a character-level convolutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-F1-score of 0.63 on the test data. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15% of the full data, namely, the original development set. When trained on the full data (training+development), our ensemble has a micro-F1-score of 0.69. Our code is available from ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02263",
        "title": "NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis",
        "authors": [
            "Edilson A. Corr\u00eaa Jr.",
            "Vanessa Queiroz Marinho",
            "Leandro Borges dos Santos"
        ],
        "abstract": "This paper describes our multi-view ensemble approach to SemEval-2017 Task 4 on Sentiment Analysis in Twitter, specifically, the Message Polarity Classification subtask for English (subtask A). Our system is a voting ensemble, where each base classifier is trained in a different feature space. The first space is a bag-of-words model and has a Linear SVM as base classifier. The second and third spaces are two different strategies of combining word embeddings to represent sentences and use a Linear SVM and a Logistic Regressor as base classifiers. The proposed system was ranked 18th out of 38 systems considering F1 score and 20th considering recall.\n    ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02293",
        "title": "Comparison of Global Algorithms in Word Sense Disambiguation",
        "authors": [
            "Lo\u00efc Vial",
            "Andon Tchechmedjiev",
            "Didier Schwab"
        ],
        "abstract": "This article compares four probabilistic algorithms (global algorithms) for Word Sense Disambiguation (WSD) in terms of the number of scorer calls (local algo- rithm) and the F1 score as determined by a gold-standard scorer. Two algorithms come from the state of the art, a Simulated Annealing Algorithm (SAA) and a Genetic Algorithm (GA) as well as two algorithms that we first adapt from WSD that are state of the art probabilistic search algorithms, namely a Cuckoo search algorithm (CSA) and a Bat Search algorithm (BS). As WSD requires to evaluate exponentially many word sense combinations (with branching factors of up to 6 or more), probabilistic algorithms allow to find approximate solution in a tractable time by sampling the search space. We find that CSA, GA and SA all eventually converge to similar results (0.98 F1 score), but CSA gets there faster (in fewer scorer calls) and reaches up to 0.95 F1 before SA in fewer scorer calls. In BA a strict convergence criterion prevents it from reaching above 0.89 F1.\n    ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02312",
        "title": "A Constrained Sequence-to-Sequence Neural Model for Sentence Simplification",
        "authors": [
            "Yaoyuan Zhang",
            "Zhenxu Ye",
            "Yansong Feng",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "abstract": "Sentence simplification reduces semantic complexity to benefit people with language impairments. Previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges. For sentence-level studies, sentences after simplification are fluent but sometimes are not really simplified. For word-level studies, words are simplified but also have potential grammar errors due to different usages of words before and after simplification. In this paper, we propose a two-step simplification framework by combining both the word-level and the sentence-level simplifications, making use of their corresponding advantages. Based on the two-step framework, we implement a novel constrained neural generation model to simplify sentences given simplified words. The final results on Wikipedia and Simple Wikipedia aligned datasets indicate that our method yields better performance than various baselines.\n    ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02362",
        "title": "Fostering User Engagement: Rhetorical Devices for Applause Generation Learnt from TED Talks",
        "authors": [
            "Zhe Liu",
            "Anbang Xu",
            "Mengdi Zhang",
            "Jalal Mahmud",
            "Vibha Sinha"
        ],
        "abstract": "One problem that every presenter faces when delivering a public discourse is how to hold the listeners' attentions or to keep them involved. Therefore, many studies in conversation analysis work on this issue and suggest qualitatively con-structions that can effectively lead to audience's applause. To investigate these proposals quantitatively, in this study we an-alyze the transcripts of 2,135 TED Talks, with a particular fo-cus on the rhetorical devices that are used by the presenters for applause elicitation. Through conducting regression anal-ysis, we identify and interpret 24 rhetorical devices as triggers of audience applauding. We further build models that can rec-ognize applause-evoking sentences and conclude this work with potential implications.\n    ",
        "submission_date": "2017-03-17T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02385",
        "title": "A Trolling Hierarchy in Social Media and A Conditional Random Field For Trolling Detection",
        "authors": [
            "Luis Gerardo Mojica"
        ],
        "abstract": "An-ever increasing number of social media websites, electronic newspapers and Internet forums allow visitors to leave comments for others to read and interact. This exchange is not free from participants with malicious intentions, which do not contribute with the written conversation. Among different communities users adopt strategies to handle such users. In this paper we present a comprehensive categorization of the trolling phenomena resource, inspired by politeness research and propose a model that jointly predicts four crucial aspects of trolling: intention, interpretation, intention disclosure and response strategy. Finally, we present a new annotated dataset containing excerpts of conversations involving trolls and the interactions with other users that we hope will be a useful resource for the research community.\n    ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02497",
        "title": "On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models",
        "authors": [
            "Steffen Eger",
            "Alexander Mehler"
        ],
        "abstract": "We consider two graph models of semantic change. The first is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods. In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the word's meaning across two time points. We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, today's embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change.\n    ",
        "submission_date": "2017-04-08T00:00:00",
        "last_modified_date": "2017-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02565",
        "title": "Prosody: The Rhythms and Melodies of Speech",
        "authors": [
            "Dafydd Gibbon"
        ],
        "abstract": "The present contribution is a tutorial on selected aspects of prosody, the rhythms and melodies of speech, based on a course of the same name at the Summer School on Contemporary Phonetics and Phonology at Tongji University, Shanghai, China in July 2016. The tutorial is not intended as an introduction to experimental methodology or as an overview of the literature on the topic, but as an outline of observationally accessible aspects of fundamental frequency and timing patterns with the aid of computational visualisation, situated in a semiotic framework of sign ranks and interpretations. After an informal introduction to the basic concepts of prosody in the introduction and a discussion of the place of prosody in the architecture of language, a selection of acoustic phonetic topics in phonemic tone and accent prosody, word prosody, phrasal prosody and discourse prosody are discussed, and a stylisation method for visualising aspects of prosody is introduced. Examples are taken from a number of typologically different languages: Anyi/Agni (Niger-Congo>Kwa, Ivory Coast), English, Kuki-Thadou (Sino-Tibetan, North-East India and Myanmar), Mandarin Chinese, Tem (Niger-Congo>Gur, Togo) and Farsi. The main focus is on fundamental frequency patterns, but issues of timing and rhythm are also discussed. In the final section, further reading and possible future research directions are outlined.\n    ",
        "submission_date": "2017-04-09T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02709",
        "title": "Improving Implicit Semantic Role Labeling by Predicting Semantic Frame Arguments",
        "authors": [
            "Quynh Ngoc Thi Do",
            "Steven Bethard",
            "Marie-Francine Moens"
        ],
        "abstract": "Implicit semantic role labeling (iSRL) is the task of predicting the semantic roles of a predicate that do not appear as explicit arguments, but rather regard common sense knowledge or are mentioned earlier in the discourse. We introduce an approach to iSRL based on a predictive recurrent neural semantic frame model (PRNSFM) that uses a large unannotated corpus to learn the probability of a sequence of semantic arguments given a predicate. We leverage the sequence probabilities predicted by the PRNSFM to estimate selectional preferences for predicates and their arguments. On the NomBank iSRL test set, our approach improves state-of-the-art performance on implicit semantic role labeling with less reliance than prior work on manually constructed language resources.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02788",
        "title": "Entity Linking for Queries by Searching Wikipedia Sentences",
        "authors": [
            "Chuanqi Tan",
            "Furu Wei",
            "Pengjie Ren",
            "Weifeng Lv",
            "Ming Zhou"
        ],
        "abstract": "We present a simple yet effective approach for linking entities in queries. The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query. Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework. The advantages of our approach lie in two aspects, which contribute to the ranking process and final linking result. First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query. Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles. We conduct experiments on two benchmark datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that our method outperforms state-of-the-art systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ dataset.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02813",
        "title": "Character-Word LSTM Language Models",
        "authors": [
            "Lyan Verwimp",
            "Joris Pelemans",
            "Hugo Van hamme",
            "Patrick Wambacq"
        ],
        "abstract": "We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02853",
        "title": "SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications",
        "authors": [
            "Isabelle Augenstein",
            "Mrinal Das",
            "Sebastian Riedel",
            "Lakshmi Vikraman",
            "Andrew McCallum"
        ],
        "abstract": "We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02923",
        "title": "Pay Attention to Those Sets! Learning Quantification from Images",
        "authors": [
            "Ionut Sorodoc",
            "Sandro Pezzelle",
            "Aur\u00e9lie Herbelot",
            "Mariella Dimiccoli",
            "Raffaella Bernardi"
        ],
        "abstract": "Major advances have recently been made in merging language and vision representations. But most tasks considered so far have confined themselves to the processing of objects and lexicalised relations amongst objects (content words). We know, however, that humans (even pre-school children) can abstract over raw data to perform certain types of higher-level reasoning, expressed in natural language by function words. A case in point is given by their ability to learn quantifiers, i.e. expressions like 'few', 'some' and 'all'. From formal semantics and cognitive linguistics, we know that quantifiers are relations over sets which, as a simplification, we can see as proportions. For instance, in 'most fish are red', most encodes the proportion of fish which are red fish. In this paper, we study how well current language and vision strategies model such relations. We show that state-of-the-art attention mechanisms coupled with a traditional linguistic formalisation of quantifiers gives best performance on the task. Additionally, we provide insights on the role of 'gist' representations in quantification. A 'logical' strategy to tackle the task would be to first obtain a numerosity estimation for the two involved sets and then compare their cardinalities. We however argue that precisely identifying the composition of the sets is not only beyond current state-of-the-art models but perhaps even detrimental to a task that is most efficiently performed by refining the approximate numerosity estimator of the system.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02963",
        "title": "Exploring Word Embeddings for Unsupervised Textual User-Generated Content Normalization",
        "authors": [
            "Thales Felipe Costa Bertaglia",
            "Maria das Gra\u00e7as Volpe Nunes"
        ],
        "abstract": "Text normalization techniques based on rules, lexicons or supervised training requiring large corpora are not scalable nor domain interchangeable, and this makes them unsuitable for normalizing user-generated content (UGC). Current tools available for Brazilian Portuguese make use of such techniques. In this work we propose a technique based on distributed representation of words (or word embeddings). It generates continuous numeric vectors of high-dimensionality to represent words. The vectors explicitly encode many linguistic regularities and patterns, as well as syntactic and semantic word relationships. Words that share semantic similarity are represented by similar vectors. Based on these features, we present a totally unsupervised, expandable and language and domain independent method for learning normalization lexicons from word embeddings. Our approach obtains high correction rate of orthographic errors and internet slang in product reviews, outperforming the current available tools for Brazilian Portuguese.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03013",
        "title": "Automatic Classification of the Complexity of Nonfiction Texts in Portuguese for Early School Years",
        "authors": [
            "Nathan Siegle Hartmann",
            "Livia Cucatto",
            "Danielle Brants",
            "Sandra Alu\u00edsio"
        ],
        "abstract": "Recent research shows that most Brazilian students have serious problems regarding their reading skills. The full development of this skill is key for the academic and professional future of every citizen. Tools for classifying the complexity of reading materials for children aim to improve the quality of the model of teaching reading and text comprehension. For English, Fengs work [11] is considered the state-of-art in grade level prediction and achieved 74% of accuracy in automatically classifying 4 levels of textual complexity for close school grades. There are no classifiers for nonfiction texts for close grades in Portuguese. In this article, we propose a scheme for manual annotation of texts in 5 grade levels, which will be used for customized reading to avoid the lack of interest by students who are more advanced in reading and the blocking of those that still need to make further progress. We obtained 52% of accuracy in classifying texts into 5 levels and 74% in 3 levels. The results prove to be promising when compared to the state-of-art work.9\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03016",
        "title": "Automatic semantic role labeling on non-revised syntactic trees of journalistic texts",
        "authors": [
            "Nathan Siegle Hartmann",
            "Magali Sanches Duran",
            "Sandra Maria Alu\u00edsio"
        ],
        "abstract": "Semantic Role Labeling (SRL) is a Natural Language Processing task that enables the detection of events described in sentences and the participants of these events. For Brazilian Portuguese (BP), there are two studies recently concluded that perform SRL in journalistic texts. [1] obtained F1-measure scores of 79.6, using the ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03084",
        "title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning",
        "authors": [
            "Baolin Peng",
            "Xiujun Li",
            "Lihong Li",
            "Jianfeng Gao",
            "Asli Celikyilmaz",
            "Sungjin Lee",
            "Kam-Fai Wong"
        ],
        "abstract": "Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to collectively complete multiple subtasks. For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in. This paper addresses this challenge by formulating the task in the mathematical framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales. The dialogue manager consists of: (1) a top-level dialogue policy that selects among subtasks or options, (2) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied. Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on handcrafted rules and the other based on flat deep reinforcement learning.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03169",
        "title": "Later-stage Minimum Bayes-Risk Decoding for Neural Machine Translation",
        "authors": [
            "Raphael Shu",
            "Hideki Nakayama"
        ],
        "abstract": "For extended periods of time, sequence generation models rely on beam search algorithm to generate output sequence. However, the correctness of beam search degrades when the a model is over-confident about a suboptimal prediction. In this paper, we propose to perform minimum Bayes-risk (MBR) decoding for some extra steps at a later stage. In order to speed up MBR decoding, we compute the Bayes risks on GPU in batch mode. In our experiments, we found that MBR reranking works with a large beam size. Later-stage MBR decoding is shown to outperform simple MBR reranking in machine translation tasks.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03223",
        "title": "Persian Wordnet Construction using Supervised Learning",
        "authors": [
            "Zahra Mousavi",
            "Heshaam Faili"
        ],
        "abstract": "This paper presents an automated supervised method for Persian wordnet construction. Using a Persian corpus and a bi-lingual dictionary, the initial links between Persian words and Princeton WordNet synsets have been generated. These links will be discriminated later as correct or incorrect by employing seven features in a trained classification system. The whole method is just a classification system, which has been trained on a train set containing FarsNet as a set of correct instances. State of the art results on the automatically derived Persian wordnet is achieved. The resulted wordnet with a precision of 91.18% includes more than 16,000 words and 22,000 synsets.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2017-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03242",
        "title": "Automatic Keyword Extraction for Text Summarization: A Survey",
        "authors": [
            "Santosh Kumar Bharti",
            "Korra Sathya Babu"
        ],
        "abstract": "In recent times, data is growing rapidly in every domain such as news, social media, banking, education, etc. Due to the excessiveness of data, there is a need of automatic summarizer which will be capable to summarize the data especially textual data in original document without losing any critical purposes. Text summarization is emerged as an important research area in recent past. In this regard, review of existing work on text summarization process is useful for carrying out further research. In this paper, recent literature on automatic keyword extraction and text summarization are presented since text summarization process is highly depend on keyword extraction. This literature includes the discussion about different methodology used for keyword extraction and text summarization. It also discusses about different databases used for text summarization in several domains along with evaluation matrices. Finally, it discusses briefly about issues and research challenges faced by researchers along with future direction.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2017-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03279",
        "title": "Unfolding and Shrinking Neural Machine Translation Ensembles",
        "authors": [
            "Felix Stahlberg",
            "Bill Byrne"
        ],
        "abstract": "Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03407",
        "title": "What we really want to find by Sentiment Analysis: The Relationship between Computational Models and Psychological State",
        "authors": [
            "Hwiyeol Jo",
            "Soo-Min Kim",
            "Jeong Ryu"
        ],
        "abstract": "As the first step to model emotional state of a person, we build sentiment analysis models with existing deep neural network algorithms and compare the models with psychological measurements to enlighten the relationship. In the experiments, we first examined psychological state of 64 participants and asked them to summarize the story of a book, Chronicle of a Death Foretold (Marquez, 1981). Secondly, we trained models using crawled 365,802 movie review data; then we evaluated participants' summaries using the pretrained model as a concept of transfer learning. With the background that emotion affects on memories, we investigated the relationship between the evaluation score of the summaries from computational models and the examined psychological measurements. The result shows that although CNN performed the best among other deep neural network algorithms (LSTM, GRU), its results are not related to the psychological state. Rather, GRU shows more explainable results depending on the psychological state. The contribution of this paper can be summarized as follows: (1) we enlighten the relationship between computational models and psychological measurements. (2) we suggest this framework as objective methods to evaluate the emotion; the real sentiment analysis of a person.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2018-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03471",
        "title": "What do Neural Machine Translation Models Learn about Morphology?",
        "authors": [
            "Yonatan Belinkov",
            "Nadir Durrani",
            "Fahim Dalvi",
            "Hassan Sajjad",
            "James Glass"
        ],
        "abstract": "Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2018-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03560",
        "title": "ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with Multilingual Relational Knowledge",
        "authors": [
            "Robyn Speer",
            "Joanna Lowry-Duda"
        ],
        "abstract": "This paper describes Luminoso's participation in SemEval 2017 Task 2, \"Multilingual and Cross-lingual Semantic Word Similarity\", with a system based on ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses on general knowledge that relates the meanings of words and phrases. Our submission to SemEval was an update of previous work that builds high-quality, multilingual word embeddings from a combination of ConceptNet and distributional semantics. Our system took first place in both subtasks. It ranked first in 4 out of 5 of the separate languages, and also ranked first in all 10 of the cross-lingual language pairs.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2018-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03617",
        "title": "Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning",
        "authors": [
            "Matthew Riemer",
            "Elham Khabiri",
            "Richard Goodwin"
        ],
        "abstract": "Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks. We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.\n    ",
        "submission_date": "2017-04-12T00:00:00",
        "last_modified_date": "2017-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03693",
        "title": "Trainable Referring Expression Generation using Overspecification Preferences",
        "authors": [
            "Thiago castro Ferreira",
            "Ivandre Paraboni"
        ],
        "abstract": "Referring expression generation (REG) models that use speaker-dependent information require a considerable amount of training data produced by every individual speaker, or may otherwise perform poorly. In this work we present a simple REG experiment that allows the use of larger training data sets by grouping speakers according to their overspecification preferences. Intrinsic evaluation shows that this method generally outperforms the personalised method found in previous work.\n    ",
        "submission_date": "2017-04-12T00:00:00",
        "last_modified_date": "2017-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03956",
        "title": "Incremental Skip-gram Model with Negative Sampling",
        "authors": [
            "Nobuhiro Kaji",
            "Hayato Kobayashi"
        ],
        "abstract": "This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.\n    ",
        "submission_date": "2017-04-13T00:00:00",
        "last_modified_date": "2017-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03987",
        "title": "Mobile Keyboard Input Decoding with Finite-State Transducers",
        "authors": [
            "Tom Ouyang",
            "David Rybach",
            "Fran\u00e7oise Beaufays",
            "Michael Riley"
        ],
        "abstract": "We propose a finite-state transducer (FST) representation for the models used to decode keyboard inputs on mobile devices. Drawing from learnings from the field of speech recognition, we describe a decoding framework that can satisfy the strict memory and latency constraints of keyboard input. We extend this framework to support functionalities typically not present in speech recognition, such as literal decoding, autocorrections, word completions, and next word predictions.\n",
        "submission_date": "2017-04-13T00:00:00",
        "last_modified_date": "2017-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04008",
        "title": "A Neural Model for User Geolocation and Lexical Dialectology",
        "authors": [
            "Afshin Rahimi",
            "Trevor Cohn",
            "Timothy Baldwin"
        ],
        "abstract": "We propose a simple yet effective text- based user geolocation model based on a neural network with one hidden layer, which achieves state of the art performance over three Twitter benchmark geolocation datasets, in addition to producing word and phrase embeddings in the hidden layer that we show to be useful for detecting dialectal terms. As part of our analysis of dialectal terms, we release DAREDS, a dataset for evaluating dialect term detection methods.\n    ",
        "submission_date": "2017-04-13T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04100",
        "title": "Cross-lingual and cross-domain discourse segmentation of entire documents",
        "authors": [
            "Chlo\u00e9 Braud",
            "Oph\u00e9lie Lacroix",
            "Anders S\u00f8gaard"
        ],
        "abstract": "Discourse segmentation is a crucial step in building end-to-end discourse parsers. However, discourse segmenters only exist for a few languages and domains. Typically they only detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains. In this paper, we propose statistical discourse segmenters for five languages and three domains that do not rely on gold pre-annotations. We also consider the problem of learning discourse segmenters when no labeled data is available for a language. Our fully supervised system obtains 89.5% F1 for English newswire, with slight drops in performance on other domains, and we report supervised and unsupervised (cross-lingual) results for five languages in total.\n    ",
        "submission_date": "2017-04-13T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04154",
        "title": "Learning Joint Multilingual Sentence Representations with Neural Machine Translation",
        "authors": [
            "Holger Schwenk",
            "Matthijs Douze"
        ],
        "abstract": "In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics. We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.\n    ",
        "submission_date": "2017-04-13T00:00:00",
        "last_modified_date": "2017-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04198",
        "title": "Room for improvement in automatic image description: an error analysis",
        "authors": [
            "Emiel van Miltenburg",
            "Desmond Elliott"
        ],
        "abstract": "In recent years we have seen rapid and significant progress in automatic image description but what are the open problems in this area? Most work has been evaluated using text-based similarity metrics, which only indicate that there have been improvements, without explaining what has improved. In this paper, we present a detailed error analysis of the descriptions generated by a state-of-the-art attention-based model. Our analysis operates on two levels: first we check the descriptions for accuracy, and then we categorize the types of errors we observe in the inaccurate descriptions. We find only 20% of the descriptions are free from errors, and surprisingly that 26% are unrelated to the image. Finally, we manually correct the most frequently occurring error types (e.g. gender identification) to estimate the performance reward for addressing these errors, observing gains of 0.2--1 BLEU point per type.\n    ",
        "submission_date": "2017-04-13T00:00:00",
        "last_modified_date": "2017-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04222",
        "title": "Learning Latent Representations for Speech Generation and Transformation",
        "authors": [
            "Wei-Ning Hsu",
            "Yu Zhang",
            "James Glass"
        ],
        "abstract": "An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.\n    ",
        "submission_date": "2017-04-13T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04259",
        "title": "Identity and Granularity of Events in Text",
        "authors": [
            "Piek Vossen",
            "Agata Cybulska"
        ],
        "abstract": "In this paper we describe a method to detect event descrip- tions in different news articles and to model the semantics of events and their components using RDF representations. We compare these descriptions to solve a cross-document event coreference task. Our com- ponent approach to event semantics defines identity and granularity of events at different levels. It performs close to state-of-the-art approaches on the cross-document event coreference task, while outperforming other works when assuming similar quality of event detection. We demonstrate how granularity and identity are interconnected and we discuss how se- mantic anomaly could be used to define differences between coreference, subevent and topical relations.\n    ",
        "submission_date": "2017-04-13T00:00:00",
        "last_modified_date": "2017-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04336",
        "title": "An entity-driven recursive neural network model for chinese discourse coherence modeling",
        "authors": [
            "Fan Xu",
            "Shujing Du",
            "Maoxi Li",
            "Mingwen Wang"
        ],
        "abstract": "Chinese discourse coherence modeling remains a challenge taskin Natural Language Processing ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04347",
        "title": "Exploiting Cross-Sentence Context for Neural Machine Translation",
        "authors": [
            "Longyue Wang",
            "Zhaopeng Tu",
            "Andy Way",
            "Qun Liu"
        ],
        "abstract": "In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04368",
        "title": "Get To The Point: Summarization with Pointer-Generator Networks",
        "authors": [
            "Abigail See",
            "Peter J. Liu",
            "Christopher D. Manning"
        ],
        "abstract": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04441",
        "title": "How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?",
        "authors": [
            "Georg Heigold",
            "G\u00fcnter Neumann",
            "Josef van Genabith"
        ],
        "abstract": "This paper investigates the robustness of NLP against perturbed word forms. While neural approaches can achieve (almost) human-like accuracy for certain tasks and conditions, they often are sensitive to small changes in the input such as non-canonical input (e.g., typos). Yet both stability and robustness are desired properties in applications involving user-generated content, and the more as humans easily cope with such noisy or adversary conditions. In this paper, we study the impact of noisy input. We consider different noise distributions (one type of noise, combination of noise types) and mismatched noise distributions for training and testing. Moreover, we empirically evaluate the robustness of different models (convolutional neural networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), and different NLP tasks (morphological tagging, machine translation).\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04451",
        "title": "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics",
        "authors": [
            "Phong Le",
            "Ivan Titov"
        ],
        "abstract": "Coreference evaluation metrics are hard to optimize directly as they are non-differentiable functions, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04452",
        "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps",
        "authors": [
            "Tobias Falke",
            "Iryna Gurevych"
        ],
        "abstract": "Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04455",
        "title": "Cardinal Virtues: Extracting Relation Cardinalities from Text",
        "authors": [
            "Paramita Mirza",
            "Simon Razniewski",
            "Fariz Darari",
            "Gerhard Weikum"
        ],
        "abstract": "Information extraction (IE) from text has largely focused on relations between individual entities, such as who has won which award. However, some facts are never fully mentioned, and no IE method has perfect recall. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel problem of extracting cardinalities and discusses the specific challenges that set it apart from standard IE. We present a distant supervision method using conditional random fields. A preliminary evaluation results in precision between 3% and 55%, depending on the difficulty of relations.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04517",
        "title": "ShapeWorld - A new test methodology for multimodal language understanding",
        "authors": [
            "Alexander Kuhnle",
            "Ann Copestake"
        ],
        "abstract": "We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter's specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require true generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating various visual question answering models on four different tasks, and show how our framework gives us detailed insights into their capabilities and limitations. By open-sourcing our framework, we hope to stimulate progress in the field of multimodal language understanding.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04520",
        "title": "Neural Machine Translation Model with a Large Vocabulary Selected by Branching Entropy",
        "authors": [
            "Zi Long",
            "Ryuichiro Kimura",
            "Takehito Utsuro",
            "Tomoharu Mitsuhashi",
            "Mikio Yamamoto"
        ],
        "abstract": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because the training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In this paper, we propose to select phrases that contain out-of-vocabulary words using the statistical approach of branching entropy. This allows the proposed NMT system to be applied to a translation task of any language pair without any language-specific knowledge about technical term identification. The selected phrases are then replaced with tokens during training and post-translated by the phrase translation table of SMT. Evaluation on Japanese-to-Chinese, Chinese-to-Japanese, Japanese-to-English and English-to-Japanese patent sentence translation proved the effectiveness of phrases selected with branching entropy, where the proposed NMT model achieves a substantial improvement over a baseline NMT model without our proposed technique. Moreover, the number of translation errors of under-translation by the baseline NMT model without our proposed technique reduces to around half by the proposed NMT model.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04521",
        "title": "Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation",
        "authors": [
            "Zi Long",
            "Takehito Utsuro",
            "Tomoharu Mitsuhashi",
            "Mikio Yamamoto"
        ],
        "abstract": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In NMTs, words that are out of vocabulary are represented by a single unknown token. In this paper, we propose a method that enables NMT to translate patent sentences comprising a large vocabulary of technical terms. We train an NMT system on bilingual data wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Further, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using SMT. We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens. Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04530",
        "title": "Neural Extractive Summarization with Side Information",
        "authors": [
            "Shashi Narayan",
            "Nikos Papasarantopoulos",
            "Shay B. Cohen",
            "Mirella Lapata"
        ],
        "abstract": "Most extractive summarization methods focus on the main body of the document from which sentences need to be extracted. However, the gist of the document may lie in side information, such as the title and image captions which are often available for newswire articles. We propose to explore side information in the context of single-document extractive summarization. We develop a framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor with attention over side information. We evaluate our model on a large scale news dataset. We show that extractive summarization with side information consistently outperforms its counterpart that does not use any side information, in terms of both informativeness and fluency.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04539",
        "title": "Cross-lingual Abstract Meaning Representation Parsing",
        "authors": [
            "Marco Damonte",
            "Shay B. Cohen"
        ],
        "abstract": "Abstract Meaning Representation (AMR) annotation efforts have mostly focused on English. In order to train parsers on other languages, we propose a method based on annotation projection, which involves exploiting annotations in a source language and a parallel corpus of the source language and a target language. Using English as the source language, we show promising results for Italian, Spanish, German and Chinese as target languages. Besides evaluating the target parsers on non-gold datasets, we further propose an evaluation method that exploits the English gold annotations and does not require access to gold annotations for the target languages. This is achieved by inverting the projection process: a new English parser is learned from the target language parser and evaluated on the existing English gold standard.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2018-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04550",
        "title": "Distributional Modeling on a Diet: One-shot Word Learning from Text Only",
        "authors": [
            "Su Wang",
            "Stephen Roller",
            "Katrin Erk"
        ],
        "abstract": "We test whether distributional models can do one-shot learning of definitional properties from text only. Using Bayesian models, we find that first learning overarching structure in the known data, regularities in textual contexts and in properties, helps one-shot learning, and that individual context items can be highly informative. Our experiments show that our model can learn properties from a single exposure when given an informative utterance.\n    ",
        "submission_date": "2017-04-14T00:00:00",
        "last_modified_date": "2017-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04565",
        "title": "Neural Paraphrase Identification of Questions with Noisy Pretraining",
        "authors": [
            "Gaurav Singh Tomar",
            "Thyago Duque",
            "Oscar T\u00e4ckstr\u00f6m",
            "Jakob Uszkoreit",
            "Dipanjan Das"
        ],
        "abstract": "We present a solution to the problem of paraphrase identification of questions. We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model (Parikh et al., 2016) results in accurate performance on this task, while being far simpler than many competing neural architectures. Furthermore, when the model is pretrained on a noisy dataset of automatically collected question paraphrases, it obtains the best reported performance on the dataset.\n    ",
        "submission_date": "2017-04-15T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04601",
        "title": "MUSE: Modularizing Unsupervised Sense Embeddings",
        "authors": [
            "Guang-He Lee",
            "Yun-Nung Chen"
        ],
        "abstract": "This paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts. Prior work focused on designing a single model to deliver both mechanisms, and thus suffered from either coarse-grained representation learning or inefficient sense selection. The proposed modular approach, MUSE, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with linear-time sense selection. We leverage reinforcement learning to enable joint training on the proposed modules, and introduce various exploration techniques on sense selection for better robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on synonym selection as well as on contextual word similarities in terms of MaxSimC.\n    ",
        "submission_date": "2017-04-15T00:00:00",
        "last_modified_date": "2018-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04675",
        "title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
        "authors": [
            "Jasmijn Bastings",
            "Ivan Titov",
            "Wilker Aziz",
            "Diego Marcheggiani",
            "Khalil Sima'an"
        ],
        "abstract": "We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.\n    ",
        "submission_date": "2017-04-15T00:00:00",
        "last_modified_date": "2020-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04683",
        "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
        "authors": [
            "Guokun Lai",
            "Qizhe Xie",
            "Hanxiao Liu",
            "Yiming Yang",
            "Eduard Hovy"
        ],
        "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at ",
        "submission_date": "2017-04-15T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04743",
        "title": "Towards String-to-Tree Neural Machine Translation",
        "authors": [
            "Roee Aharoni",
            "Yoav Goldberg"
        ],
        "abstract": "We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system.\n    ",
        "submission_date": "2017-04-16T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04856",
        "title": "A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes",
        "authors": [
            "Pablo Loyola",
            "Edison Marrese-Taylor",
            "Yutaka Matsuo"
        ],
        "abstract": "We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.\n    ",
        "submission_date": "2017-04-17T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04859",
        "title": "Learning Character-level Compositionality with Visual Features",
        "authors": [
            "Frederick Liu",
            "Han Lu",
            "Chieh Lo",
            "Graham Neubig"
        ],
        "abstract": "Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry semantic content, resulting in embeddings that are coherent in visual space.\n    ",
        "submission_date": "2017-04-17T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04920",
        "title": "Deep Joint Entity Disambiguation with Local Neural Attention",
        "authors": [
            "Octavian-Eugen Ganea",
            "Thomas Hofmann"
        ],
        "abstract": "We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or state-of-the-art accuracy at moderate computational costs.\n    ",
        "submission_date": "2017-04-17T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05021",
        "title": "Sparse Communication for Distributed Gradient Descent",
        "authors": [
            "Alham Fikri Aji",
            "Kenneth Heafield"
        ],
        "abstract": "We make distributed stochastic gradient descent faster by exchanging sparse updates instead of dense updates. Gradient updates are positively skewed as most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices. This method can be combined with quantization to further improve the compression. We explore different configurations and apply them to neural machine translation and MNIST image classification tasks. Most configurations work on MNIST, whereas different configurations reduce convergence rate on the more complex translation task. Our experiments show that we can achieve up to 49% speed up on MNIST and 22% on NMT without damaging the final accuracy or BLEU.\n    ",
        "submission_date": "2017-04-17T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05091",
        "title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings",
        "authors": [
            "Pedro Saleiro",
            "Eduarda Mendes Rodrigues",
            "Carlos Soares",
            "Eug\u00e9nio Oliveira"
        ],
        "abstract": "This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-of-embeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S\\&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.\n    ",
        "submission_date": "2017-04-17T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05162",
        "title": "Automatic Disambiguation of French Discourse Connectives",
        "authors": [
            "Majid Laali",
            "Leila Kosseim"
        ],
        "abstract": "Discourse connectives (e.g. however, because) are terms that can explicitly convey a discourse relation within a text. While discourse connectives have been shown to be an effective clue to automatically identify discourse relations, they are not always used to convey such relations, thus they should first be disambiguated between discourse-usage non-discourse-usage. In this paper, we investigate the applicability of features proposed for the disambiguation of English discourse connectives for French. Our results with the French Discourse Treebank (FDTB) show that syntactic and lexical features developed for English texts are as effective for French and allow the disambiguation of French discourse connectives with an accuracy of 94.2%.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05179",
        "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine",
        "authors": [
            "Matthew Dunn",
            "Levent Sagun",
            "Mike Higgins",
            "V. Ugur Guney",
            "Volkan Cirik",
            "Kyunghyun Cho"
        ],
        "abstract": "We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05228",
        "title": "Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees",
        "authors": [
            "Mathias Kraus",
            "Stefan Feuerriegel"
        ],
        "abstract": "Prominent applications of sentiment analysis are countless, covering areas such as marketing, customer service and communication. The conventional bag-of-words approach for measuring sentiment merely counts term frequencies; however, it neglects the position of the terms within the discourse. As a remedy, we develop a discourse-aware method that builds upon the discourse structure of documents. For this purpose, we utilize rhetorical structure theory to label (sub-)clauses according to their hierarchical relationships and then assign polarity scores to individual leaves. To learn from the resulting rhetorical structure, we propose a tensor-based, tree-structured deep neural network (named Discourse-LSTM) in order to process the complete discourse tree. The underlying tensors infer the salient passages of narrative materials. In addition, we suggest two algorithms for data augmentation (node reordering and artificial leaf insertion) that increase our training set and reduce overfitting. Our benchmarks demonstrate the superior performance of our approach. Moreover, our tensor structure reveals the salient text passages and thereby provides explanatory insights.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2018-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05347",
        "title": "Baselines and test data for cross-lingual inference",
        "authors": [
            "\u017deljko Agi\u0107",
            "Natalie Schluter"
        ],
        "abstract": "The recent years have seen a revival of interest in textual entailment, sparked by i) the emergence of powerful deep neural network learners for natural language processing and ii) the timely development of large-scale evaluation datasets such as SNLI. Recast as natural language inference, the problem now amounts to detecting the relation between pairs of statements: they either contradict or entail one another, or they are mutually neutral. Current research in natural language inference is effectively exclusive to English. In this paper, we propose to advance the research in SNLI-style natural language inference toward multilingual evaluation. To that end, we provide test data for four major languages: Arabic, French, Spanish, and Russian. We experiment with a set of baselines. Our systems are based on cross-lingual word embeddings and machine translation. While our best system scores an average accuracy of just over 75%, we focus largely on enabling further research in multilingual inference.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2018-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05358",
        "title": "Representing Sentences as Low-Rank Subspaces",
        "authors": [
            "Jiaqi Mu",
            "Suma Bhat",
            "Pramod Viswanath"
        ],
        "abstract": "Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences -- the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15% on average.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05415",
        "title": "An Empirical Analysis of NMT-Derived Interlingual Embeddings and their Use in Parallel Sentence Identification",
        "authors": [
            "Cristina Espa\u00f1a-Bonet",
            "\u00c1d\u00e1m Csaba Varga",
            "Alberto Barr\u00f3n-Cede\u00f1o",
            "Josef van Genabith"
        ],
        "abstract": "End-to-end neural machine translation has overtaken statistical machine translation in terms of translation quality for some language pairs, specially those with large amounts of parallel data. Besides this palpable improvement, neural networks provide several new properties. A single system can be trained to translate between many languages at almost no additional cost other than training time. Furthermore, internal representations learned by the network serve as a new semantic representation of words -or sentences- which, unlike standard word embeddings, are learned in an essentially bilingual or even multilingual context. In view of these properties, the contribution of the present work is two-fold. First, we systematically study the NMT context vectors, i.e. output of the encoder, and their power as an interlingua representation of a sentence. We assess their quality and effectiveness by measuring similarities across translations, as well as semantically related and semantically unrelated sentence pairs. Second, as extrinsic evaluation of the first point, we identify parallel sentences in comparable corpora, obtaining an F1=98.2% on data from a shared task when using only NMT context vectors. Using context vectors jointly with similarity measures F1 reaches 98.9%.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05426",
        "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
        "authors": [
            "Adina Williams",
            "Nikita Nangia",
            "Samuel R. Bowman"
        ],
        "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2018-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05550",
        "title": "Extractive Summarization: Limits, Compression, Generalized Model and Heuristics",
        "authors": [
            "Rakesh Verma",
            "Daniel Lee"
        ],
        "abstract": "Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1-scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05571",
        "title": "Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain",
        "authors": [
            "Mayank Kejriwal"
        ],
        "abstract": "Word embeddings have made enormous inroads in recent years in a wide variety of text mining applications. In this paper, we explore a word embedding-based architecture for predicting the relevance of a role between two financial entities within the context of natural language sentences. In this extended abstract, we propose a pooled approach that uses a collection of sentences to train word embeddings using the skip-gram word2vec architecture. We use the word embeddings to obtain context vectors that are assigned one or more labels based on manual annotations. We train a machine learning classifier using the labeled context vectors, and use the trained classifier to predict contextual role relevance on test data. Our approach serves as a good minimal-expertise baseline for the task as it is simple and intuitive, uses open-source modules, requires little feature crafting effort and performs well across roles.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05579",
        "title": "A Large Self-Annotated Corpus for Sarcasm",
        "authors": [
            "Mikhail Khodak",
            "Nikunj Saunshi",
            "Kiran Vodrahalli"
        ],
        "abstract": "We introduce the Self-Annotated Reddit Corpus (SARC), a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection. The corpus has 1.3 million sarcastic statements -- 10 times more than any previous dataset -- and many times more instances of non-sarcastic statements, allowing for learning in both balanced and unbalanced label regimes. Each statement is furthermore self-annotated -- sarcasm is labeled by the author, not an independent annotator -- and provided with user, topic, and conversation context. We evaluate the corpus for accuracy, construct benchmarks for sarcasm detection, and evaluate baseline methods.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2018-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05611",
        "title": "Dependency resolution and semantic mining using Tree Adjoining Grammars for Tamil Language",
        "authors": [
            "Vijay Krishna Menon",
            "S Rajendran",
            "M Anandkumar",
            "K P Soman"
        ],
        "abstract": "Tree adjoining grammars (TAGs) provide an ample tool to capture syntax of many Indian languages. Tamil represents a special challenge to computational formalisms as it has extensive agglutinative morphology and a comparatively difficult argument structure. Modelling Tamil syntax and morphology using TAG is an interesting problem which has not been in focus even though TAGs are over 4 decades old, since its inception. Our research with Tamil TAGs have shown us that we can not only represent syntax of the language, but to an extent mine out semantics through dependency resolution of the sentence. But in order to demonstrate this phenomenal property, we need to parse Tamil language sentences using TAGs we have built and through parsing obtain a derivation we could use to resolve dependencies, thus proving the semantic property. We use an in-house developed pseudo lexical TAG chart parser; algorithm given by Schabes and Joshi (1988), for generating derivations of sentences. We do not use any statistics to rank out ambiguous derivations but rather use all of them to understand the mentioned semantic relation with in TAGs for Tamil. We shall also present a brief parser analysis for the completeness of our discussions.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05742",
        "title": "Adversarial Multi-task Learning for Text Classification",
        "authors": [
            "Pengfei Liu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at \\url{",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05753",
        "title": "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection",
        "authors": [
            "Youxuan Jiang",
            "Jonathan K. Kummerfeld",
            "Walter S. Lasecki"
        ],
        "abstract": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2017-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05781",
        "title": "Redefining Context Windows for Word Embedding Models: An Experimental Study",
        "authors": [
            "Pierre Lison",
            "Andrey Kutuzov"
        ],
        "abstract": "Distributional semantic models learn vector representations of words through the contexts they occur in. Although the choice of context (which often takes the form of a sliding window) has a direct influence on the resulting embeddings, the exact role of this model component is still not fully understood. This paper presents a systematic analysis of context windows based on a set of four distinct hyper-parameters. We train continuous Skip-Gram models on two English-language corpora for various combinations of these hyper-parameters, and evaluate them on both lexical similarity and analogy tasks. Notable experimental results are the positive impact of cross-sentential contexts and the surprisingly good performance of right-context windows.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05907",
        "title": "End-to-End Multi-View Networks for Text Classification",
        "authors": [
            "Hongyu Guo",
            "Colin Cherry",
            "Jiang Su"
        ],
        "abstract": "We propose a multi-view network for text classification. Our method automatically creates various views of its input text, each taking the form of soft attention weights that distribute the classifier's focus among a set of base features. For a bag-of-words representation, each view focuses on a different subset of the text's words. Aggregating many such views results in a more discriminative and robust representation. Through a novel architecture that both stacks and concatenates views, we produce a network that emphasizes both depth and width, allowing training to converge quickly. Using our multi-view architecture, we establish new state-of-the-art accuracies on two benchmark tasks.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05908",
        "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion",
        "authors": [
            "Qizhe Xie",
            "Xuezhe Ma",
            "Zihang Dai",
            "Eduard Hovy"
        ],
        "abstract": "Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, \\emph{ITransF}, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets---WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2017-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05958",
        "title": "Global Relation Embedding for Relation Extraction",
        "authors": [
            "Yu Su",
            "Honglei Liu",
            "Semih Yavuz",
            "Izzeddin Gur",
            "Huan Sun",
            "Xifeng Yan"
        ],
        "abstract": "We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2018-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05972",
        "title": "SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours",
        "authors": [
            "Leon Derczynski",
            "Kalina Bontcheva",
            "Maria Liakata",
            "Rob Procter",
            "Geraldine Wong Sak Hoi",
            "Arkaitz Zubiaga"
        ],
        "abstract": "Media is full of false claims. Even Oxford Dictionaries named \"post-truth\" as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the kind of discourse there is around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics - each having their own families of claims and replies - and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05973",
        "title": "Call Attention to Rumors: Deep Attention Based Recurrent Neural Networks for Early Rumor Detection",
        "authors": [
            "Tong Chen",
            "Lin Wu",
            "Xue Li",
            "Jun Zhang",
            "Hongzhi Yin",
            "Yang Wang"
        ],
        "abstract": "The proliferation of social media in communication and information dissemination has made it an ideal platform for spreading rumors. Automatically debunking rumors at their stage of diffusion is known as \\textit{early rumor detection}, which refers to dealing with sequential posts regarding disputed factual claims with certain variations and highly textual duplication over time. Thus, identifying trending rumors demands an efficient yet flexible model that is able to capture long-range dependencies among postings and produce distinct representations for the accurate early detection. However, it is a challenging task to apply conventional classification algorithms to rumor detection in earliness since they rely on hand-crafted features which require intensive manual efforts in the case of large amount of posts. This paper presents a deep attention model on the basis of recurrent neural networks (RNN) to learn \\textit{selectively} temporal hidden representations of sequential posts for identifying rumors. The proposed model delves soft-attention into the recurrence to simultaneously pool out distinct features with particular focus and produce hidden representations that capture contextual variations of relevant posts over time. Extensive experiments on real datasets collected from social media websites demonstrate that (1) the deep attention based RNN model outperforms state-of-the-arts that rely on hand-crafted features; (2) the introduction of soft attention mechanism can effectively distill relevant parts to rumors from original posts in advance; (3) the proposed method detects rumors more quickly and accurately than competitors.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05974",
        "title": "Cross-domain Semantic Parsing via Paraphrasing",
        "authors": [
            "Yu Su",
            "Xifeng Yan"
        ],
        "abstract": "Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular Overnight dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06104",
        "title": "Neural End-to-End Learning for Computational Argumentation Mining",
        "authors": [
            "Steffen Eger",
            "Johannes Daxenberger",
            "Iryna Gurevych"
        ],
        "abstract": "We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning 'natural' subtasks, in a multi-task learning setup, improves performance.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2017-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06125",
        "title": "BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs",
        "authors": [
            "Mathieu Cliche"
        ],
        "abstract": "In this paper we describe our attempt at producing a state-of-the-art Twitter sentiment classifier using Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTMs) networks. Our system leverages a large amount of unlabeled data to pre-train word embeddings. We then use a subset of the unlabeled data to fine tune the embeddings using distant supervision. The final CNNs and LSTMs are trained on the SemEval-2017 Twitter dataset where the embeddings are fined tuned again. To boost performances we ensemble several CNNs and LSTMs together. Our approach achieved first rank on all of the five English subtasks amongst 40 teams.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06194",
        "title": "Improved Neural Relation Detection for Knowledge Base Question Answering",
        "authors": [
            "Mo Yu",
            "Wenpeng Yin",
            "Kazi Saidul Hasan",
            "Cicero dos Santos",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2017-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06217",
        "title": "Reinforcement Learning with External Knowledge and Two-Stage Q-functions for Predicting Popular Reddit Threads",
        "authors": [
            "Ji He",
            "Mari Ostendorf",
            "Xiaodong He"
        ],
        "abstract": "This paper addresses the problem of predicting popularity of comments in an online discussion forum using reinforcement learning, particularly addressing two challenges that arise from having natural language state and action spaces. First, the state representation, which characterizes the history of comments tracked in a discussion at a particular point, is augmented to incorporate the global context represented by discussions on world events available in an external knowledge source. Second, a two-stage Q-learning framework is introduced, making it feasible to search the combinatorial action space while also accounting for redundancy among sub-actions. We experiment with five Reddit communities, showing that the two methods improve over previous reported results on this task.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06259",
        "title": "A Semantic QA-Based Approach for Text Summarization Evaluation",
        "authors": [
            "Ping Chen",
            "Fei Wu",
            "Tong Wang",
            "Wei Ding"
        ],
        "abstract": "Many Natural Language Processing and Computational Linguistics applications involves the generation of new texts based on some existing texts, such as summarization, text simplification and machine translation. However, there has been a serious problem haunting these applications for decades, that is, how to automatically and accurately assess quality of these applications. In this paper, we will present some preliminary results on one especially useful and challenging problem in NLP system evaluation: how to pinpoint content differences of two text passages (especially for large pas-sages such as articles and books). Our idea is intuitive and very different from existing approaches. We treat one text passage as a small knowledge base, and ask it a large number of questions to exhaustively identify all content points in it. By comparing the correctly answered questions from two text passages, we will be able to compare their content precisely. The experiment using 2007 DUC summarization corpus clearly shows promising results.\n    ",
        "submission_date": "2017-04-21T00:00:00",
        "last_modified_date": "2018-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06358",
        "title": "Stability and Fluctuations in a Simple Model of Phonetic Category Change",
        "authors": [
            "Benjamin Goodman",
            "Paul Tupper"
        ],
        "abstract": "In spoken languages, speakers divide up the space of phonetic possibilities into different regions, corresponding to different phonemes. We consider a simple exemplar model of how this division of phonetic space varies over time among a population of language users. In the particular model we consider, we show that, once the system is initialized with a given set of phonemes, that phonemes do not become extinct: all phonemes will be maintained in the system for all time. This is in contrast to what is observed in more complex models. Furthermore, we show that the boundaries between phonemes fluctuate and we quantitatively study the fluctuations in a simple instance of our model. These results prepare the ground for more sophisticated models in which some phonemes go extinct or new phonemes emerge through other processes.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2018-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06360",
        "title": "SwellShark: A Generative Model for Biomedical Named Entity Recognition without Labeled Data",
        "authors": [
            "Jason Fries",
            "Sen Wu",
            "Alex Ratner",
            "Christopher R\u00e9"
        ],
        "abstract": "We present SwellShark, a framework for building biomedical named entity recognition (NER) systems quickly and without hand-labeled data. Our approach views biomedical resources like lexicons as function primitives for autogenerating weak supervision. We then use a generative model to unify and denoise this supervision and construct large-scale, probabilistically labeled datasets for training high-accuracy NER taggers. In three biomedical NER tasks, SwellShark achieves competitive scores with state-of-the-art supervised benchmarks using no hand-labeled training data. In a drug name extraction task using patient medical records, one domain expert using SwellShark achieved within 5.1% of a crowdsourced annotation approach -- which originally utilized 20 teams over the course of several weeks -- in 24 hours.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06380",
        "title": "Improving Context Aware Language Models",
        "authors": [
            "Aaron Jaech",
            "Mari Ostendorf"
        ],
        "abstract": "Increased adaptability of RNN language models leads to improved predictions that benefit many applications. However, current methods do not take full advantage of the RNN structure. We show that the most widely-used approach to adaptation (concatenating the context with the word embedding at the input to the recurrent layer) is outperformed by a model that has some low-cost improvements: adaptation of both the hidden and output layers. and a feature hashing bias term to capture context idiosyncrasies. Experiments on language modeling and classification tasks using three different corpora demonstrate the advantages of the proposed techniques.\n    ",
        "submission_date": "2017-04-21T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06393",
        "title": "Neural System Combination for Machine Translation",
        "authors": [
            "Long Zhou",
            "Wenpeng Hu",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "Neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT).\n",
        "submission_date": "2017-04-21T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06567",
        "title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning",
        "authors": [
            "Jind\u0159ich Libovick\u00fd",
            "Jind\u0159ich Helcl"
        ],
        "abstract": "Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed methods achieve competitive results on both tasks.\n    ",
        "submission_date": "2017-04-21T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06619",
        "title": "Scientific Article Summarization Using Citation-Context and Article's Discourse Structure",
        "authors": [
            "Arman Cohan",
            "Nazli Goharian"
        ],
        "abstract": "We propose a summarization approach for scientific articles which takes advantage of citation-context and the document discourse model. While citations have been previously used in generating scientific summaries, they lack the related context from the referenced article and therefore do not accurately reflect the article's content. Our method overcomes the problem of inconsistency between the citation summary and the article's content by providing context for each citation. We also leverage the inherent scientific article's discourse for producing better summaries. We show that our proposed method effectively improves over existing summarization approaches (greater than 30% improvement over the best performing baseline) in terms of \\textsc{Rouge} scores on TAC2014 scientific summarization dataset. While the dataset we use for evaluation is in the biomedical domain, most of our approaches are general and therefore adaptable to other domains.\n    ",
        "submission_date": "2017-04-21T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06692",
        "title": "Improving Semantic Composition with Offset Inference",
        "authors": [
            "Thomas Kober",
            "Julie Weeds",
            "Jeremy Reffin",
            "David Weir"
        ],
        "abstract": "Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers missing data by the same mechanism that is used for semantic composition.\n    ",
        "submission_date": "2017-04-21T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06779",
        "title": "Lexical Features in Coreference Resolution: To be Used With Caution",
        "authors": [
            "Nafise Sadat Moosavi",
            "Michael Strube"
        ],
        "abstract": "Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.\n    ",
        "submission_date": "2017-04-22T00:00:00",
        "last_modified_date": "2017-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06836",
        "title": "Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation",
        "authors": [
            "Lotem Peled",
            "Roi Reichart"
        ],
        "abstract": "Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, \"Sarcasm is the giant chasm between what I say, and the person who doesn't get it.\". In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN's interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new task.\n    ",
        "submission_date": "2017-04-22T00:00:00",
        "last_modified_date": "2017-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06841",
        "title": "Medical Text Classification using Convolutional Neural Networks",
        "authors": [
            "Mark Hughes",
            "Irene Li",
            "Spyros Kotoulas",
            "Toyotaro Suzumura"
        ],
        "abstract": "We present an approach to automatically classify clinical text at a sentence level. We are using deep convolutional neural networks to represent complex features. We train the network on a dataset providing a broad categorization of health information. Through a detailed evaluation, we demonstrate that our method outperforms several approaches widely used in natural language processing tasks by about 15%.\n    ",
        "submission_date": "2017-04-22T00:00:00",
        "last_modified_date": "2017-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06851",
        "title": "Affect-LM: A Neural Language Model for Customizable Affective Text Generation",
        "authors": [
            "Sayan Ghosh",
            "Mathieu Chollet",
            "Eugene Laksana",
            "Louis-Philippe Morency",
            "Stefan Scherer"
        ],
        "abstract": "Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM generates naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.\n    ",
        "submission_date": "2017-04-22T00:00:00",
        "last_modified_date": "2017-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06855",
        "title": "Deep Multitask Learning for Semantic Dependency Parsing",
        "authors": [
            "Hao Peng",
            "Sam Thomson",
            "Noah A. Smith"
        ],
        "abstract": "We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches---one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at ",
        "submission_date": "2017-04-22T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06869",
        "title": "Argument Mining with Structured SVMs and RNNs",
        "authors": [
            "Vlad Niculae",
            "Joonsuk Park",
            "Claire Cardie"
        ],
        "abstract": "We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06877",
        "title": "Learning to Skim Text",
        "authors": [
            "Adams Wei Yu",
            "Hongrae Lee",
            "Quoc V. Le"
        ],
        "abstract": "Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\\&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06879",
        "title": "Deep Keyphrase Generation",
        "authors": [
            "Rui Meng",
            "Sanqiang Zhao",
            "Shuguang Han",
            "Daqing He",
            "Peter Brusilovsky",
            "Yu Chi"
        ],
        "abstract": "Keyphrase provides highly-condensed information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2021-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06913",
        "title": "Learning weakly supervised multimodal phoneme embeddings",
        "authors": [
            "Rahma Chaabouni",
            "Ewan Dunbar",
            "Neil Zeghidour",
            "Emmanuel Dupoux"
        ],
        "abstract": "Recent works have explored deep architectures for learning multimodal speech representation (e.g. audio and images, articulation and audio) in a supervised way. Here we investigate the role of combining different speech modalities, i.e. audio and visual information representing the lips movements, in a weakly supervised way using Siamese networks and lexical same-different side information. In particular, we ask whether one modality can benefit from the other to provide a richer representation for phone recognition in a weakly supervised setting. We introduce mono-task and multi-task methods for merging speech and visual modalities for phone recognition. The mono-task learning consists in applying a Siamese network on the concatenation of the two modalities, while the multi-task learning receives several different combinations of modalities at train time. We show that multi-task learning enhances discriminability for visual and multimodal inputs while minimally impacting auditory inputs. Furthermore, we present a qualitative analysis of the obtained phone embeddings, and show that cross-modal visual input can improve the discriminability of phonological features which are visually discernable (rounding, open/close, labial place of articulation), resulting in representations that are closer to abstract linguistic features than those based on audio only.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06918",
        "title": "Neural Machine Translation via Binary Code Prediction",
        "authors": [
            "Yusuke Oda",
            "Philip Arthur",
            "Graham Neubig",
            "Koichiro Yoshino",
            "Satoshi Nakamura"
        ],
        "abstract": "In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06933",
        "title": "Adversarial Neural Machine Translation",
        "authors": [
            "Lijun Wu",
            "Yingce Xia",
            "Li Zhao",
            "Fei Tian",
            "Tao Qin",
            "Jianhuang Lai",
            "Tie-Yan Liu"
        ],
        "abstract": "In this paper, we study a new learning paradigm for Neural Machine Translation (NMT). Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by an NMT model. To achieve this goal, inspired by the recent success of generative adversarial networks (GANs), we employ an adversarial training architecture and name it as Adversarial-NMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed Convolutional Neural Network (CNN). The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on English$\\rightarrow$French and German$\\rightarrow$English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines.\n    ",
        "submission_date": "2017-04-20T00:00:00",
        "last_modified_date": "2018-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06936",
        "title": "A* CCG Parsing with a Supertag and Dependency Factored Model",
        "authors": [
            "Masashi Yoshikawa",
            "Hiroshi Noji",
            "Yuji Matsumoto"
        ],
        "abstract": "We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06956",
        "title": "Naturalizing a Programming Language via Interactive Learning",
        "authors": [
            "Sida I. Wang",
            "Samuel Ginn",
            "Percy Liang",
            "Christoper D. Manning"
        ],
        "abstract": "Our goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to \"naturalize\" the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9\\% of the last 10K utterances.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06960",
        "title": "Translating Neuralese",
        "authors": [
            "Jacob Andreas",
            "Anca Dragan",
            "Dan Klein"
        ],
        "abstract": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2018-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06970",
        "title": "Differentiable Scheduled Sampling for Credit Assignment",
        "authors": [
            "Kartik Goyal",
            "Chris Dyer",
            "Taylor Berg-Kirkpatrick"
        ],
        "abstract": "We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)--a well-known technique for correcting exposure bias--we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06986",
        "title": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling",
        "authors": [
            "Kazuya Kawakami",
            "Chris Dyer",
            "Phil Blunsom"
        ],
        "abstract": "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the \"bursty\" distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus, MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07047",
        "title": "Fast and Accurate Neural Word Segmentation for Chinese",
        "authors": [
            "Deng Cai",
            "Hai Zhao",
            "Zhisong Zhang",
            "Yuan Xin",
            "Yongjian Wu",
            "Feiyue Huang"
        ],
        "abstract": "Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07050",
        "title": "Using Global Constraints and Reranking to Improve Cognates Detection",
        "authors": [
            "Michael Bloodgood",
            "Benjamin Strauss"
        ],
        "abstract": "Global constraints and reranking have not been used in cognates detection research to date. We propose methods for using global constraints by performing rescoring of the score matrices produced by state of the art cognates detection systems. Using global constraints to perform rescoring is complementary to state of the art methods for performing cognates detection and results in significant performance improvements beyond current state of the art performance on publicly available datasets with different language pairs and various conditions such as different levels of baseline state of the art performance and different data size conditions, including with more realistic large data size conditions than have been evaluated with in the past.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07073",
        "title": "Selective Encoding for Abstractive Sentence Summarization",
        "authors": [
            "Qingyu Zhou",
            "Nan Yang",
            "Furu Wei",
            "Ming Zhou"
        ],
        "abstract": "We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07092",
        "title": "Robust Incremental Neural Semantic Graph Parsing",
        "authors": [
            "Jan Buys",
            "Phil Blunsom"
        ],
        "abstract": "Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07121",
        "title": "Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets",
        "authors": [
            "Wei-Lun Chao",
            "Hexiang Hu",
            "Fei Sha"
        ],
        "abstract": "Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (\\ie the correct one) and the decoys (\\ie the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular Visual QA datasets as well as to create a new Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2018-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07129",
        "title": "An Analysis of Action Recognition Datasets for Language and Vision Tasks",
        "authors": [
            "Spandana Gella",
            "Frank Keller"
        ],
        "abstract": "A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods. One such task is action recognition, whose applications include image annotation, scene under- standing and image retrieval. In this survey, we categorize the existing ap- proaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their di- versity as well as advantages and disad- vantages. We focus on recently devel- oped datasets which link visual informa- tion with linguistic resources and provide a fine-grained syntactic and semantic anal- ysis of actions in images.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07130",
        "title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings",
        "authors": [
            "He He",
            "Anusha Balakrishnan",
            "Mihail Eric",
            "Percy Liang"
        ],
        "abstract": "We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07138",
        "title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search",
        "authors": [
            "Chris Hokamp",
            "Qun Liu"
        ],
        "abstract": "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model that generates a sequence $ \\mathbf{\\hat{y}} = \\{y_{0}\\ldots y_{T}\\} $, by maximizing $ p(\\mathbf{y} | \\mathbf{x}) = \\prod\\limits_{t}p(y_{t} | \\mathbf{x}; \\{y_{0} \\ldots y_{t-1}\\}) $. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate additional knowledge into a model's output without requiring any modification of the model parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07146",
        "title": "Found in Translation: Reconstructing Phylogenetic Language Trees from Translations",
        "authors": [
            "Ella Rabinovich",
            "Noam Ordan",
            "Shuly Wintner"
        ],
        "abstract": "Translation has played an important role in trade, law, commerce, politics, and literature for thousands of years. Translators have always tried to be invisible; ideal translations should look as if they were written originally in the target language. We show that traces of the source language remain in the translation product to the extent that it is possible to uncover the history of the source language by looking only at the translation. Specifically, we automatically reconstruct phylogenetic language trees from monolingual texts (translated from several source languages). The signal of the source language is so powerful that it is retained even after two phases of translation. This strongly indicates that source language interference is the most dominant characteristic of translated texts, overshadowing the more subtle signals of universal properties of translation.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07156",
        "title": "Semi-supervised Multitask Learning for Sequence Labeling",
        "authors": [
            "Marek Rei"
        ],
        "abstract": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07157",
        "title": "Watset: Automatic Induction of Synsets from a Graph of Synonyms",
        "authors": [
            "Dmitry Ustalov",
            "Alexander Panchenko",
            "Chris Biemann"
        ],
        "abstract": "This paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings. First, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary. Second, we apply word sense induction to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into synsets. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of F-score on three gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07203",
        "title": "What is the Essence of a Claim? Cross-Domain Claim Identification",
        "authors": [
            "Johannes Daxenberger",
            "Steffen Eger",
            "Ivan Habernal",
            "Christian Stab",
            "Iryna Gurevych"
        ],
        "abstract": "Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent perception of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07221",
        "title": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM",
        "authors": [
            "Elena Kochkina",
            "Maria Liakata",
            "Isabelle Augenstein"
        ],
        "abstract": "This paper describes team Turing's submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07287",
        "title": "Parsing Speech: A Neural Approach to Integrating Lexical and Acoustic-Prosodic Information",
        "authors": [
            "Trang Tran",
            "Shubham Toshniwal",
            "Mohit Bansal",
            "Kevin Gimpel",
            "Karen Livescu",
            "Mari Ostendorf"
        ],
        "abstract": "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline. For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2018-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07329",
        "title": "A Trie-Structured Bayesian Model for Unsupervised Morphological Segmentation",
        "authors": [
            "Murathan Kurfal\u0131",
            "Ahmet \u00dcst\u00fcn",
            "Burcu Can"
        ],
        "abstract": "In this paper, we introduce a trie-structured Bayesian model for unsupervised morphological segmentation. We adopt prior information from different sources in the model. We use neural word embeddings to discover words that are morphologically derived from each other and thereby that are semantically similar. We use letter successor variety counts obtained from tries that are built by neural word embeddings. Our results show that using different information sources such as neural word embeddings and letter successor variety as prior information improves morphological segmentation in a Bayesian model. Our model outperforms other unsupervised morphological segmentation models on Turkish and gives promising results on English and German for scarce resources.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07398",
        "title": "Predicting Native Language from Gaze",
        "authors": [
            "Yevgeni Berzak",
            "Chie Nakamura",
            "Suzanne Flynn",
            "Boris Katz"
        ],
        "abstract": "A fundamental question in language learning concerns the role of a speaker's first language in second language acquisition. We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text. Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented framework complements production studies and offers new ground for advancing research on multilingualism.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07415",
        "title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention",
        "authors": [
            "Yichen Gong",
            "Samuel R. Bowman"
        ],
        "abstract": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07427",
        "title": "Recognizing Descriptive Wikipedia Categories for Historical Figures",
        "authors": [
            "Yanqing Chen",
            "Steven Skiena"
        ],
        "abstract": "Wikipedia is a useful knowledge source that benefits many applications in language processing and knowledge representation. An important feature of Wikipedia is that of categories. Wikipedia pages are assigned different categories according to their contents as human-annotated labels which can be used in information retrieval, ad hoc search improvements, entity ranking and tag recommendations. However, important pages are usually assigned too many categories, which makes it difficult to recognize the most important ones that give the best descriptions.\n",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07431",
        "title": "A Challenge Set Approach to Evaluating Machine Translation",
        "authors": [
            "Pierre Isabelle",
            "Colin Cherry",
            "George Foster"
        ],
        "abstract": "Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system's capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07441",
        "title": "Detecting English Writing Styles For Non Native Speakers",
        "authors": [
            "Yanging Chen",
            "Rami Al-Rfou'",
            "Yejin Choi"
        ],
        "abstract": "This paper presents the first attempt, up to our knowledge, to classify English writing styles on this scale with the challenge of classifying day to day language written by writers with different backgrounds covering various areas of ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07463",
        "title": "Streaming Word Embeddings with the Space-Saving Algorithm",
        "authors": [
            "Chandler May",
            "Kevin Duh",
            "Benjamin Van Durme",
            "Ashwin Lall"
        ],
        "abstract": "We develop a streaming (one-pass, bounded-memory) word embedding algorithm based on the canonical skip-gram with negative sampling algorithm implemented in word2vec. We compare our streaming algorithm to word2vec empirically by measuring the cosine similarity between word pairs under each algorithm and by applying each algorithm in the downstream task of hashtag prediction on a two-month interval of the Twitter sample stream. We then discuss the results of these experiments, concluding they provide partial validation of our approach as a streaming replacement for word2vec. Finally, we discuss potential failure modes and suggest directions for future work.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07489",
        "title": "Multi-Task Video Captioning with Video and Entailment Generation",
        "authors": [
            "Ramakanth Pasunuru",
            "Mohit Bansal"
        ],
        "abstract": "Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailed caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.\n    ",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07535",
        "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing",
        "authors": [
            "Maxim Rabinovich",
            "Mitchell Stern",
            "Dan Klein"
        ],
        "abstract": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07556",
        "title": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation",
        "authors": [
            "Xinchi Chen",
            "Zhan Shi",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07616",
        "title": "Joint POS Tagging and Dependency Parsing with Transition-based Neural Networks",
        "authors": [
            "Liner Yang",
            "Meishan Zhang",
            "Yang Liu",
            "Nan Yu",
            "Maosong Sun",
            "Guohong Fu"
        ],
        "abstract": "While part-of-speech (POS) tagging and dependency parsing are observed to be closely related, existing work on joint modeling with manually crafted feature templates suffers from the feature sparsity and incompleteness problems. In this paper, we propose an approach to joint POS tagging and dependency parsing using transition-based neural networks. Three neural network based classifiers are designed to resolve shift/reduce, tagging, and labeling conflicts. Experiments show that our approach significantly outperforms previous methods for joint POS tagging and dependency parsing across a variety of natural languages.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07624",
        "title": "280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia using Character-level Classification",
        "authors": [
            "Amit Gupta",
            "R\u00e9mi Lebret",
            "Hamza Harkous",
            "Karl Aberer"
        ],
        "abstract": "We propose a simple, yet effective, approach towards inducing multilingual taxonomies from Wikipedia. Given an English taxonomy, our approach leverages the interlanguage links of Wikipedia followed by character-level classifiers to induce high-precision, high-coverage taxonomies in other languages. Through experiments, we demonstrate that our approach significantly outperforms the state-of-the-art, heuristics-heavy approaches for six languages. As a consequence of our work, we release presumably the largest and the most accurate multilingual taxonomic resource spanning over 280 languages.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07751",
        "title": "Fine-Grained Entity Typing with High-Multiplicity Assignments",
        "authors": [
            "Maxim Rabinovich",
            "Dan Klein"
        ],
        "abstract": "As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07875",
        "title": "Automatic Compositor Attribution in the First Folio of Shakespeare",
        "authors": [
            "Maria Ryskina",
            "Hannah Alpert-Abrams",
            "Dan Garrette",
            "Taylor Berg-Kirkpatrick"
        ],
        "abstract": "Compositor attribution, the clustering of pages in a historical printed document by the individual who set the type, is a bibliographic task that relies on analysis of orthographic variation and inspection of visual details of the printed page. In this paper, we introduce a novel unsupervised model that jointly describes the textual and visual features needed to distinguish compositors. Applied to images of Shakespeare's First Folio, our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87%, even on text that is the output of OCR.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07986",
        "title": "Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization",
        "authors": [
            "Akira Sasaki",
            "Kazuaki Hanawa",
            "Naoaki Okazaki",
            "Kentaro Inui"
        ],
        "abstract": "We present in this paper our approach for modeling inter-topic preferences of Twitter users: for example, those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion surveys, electoral predictions, electoral campaigns, and online debates. In order to extract users' preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., \"A is completely wrong\"). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users' preferences as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our proposed approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08012",
        "title": "Topically Driven Neural Language Model",
        "authors": [
            "Jey Han Lau",
            "Timothy Baldwin",
            "Trevor Cohn"
        ],
        "abstract": "Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08059",
        "title": "Riemannian Optimization for Skip-Gram Negative Sampling",
        "authors": [
            "Alexander Fonarev",
            "Oleksii Hrinchuk",
            "Gleb Gusev",
            "Pavel Serdyukov",
            "Ivan Oseledets"
        ],
        "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08088",
        "title": "Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts",
        "authors": [
            "Leandro B. dos Santos",
            "Edilson A. Corr\u00eaa Jr",
            "Osvaldo N. Oliveira Jr",
            "Diego R. Amancio",
            "Let\u00edcia L. Mansur",
            "Sandra M. Alu\u00edsio"
        ],
        "abstract": "Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts. In this paper, we modeled transcripts into complex networks and enriched them with word embedding (CNE) to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08092",
        "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations",
        "authors": [
            "Samuel R\u00f6nnqvist",
            "Niko Schenk",
            "Christian Chiarcos"
        ],
        "abstract": "We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08224",
        "title": "Punny Captions: Witty Wordplay in Image Descriptions",
        "authors": [
            "Arjun Chandrasekaran",
            "Devi Parikh",
            "Mohit Bansal"
        ],
        "abstract": "Wit is a form of rich interaction that is often grounded in a specific situation (e.g., a comment in response to an event). In this work, we attempt to build computational models that can produce witty descriptions for a given image. Inspired by a cognitive account of humor appreciation, we employ linguistic wordplay, specifically puns, in image descriptions. We develop two approaches which involve retrieving witty descriptions for a given image from a large corpus of sentences, or generating them via an encoder-decoder neural network architecture. We compare our approach against meaningful baseline approaches via human studies and show substantial improvements. We find that when a human is subject to similar constraints as the model regarding word usage and style, people vote the image descriptions generated by our model to be slightly wittier than human-written witty descriptions. Unsurprisingly, humans are almost always wittier than the model when they are free to choose the vocabulary, style, etc.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2018-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08300",
        "title": "Diversity driven Attention Model for Query-based Abstractive Summarization",
        "authors": [
            "Preksha Nema",
            "Mitesh Khapra",
            "Anirban Laha",
            "Balaraman Ravindran"
        ],
        "abstract": "Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2018-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08352",
        "title": "From Characters to Words to in Between: Do We Capture Morphology?",
        "authors": [
            "Clara Vania",
            "Adam Lopez"
        ],
        "abstract": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08381",
        "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
        "authors": [
            "Ioannis Konstas",
            "Srinivasan Iyer",
            "Mark Yatskar",
            "Yejin Choi",
            "Luke Zettlemoyer"
        ],
        "abstract": "Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text usingAbstract Meaning Representation (AMR)has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08384",
        "title": "Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks",
        "authors": [
            "Rajarshi Das",
            "Manzil Zaheer",
            "Siva Reddy",
            "Andrew McCallum"
        ],
        "abstract": "Existing question answering methods infer answers either from a knowledge base or from raw text. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, web text contains millions of facts that are absent in the KB, however in an unstructured form. {\\it Universal schema} can support reasoning on the union of both structured KBs and unstructured text by aligning them in a common embedded space. In this paper we extend universal schema to natural language question answering, employing \\emph{memory networks} to attend to the large body of facts in the combination of text and KB. Our models can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on \\spades fill-in-the-blank question answering dataset show that exploiting universal schema for question answering is better than using either a KB or text alone. This model also outperforms the current state-of-the-art by 8.5 $F_1$ points.\\footnote{Code and data available in \\url{",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08387",
        "title": "Learning Structured Natural Language Representations for Semantic Parsing",
        "authors": [
            "Jianpeng Cheng",
            "Siva Reddy",
            "Vijay Saraswat",
            "Mirella Lapata"
        ],
        "abstract": "We introduce a neural semantic parser that converts natural language utterances to intermediate representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We obtain competitive results on various datasets. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.\n    ",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2017-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08388",
        "title": "Duluth at Semeval-2017 Task 7 : Puns upon a midnight dreary, Lexical Semantics for the weak and weary",
        "authors": [
            "Ted Pedersen"
        ],
        "abstract": "This paper describes the Duluth systems that participated in SemEval-2017 Task 7 : Detection and Interpretation of English Puns. The Duluth systems participated in all three subtasks, and relied on methods that included word sense disambiguation and measures of semantic relatedness.\n    ",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2017-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08390",
        "title": "Duluth at SemEval-2017 Task 6: Language Models in Humor Detection",
        "authors": [
            "Xinru Yan",
            "Ted Pedersen"
        ],
        "abstract": "This paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.\n    ",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08430",
        "title": "A GRU-Gated Attention Model for Neural Machine Translation",
        "authors": [
            "Biao Zhang",
            "Deyi Xiong",
            "Jinsong Su"
        ],
        "abstract": "Neural machine translation (NMT) heavily relies on an attention network to produce a context vector for each target word prediction. In practice, we find that context vectors for different target words are quite similar to one another and therefore are insufficient in discriminatively predicting target words. The reason for this might be that context vectors produced by the vanilla attention network are just a weighted sum of source representations that are invariant to decoder states. In this paper, we propose a novel GRU-gated attention model (GAtt) for NMT which enhances the degree of discrimination of context vectors by enabling source representations to be sensitive to the partial translation generated by the decoder. GAtt uses a gated recurrent unit (GRU) to combine two types of information: treating a source annotation vector originally produced by the bidirectional encoder as the history state while the corresponding previous decoder state as the input to the GRU. The GRU-combined information forms a new source annotation vector. In this way, we can obtain translation-sensitive source representations which are then feed into the attention network to generate discriminative context vectors. We further propose a variant that regards a source annotation vector as the current input while the previous decoder state as the history. Experiments on NIST Chinese-English translation tasks show that both GAtt-based models achieve significant improvements over the vanilla attentionbased NMT. Further analyses on attention weights and context vectors demonstrate the effectiveness of GAtt in improving the discrimination power of representations and handling the challenging issue of over-translation.\n    ",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2019-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08531",
        "title": "A Survey of Neural Network Techniques for Feature Extraction from Text",
        "authors": [
            "Vineet John"
        ],
        "abstract": "This paper aims to catalyze the discussions about text feature extraction techniques using neural network architectures. The research questions discussed in the paper focus on the state-of-the-art neural network techniques that have proven to be useful tools for language processing, language generation, text classification and other computational linguistics tasks.\n    ",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08760",
        "title": "Learning a Neural Semantic Parser from User Feedback",
        "authors": [
            "Srinivasan Iyer",
            "Ioannis Konstas",
            "Alvin Cheung",
            "Jayant Krishnamurthy",
            "Luke Zettlemoyer"
        ],
        "abstract": "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.\n    ",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08795",
        "title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning",
        "authors": [
            "Dipendra Misra",
            "John Langford",
            "Yoav Artzi"
        ],
        "abstract": "We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent's exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.\n    ",
        "submission_date": "2017-04-28T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08798",
        "title": "Word Affect Intensities",
        "authors": [
            "Saif M. Mohammad"
        ],
        "abstract": "Words often convey affect -- emotions, feelings, and attitudes. Further, different words can convey affect to various degrees (intensities). However, existing manually created lexicons for basic emotions (such as anger and fear) indicate only coarse categories of affect association (for example, associated with anger or not associated with anger). Automatic lexicons of affect provide fine degrees of association, but they tend not to be accurate as human-created lexicons. Here, for the first time, we present a manually created affect intensity lexicon with real-valued scores of intensity for four basic emotions: anger, fear, joy, and sadness. (We will subsequently add entries for more emotions such as disgust, anticipation, trust, and surprise.) We refer to this dataset as the NRC Affect Intensity Lexicon, or AIL for short. AIL has entries for close to 6,000 English words. We used a technique called best-worst scaling (BWS) to create the lexicon. BWS improves annotation consistency and obtains reliable fine-grained scores (split-half reliability > 0.91). We also compare the entries in AIL with the entries in the NRC VAD Lexicon, which has valence, arousal, and dominance (VAD) scores for 20K English words. We find that anger, fear, and sadness words, on average, have very similar VAD scores. However, sadness words tend to have slightly lower dominance scores than fear and anger words. The Affect Intensity Lexicon has applications in automatic emotion analysis in a number of domains such as commerce, education, intelligence, and public health. AIL is also useful in the building of natural language generation systems.\n    ",
        "submission_date": "2017-04-28T00:00:00",
        "last_modified_date": "2022-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08893",
        "title": "How compatible are our discourse annotations? Insights from mapping RST-DT and PDTB annotations",
        "authors": [
            "Vera Demberg",
            "Fatemeh Torabi Asr",
            "Merel Scholman"
        ],
        "abstract": "Discourse-annotated corpora are an important resource for the community, but they are often annotated according to different frameworks. This makes comparison of the annotations difficult, thereby also preventing researchers from searching the corpora in a unified way, or using all annotated data jointly to train computational systems. Several theoretical proposals have recently been made for mapping the relational labels of different frameworks to each other, but these proposals have so far not been validated against existing annotations. The two largest discourse relation annotated resources, the Penn Discourse Treebank and the Rhetorical Structure Theory Discourse Treebank, have however been annotated on the same text, allowing for a direct comparison of the annotation layers. We propose a method for automatically aligning the discourse segments, and then evaluate existing mapping proposals by comparing the empirically observed against the proposed mappings. Our analysis highlights the influence of segmentation on subsequent discourse relation labeling, and shows that while agreement between frameworks is reasonable for explicit relations, agreement on implicit relations is low. We identify several sources of systematic discrepancies between the two annotation schemes and discuss consequences of these discrepancies for future annotation and for the training of automatic discourse relation labellers.\n    ",
        "submission_date": "2017-04-28T00:00:00",
        "last_modified_date": "2018-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08914",
        "title": "Past, Present, Future: A Computational Investigation of the Typology of Tense in 1000 Languages",
        "authors": [
            "Ehsaneddin Asgari",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We present SuperPivot, an analysis method for low-resource languages that occur in a superparallel corpus, i.e., in a corpus that contains an order of magnitude more languages than parallel corpora currently in use. We show that SuperPivot performs well for the crosslingual analysis of the linguistic phenomenon of tense. We produce analysis results for more than 1000 languages, conducting - to the best of our knowledge - the largest crosslingual computational study performed to date. We extend existing methodology for leveraging parallel corpora for typological analysis by overcoming a limiting assumption of earlier work: We only require that a linguistic feature is overtly marked in a few of thousands of languages as opposed to requiring that it be marked in all languages under investigation.\n    ",
        "submission_date": "2017-04-28T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08960",
        "title": "Neural Word Segmentation with Rich Pretraining",
        "authors": [
            "Jie Yang",
            "Yue Zhang",
            "Fei Dong"
        ],
        "abstract": "Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.\n    ",
        "submission_date": "2017-04-28T00:00:00",
        "last_modified_date": "2017-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08966",
        "title": "Not All Dialogues are Created Equal: Instance Weighting for Neural Conversational Models",
        "authors": [
            "Pierre Lison",
            "Serge Bibauw"
        ],
        "abstract": "Neural conversational models require substantial amounts of dialogue data for their parameter estimation and are therefore usually learned on large corpora such as chat forums or movie subtitles. These corpora are, however, often challenging to work with, notably due to their frequent lack of turn segmentation and the presence of multiple references external to the dialogue itself. This paper shows that these challenges can be mitigated by adding a weighting model into the architecture. The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling. At training time, these sample weights are included into the empirical loss to be minimised. Evaluation results on retrieval-based models trained on movie and TV subtitles demonstrate that the inclusion of such a weighting model improves the model performance on unsupervised metrics.\n    ",
        "submission_date": "2017-04-28T00:00:00",
        "last_modified_date": "2017-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00045",
        "title": "Understanding and Detecting Supporting Arguments of Diverse Types",
        "authors": [
            "Xinyu Hua",
            "Lu Wang"
        ],
        "abstract": "We investigate the problem of sentence-level supporting argument detection from relevant documents for user-specified claims. A dataset containing claims and associated citation articles is collected from online debate website ",
        "submission_date": "2017-04-28T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00106",
        "title": "Learning to Ask: Neural Question Generation for Reading Comprehension",
        "authors": [
            "Xinya Du",
            "Junru Shao",
            "Claire Cardie"
        ],
        "abstract": "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).\n    ",
        "submission_date": "2017-04-29T00:00:00",
        "last_modified_date": "2017-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00108",
        "title": "Semi-supervised sequence tagging with bidirectional language models",
        "authors": [
            "Matthew E. Peters",
            "Waleed Ammar",
            "Chandra Bhagavatula",
            "Russell Power"
        ],
        "abstract": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.\n    ",
        "submission_date": "2017-04-29T00:00:00",
        "last_modified_date": "2017-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00217",
        "title": "Extending and Improving Wordnet via Unsupervised Word Embeddings",
        "authors": [
            "Mikhail Khodak",
            "Andrej Risteski",
            "Christiane Fellbaum",
            "Sanjeev Arora"
        ],
        "abstract": "This work presents an unsupervised approach for improving WordNet that builds upon recent advances in document and sense representation via distributional semantics. We apply our methods to construct Wordnets in French and Russian, languages which both lack good manual constructions.1 These are evaluated on two new 600-word test sets for word-to-synset matching and found to improve greatly upon synset recall, outperforming the best automated Wordnets in F-score. Our methods require very few linguistic resources, thus being applicable for Wordnet construction in low-resources languages, and may further be applied to sense clustering and other Wordnet improvements.\n    ",
        "submission_date": "2017-04-29T00:00:00",
        "last_modified_date": "2017-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00251",
        "title": "Lifelong Learning CRF for Supervised Aspect Extraction",
        "authors": [
            "Lei Shu",
            "Hu Xu",
            "Bing Liu"
        ],
        "abstract": "This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications.\n    ",
        "submission_date": "2017-04-29T00:00:00",
        "last_modified_date": "2017-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00316",
        "title": "A Conditional Variational Framework for Dialog Generation",
        "authors": [
            "Xiaoyu Shen",
            "Hui Su",
            "Yanran Li",
            "Wenjie Li",
            "Shuzi Niu",
            "Yang Zhao",
            "Akiko Aizawa",
            "Guoping Long"
        ],
        "abstract": "Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.\n    ",
        "submission_date": "2017-04-30T00:00:00",
        "last_modified_date": "2017-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00335",
        "title": "Quantifying Mental Health from Social Media with Neural User Embeddings",
        "authors": [
            "Silvio Amir",
            "Glen Coppersmith",
            "Paula Carvalho",
            "M\u00e1rio J. Silva",
            "Byron C. Wallace"
        ],
        "abstract": "Mental illnesses adversely affect a significant proportion of the population worldwide. However, the methods traditionally used for estimating and characterizing the prevalence of mental health conditions are time-consuming and expensive. Consequently, best-available estimates concerning the prevalence of mental health conditions are often years out of date. Automated approaches to supplement these survey methods with broad, aggregated information derived from social media content provides a potential means for near real-time estimates at scale. These may, in turn, provide grist for supporting, evaluating and iteratively improving upon public health programs and interventions.\n",
        "submission_date": "2017-04-30T00:00:00",
        "last_modified_date": "2017-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00364",
        "title": "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings",
        "authors": [
            "John Wieting",
            "Kevin Gimpel"
        ],
        "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.\n    ",
        "submission_date": "2017-04-30T00:00:00",
        "last_modified_date": "2017-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00390",
        "title": "Duluth at SemEval--2016 Task 14 : Extending Gloss Overlaps to Enrich Semantic Taxonomies",
        "authors": [
            "Ted Pedersen"
        ],
        "abstract": "This paper describes the Duluth systems that participated in Task 14 of SemEval 2016, Semantic Taxonomy Enrichment. There were three related systems in the formal evaluation which are discussed here, along with numerous post--evaluation runs. All of these systems identified synonyms between WordNet and other dictionaries by measuring the gloss overlaps between them. These systems perform better than the random baseline and one post--evaluation variation was within a respectable margin of the median result attained by all participating systems.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00403",
        "title": "Dependency Parsing with Dilated Iterated Graph CNNs",
        "authors": [
            "Emma Strubell",
            "Andrew McCallum"
        ],
        "abstract": "Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale. Recent advances in GPU hardware have enabled neural networks to achieve significant gains over the previous best models, these models still fail to leverage GPUs' capability for massive parallelism due to their requirement of sequential processing of the sentence. In response, we propose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for graph-based dependency parsing, a graph convolutional architecture that allows for efficient end-to-end GPU parsing. In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best neural network parsers.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00424",
        "title": "Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary",
        "authors": [
            "Meng Fang",
            "Trevor Cohn"
        ],
        "abstract": "Cross-lingual model transfer is a compelling and popular method for predicting annotations in a low-resource language, whereby parallel corpora provide a bridge to a high-resource language and its associated annotated corpora. However, parallel data is not readily available for many languages, limiting the applicability of these approaches. We address these drawbacks in our framework which takes advantage of cross-lingual word embeddings trained solely on a high coverage bilingual dictionary. We propose a novel neural network model for joint training from both sources of data based on cross-lingual word embeddings, and show substantial empirical improvements over baseline techniques. We also propose several active learning heuristics, which result in improvements over competitive benchmark methods.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00440",
        "title": "Data Augmentation for Low-Resource Neural Machine Translation",
        "authors": [
            "Marzieh Fadaee",
            "Arianna Bisazza",
            "Christof Monz"
        ],
        "abstract": "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00441",
        "title": "Learning Topic-Sensitive Word Representations",
        "authors": [
            "Marzieh Fadaee",
            "Arianna Bisazza",
            "Christof Monz"
        ],
        "abstract": "Distributed word representations are widely used for modeling words in NLP tasks. Most of the existing models generate one representation per word and do not consider different meanings of a word. We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00464",
        "title": "Speech-Based Visual Question Answering",
        "authors": [
            "Ted Zhang",
            "Dengxin Dai",
            "Tinne Tuytelaars",
            "Marie-Francine Moens",
            "Luc Van Gool"
        ],
        "abstract": "This paper introduces speech-based visual question answering (VQA), the task of generating an answer given an image and a spoken question. Two methods are studied: an end-to-end, deep neural network that directly uses audio waveforms as input versus a pipelined approach that performs ASR (Automatic Speech Recognition) on the question, followed by text-based visual question answering. Furthermore, we investigate the robustness of both methods by injecting various levels of noise into the spoken question and find both methods to be tolerate noise at similar levels.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00545",
        "title": "Labelled network subgraphs reveal stylistic subtleties in written texts",
        "authors": [
            "Vanessa Q. Marinho",
            "Graeme Hirst",
            "Diego R. Amancio"
        ],
        "abstract": "The vast amount of data and increase of computational capacity have allowed the analysis of texts from several perspectives, including the representation of texts as complex networks. Nodes of the network represent the words, and edges represent some relationship, usually word co-occurrence. Even though networked representations have been applied to study some tasks, such approaches are not usually combined with traditional models relying upon statistical paradigms. Because networked models are able to grasp textual patterns, we devised a hybrid classifier, called labelled subgraphs, that combines the frequency of common words with small structures found in the topology of the network, known as motifs. Our approach is illustrated in two contexts, authorship attribution and translationese identification. In the former, a set of novels written by different authors is analyzed. To identify translationese, texts from the Canadian Hansard and the European parliament were classified as to original and translated instances. Our results suggest that labelled subgraphs are able to represent texts and it should be further explored in other tasks, such as the analysis of text complexity, language proficiency, and machine translation.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00557",
        "title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning",
        "authors": [
            "Yacine Jernite",
            "Samuel R. Bowman",
            "David Sontag"
        ],
        "abstract": "This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.\n    ",
        "submission_date": "2017-04-23T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00571",
        "title": "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines",
        "authors": [
            "Andrew Moore",
            "Paul Rayson"
        ],
        "abstract": "This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between -1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00648",
        "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection",
        "authors": [
            "William Yang Wang"
        ],
        "abstract": "Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00652",
        "title": "Efficient Natural Language Response Suggestion for Smart Reply",
        "authors": [
            "Matthew Henderson",
            "Rami Al-Rfou",
            "Brian Strope",
            "Yun-hsuan Sung",
            "Laszlo Lukacs",
            "Ruiqi Guo",
            "Sanjiv Kumar",
            "Balint Miklos",
            "Ray Kurzweil"
        ],
        "abstract": "This paper presents a computationally efficient machine-learned method for natural language response suggestion. Feed-forward neural networks using n-gram embedding features encode messages into vectors which are optimized to give message-response pairs a high dot-product value. An optimized search finds response suggestions. The method is evaluated in a large-scale commercial e-mail application, Inbox by Gmail. Compared to a sequence-to-sequence approach, the new system achieves the same quality at a small fraction of the computational requirements and latency.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00697",
        "title": "From Imitation to Prediction, Data Compression vs Recurrent Neural Networks for Natural Language Processing",
        "authors": [
            "Juan Andr\u00e9s Laura",
            "Gabriel Masi",
            "Luis Argerich"
        ],
        "abstract": "In recent studies [1][13][12] Recurrent Neural Networks were used for generative processes and their surprising performance can be explained by their ability to create good predictions. In addition, data compression is also based on predictions. What the problem comes down to is whether a data compressor could be used to perform as well as recurrent neural networks in natural language processing tasks. If this is possible,then the problem comes down to determining if a compression algorithm is even more intelligent than a neural network in specific tasks related to human language. In our journey we discovered what we think is the fundamental difference between a Data Compression Algorithm and a Recurrent Neural Network.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00746",
        "title": "Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems",
        "authors": [
            "Satoshi Akasaki",
            "Nobuhiro Kaji"
        ],
        "abstract": "Recently emerged intelligent assistants on smartphones and home electronics (e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific task-oriented spoken dialogue systems and open-domain non-task-oriented ones. To realize such hybrid dialogue systems, this paper investigates determining whether or not a user is going to have a chat with the system. To address the lack of benchmark datasets for this task, we construct a new dataset consisting of 15; 160 utterances collected from the real log data of a commercial intelligent assistant (and will release the dataset to facilitate future research activity). In addition, we investigate using tweets and Web search queries for handling open-domain user utterances, which characterize the task of chat detection. Experiments demonstrated that, while simple supervised methods are effective, the use of the tweets and search queries further improves the F1-score from 86.21 to 87.53.\n    ",
        "submission_date": "2017-05-02T00:00:00",
        "last_modified_date": "2018-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00753",
        "title": "A Teacher-Student Framework for Zero-Resource Neural Machine Translation",
        "authors": [
            "Yun Chen",
            "Yang Liu",
            "Yong Cheng",
            "Victor O.K. Li"
        ],
        "abstract": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on this assumption, our method is able to train a source-to-target NMT model (\"student\") without parallel corpora available, guided by an existing pivot-to-target NMT model (\"teacher\") on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.\n    ",
        "submission_date": "2017-05-02T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00823",
        "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset",
        "authors": [
            "Yuya Yoshikawa",
            "Yutaro Shigeto",
            "Akikazu Takeuchi"
        ],
        "abstract": "In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.\n    ",
        "submission_date": "2017-05-02T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00861",
        "title": "Deep Neural Machine Translation with Linear Associative Unit",
        "authors": [
            "Mingxuan Wang",
            "Zhengdong Lu",
            "Jie Zhou",
            "Qun Liu"
        ],
        "abstract": "Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with their capability in modeling complex functions and capturing complex linguistic structures. However NMT systems with deep architecture in their encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often make the optimization much more difficult. To address this problem we propose novel linear associative units (LAU) to reduce the gradient propagation length inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs utilizes linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time direction. The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.\n    ",
        "submission_date": "2017-05-02T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01020",
        "title": "Modeling Source Syntax for Neural Machine Translation",
        "authors": [
            "Junhui Li",
            "Deyi Xiong",
            "Zhaopeng Tu",
            "Muhua Zhu",
            "Min Zhang",
            "Guodong Zhou"
        ],
        "abstract": "Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.\n    ",
        "submission_date": "2017-05-02T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01042",
        "title": "Entity Linking with people entity on Wikipedia",
        "authors": [
            "Weiqian Yan",
            "Kanchan Khurad"
        ],
        "abstract": "This paper introduces a new model that uses named entity recognition, coreference resolution, and entity linking techniques, to approach the task of linking people entities on Wikipedia people pages to their corresponding Wikipedia pages if applicable. Our task is different from general and traditional entity linking because we are working in a limited domain, namely, people entities, and we are including pronouns as entities, whereas in the past, pronouns were never considered as entities in entity linking. We have built 2 models, both outperforms our baseline model significantly. The purpose of our project is to build a model that could be use to generate cleaner data for future entity linking tasks. Our contribution include a clean data set consisting of 50Wikipedia people pages, and 2 entity linking models, specifically tuned for this domain.\n    ",
        "submission_date": "2017-05-02T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01214",
        "title": "A Hybrid Architecture for Multi-Party Conversational Systems",
        "authors": [
            "Maira Gatti de Bayser",
            "Paulo Cavalin",
            "Renan Souza",
            "Alan Braz",
            "Heloisa Candello",
            "Claudio Pinhanez",
            "Jean-Pierre Briot"
        ],
        "abstract": "Multi-party Conversational Systems are systems with natural language interaction between one or more people or systems. From the moment that an utterance is sent to a group, to the moment that it is replied in the group by a member, several activities must be done by the system: utterance understanding, information search, reasoning, among others. In this paper we present the challenges of designing and building multi-party conversational systems, the state of the art, our proposed hybrid architecture using both rules and machine learning and some insights after implementing and evaluating one on the finance domain.\n    ",
        "submission_date": "2017-05-03T00:00:00",
        "last_modified_date": "2017-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01265",
        "title": "On the effectiveness of feature set augmentation using clusters of word embeddings",
        "authors": [
            "Georgios Balikas",
            "Ioannis Partalas"
        ],
        "abstract": "Word clusters have been empirically shown to offer important performance improvements on various tasks. Despite their importance, their incorporation in the standard pipeline of feature engineering relies more on a trial-and-error procedure where one evaluates several hyper-parameters, like the number of clusters to be used. In order to better understand the role of such features we systematically evaluate their effect on four tasks, those of named entity segmentation and classification as well as, those of five-point sentiment classification and quantification. Our results strongly suggest that cluster membership features improve the performance.\n    ",
        "submission_date": "2017-05-03T00:00:00",
        "last_modified_date": "2018-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01306",
        "title": "Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment Detection on Twitter",
        "authors": [
            "Alon Rozental",
            "Daniel Fleischer"
        ],
        "abstract": "This paper describes the Amobee sentiment analysis system, adapted to compete in SemEval 2017 task 4. The system consists of two parts: a supervised training of RNN models based on a Twitter sentiment treebank, and the use of feedforward NN, Naive Bayes and logistic regression classifiers to produce predictions for the different sub-tasks. The algorithm reached the 3rd place on the 5-label classification task (sub-task C).\n    ",
        "submission_date": "2017-05-03T00:00:00",
        "last_modified_date": "2017-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01346",
        "title": "Going Wider: Recurrent Neural Network With Parallel Cells",
        "authors": [
            "Danhao Zhu",
            "Si Shen",
            "Xin-Yu Dai",
            "Jiajun Chen"
        ],
        "abstract": "Recurrent Neural Network (RNN) has been widely applied for sequence modeling. In RNN, the hidden states at current step are full connected to those at previous step, thus the influence from less related features at previous step may potentially decrease model's learning ability. We propose a simple technique called parallel cells (PCs) to enhance the learning ability of Recurrent Neural Network (RNN). In each layer, we run multiple small RNN cells rather than one single large cell. In this paper, we evaluate PCs on 2 tasks. On language modeling task on PTB (Penn Tree Bank), our model outperforms state of art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English translation task, our model increases BLEU score for 0.39 points than baseline model.\n    ",
        "submission_date": "2017-05-03T00:00:00",
        "last_modified_date": "2017-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01452",
        "title": "Chunk-Based Bi-Scale Decoder for Neural Machine Translation",
        "authors": [
            "Hao Zhou",
            "Zhaopeng Tu",
            "Shujian Huang",
            "Xiaohua Liu",
            "Hang Li",
            "Jiajun Chen"
        ],
        "abstract": "In typical neural machine translation~(NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same time-scale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.\n    ",
        "submission_date": "2017-05-03T00:00:00",
        "last_modified_date": "2017-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01684",
        "title": "Probabilistic Typology: Deep Generative Models of Vowel Inventories",
        "authors": [
            "Ryan Cotterell",
            "Jason Eisner"
        ],
        "abstract": "Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.\n    ",
        "submission_date": "2017-05-04T00:00:00",
        "last_modified_date": "2017-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01833",
        "title": "A Finite State and Rule-based Akshara to Prosodeme (A2P) Converter in Hindi",
        "authors": [
            "Somnath Roy"
        ],
        "abstract": "This article describes a software module called Akshara to Prosodeme (A2P) converter in Hindi. It converts an input grapheme into prosedeme (sequence of phonemes with the specification of syllable boundaries and prosodic labels). The software is based on two proposed finite state machines\\textemdash one for the syllabification and another for the syllable labeling. In addition to that, it also uses a set of nonlinear phonological rules proposed for foot formation in Hindi, which encompass solutions to schwa-deletion in simple, compound, derived and inflected words. The nonlinear phonological rules are based on metrical phonology with the provision of recursive foot structure. A software module is implemented in Python. The testing of the software for syllabification, syllable labeling, schwa deletion and prosodic labeling yield an accuracy of more than 99% on a lexicon of size 28664 words.\n    ",
        "submission_date": "2017-05-04T00:00:00",
        "last_modified_date": "2017-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01991",
        "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU",
        "authors": [
            "Jacob Devlin"
        ],
        "abstract": "Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems. Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/throughput close to that of a phrasal decoder.\n",
        "submission_date": "2017-05-04T00:00:00",
        "last_modified_date": "2017-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02012",
        "title": "Machine Comprehension by Text-to-Text Neural Question Generation",
        "authors": [
            "Xingdi Yuan",
            "Tong Wang",
            "Caglar Gulcehre",
            "Alessandro Sordoni",
            "Philip Bachman",
            "Sandeep Subramanian",
            "Saizheng Zhang",
            "Adam Trischler"
        ],
        "abstract": "We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD.\n    ",
        "submission_date": "2017-05-04T00:00:00",
        "last_modified_date": "2017-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02023",
        "title": "Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification",
        "authors": [
            "Hussam Hamdan"
        ],
        "abstract": "This paper presents Senti17 system which uses ten convolutional neural networks (ConvNet) to assign a sentiment label to a tweet. The network consists of a convolutional layer followed by a fully-connected layer and a Softmax on top. Ten instances of this network are initialized with the same word embeddings as inputs but with different initializations for the network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4%\n    ",
        "submission_date": "2017-05-04T00:00:00",
        "last_modified_date": "2017-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02073",
        "title": "Cross-lingual Distillation for Text Classification",
        "authors": [
            "Ruochen Xu",
            "Yiming Yang"
        ],
        "abstract": "Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.\n    ",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2018-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02077",
        "title": "Crowdsourcing Argumentation Structures in Chinese Hotel Reviews",
        "authors": [
            "Mengxue Li",
            "Shiqiang Geng",
            "Yang Gao",
            "Haijing Liu",
            "Hao Wang"
        ],
        "abstract": "Argumentation mining aims at automatically extracting the premises-claim discourse structures in natural language texts. There is a great demand for argumentation corpora for customer reviews. However, due to the controversial nature of the argumentation annotation task, there exist very few large-scale argumentation corpora for customer reviews. In this work, we novelly use the crowdsourcing technique to collect argumentation annotations in Chinese hotel reviews. As the first Chinese argumentation dataset, our corpus includes 4814 argument component annotations and 411 argument relation annotations, and its annotations qualities are comparable to some widely used argumentation corpora in other languages.\n    ",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02131",
        "title": "Joint RNN Model for Argument Component Boundary Detection",
        "authors": [
            "Minglan Li",
            "Yang Gao",
            "Hui Wen",
            "Yang Du",
            "Haijing Liu",
            "Hao Wang"
        ],
        "abstract": "Argument Component Boundary Detection (ACBD) is an important sub-task in argumentation mining; it aims at identifying the word sequences that constitute argument components, and is usually considered as the first sub-task in the argumentation mining pipeline. Existing ACBD methods heavily depend on task-specific knowledge, and require considerable human efforts on feature-engineering. To tackle these problems, in this work, we formulate ACBD as a sequence labeling problem and propose a variety of Recurrent Neural Network (RNN) based methods, which do not use domain specific or handcrafted features beyond the relative position of the sentence in the document. In particular, we propose a novel joint RNN model that can predict whether sentences are argumentative or not, and use the predicted results to more precisely detect the argument component boundaries. We evaluate our techniques on two corpora from two different genres; results suggest that our joint RNN model obtain the state-of-the-art performance on both datasets.\n    ",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02269",
        "title": "Sequential Attention: A Context-Aware Alignment Function for Machine Reading",
        "authors": [
            "Sebastian Brarda",
            "Philip Yeres",
            "Samuel R. Bowman"
        ],
        "abstract": "In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (on the Who did What and CNN datasets) and show that it dramatically improves a strong baseline--the Stanford Reader--and is competitive with the state of the art.\n    ",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2017-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02304",
        "title": "Deep Speaker: an End-to-End Neural Speaker Embedding System",
        "authors": [
            "Chao Li",
            "Xiaokong Ma",
            "Bing Jiang",
            "Xiangang Li",
            "Xuewei Zhang",
            "Xiao Liu",
            "Ying Cao",
            "Ajay Kannan",
            "Zhenyao Zhu"
        ],
        "abstract": "We present Deep Speaker, a neural speaker embedding system that maps utterances to a hypersphere where speaker similarity is measured by cosine similarity. The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN and GRU architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity. Experiments on three distinct datasets suggest that Deep Speaker outperforms a DNN-based i-vector baseline. For example, Deep Speaker reduces the verification equal error rate by 50% (relatively) and improves the identification accuracy by 60% (relatively) on a text-independent dataset. We also present results that suggest adapting from a model trained with Mandarin can improve accuracy for English speaker recognition.\n    ",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02314",
        "title": "Building Morphological Chains for Agglutinative Languages",
        "authors": [
            "Serkan Ozen",
            "Burcu Can"
        ],
        "abstract": "In this paper, we build morphological chains for agglutinative languages by using a log-linear model for the morphological segmentation task. The model is based on the unsupervised morphological segmentation system called MorphoChains. We extend MorphoChains log linear model by expanding the candidate space recursively to cover more split points for agglutinative languages such as Turkish, whereas in the original model candidates are generated by considering only binary segmentation of each word. The results show that we improve the state-of-art Turkish scores by 12% having a F-measure of 72% and we improve the English scores by 3% having a F-measure of 74%. Eventually, the system outperforms both MorphoChains and other well-known unsupervised morphological segmentation systems. The results indicate that candidate generation plays an important role in such an unsupervised log-linear model that is learned using contrastive estimation with negative samples.\n    ",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02364",
        "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
        "authors": [
            "Alexis Conneau",
            "Douwe Kiela",
            "Holger Schwenk",
            "Loic Barrault",
            "Antoine Bordes"
        ],
        "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.\n    ",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2018-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02394",
        "title": "Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks",
        "authors": [
            "Jonathan Chang",
            "Stefan Scherer"
        ],
        "abstract": "Automatically assessing emotional valence in human speech has historically been a difficult task for machine learning algorithms. The subtle changes in the voice of the speaker that are indicative of positive or negative emotional states are often \"overshadowed\" by voice characteristics relating to emotional intensity or emotional activation. In this work we explore a representation learning approach that automatically derives discriminative representations of emotional speech. In particular, we investigate two machine learning strategies to improve classifier performance: (1) utilization of unlabeled data using a deep convolutional generative adversarial network (DCGAN), and (2) multitask learning. Within our extensive experiments we leverage a multitask annotated emotional corpus as well as a large unlabeled meeting corpus (around 100 hours). Our speaker-independent classification experiments show that in particular the use of unlabeled data in our investigations improves performance of the classifiers and both fully supervised baseline approaches are outperformed considerably. We improve the classification of emotional valence on a discrete 5-point scale to 43.88% and on a 3-point scale to 49.80%, which is competitive to state-of-the-art performance.\n    ",
        "submission_date": "2017-04-22T00:00:00",
        "last_modified_date": "2017-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02395",
        "title": "On Using Active Learning and Self-Training when Mining Performance Discussions on Stack Overflow",
        "authors": [
            "Markus Borg",
            "Iben Lennerstad",
            "Rasmus Ros",
            "Elizabeth Bjarnason"
        ],
        "abstract": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02411",
        "title": "Max-Pooling Loss Training of Long Short-Term Memory Networks for Small-Footprint Keyword Spotting",
        "authors": [
            "Ming Sun",
            "Anirudh Raju",
            "George Tucker",
            "Sankaran Panchapagesan",
            "Gengshen Fu",
            "Arindam Mandal",
            "Spyros Matsoukas",
            "Nikko Strom",
            "Shiv Vitaladevuni"
        ],
        "abstract": "We propose a max-pooling based loss function for training Long Short-Term Memory (LSTM) networks for small-footprint keyword spotting (KWS), with low CPU, memory, and latency requirements. The max-pooling loss training can be further guided by initializing with a cross-entropy loss trained network. A posterior smoothing based evaluation approach is employed to measure keyword spotting performance. Our experimental results show that LSTM models trained using cross-entropy loss or max-pooling loss outperform a cross-entropy loss trained baseline feed-forward Deep Neural Network (DNN). In addition, max-pooling loss trained LSTM with randomly initialized network performs better compared to cross-entropy loss trained LSTM. Finally, the max-pooling loss trained LSTM initialized with a cross-entropy pre-trained network shows the best performance, which yields $67.6\\%$ relative reduction compared to baseline feed-forward DNN in Area Under the Curve (AUC) measure.\n    ",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02452",
        "title": "A Generative Model of a Pronunciation Lexicon for Hindi",
        "authors": [
            "Pramod Pandey",
            "Somnath Roy"
        ],
        "abstract": "Voice browser applications in Text-to- Speech (TTS) and Automatic Speech Recognition (ASR) systems crucially depend on a pronunciation lexicon. The present paper describes the model of pronunciation lexicon of Hindi developed to automatically generate the output forms of Hindi at two levels, the <phoneme> and the <PS> (PS, in short for Prosodic Structure). The latter level involves both syllable-division and stress placement. The paper describes the tool developed for generating the two-level outputs of lexica in Hindi.\n    ",
        "submission_date": "2017-05-06T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02494",
        "title": "Learning Distributed Representations of Texts and Entities from Knowledge Base",
        "authors": [
            "Ikuya Yamada",
            "Hiroyuki Shindo",
            "Hideaki Takeda",
            "Yoshiyasu Takefuji"
        ],
        "abstract": "We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research.\n    ",
        "submission_date": "2017-05-06T00:00:00",
        "last_modified_date": "2017-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02700",
        "title": "Generating Memorable Mnemonic Encodings of Numbers",
        "authors": [
            "Vincent Fiorentini",
            "Megan Shao",
            "Julie Medero"
        ],
        "abstract": "The major system is a mnemonic system that can be used to memorize sequences of numbers. In this work, we present a method to automatically generate sentences that encode a given number. We propose several encoding models and compare the most promising ones in a password memorability study. The results of the study show that a model combining part-of-speech sentence templates with an $n$-gram language model produces the most memorable password representations.\n    ",
        "submission_date": "2017-05-07T00:00:00",
        "last_modified_date": "2017-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02735",
        "title": "Combating Human Trafficking with Deep Multimodal Models",
        "authors": [
            "Edmund Tong",
            "Amir Zadeh",
            "Cara Jones",
            "Louis-Philippe Morency"
        ],
        "abstract": "Human trafficking is a global epidemic affecting millions of people across the planet. Sex trafficking, the dominant form of human trafficking, has seen a significant rise mostly due to the abundance of escort websites, where human traffickers can openly advertise among at-will escort advertisements. In this paper, we take a major step in the automatic detection of advertisements suspected to pertain to human trafficking. We present a novel dataset called Trafficking-10k, with more than 10,000 advertisements annotated for this task. The dataset contains two sources of information per advertisement: text and images. For the accurate detection of trafficking advertisements, we designed and trained a deep multimodal model called the Human Trafficking Deep Network (HTDN).\n    ",
        "submission_date": "2017-05-08T00:00:00",
        "last_modified_date": "2017-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02750",
        "title": "Density Estimation for Geolocation via Convolutional Mixture Density Network",
        "authors": [
            "Hayate Iso",
            "Shoko Wakamiya",
            "Eiji Aramaki"
        ],
        "abstract": "Nowadays, geographic information related to Twitter is crucially important for fine-grained applications. However, the amount of geographic information avail- able on Twitter is low, which makes the pursuit of many applications challenging. Under such circumstances, estimating the location of a tweet is an important goal of the study. Unlike most previous studies that estimate the pre-defined district as the classification task, this study employs a probability distribution to represent richer information of the tweet, not only the location but also its ambiguity. To realize this modeling, we propose the convolutional mixture density network (CMDN), which uses text data to estimate the mixture model parameters. Experimentally obtained results reveal that CMDN achieved the highest prediction performance among the method for predicting the exact coordinates. It also provides a quantitative representation of the location ambiguity for each tweet that properly works for extracting the reliable location estimations.\n    ",
        "submission_date": "2017-05-08T00:00:00",
        "last_modified_date": "2017-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02798",
        "title": "Reinforced Mnemonic Reader for Machine Reading Comprehension",
        "authors": [
            "Minghao Hu",
            "Yuxing Peng",
            "Zhen Huang",
            "Xipeng Qiu",
            "Furu Wei",
            "Ming Zhou"
        ],
        "abstract": "In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets.\n    ",
        "submission_date": "2017-05-08T00:00:00",
        "last_modified_date": "2018-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02925",
        "title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
        "authors": [
            "Pradeep Dasigi",
            "Waleed Ammar",
            "Chris Dyer",
            "Eduard Hovy"
        ],
        "abstract": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase(PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.\n    ",
        "submission_date": "2017-05-08T00:00:00",
        "last_modified_date": "2017-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03122",
        "title": "Convolutional Sequence to Sequence Learning",
        "authors": [
            "Jonas Gehring",
            "Michael Auli",
            "David Grangier",
            "Denis Yarats",
            "Yann N. Dauphin"
        ],
        "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.\n    ",
        "submission_date": "2017-05-08T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03127",
        "title": "Word and Phrase Translation with word2vec",
        "authors": [
            "Stefan Jansen"
        ],
        "abstract": "Word and phrase tables are key inputs to machine translations, but costly to produce. New unsupervised learning methods represent words and phrases in a high-dimensional vector space, and these monolingual embeddings have been shown to encode syntactic and semantic relationships between language elements. The information captured by these embeddings can be exploited for bilingual translation by learning a transformation matrix that allows matching relative positions across two monolingual vector spaces. This method aims to identify high-quality candidates for word and phrase translation more cost-effectively from unlabeled data.\n",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2018-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03151",
        "title": "Phonetic Temporal Neural Model for Language Identification",
        "authors": [
            "Zhiyuan Tang",
            "Dong Wang",
            "Yixiang Chen",
            "Lantian Li",
            "Andrew Abel"
        ],
        "abstract": "Deep neural models, particularly the LSTM-RNN model, have shown great potential for language identification (LID). However, the use of phonetic information has been largely overlooked by most existing neural LID methods, although this information has been used very successfully in conventional phonetic LID systems. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system that accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features. This new model is similar to traditional phonetic LID methods, but the phonetic knowledge here is much richer: it is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16-OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional i-vector approach on short utterances and in noisy conditions.\n    ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03152",
        "title": "Phone-aware Neural Language Identification",
        "authors": [
            "Zhiyuan Tang",
            "Dong Wang",
            "Yixiang Chen",
            "Ying Shi",
            "Lantian Li"
        ],
        "abstract": "Pure acoustic neural models, particularly the LSTM-RNN model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID models, although this information has been used in the conventional phonetic LID systems with a great success. We present a phone-aware neural LID architecture, which is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR system. By utilizing the phonetic knowledge, the LID performance can be significantly improved. Interestingly, even if the test language is not involved in the ASR training, the phonetic knowledge still presents a large contribution. Our experiments conducted on four languages within the Babel corpus demonstrated that the phone-aware approach is highly effective.\n    ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2017-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03202",
        "title": "Does William Shakespeare REALLY Write Hamlet? Knowledge Representation Learning with Confidence",
        "authors": [
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Fen Lin",
            "Leyu Lin"
        ],
        "abstract": "Knowledge graphs (KGs), which could provide essential relational information between entities, have been widely utilized in various knowledge-driven applications. Since the overall human knowledge is innumerable that still grows explosively and changes frequently, knowledge construction and update inevitably involve automatic mechanisms with less human supervision, which usually bring in plenty of noises and conflicts to KGs. However, most conventional knowledge representation learning methods assume that all triple facts in existing KGs share the same significance without any noises. To address this problem, we propose a novel confidence-aware knowledge representation learning framework (CKRL), which detects possible noises in KGs while learning knowledge representations with confidence simultaneously. Specifically, we introduce the triple confidence to conventional translation-based methods for knowledge representation learning. To make triple confidence more flexible and universal, we only utilize the internal structural information in KGs, and propose three kinds of triple confidences considering both local and global structural information. In experiments, We evaluate our models on knowledge graph noise detection, knowledge graph completion and triple classification. Experimental results demonstrate that our confidence-aware models achieve significant and consistent improvements on all tasks, which confirms the capability of CKRL modeling confidence with structural information in both KG noise detection and knowledge representation learning.\n    ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2018-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03247",
        "title": "A Systematic Review of Hindi Prosody",
        "authors": [
            "Somnath Roy"
        ],
        "abstract": "Prosody describes both form and function of a sentence using the suprasegmental features of speech. Prosody phenomena are explored in the domain of higher phonological constituents such as word, phonological phrase and intonational phrase. The study of prosody at the word level is called word prosody and above word level is called sentence prosody. Word Prosody describes stress pattern by comparing the prosodic features of its constituent syllables. Sentence Prosody involves the study on phrasing pattern and intonatonal pattern of a language. The aim of this study is to summarize the existing works on Hindi prosody carried out in different domain of language and speech processing. The review is presented in a systematic fashion so that it could be a useful resource for one who wants to build on the existing works.\n    ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2017-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03261",
        "title": "Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers",
        "authors": [
            "Zibo Yi",
            "Shasha Li",
            "Jie Yu",
            "Qingbo Wu"
        ],
        "abstract": "Drug-drug interaction (DDI) is a vital information when physicians and pharmacists intend to co-administer two or more drugs. Thus, several DDI databases are constructed to avoid mistakenly combined use. In recent years, automatically extracting DDIs from biomedical text has drawn researchers' attention. However, the existing work utilize either complex feature engineering or NLP tools, both of which are insufficient for sentence comprehension. Inspired by the deep learning approaches in natural language processing, we propose a recur- rent neural network model with multiple attention layers for DDI classification. We evaluate our model on 2013 SemEval DDIExtraction dataset. The experiments show that our model classifies most of the drug pairs into correct DDI categories, which outperforms the existing NLP or deep learning methods.\n    ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2017-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03389",
        "title": "Logical Parsing from Natural Language Based on a Neural Translation Model",
        "authors": [
            "Liang Li",
            "Pengyu Li",
            "Yifan Liu",
            "Tao Wan",
            "Zengchang Qin"
        ],
        "abstract": "Semantic parsing has emerged as a significant and powerful paradigm for natural language interface and question answering systems. Traditional methods of building a semantic parser rely on high-quality lexicons, hand-crafted grammars and linguistic features which are limited by applied domain or representation. In this paper, we propose a general approach to learn from denotations based on Seq2Seq model augmented with attention mechanism. We encode input sequence into vectors and use dynamic programming to infer candidate logical forms. We utilize the fact that similar utterances should have similar logical forms to help reduce the searching space. Under our learning policy, the Seq2Seq model can learn mappings gradually with noises. Curriculum learning is adopted to make the learning smoother. We test our method on the arithmetic domain which shows our model can successfully infer the correct logical forms and learn the word meanings, compositionality and operation orders simultaneously.\n    ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2017-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03454",
        "title": "The Pragmatics of Indirect Commands in Collaborative Discourse",
        "authors": [
            "Matthew Lamm",
            "Mihail Eric"
        ],
        "abstract": "Today's artificial assistants are typically prompted to perform tasks through direct, imperative commands such as \\emph{Set a timer} or \\emph{Pick up the box}. However, to progress toward more natural exchanges between humans and these assistants, it is important to understand the way non-imperative utterances can indirectly elicit action of an addressee. In this paper, we investigate command types in the setting of a grounded, collaborative game. We focus on a less understood family of utterances for eliciting agent action, locatives like \\emph{The chair is in the other room}, and demonstrate how these utterances indirectly command in specific game state contexts. Our work shows that models with domain-specific grounding can effectively realize the pragmatic reasoning that is necessary for more robust natural language interaction.\n    ",
        "submission_date": "2017-05-08T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03455",
        "title": "Sequential Dialogue Context Modeling for Spoken Language Understanding",
        "authors": [
            "Ankur Bapna",
            "Gokhan Tur",
            "Dilek Hakkani-Tur",
            "Larry Heck"
        ],
        "abstract": "Spoken Language Understanding (SLU) is a key component of goal oriented dialogue systems that would parse user utterances into semantic frame representations. Traditionally SLU does not utilize the dialogue history beyond the previous system turn and contextual ambiguities are resolved by the downstream components. In this paper, we explore novel approaches for modeling dialogue context in a recurrent neural network (RNN) based language understanding system. We propose the Sequential Dialogue Encoder Network, that allows encoding context from the dialogue history in chronological order. We compare the performance of our proposed architecture with two context models, one that uses just the previous turn context and another that encodes dialogue context in a memory network, but loses the order of utterances in the dialogue history. Experiments with a multi-domain dialogue dataset demonstrate that the proposed architecture results in reduced semantic frame error rates.\n    ",
        "submission_date": "2017-05-08T00:00:00",
        "last_modified_date": "2017-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03508",
        "title": "DeepDeath: Learning to Predict the Underlying Cause of Death with Big Data",
        "authors": [
            "Hamid Reza Hassanzadeh",
            "Ying Sha",
            "May D. Wang"
        ],
        "abstract": "Multiple cause-of-death data provides a valuable source of information that can be used to enhance health standards by predicting health related trajectories in societies with large populations. These data are often available in large quantities across U.S. states and require Big Data techniques to uncover complex hidden patterns. We design two different classes of models suitable for large-scale analysis of mortality data, a Hadoop-based ensemble of random forests trained over N-grams, and the DeepDeath, a deep classifier based on the recurrent neural network (RNN). We apply both classes to the mortality data provided by the National Center for Health Statistics and show that while both perform significantly better than the random classifier, the deep model that utilizes long short-term memory networks (LSTMs), surpasses the N-gram based models and is capable of learning the temporal aspect of the data without a need for building ad-hoc, expert-driven features.\n    ",
        "submission_date": "2017-05-06T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03551",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "authors": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel S. Weld",
            "Luke Zettlemoyer"
        ],
        "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2017-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03557",
        "title": "DeepTingle",
        "authors": [
            "Ahmed Khalifa",
            "Gabriella A. B. Barros",
            "Julian Togelius"
        ],
        "abstract": "DeepTingle is a text prediction and classification system trained on the collected works of the renowned fantastic gay erotica author Chuck Tingle. Whereas the writing assistance tools you use everyday (in the form of predictive text, translation, grammar checking and so on) are trained on generic, purportedly \"neutral\" datasets, DeepTingle is trained on a very specific, internally consistent but externally arguably eccentric dataset. This allows us to foreground and confront the norms embedded in data-driven creativity and productivity assistance tools. As such tools effectively function as extensions of our cognition into technology, it is important to identify the norms they embed within themselves and, by extension, us. DeepTingle is realized as a web application based on LSTM networks and the GloVe word embedding, implemented in JavaScript with Keras-JS.\n    ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2019-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03645",
        "title": "A Survey of Deep Learning Methods for Relation Extraction",
        "authors": [
            "Shantanu Kumar"
        ],
        "abstract": "Relation Extraction is an important sub-task of Information Extraction which has the potential of employing deep learning (DL) models with the creation of large datasets using distant supervision. In this review, we compare the contributions and pitfalls of the various DL models that have been used for the task, to help guide the path ahead.\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03802",
        "title": "Analysing Data-To-Text Generation Benchmarks",
        "authors": [
            "Laura Perez-Beltrachini",
            "Claire Gardent"
        ],
        "abstract": "Recently, several data-sets associating data to text have been created to train data-to-text surface realisers. It is unclear however to what extent the surface realisation task exercised by these data-sets is linguistically challenging. Do these data-sets provide enough variety to encourage the development of generic, high-quality data-to-text surface realisers ? In this paper, we argue that these data-sets have important drawbacks. We back up our claim using statistics, metrics and manual evaluation. We conclude by eliciting a set of criteria for the creation of a data-to-text benchmark which could help better support the development, evaluation and comparison of linguistically sophisticated data-to-text surface realisers.\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03865",
        "title": "Survey of Visual Question Answering: Datasets and Techniques",
        "authors": [
            "Akshay Kumar Gupta"
        ],
        "abstract": "Visual question answering (or VQA) is a new and exciting problem that combines natural language processing and computer vision techniques. We present a survey of the various datasets and models that have been used to tackle this task. The first part of the survey details the various datasets for VQA and compares them along some common factors. The second part of this survey details the different approaches for VQA, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into the first three. Finally, we compare the performances of these approaches and provide some directions for future work.\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03919",
        "title": "A Minimal Span-Based Neural Constituency Parser",
        "authors": [
            "Mitchell Stern",
            "Jacob Andreas",
            "Dan Klein"
        ],
        "abstract": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03995",
        "title": "Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix",
        "authors": [
            "Bingfeng Luo",
            "Yansong Feng",
            "Zheng Wang",
            "Zhanxing Zhu",
            "Songfang Huang",
            "Rui Yan",
            "Dongyan Zhao"
        ],
        "abstract": "Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application of distant supervision in relation extraction. We show that the dynamic transition matrix can effectively characterize the noise in the training data built by distant supervision. The transition matrix can be effectively trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04003",
        "title": "Content-based Approach for Vietnamese Spam SMS Filtering",
        "authors": [
            "Thai-Hoang Pham",
            "Phuong Le-Hong"
        ],
        "abstract": "Short Message Service (SMS) spam is a serious problem in Vietnam because of the availability of very cheap pre-paid SMS packages. There are some systems to detect and filter spam messages for English, most of which use machine learning techniques to analyze the content of messages and classify them. For Vietnamese, there is some research on spam email filtering but none focused on SMS. In this work, we propose the first system for filtering Vietnamese spam SMS. We first propose an appropriate preprocessing method since existing tools for Vietnamese preprocessing cannot give good accuracy on our dataset. We then experiment with vector representations and classifiers to find the best model for this problem. Our system achieves an accuracy of 94% when labelling spam messages while the misclassification rate of legitimate messages is relatively small, about only 0.4%. This is an encouraging result compared to that of English and can be served as a strong baseline for future development of Vietnamese SMS spam prevention systems.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04038",
        "title": "Building a Semantic Role Labelling System for Vietnamese",
        "authors": [
            "Thai-Hoang Pham",
            "Xuan-Khoai Pham",
            "Phuong Le-Hong"
        ],
        "abstract": "Semantic role labelling (SRL) is a task in natural language processing which detects and classifies the semantic arguments associated with the predicates of a sentence. It is an important step towards understanding the meaning of a natural language. There exists SRL systems for well-studied languages like English, Chinese or Japanese but there is not any such system for the Vietnamese language. In this paper, we present the first SRL system for Vietnamese with encouraging accuracy. We first demonstrate that a simple application of SRL techniques developed for English could not give a good accuracy for Vietnamese. We then introduce a new algorithm for extracting candidate syntactic constituents, which is much more accurate than the common node-mapping algorithm usually used in the identification step. Finally, in the classification step, in addition to the common linguistic features, we propose novel and useful features for use in SRL. Our SRL system achieves an $F_1$ score of 73.53\\% on the Vietnamese PropBank corpus. This system, including software and corpus, is available as an open source project and we believe that it is a good baseline for the development of future Vietnamese SRL systems.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04044",
        "title": "End-to-end Recurrent Neural Network Models for Vietnamese Named Entity Recognition: Word-level vs. Character-level",
        "authors": [
            "Thai-Hoang Pham",
            "Phuong Le-Hong"
        ],
        "abstract": "This paper demonstrates end-to-end neural network architectures for Vietnamese named entity recognition. Our best model is a combination of bidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network (CNN), Conditional Random Field (CRF), using pre-trained word embeddings as input, which achieves an F1 score of 88.59% on a standard test set. Our system is able to achieve a comparable performance to the first-rank system of the VLSP campaign without using any syntactic or hand-crafted features. We also give an extensive empirical study on using common deep learning models for Vietnamese NER, at both word and character level.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04153",
        "title": "Dynamic Compositional Neural Networks over Tree Structure",
        "authors": [
            "Pengfei Liu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Tree-structured neural networks have proven to be effective in learning semantic representations by exploiting syntactic information. In spite of their success, most existing models suffer from the underfitting problem: they recursively use the same shared compositional function throughout the whole compositional process and lack expressive power due to inability to capture the richness of compositionality. In this paper, we address this issue by introducing the dynamic compositional neural networks over tree structure (DC-TreeNN), in which the compositional function is dynamically generated by a meta network. The role of meta-network is to capture the metaknowledge across the different compositional rules and formulate them. Experimental results on two typical tasks show the effectiveness of the proposed models.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04187",
        "title": "On the role of words in the network structure of texts: application to authorship attribution",
        "authors": [
            "Camilo Akimushkin",
            "Diego R. Amancio",
            "Osvaldo N. Oliveira Jr"
        ],
        "abstract": "Well-established automatic analyses of texts mainly consider frequencies of linguistic units, e.g. letters, words and bigrams, while methods based on co-occurrence networks consider the structure of texts regardless of the nodes label (i.e. the words semantics). In this paper, we reconcile these distinct viewpoints by introducing a generalized similarity measure to compare texts which accounts for both the network structure of texts and the role of individual words in the networks. We use the similarity measure for authorship attribution of three collections of books, each composed of 8 authors and 10 books per author. High accuracy rates were obtained with typical values from 90% to 98.75%, much higher than with the traditional the TF-IDF approach for the same collections. These accuracies are also higher than taking only the topology of networks into account. We conclude that the different properties of specific words on the macroscopic scale structure of a whole text are as relevant as their frequency of appearance; conversely, considering the identity of nodes brings further knowledge about a piece of text represented as a network.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04253",
        "title": "Sketching Word Vectors Through Hashing",
        "authors": [
            "Behrang QasemiZadeh",
            "Laura Kallmeyer"
        ],
        "abstract": "We propose a new fast word embedding technique using hash functions. The method is a derandomization of a new type of random projections: By disregarding the classic constraint used in designing random projections (i.e., preserving pairwise distances in a particular normed space), our solution exploits extremely sparse non-negative random projections. Our experiments show that the proposed method can achieve competitive results, comparable to neural embedding learning techniques, however, with only a fraction of the computational complexity of these methods. While the proposed derandomization enhances the computational and space complexity of our method, the possibility of applying weighting methods such as positive pointwise mutual information (PPMI) to our models after their construction (and at a reduced dimensionality) imparts a high discriminatory power to the resulting embeddings. Obviously, this method comes with other known benefits of random projection-based techniques such as ease of update.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2018-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04304",
        "title": "A Deep Reinforced Model for Abstractive Summarization",
        "authors": [
            "Romain Paulus",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04350",
        "title": "Imagination improves Multimodal Translation",
        "authors": [
            "Desmond Elliott",
            "\u00c1kos K\u00e1d\u00e1r"
        ],
        "abstract": "We decompose multimodal translation into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04400",
        "title": "Reducing Bias in Production Speech Models",
        "authors": [
            "Eric Battenberg",
            "Rewon Child",
            "Adam Coates",
            "Christopher Fougner",
            "Yashesh Gaur",
            "Jiaji Huang",
            "Heewoo Jun",
            "Ajay Kannan",
            "Markus Kliegl",
            "Atul Kumar",
            "Hairong Liu",
            "Vinay Rao",
            "Sanjeev Satheesh",
            "David Seetapun",
            "Anuroop Sriram",
            "Zhenyao Zhu"
        ],
        "abstract": "Replacing hand-engineered pipelines with end-to-end deep learning systems has enabled strong results in applications like speech and object recognition. However, the causality and latency constraints of production systems put end-to-end speech models back into the underfitting regime and expose biases in the model that we show cannot be overcome by \"scaling up\", i.e., training bigger models on more data. In this work we systematically identify and address sources of bias, reducing error rates by up to 20% while remaining practical for deployment. We achieve this by utilizing improved neural architectures for streaming inference, solving optimization issues, and employing strategies that increase audio and label modelling versatility.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04416",
        "title": "Evaluating vector-space models of analogy",
        "authors": [
            "Dawn Chen",
            "Joshua C. Peterson",
            "Thomas L. Griffiths"
        ],
        "abstract": "Vector-space representations provide geometric tools for reasoning about the similarity of a set of objects and their relationships. Recent machine learning methods for deriving vector-space embeddings of words (e.g., word2vec) have achieved considerable success in natural language processing. These vector spaces have also been shown to exhibit a surprising capacity to capture verbal analogies, with similar results for natural images, giving new life to a classic model of analogies as parallelograms that was first proposed by cognitive scientists. We evaluate the parallelogram model of analogy as applied to modern word embeddings, providing a detailed analysis of the extent to which this approach captures human relational similarity judgments in a large benchmark dataset. We find that that some semantic relationships are better captured than others. We then provide evidence for deeper limitations of the parallelogram model based on the intrinsic geometric constraints of vector spaces, paralleling classic results for first-order similarity.\n    ",
        "submission_date": "2017-05-12T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04434",
        "title": "Arc-swift: A Novel Transition System for Dependency Parsing",
        "authors": [
            "Peng Qi",
            "Christopher D. Manning"
        ],
        "abstract": "Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments. Correct individual decisions hence require global information about the sentence context and mistakes cause error propagation. This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a single transition. This allows the parser to leverage lexical information more directly in transition decisions. Hence, arc-swift can achieve significantly better performance with a very small beam size. Our parsers reduce error by 3.7--7.6% relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.\n    ",
        "submission_date": "2017-05-12T00:00:00",
        "last_modified_date": "2017-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04815",
        "title": "Learning Semantic Correspondences in Technical Documentation",
        "authors": [
            "Kyle Richardson",
            "Jonas Kuhn"
        ],
        "abstract": "We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.\n    ",
        "submission_date": "2017-05-13T00:00:00",
        "last_modified_date": "2017-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04839",
        "title": "Annotating and Modeling Empathy in Spoken Conversations",
        "authors": [
            "Firoj Alam",
            "Morena Danieli",
            "Giuseppe Riccardi"
        ],
        "abstract": "Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. The lack of an operational definition of empathy makes it difficult to measure it. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline.\n    ",
        "submission_date": "2017-05-13T00:00:00",
        "last_modified_date": "2017-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05039",
        "title": "Joint Modeling of Content and Discourse Relations in Dialogues",
        "authors": [
            "Kechen Qin",
            "Lu Wang",
            "Joseph Kim"
        ],
        "abstract": "We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform state-of-the-art approaches for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members' understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.\n    ",
        "submission_date": "2017-05-14T00:00:00",
        "last_modified_date": "2017-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05040",
        "title": "Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes",
        "authors": [
            "Lu Wang",
            "Nick Beauchamp",
            "Sarah Shugars",
            "Kechen Qin"
        ],
        "abstract": "Debate and deliberation play essential roles in politics and government, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model's combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74% accuracy, significantly outperforming linguistic features alone (66%). Our model finds that winning sides employ stronger arguments, and allows us to identify the linguistic features associated with strong or weak arguments.\n    ",
        "submission_date": "2017-05-15T00:00:00",
        "last_modified_date": "2017-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05183",
        "title": "Representation learning of drug and disease terms for drug repositioning",
        "authors": [
            "Sahil Manchanda",
            "Ashish Anand"
        ],
        "abstract": "Drug repositioning (DR) refers to identification of novel indications for the approved drugs. The requirement of huge investment of time as well as money and risk of failure in clinical trials have led to surge in interest in drug repositioning. DR exploits two major aspects associated with drugs and diseases: existence of similarity among drugs and among diseases due to their shared involved genes or pathways or common biological effects. Existing methods of identifying drug-disease association majorly rely on the information available in the structured databases only. On the other hand, abundant information available in form of free texts in biomedical research articles are not being fully exploited. Word-embedding or obtaining vector representation of words from a large corpora of free texts using neural network methods have been shown to give significant performance for several natural language processing tasks. In this work we propose a novel way of representation learning to obtain features of drugs and diseases by combining complementary information available in unstructured texts and structured datasets. Next we use matrix completion approach on these feature vectors to learn projection matrix between drug and disease vector spaces. The proposed method has shown competitive performance with state-of-the-art methods. Further, the case studies on Alzheimer's and Hypertension diseases have shown that the predicted associations are matching with the existing knowledge.\n    ",
        "submission_date": "2017-05-15T00:00:00",
        "last_modified_date": "2017-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05414",
        "title": "Key-Value Retrieval Networks for Task-Oriented Dialogue",
        "authors": [
            "Mihail Eric",
            "Christopher D. Manning"
        ],
        "abstract": "Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.\n    ",
        "submission_date": "2017-05-15T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05437",
        "title": "A Biomedical Information Extraction Primer for NLP Researchers",
        "authors": [
            "Surag Nair"
        ],
        "abstract": "Biomedical Information Extraction is an exciting field at the crossroads of Natural Language Processing, Biology and Medicine. It encompasses a variety of different tasks that require application of state-of-the-art NLP techniques, such as NER and Relation Extraction. This paper provides an overview of the problems in the field and discusses some of the techniques used for solving them.\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05487",
        "title": "NeuroNER: an easy-to-use program for named-entity recognition based on neural networks",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Peter Szolovits"
        ],
        "abstract": "Named-entity recognition (NER) aims at identifying entities of interest in a text. Artificial neural networks (ANNs) have recently been shown to outperform existing NER systems. However, ANNs remain challenging to use for non-expert users. In this paper, we present NeuroNER, an easy-to-use named-entity recognition tool based on ANNs. Users can annotate entities using a graphical web-based user interface (BRAT): the annotations are then used to train an ANN, which in turn predict entities' locations and categories in new texts. NeuroNER makes this annotation-training-prediction flow smooth and accessible to anyone.\n    ",
        "submission_date": "2017-05-16T00:00:00",
        "last_modified_date": "2017-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05633",
        "title": "Social Media-based Substance Use Prediction",
        "authors": [
            "Tao Ding",
            "Warren K. Bickel",
            "Shimei Pan"
        ],
        "abstract": "In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems. Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different feature learning methods to take advantage of a large amount of unsupervised social media data. We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as Facebook `\"likes\" and \"status updates\" to enhance system performance. Based on our evaluation, our best models achieved 86% AUC for predicting tobacco use, 81% for alcohol use and 84% for drug use, all of which significantly outperformed existing methods. Our investigation has also uncovered interesting relations between a user's social media behavior (e.g., word usage) and substance use.\n    ",
        "submission_date": "2017-05-16T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05940",
        "title": "Subregular Complexity and Deep Learning",
        "authors": [
            "Enes Avcu",
            "Chihiro Shibata",
            "Jeffrey Heinz"
        ],
        "abstract": "This paper argues that the judicial use of formal language theory and grammatical inference are invaluable tools in understanding how deep neural networks can and cannot represent and learn long-term dependencies in temporal sequences. Learning experiments were conducted with two types of Recurrent Neural Networks (RNNs) on six formal languages drawn from the Strictly Local (SL) and Strictly Piecewise (SP) classes. The networks were Simple RNNs (s-RNNs) and Long Short-Term Memory RNNs (LSTMs) of varying sizes. The SL and SP classes are among the simplest in a mathematically well-understood hierarchy of subregular classes. They encode local and long-term dependencies, respectively. The grammatical inference algorithm Regular Positive and Negative Inference (RPNI) provided a baseline. According to earlier research, the LSTM architecture should be capable of learning long-term dependencies and should outperform s-RNNs. The results of these experiments challenge this narrative. First, the LSTMs' performance was generally worse in the SP experiments than in the SL ones. Second, the s-RNNs out-performed the LSTMs on the most complex SP experiment and performed comparably to them on the others.\n    ",
        "submission_date": "2017-05-16T00:00:00",
        "last_modified_date": "2017-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05952",
        "title": "A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing",
        "authors": [
            "Dat Quoc Nguyen",
            "Mark Dras",
            "Mark Johnson"
        ],
        "abstract": "We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in a new state of the art. Our code is open-source and available together with pre-trained models at: ",
        "submission_date": "2017-05-16T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05992",
        "title": "Frame Stacking and Retaining for Recurrent Neural Network Acoustic Model",
        "authors": [
            "Xu Tian",
            "Jun Zhang",
            "Zejun Ma",
            "Yi He",
            "Juan Wei"
        ],
        "abstract": "Frame stacking is broadly applied in end-to-end neural network training like connectionist temporal classification (CTC), and it leads to more accurate models and faster decoding. However, it is not well-suited to conventional neural network based on context-dependent state acoustic model, if the decoder is unchanged. In this paper, we propose a novel frame retaining method which is applied in decoding. The system which combined frame retaining with frame stacking could reduces the time consumption of both training and decoding. Long short-term memory (LSTM) recurrent neural networks (RNNs) using it achieve almost linear training speedup and reduces relative 41\\% real time factor (RTF). At the same time, recognition performance is no degradation or improves sightly on Shenma voice search dataset in Mandarin.\n    ",
        "submission_date": "2017-05-17T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06031",
        "title": "Learning to Identify Ambiguous and Misleading News Headlines",
        "authors": [
            "Wei Wei",
            "Xiaojun Wan"
        ],
        "abstract": "Accuracy is one of the basic principles of journalism. However, it is increasingly hard to manage due to the diversity of news media. Some editors of online news tend to use catchy headlines which trick readers into clicking. These headlines are either ambiguous or misleading, degrading the reading experience of the audience. Thus, identifying inaccurate news headlines is a task worth studying. Previous work names these headlines \"clickbaits\" and mainly focus on the features extracted from the headlines, which limits the performance since the consistency between headlines and news bodies is underappreciated. In this paper, we clearly redefine the problem and identify ambiguous and misleading headlines separately. We utilize class sequential rules to exploit structure information when detecting ambiguous headlines. For the identification of misleading headlines, we extract features based on the congruence between headlines and bodies. To make use of the large unlabeled data set, we apply a co-training method and gain an increase in performance. The experiment results show the effectiveness of our methods. Then we use our classifiers to detect inaccurate headlines crawled from different sources and conduct a data analysis.\n    ",
        "submission_date": "2017-05-17T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06106",
        "title": "Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models",
        "authors": [
            "Katharina Kann",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We present a semi-supervised way of training a character-based encoder-decoder recurrent neural network for morphological reinflection, the task of generating one inflected word form from another. This is achieved by using unlabeled tokens or random strings as training data for an autoencoding task, adapting a network for morphological reinflection, and performing multi-task training. We thus use limited labeled data more effectively, obtaining up to 9.9% improvement over state-of-the-art baselines for 8 different languages.\n    ",
        "submission_date": "2017-05-17T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06262",
        "title": "Utility of General and Specific Word Embeddings for Classifying Translational Stages of Research",
        "authors": [
            "Vincent Major",
            "Alisa Surkis",
            "Yindalon Aphinyanaphongs"
        ],
        "abstract": "Conventional text classification models make a bag-of-words assumption reducing text into word occurrence counts per document. Recent algorithms such as word2vec are capable of learning semantic meaning and similarity between words in an entirely unsupervised manner using a contextual window and doing so much faster than previous methods. Each word is projected into vector space such that similar meaning words such as \"strong\" and \"powerful\" are projected into the same general Euclidean space. Open questions about these embeddings include their utility across classification tasks and the optimal properties and source of documents to construct broadly functional embeddings. In this work, we demonstrate the usefulness of pre-trained embeddings for classification in our task and demonstrate that custom word embeddings, built in the domain and for the tasks, can improve performance over word embeddings learnt on more general data including news articles or Wikipedia.\n    ",
        "submission_date": "2017-05-17T00:00:00",
        "last_modified_date": "2018-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06273",
        "title": "Transfer Learning for Named-Entity Recognition with Neural Networks",
        "authors": [
            "Ji Young Lee",
            "Franck Dernoncourt",
            "Peter Szolovits"
        ],
        "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for named-entity recognition (NER). In order to achieve high performances, ANNs need to be trained on a large labeled dataset. However, labels might be difficult to obtain for the dataset on which the user wants to perform NER: label scarcity is particularly pronounced for patient note de-identification, which is an instance of NER. In this work, we analyze to what extent transfer learning may address this issue. In particular, we demonstrate that transferring an ANN model trained on a large labeled dataset to another dataset with a limited number of labels improves upon the state-of-the-art results on two different datasets for patient note de-identification.\n    ",
        "submission_date": "2017-05-17T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06353",
        "title": "Political Footprints: Political Discourse Analysis using Pre-Trained Word Vectors",
        "authors": [
            "Christophe Bruchansky"
        ],
        "abstract": "In this paper, we discuss how machine learning could be used to produce a systematic and more objective political discourse analysis. Political footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word, and is produced by training the English lexicon on large text corpora. This paper presents a simple implementation of political footprints, some heuristics on how to use them, and their application to four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S. presidential elections. The reader will be offered a number of reasons to believe that political footprints produce meaningful results, along with some suggestions on how to improve their implementation.\n    ",
        "submission_date": "2017-05-17T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06369",
        "title": "Decoding Sentiment from Distributed Representations of Sentences",
        "authors": [
            "Edoardo Maria Ponti",
            "Ivan Vuli\u0107",
            "Anna Korhonen"
        ],
        "abstract": "Distributed representations of sentences have been developed recently to represent their meaning as real-valued vectors. However, it is not clear how much information such representations retain about the polarity of sentences. To study this question, we decode sentiment from unsupervised sentence representations learned with different architectures (sensitive to the order of words, the order of sentences, or none) in 9 typologically diverse languages. Sentiment results from the (recursive) composition of lexical items and grammatical strategies such as negation and concession. The results are manifold: we show that there is no `one-size-fits-all' representation architecture outperforming the others across the board. Rather, the top-ranking architectures depend on the language and data at hand. Moreover, we find that in several cases the additive composition model based on skip-gram word vectors may surpass supervised state-of-art architectures such as bidirectional LSTMs. Finally, we provide a possible explanation of the observed variation based on the type of negative constructions in each language.\n    ",
        "submission_date": "2017-05-17T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06457",
        "title": "Information Density as a Factor for Variation in the Embedding of Relative Clauses",
        "authors": [
            "Augustin Speyer",
            "Robin Lemke"
        ],
        "abstract": "In German, relative clauses can be positioned in-situ or extraposed. A potential factor for the variation might be information density. In this study, this hypothesis is tested with a corpus of 17th century German funeral sermons. For each referent in the relative clauses and their matrix clauses, the attention state was determined (first calculation). In a second calculation, for each word the surprisal values were determined, using a bi-gram language model. In a third calculation, the surprisal values were accommodated as to whether it is the first occurrence of the word in question or not. All three calculations pointed in the same direction: With in-situ relative clauses, the rate of new referents was lower and the average surprisal values were lower, especially the accommodated surprisal values, than with extraposed relative clauses. This indicated that in-formation density is a factor governing the choice between in-situ and extraposed relative clauses. The study also sheds light on the intrinsic relation-ship between the information theoretic concept of information density and in-formation structural concepts such as givenness which are used under a more linguistic perspective.\n    ",
        "submission_date": "2017-05-18T00:00:00",
        "last_modified_date": "2017-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06463",
        "title": "Universal Dependencies Parsing for Colloquial Singaporean English",
        "authors": [
            "Hongmin Wang",
            "Yue Zhang",
            "GuangYong Leonard Chan",
            "Jie Yang",
            "Hai Leong Chieu"
        ],
        "abstract": "Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.\n    ",
        "submission_date": "2017-05-18T00:00:00",
        "last_modified_date": "2017-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06476",
        "title": "ParlAI: A Dialog Research Software Platform",
        "authors": [
            "Alexander H. Miller",
            "Will Feng",
            "Adam Fisch",
            "Jiasen Lu",
            "Dhruv Batra",
            "Antoine Bordes",
            "Devi Parikh",
            "Jason Weston"
        ],
        "abstract": "We introduce ParlAI (pronounced \"par-lay\"), an open-source software platform for dialog research implemented in Python, available at ",
        "submission_date": "2017-05-18T00:00:00",
        "last_modified_date": "2018-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07008",
        "title": "A Lightweight Regression Method to Infer Psycholinguistic Properties for Brazilian Portuguese",
        "authors": [
            "Leandro B. dos Santos",
            "Magali S. Duran",
            "Nathan S. Hartmann",
            "Arnaldo Candido Jr.",
            "Gustavo H. Paetzold",
            "Sandra M. Aluisio"
        ],
        "abstract": "Psycholinguistic properties of words have been used in various approaches to Natural Language Processing tasks, such as text simplification and readability assessment. Most of these properties are subjective, involving costly and time-consuming surveys to be gathered. Recent approaches use the limited datasets of psycholinguistic properties to extend them automatically to large lexicons. However, some of the resources used by such approaches are not available to most languages. This study presents a method to infer psycholinguistic properties for Brazilian Portuguese (BP) using regressors built with a light set of features usually available for less resourced languages: word length, frequency lists, lexical databases composed of school dictionaries and word embedding models. The correlations between the properties inferred are close to those obtained by related works. The resulting resource contains 26,874 words in BP annotated with concreteness, age of acquisition, imageability and subjective frequency.\n    ",
        "submission_date": "2017-05-19T00:00:00",
        "last_modified_date": "2017-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07267",
        "title": "Search Engine Guided Non-Parametric Neural Machine Translation",
        "authors": [
            "Jiatao Gu",
            "Yong Wang",
            "Kyunghyun Cho",
            "Victor O.K. Li"
        ],
        "abstract": "In this paper, we extend an attention-based neural machine translation (NMT) model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage--retrieval stage--, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage--translation stage--, a novel translation model, called translation memory enhanced NMT (TM-NMT), seamlessly uses both the source sentence and a set of retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs (En-Fr, En-De, and En-Es) shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved.\n    ",
        "submission_date": "2017-05-20T00:00:00",
        "last_modified_date": "2018-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07318",
        "title": "Formalized Lambek Calculus in Higher Order Logic (HOL4)",
        "authors": [
            "Chun Tian"
        ],
        "abstract": "In this project, a rather complete proof-theoretical formalization of Lambek Calculus (non-associative with arbitrary extensions) has been ported from Coq proof assistent to HOL4 theorem prover, with some improvements and new theorems.\n",
        "submission_date": "2017-05-20T00:00:00",
        "last_modified_date": "2017-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07368",
        "title": "Mixed Membership Word Embeddings for Computational Social Science",
        "authors": [
            "James Foulds"
        ],
        "abstract": "Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. Despite their success in many applications, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. I show how to train the model using a combination of state-of-the-art training techniques for word embeddings and topic models. The experimental results show an improvement in predictive language modeling of up to 63% in MRR over the skip-gram, and demonstrate that the representations are beneficial for supervised learning. I illustrate the interpretability of the models with computational social science case studies on State of the Union addresses and NIPS articles.\n    ",
        "submission_date": "2017-05-20T00:00:00",
        "last_modified_date": "2018-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07371",
        "title": "Spelling Correction as a Foreign Language",
        "authors": [
            "Yingbo Zhou",
            "Utkarsh Porwal",
            "Roberto Konow"
        ],
        "abstract": "In this paper, we reformulated the spell correction problem as a machine translation task under the encoder-decoder framework. This reformulation enabled us to use a single model for solving the problem that is traditionally formulated as learning a language model and an error model. This model employs multi-layer recurrent neural networks as an encoder and a decoder. We demonstrate the effectiveness of this model using an internal dataset, where the training data is automatically obtained from user logs. The model offers competitive performance as compared to the state of the art methods but does not require any feature engineering nor hand tuning between models.\n    ",
        "submission_date": "2017-05-21T00:00:00",
        "last_modified_date": "2019-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07393",
        "title": "Recurrent Additive Networks",
        "authors": [
            "Kenton Lee",
            "Omer Levy",
            "Luke Zettlemoyer"
        ],
        "abstract": "We introduce recurrent additive networks (RANs), a new gated RNN which is distinguished by the use of purely additive latent state updates. At every time step, the new state is computed as a gated component-wise sum of the input and the previous state, without any of the non-linearities commonly used in RNN transition dynamics. We formally show that RAN states are weighted sums of the input vectors, and that the gates only contribute to computing the weights of these sums. Despite this relatively simple functional form, experiments demonstrate that RANs perform on par with LSTMs on benchmark language modeling problems. This result shows that many of the non-linear computations in LSTMs and related networks are not essential, at least for the problems we consider, and suggests that the gates are doing more of the computational work than previously understood.\n    ",
        "submission_date": "2017-05-21T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07425",
        "title": "Learning Semantic Relatedness From Human Feedback Using Metric Learning",
        "authors": [
            "Thomas Niebler",
            "Martin Becker",
            "Christian P\u00f6litz",
            "Andreas Hotho"
        ],
        "abstract": "Assessing the degree of semantic relatedness between words is an important task with a variety of semantic applications, such as ontology learning for the Semantic Web, semantic search or query expansion. To accomplish this in an automated fashion, many relatedness measures have been proposed. However, most of these metrics only encode information contained in the underlying corpus and thus do not directly model human intuition. To solve this, we propose to utilize a metric learning approach to improve existing semantic relatedness measures by learning from additional information, such as explicit human feedback. For this, we argue to use word embeddings instead of traditional high-dimensional vector representations in order to leverage their semantic density and to reduce computational cost. We rigorously test our approach on several domains including tagging data as well as publicly available embeddings based on Wikipedia texts and navigation. Human feedback about semantic relatedness for learning and evaluation is extracted from publicly available datasets such as MEN or WS-353. We find that our method can significantly improve semantic relatedness measures by learning from additional information, such as explicit human feedback. For tagging data, we are the first to generate and study embeddings. Our results are of special interest for ontology and recommendation engineers, but also for any other researchers and practitioners of Semantic Web techniques.\n    ",
        "submission_date": "2017-05-21T00:00:00",
        "last_modified_date": "2017-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07687",
        "title": "W2VLDA: Almost Unsupervised System for Aspect Based Sentiment Analysis",
        "authors": [
            "Aitor Garc\u00eda-Pablos",
            "Montse Cuadros",
            "German Rigau"
        ],
        "abstract": "With the increase of online customer opinions in specialised websites and social networks, the necessity of automatic systems to help to organise and classify customer reviews by domain-specific aspect/categories and sentiment polarity is more important than ever. Supervised approaches to Aspect Based Sentiment Analysis obtain good results for the domain/language their are trained on, but having manually labelled data for training supervised systems for all domains and languages are usually very costly and time consuming. In this work we describe W2VLDA, an almost unsupervised system based on topic modelling, that combined with some other unsupervised methods and a minimal configuration, performs aspect/category classifiation, aspect-terms/opinion-words separation and sentiment polarity classification for any given domain and language. We evaluate the performance of the aspect and sentiment classification in the multilingual SemEval 2016 task 5 (ABSA) dataset. We show competitive results for several languages (English, Spanish, French and Dutch) and domains (hotels, restaurants, electronic-devices).\n    ",
        "submission_date": "2017-05-22T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07830",
        "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning",
        "authors": [
            "Christian Buck",
            "Jannis Bulian",
            "Massimiliano Ciaramita",
            "Wojciech Gajewski",
            "Andrea Gesmundo",
            "Neil Houlsby",
            "Wei Wang"
        ],
        "abstract": "We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.\n    ",
        "submission_date": "2017-05-22T00:00:00",
        "last_modified_date": "2018-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08018",
        "title": "Use of Knowledge Graph in Rescoring the N-Best List in Automatic Speech Recognition",
        "authors": [
            "Ashwini Jaya Kumar",
            "Camilo Morales",
            "Maria-Esther Vidal",
            "Christoph Schmidt",
            "S\u00f6ren Auer"
        ],
        "abstract": "With the evolution of neural network based methods, automatic speech recognition (ASR) field has been advanced to a level where building an application with speech interface is a reality. In spite of these advances, building a real-time speech recogniser faces several problems such as low recognition accuracy, domain constraint, and out-of-vocabulary words. The low recognition accuracy problem is addressed by improving the acoustic model, language model, decoder and by rescoring the N-best list at the output of the decoder. We are considering the N-best list rescoring approach to improve the recognition accuracy. Most of the methods in the literature use the grammatical, lexical, syntactic and semantic connection between the words in a recognised sentence as a feature to rescore. In this paper, we have tried to see the semantic relatedness between the words in a sentence to rescore the N-best list. Semantic relatedness is computed using TransE~\\cite{bordes2013translating}, a method for low dimensional embedding of a triple in a knowledge graph. The novelty of the paper is the application of semantic web to automatic speech recognition.\n    ",
        "submission_date": "2017-05-22T00:00:00",
        "last_modified_date": "2017-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08038",
        "title": "Latent Human Traits in the Language of Social Media: An Open-Vocabulary Approach",
        "authors": [
            "Vivek Kulkarni",
            "Margaret L. Kern",
            "David Stillwell",
            "Michal Kosinski",
            "Sandra Matz",
            "Lyle Ungar",
            "Steven Skiena",
            "H. Andrew Schwartz"
        ],
        "abstract": "Over the past century, personality theory and research has successfully identified core sets of characteristics that consistently describe and explain fundamental differences in the way people think, feel and behave. Such characteristics were derived through theory, dictionary analyses, and survey research using explicit self-reports. The availability of social media data spanning millions of users now makes it possible to automatically derive characteristics from language use -- at large scale. Taking advantage of linguistic information available through Facebook, we study the process of inferring a new set of potential human traits based on unprompted language use. We subject these new traits to a comprehensive set of evaluations and compare them with a popular five factor model of personality. We find that our language-based trait construct is often more generalizable in that it often predicts non-questionnaire-based outcomes better than questionnaire-based traits (e.g. entities someone likes, income and intelligence quotient), while the factors remain nearly as stable as traditional factors. Our approach suggests a value in new constructs of personality derived from everyday human language use.\n    ",
        "submission_date": "2017-05-22T00:00:00",
        "last_modified_date": "2017-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08063",
        "title": "Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge",
        "authors": [
            "Arman Cohan",
            "Nazli Goharian"
        ],
        "abstract": "Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to reflect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the effectiveness of our model by significantly outperforming the state-of-the-art. We furthermore demonstrate how an effective contextualization method results in improving citation-based summarization of the scientific articles.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2017-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08091",
        "title": "Local Monotonic Attention Mechanism for End-to-End Speech and Language Processing",
        "authors": [
            "Andros Tjandra",
            "Sakriani Sakti",
            "Satoshi Nakamura"
        ],
        "abstract": "Recently, encoder-decoder neural networks have shown impressive performance on many sequence-related tasks. The architecture commonly uses an attentional mechanism which allows the model to learn alignments between the source and the target sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in several tasks, such as automatic speech recognition (ASR), grapheme-to-phoneme (G2P), etc. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results on ASR, G2P and machine translation between two languages with similar sentence structures, demonstrate that the proposed encoder-decoder model with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2017-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08386",
        "title": "Better Text Understanding Through Image-To-Text Transfer",
        "authors": [
            "Karol Kurach",
            "Sylvain Gelly",
            "Michal Jastrzebski",
            "Philip Haeusser",
            "Olivier Teytaud",
            "Damien Vincent",
            "Olivier Bousquet"
        ],
        "abstract": "Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08432",
        "title": "Question-Answering with Grammatically-Interpretable Representations",
        "authors": [
            "Hamid Palangi",
            "Paul Smolensky",
            "Xiaodong He",
            "Li Deng"
        ],
        "abstract": "We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In our application of TPRN, internal representations learned by end-to-end optimization in a deep neural network performing a textual question-answering (QA) task can be interpreted using basic concepts from linguistic theory. No performance penalty need be paid for this increased interpretability: the proposed model performs comparably to a state-of-the-art system on the SQuAD QA task. The internal representation which is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Fine-grained analysis reveals specific correspondences between the learned roles and parts of speech as assigned by a standard tagger (Toutanova et al. 2003), and finds several discrepancies in the model's favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means to build representations using symbols and roles, with an inductive bias favoring use of these in an approximately discrete manner.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2017-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08488",
        "title": "Second-Order Word Embeddings from Nearest Neighbor Topological Features",
        "authors": [
            "Denis Newman-Griffis",
            "Eric Fosler-Lussier"
        ],
        "abstract": "We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2017-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08828",
        "title": "Deep Investigation of Cross-Language Plagiarism Detection Methods",
        "authors": [
            "Jeremy Ferrero",
            "Laurent Besacier",
            "Didier Schwab",
            "Frederic Agnes"
        ],
        "abstract": "This paper is a deep investigation of cross-language plagiarism detection methods on a new recently introduced open dataset, which contains parallel and comparable collections of documents with multiple characteristics (different genres, languages and sizes of texts). We investigate cross-language plagiarism detection methods for 6 language pairs on 2 granularities of text units in order to draw robust conclusions on the best methods while deeply analyzing correlations across document styles and languages.\n    ",
        "submission_date": "2017-05-24T00:00:00",
        "last_modified_date": "2017-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08843",
        "title": "Parsing with CYK over Distributed Representations",
        "authors": [
            "Fabio Massimo Zanzotto",
            "Giordano Cristini",
            "Giorgio Satta"
        ],
        "abstract": "Syntactic parsing is a key task in natural language processing. This task has been dominated by symbolic, grammar-based parsers. Neural networks, with their distributed representations, are challenging these methods. In this article we show that existing symbolic parsing algorithms can cross the border and be entirely formulated over distributed representations. To this end we introduce a version of the traditional Cocke-Younger-Kasami (CYK) algorithm, called D-CYK, which is entirely defined over distributed representations. Our D-CYK uses matrix multiplication on real number matrices of size independent of the length of the input string. These operations are compatible with traditional neural networks. Experiments show that our D-CYK approximates the original CYK algorithm. By showing that CYK can be entirely performed on distributed representations, we open the way to the definition of recurrent layers of CYK-informed neural networks.\n    ",
        "submission_date": "2017-05-24T00:00:00",
        "last_modified_date": "2019-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08942",
        "title": "Joint PoS Tagging and Stemming for Agglutinative Languages",
        "authors": [
            "Necva B\u00f6l\u00fcc\u00fc",
            "Burcu Can"
        ],
        "abstract": "The number of word forms in agglutinative languages is theoretically infinite and this variety in word forms introduces sparsity in many natural language processing tasks. Part-of-speech tagging (PoS tagging) is one of these tasks that often suffers from sparsity. In this paper, we present an unsupervised Bayesian model using Hidden Markov Models (HMMs) for joint PoS tagging and stemming for agglutinative languages. We use stemming to reduce sparsity in PoS tagging. Two tasks are jointly performed to provide a mutual benefit in both tasks. Our results show that joint POS tagging and stemming improves PoS tagging scores. We present results for Turkish and Finnish as agglutinative languages and English as a morphologically poor language.\n    ",
        "submission_date": "2017-05-24T00:00:00",
        "last_modified_date": "2017-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08947",
        "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech",
        "authors": [
            "Sercan Arik",
            "Gregory Diamos",
            "Andrew Gibiansky",
            "John Miller",
            "Kainan Peng",
            "Wei Ping",
            "Jonathan Raiman",
            "Yanqi Zhou"
        ],
        "abstract": "We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.\n    ",
        "submission_date": "2017-05-24T00:00:00",
        "last_modified_date": "2017-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09054",
        "title": "Max-Cosine Matching Based Neural Models for Recognizing Textual Entailment",
        "authors": [
            "Zhipeng Xie",
            "Junfeng Hu"
        ],
        "abstract": "Recognizing textual entailment is a fundamental task in a variety of text mining or natural language processing applications. This paper proposes a simple neural model for RTE problem. It first matches each word in the hypothesis with its most-similar word in the premise, producing an augmented representation of the hypothesis conditioned on the premise as a sequence of word pairs. The LSTM model is then used to model this augmented sequence, and the final output from the LSTM is fed into a softmax layer to make the prediction. Besides the base model, in order to enhance its performance, we also proposed three techniques: the integration of multiple word-embedding library, bi-way integration, and ensemble based on model averaging. Experimental results on the SNLI dataset have shown that the three techniques are effective in boosting the predicative accuracy and that our method outperforms several state-of-the-state ones.\n    ",
        "submission_date": "2017-05-25T00:00:00",
        "last_modified_date": "2017-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09189",
        "title": "Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs",
        "authors": [
            "Jean Maillard",
            "Stephen Clark",
            "Dani Yogatama"
        ],
        "abstract": "We introduce a neural network that represents sentences by composing their words according to induced binary parse trees. We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser. Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees. As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation. We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task.\n    ",
        "submission_date": "2017-05-25T00:00:00",
        "last_modified_date": "2017-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09207",
        "title": "Learning Structured Text Representations",
        "authors": [
            "Yang Liu",
            "Mirella Lapata"
        ],
        "abstract": "In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.\n    ",
        "submission_date": "2017-05-25T00:00:00",
        "last_modified_date": "2018-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09515",
        "title": "ASR error management for improving spoken language understanding",
        "authors": [
            "Edwin Simonnet",
            "Sahar Ghannay",
            "Nathalie Camelin",
            "Yannick Est\u00e8ve",
            "Renato De Mori"
        ],
        "abstract": "This paper addresses the problem of automatic speech recognition (ASR) error detection and their use for improving spoken language understanding (SLU) systems. In this study, the SLU task consists in automatically extracting, from ASR transcriptions , semantic concepts and concept/values pairs in a e.g touristic information system. An approach is proposed for enriching the set of semantic labels with error specific labels and by using a recently proposed neural approach based on word embeddings to compute well calibrated ASR confidence measures. Experimental results are reported showing that it is possible to decrease significantly the Concept/Value Error Rate with a state of the art system, outperforming previously published results performance on the same experimental data. It also shown that combining an SLU approach based on conditional random fields with a neural encoder/decoder attention based architecture , it is possible to effectively identifying confidence islands and uncertain semantic output segments useful for deciding appropriate error handling actions by the dialogue manager strategy .\n    ",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09516",
        "title": "Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models",
        "authors": [
            "Patchigolla V S S Rahul",
            "Sunil Kumar Sahu",
            "Ashish Anand"
        ],
        "abstract": "Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex hand-crafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network (RNN) to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task.\n    ",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09585",
        "title": "Detecting and Explaining Crisis",
        "authors": [
            "Rohan Kshirsagar",
            "Robert Morris",
            "Sam Bowman"
        ],
        "abstract": "Individuals on social media may reveal themselves to be in various states of crisis (e.g. suicide, self-harm, abuse, or eating disorders). Detecting crisis from social media text automatically and accurately can have profound consequences. However, detecting a general state of crisis without explaining why has limited applications. An explanation in this context is a coherent, concise subset of the text that rationalizes the crisis detection. We explore several methods to detect and explain crisis using a combination of neural and non-neural techniques. We evaluate these techniques on a unique data set obtained from Koko, an anonymous emotional support network available through various messaging applications. We annotate a small subset of the samples labeled with crisis with corresponding explanations. Our best technique significantly outperforms the baseline for detection and explanation.\n    ",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09655",
        "title": "Style Transfer from Non-Parallel Text by Cross-Alignment",
        "authors": [
            "Tianxiao Shen",
            "Tao Lei",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ],
        "abstract": "This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.\n    ",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09656",
        "title": "Helping News Editors Write Better Headlines: A Recommender to Improve the Keyword Contents & Shareability of News Headlines",
        "authors": [
            "Terrence Szymanski",
            "Claudia Orellana-Rodriguez",
            "Mark T. Keane"
        ],
        "abstract": "We present a software tool that employs state-of-the-art natural language processing (NLP) and machine learning techniques to help newspaper editors compose effective headlines for online publication. The system identifies the most salient keywords in a news article and ranks them based on both their overall popularity and their direct relevance to the article. The system also uses a supervised regression model to identify headlines that are likely to be widely shared on social media. The user interface is designed to simplify and speed the editor's decision process on the composition of the headline. As such, the tool provides an efficient way to combine the benefits of automated predictors of engagement and search-engine optimization (SEO) with human judgments of overall headline quality.\n    ",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09724",
        "title": "Semi-Supervised Model Training for Unbounded Conversational Speech Recognition",
        "authors": [
            "Shane Walker",
            "Morten Pedersen",
            "Iroro Orife",
            "Jason Flaks"
        ],
        "abstract": "For conversational large-vocabulary continuous speech recognition (LVCSR) tasks, up to about two thousand hours of audio is commonly used to train state of the art models. Collection of labeled conversational audio however, is prohibitively expensive, laborious and error-prone. Furthermore, academic corpora like Fisher English (2004) or Switchboard (1992) are inadequate to train models with sufficient accuracy in the unbounded space of conversational speech. These corpora are also timeworn due to dated acoustic telephony features and the rapid advancement of colloquial vocabulary and idiomatic speech over the last decades. Utilizing the colossal scale of our unlabeled telephony dataset, we propose a technique to construct a modern, high quality conversational speech training corpus on the order of hundreds of millions of utterances (or tens of thousands of hours) for both acoustic and language model training. We describe the data collection, selection and training, evaluating the results of our updated speech recognition system on a test corpus of 7K manually transcribed utterances. We show relative word error rate (WER) reductions of {35%, 19%} on {agent, caller} utterances over our seed model and 5% absolute WER improvements over IBM Watson STT on this conversational speech task.\n    ",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09755",
        "title": "word2vec Skip-Gram with Negative Sampling is a Weighted Logistic PCA",
        "authors": [
            "Andrew J. Landgraf",
            "Jeremy Bellay"
        ],
        "abstract": "We show that the skip-gram formulation of word2vec trained with negative sampling is equivalent to a weighted logistic PCA. This connection allows us to better understand the objective, compare it to other word embedding methods, and extend it to higher dimensional models.\n    ",
        "submission_date": "2017-05-27T00:00:00",
        "last_modified_date": "2017-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09837",
        "title": "On the relation between dependency distance, crossing dependencies, and parsing. Comment on \"Dependency distance: a new perspective on syntactic patterns in natural languages\" by Haitao Liu et al",
        "authors": [
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "abstract": "Liu et al. (2017) provide a comprehensive account of research on dependency distance in human languages. While the article is a very rich and useful report on this complex subject, here I will expand on a few specific issues where research in computational linguistics (specifically natural language processing) can inform DDM research, and vice versa. These aspects have not been explored much in the article by Liu et al. or elsewhere, probably due to the little overlap between both research communities, but they may provide interesting insights for improving our understanding of the evolution of human languages, the mechanisms by which the brain processes and understands language, and the construction of effective computer systems to achieve this goal.\n    ",
        "submission_date": "2017-05-27T00:00:00",
        "last_modified_date": "2017-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09899",
        "title": "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
        "authors": [
            "Zeerak Waseem",
            "Thomas Davidson",
            "Dana Warmsley",
            "Ingmar Weber"
        ],
        "abstract": "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and we discuss its implications for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.\n    ",
        "submission_date": "2017-05-28T00:00:00",
        "last_modified_date": "2017-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09906",
        "title": "Listen, Interact and Talk: Learning to Speak via Interaction",
        "authors": [
            "Haichao Zhang",
            "Haonan Yu",
            "Wei Xu"
        ],
        "abstract": "One of the long-term goals of artificial intelligence is to build an agent that can communicate intelligently with human in natural language. Most existing work on natural language learning relies heavily on training over a pre-collected dataset with annotated labels, leading to an agent that essentially captures the statistics of the fixed external training data. As the training data is essentially a static snapshot representation of the knowledge from the annotator, the agent trained this way is limited in adaptiveness and generalization of its behavior. Moreover, this is very different from the language learning process of humans, where language is acquired during communication by taking speaking action and learning from the consequences of speaking action in an interactive manner. This paper presents an interactive setting for grounded natural language learning, where an agent learns natural language by interacting with a teacher and learning from feedback, thus learning and improving language skills while taking part in the conversation. To achieve this goal, we propose a model which incorporates both imitation and reinforcement by leveraging jointly sentence and reward feedbacks from the teacher. Experiments are conducted to validate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2017-05-28T00:00:00",
        "last_modified_date": "2017-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09932",
        "title": "The placement of the head that maximizes predictability. An information theoretic approach",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "The minimization of the length of syntactic dependencies is a well-established principle of word order and the basis of a mathematical theory of word order. Here we complete that theory from the perspective of information theory, adding a competing word order principle: the maximization of predictability of a target element. These two principles are in conflict: to maximize the predictability of the head, the head should appear last, which maximizes the costs with respect to dependency length minimization. The implications of such a broad theoretical framework to understand the optimality, diversity and evolution of the six possible orderings of subject, object and verb are reviewed.\n    ",
        "submission_date": "2017-05-28T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09980",
        "title": "Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations",
        "authors": [
            "Rik van Noord",
            "Johan Bos"
        ],
        "abstract": "We evaluate the character-level translation method for neural semantic parsing on a large corpus of sentences annotated with Abstract Meaning Representations (AMRs). Using a sequence-to-sequence model, and some trivial preprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1 (F-score on AMR-triples). We examine five different approaches to improve this baseline result: (i) reordering AMR branches to match the word order of the input sentence increases performance to 58.3; (ii) adding part-of-speech tags (automatically produced) to the input shows improvement as well (57.2); (iii) So does the introduction of super characters (conflating frequent sequences of characters to a single character), reaching 57.4; (iv) optimizing the training process by using pre-training and averaging a set of models increases performance to 58.7; (v) adding silver-standard training data obtained by an off-the-shelf parser yields the biggest improvement, resulting in an F-score of 64.0. Combining all five techniques leads to an F-score of 71.0 on holdout data, which is state-of-the-art in AMR parsing. This is remarkable because of the relative simplicity of the approach.\n    ",
        "submission_date": "2017-05-28T00:00:00",
        "last_modified_date": "2017-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09993",
        "title": "Deep Learning for User Comment Moderation",
        "authors": [
            "John Pavlopoulos",
            "Prodromos Malakasiotis",
            "Ion Androutsopoulos"
        ],
        "abstract": "Experimenting with a new dataset of 1.6M user comments from a Greek news portal and existing datasets of English Wikipedia comments, we show that an RNN outperforms the previous state of the art in moderation. A deep, classification-specific attention mechanism improves further the overall performance of the RNN. We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.\n    ",
        "submission_date": "2017-05-28T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09995",
        "title": "Subject Specific Stream Classification Preprocessing Algorithm for Twitter Data Stream",
        "authors": [
            "Nisansa de Silva",
            "Danaja Maldeniya",
            "Chamilka Wijeratne"
        ],
        "abstract": "Micro-blogging service Twitter is a lucrative source for data mining applications on global sentiment. But due to the omnifariousness of the subjects mentioned in each data item; it is inefficient to run a data mining algorithm on the raw data. This paper discusses an algorithm to accurately classify the entire stream in to a given number of mutually exclusive collectively exhaustive streams upon each of which the data mining algorithm can be run separately yielding more relevant results with a high efficiency.\n    ",
        "submission_date": "2017-05-28T00:00:00",
        "last_modified_date": "2017-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10030",
        "title": "Supervised Complementary Entity Recognition with Augmented Key-value Pairs of Knowledge",
        "authors": [
            "Hu Xu",
            "Lei Shu",
            "Philip S. Yu"
        ],
        "abstract": "Extracting opinion targets is an important task in sentiment analysis on product reviews and complementary entities (products) are one important type of opinion targets that may work together with the reviewed product. In this paper, we address the problem of Complementary Entity Recognition (CER) as a supervised sequence labeling with the capability of expanding domain knowledge as key-value pairs from unlabeled reviews, by automatically learning and enhancing knowledge-based features. We use Conditional Random Field (CRF) as the base learner and augment CRF with knowledge-based features (called the Knowledge-based CRF or KCRF for short). We conduct experiments to show that KCRF effectively improves the performance of supervised CER task.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10112",
        "title": "Dynamics of core of language vocabulary",
        "authors": [
            "Valery D. Solovyev",
            "Vladimir V. Bochkarev",
            "Anna V. Shevlyakova"
        ],
        "abstract": "Studies of the overall structure of vocabulary and its dynamics became possible due to creation of diachronic text corpora, especially Google Books Ngram. This article discusses the question of core change rate and the degree to which the core words cover the texts. Different periods of the last three centuries and six main European languages presented in Google Books Ngram are compared. The main result is high stability of core change rate, which is analogous to stability of the Swadesh list.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10130",
        "title": "An Automatic Contextual Analysis and Clustering Classifiers Ensemble approach to Sentiment Analysis",
        "authors": [
            "Murtadha Talib AL-Sharuee",
            "Fei Liu",
            "Mahardhika Pratama"
        ],
        "abstract": "Products reviews are one of the major resources to determine the public sentiment. The existing literature on reviews sentiment analysis mainly utilizes supervised paradigm, which needs labeled data to be trained on and suffers from domain-dependency. This article addresses these issues by describes a completely automatic approach for sentiment analysis based on unsupervised ensemble learning. The method consists of two phases. The first phase is contextual analysis, which has five processes, namely (1) data preparation; (2) spelling correction; (3) intensifier handling; (4) negation handling and (5) contrast handling. The second phase comprises the unsupervised learning approach, which is an ensemble of clustering classifiers using a majority voting mechanism with different weight schemes. The base classifier of the ensemble method is a modified k-means algorithm. The base classifier is modified by extracting initial centroids from the feature set via using SentWordNet (SWN). We also introduce new sentiment analysis problems of Australian airlines and home builders which offer potential benchmark problems in the sentiment analysis field. Our experiments on datasets from different domains show that contextual analysis and the ensemble phases improve the clustering performance in term of accuracy, stability and generalization ability.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10209",
        "title": "On Multilingual Training of Neural Dependency Parsers",
        "authors": [
            "Micha\u0142 Zapotoczny",
            "Pawe\u0142 Rychlikowski",
            "Jan Chorowski"
        ],
        "abstract": "We show that a recently proposed neural dependency parser can be improved by joint training on multiple languages from the same family. The parser is implemented as a deep neural network whose only input is orthographic representations of words. In order to successfully parse, the network has to discover how linguistically relevant concepts can be inferred from word spellings. We analyze the representations of characters and words that are learned by the network to establish which properties of languages were accounted for. In particular we show that the parser has approximately learned to associate Latin characters with their Cyrillic counterparts and that it can group Polish and Russian words that have a similar grammatical function. Finally, we evaluate the parser on selected languages from the Universal Dependencies dataset and show that it is competitive with other recently proposed state-of-the art methods, while having a simple structure.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10229",
        "title": "Latent Intention Dialogue Models",
        "authors": [
            "Tsung-Hsien Wen",
            "Yishu Miao",
            "Phil Blunsom",
            "Steve Young"
        ],
        "abstract": "Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10272",
        "title": "Who's to say what's funny? A computer using Language Models and Deep Learning, That's Who!",
        "authors": [
            "Xinru Yan",
            "Ted Pedersen"
        ],
        "abstract": "Humor is a defining characteristic of human beings. Our goal is to develop methods that automatically detect humorous statements and rank them on a continuous scale. In this paper we report on results using a Language Model approach, and outline our plans for using methods from Deep Learning.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10415",
        "title": "On the \"Calligraphy\" of Books",
        "authors": [
            "Vanessa Q. Marinho",
            "Henrique F. de Arruda",
            "Thales S. Lima",
            "Luciano F. Costa",
            "Diego R. Amancio"
        ],
        "abstract": "Authorship attribution is a natural language processing task that has been widely studied, often by considering small order statistics. In this paper, we explore a complex network approach to assign the authorship of texts based on their mesoscopic representation, in an attempt to capture the flow of the narrative. Indeed, as reported in this work, such an approach allowed the identification of the dominant narrative structure of the studied authors. This has been achieved due to the ability of the mesoscopic approach to take into account relationships between different, not necessarily adjacent, parts of the text, which is able to capture the story flow. The potential of the proposed approach has been illustrated through principal component analysis, a comparison with the chance baseline method, and network visualization. Such visualizations reveal individual characteristics of the authors, which can be understood as a kind of calligraphy.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10586",
        "title": "Character-Based Text Classification using Top Down Semantic Model for Sentence Representation",
        "authors": [
            "Zhenzhou Wu",
            "Xin Zheng",
            "Daniel Dahlmeier"
        ],
        "abstract": "Despite the success of deep learning on many fronts especially image and speech, its application in text classification often is still not as good as a simple linear SVM on n-gram TF-IDF representation especially for smaller datasets. Deep learning tends to emphasize on sentence level semantics when learning a representation with models like recurrent neural network or recursive neural network, however from the success of TF-IDF representation, it seems a bag-of-words type of representation has its strength. Taking advantage of both representions, we present a model known as TDSM (Top Down Semantic Model) for extracting a sentence representation that considers both the word-level semantics by linearly combining the words with attention weights and the sentence-level semantics with BiLSTM and use it on text classification. We apply the model on characters and our results show that our model is better than all the other character-based and word-based convolutional neural network models by \\cite{zhang15} across seven different datasets with only 1\\% of their parameters. We also demonstrate that this model beats traditional linear models on TF-IDF vectors on small and polished datasets like news article in which typically deep learning models surrender.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10610",
        "title": "The Importance of Automatic Syntactic Features in Vietnamese Named Entity Recognition",
        "authors": [
            "Thai-Hoang Pham",
            "Phuong Le-Hong"
        ],
        "abstract": "This paper presents a state-of-the-art system for Vietnamese Named Entity Recognition (NER). By incorporating automatic syntactic features with word embeddings as input for bidirectional Long Short-Term Memory (Bi-LSTM), our system, although simpler than some deep learning architectures, achieves a much better result for Vietnamese NER. The proposed method achieves an overall F1 score of 92.05% on the test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community. Our named entity recognition system outperforms the best previous systems for Vietnamese NER by a large margin.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2017-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10754",
        "title": "A Low Dimensionality Representation for Language Variety Identification",
        "authors": [
            "Francisco Rangel",
            "Marc Franco-Salvador",
            "Paolo Rosso"
        ],
        "abstract": "Language variety identification aims at labelling texts in a native language (e.g. Spanish, Portuguese, English) with its specific variation (e.g. Argentina, Chile, Mexico, Peru, Spain; Brazil, Portugal; UK, US). In this work we propose a low dimensionality representation (LDR) to address this task with five different varieties of Spanish: Argentina, Chile, Mexico, Peru and Spain. We compare our LDR method with common state-of-the-art representations and show an increase in accuracy of ~35%. Furthermore, we compare LDR with two reference distributed representation models. Experimental results show competitive performance while dramatically reducing the dimensionality --and increasing the big data suitability-- to only 6 features per variety. Additionally, we analyse the behaviour of the employed machine learning algorithms and the most discriminating features. Finally, we employ an alternative dataset to test the robustness of our low dimensionality representation with another set of similar languages.\n    ",
        "submission_date": "2017-05-30T00:00:00",
        "last_modified_date": "2017-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10814",
        "title": "Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages",
        "authors": [
            "Xiang Yu",
            "Ngoc Thang Vu"
        ],
        "abstract": "We present a transition-based dependency parser that uses a convolutional neural network to compose word representations from characters. The character composition model shows great improvement over the word-lookup model, especially for parsing agglutinative languages. These improvements are even better than using pre-trained word embeddings from extra data. On the SPMRL data sets, our system outperforms the previous best greedy parser (Ballesteros et al., 2015) by a margin of 3% on average.\n    ",
        "submission_date": "2017-05-30T00:00:00",
        "last_modified_date": "2017-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10900",
        "title": "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology Based Representations",
        "authors": [
            "Paul Michel",
            "Abhilasha Ravichander",
            "Shruti Rijhwani"
        ],
        "abstract": "We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like $\\textit{tf-idf}$, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.\n    ",
        "submission_date": "2017-05-31T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10929",
        "title": "Adversarial Generation of Natural Language",
        "authors": [
            "Sai Rajeswar",
            "Sandeep Subramanian",
            "Francis Dutil",
            "Christopher Pal",
            "Aaron Courville"
        ],
        "abstract": "Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating natural language with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.\n    ",
        "submission_date": "2017-05-31T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10962",
        "title": "Analysis of the Effect of Dependency Information on Predicate-Argument Structure Analysis and Zero Anaphora Resolution",
        "authors": [
            "Koichiro Yoshino",
            "Shinsuke Mori",
            "Satoshi Nakamura"
        ],
        "abstract": "This paper investigates and analyzes the effect of dependency information on predicate-argument structure analysis (PASA) and zero anaphora resolution (ZAR) for Japanese, and shows that a straightforward approach of PASA and ZAR works effectively even if dependency information was not available. We constructed an analyzer that directly predicts relationships of predicates and arguments with their semantic roles from a POS-tagged corpus. The features of the system are designed to compensate for the absence of syntactic information by using features used in dependency parsing as a reference. We also constructed analyzers that use the oracle dependency and the real dependency parsing results, and compared with the system that does not use any syntactic information to verify that the improvement provided by dependencies is not crucial.\n    ",
        "submission_date": "2017-05-31T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.11001",
        "title": "Adversarial Ranking for Language Generation",
        "authors": [
            "Kevin Lin",
            "Dianqi Li",
            "Xiaodong He",
            "Zhengyou Zhang",
            "Ming-Ting Sun"
        ],
        "abstract": "Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than training the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2017-05-31T00:00:00",
        "last_modified_date": "2018-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.11160",
        "title": "Learning When to Attend for Neural Machine Translation",
        "authors": [
            "Junhui Li",
            "Muhua Zhu"
        ],
        "abstract": "In the past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. However, previous attention models always refer to some source words when predicting a target word, which contradicts with the fact that some target words have no corresponding source words. Motivated by this observation, we propose a novel attention model that has the capability of determining when a decoder should attend to source words and when it should not. Experimental results on NIST Chinese-English translation tasks show that the new model achieves an improvement of 0.8 BLEU score over a state-of-the-art baseline.\n    ",
        "submission_date": "2017-05-31T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.11168",
        "title": "Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning",
        "authors": [
            "Li Lucy",
            "Jon Gauthier"
        ],
        "abstract": "Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.\n    ",
        "submission_date": "2017-05-31T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00130",
        "title": "Teaching Machines to Describe Images via Natural Language Feedback",
        "authors": [
            "Huan Ling",
            "Sanja Fidler"
        ],
        "abstract": "Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00134",
        "title": "Semantic Refinement GRU-based Neural Language Generation for Spoken Dialogue Systems",
        "authors": [
            "Van-Khanh Tran",
            "Le-Minh Nguyen"
        ],
        "abstract": "Natural language generation (NLG) plays a critical role in spoken dialogue systems. This paper presents a new approach to NLG by using recurrent neural networks (RNN), in which a gating mechanism is applied before RNN computation. This allows the proposed model to generate appropriate sentences. The RNN-based generator can be learned from unaligned data by jointly training sentence planning and surface realization to produce natural language responses. The model was extensively evaluated on four different NLG domains. The results show that the proposed generator achieved better performance on all the NLG domains compared to previous generators.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00139",
        "title": "Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks",
        "authors": [
            "Van-Khanh Tran",
            "Le-Minh Nguyen"
        ],
        "abstract": "Natural language generation (NLG) is a critical component in a spoken dialogue system. This paper presents a Recurrent Neural Network based Encoder-Decoder architecture, in which an LSTM-based decoder is introduced to select, aggregate semantic elements produced by an attention mechanism over the input elements, and to produce the required utterances. The proposed generator can be jointly trained both sentence planning and surface realization to produce natural language sentences. The proposed model was extensively evaluated on four different NLG datasets. The experimental results showed that the proposed generators not only consistently outperform the previous methods across all the NLG domains but also show an ability to generalize from a new, unseen domain and learn from multi-domain datasets.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00188",
        "title": "Deep Learning for Hate Speech Detection in Tweets",
        "authors": [
            "Pinkesh Badjatiya",
            "Shashank Gupta",
            "Manish Gupta",
            "Vasudeva Varma"
        ],
        "abstract": "Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by ~18 F1 points.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00245",
        "title": "Polish Read Speech Corpus for Speech Tools and Services",
        "authors": [
            "Danijel Kor\u017einek",
            "Krzysztof Marasek",
            "\u0141ukasz Brocki",
            "Krzysztof Wo\u0142k"
        ],
        "abstract": "This paper describes the speech processing activities conducted at the Polish consortium of the CLARIN project. The purpose of this segment of the project was to develop specific tools that would allow for automatic and semi-automatic processing of large quantities of acoustic speech data. The tools include the following: grapheme-to-phoneme conversion, speech-to-text alignment, voice activity detection, speaker diarization, keyword spotting and automatic speech transcription. Furthermore, in order to develop these tools, a large high-quality studio speech corpus was recorded and released under an open license, to encourage development in the area of Polish speech research. Another purpose of the corpus was to serve as a reference for studies in phonetics and pronunciation. All the tools and resources were released on the the Polish CLARIN website. This paper discusses the current status and future plans for the project.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00321",
        "title": "Using of heterogeneous corpora for training of an ASR system",
        "authors": [
            "Jan Trmal",
            "Gaurav Kumar",
            "Vimal Manohar",
            "Sanjeev Khudanpur",
            "Matt Post",
            "Paul McNamee"
        ],
        "abstract": "The paper summarizes the development of the LVCSR system built as a part of the Pashto speech-translation system at the SCALE (Summer Camp for Applied Language Exploration) 2015 workshop on \"Speech-to-text-translation for low-resource languages\". The Pashto language was chosen as a good \"proxy\" low-resource language, exhibiting multiple phenomena which make the speech-recognition and and speech-to-text-translation systems development hard.\n",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00359",
        "title": "Discovering Discrete Latent Topics with Neural Variational Inference",
        "authors": [
            "Yishu Miao",
            "Edward Grefenstette",
            "Phil Blunsom"
        ],
        "abstract": "Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2018-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00374",
        "title": "Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints",
        "authors": [
            "Nikola Mrk\u0161i\u0107",
            "Ivan Vuli\u0107",
            "Diarmuid \u00d3 S\u00e9aghdha",
            "Ira Leviant",
            "Roi Reichart",
            "Milica Ga\u0161i\u0107",
            "Anna Korhonen",
            "Steve Young"
        ],
        "abstract": "We present Attract-Repel, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Attract-Repel facilitates the use of constraints from mono- and cross-lingual resources, yielding semantically specialised cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct high-quality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource ones. The effectiveness of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in six languages. We next show that Attract-Repel-specialised vectors boost performance in the downstream task of dialogue state tracking (DST) across multiple languages. Finally, we show that cross-lingual vector spaces produced by our algorithm facilitate the training of multilingual DST models, which brings further performance improvements.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00377",
        "title": "Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules",
        "authors": [
            "Ivan Vuli\u0107",
            "Nikola Mrk\u0161i\u0107",
            "Roi Reichart",
            "Diarmuid \u00d3 S\u00e9aghdha",
            "Steve Young",
            "Anna Korhonen"
        ],
        "abstract": "Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for low-frequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that 'inexpensive' is a rephrasing for 'expensive' or may not associate 'acquire' with 'acquires'. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00457",
        "title": "NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems",
        "authors": [
            "Ozan Caglayan",
            "Mercedes Garc\u00eda-Mart\u00ednez",
            "Adrien Bardet",
            "Walid Aransa",
            "Fethi Bougares",
            "Lo\u00efc Barrault"
        ],
        "abstract": "In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUM's top-ranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00465",
        "title": "Machine Assisted Analysis of Vowel Length Contrasts in Wolof",
        "authors": [
            "Elodie Gauthier",
            "Laurent Besacier",
            "Sylvie Voisin"
        ],
        "abstract": "Growing digital archives and improving algorithms for automatic analysis of text and speech create new research opportunities for fundamental research in phonetics. Such empirical approaches allow statistical evaluation of a much larger set of hypothesis about phonetic variation and its conditioning factors (among them geographical / dialectal variants). This paper illustrates this vision and proposes to challenge automatic methods for the analysis of a not easily observable phenomenon: vowel length contrast. We focus on Wolof, an under-resourced language from Sub-Saharan Africa. In particular, we propose multiple features to make a fine evaluation of the degree of length contrast under different factors such as: read vs semi spontaneous speech ; standard vs dialectal Wolof. Our measures made fully automatically on more than 20k vowel tokens show that our proposed features can highlight different degrees of contrast for each vowel considered. We notably show that contrast is weaker in semi-spontaneous speech and in a non standard semi-spontaneous dialect.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00468",
        "title": "Function Assistant: A Tool for NL Querying of APIs",
        "authors": [
            "Kyle Richardson",
            "Jonas Kuhn"
        ],
        "abstract": "In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries and descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of Richardson and Kuhn (2017). Translations are automatically learned from example text-code pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00506",
        "title": "Morphological Embeddings for Named Entity Recognition in Morphologically Rich Languages",
        "authors": [
            "Onur Gungor",
            "Eray Yildiz",
            "Suzan Uskudarli",
            "Tunga Gungor"
        ],
        "abstract": "In this work, we present new state-of-the-art results of 93.59,% and 79.59,% for Turkish and Czech named entity recognition based on the model of (Lample et al., 2016). We contribute by proposing several schemes for representing the morphological analysis of a word in the context of named entity recognition. We show that a concatenation of this representation with the word and character embeddings improves the performance. The effect of these representation schemes on the tagging performance is also investigated.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00593",
        "title": "Joint Modeling of Topics, Citations, and Topical Authority in Academic Corpora",
        "authors": [
            "Jooyeon Kim",
            "Dongwoo Kim",
            "Alice Oh"
        ],
        "abstract": "Much of scientific progress stems from previously published findings, but searching through the vast sea of scientific publications is difficult. We often rely on metrics of scholarly authority to find the prominent authors but these authority indices do not differentiate authority based on research topics. We present Latent Topical-Authority Indexing (LTAI) for jointly modeling the topics, citations, and topical authority in a corpus of academic papers. Compared to previous models, LTAI differs in two main aspects. First, it explicitly models the generative process of the citations, rather than treating the citations as given. Second, it models each author's influence on citations of a paper based on the topics of the cited papers, as well as the citing papers. We fit LTAI to four academic corpora: CORA, Arxiv Physics, PNAS, and Citeseer. We compare the performance of LTAI against various baselines, starting with the latent Dirichlet allocation, to the more advanced models including author-link topic model and dynamic author citation topic model. The results show that LTAI achieves improved accuracy over other similar models when predicting words, citations and authors of publications.\n    ",
        "submission_date": "2017-06-02T00:00:00",
        "last_modified_date": "2017-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00612",
        "title": "Attentive Convolutional Neural Network based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech",
        "authors": [
            "Michael Neumann",
            "Ngoc Thang Vu"
        ],
        "abstract": "Speech emotion recognition is an important and challenging task in the realm of human-computer interaction. Prior work proposed a variety of models and feature sets for training a system. In this work, we conduct extensive experiments using an attentive convolutional neural network with multi-view learning objective function. We compare system performance using different lengths of the input signal, different types of acoustic features and different types of emotion speech (improvised/scripted). Our experimental results on the Interactive Emotional Motion Capture (IEMOCAP) database reveal that the recognition performance strongly depends on the type of speech data independent of the choice of input features. Furthermore, we achieved state-of-the-art results on the improvised speech data of IEMOCAP.\n    ",
        "submission_date": "2017-06-02T00:00:00",
        "last_modified_date": "2017-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00741",
        "title": "Prosodic Event Recognition using Convolutional Neural Networks with Context Information",
        "authors": [
            "Sabrina Stehwien",
            "Ngoc Thang Vu"
        ],
        "abstract": "This paper demonstrates the potential of convolutional neural networks (CNN) for detecting and classifying prosodic events on words, specifically pitch accents and phrase boundary tones, from frame-based acoustic features. Typical approaches use not only feature representations of the word in question but also its surrounding context. We show that adding position features indicating the current word benefits the CNN. In addition, this paper discusses the generalization from a speaker-dependent modelling approach to a speaker-independent setup. The proposed method is simple and efficient and yields strong results not only in speaker-dependent but also speaker-independent cases.\n    ",
        "submission_date": "2017-06-02T00:00:00",
        "last_modified_date": "2017-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00884",
        "title": "Task-specific Word Identification from Short Texts Using a Convolutional Neural Network",
        "authors": [
            "Shuhan Yuan",
            "Xintao Wu",
            "Yang Xiang"
        ],
        "abstract": "Task-specific word identification aims to choose the task-related words that best describe a short text. Existing approaches require well-defined seed words or lexical dictionaries (e.g., WordNet), which are often unavailable for many applications such as social discrimination detection and fake review detection. However, we often have a set of labeled short texts where each short text has a task-related class label, e.g., discriminatory or non-discriminatory, specified by users or learned by classification algorithms. In this paper, we focus on identifying task-specific words and phrases from short texts by exploiting their class labels rather than using seed words or lexical dictionaries. We consider the task-specific word and phrase identification as feature learning. We train a convolutional neural network over a set of labeled texts and use score vectors to localize the task-specific words and phrases. Experimental results on sentiment word identification show that our approach significantly outperforms existing methods. We further conduct two case studies to show the effectiveness of our approach. One case study on a crawled tweets dataset demonstrates that our approach can successfully capture the discrimination-related words/phrases. The other case study on fake review detection shows that our approach can identify the fake-review words/phrases.\n    ",
        "submission_date": "2017-06-03T00:00:00",
        "last_modified_date": "2017-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00927",
        "title": "Concept Transfer Learning for Adaptive Language Understanding",
        "authors": [
            "Su Zhu",
            "Kai Yu"
        ],
        "abstract": "Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between concepts. A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of {\\em atomic concepts}. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2\\&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-of-the-art performance ($F_1$-score 96.08\\%) on ATIS by only using lexicon features.\n    ",
        "submission_date": "2017-06-03T00:00:00",
        "last_modified_date": "2019-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01069",
        "title": "CRNN: A Joint Neural Network for Redundancy Detection",
        "authors": [
            "Xinyu Fu",
            "Eugene Ch'ng",
            "Uwe Aickelin",
            "Simon See"
        ],
        "abstract": "This paper proposes a novel framework for detecting redundancy in supervised sentence categorisation. Unlike traditional singleton neural network, our model incorporates character-aware convolutional neural network (Char-CNN) with character-aware recurrent neural network (Char-RNN) to form a convolutional recurrent neural network (CRNN). Our model benefits from Char-CNN in that only salient features are selected and fed into the integrated Char-RNN. Char-RNN effectively learns long sequence semantics via sophisticated update mechanism. We compare our framework against the state-of-the-art text classification algorithms on four popular benchmarking corpus. For instance, our model achieves competing precision rate, recall ratio, and F1 score on the Google-news data-set. For twenty-news-groups data stream, our algorithm obtains the optimum on precision rate, recall ratio, and F1 score. For Brown Corpus, our framework obtains the best F1 score and almost equivalent precision rate and recall ratio over the top competitor. For the question classification collection, CRNN produces the optimal recall rate and F1 score and comparable precision rate. We also analyse three different RNN hidden recurrent cells' impact on performance and their runtime efficiency. We observe that MGU achieves the optimal runtime and comparable performance against GRU and LSTM. For TFIDF based algorithms, we experiment with word2vec, GloVe, and sent2vec embeddings and report their performance differences.\n    ",
        "submission_date": "2017-06-04T00:00:00",
        "last_modified_date": "2017-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01206",
        "title": "One-step and Two-step Classification for Abusive Language Detection on Twitter",
        "authors": [
            "Ji Ho Park",
            "Pascale Fung"
        ],
        "abstract": "Automatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps.\n    ",
        "submission_date": "2017-06-05T00:00:00",
        "last_modified_date": "2017-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01322",
        "title": "Deep learning evaluation using deep linguistic processing",
        "authors": [
            "Alexander Kuhnle",
            "Ann Copestake"
        ],
        "abstract": "We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing 'deep' linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail, as compared to a single performance value on a static and monolithic dataset.\n    ",
        "submission_date": "2017-06-05T00:00:00",
        "last_modified_date": "2018-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01331",
        "title": "Event Representations for Automated Story Generation with Deep Neural Nets",
        "authors": [
            "Lara J. Martin",
            "Prithviraj Ammanabrolu",
            "Xinyu Wang",
            "William Hancock",
            "Shruti Singh",
            "Brent Harrison",
            "Mark O. Riedl"
        ],
        "abstract": "Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.\n    ",
        "submission_date": "2017-06-05T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01340",
        "title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor",
        "authors": [
            "Robin Ruede",
            "Markus M\u00fcller",
            "Sebastian St\u00fcker",
            "Alex Waibel"
        ],
        "abstract": "Using supporting backchannel (BC) cues can make human-computer interaction more social. BCs provide a feedback from the listener to the speaker indicating to the speaker that he is still listened to. BCs can be expressed in different ways, depending on the modality of the interaction, for example as gestures or acoustic cues. In this work, we only considered acoustic cues. We are proposing an approach towards detecting BC opportunities based on acoustic input features like power and pitch. While other works in the field rely on the use of a hand-written rule set or specialized features, we made use of artificial neural networks. They are capable of deriving higher order features from input features themselves. In our setup, we first used a fully connected feed-forward network to establish an updated baseline in comparison to our previously proposed setup. We also extended this setup by the use of Long Short-Term Memory (LSTM) networks which have shown to outperform feed-forward based setups on various tasks. Our best system achieved an F1-Score of 0.37 using power and pitch features. Adding linguistic information using word2vec, the score increased to 0.39.\n    ",
        "submission_date": "2017-06-02T00:00:00",
        "last_modified_date": "2017-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01399",
        "title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training",
        "authors": [
            "Ofir Press",
            "Amir Bar",
            "Ben Bogin",
            "Jonathan Berant",
            "Lior Wolf"
        ],
        "abstract": "Generative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for language generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning, by slowly teaching the model to generate sequences of increasing and variable length. We empirically show that our approach vastly improves the quality of generated sequences compared to a convolutional baseline.\n    ",
        "submission_date": "2017-06-05T00:00:00",
        "last_modified_date": "2017-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01427",
        "title": "A simple neural network module for relational reasoning",
        "authors": [
            "Adam Santoro",
            "David Raposo",
            "David G.T. Barrett",
            "Mateusz Malinowski",
            "Razvan Pascanu",
            "Peter Battaglia",
            "Timothy Lillicrap"
        ],
        "abstract": "Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.\n    ",
        "submission_date": "2017-06-05T00:00:00",
        "last_modified_date": "2017-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01450",
        "title": "A Joint Model for Question Answering and Question Generation",
        "authors": [
            "Tong Wang",
            "Xingdi Yuan",
            "Adam Trischler"
        ],
        "abstract": "We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks. We believe the joint model's novelty offers a new perspective on machine comprehension beyond architectural engineering, and serves as a first step towards autonomous information seeking.\n    ",
        "submission_date": "2017-06-05T00:00:00",
        "last_modified_date": "2017-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01556",
        "title": "Deep learning for extracting protein-protein interactions from biomedical literature",
        "authors": [
            "Yifan Peng",
            "Zhiyong Lu"
        ],
        "abstract": "State-of-the-art methods for protein-protein interaction (PPI) extraction are primarily feature-based or kernel-based by leveraging lexical and syntactic information. But how to incorporate such knowledge in the recent deep learning methods remains an open question. In this paper, we propose a multichannel dependency-based convolutional neural network model (McDepCNN). It applies one channel to the embedding vector of each word in the sentence, and another channel to the embedding vector of the head of the corresponding word. Therefore, the model can use richer information obtained from different channels. Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that McDepCNN compares favorably to the state-of-the-art rich-feature and single-kernel based methods. In addition, McDepCNN achieves 24.4% relative improvement in F1-score over the state-of-the-art methods on cross-corpus evaluation and 12% improvement in F1-score over kernel-based methods on \"difficult\" instances. These results suggest that McDepCNN generalizes more easily over different corpora, and is capable of capturing long distance features in the sentences.\n    ",
        "submission_date": "2017-06-05T00:00:00",
        "last_modified_date": "2017-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01570",
        "title": "Acquisition of Translation Lexicons for Historically Unwritten Languages via Bridging Loanwords",
        "authors": [
            "Michael Bloodgood",
            "Benjamin Strauss"
        ],
        "abstract": "With the advent of informal electronic communications such as social media, colloquial languages that were historically unwritten are being written for the first time in heavily code-switched environments. We present a method for inducing portions of translation lexicons through the use of expert knowledge in these settings where there are approximately zero resources available other than a language informant, potentially not even large amounts of monolingual data. We investigate inducing a Moroccan Darija-English translation lexicon via French loanwords bridging into English and find that a useful lexicon is induced for human-assisted translation and statistical machine translation.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01678",
        "title": "Text Summarization using Abstract Meaning Representation",
        "authors": [
            "Shibhansh Dohare",
            "Harish Karnick",
            "Vivek Gupta"
        ],
        "abstract": "With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01690",
        "title": "A Frame Tracking Model for Memory-Enhanced Dialogue Systems",
        "authors": [
            "Hannes Schulz",
            "Jeremie Zumer",
            "Layla El Asri",
            "Shikhar Sharma"
        ],
        "abstract": "Recently, resources and tasks were proposed to go beyond state tracking in dialogue systems. An example is the frame tracking task, which requires recording multiple frames, one for each user goal set during the dialogue. This allows a user, for instance, to compare items corresponding to different goals. This paper proposes a model which takes as input the list of frames created so far during the dialogue, the current user utterance as well as the dialogue acts, slot types, and slot values associated with this utterance. The model then outputs the frame being referenced by each triple of dialogue act, slot type, and slot value. We show that on the recently published Frames dataset, this model significantly outperforms a previously proposed rule-based baseline. In addition, we propose an extensive analysis of the frame tracking task by dividing it into sub-tasks and assessing their difficulty with respect to our model.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01723",
        "title": "A General-Purpose Tagger with Convolutional Neural Networks",
        "authors": [
            "Xiang Yu",
            "Agnieszka Fale\u0144ska",
            "Ngoc Thang Vu"
        ],
        "abstract": "We present a general-purpose tagger based on convolutional neural networks (CNN), used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks: without task-specific tuning of hyper-parameters, it achieves state-of-the-art results in part-of-speech tagging, morphological tagging and supertagging. The CNN tagger is also robust against the out-of-vocabulary problem, it performs well on artificially unnormalized texts.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01740",
        "title": "Label-Dependencies Aware Recurrent Neural Networks",
        "authors": [
            "Yoann Dupont",
            "Marco Dinarelli",
            "Isabelle Tellier"
        ],
        "abstract": "In the last few years, Recurrent Neural Networks (RNNs) have proved effective on several NLP tasks. Despite such great success, their ability to model \\emph{sequence labeling} is still limited. This lead research toward solutions where RNNs are combined with models which already proved effective in this domain, such as CRFs. In this work we propose a solution far simpler but very effective: an evolution of the simple Jordan RNN, where labels are re-injected as input into the network, and converted into embeddings, in the same way as words. We compare this RNN variant to all the other RNN models, Elman and Jordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language Understanding (SLU). Thanks to label embeddings and their combination at the hidden layer, the proposed variant, which uses more parameters than Elman and Jordan RNNs, but far fewer than LSTM and GRU, is more effective than other RNNs, but also outperforms sophisticated CRF models.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01758",
        "title": "A WL-SPPIM Semantic Model for Document Classification",
        "authors": [
            "Ming Li",
            "Peilun Xiao",
            "Ju Zhang"
        ],
        "abstract": "In this paper, we explore SPPIM-based text classification method, and the experiment reveals that the SPPIM method is equal to or even superior than SGNS method in text classification task on three international and standard text datasets, namely 20newsgroups, Reuters52 and WebKB. Comparing to SGNS, although SPPMI provides a better solution, it is not necessarily better than SGNS in text classification tasks. Based on our analysis, SGNS takes into the consideration of weight calculation during decomposition process, so it has better performance than SPPIM in some standard datasets. Inspired by this, we propose a WL-SPPIM semantic model based on SPPIM model, and experiment shows that WL-SPPIM approach has better classification and higher scalability in the text classification task compared with LDA, SGNS and SPPIM approaches.\n    ",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01839",
        "title": "Assessing the Linguistic Productivity of Unsupervised Deep Neural Networks",
        "authors": [
            "Lawrence Phillips",
            "Nathan Hodas"
        ],
        "abstract": "Increasingly, cognitive scientists have demonstrated interest in applying tools from deep learning. One use for deep learning is in language acquisition where it is useful to know if a linguistic phenomenon can be learned through domain-general means. To assess whether unsupervised deep learning is appropriate, we first pose a smaller question: Can unsupervised neural networks apply linguistic rules productively, using them in novel situations? We draw from the literature on determiner/noun productivity by training an unsupervised, autoencoder network measuring its ability to combine nouns with determiners. Our simple autoencoder creates combinations it has not previously encountered and produces a degree of overlap matching adults. While this preliminary work does not provide conclusive evidence for productivity, it warrants further investigation with more complex models. Further, this work helps lay the foundations for future collaboration between the deep learning and cognitive science communities.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01847",
        "title": "Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext",
        "authors": [
            "John Wieting",
            "Jonathan Mallinson",
            "Kevin Gimpel"
        ],
        "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and data sources, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from human-written sentences, finding clear differences in length, the amount of repetition, and the use of rare words.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01863",
        "title": "Marmara Turkish Coreference Corpus and Coreference Resolution Baseline",
        "authors": [
            "Peter Sch\u00fcller",
            "K\u00fcbra C\u0131ng\u0131ll\u0131",
            "Ferit Tun\u00e7er",
            "Bar\u0131\u015f G\u00fcn S\u00fcrmeli",
            "Ay\u015feg\u00fcl Pekel",
            "Ay\u015fe Hande Karatay",
            "Hacer Ezgi Karaka\u015f"
        ],
        "abstract": "We describe the Marmara Turkish Coreference Corpus, which is an annotation of the whole METU-Sabanci Turkish Treebank with mentions and coreference chains. Collecting eight or more independent annotations for each document allowed for fully automatic adjudication. We provide a baseline system for Turkish mention detection and coreference resolution and evaluate it on the corpus.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2018-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01875",
        "title": "Measuring Offensive Speech in Online Political Discourse",
        "authors": [
            "Rishab Nithyanand",
            "Brian Schaffner",
            "Phillipa Gill"
        ],
        "abstract": "The Internet and online forums such as Reddit have become an increasingly popular medium for citizens to engage in political conversations. However, the online disinhibition effect resulting from the ability to use pseudonymous identities may manifest in the form of offensive speech, consequently making political discussions more aggressive and polarizing than they already are. Such environments may result in harassment and self-censorship from its targets. In this paper, we present preliminary results from a large-scale temporal measurement aimed at quantifying offensiveness in online political discussions.\n",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01967",
        "title": "Synergistic Union of Word2Vec and Lexicon for Domain Specific Semantic Similarity",
        "authors": [
            "Keet Sugathadasa",
            "Buddhi Ayesha",
            "Nisansa de Silva",
            "Amal Shehan Perera",
            "Vindula Jayawardana",
            "Dimuthu Lakmal",
            "Madhavi Perera"
        ],
        "abstract": "Semantic similarity measures are an important part in Natural Language Processing tasks. However Semantic similarity measures built for general use do not perform well within specific domains. Therefore in this study we introduce a domain specific semantic similarity measure that was created by the synergistic union of word2vec, a word embedding method that is used for semantic similarity calculation and lexicon based (lexical) semantic similarity methods. We prove that this proposed methodology out performs word embedding methods trained on generic corpus and methods trained on domain specific corpus but do not use lexical semantic similarity methods to augment the results. Further, we prove that text lemmatization can improve the performance of word embedding methods.\n    ",
        "submission_date": "2017-06-06T00:00:00",
        "last_modified_date": "2017-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02027",
        "title": "Question Answering and Question Generation as Dual Tasks",
        "authors": [
            "Duyu Tang",
            "Nan Duan",
            "Tao Qin",
            "Zhao Yan",
            "Ming Zhou"
        ],
        "abstract": "We study the problem of joint question answering (QA) and question generation (QG) in this paper.\n",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02095",
        "title": "Macquarie University at BioASQ 5b -- Query-based Summarisation Techniques for Selecting the Ideal Answers",
        "authors": [
            "Diego Molla-Aliod"
        ],
        "abstract": "Macquarie University's contribution to the BioASQ challenge (Task 5b Phase B) focused on the use of query-based extractive summarisation techniques for the generation of the ideal answers. Four runs were submitted, with approaches ranging from a trivial system that selected the first $n$ snippets, to the use of deep learning approaches under a regression framework. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge.\n    ",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02124",
        "title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks",
        "authors": [
            "Marian Tietz",
            "Tayfun Alpay",
            "Johannes Twiefel",
            "Stefan Wermter"
        ],
        "abstract": "Ladder networks are a notable new concept in the field of semi-supervised learning by showing state-of-the-art results in image recognition tasks while being compatible with many existing neural architectures. We present the recurrent ladder network, a novel modification of the ladder network, for semi-supervised learning of recurrent neural networks which we evaluate with a phoneme recognition task on the TIMIT corpus. Our results show that the model is able to consistently outperform the baseline and achieve fully-supervised baseline performance with only 75% of all labels which demonstrates that the model is capable of using unsupervised data as an effective regulariser.\n    ",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02141",
        "title": "How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on Rule-Based Sentiment Analysis",
        "authors": [
            "Carlos G\u00f3mez-Rodr\u00edguez",
            "Iago Alonso-Alonso",
            "David Vilares"
        ],
        "abstract": "Syntactic parsing, the process of obtaining the internal structure of sentences in natural languages, is a crucial task for artificial intelligence applications that need to extract meaning from natural language text or speech. Sentiment analysis is one example of application for which parsing has recently proven useful.\n",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02241",
        "title": "Insights into Analogy Completion from the Biomedical Domain",
        "authors": [
            "Denis Newman-Griffis",
            "Albert M Lai",
            "Eric Fosler-Lussier"
        ],
        "abstract": "Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods.\n    ",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2017-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02256",
        "title": "A Mention-Ranking Model for Abstract Anaphora Resolution",
        "authors": [
            "Ana Marasovi\u0107",
            "Leo Born",
            "Juri Opitz",
            "Anette Frank"
        ],
        "abstract": "Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentence--antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors. Our model selects syntactically plausible candidates and -- if disregarding syntax -- discriminates candidates using deeper features.\n    ",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02427",
        "title": "Content-Based Table Retrieval for Web Queries",
        "authors": [
            "Zhao Yan",
            "Duyu Tang",
            "Nan Duan",
            "Junwei Bao",
            "Yuanhua Lv",
            "Ming Zhou",
            "Zhoujun Li"
        ],
        "abstract": "Understanding the connections between unstructured text and semi-structured table is an important yet neglected problem in natural language processing. In this work, we focus on content-based table retrieval. Given a query, the task is to find the most relevant table from a collection of tables. Further progress towards improving this area requires powerful models of semantic matching and richer training and evaluation resources. To remedy this, we present a ranking based approach, and implement both carefully designed features and neural network architectures to measure the relevance between a query and the content of a table. Furthermore, we release an open-domain dataset that includes 21,113 web queries for 273,816 tables. We conduct comprehensive experiments on both real world and synthetic datasets. Results verify the effectiveness of our approach and present the challenges for this task.\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02459",
        "title": "Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization",
        "authors": [
            "Shuming Ma",
            "Xu Sun",
            "Jingjing Xu",
            "Houfeng Wang",
            "Wenjie Li",
            "Qi Su"
        ],
        "abstract": "Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02551",
        "title": "The Algorithmic Inflection of Russian and Generation of Grammatically Correct Text",
        "authors": [
            "T.M. Sadykov",
            "T.A. Zhukov"
        ],
        "abstract": "We present a deterministic algorithm for Russian inflection. This algorithm is implemented in a publicly available web-service ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02596",
        "title": "Dynamic Integration of Background Knowledge in Neural NLU Systems",
        "authors": [
            "Dirk Weissenborn",
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Chris Dyer"
        ],
        "abstract": "Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way.\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2018-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02737",
        "title": "Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM",
        "authors": [
            "Takaaki Hori",
            "Shinji Watanabe",
            "Yu Zhang",
            "William Chan"
        ],
        "abstract": "We present a state-of-the-art end-to-end Automatic Speech Recognition (ASR) model. We learn to listen and write characters with a joint Connectionist Temporal Classification (CTC) and attention-based encoder-decoder network. The encoder is a deep Convolutional Neural Network (CNN) based on the VGG network. The CTC network sits on top of the encoder and is jointly trained with the attention-based decoder. During the beam search process, we combine the CTC predictions, the attention-based decoder predictions and a separately trained LSTM language model. We achieve a 5-10\\% error reduction compared to prior systems on spontaneous Japanese and Chinese speech, and our end-to-end model beats out traditional hybrid ASR systems.\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02776",
        "title": "Optimizing expected word error rate via sampling for speech recognition",
        "authors": [
            "Matt Shannon"
        ],
        "abstract": "State-level minimum Bayes risk (sMBR) training has become the de facto standard for sequence-level training of speech recognition acoustic models. It has an elegant formulation using the expectation semiring, and gives large improvements in word error rate (WER) over models trained solely using cross-entropy (CE) or connectionist temporal classification (CTC). sMBR training optimizes the expected number of frames at which the reference and hypothesized acoustic states differ. It may be preferable to optimize the expected WER, but WER does not interact well with the expectation semiring, and previous approaches based on computing expected WER exactly involve expanding the lattices used during training. In this paper we show how to perform optimization of the expected WER by sampling paths from the lattices used during conventional sMBR training. The gradient of the expected WER is itself an expectation, and so may be approximated using Monte Carlo sampling. We show experimentally that optimizing WER during acoustic model training gives 5% relative improvement in WER over a well-tuned sMBR baseline on a 2-channel query recognition task (Google Home).\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02807",
        "title": "Learning to Embed Words in Context for Syntactic Tasks",
        "authors": [
            "Lifu Tu",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "abstract": "We present models for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data. We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.\n    ",
        "submission_date": "2017-06-09T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02861",
        "title": "Assigning personality/identity to a chatting machine for coherent conversation generation",
        "authors": [
            "Qiao Qian",
            "Minlie Huang",
            "Haizhou Zhao",
            "Jingfang Xu",
            "Xiaoyan Zhu"
        ],
        "abstract": "Endowing a chatbot with personality or an identity is quite challenging but critical to deliver more realistic and natural conversations. In this paper, we address the issue of generating responses that are coherent to a pre-specified agent profile. We design a model consisting of three modules: a profile detector to decide whether a post should be responded using the profile and which key should be addressed, a bidirectional decoder to generate responses forward and backward starting from a selected profile value, and a position detector that predicts a word position from which decoding should start given a selected profile value. We show that general conversation data from social media can be used to generate profile-coherent responses. Manual and automatic evaluation shows that our model can deliver more coherent, natural, and diversified responses.\n    ",
        "submission_date": "2017-06-09T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02883",
        "title": "Overview of the NLPCC 2017 Shared Task: Chinese News Headline Categorization",
        "authors": [
            "Xipeng Qiu",
            "Jingjing Gong",
            "Xuanjing Huang"
        ],
        "abstract": "In this paper, we give an overview for the shared task at the CCF Conference on Natural Language Processing \\& Chinese Computing (NLPCC 2017): Chinese News Headline Categorization. The dataset of this shared task consists 18 classes, 12,000 short texts along with corresponded labels for each class. The dataset and example code can be accessed at ",
        "submission_date": "2017-06-09T00:00:00",
        "last_modified_date": "2017-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02909",
        "title": "Deriving a Representative Vector for Ontology Classes with Instance Word Vector Embeddings",
        "authors": [
            "Vindula Jayawardana",
            "Dimuthu Lakmal",
            "Nisansa de Silva",
            "Amal Shehan Perera",
            "Keet Sugathadasa",
            "Buddhi Ayesha"
        ],
        "abstract": "Selecting a representative vector for a set of vectors is a very common requirement in many algorithmic tasks. Traditionally, the mean or median vector is selected. Ontology classes are sets of homogeneous instance objects that can be converted to a vector space by word vector embeddings. This study proposes a methodology to derive a representative vector for ontology classes whose instances were converted to the vector space. We start by deriving five candidate vectors which are then used to train a machine learning model that would calculate a representative vector for the class. We show that our methodology out-performs the traditional mean and median vector representations.\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03059",
        "title": "Depthwise Separable Convolutions for Neural Machine Translation",
        "authors": [
            "Lukasz Kaiser",
            "Aidan N. Gomez",
            "Francois Chollet"
        ],
        "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new \"super-separable\" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.\n    ",
        "submission_date": "2017-06-09T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03146",
        "title": "Rethinking Skip-thought: A Neighborhood based Approach",
        "authors": [
            "Shuai Tang",
            "Hailin Jin",
            "Chen Fang",
            "Zhaowen Wang",
            "Virginia R. de Sa"
        ],
        "abstract": "We study the skip-thought model with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our model didn't aid our model to perform better, while it hurts the performance of the skip-thought model.\n    ",
        "submission_date": "2017-06-09T00:00:00",
        "last_modified_date": "2017-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03148",
        "title": "Trimming and Improving Skip-thought Vectors",
        "authors": [
            "Shuai Tang",
            "Hailin Jin",
            "Chen Fang",
            "Zhaowen Wang",
            "Virginia R. de Sa"
        ],
        "abstract": "The skip-thought model has been proven to be effective at learning sentence representations and capturing sentence semantics. In this paper, we propose a suite of techniques to trim and improve it. First, we validate a hypothesis that, given a current sentence, inferring the previous and inferring the next sentence provide similar supervision power, therefore only one decoder for predicting the next sentence is preserved in our trimmed skip-thought model. Second, we present a connection layer between encoder and decoder to help the model to generalize better on semantic relatedness tasks. Third, we found that a good word embedding initialization is also essential for learning better sentence representations. We train our model unsupervised on a large corpus with contiguous sentences, and then evaluate the trained model on 7 supervised tasks, which includes semantic relatedness, paraphrase detection, and text classification benchmarks. We empirically show that, our proposed model is a faster, lighter-weight and equally powerful alternative to the original skip-thought model.\n    ",
        "submission_date": "2017-06-09T00:00:00",
        "last_modified_date": "2017-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03191",
        "title": "Classification of Questions and Learning Outcome Statements (LOS) Into Blooms Taxonomy (BT) By Similarity Measurements Towards Extracting Of Learning Outcome from Learning Material",
        "authors": [
            "Shadi Diab",
            "Badie Sartawi"
        ],
        "abstract": "Blooms Taxonomy (BT) have been used to classify the objectives of learning outcome by dividing the learning into three different domains; the cognitive domain, the effective domain and the psychomotor domain. In this paper, we are introducing a new approach to classify the questions and learning outcome statements (LOS) into Blooms taxonomy (BT) and to verify BT verb lists, which are being cited and used by academicians to write questions and (LOS). An experiment was designed to investigate the semantic relationship between the action verbs used in both questions and LOS to obtain more accurate classification of the levels of BT. A sample of 775 different action verbs collected from different universities allows us to measure an accurate and clear-cut cognitive level for the action verb. It is worth mentioning that natural language processing techniques were used to develop our rules as to induce the questions into chunks in order to extract the action verbs. Our proposed solution was able to classify the action verb into a precise level of the cognitive domain. We, on our side, have tested and evaluated our proposed solution using confusion matrix. The results of evaluation tests yielded 97% for the macro average of precision and 90% for F1. Thus, the outcome of the research suggests that it is crucial to analyse and verify the action verbs cited and used by academicians to write LOS and classify their questions based on blooms taxonomy in order to obtain a definite and more accurate classification.\n    ",
        "submission_date": "2017-06-10T00:00:00",
        "last_modified_date": "2019-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03216",
        "title": "Articulation rate in Swedish child-directed speech increases as a function of the age of the child even when surprisal is controlled for",
        "authors": [
            "Johan Sjons",
            "Thomas H\u00f6rberg",
            "Robert \u00d6stling",
            "Johannes Bjerva"
        ],
        "abstract": "In earlier work, we have shown that articulation rate in Swedish child-directed speech (CDS) increases as a function of the age of the child, even when utterance length and differences in articulation rate between subjects are controlled for. In this paper we show on utterance level in spontaneous Swedish speech that i) for the youngest children, articulation rate in CDS is lower than in adult-directed speech (ADS), ii) there is a significant negative correlation between articulation rate and surprisal (the negative log probability) in ADS, and iii) the increase in articulation rate in Swedish CDS as a function of the age of the child holds, even when surprisal along with utterance length and differences in articulation rate between speakers are controlled for. These results indicate that adults adjust their articulation rate to make it fit the linguistic capacity of the child.\n    ",
        "submission_date": "2017-06-10T00:00:00",
        "last_modified_date": "2017-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03335",
        "title": "Exploring Automated Essay Scoring for Nonnative English Speakers",
        "authors": [
            "Amber Nigam"
        ],
        "abstract": "Automated Essay Scoring (AES) has been quite popular and is being widely used. However, lack of appropriate methodology for rating nonnative English speakers' essays has meant a lopsided advancement in this field. In this paper, we report initial results of our experiments with nonnative AES that learns from manual evaluation of nonnative essays. For this purpose, we conducted an exercise in which essays written by nonnative English speakers in test environment were rated both manually and by the automated system designed for the experiment. In the process, we experimented with a few features to learn about nuances linked to nonnative evaluation. The proposed methodology of automated essay evaluation has yielded a correlation coefficient of 0.750 with the manual evaluation.\n    ",
        "submission_date": "2017-06-11T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03357",
        "title": "Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing",
        "authors": [
            "Anssi Yli-Jyr\u00e4",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "abstract": "We present a simple encoding for unlabeled noncrossing graphs and show how its latent counterpart helps us to represent several families of directed and undirected graphs used in syntactic and semantic parsing of natural language as context-free languages. The families are separated purely on the basis of forbidden patterns in latent encoding, eliminating the need to differentiate the families of non-crossing graphs in inference algorithms: one algorithm works for all when the search space can be controlled in parser input.\n    ",
        "submission_date": "2017-06-11T00:00:00",
        "last_modified_date": "2017-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03367",
        "title": "A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing",
        "authors": [
            "Daniel Fern\u00e1ndez-Gonz\u00e1lez",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "abstract": "Restricted non-monotonicity has been shown beneficial for the projective arc-eager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information. In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm. As a non-monotonic system requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss. Experiments on datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the monotonic version in the majority of languages.\n    ",
        "submission_date": "2017-06-11T00:00:00",
        "last_modified_date": "2017-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03441",
        "title": "Dialog Structure Through the Lens of Gender, Gender Environment, and Power",
        "authors": [
            "Vinodkumar Prabhakaran",
            "Owen Rambow"
        ],
        "abstract": "Understanding how the social context of an interaction affects our dialog behavior is of great interest to social scientists who study human behavior, as well as to computer scientists who build automatic methods to infer those social contexts. In this paper, we study the interaction of power, gender, and dialog behavior in organizational interactions. In order to perform this study, we first construct the Gender Identified Enron Corpus of emails, in which we semi-automatically assign the gender of around 23,000 individuals who authored around 97,000 email messages in the Enron corpus. This corpus, which is made freely available, is orders of magnitude larger than previously existing gender identified corpora in the email domain. Next, we use this corpus to perform a large-scale data-oriented study of the interplay of gender and manifestations of power. We argue that, in addition to one's own gender, the \"gender environment\" of an interaction, i.e., the gender makeup of one's interlocutors, also affects the way power is manifested in dialog. We focus especially on manifestations of power in the dialog structure --- both, in a shallow sense that disregards the textual content of messages (e.g., how often do the participants contribute, how often do they get replies etc.), as well as the structure that is expressed within the textual content (e.g., who issues requests and how are they made, whose requests get responses etc.). We find that both gender and gender environment affect the ways power is manifested in dialog, resulting in patterns that reveal the underlying factors. Finally, we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03449",
        "title": "Scientific document summarization via citation contextualization and scientific discourse",
        "authors": [
            "Arman Cohan",
            "Nazli Goharian"
        ],
        "abstract": "The rapid growth of scientific literature has made it difficult for the researchers to quickly learn about the developments in their respective fields. Scientific document summarization addresses this challenge by providing summaries of the important contributions of scientific papers. We present a framework for scientific summarization which takes advantage of the citations and the scientific discourse structure. Citation texts often lack the evidence and context to support the content of the cited paper and are even sometimes inaccurate. We first address the problem of inaccuracy of the citation texts by finding the relevant context from the cited paper. We propose three approaches for contextualizing citations which are based on query reformulation, word embeddings, and supervised learning. We then train a model to identify the discourse facets for each citation. We finally propose a method for summarizing scientific papers by leveraging the faceted citations and their corresponding contexts. We evaluate our proposed method on two scientific summarization datasets in the biomedical and computational linguistics domains. Extensive evaluation results show that our methods can improve over the state of the art by large margins.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03499",
        "title": "SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models",
        "authors": [
            "Robert \u00d6stling",
            "Johannes Bjerva"
        ],
        "abstract": "This paper describes the Stockholm University/University of Groningen (SU-RUG) system for the SIGMORPHON 2017 shared task on morphological inflection. Our system is based on an attentional sequence-to-sequence neural network model using Long Short-Term Memory (LSTM) cells, with joint training of morphological inflection and the inverse transformation, i.e. lemmatization and morphological analysis. Our system outperforms the baseline with a large margin, and our submission ranks as the 4th best team for the track we participate in (task 1, high-resource).\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03530",
        "title": "Candidate sentence selection for language learning exercises: from a comprehensive framework to an empirical evaluation",
        "authors": [
            "Ildik\u00f3 Pil\u00e1n",
            "Elena Volodina",
            "Lars Borin"
        ],
        "abstract": "We present a framework and its implementation relying on Natural Language Processing methods, which aims at the identification of exercise item candidates from corpora. The hybrid system combining heuristics and machine learning methods includes a number of relevant selection criteria. We focus on two fundamental aspects: linguistic complexity and the dependence of the extracted sentences on their original context. Previous work on exercise generation addressed these two criteria only to a limited extent, and a refined overall candidate sentence selection framework appears also to be lacking. In addition to a detailed description of the system, we present the results of an empirical evaluation conducted with language teachers and learners which indicate the usefulness of the system for educational purposes. We have integrated our system into a freely available online learning platform.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03542",
        "title": "Exploring the Syntactic Abilities of RNNs with Multi-task Learning",
        "authors": [
            "Emile Enguehard",
            "Yoav Goldberg",
            "Tal Linzen"
        ],
        "abstract": "Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03610",
        "title": "Neural Domain Adaptation for Biomedical Question Answering",
        "authors": [
            "Georg Wiese",
            "Dirk Weissenborn",
            "Mariana Neves"
        ],
        "abstract": "Factoid question answering (QA) has recently benefited from the development of deep learning (DL) systems. Neural network models outperform traditional approaches in domains where large datasets exist, such as SQuAD (ca. 100,000 questions) for Wikipedia articles. However, these systems have not yet been applied to QA in more specific domains, such as biomedicine, because datasets are generally too small to train a DL system from scratch. For example, the BioASQ dataset for biomedical QA comprises less then 900 factoid (single answer) and list (multiple answers) QA instances. In this work, we adapt a neural QA system trained on a large open-domain dataset (SQuAD, source) to a biomedical dataset (BioASQ, target) by employing various transfer learning techniques. Our network architecture is based on a state-of-the-art QA system, extended with biomedical word embeddings and a novel mechanism to answer list questions. In contrast to existing biomedical QA systems, our system does not rely on domain-specific ontologies, parsers or entity taggers, which are expensive to create. Despite this fact, our systems achieve state-of-the-art results on factoid questions and competitive results on list questions.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03747",
        "title": "Acoustic data-driven lexicon learning based on a greedy pronunciation selection framework",
        "authors": [
            "Xiaohui Zhang",
            "Vimal Manohar",
            "Daniel Povey",
            "Sanjeev Khudanpur"
        ],
        "abstract": "Speech recognition systems for irregularly-spelled languages like English normally require hand-written pronunciations. In this paper, we describe a system for automatically obtaining pronunciations of words for which pronunciations are not available, but for which transcribed data exists. Our method integrates information from the letter sequence and from the acoustic evidence. The novel aspect of the problem that we address is the problem of how to prune entries from such a lexicon (since, empirically, lexicons with too many entries do not tend to be good for ASR performance). Experiments on various ASR tasks show that, with the proposed framework, starting with an initial lexicon of several thousand words, we are able to learn a lexicon which performs close to a full expert lexicon in terms of WER performance on test data, and is better than lexicons built using G2P alone or with a pruning criterion based on pronunciation probability.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03757",
        "title": "Semantic Entity Retrieval Toolkit",
        "authors": [
            "Christophe Van Gysel",
            "Maarten de Rijke",
            "Evangelos Kanoulas"
        ],
        "abstract": "Unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained attention. In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models. The toolkit provides a unified interface to different representation learning algorithms, fine-grained parsing configuration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. After model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03762",
        "title": "Attention Is All You Need",
        "authors": [
            "Ashish Vaswani",
            "Noam Shazeer",
            "Niki Parmar",
            "Jakob Uszkoreit",
            "Llion Jones",
            "Aidan N. Gomez",
            "Lukasz Kaiser",
            "Illia Polosukhin"
        ],
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2023-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03799",
        "title": "Verb Physics: Relative Physical Knowledge of Actions and Objects",
        "authors": [
            "Maxwell Forbes",
            "Yejin Choi"
        ],
        "abstract": "Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., \"My house is bigger than me.\" However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, \"Tyler entered his house\" implies that his house is bigger than Tyler.\n",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03815",
        "title": "Encoding of phonology in a recurrent neural model of grounded speech",
        "authors": [
            "Afra Alishahi",
            "Marie Barking",
            "Grzegorz Chrupa\u0142a"
        ],
        "abstract": "We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03818",
        "title": "Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings",
        "authors": [
            "Shane Settle",
            "Keith Levin",
            "Herman Kamper",
            "Karen Livescu"
        ],
        "abstract": "Query-by-example search often uses dynamic time warping (DTW) for comparing queries and proposed matching segments. Recent work has shown that comparing speech segments by representing them as fixed-dimensional vectors --- acoustic word embeddings --- and measuring their vector distance (e.g., cosine distance) can discriminate between words more accurately than DTW-based approaches. We consider an approach to query-by-example search that embeds both the query and database segments according to a neural model, followed by nearest-neighbor search to find the matching segments. Earlier work on embedding-based query-by-example, using template-based acoustic word embeddings, achieved competitive performance. We find that our embeddings, based on recurrent neural networks trained to optimize word discrimination, achieve substantial improvements in performance and run-time efficiency over the previous approaches.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03824",
        "title": "Attention-based Vocabulary Selection for NMT Decoding",
        "authors": [
            "Baskaran Sankaran",
            "Markus Freitag",
            "Yaser Al-Onaizan"
        ],
        "abstract": "Neural Machine Translation (NMT) models usually use large target vocabulary sizes to capture most of the words in the target language. The vocabulary size is a big factor when decoding new sentences as the final softmax layer normalizes over all possible target words. To address this problem, it is widely common to restrict the target vocabulary with candidate lists based on the source sentence. Usually, the candidate lists are a combination of external word-to-word aligner, phrase table entries or most frequent words. In this work, we propose a simple and yet novel approach to learn candidate lists directly from the attention layer during NMT training. The candidate lists are highly optimized for the current NMT model and do not need any external computation of the candidate pool. We show significant decoding speedup compared with using the entire vocabulary, without losing any translation quality for two language pairs.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03872",
        "title": "Six Challenges for Neural Machine Translation",
        "authors": [
            "Philipp Koehn",
            "Rebecca Knowles"
        ],
        "abstract": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03946",
        "title": "A Supervised Approach to Extractive Summarisation of Scientific Papers",
        "authors": [
            "Ed Collins",
            "Isabelle Augenstein",
            "Sebastian Riedel"
        ],
        "abstract": "Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.\n    ",
        "submission_date": "2017-06-13T00:00:00",
        "last_modified_date": "2017-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03952",
        "title": "Modelling prosodic structure using Artificial Neural Networks",
        "authors": [
            "Jean-Philippe Bernardy",
            "Charalambos Themistocleous"
        ],
        "abstract": "The ability to accurately perceive whether a speaker is asking a question or is making a statement is crucial for any successful interaction. However, learning and classifying tonal patterns has been a challenging task for automatic speech recognition and for models of tonal representation, as tonal contours are characterized by significant variation. This paper provides a classification model of Cypriot Greek questions and statements. We evaluate two state-of-the-art network architectures: a Long Short-Term Memory (LSTM) network and a convolutional network (ConvNet). The ConvNet outperforms the LSTM in the classification task and exhibited an excellent performance with 95% classification accuracy.\n    ",
        "submission_date": "2017-06-13T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04115",
        "title": "Zero-Shot Relation Extraction via Reading Comprehension",
        "authors": [
            "Omer Levy",
            "Minjoon Seo",
            "Eunsol Choi",
            "Luke Zettlemoyer"
        ],
        "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.\n    ",
        "submission_date": "2017-06-13T00:00:00",
        "last_modified_date": "2017-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04138",
        "title": "An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing",
        "authors": [
            "Marcin Junczys-Dowmunt",
            "Roman Grundkiewicz"
        ],
        "abstract": "In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs $mt$ (raw MT output) and $src$ (source language input) in a single neural architecture, modeling $\\{mt, src\\} \\rightarrow pe$ directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT-2016 shared task on automatic post-editing and can demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Dual-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.\n    ",
        "submission_date": "2017-06-13T00:00:00",
        "last_modified_date": "2017-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04206",
        "title": "Identifying Condition-Action Statements in Medical Guidelines Using Domain-Independent Features",
        "authors": [
            "Hossein Hematialam",
            "Wlodek Zadrozny"
        ],
        "abstract": "This paper advances the state of the art in text understanding of medical guidelines by releasing two new annotated clinical guidelines datasets, and establishing baselines for using machine learning to extract condition-action pairs. In contrast to prior work that relies on manually created rules, we report experiment with several supervised machine learning techniques to classify sentences as to whether they express conditions and actions. We show the limitations and possible extensions of this work on text mining of medical guidelines.\n    ",
        "submission_date": "2017-06-13T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04326",
        "title": "Transfer Learning for Neural Semantic Parsing",
        "authors": [
            "Xing Fan",
            "Emilio Monti",
            "Lambert Mathias",
            "Markus Dreyer"
        ],
        "abstract": "The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with a focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence modeling and compare their performance with an independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to a target task with smaller labeled data. We see absolute accuracy gains ranging from 1.0% to 4.4% in our in- house data set, and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.\n    ",
        "submission_date": "2017-06-14T00:00:00",
        "last_modified_date": "2017-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04389",
        "title": "Fine-grained human evaluation of neural versus phrase-based machine translation",
        "authors": [
            "Filip Klubi\u010dka",
            "Antonio Toral",
            "V\u00edctor M. S\u00e1nchez-Cartagena"
        ],
        "abstract": "We compare three approaches to statistical machine translation (pure phrase-based, factored phrase-based and neural) by performing a fine-grained manual evaluation via error annotation of the systems' outputs. The error types in our annotation are compliant with the multidimensional quality metrics (MQM), and the annotation is performed by two annotators. Inter-annotator agreement is high for such a task, and results show that the best performing system (neural) reduces the errors produced by the worst system (phrase-based) by 54%.\n    ",
        "submission_date": "2017-06-14T00:00:00",
        "last_modified_date": "2017-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04473",
        "title": "Idea density for predicting Alzheimer's disease from transcribed speech",
        "authors": [
            "Kairit Sirts",
            "Olivier Piguet",
            "Mark Johnson"
        ],
        "abstract": "Idea Density (ID) measures the rate at which ideas or elementary predications are expressed in an utterance or in a text. Lower ID is found to be associated with an increased risk of developing Alzheimer's disease (AD) (Snowdon et al., 1996; Engelman et al., 2010). ID has been used in two different versions: propositional idea density (PID) counts the expressed ideas and can be applied to any text while semantic idea density (SID) counts pre-defined information content units and is naturally more applicable to normative domains, such as picture description tasks. In this paper, we develop DEPID, a novel dependency-based method for computing PID, and its version DEPID-R that enables to exclude repeating ideas---a feature characteristic to AD speech. We conduct the first comparison of automatically extracted PID and SID in the diagnostic classification task on two different AD datasets covering both closed-topic and free-recall domains. While SID performs better on the normative dataset, adding PID leads to a small but significant improvement (+1.7 F-score). On the free-topic dataset, PID performs better than SID as expected (77.6 vs 72.3 in F-score) but adding the features derived from the word embedding clustering underlying the automatic SID increases the results considerably, leading to an F-score of 84.8.\n    ",
        "submission_date": "2017-06-14T00:00:00",
        "last_modified_date": "2017-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04560",
        "title": "Neural Models for Key Phrase Detection and Question Generation",
        "authors": [
            "Sandeep Subramanian",
            "Tong Wang",
            "Xingdi Yuan",
            "Saizheng Zhang",
            "Yoshua Bengio",
            "Adam Trischler"
        ],
        "abstract": "We propose a two-stage neural model to tackle question generation from documents. First, our model estimates the probability that word sequences in a document are ones that a human would pick when selecting candidate answers by training a neural key-phrase extractor on the answers in a question-answering corpus. Predicted key phrases then act as target answers and condition a sequence-to-sequence question-generation model with a copy mechanism. Empirically, our key-phrase extraction model significantly outperforms an entity-tagging baseline and existing rule-based approaches. We further demonstrate that our question generation system formulates fluent, answerable questions from key phrases. This two-stage system could be used to augment or generate reading comprehension datasets, which may be leveraged to improve machine reading systems or in educational settings.\n    ",
        "submission_date": "2017-06-14T00:00:00",
        "last_modified_date": "2018-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04815",
        "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension",
        "authors": [
            "Chuanqi Tan",
            "Furu Wei",
            "Nan Yang",
            "Bowen Du",
            "Weifeng Lv",
            "Ming Zhou"
        ],
        "abstract": "In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defines the task as answering a question from multiple passages and the words in the answer are not necessary in the passages. We therefore develop an extraction-then-synthesis framework to synthesize answers from extraction results. Specifically, the answer extraction model is first employed to predict the most important sub-spans from the passage as evidence, and the answer synthesis model takes the evidence as additional features along with the question and passage to further elaborate the final answers. We build the answer extraction model with state-of-the-art neural networks for single passage reading comprehension, and propose an additional task of passage ranking to help answer extraction in multiple passages. The answer synthesis model is based on the sequence-to-sequence neural networks with extracted evidences as features. Experiments show that our extraction-then-synthesis method outperforms state-of-the-art methods.\n    ",
        "submission_date": "2017-06-15T00:00:00",
        "last_modified_date": "2018-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04872",
        "title": "Towards a theory of word order. Comment on \"Dependency distance: a new perspective on syntactic patterns in natural language\" by Haitao Liu et al",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Comment on \"Dependency distance: a new perspective on syntactic patterns in natural language\" by Haitao Liu et al\n    ",
        "submission_date": "2017-06-15T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04902",
        "title": "A Survey Of Cross-lingual Word Embedding Models",
        "authors": [
            "Sebastian Ruder",
            "Ivan Vuli\u0107",
            "Anders S\u00f8gaard"
        ],
        "abstract": "Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.\n    ",
        "submission_date": "2017-06-15T00:00:00",
        "last_modified_date": "2019-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04971",
        "title": "German in Flux: Detecting Metaphoric Change via Word Entropy",
        "authors": [
            "Dominik Schlechtweg",
            "Stefanie Eckmann",
            "Enrico Santus",
            "Sabine Schulte im Walde",
            "Daniel Hole"
        ],
        "abstract": "This paper explores the information-theoretic measure entropy to detect metaphoric change, transferring ideas from hypernym detection to research on language change. We also build the first diachronic test set for German as a standard for metaphoric change annotation. Our model shows high performance, is unsupervised, language-independent and generalizable to other processes of semantic change.\n    ",
        "submission_date": "2017-06-15T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04997",
        "title": "Extracting Formal Models from Normative Texts",
        "authors": [
            "John J. Camilleri",
            "Normunds Gr\u016bz\\=\u0131tis",
            "Gerardo Schneider"
        ],
        "abstract": "We are concerned with the analysis of normative texts - documents based on the deontic notions of obligation, permission, and prohibition. Our goal is to make queries about these notions and verify that a text satisfies certain properties concerning causality of actions and timing constraints. This requires taking the original text and building a representation (model) of it in a formal language, in our case the C-O Diagram formalism. We present an experimental, semi-automatic aid that helps to bridge the gap between a normative text in natural language and its C-O Diagram representation. Our approach consists of using dependency structures obtained from the state-of-the-art Stanford Parser, and applying our own rules and heuristics in order to extract the relevant components. The result is a tabular data structure where each sentence is split into suitable fields, which can then be converted into a C-O Diagram. The process is not fully automatic however, and some post-editing is generally required of the user. We apply our tool and perform experiments on documents from different domains, and report an initial evaluation of the accuracy and feasibility of our approach.\n    ",
        "submission_date": "2017-06-15T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05075",
        "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
        "authors": [
            "Suncong Zheng",
            "Feng Wang",
            "Hongyun Bao",
            "Yuexing Hao",
            "Peng Zhou",
            "Bo Xu"
        ],
        "abstract": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.\n    ",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2017-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05083",
        "title": "Ensembling Factored Neural Machine Translation Models for Automatic Post-Editing and Quality Estimation",
        "authors": [
            "Chris Hokamp"
        ],
        "abstract": "This work presents a novel approach to Automatic Post-Editing (APE) and Word-Level Quality Estimation (QE) using ensembles of specialized Neural Machine Translation (NMT) systems. Word-level features that have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis. We train a suite of NMT models that use different input representations, but share the same output space. These models are then ensembled together, and tuned for both the APE and the QE task. We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework. Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.\n    ",
        "submission_date": "2017-06-15T00:00:00",
        "last_modified_date": "2017-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05084",
        "title": "Topic supervised non-negative matrix factorization",
        "authors": [
            "Kelsey MacMillan",
            "James D. Wilson"
        ],
        "abstract": "Topic models have been extensively used to organize and interpret the contents of large, unstructured corpora of text documents. Although topic models often perform well on traditional training vs. test set evaluations, it is often the case that the results of a topic model do not align with human interpretation. This interpretability fallacy is largely due to the unsupervised nature of topic models, which prohibits any user guidance on the results of a model. In this paper, we introduce a semi-supervised method called topic supervised non-negative matrix factorization (TS-NMF) that enables the user to provide labeled example documents to promote the discovery of more meaningful semantic structure of a corpus. In this way, the results of TS-NMF better match the intuition and desired labeling of the user. The core of TS-NMF relies on solving a non-convex optimization problem for which we derive an iterative algorithm that is shown to be monotonic and convergent to a local optimum. We demonstrate the practical utility of TS-NMF on the Reuters and PubMed corpora, and find that TS-NMF is especially useful for conceptual or broad topics, where topic key terms are not well understood. Although identifying an optimal latent structure for the data is not a primary objective of the proposed approach, we find that TS-NMF achieves higher weighted Jaccard similarity scores than the contemporary methods, (unsupervised) NMF and latent Dirichlet allocation, at supervision rates as low as 10% to 20%.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05087",
        "title": "Plan, Attend, Generate: Character-level Neural Machine Translation with Planning in the Decoder",
        "authors": [
            "Caglar Gulcehre",
            "Francis Dutil",
            "Adam Trischler",
            "Yoshua Bengio"
        ],
        "abstract": "We investigate the integration of a planning mechanism into an encoder-decoder architecture with an explicit alignment for character-level machine translation. We develop a model that plans ahead when it computes alignments between the source and target sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the strategic attentive reader and writer (STRAW) model. Our proposed model is end-to-end trainable with fully differentiable operations. We show that it outperforms a strong baseline on three character-level decoder neural machine translation on WMT'15 corpus. Our analysis demonstrates that our model can compute qualitatively intuitive alignments and achieves superior performance with fewer parameters.\n    ",
        "submission_date": "2017-06-13T00:00:00",
        "last_modified_date": "2017-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05089",
        "title": "Number game",
        "authors": [
            "Go Sugimoto"
        ],
        "abstract": "CLARIN (Common Language Resources and Technology Infrastructure) is regarded as one of the most important European research infrastructures, offering and promoting a wide array of useful services for (digital) research in linguistics and humanities. However, the assessment of the users for its core technical development has been highly limited, therefore, it is unclear if the community is thoroughly aware of the status-quo of the growing infrastructure. In addition, CLARIN does not seem to be fully materialised marketing and business plans and strategies despite its strong technical assets. This article analyses the web traffic of the Virtual Language Observatory, one of the main web applications of CLARIN and a symbol of pan-European re-search cooperation, to evaluate the users and performance of the service in a transparent and scientific way. It is envisaged that the paper can raise awareness of the pressing issues on objective and transparent operation of the infrastructure though Open Evaluation, and the synergy between marketing and technical development. It also investigates the \"science of web analytics\" in an attempt to document the research process for the purpose of reusability and reproducibility, thus to find universal lessons for the use of a web analytics, rather than to merely produce a statistical report of a particular website which loses its value outside its context.\n    ",
        "submission_date": "2017-06-14T00:00:00",
        "last_modified_date": "2017-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05111",
        "title": "A Mixture Model for Learning Multi-Sense Word Embeddings",
        "authors": [
            "Dai Quoc Nguyen",
            "Dat Quoc Nguyen",
            "Ashutosh Modi",
            "Stefan Thater",
            "Manfred Pinkal"
        ],
        "abstract": "Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks.\n    ",
        "submission_date": "2017-06-15T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05122",
        "title": "Bib2vec: An Embedding-based Search System for Bibliographic Information",
        "authors": [
            "Takuma Yoneda",
            "Koki Mori",
            "Makoto Miwa",
            "Yutaka Sasaki"
        ],
        "abstract": "We propose a novel embedding model that represents relationships among several elements in bibliographic information with high representation ability and flexibility. Based on this model, we present a novel search system that shows the relationships among the elements in the ACL Anthology Reference Corpus. The evaluation results show that our model can achieve a high prediction ability and produce reasonable search results.\n    ",
        "submission_date": "2017-06-16T00:00:00",
        "last_modified_date": "2018-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05140",
        "title": "An Automatic Approach for Document-level Topic Model Evaluation",
        "authors": [
            "Shraey Bhatia",
            "Jey Han Lau",
            "Timothy Baldwin"
        ],
        "abstract": "Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.\n    ",
        "submission_date": "2017-06-16T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05565",
        "title": "Towards Neural Phrase-based Machine Translation",
        "authors": [
            "Po-Sen Huang",
            "Chong Wang",
            "Sitao Huang",
            "Dengyong Zhou",
            "Li Deng"
        ],
        "abstract": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.\n    ",
        "submission_date": "2017-06-17T00:00:00",
        "last_modified_date": "2018-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05585",
        "title": "Accelerating Innovation Through Analogy Mining",
        "authors": [
            "Tom Hope",
            "Joel Chan",
            "Aniket Kittur",
            "Dafna Shahaf"
        ],
        "abstract": "The availability of large idea repositories (e.g., the U.S. patent database) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems. However, finding useful analogies in these large, messy, real-world repositories remains a persistent challenge for either human or automated methods. Previous approaches include costly hand-created databases that have high relational structure (e.g., predicate calculus representations) but are very sparse. Simpler machine-learning/information-retrieval similarity metrics can scale to large, natural-language datasets, but struggle to account for structural similarity, which is central to analogy. In this paper we explore the viability and value of learning simpler structural representations, specifically, \"problem schemas\", which specify the purpose of a product and the mechanisms by which it achieves that purpose. Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions. We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods. In an ideation experiment, analogies retrieved by our models significantly increased people's likelihood of generating creative ideas compared to analogies retrieved by traditional methods. Our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations.\n    ",
        "submission_date": "2017-06-17T00:00:00",
        "last_modified_date": "2017-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05674",
        "title": "Knowledge Transfer for Out-of-Knowledge-Base Entities: A Graph Neural Network Approach",
        "authors": [
            "Takuo Hamaguchi",
            "Hidekazu Oiwa",
            "Masashi Shimbo",
            "Yuji Matsumoto"
        ],
        "abstract": "Knowledge base completion (KBC) aims to predict missing information in a knowledge ",
        "submission_date": "2017-06-18T00:00:00",
        "last_modified_date": "2017-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05723",
        "title": "Detecting Large Concept Extensions for Conceptual Analysis",
        "authors": [
            "Louis Chartrand",
            "Jackie C.K. Cheung",
            "Mohamed Bouguessa"
        ],
        "abstract": "When performing a conceptual analysis of a concept, philosophers are interested in all forms of expression of a concept in a text---be it direct or indirect, explicit or implicit. In this paper, we experiment with topic-based methods of automating the detection of concept expressions in order to facilitate philosophical conceptual analysis. We propose six methods based on LDA, and evaluate them on a new corpus of court decision that we had annotated by experts and non-experts. Our results indicate that these methods can yield important improvements over the keyword heuristic, which is often used as a concept detection heuristic in many contexts. While more work remains to be done, this indicates that detecting concepts through topics can serve as a general-purpose method for at least some forms of concept expression that are not captured using naive keyword approaches.\n    ",
        "submission_date": "2017-06-18T00:00:00",
        "last_modified_date": "2017-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05765",
        "title": "An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation",
        "authors": [
            "Makoto Morishita",
            "Yusuke Oda",
            "Graham Neubig",
            "Koichiro Yoshino",
            "Katsuhito Sudoh",
            "Satoshi Nakamura"
        ],
        "abstract": "Training of neural machine translation (NMT) models usually uses mini-batches for efficiency purposes. During the mini-batched training process, it is necessary to pad shorter sentences in a mini-batch to be equal in length to the longest sentence therein for efficient computation. Previous work has noted that sorting the corpus based on the sentence length before making mini-batches reduces the amount of padding and increases the processing speed. However, despite the fact that mini-batch creation is an essential step in NMT training, widely used NMT toolkits implement disparate strategies for doing so, which have not been empirically validated or compared. This work investigates mini-batch creation strategies with experiments over two different datasets. Our results suggest that the choice of a mini-batch creation strategy has a large effect on NMT training and some length-based sorting strategies do not always work well compared with simple shuffling.\n    ",
        "submission_date": "2017-06-19T00:00:00",
        "last_modified_date": "2017-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06177",
        "title": "Topic Modeling for Classification of Clinical Reports",
        "authors": [
            "Efsun Sarioglu Kayi",
            "Kabir Yadav",
            "James M. Chamberlain",
            "Hyeong-Ah Choi"
        ],
        "abstract": "Electronic health records (EHRs) contain important clinical information about patients. Efficient and effective use of this information could supplement or even replace manual chart review as a means of studying and improving the quality and safety of healthcare delivery. However, some of these clinical data are in the form of free text and require pre-processing before use in automated systems. A common free text data source is radiology reports, typically dictated by radiologists to explain their interpretations. We sought to demonstrate machine learning classification of computed tomography (CT) imaging reports into binary outcomes, i.e. positive and negative for fracture, using regular text classification and classifiers based on topic modeling. Topic modeling provides interpretable themes (topic distributions) in reports, a representation that is more compact than the commonly used bag-of-words representation and can be processed faster than raw text in subsequent automated processes. We demonstrate new classifiers based on this topic modeling representation of the reports. Aggregate topic classifier (ATC) and confidence-based topic classifier (CTC) use a single topic that is determined from the training dataset based on different measures to classify the reports on the test dataset. Alternatively, similarity-based topic classifier (STC) measures the similarity between the reports' topic distributions to determine the predicted class. Our proposed topic modeling-based classifier systems are shown to be competitive with existing text classification techniques and provides an efficient and interpretable representation.\n    ",
        "submission_date": "2017-06-19T00:00:00",
        "last_modified_date": "2017-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06210",
        "title": "Sub-domain Modelling for Dialogue Management with Hierarchical Reinforcement Learning",
        "authors": [
            "Pawe\u0142 Budzianowski",
            "Stefan Ultes",
            "Pei-Hao Su",
            "Nikola Mrk\u0161i\u0107",
            "Tsung-Hsien Wen",
            "I\u00f1igo Casanueva",
            "Lina Rojas-Barahona",
            "Milica Ga\u0161i\u0107"
        ],
        "abstract": "Human conversation is inherently complex, often spanning many different topics/domains. This makes policy learning for dialogue systems very challenging. Standard flat reinforcement learning methods do not provide an efficient framework for modelling such dialogues. In this paper, we focus on the under-explored problem of multi-domain dialogue management. First, we propose a new method for hierarchical reinforcement learning using the option framework. Next, we show that the proposed architecture learns faster and arrives at a better policy than the existing flat ones do. Moreover, we show how pretrained policies can be adapted to more complex systems with an additional set of new actions. In doing that, we show that our approach has the potential to facilitate policy optimisation for more sophisticated multi-domain dialogue systems.\n    ",
        "submission_date": "2017-06-19T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06363",
        "title": "Improving text classification with vectors of reduced precision",
        "authors": [
            "Krzysztof Wr\u00f3bel",
            "Maciej Wielgosz",
            "Marcin Pietro\u0144",
            "Micha\u0142 Karwatowski",
            "Aleksander Smywi\u0144ski-Pohl"
        ],
        "abstract": "This paper presents the analysis of the impact of a floating-point number precision reduction on the quality of text classification. The precision reduction of the vectors representing the data (e.g. TF-IDF representation in our case) allows for a decrease of computing time and memory footprint on dedicated hardware platforms. The impact of precision reduction on the classification quality was performed on 5 corpora, using 4 different classifiers. Also, dimensionality reduction was taken into account. Results indicate that the precision reduction improves classification accuracy for most cases (up to 25% of error reduction). In general, the reduction from 64 to 4 bits gives the best scores and ensures that the results will not be worse than with the full floating-point representation.\n    ",
        "submission_date": "2017-06-20T00:00:00",
        "last_modified_date": "2017-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06415",
        "title": "THUMT: An Open Source Toolkit for Neural Machine Translation",
        "authors": [
            "Jiacheng Zhang",
            "Yanzhuo Ding",
            "Shiqi Shen",
            "Yong Cheng",
            "Maosong Sun",
            "Huanbo Luan",
            "Yang Liu"
        ],
        "abstract": "This paper introduces THUMT, an open-source toolkit for neural machine translation (NMT) developed by the Natural Language Processing Group at Tsinghua University. THUMT implements the standard attention-based encoder-decoder framework on top of Theano and supports three training criteria: maximum likelihood estimation, minimum risk training, and semi-supervised training. It features a visualization tool for displaying the relevance between hidden states in neural networks and contextual words, which helps to analyze the internal workings of NMT. Experiments on Chinese-English datasets show that THUMT using minimum risk training significantly outperforms GroundHog, a state-of-the-art toolkit for NMT.\n    ",
        "submission_date": "2017-06-20T00:00:00",
        "last_modified_date": "2017-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06428",
        "title": "An online sequence-to-sequence model for noisy speech recognition",
        "authors": [
            "Chung-Cheng Chiu",
            "Dieterich Lawson",
            "Yuping Luo",
            "George Tucker",
            "Kevin Swersky",
            "Ilya Sutskever",
            "Navdeep Jaitly"
        ],
        "abstract": "Generative models have long been the dominant approach for speech recognition. The success of these models however relies on the use of sophisticated recipes and complicated machinery that is not easily accessible to non-practitioners. Recent innovations in Deep Learning have given rise to an alternative - discriminative models called Sequence-to-Sequence models, that can almost match the accuracy of state of the art generative models. While these models are easy to train as they can be trained end-to-end in a single step, they have a practical limitation that they can only be used for offline recognition. This is because the models require that the entirety of the input sequence be available at the beginning of inference, an assumption that is not valid for instantaneous speech recognition. To address this problem, online sequence-to-sequence models were recently introduced. These models are able to start producing outputs as data arrives, and the model feels confident enough to output partial transcripts. These models, like sequence-to-sequence are causal - the output produced by the model until any time, $t$, affects the features that are computed subsequently. This makes the model inherently more powerful than generative models that are unable to change features that are computed from the data. This paper highlights two main contributions - an improvement to online sequence-to-sequence model training, and its application to noisy settings with mixed speech from two speakers.\n    ",
        "submission_date": "2017-06-16T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06542",
        "title": "Extract with Order for Coherent Multi-Document Summarization",
        "authors": [
            "Mir Tafseer Nayeem",
            "Yllias Chali"
        ],
        "abstract": "In this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a model to tackle summary coherence for increasing readability. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2020-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06551",
        "title": "Grounded Language Learning in a Simulated 3D World",
        "authors": [
            "Karl Moritz Hermann",
            "Felix Hill",
            "Simon Green",
            "Fumin Wang",
            "Ryan Faulkner",
            "Hubert Soyer",
            "David Szepesvari",
            "Wojciech Marian Czarnecki",
            "Max Jaderberg",
            "Denis Teplyashin",
            "Marcus Wainwright",
            "Chris Apps",
            "Demis Hassabis",
            "Phil Blunsom"
        ],
        "abstract": "We are increasingly surrounded by artificially intelligent technology that takes decisions and executes actions on our behalf. This creates a pressing need for general means to communicate with, instruct and guide artificial agents, with human language the most compelling means for such communication. To achieve this in a scalable fashion, agents must be able to relate language to the world and to actions; that is, their understanding of language must be grounded and embodied. However, learning grounded language is a notoriously challenging problem in artificial intelligence research. Here we present an agent that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions. Trained via a combination of reinforcement and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. The agent's comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions. Moreover, the speed with which this agent learns new words increases as its semantic knowledge grows. This facility for generalising and bootstrapping semantic knowledge indicates the potential of the present approach for reconciling ambiguous natural language with the complexity of the physical world.\n    ",
        "submission_date": "2017-06-20T00:00:00",
        "last_modified_date": "2017-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06681",
        "title": "Graph-based Neural Multi-Document Summarization",
        "authors": [
            "Michihiro Yasunaga",
            "Rui Zhang",
            "Kshitijh Meelu",
            "Ayush Pareek",
            "Krishnan Srinivasan",
            "Dragomir Radev"
        ],
        "abstract": "We propose a neural multi-document summarization (MDS) system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences while avoiding redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.\n    ",
        "submission_date": "2017-06-20T00:00:00",
        "last_modified_date": "2017-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06714",
        "title": "Neural-based Natural Language Generation in Dialogue using RNN Encoder-Decoder with Semantic Aggregation",
        "authors": [
            "Van-Khanh Tran",
            "Le-Minh Nguyen"
        ],
        "abstract": "Natural language generation (NLG) is an important component in spoken dialogue systems. This paper presents a model called Encoder-Aggregator-Decoder which is an extension of an Recurrent Neural Network based Encoder-Decoder architecture. The proposed Semantic Aggregator consists of two components: an Aligner and a Refiner. The Aligner is a conventional attention calculated over the encoded input information, while the Refiner is another attention or gating mechanism stacked over the attentive Aligner in order to further select and aggregate the semantic elements. The proposed model can be jointly trained both sentence planning and surface realization to produce natural language utterances. The model was extensively assessed on four different NLG domains, in which the experimental results showed that the proposed generator consistently outperforms the previous methods on all the NLG domains.\n    ",
        "submission_date": "2017-06-21T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06749",
        "title": "Cross-language Learning with Adversarial Neural Networks: Application to Community Question Answering",
        "authors": [
            "Shafiq Joty",
            "Preslav Nakov",
            "Llu\u00eds M\u00e0rquez",
            "Israa Jaradat"
        ],
        "abstract": "We address the problem of cross-language adaptation for question-question similarity reranking in community question answering, with the objective to port a system trained on one input language to another input language given labeled training data for the first language and only unlabeled data for the second language. In particular, we propose to use adversarial training of neural networks to learn high-level features that are discriminative for the main learning task, and at the same time are invariant across the input languages. The evaluation results show sizable improvements for our cross-language adversarial neural network (CLANN) model over a strong non-adversarial system.\n    ",
        "submission_date": "2017-06-21T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06802",
        "title": "JaTeCS an open-source JAva TExt Categorization System",
        "authors": [
            "Andrea Esuli",
            "Tiziano Fagni",
            "Alejandro Moreo Fernandez"
        ],
        "abstract": "JaTeCS is an open source Java library that supports research on automatic text categorization and other related problems, such as ordinal regression and quantification, which are of special interest in opinion mining applications. It covers all the steps of an experimental activity, from reading the corpus to the evaluation of the experimental results. As JaTeCS is focused on text as the main input data, it provides the user with many text-dedicated tools, e.g.: data readers for many formats, including the most commonly used text corpora and lexical resources, natural language processing tools, multi-language support, methods for feature selection and weighting, the implementation of many machine learning algorithms as well as wrappers for well-known external software (e.g., SVM_light) which enable their full control from code. JaTeCS support its expansion by abstracting through interfaces many of the typical tools and procedures used in text processing tasks. The library also provides a number of \"template\" implementations of typical experimental setups (e.g., train-test, k-fold validation, grid-search optimization, randomized runs) which enable fast realization of experiments just by connecting the templates with data readers, learning algorithms and evaluation measures.\n    ",
        "submission_date": "2017-06-21T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06894",
        "title": "Stance Detection in Turkish Tweets",
        "authors": [
            "Dilek K\u00fc\u00e7\u00fck"
        ],
        "abstract": "Stance detection is a classification problem in natural language processing where for a text and target pair, a class result from the set {Favor, Against, Neither} is expected. It is similar to the sentiment analysis problem but instead of the sentiment of the text author, the stance expressed for a particular target is investigated in stance detection. In this paper, we present a stance detection tweet data set for Turkish comprising stance annotations of these tweets for two popular sports clubs as targets. Additionally, we provide the evaluation results of SVM classifiers for each target on this data set, where the classifiers use unigram, bigram, and hashtag features. This study is significant as it presents one of the initial stance detection data sets proposed so far and the first one for Turkish language, to the best of our knowledge. The data set and the evaluation results of the corresponding SVM-based approaches will form plausible baselines for the comparison of future studies on stance detection.\n    ",
        "submission_date": "2017-06-21T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06896",
        "title": "Effective Spoken Language Labeling with Deep Recurrent Neural Networks",
        "authors": [
            "Marco Dinarelli",
            "Yoann Dupont",
            "Isabelle Tellier"
        ],
        "abstract": "Understanding spoken language is a highly complex problem, which can be decomposed into several simpler tasks. In this paper, we focus on Spoken Language Understanding (SLU), the module of spoken dialog systems responsible for extracting a semantic interpretation from the user utterance. The task is treated as a labeling problem. In the past, SLU has been performed with a wide variety of probabilistic models. The rise of neural networks, in the last couple of years, has opened new interesting research directions in this domain. Recurrent Neural Networks (RNNs) in particular are able not only to represent several pieces of information as embeddings but also, thanks to their recurrent architecture, to encode as embeddings relatively long contexts. Such long contexts are in general out of reach for models previously used for SLU. In this paper we propose novel RNNs architectures for SLU which outperform previous ones. Starting from a published idea as base block, we design new deep RNNs achieving state-of-the-art results on two widely used corpora for SLU: ATIS (Air Traveling Information System), in English, and MEDIA (Hotel information and reservation in France), in French.\n    ",
        "submission_date": "2017-06-20T00:00:00",
        "last_modified_date": "2017-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06987",
        "title": "A Generative Model of Group Conversation",
        "authors": [
            "Hannah Morrison",
            "Chris Martens"
        ],
        "abstract": "Conversations with non-player characters (NPCs) in games are typically confined to dialogue between a human player and a virtual agent, where the conversation is initiated and controlled by the player. To create richer, more believable environments for players, we need conversational behavior to reflect initiative on the part of the NPCs, including conversations that include multiple NPCs who interact with one another as well as the player. We describe a generative computational model of group conversation between agents, an abstract simulation of discussion in a small group setting. We define conversational interactions in terms of rules for turn taking and interruption, as well as belief change, sentiment change, and emotional response, all of which are dependent on agent personality, context, and relationships. We evaluate our model using a parameterized expressive range analysis, observing correlations between simulation parameters and features of the resulting conversations. This analysis confirms, for example, that character personalities will predict how often they speak, and that heterogeneous groups of characters will generate more belief change.\n    ",
        "submission_date": "2017-06-21T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06996",
        "title": "Statistical Inferences for Polarity Identification in Natural Language",
        "authors": [
            "Nicolas Pr\u00f6llochs",
            "Stefan Feuerriegel",
            "Dirk Neumann"
        ],
        "abstract": "Information forms the basis for all human behavior, including the ubiquitous decision-making that people constantly perform in their every day lives. It is thus the mission of researchers to understand how humans process information to reach decisions. In order to facilitate this task, this work proposes a novel method of studying the reception of granular expressions in natural language. The approach utilizes LASSO regularization as a statistical tool to extract decisive words from textual content and draw statistical inferences based on the correspondence between the occurrences of words and an exogenous response variable. Accordingly, the method immediately suggests significant implications for social sciences and Information Systems research: everyone can now identify text segments and word choices that are statistically relevant to authors or readers and, based on this knowledge, test hypotheses from behavioral research. We demonstrate the contribution of our method by examining how authors communicate subjective information through narrative materials. This allows us to answer the question of which words to choose when communicating negative information. On the other hand, we show that investors trade not only upon facts in financial disclosures but are distracted by filler words and non-informative language. Practitioners - for example those in the fields of investor communications or marketing - can exploit our insights to enhance their writings based on the true perception of word choice.\n    ",
        "submission_date": "2017-06-21T00:00:00",
        "last_modified_date": "2018-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07179",
        "title": "RelNet: End-to-End Modeling of Entities & Relations",
        "authors": [
            "Trapit Bansal",
            "Arvind Neelakantan",
            "Andrew McCallum"
        ],
        "abstract": "We introduce RelNet: a new model for relational reasoning. RelNet is a memory augmented neural network which models entities as abstract memory slots and is equipped with an additional relational memory which models relations between all memory pairs. The model thus builds an abstract knowledge graph on the entities and relations present in a document which can then be used to answer questions about the document. It is trained end-to-end: only supervision to the model is in the form of correct answers to the questions. We test the model on the 20 bAbI question-answering tasks with 10k examples per task and find that it solves all the tasks with a mean error of 0.3%, achieving 0% error on 11 of the 20 tasks.\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2017-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07206",
        "title": "Explaining Recurrent Neural Network Predictions in Sentiment Analysis",
        "authors": [
            "Leila Arras",
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ],
        "abstract": "Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07238",
        "title": "Automatic Quality Estimation for ASR System Combination",
        "authors": [
            "Shahab Jalalvand",
            "Matteo Negri",
            "Daniele Falavigna",
            "Marco Matassoni",
            "Marco Turchi"
        ],
        "abstract": "Recognizer Output Voting Error Reduction (ROVER) has been widely used for system combination in automatic speech recognition (ASR). In order to select the most appropriate words to insert at each position in the output transcriptions, some ROVER extensions rely on critical information such as confidence scores and other ASR decoder features. This information, which is not always available, highly depends on the decoding process and sometimes tends to over estimate the real quality of the recognized words. In this paper we propose a novel variant of ROVER that takes advantage of ASR quality estimation (QE) for ranking the transcriptions at \"segment level\" instead of: i) relying on confidence scores, or ii) feeding ROVER with randomly ordered hypotheses. We first introduce an effective set of features to compensate for the absence of ASR decoder information. Then, we apply QE techniques to perform accurate hypothesis ranking at segment-level before starting the fusion process. The evaluation is carried out on two different tasks, in which we respectively combine hypotheses coming from independent ASR systems and multi-microphone recordings. In both tasks, it is assumed that the ASR decoder information is not available. The proposed approach significantly outperforms standard ROVER and it is competitive with two strong oracles that e xploit prior knowledge about the real quality of the hypotheses to be combined. Compared to standard ROVER, the abs olute WER improvements in the two evaluation scenarios range from 0.5% to 7.3%.\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2017-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07276",
        "title": "Jointly Learning Word Embeddings and Latent Topics",
        "authors": [
            "Bei Shi",
            "Wai Lam",
            "Shoaib Jameel",
            "Steven Schockaert",
            "Kwun Ping Lai"
        ],
        "abstract": "Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such \"two-step\" methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.\n    ",
        "submission_date": "2017-06-21T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07440",
        "title": "End-to-end Conversation Modeling Track in DSTC6",
        "authors": [
            "Chiori Hori",
            "Takaaki Hori"
        ],
        "abstract": "End-to-end training of neural networks is a promising approach to automatic construction of dialog systems using a human-to-human dialog corpus. Recently, Vinyals et al. tested neural conversation models using OpenSubtitles. Lowe et al. released the Ubuntu Dialogue Corpus for researching unstructured multi-turn dialogue systems. Furthermore, the approach has been extended to accomplish task oriented dialogs to provide information properly with natural conversation. For example, Ghazvininejad et al. proposed a knowledge grounded neural conversation model [3], where the research is aiming at combining conversational dialogs with task-oriented knowledge using unstructured data such as Twitter data for conversation and Foursquare data for external ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2018-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07503",
        "title": "Personalization in Goal-Oriented Dialog",
        "authors": [
            "Chaitanya K. Joshi",
            "Fei Mi",
            "Boi Faltings"
        ],
        "abstract": "The main goal of modeling human conversation is to create agents which can interact with people in both open-ended and goal-oriented scenarios. End-to-end trained neural dialog systems are an important line of research for such generalized dialog models as they do not resort to any situation-specific handcrafting of rules. However, incorporating personalization into such systems is a largely unexplored topic as there are no existing corpora to facilitate such work. In this paper, we present a new dataset of goal-oriented dialogs which are influenced by speaker profiles attached to them. We analyze the shortcomings of an existing end-to-end dialog system based on Memory Networks and propose modifications to the architecture which enable personalization. We also investigate personalization in dialog as a multi-task learning problem, and show that a single model which shares features among various profiles outperforms separate models for each profile.\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2017-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07518",
        "title": "Neural Machine Translation with Gumbel-Greedy Decoding",
        "authors": [
            "Jiatao Gu",
            "Daniel Jiwoong Im",
            "Victor O.K. Li"
        ],
        "abstract": "Previous neural machine translation models used some heuristic search algorithms (e.g., beam search) in order to avoid solving the maximum a posteriori problem over translation sentences at test time. In this paper, we propose the Gumbel-Greedy Decoding which trains a generative network to predict translation under a trained model. We solve such a problem using the Gumbel-Softmax reparameterization, which makes our generative network differentiable and trainable through standard stochastic gradient methods. We empirically demonstrate that our proposed model is effective for generating sequences of discrete words.\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2017-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07598",
        "title": "Named Entity Recognition with stack residual LSTM and trainable bias decoding",
        "authors": [
            "Quan Tran",
            "Andrew MacKinlay",
            "Antonio Jimeno Yepes"
        ],
        "abstract": "Recurrent Neural Network models are the state-of-the-art for Named Entity Recognition (NER). We present two innovations to improve the performance of these models. The first innovation is the introduction of residual connections between the Stacked Recurrent Neural Network model to address the degradation problem of deep neural networks. The second innovation is a bias decoding mechanism that allows the trained system to adapt to non-differentiable and externally computed objectives, such as the entity-based F-measure. Our work improves the state-of-the-art results for both Spanish and English languages on the standard train/development/test split of the CoNLL 2003 Shared Task NER dataset.\n    ",
        "submission_date": "2017-06-23T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07786",
        "title": "Comparison of Modified Kneser-Ney and Witten-Bell Smoothing Techniques in Statistical Language Model of Bahasa Indonesia",
        "authors": [
            "Ismail Rusli"
        ],
        "abstract": "Smoothing is one technique to overcome data sparsity in statistical language model. Although in its mathematical definition there is no explicit dependency upon specific natural language, different natures of natural languages result in different effects of smoothing techniques. This is true for Russian language as shown by Whittaker (1998). In this paper, We compared Modified Kneser-Ney and Witten-Bell smoothing techniques in statistical language model of Bahasa Indonesia. We used train sets of totally 22M words that we extracted from Indonesian version of Wikipedia. As far as we know, this is the largest train set used to build statistical language model for Bahasa Indonesia. The experiments with 3-gram, 5-gram, and 7-gram showed that Modified Kneser-Ney consistently outperforms Witten-Bell smoothing technique in term of perplexity values. It is interesting to note that our experiments showed 5-gram model for Modified Kneser-Ney smoothing technique outperforms that of 7-gram. Meanwhile, Witten-Bell smoothing is consistently improving over the increase of n-gram order.\n    ",
        "submission_date": "2017-06-23T00:00:00",
        "last_modified_date": "2017-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07905",
        "title": "Encoder-Decoder Shift-Reduce Syntactic Parsing",
        "authors": [
            "Jiangming Liu",
            "Yue Zhang"
        ],
        "abstract": "Starting from NMT, encoder-decoder neu- ral networks have been used for many NLP problems. Graph-based models and transition-based models borrowing the en- coder components achieve state-of-the-art performance on dependency parsing and constituent parsing, respectively. How- ever, there has not been work empirically studying the encoder-decoder neural net- works for transition-based parsing. We apply a simple encoder-decoder to this end, achieving comparable results to the parser of Dyer et al. (2015) on standard de- pendency parsing, and outperforming the parser of Vinyals et al. (2015) on con- stituent parsing.\n    ",
        "submission_date": "2017-06-24T00:00:00",
        "last_modified_date": "2017-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08032",
        "title": "A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking",
        "authors": [
            "Huy Nguyen",
            "Minh-Le Nguyen"
        ],
        "abstract": "This paper introduces a novel deep learning framework including a lexicon-based approach for sentence-level prediction of sentiment label distribution. We propose to first apply semantic rules and then use a Deep Convolutional Neural Network (DeepCNN) for character-level embeddings in order to increase information for word-level embedding. After that, a Bidirectional Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking.\n    ",
        "submission_date": "2017-06-25T00:00:00",
        "last_modified_date": "2017-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08160",
        "title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context",
        "authors": [
            "Shyam Upadhyay",
            "Kai-Wei Chang",
            "Matt Taddy",
            "Adam Kalai",
            "James Zou"
        ],
        "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields comparable performance to a state of the art mono-lingual model trained on five times more training data.\n    ",
        "submission_date": "2017-06-25T00:00:00",
        "last_modified_date": "2017-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08162",
        "title": "Automated text summarisation and evidence-based medicine: A survey of two domains",
        "authors": [
            "Abeed Sarker",
            "Diego Molla",
            "Cecile Paris"
        ],
        "abstract": "The practice of evidence-based medicine (EBM) urges medical practitioners to utilise the latest research evidence when making clinical decisions. Because of the massive and growing volume of published research on various medical topics, practitioners often find themselves overloaded with information. As such, natural language processing research has recently commenced exploring techniques for performing medical domain-specific automated text summarisation (ATS) techniques-- targeted towards the task of condensing large medical texts. However, the development of effective summarisation techniques for this task requires cross-domain knowledge. We present a survey of EBM, the domain-specific needs for EBM, automated summarisation techniques, and how they have been applied hitherto. We envision that this survey will serve as a first resource for the development of future operational text summarisation techniques for EBM.\n    ",
        "submission_date": "2017-06-25T00:00:00",
        "last_modified_date": "2017-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08186",
        "title": "Automatic Synonym Discovery with Knowledge Bases",
        "authors": [
            "Meng Qu",
            "Xiang Ren",
            "Jiawei Han"
        ],
        "abstract": "Recognizing entity synonyms from text has become a crucial task in many entity-leveraging applications. However, discovering entity synonyms from domain-specific text corpora (e.g., news articles, scientific papers) is rather challenging. Current systems take an entity name string as input to find out other names that are synonymous, ignoring the fact that often times a name string can refer to multiple entities (e.g., \"apple\" could refer to both Apple Inc and the fruit apple). Moreover, most existing methods require training data manually created by domain experts to construct supervised-learning systems. In this paper, we study the problem of automatic synonym discovery with knowledge bases, that is, identifying synonyms for knowledge base entities in a given domain-specific corpus. The manually-curated synonyms for each entity stored in a knowledge base not only form a set of name strings to disambiguate the meaning for each other, but also can serve as \"distant\" supervision to help determine important features for the task. We propose a novel framework, called DPE, to integrate two kinds of mutually-complementing signals for synonym discovery, i.e., distributional features based on corpus-level statistics and textual patterns based on local contexts. In particular, DPE jointly optimizes the two kinds of signals in conjunction with distant supervision, so that they can mutually enhance each other in the training stage. At the inference stage, both signals will be utilized to discover synonyms for the given entities. Experimental results prove the effectiveness of the proposed framework.\n    ",
        "submission_date": "2017-06-25T00:00:00",
        "last_modified_date": "2017-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08198",
        "title": "English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor",
        "authors": [
            "Yukio Matsumura",
            "Takayuki Sato",
            "Mamoru Komachi"
        ],
        "abstract": "Neural machine translation (NMT) has recently become popular in the field of machine translation. However, NMT suffers from the problem of repeating or missing words in the translation. To address this problem, Tu et al. (2017) proposed an encoder-decoder-reconstructor framework for NMT using back-translation. In this method, they selected the best forward translation model in the same manner as Bahdanau et al. (2015), and then trained a bi-directional translation model as fine-tuning. Their experiments show that it offers significant improvement in BLEU scores in Chinese-English translation task. We confirm that our re-implementation also shows the same tendency and alleviates the problem of repeating and missing words in the translation on a English-Japanese task too. In addition, we evaluate the effectiveness of pre-training by comparing it with a jointly-trained model of forward translation and back-translation.\n    ",
        "submission_date": "2017-06-26T00:00:00",
        "last_modified_date": "2017-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08476",
        "title": "Generative Encoder-Decoder Models for Task-Oriented Spoken Dialog Systems with Chatting Capability",
        "authors": [
            "Tiancheng Zhao",
            "Allen Lu",
            "Kyusong Lee",
            "Maxine Eskenazi"
        ],
        "abstract": "Generative encoder-decoder models offer great promise in developing domain-general dialog systems. However, they have mainly been applied to open-domain conversations. This paper presents a practical and novel framework for building task-oriented dialog systems based on encoder-decoder models. This framework enables encoder-decoder models to accomplish slot-value independent decision-making and interact with external databases. Moreover, this paper shows the flexibility of the proposed method by interleaving chatting capability with a slot-filling system for better out-of-domain recovery. The models were trained on both real-user data from a bus information system and human-human chat data. Results show that the proposed framework achieves good performance in both offline evaluation metrics and in task success rate with human users.\n    ",
        "submission_date": "2017-06-26T00:00:00",
        "last_modified_date": "2017-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08502",
        "title": "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog",
        "authors": [
            "Satwik Kottur",
            "Jos\u00e9 M.F. Moura",
            "Stefan Lee",
            "Dhruv Batra"
        ],
        "abstract": "A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision!\n",
        "submission_date": "2017-06-26T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08568",
        "title": "Neural Question Answering at BioASQ 5B",
        "authors": [
            "Georg Wiese",
            "Dirk Weissenborn",
            "Mariana Neves"
        ],
        "abstract": "This paper describes our submission to the 2017 BioASQ challenge. We participated in Task B, Phase B which is concerned with biomedical question answering (QA). We focus on factoid and list question, using an extractive QA model, that is, we restrict our system to output substrings of the provided text snippets. At the core of our system, we use FastQA, a state-of-the-art neural QA system. We extended it with biomedical word embeddings and changed its answer layer to be able to answer list questions in addition to factoid questions. We pre-trained the model on a large-scale open-domain QA dataset, SQuAD, and then fine-tuned the parameters on the BioASQ training set. With our approach, we achieve state-of-the-art results on factoid questions and competitive results on list questions.\n    ",
        "submission_date": "2017-06-26T00:00:00",
        "last_modified_date": "2017-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08609",
        "title": "The Minor Fall, the Major Lift: Inferring Emotional Valence of Musical Chords through Lyrics",
        "authors": [
            "Artemy Kolchinsky",
            "Nakul Dhande",
            "Kengjeun Park",
            "Yong-Yeol Ahn"
        ],
        "abstract": "We investigate the association between musical chords and lyrics by analyzing a large dataset of user-contributed guitar tablatures. Motivated by the idea that the emotional content of chords is reflected in the words used in corresponding lyrics, we analyze associations between lyrics and chord categories. We also examine the usage patterns of chords and lyrics in different musical genres, historical eras, and geographical regions. Our overall results confirms a previously known association between Major chords and positive valence. We also report a wide variation in this association across regions, genres, and eras. Our results suggest possible existence of different emotional associations for other types of chords.\n    ",
        "submission_date": "2017-06-26T00:00:00",
        "last_modified_date": "2017-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08683",
        "title": "Memory-augmented Chinese-Uyghur Neural Machine Translation",
        "authors": [
            "Shiyue Zhang",
            "Gulnigar Mahmut",
            "Dong Wang",
            "Askar Hamdulla"
        ],
        "abstract": "Neural machine translation (NMT) has achieved notable performance recently. However, this approach has not been widely applied to the translation task between Chinese and Uyghur, partly due to the limited parallel data resource and the large proportion of rare words caused by the agglutinative nature of Uyghur. In this paper, we collect ~200,000 sentence pairs and show that with this middle-scale database, an attention-based NMT can perform very well on Chinese-Uyghur/Uyghur-Chinese translation. To tackle rare words, we propose a novel memory structure to assist the NMT inference. Our experiments demonstrated that the memory-augmented NMT (M-NMT) outperforms both the vanilla NMT and the phrase-based statistical machine translation (SMT). Interestingly, the memory structure provides an elegant way for dealing with words that are out of vocabulary.\n    ",
        "submission_date": "2017-06-27T00:00:00",
        "last_modified_date": "2017-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09031",
        "title": "CoNLL-SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection in 52 Languages",
        "authors": [
            "Ryan Cotterell",
            "Christo Kirov",
            "John Sylak-Glassman",
            "G\u00e9raldine Walther",
            "Ekaterina Vylomova",
            "Patrick Xia",
            "Manaal Faruqui",
            "Sandra K\u00fcbler",
            "David Yarowsky",
            "Jason Eisner",
            "Mans Hulden"
        ],
        "abstract": "The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation required systems to be trained and tested in each of 52 typologically diverse languages. In sub-task 1, submitted systems were asked to predict a specific inflected form of a given lemma. In sub-task 2, systems were given a lemma and some of its specific inflected forms, and asked to complete the inflectional paradigm by predicting all of the remaining inflected forms. Both sub-tasks included high, medium, and low-resource conditions. Sub-task 1 received 24 system submissions, while sub-task 2 received 3 system submissions. Following the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared task, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in disjoint sets of inflected forms being predicted correctly, suggesting that there is room for future improvement.\n    ",
        "submission_date": "2017-06-27T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09147",
        "title": "Named Entity Disambiguation for Noisy Text",
        "authors": [
            "Yotam Eshel",
            "Noam Cohen",
            "Kira Radinsky",
            "Shaul Markovitch",
            "Ikuya Yamada",
            "Omer Levy"
        ],
        "abstract": "We address the task of Named Entity Disambiguation (NED) for noisy text. We present WikilinksNED, a large-scale NED dataset of text fragments from the web, which is significantly noisier and more challenging than existing news-based datasets. To capture the limited and noisy local context surrounding each mention, we design a neural model and train it with a novel method for sampling informative negative examples. We also describe a new way of initializing word and entity embeddings that significantly improves performance. Our model significantly outperforms existing state-of-the-art methods on WikilinksNED while achieving comparable performance on a smaller newswire dataset.\n    ",
        "submission_date": "2017-06-28T00:00:00",
        "last_modified_date": "2017-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09254",
        "title": "The E2E Dataset: New Challenges For End-to-End Generation",
        "authors": [
            "Jekaterina Novikova",
            "Ond\u0159ej Du\u0161ek",
            "Verena Rieser"
        ],
        "abstract": "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.\n    ",
        "submission_date": "2017-06-28T00:00:00",
        "last_modified_date": "2017-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09335",
        "title": "Generating Appealing Brand Names",
        "authors": [
            "Gaurush Hiranandani",
            "Pranav Maneriker",
            "Harsh Jhamtani"
        ],
        "abstract": "Providing appealing brand names to newly launched products, newly formed companies or for renaming existing companies is highly important as it can play a crucial role in deciding its success or failure. In this work, we propose a computational method to generate appealing brand names based on the description of such entities. We use quantitative scores for readability, pronounceability, memorability and uniqueness of the generated names to rank order them. A set of diverse appealing names is recommended to the user for the brand naming task. Experimental results show that the names generated by our approach are more appealing than names which prior approaches and recruited humans could come up.\n    ",
        "submission_date": "2017-06-28T00:00:00",
        "last_modified_date": "2017-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09433",
        "title": "Data-driven Natural Language Generation: Paving the Road to Success",
        "authors": [
            "Jekaterina Novikova",
            "Ond\u0159ej Du\u0161ek",
            "Verena Rieser"
        ],
        "abstract": "We argue that there are currently two major bottlenecks to the commercial use of statistical machine learning approaches for natural language generation (NLG): (a) The lack of reliable automatic evaluation metrics for NLG, and (b) The scarcity of high quality in-domain corpora. We address the first problem by thoroughly analysing current evaluation metrics and motivating the need for a new, more reliable metric. The second problem is addressed by presenting a novel framework for developing and evaluating a high quality corpus for NLG training.\n    ",
        "submission_date": "2017-06-28T00:00:00",
        "last_modified_date": "2017-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09453",
        "title": "Toward Computation and Memory Efficient Neural Network Acoustic Models with Binary Weights and Activations",
        "authors": [
            "Liang Lu"
        ],
        "abstract": "Neural network acoustic models have significantly advanced state of the art speech recognition over the past few years. However, they are usually computationally expensive due to the large number of matrix-vector multiplications and nonlinearity operations. Neural network models also require significant amounts of memory for inference because of the large model size. For these two reasons, it is challenging to deploy neural network based speech recognizers on resource-constrained platforms such as embedded devices. This paper investigates the use of binary weights and activations for computation and memory efficient neural network acoustic models. Compared to real-valued weight matrices, binary weights require much fewer bits for storage, thereby cutting down the memory footprint. Furthermore, with binary weights or activations, the matrix-vector multiplications are turned into addition and subtraction operations, which are computationally much faster and more energy efficient for hardware platforms. In this paper, we study the applications of binary weights and activations for neural network acoustic modeling, reporting encouraging results on the WSJ and AMI corpora.\n    ",
        "submission_date": "2017-06-28T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09528",
        "title": "Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a Syntactic Scaffold",
        "authors": [
            "Swabha Swayamdipta",
            "Sam Thomson",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We present a new, efficient frame-semantic parser that labels semantic arguments to FrameNet predicates. Built using an extension to the segmental RNN that emphasizes recall, our basic system achieves competitive performance without any calls to a syntactic parser. We then introduce a method that uses phrase-syntactic annotations from the Penn Treebank during training only, through a multitask objective; no parsing is required at training or test time. This \"syntactic scaffold\" offers a cheaper alternative to traditional syntactic pipelining, and achieves state-of-the-art performance.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09562",
        "title": "Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles",
        "authors": [
            "Francis Ferraro",
            "Adam Poliak",
            "Ryan Cotterell",
            "Benjamin Van Durme"
        ],
        "abstract": "We study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10% gains over baselines.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09569",
        "title": "Recurrent neural networks with specialized word embeddings for health-domain named-entity recognition",
        "authors": [
            "Inigo Jauregi Unanue",
            "Ehsan Zare Borzeshi",
            "Massimo Piccardi"
        ],
        "abstract": "Background. Previous state-of-the-art systems on Drug Name Recognition (DNR) and Clinical Concept Extraction (CCE) have focused on a combination of text \"feature engineering\" and conventional machine learning algorithms such as conditional random fields and support vector machines. However, developing good features is inherently heavily time-consuming. Conversely, more modern machine learning approaches such as recurrent neural networks (RNNs) have proved capable of automatically learning effective features from either random assignments or automated word \"embeddings\". Objectives. (i) To create a highly accurate DNR and CCE system that avoids conventional, time-consuming feature engineering. (ii) To create richer, more specialized word embeddings by using health domain datasets such as MIMIC-III. (iii) To evaluate our systems over three contemporary datasets. Methods. Two deep learning methods, namely the Bidirectional LSTM and the Bidirectional LSTM-CRF, are evaluated. A CRF model is set as the baseline to compare the deep learning systems to a traditional machine learning approach. The same features are used for all the models. Results. We have obtained the best results with the Bidirectional LSTM-CRF model, which has outperformed all previously proposed systems. The specialized embeddings have helped to cover unusual words in DDI-DrugBank and DDI-MedLine, but not in the 2010 i2b2/VA IRB Revision dataset. Conclusion. We present a state-of-the-art system for DNR and CCE. Automated word embeddings has allowed us to avoid costly feature engineering and achieve higher accuracy. Nevertheless, the embeddings need to be retrained over datasets that are adequate for the domain, in order to adequately cover the domain-specific vocabulary.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2018-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09673",
        "title": "Improving Distributed Representations of Tweets - Present and Future",
        "authors": [
            "Ganesh J"
        ],
        "abstract": "Unsupervised representation learning for tweets is an important research field which helps in solving several business applications such as sentiment analysis, hashtag prediction, paraphrase detection and microblog ranking. A good tweet representation learning model must handle the idiosyncratic nature of tweets which poses several challenges such as short length, informal words, unusual grammar and misspellings. However, there is a lack of prior work which surveys the representation learning models with a focus on tweets. In this work, we organize the models based on its objective function which aids the understanding of the literature. We also provide interesting future directions, which we believe are fruitful in advancing this field by building high-quality tweet representation learning models.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09733",
        "title": "Stronger Baselines for Trustable Results in Neural Machine Translation",
        "authors": [
            "Michael Denkowski",
            "Graham Neubig"
        ],
        "abstract": "Interest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios. New research regularly introduces architectural and algorithmic improvements that lead to significant gains over \"vanilla\" NMT implementations. However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-theart production and shared-task systems. As a result, it is often difficult to determine whether improvements from research will carry over to systems deployed for real-world use. In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems. Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed. We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen. This indicates that choosing a strong baseline is crucial for reporting reliable experimental results.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09742",
        "title": "AP17-OLR Challenge: Data, Plan, and Baseline",
        "authors": [
            "Zhiyuan Tang",
            "Dong Wang",
            "Yixiang Chen",
            "Qing Chen"
        ],
        "abstract": "We present the data profile and the evaluation plan of the second oriental language recognition (OLR) challenge AP17-OLR. Compared to the event last year (AP16-OLR), the new challenge involves more languages and focuses more on short utterances. The data is offered by SpeechOcean and the NSFC M2ASR project. Two types of baselines are constructed to assist the participants, one is based on the i-vector model and the other is based on various neural networks. We report the baseline results evaluated with various metrics defined by the AP17-OLR evaluation plan and demonstrate that the combined database is a reasonable data resource for multilingual research. All the data is free for participants, and the Kaldi recipes for the baselines have been published online.\n    ",
        "submission_date": "2017-06-28T00:00:00",
        "last_modified_date": "2017-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09789",
        "title": "Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension",
        "authors": [
            "David Golub",
            "Po-Sen Huang",
            "Xiaodong He",
            "Li Deng"
        ],
        "abstract": "We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed SynNet with a pretrained model from the SQuAD dataset on the challenging NewsQA dataset, we achieve an F1 measure of 44.3% with a single model and 46.6% with an ensemble, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline of 7.6%, without use of provided annotations.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2017-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09799",
        "title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation",
        "authors": [
            "Shikhar Sharma",
            "Layla El Asri",
            "Hannes Schulz",
            "Jeremie Zumer"
        ],
        "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09856",
        "title": "Automatic Mapping of French Discourse Connectives to PDTB Discourse Relations",
        "authors": [
            "Majid Laali",
            "Leila Kosseim"
        ],
        "abstract": "In this paper, we present an approach to exploit phrase tables generated by statistical machine translation in order to map French discourse connectives to discourse relations. Using this approach, we created ConcoLeDisCo, a lexicon of French discourse connectives and their PDTB relations. When evaluated against LEXCONN, ConcoLeDisCo achieves a recall of 0.81 and an Average Precision of 0.68 for the Concession and Condition relations.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00079",
        "title": "Synthetic Data for Neural Machine Translation of Spoken-Dialects",
        "authors": [
            "Hany Hassan",
            "Mostafa Elaraby",
            "Ahmed Tawfik"
        ],
        "abstract": "In this paper, we introduce a novel approach to generate synthetic data for training Neural Machine Translation systems. The proposed approach transforms a given parallel corpus between a written language and a target language to a parallel corpus between a spoken dialect variant and the target language. Our approach is language independent and can be used to generate data for any variant of the source language such as slang or spoken dialect or even for a different language that is closely related to the source language.\n",
        "submission_date": "2017-07-01T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00110",
        "title": "Efficient Attention using a Fixed-Size Memory Representation",
        "authors": [
            "Denny Britz",
            "Melody Y. Guan",
            "Minh-Thang Luong"
        ],
        "abstract": "The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments.\n    ",
        "submission_date": "2017-07-01T00:00:00",
        "last_modified_date": "2017-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00117",
        "title": "SAM: Semantic Attribute Modulation for Language Modeling and Style Variation",
        "authors": [
            "Wenbo Hu",
            "Lifeng Hua",
            "Lei Li",
            "Hang Su",
            "Tian Wang",
            "Ning Chen",
            "Bo Zhang"
        ],
        "abstract": "This paper presents a Semantic Attribute Modulation (SAM) for language modeling and style variation. The semantic attribute modulation includes various document attributes, such as titles, authors, and document categories. We consider two types of attributes, (title attributes and category attributes), and a flexible attribute selection scheme by automatically scoring them via an attribute attention mechanism. The semantic attributes are embedded into the hidden semantic space as the generation inputs. With the attributes properly harnessed, our proposed SAM can generate interpretable texts with regard to the input attributes. Qualitative analysis, including word semantic analysis and attention values, shows the interpretability of SAM. On several typical text datasets, we empirically demonstrate the superiority of the Semantic Attribute Modulated language model with different combinations of document attributes. Moreover, we present a style variation for the lyric generation using SAM, which shows a strong connection between the style variation and the semantic attributes.\n    ",
        "submission_date": "2017-07-01T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00130",
        "title": "Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management",
        "authors": [
            "Pei-Hao Su",
            "Pawel Budzianowski",
            "Stefan Ultes",
            "Milica Gasic",
            "Steve Young"
        ],
        "abstract": "Deep reinforcement learning (RL) methods have significant potential for dialogue policy optimisation. However, they suffer from a poor performance in the early stages of learning. This is especially problematic for on-line learning with real users. Two approaches are introduced to tackle this problem. Firstly, to speed up the learning process, two sample-efficient neural networks algorithms: trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER) are presented. For TRACER, the trust region helps to control the learning step size and avoid catastrophic model changes. For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence. Both models employ off-policy learning with experience replay to improve sample-efficiency. Secondly, to mitigate the cold start issue, a corpus of demonstration data is utilised to pre-train the models prior to on-line reinforcement learning. Combining these two approaches, we demonstrate a practical approach to learn deep RL-based dialogue policies and demonstrate their effectiveness in a task-oriented information seeking domain.\n    ",
        "submission_date": "2017-07-01T00:00:00",
        "last_modified_date": "2017-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00166",
        "title": "Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach",
        "authors": [
            "Liyuan Liu",
            "Xiang Ren",
            "Qi Zhu",
            "Shi Zhi",
            "Huan Gui",
            "Heng Ji",
            "Jiawei Han"
        ],
        "abstract": "Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art.\n    ",
        "submission_date": "2017-07-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00248",
        "title": "DAG-based Long Short-Term Memory for Neural Word Segmentation",
        "authors": [
            "Xinchi Chen",
            "Zhan Shi",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Neural word segmentation has attracted more and more research interests for its ability to alleviate the effort of feature engineering and utilize the external resource by the pre-trained character or word embeddings. In this paper, we propose a new neural model to incorporate the word-level information for Chinese word segmentation. Unlike the previous word-based models, our model still adopts the framework of character-based sequence labeling, which has advantages on both effectiveness and efficiency at the inference stage. To utilize the word-level information, we also propose a new long short-term memory (LSTM) architecture over directed acyclic graph (DAG). Experimental results demonstrate that our model leads to better performances than the baseline models.\n    ",
        "submission_date": "2017-07-02T00:00:00",
        "last_modified_date": "2017-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00299",
        "title": "Grammatical Error Correction with Neural Reinforcement Learning",
        "authors": [
            "Keisuke Sakaguchi",
            "Matt Post",
            "Benjamin Van Durme"
        ],
        "abstract": "We propose a neural encoder-decoder model with reinforcement learning (NRL) for grammatical error correction (GEC). Unlike conventional maximum likelihood estimation (MLE), the model directly optimizes towards an objective that considers a sentence-level, task-specific evaluation metric, avoiding the exposure bias issue in MLE. We demonstrate that NRL outperforms MLE both in human and automated evaluation metrics, achieving the state-of-the-art on a fluency-oriented GEC corpus.\n    ",
        "submission_date": "2017-07-02T00:00:00",
        "last_modified_date": "2017-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00621",
        "title": "Including Dialects and Language Varieties in Author Profiling",
        "authors": [
            "Alina Maria Ciobanu",
            "Marcos Zampieri",
            "Shervin Malmasi",
            "Liviu P. Dinu"
        ],
        "abstract": "This paper presents a computational approach to author profiling taking gender and language variety into account. We apply an ensemble system with the output of multiple linear SVM classifiers trained on character and word $n$-grams. We evaluate the system using the dataset provided by the organizers of the 2017 PAN lab on author profiling. Our approach achieved 75% average accuracy on gender identification on tweets written in four languages and 97% accuracy on language variety identification for Portuguese.\n    ",
        "submission_date": "2017-07-03T00:00:00",
        "last_modified_date": "2017-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00722",
        "title": "Improving LSTM-CTC based ASR performance in domains with limited training data",
        "authors": [
            "Jayadev Billa"
        ],
        "abstract": "This paper addresses the observed performance gap between automatic speech recognition (ASR) systems based on Long Short Term Memory (LSTM) neural networks trained with the connectionist temporal classification (CTC) loss function and systems based on hybrid Deep Neural Networks (DNNs) trained with the cross entropy (CE) loss function on domains with limited data. We step through a number of experiments that show incremental improvements on a baseline EESEN toolkit based LSTM-CTC ASR system trained on the Librispeech 100hr (train-clean-100) corpus. Our results show that with effective combination of data augmentation and regularization, a LSTM-CTC based system can exceed the performance of a strong Kaldi based baseline trained on the same data.\n    ",
        "submission_date": "2017-07-03T00:00:00",
        "last_modified_date": "2018-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00781",
        "title": "Mapping the Americanization of English in Space and Time",
        "authors": [
            "Bruno Gon\u00e7alves",
            "Luc\u00eda Loureiro-Porto",
            "Jos\u00e9 J. Ramasco",
            "David S\u00e1nchez"
        ],
        "abstract": "As global political preeminence gradually shifted from the United Kingdom to the United States, so did the capacity to culturally influence the rest of the world. In this work, we analyze how the world-wide varieties of written English are evolving. We study both the spatial and temporal variations of vocabulary and spelling of English using a large corpus of geolocated tweets and the Google Books datasets corresponding to books published in the US and the UK. The advantage of our approach is that we can address both standard written language (Google Books) and the more colloquial forms of microblogging messages (Twitter). We find that American English is the dominant form of English outside the UK and that its influence is felt even within the UK borders. Finally, we analyze how this trend has evolved over time and the impact that some cultural events have had in shaping it.\n    ",
        "submission_date": "2017-07-03T00:00:00",
        "last_modified_date": "2018-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00896",
        "title": "Multilingual Hierarchical Attention Networks for Document Classification",
        "authors": [
            "Nikolaos Pappas",
            "Andrei Popescu-Belis"
        ],
        "abstract": "Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00995",
        "title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation",
        "authors": [
            "Jean-Benoit Delbrouck",
            "St\u00e9phane Dupont"
        ],
        "abstract": "In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task (English, image to German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01009",
        "title": "Visually Grounded Word Embeddings and Richer Visual Features for Improving Multimodal Neural Machine Translation",
        "authors": [
            "Jean-Benoit Delbrouck",
            "St\u00e9phane Dupont",
            "Omar Seddati"
        ],
        "abstract": "In Multimodal Neural Machine Translation (MNMT), a neural model generates a translated sentence that describes an image, given the image itself and one source descriptions in English. This is considered as the multimodal image caption translation task. The images are processed with Convolutional Neural Network (CNN) to extract visual features exploitable by the translation model. So far, the CNNs used are pre-trained on object detection and localization task. We hypothesize that richer architecture, such as dense captioning models, may be more suitable for MNMT and could lead to improved translations. We extend this intuition to the word-embeddings, where we compute both linguistic and visual representation for our corpus vocabulary. We combine and compare different confi\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01066",
        "title": "Zero-Shot Transfer Learning for Event Extraction",
        "authors": [
            "Lifu Huang",
            "Heng Ji",
            "Kyunghyun Cho",
            "Clare R. Voss"
        ],
        "abstract": "Most previous event extraction studies have relied heavily on features derived from annotated event mentions, thus cannot be applied to new event types without annotation effort. In this work, we take a fresh look at event extraction and model it as a grounding problem. We design a transferable neural architecture, mapping event mentions and types jointly into a shared semantic space using structural and compositional neural networks, where the type of each event mention can be determined by the closest of all candidate types . By leveraging (1)~available manual annotations for a small set of existing event types and (2)~existing event ontologies, our framework applies to new event types without requiring additional annotation. Experiments on both existing event types (e.g., ACE, ERE) and new event types (e.g., FrameNet) demonstrate the effectiveness of our approach. \\textit{Without any manual annotations} for 23 new event types, our zero-shot framework achieved performance comparable to a state-of-the-art supervised model which is trained from the annotations of 500 event mentions.\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01075",
        "title": "Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures",
        "authors": [
            "Lifu Huang",
            "Avirup Sil",
            "Heng Ji",
            "Radu Florian"
        ],
        "abstract": "Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person:cities\\_of\\_residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies: (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler; (2). Incorporate two attention mechanisms: local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms state-of-the-art on both relation extraction (16\\% absolute F-score gain) and slot filling validation for each individual system (up to 8.5\\% absolute F-score gain).\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01161",
        "title": "Shakespearizing Modern Language Using Copy-Enriched Sequence-to-Sequence Models",
        "authors": [
            "Harsh Jhamtani",
            "Varun Gangal",
            "Eduard Hovy",
            "Eric Nyberg"
        ],
        "abstract": "Variations in writing styles are commonly used to adapt the content to a specific context, audience, or purpose. However, applying stylistic variations is still by and large a manual process, and there have been little efforts towards automating it. In this paper we explore automated methods to transform text from modern English to Shakespearean English using an end to end trainable neural model with pointers to enable copy action. To tackle limited amount of parallel data, we pre-train embeddings of words by leveraging external dictionaries mapping Shakespearean words to modern English words as well as additional text. Our methods are able to get a BLEU score of 31+, an improvement of ~6 points above the strongest baseline. We publicly release our code to foster further research in this area.\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01176",
        "title": "CharManteau: Character Embedding Models For Portmanteau Creation",
        "authors": [
            "Varun Gangal",
            "Harsh Jhamtani",
            "Graham Neubig",
            "Eduard Hovy",
            "Eric Nyberg"
        ],
        "abstract": "Portmanteaus are a word formation phenomenon where two words are combined to form a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01183",
        "title": "Complexity Metric for Code-Mixed Social Media Text",
        "authors": [
            "Souvick Ghosh",
            "Satanu Ghosh",
            "Dipankar Das"
        ],
        "abstract": "An evaluation metric is an absolute necessity for measuring the performance of any system and complexity of any data. In this paper, we have discussed how to determine the level of complexity of code-mixed social media texts that are growing rapidly due to multilingual interference. In general, texts written in multiple languages are often hard to comprehend and analyze. At the same time, in order to meet the demands of analysis, it is also necessary to determine the complexity of a particular document or a text segment. Thus, in the present paper, we have discussed the existing metrics for determining the code-mixing complexity of a corpus, their advantages, and shortcomings as well as proposed several improvements on the existing metrics. The new index better reflects the variety and complexity of a multilingual document. Also, the index can be applied to a sentence and seamlessly extended to a paragraph or an entire document. We have employed two existing code-mixed corpora to suit the requirements of our study.\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01184",
        "title": "Sentiment Identification in Code-Mixed Social Media Text",
        "authors": [
            "Souvick Ghosh",
            "Satanu Ghosh",
            "Dipankar Das"
        ],
        "abstract": "Sentiment analysis is the Natural Language Processing (NLP) task dealing with the detection and classification of sentiments in texts. While some tasks deal with identifying the presence of sentiment in the text (Subjectivity analysis), other tasks aim at determining the polarity of the text categorizing them as positive, negative and neutral. Whenever there is a presence of sentiment in the text, it has a source (people, group of people or any entity) and the sentiment is directed towards some entity, object, event or person. Sentiment analysis tasks aim to determine the subject, the target and the polarity or valence of the sentiment. In our work, we try to automatically extract sentiment (positive or negative) from Facebook posts using a machine learning ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01265",
        "title": "Multiple Range-Restricted Bidirectional Gated Recurrent Units with Attention for Relation Classification",
        "authors": [
            "Jonggu Kim",
            "Jong-Hyeok Lee"
        ],
        "abstract": "Most of neural approaches to relation classification have focused on finding short patterns that represent the semantic relation using Convolutional Neural Networks (CNNs) and those approaches have generally achieved better performances than using Recurrent Neural Networks (RNNs). In a similar intuition to the CNN models, we propose a novel RNN-based model that strongly focuses on only important parts of a sentence using multiple range-restricted bidirectional layers and attention for relation classification. Experimental results on the SemEval-2010 relation classification task show that our model is comparable to the state-of-the-art CNN-based and RNN-based models that use additional linguistic information.\n    ",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01321",
        "title": "The Influence of Feature Representation of Text on the Performance of Document Classification",
        "authors": [
            "Sanda Martin\u010di\u0107-Ip\u0161i\u0107",
            "Tanja Mili\u010di\u0107",
            "Ljup\u010do Todorovski"
        ],
        "abstract": "In this paper we perform a comparative analysis of three models for feature representation of text documents in the context of document classification. In particular, we consider the most often used family of models bag-of-words, recently proposed continuous space models word2vec and doc2vec, and the model based on the representation of text documents as language networks. While the bag-of-word models have been extensively used for the document classification task, the performance of the other two models for the same task have not been well understood. This is especially true for the network-based model that have been rarely considered for representation of text documents for classification. In this study, we measure the performance of the document classifiers trained using the method of random forests for features generated the three models and their variants. The results of the empirical comparison show that the commonly used bag-of-words model has performance comparable to the one obtained by the emerging continuous-space model of doc2vec. In particular, the low-dimensional variants of doc2vec generating up to 75 features are among the top-performing document representation models. The results finally point out that doc2vec shows a superior performance in the tasks of classifying large documents.\n    ",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01355",
        "title": "Align and Copy: UZH at SIGMORPHON 2017 Shared Task for Morphological Reinflection",
        "authors": [
            "Peter Makarov",
            "Tatiana Ruzsics",
            "Simon Clematide"
        ],
        "abstract": "This paper presents the submissions by the University of Zurich to the SIGMORPHON 2017 shared task on morphological reinflection. The task is to predict the inflected form given a lemma and a set of morpho-syntactic features. We focus on neural network approaches that can tackle the task in a limited-resource setting. As the transduction of the lemma into the inflected form is dominated by copying over lemma characters, we propose two recurrent neural network architectures with hard monotonic attention that are strong at copying and, yet, substantially different in how they achieve this. The first approach is an encoder-decoder model with a copy mechanism. The second approach is a neural state-transition system over a set of explicit edit actions, including a designated COPY action. We experiment with character alignment and find that naive, greedy alignment consistently produces strong results for some languages. Our best system combination is the overall winner of the SIGMORPHON 2017 Shared Task 1 without external resources. At a setting with 100 training samples, both our approaches, as ensembles of models, outperform the next best competitor.\n    ",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01378",
        "title": "An Attention Mechanism for Answer Selection Using a Combined Global and Local View",
        "authors": [
            "Yoram Bachrach",
            "Andrej Zukov-Gregoric",
            "Sam Coope",
            "Ed Tovell",
            "Bogdan Maksak",
            "Jose Rodriguez",
            "Conan McMurtie"
        ],
        "abstract": "We propose a new attention mechanism for neural based question answering, which depends on varying granularities of the input. Previous work focused on augmenting recurrent neural networks with simple attention mechanisms which are a function of the similarity between a question embedding and an answer embeddings across time. We extend this by making the attention mechanism dependent on a global embedding of the answer attained using a separate network.\n",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01521",
        "title": "Context Aware Document Embedding",
        "authors": [
            "Zhaocheng Zhu",
            "Junfeng Hu"
        ],
        "abstract": "Recently, doc2vec has achieved excellent results in different tasks. In this paper, we present a context aware variant of doc2vec. We introduce a novel weight estimating mechanism that generates weights for each word occurrence according to its contribution in the context, using deep neural networks. Our context aware model can achieve similar results compared to doc2vec initialized byWikipedia trained vectors, while being much more efficient and free from heavy external corpus. Analysis of context aware weights shows they are a kind of enhanced IDF weights that capture sub-topic level keywords in documents. They might result from deep neural networks that learn hidden representations with the least entropy.\n    ",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01555",
        "title": "A Deep Network with Visual Text Composition Behavior",
        "authors": [
            "Hongyu Guo"
        ],
        "abstract": "While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.\n    ",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01561",
        "title": "Automatic Generation of Natural Language Explanations",
        "authors": [
            "Felipe Costa",
            "Sixun Ouyang",
            "Peter Dolog",
            "Aonghus Lawlor"
        ],
        "abstract": "An important task for recommender system is to generate explanations according to a user's preferences. Most of the current methods for explainable recommendations use structured sentences to provide descriptions along with the recommendations they produce. However, those methods have neglected the review-oriented way of writing a text, even though it is known that these reviews have a strong influence over user's decision.\n",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01626",
        "title": "Cross-Lingual Sentiment Analysis Without (Good) Translation",
        "authors": [
            "Mohamed Abdalla",
            "Graeme Hirst"
        ],
        "abstract": "Current approaches to cross-lingual sentiment analysis try to leverage the wealth of labeled English data using bilingual lexicons, bilingual vector space embeddings, or machine translation systems. Here we show that it is possible to use a single linear transformation, with as few as 2000 word pairs, to capture fine-grained sentiment relationships between words in a cross-lingual setting. We apply these cross-lingual sentiment models to a diverse set of tasks to demonstrate their functionality in a non-English context. By effectively leveraging English sentiment knowledge without the need for accurate translation, we can analyze and extract features from other languages with scarce data at a very low cost, thus making sentiment and related analyses for many languages inexpensive.\n    ",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01662",
        "title": "An Embedded Deep Learning based Word Prediction",
        "authors": [
            "Seunghak Yu",
            "Nilesh Kulkarni",
            "Haejun Lee",
            "Jihie Kim"
        ],
        "abstract": "Recent developments in deep learning with application to language modeling have led to success in tasks of text processing, summarizing and machine translation. However, deploying huge language models for mobile device such as on-device keyboards poses computation as a bottle-neck due to their puny computation capacities. In this work we propose an embedded deep learning based word prediction method that optimizes run-time memory and also provides a real time prediction environment. Our model size is 7.40MB and has average prediction time of 6.47 ms. We improve over the existing methods for word prediction in terms of key stroke savings and word prediction rate.\n    ",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2017-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01736",
        "title": "Cross-linguistic differences and similarities in image descriptions",
        "authors": [
            "Emiel van Miltenburg",
            "Desmond Elliott",
            "Piek Vossen"
        ],
        "abstract": "Automatic image description systems are commonly trained and evaluated on large image description datasets. Recently, researchers have started to collect such datasets for languages other than English. An unexplored question is how different these datasets are from English and, if there are any differences, what causes them to differ. This paper provides a cross-linguistic comparison of Dutch, English, and German image descriptions. We find that these descriptions are similar in many respects, but the familiarity of crowd workers with the subjects of the images has a noticeable influence on description specificity.\n    ",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2017-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01780",
        "title": "On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis",
        "authors": [
            "Jose Camacho-Collados",
            "Mohammad Taher Pilehvar"
        ],
        "abstract": "Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.\n    ",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2018-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01793",
        "title": "A Simple Approach to Learn Polysemous Word Embeddings",
        "authors": [
            "Yifan Sun",
            "Nikhil Rao",
            "Weicong Ding"
        ],
        "abstract": "Many NLP applications require disambiguating polysemous words. Existing methods that learn polysemous word vector representations involve first detecting various senses and optimizing the sense-specific embeddings separately, which are invariably more involved than single sense learning methods such as word2vec. Evaluating these methods is also problematic, as rigorous quantitative evaluations in this space is limited, especially when compared with single-sense embeddings. In this paper, we propose a simple method to learn a word representation, given any context. Our method only requires learning the usual single sense representation, and coefficients that can be learnt via a single pass over the data. We propose several new test sets for evaluating word sense induction, relevance detection, and contextual word similarity, significantly supplementing the currently available tests. Results on these and other tests show that while our method is embarrassingly simple, it achieves excellent results when compared to the state of the art models for unsupervised polysemous word representation learning.\n    ",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01830",
        "title": "Single-Queue Decoding for Neural Machine Translation",
        "authors": [
            "Raphael Shu",
            "Hideki Nakayama"
        ],
        "abstract": "Neural machine translation models rely on the beam search algorithm for decoding. In practice, we found that the quality of hypotheses in the search space is negatively affected owing to the fixed beam size. To mitigate this problem, we store all hypotheses in a single priority queue and use a universal score function for hypothesis selection. The proposed algorithm is more flexible as the discarded hypotheses can be revisited in a later step. We further design a penalty function to punish the hypotheses that tend to produce a final translation that is much longer or shorter than expected. Despite its simplicity, we show that the proposed decoding algorithm is able to select hypotheses with better qualities and improve the translation performance.\n    ",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2017-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01917",
        "title": "Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation",
        "authors": [
            "Madhav Nimishakavi",
            "Partha Talukdar"
        ],
        "abstract": "Relation Schema Induction (RSI) is the problem of identifying type signatures of arguments of relations from unlabeled text. Most of the previous work in this area have focused only on binary RSI, i.e., inducing only the subject and object type signatures per relation. However, in practice, many relations are high-order, i.e., they have more than two arguments and inducing type signatures of all arguments is necessary. For example, in the sports domain, inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is more informative than inducing just win(WinningPlayer, OpponentPlayer). We refer to this problem as Higher-order Relation Schema Induction (HRSI). In this paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a novel framework for the HRSI problem. To the best of our knowledge, this is the first attempt at inducing higher-order relation schemata from unlabeled text. Using the experimental analysis on three real world datasets, we show how TFBA helps in dealing with sparsity and induce higher order schemata.\n    ",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2018-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01961",
        "title": "Long-Term Memory Networks for Question Answering",
        "authors": [
            "Fenglong Ma",
            "Radha Chitta",
            "Saurabh Kataria",
            "Jing Zhou",
            "Palghat Ramesh",
            "Tong Sun",
            "Jing Gao"
        ],
        "abstract": "Question answering is an important and difficult task in the natural language processing domain, because many basic natural language processing tasks can be cast into a question answering task. Several deep neural network architectures have been developed recently, which employ memory and inference components to memorize and reason over text information, and generate answers to questions. However, a major drawback of many such models is that they are capable of only generating single-word answers. In addition, they require large amount of training data to generate accurate answers. In this paper, we introduce the Long-Term Memory Network (LTMN), which incorporates both an external memory module and a Long Short-Term Memory (LSTM) module to comprehend the input data and generate multi-word answers. The LTMN model can be trained end-to-end using back-propagation and requires minimal supervision. We test our model on two synthetic data sets (based on Facebook's bAbI data set) and the real-world Stanford question answering data set, and show that it can achieve state-of-the-art performance.\n    ",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2017-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02026",
        "title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction",
        "authors": [
            "Jianshu Ji",
            "Qinlong Wang",
            "Kristina Toutanova",
            "Yongen Gong",
            "Steven Truong",
            "Jianfeng Gao"
        ],
        "abstract": "Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information,and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.\n    ",
        "submission_date": "2017-07-07T00:00:00",
        "last_modified_date": "2017-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02063",
        "title": "External Evaluation of Event Extraction Classifiers for Automatic Pathway Curation: An extended study of the mTOR pathway",
        "authors": [
            "Wojciech Kusa",
            "Michael Spranger"
        ],
        "abstract": "This paper evaluates the impact of various event extraction systems on automatic pathway curation using the popular mTOR pathway. We quantify the impact of training data sets as well as different machine learning classifiers and show that some improve the quality of automatically extracted pathways.\n    ",
        "submission_date": "2017-07-07T00:00:00",
        "last_modified_date": "2017-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02230",
        "title": "Computational Models of Tutor Feedback in Language Acquisition",
        "authors": [
            "Jens Nevens",
            "Michael Spranger"
        ],
        "abstract": "This paper investigates the role of tutor feedback in language learning using computational models. We compare two dominant paradigms in language learning: interactive learning and cross-situational learning - which differ primarily in the role of social feedback such as gaze or pointing. We analyze the relationship between these two paradigms and propose a new mixed paradigm that combines the two paradigms and allows to test algorithms in experiments that combine no feedback and social feedback. To deal with mixed feedback experiments, we develop new algorithms and show how they perform with respect to traditional knn and prototype approaches.\n    ",
        "submission_date": "2017-07-07T00:00:00",
        "last_modified_date": "2017-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02268",
        "title": "Text Summarization Techniques: A Brief Survey",
        "authors": [
            "Mehdi Allahyari",
            "Seyedamin Pouriyeh",
            "Mehdi Assefi",
            "Saeid Safaei",
            "Elizabeth D. Trippe",
            "Juan B. Gutierrez",
            "Krys Kochut"
        ],
        "abstract": "In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.\n    ",
        "submission_date": "2017-07-07T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02275",
        "title": "A parallel corpus of Python functions and documentation strings for automated code documentation and code generation",
        "authors": [
            "Antonio Valerio Miceli Barone",
            "Rico Sennrich"
        ],
        "abstract": "Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains.\n",
        "submission_date": "2017-07-07T00:00:00",
        "last_modified_date": "2017-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02377",
        "title": "Efficient Vector Representation for Documents through Corruption",
        "authors": [
            "Minmin Chen"
        ],
        "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n    ",
        "submission_date": "2017-07-08T00:00:00",
        "last_modified_date": "2017-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02459",
        "title": "Improving Multilingual Named Entity Recognition with Wikipedia Entity Type Mapping",
        "authors": [
            "Jian Ni",
            "Radu Florian"
        ],
        "abstract": "The state-of-the-art named entity recognition (NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information. However, such a model could still make mistakes if its features favor a wrong entity type. In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-accuracy, high-coverage multilingual Wikipedia entity type mappings. These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages. Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F1 score improvement).\n    ",
        "submission_date": "2017-07-08T00:00:00",
        "last_modified_date": "2017-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02483",
        "title": "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection",
        "authors": [
            "Jian Ni",
            "Georgiana Dinu",
            "Radu Florian"
        ],
        "abstract": "The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.\n    ",
        "submission_date": "2017-07-08T00:00:00",
        "last_modified_date": "2017-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02499",
        "title": "Predicting the Quality of Short Narratives from Social Media",
        "authors": [
            "Tong Wang",
            "Ping Chen",
            "Boyang Li"
        ],
        "abstract": "An important and difficult challenge in building computational models for narratives is the automatic evaluation of narrative quality. Quality evaluation connects narrative understanding and generation as generation systems need to evaluate their own products. To circumvent difficulties in acquiring annotations, we employ upvotes in social media as an approximate measure for story quality. We collected 54,484 answers from a crowd-powered question-and-answer website, Quora, and then used active learning to build a classifier that labeled 28,320 answers as stories. To predict the number of upvotes without the use of social network features, we create neural networks that model textual regions and the interdependence among regions, which serve as strong benchmarks for future research. To our best knowledge, this is the first large-scale study for automatic evaluation of narrative quality.\n    ",
        "submission_date": "2017-07-08T00:00:00",
        "last_modified_date": "2017-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02575",
        "title": "Neural Machine Translation between Herbal Prescriptions and Diseases",
        "authors": [
            "Sun-Chong Wang"
        ],
        "abstract": "The current study applies deep learning to herbalism. Toward the goal, we acquired the de-identified health insurance reimbursements that were claimed in a 10-year period from 2004 to 2013 in the National Health Insurance Database of Taiwan, the total number of reimbursement records equaling 340 millions. Two artificial intelligence techniques were applied to the dataset: residual convolutional neural network multitask classifier and attention-based recurrent neural network. The former works to translate from herbal prescriptions to diseases; and the latter from diseases to herbal prescriptions. Analysis of the classification results indicates that herbal prescriptions are specific to: anatomy, pathophysiology, sex and age of the patient, and season and year of the prescription. Further analysis identifies temperature and gross domestic product as the meteorological and socioeconomic factors that are associated with herbal prescriptions. Analysis of the neural machine transitional result indicates that the recurrent neural network learnt not only syntax but also semantics of diseases and herbal prescriptions.\n    ",
        "submission_date": "2017-07-09T00:00:00",
        "last_modified_date": "2017-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02633",
        "title": "Controlling Linguistic Style Aspects in Neural Language Generation",
        "authors": [
            "Jessica Ficler",
            "Yoav Goldberg"
        ],
        "abstract": "Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text. We experiment with controlling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content.\n    ",
        "submission_date": "2017-07-09T00:00:00",
        "last_modified_date": "2017-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02657",
        "title": "PELESent: Cross-domain polarity classification using distant supervision",
        "authors": [
            "Edilson A. Corr\u00eaa Jr",
            "Vanessa Q. Marinho",
            "Leandro B. dos Santos",
            "Thales F. C. Bertaglia",
            "Marcos V. Treviso",
            "Henrico B. Brum"
        ],
        "abstract": "The enormous amount of texts published daily by Internet users has fostered the development of methods to analyze this content in several natural language processing areas, such as sentiment analysis. The main goal of this task is to classify the polarity of a message. Even though many approaches have been proposed for sentiment analysis, some of the most successful ones rely on the availability of large annotated corpus, which is an expensive and time-consuming process. In recent years, distant supervision has been used to obtain larger datasets. So, inspired by these techniques, in this paper we extend such approaches to incorporate popular graphic symbols used in electronic messages, the emojis, in order to create a large sentiment corpus for Portuguese. Trained on almost one million tweets, several models were tested in both same domain and cross-domain corpora. Our methods obtained very competitive results in five annotated corpora from mixed domains (Twitter and product reviews), which proves the domain-independent property of such approach. In addition, our results suggest that the combination of emoticons and emojis is able to properly capture the sentiment of a message.\n    ",
        "submission_date": "2017-07-09T00:00:00",
        "last_modified_date": "2017-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02774",
        "title": "Understanding State Preferences With Text As Data: Introducing the UN General Debate Corpus",
        "authors": [
            "Alexander Baturo",
            "Niheer Dasandi",
            "Slava J. Mikhaylov"
        ],
        "abstract": "Every year at the United Nations, member states deliver statements during the General Debate discussing major issues in world politics. These speeches provide invaluable information on governments' perspectives and preferences on a wide range of issues, but have largely been overlooked in the study of international politics. This paper introduces a new dataset consisting of over 7,701 English-language country statements from 1970-2016. We demonstrate how the UN General Debate Corpus (UNGDC) can be used to derive country positions on different policy dimensions using text analytic methods. The paper provides applications of these estimates, demonstrating the contribution the UNGDC can make to the study of international politics.\n    ",
        "submission_date": "2017-07-10T00:00:00",
        "last_modified_date": "2017-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02786",
        "title": "Learning to Compose Task-Specific Tree Structures",
        "authors": [
            "Jihun Choi",
            "Kang Min Yoo",
            "Sang-goo Lee"
        ],
        "abstract": "For years, recursive neural networks (RvNNs) have been shown to be suitable for representing text into fixed-length vectors and achieved good performance on several natural language processing tasks. However, the main drawback of RvNNs is that they require structured input, which makes data preparation and model implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel tree-structured long short-term memory architecture that learns how to compose task-specific tree structures only from plain text data efficiently. Our model uses Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision. We evaluate the proposed model on natural language inference and sentiment analysis, and show that our model outperforms or is at least comparable to previous models. We also find that our model converges significantly faster than other models.\n    ",
        "submission_date": "2017-07-10T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02892",
        "title": "A Generalized Recurrent Neural Architecture for Text Classification with Multi-Task Learning",
        "authors": [
            "Honglun Zhang",
            "Liqiang Xiao",
            "Yongkun Wang",
            "Yaohui Jin"
        ],
        "abstract": "Multi-task learning leverages potential correlations among related tasks to extract common features and yield performance gains. However, most previous works only consider simple or weak interactions, thereby failing to model complex correlations among three or more tasks. In this paper, we propose a multi-task learning architecture with four types of recurrent neural layers to fuse information across multiple related tasks. The architecture is structurally flexible and considers various interactions among tasks, which can be regarded as a generalized case of many previous works. Extensive experiments on five benchmark datasets for text classification show that our model can significantly improve performances of related tasks with additional information from others.\n    ",
        "submission_date": "2017-07-10T00:00:00",
        "last_modified_date": "2017-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02919",
        "title": "A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques",
        "authors": [
            "Mehdi Allahyari",
            "Seyedamin Pouriyeh",
            "Mehdi Assefi",
            "Saied Safaei",
            "Elizabeth D. Trippe",
            "Juan B. Gutierrez",
            "Krys Kochut"
        ],
        "abstract": "The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.\n    ",
        "submission_date": "2017-07-10T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03058",
        "title": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects",
        "authors": [
            "Daniel Fried",
            "Mitchell Stern",
            "Dan Klein"
        ],
        "abstract": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.\n    ",
        "submission_date": "2017-07-10T00:00:00",
        "last_modified_date": "2017-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03095",
        "title": "Look Who's Talking: Bipartite Networks as Representations of a Topic Model of New Zealand Parliamentary Speeches",
        "authors": [
            "Ben Curran",
            "Kyle Higham",
            "Elisenda Ortiz",
            "Demival Vasques Filho"
        ],
        "abstract": "Quantitative methods to measure the participation to parliamentary debate and discourse of elected Members of Parliament (MPs) and the parties they belong to are lacking. This is an exploratory study in which we propose the development of a new approach for a quantitative analysis of such participation. We utilize the New Zealand government's digital Hansard database to construct a topic model of parliamentary speeches consisting of nearly 40 million words in the period 2003-2016. A Latent Dirichlet Allocation topic model is implemented in order to reveal the thematic structure of our set of documents. This generative statistical model enables the detection of major themes or topics that are publicly discussed in the New Zealand parliament, as well as permitting their classification by MP. Information on topic proportions is subsequently analyzed using a combination of statistical methods. We observe patterns arising from time-series analysis of topic frequencies which can be related to specific social, economic and legislative events. We then construct a bipartite network representation, linking MPs to topics, for each of four parliamentary terms in this time frame. We build projected networks (onto the set of nodes represented by MPs) and proceed to the study of the dynamical changes of their topology, including community structure. By performing this longitudinal network analysis, we can observe the evolution of the New Zealand parliamentary topic network and its main parties in the period studied.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03103",
        "title": "Refining Raw Sentence Representations for Textual Entailment Recognition via Attention",
        "authors": [
            "Jorge A. Balazs",
            "Edison Marrese-Taylor",
            "Pablo Loyola",
            "Yutaka Matsuo"
        ],
        "abstract": "In this paper we present the model used by the team Rivercorners for the 2017 RepEval shared task. First, our model separately encodes a pair of sentences into variable-length representations by using a bidirectional LSTM. Later, it creates fixed-length raw representations by means of simple aggregation functions, which are then refined using an attention mechanism. Finally it combines the refined representations of both sentences into a single vector to be used for classification. With this model we obtained test accuracies of 72.057% and 72.055% in the matched and mismatched evaluation tracks respectively, outperforming the LSTM baseline, and obtaining performances similar to a model that relies on shared information between sentences (ESIM). When using an ensemble both accuracies increased to 72.247% and 72.827% respectively.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03172",
        "title": "Dataset for a Neural Natural Language Interface for Databases (NNLIDB)",
        "authors": [
            "Florin Brad",
            "Radu Iacob",
            "Ionel Hosu",
            "Traian Rebedea"
        ],
        "abstract": "Progress in natural language interfaces to databases (NLIDB) has been slow mainly due to linguistic issues (such as language ambiguity) and domain portability. Moreover, the lack of a large corpus to be used as a standard benchmark has made data-driven approaches difficult to develop and compare. In this paper, we revisit the problem of NLIDBs and recast it as a sequence translation problem. To this end, we introduce a large dataset extracted from the Stack Exchange Data Explorer website, which can be used for training neural natural language interfaces for databases. We also report encouraging baseline results on a smaller manually annotated test corpus, obtained using an attention-based sequence-to-sequence neural network.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03228",
        "title": "A non-projective greedy dependency parser with bidirectional LSTMs",
        "authors": [
            "David Vilares",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "abstract": "The LyS-FASTPARSE team presents BIST-COVINGTON, a neural implementation of the Covington (2001) algorithm for non-projective dependency parsing. The bidirectional LSTM approach by Kipperwasser and Goldberg (2016) is used to train a greedy parser with a dynamic oracle to mitigate error propagation. The model participated in the CoNLL 2017 UD Shared Task. In spite of not using any ensemble methods and using the baseline segmentation and PoS tagging, the parser obtained good results on both macro-average LAS and UAS in the big treebanks category (55 languages), ranking 7th out of 33 teams. In the all treebanks category (LAS and UAS) we ranked 16th and 12th. The gap between the all and big categories is mainly due to the poor performance on four parallel PUD treebanks, suggesting that some `suffixed' treebanks (e.g. Spanish-AnCora) perform poorly on cross-treebank settings, which does not occur with the corresponding `unsuffixed' treebank (e.g. Spanish). By changing that, we obtain the 11th best LAS among all runs (official and unofficial). The code is made available at ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03253",
        "title": "Leipzig Corpus Miner - A Text Mining Infrastructure for Qualitative Data Analysis",
        "authors": [
            "Andreas Niekler",
            "Gregor Wiedemann",
            "Gerhard Heyer"
        ],
        "abstract": "This paper presents the \"Leipzig Corpus Miner\", a technical infrastructure for supporting qualitative and quantitative content analysis. The infrastructure aims at the integration of 'close reading' procedures on individual documents with procedures of 'distant reading', e.g. lexical characteristics of large document collections. Therefore information retrieval systems, lexicometric statistics and machine learning procedures are combined in a coherent framework which enables qualitative data analysts to make use of state-of-the-art Natural Language Processing techniques on very large document collections. Applicability of the framework ranges from social sciences to media studies and market research. As an example we introduce the usage of the framework in a political science study on post-democracy and neoliberalism.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03255",
        "title": "Modeling the dynamics of domain specific terminology in diachronic corpora",
        "authors": [
            "Gerhard Heyer",
            "Cathleen Kantner",
            "Andreas Niekler",
            "Max Overbeck",
            "Gregor Wiedemann"
        ],
        "abstract": "In terminology work, natural language processing, and digital humanities, several studies address the analysis of variations in context and meaning of terms in order to detect semantic change and the evolution of terms. We distinguish three different approaches to describe contextual variations: methods based on the analysis of patterns and linguistic clues, methods exploring the latent semantic space of single words, and methods for the analysis of topic membership. The paper presents the notion of context volatility as a new measure for detecting semantic change and applies it to key term extraction in a political science case study. The measure quantifies the dynamics of a term's contextual variation within a diachronic corpus to identify periods of time that are characterised by intense controversial debates or substantial semantic transformations.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03264",
        "title": "A simple but tough-to-beat baseline for the Fake News Challenge stance detection task",
        "authors": [
            "Benjamin Riedel",
            "Isabelle Augenstein",
            "Georgios P. Spithourakis",
            "Sebastian Riedel"
        ],
        "abstract": "Identifying public misinformation is a complicated and challenging task. An important part of checking the veracity of a specific claim is to evaluate the stance different news sources take towards the assertion. Automatic stance evaluation, i.e. stance detection, would arguably facilitate the process of fact checking. In this paper, we present our stance detection system which claimed third place in Stage 1 of the Fake News Challenge. Despite our straightforward approach, our system performs at a competitive level with the complex ensembles of the top two winning teams. We therefore propose our system as the 'simple but tough-to-beat baseline' for the Fake News Challenge stance detection task.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2018-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03490",
        "title": "Detecting Policy Preferences and Dynamics in the UN General Debate with Neural Word Embeddings",
        "authors": [
            "Stefano Gurciullo",
            "Slava Mikhaylov"
        ],
        "abstract": "Foreign policy analysis has been struggling to find ways to measure policy preferences and paradigm shifts in international political systems. This paper presents a novel, potential solution to this challenge, through the application of a neural word embedding (Word2vec) model on a dataset featuring speeches by heads of state or government in the United Nations General Debate. The paper provides three key contributions based on the output of the Word2vec model. First, it presents a set of policy attention indices, synthesizing the semantic proximity of political speeches to specific policy themes. Second, it introduces country-specific semantic centrality indices, based on topological analyses of countries' semantic positions with respect to each other. Third, it tests the hypothesis that there exists a statistical relation between the semantic content of political speeches and UN voting behavior, falsifying it and suggesting that political speeches contain information of different nature then the one behind voting outcomes. The paper concludes with a discussion of the practical use of its results and consequences for foreign policy analysis, public accountability, and transparency.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03550",
        "title": "Geospatial Semantics",
        "authors": [
            "Yingjie Hu"
        ],
        "abstract": "Geospatial semantics is a broad field that involves a variety of research areas. The term semantics refers to the meaning of things, and is in contrast with the term syntactics. Accordingly, studies on geospatial semantics usually focus on understanding the meaning of geographic entities as well as their counterparts in the cognitive and digital world, such as cognitive geographic concepts and digital gazetteers. Geospatial semantics can also facilitate the design of geographic information systems (GIS) by enhancing the interoperability of distributed systems and developing more intelligent interfaces for user interactions. During the past years, a lot of research has been conducted, approaching geospatial semantics from different perspectives, using a variety of methods, and targeting different problems. Meanwhile, the arrival of big geo data, especially the large amount of unstructured text data on the Web, and the fast development of natural language processing methods enable new research directions in geospatial semantics. This chapter, therefore, provides a systematic review on the existing geospatial semantic research. Six major research areas are identified and discussed, including semantic interoperability, digital gazetteers, geographic information retrieval, geospatial Semantic Web, place semantics, and cognitive geographic concepts.\n    ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03736",
        "title": "The Case for Being Average: A Mediocrity Approach to Style Masking and Author Obfuscation",
        "authors": [
            "Georgi Karadjov",
            "Tsvetomila Mihaylova",
            "Yasen Kiprov",
            "Georgi Georgiev",
            "Ivan Koychev",
            "Preslav Nakov"
        ],
        "abstract": "Users posting online expect to remain anonymous unless they have logged in, which is often needed for them to be able to discuss freely on various topics. Preserving the anonymity of a text's writer can be also important in some other contexts, e.g., in the case of witness protection or anonymity programs. However, each person has his/her own style of writing, which can be analyzed using stylometry, and as a result, the true identity of the author of a piece of text can be revealed even if s/he has tried to hide it. Thus, it could be helpful to design automatic tools that can help a person obfuscate his/her identity when writing text. In particular, here we propose an approach that changes the text, so that it is pushed towards average values for some general stylometric characteristics, thus making the use of these characteristics less discriminative. The approach consists of three main steps: first, we calculate the values for some popular stylometric metrics that can indicate authorship; then we apply various transformations to the text, so that these metrics are adjusted towards the average level, while preserving the semantics and the soundness of the text; and finally, we add random noise. This approach turned out to be very efficient, and yielded the best performance on the Author Obfuscation task at the PAN-2016 competition.\n    ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03764",
        "title": "N-GrAM: New Groningen Author-profiling Model",
        "authors": [
            "Angelo Basile",
            "Gareth Dwyer",
            "Maria Medvedeva",
            "Josine Rawee",
            "Hessel Haagsma",
            "Malvina Nissim"
        ],
        "abstract": "We describe our participation in the PAN 2017 shared task on Author Profiling, identifying authors' gender and language variety for English, Spanish, Arabic and Portuguese. We describe both the final, submitted system, and a series of negative results. Our aim was to create a single model for both gender and language, and for all language varieties. Our best-performing system (on cross-validated results) is a linear support vector machine (SVM) with word unigrams and character 3- to 5-grams as features. A set of additional features, including POS tags, additional datasets, geographic entities, and Twitter handles, hurt, rather than improve, performance. Results from cross-validation indicated high performance overall and results on the test set confirmed them, at 0.86 averaged accuracy, with performance on sub-tasks ranging from 0.68 to 0.98.\n    ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03804",
        "title": "Source-Target Inference Models for Spatial Instruction Understanding",
        "authors": [
            "Hao Tan",
            "Mohit Bansal"
        ],
        "abstract": "Models that can execute natural language instructions for situated robotic tasks such as assembly and navigation have several useful applications in homes, offices, and remote scenarios. We study the semantics of spatially-referred configuration and arrangement instructions, based on the challenging Bisk-2016 blank-labeled block dataset. This task involves finding a source block and moving it to the target position (mentioned via a reference block and offset), where the blocks have no names or colors and are just referred to via spatial location features. We present novel models for the subtasks of source block classification and target position regression, based on joint-loss language and spatial-world representation learning, as well as CNN-based and dual attention models to compute the alignment between the world blocks and the instruction phrases. For target position prediction, we compare two inference approaches: annealed sampling via policy gradient versus expectation inference via supervised regression. Our models achieve the new state-of-the-art on this task, with an improvement of 47% on source block accuracy and 22% on target position distance.\n    ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03819",
        "title": "A Critique of a Critique of Word Similarity Datasets: Sanity Check or Unnecessary Confusion?",
        "authors": [
            "Minh Le"
        ],
        "abstract": "Critical evaluation of word similarity datasets is very important for computational lexical semantics. This short report concerns the sanity check proposed in Batchkarov et al. (2016) to evaluate several popular datasets such as MC, RG and MEN -- the first two reportedly failed. I argue that this test is unstable, offers no added insight, and needs major revision in order to fulfill its purported goal.\n    ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03903",
        "title": "Negative Sampling Improves Hypernymy Extraction Based on Projection Learning",
        "authors": [
            "Dmitry Ustalov",
            "Nikolay Arefyev",
            "Chris Biemann",
            "Alexander Panchenko"
        ],
        "abstract": "We present a new approach to extraction of hypernyms based on projection learning and word embeddings. In contrast to classification-based approaches, projection-based methods require no candidate hyponym-hypernym pairs. While it is natural to use both positive and negative training examples in supervised relation extraction, the impact of negative examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for regularization of the model significantly improve performance compared to the state-of-the-art approach of Fu et al. (2014) on three datasets from different languages.\n    ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03904",
        "title": "Quasar: Datasets for Question Answering by Search and Reading",
        "authors": [
            "Bhuwan Dhingra",
            "Kathryn Mazaitis",
            "William W. Cohen"
        ],
        "abstract": "We present two new large-scale datasets aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. The Quasar-S dataset consists of 37000 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background corpus for answering the cloze questions. The Quasar-T dataset consists of 43000 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. We pose these datasets as a challenge for two related subtasks of factoid Question Answering: (1) searching for relevant pieces of text that include the correct answer to a query, and (2) reading the retrieved text to answer the query. We also describe a retrieval system for extracting relevant sentences and documents from the corpus given a query, and include these in the release for researchers wishing to only focus on (2). We evaluate several baselines on both datasets, ranging from simple heuristics to powerful neural models, and show that these lag behind human performance by 16.4% and 32.1% for Quasar-S and -T respectively. The datasets are available at ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03938",
        "title": "Representation Learning for Grounded Spatial Reasoning",
        "authors": [
            "Michael Janner",
            "Karthik Narasimhan",
            "Regina Barzilay"
        ],
        "abstract": "The interpretation of spatial references is highly contextual, requiring joint inference over both language and the environment. We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards. The proposed model learns a representation of the world steered by instruction text. This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our model with reinforcement learning using a variant of generalized value iteration. The model outperforms state-of-the-art approaches on several metrics, yielding a 45% reduction in goal localization error.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03968",
        "title": "Predicting Causes of Reformulation in Intelligent Assistants",
        "authors": [
            "Shumpei Sano",
            "Nobuhiro Kaji",
            "Manabu Sassano"
        ],
        "abstract": "Intelligent assistants (IAs) such as Siri and Cortana conversationally interact with users and execute a wide range of actions (e.g., searching the Web, setting alarms, and chatting). IAs can support these actions through the combination of various components such as automatic speech recognition, natural language understanding, and language generation. However, the complexity of these components hinders developers from determining which component causes an error. To remove this hindrance, we focus on reformulation, which is a useful signal of user dissatisfaction, and propose a method to predict the reformulation causes. We evaluate the method using the user logs of a commercial IA. The experimental results have demonstrated that features designed to detect the error of a specific component improve the performance of reformulation cause detection.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03997",
        "title": "A Web-Based Tool for Analysing Normative Documents in English",
        "authors": [
            "John J. Camilleri",
            "Mohammad Reza Haghshenas",
            "Gerardo Schneider"
        ],
        "abstract": "Our goal is to use formal methods to analyse normative documents written in English, such as privacy policies and service-level agreements. This requires the combination of a number of different elements, including information extraction from natural language, formal languages for model representation, and an interface for property specification and verification. We have worked on a collection of components for this task: a natural language extraction tool, a suitable formalism for representing such documents, an interface for building models in this formalism, and methods for answering queries asked of a given model. In this work, each of these concerns is brought together in a web-based tool, providing a single interface for analysing normative texts in English. Through the use of a running example, we describe each component and demonstrate the workflow established by our tool.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04095",
        "title": "Is writing style predictive of scientific fraud?",
        "authors": [
            "Chlo\u00e9 Braud",
            "Anders S\u00f8gaard"
        ],
        "abstract": "The problem of detecting scientific fraud using machine learning was recently introduced, with initial, positive results from a model taking into account various general indicators. The results seem to suggest that writing style is predictive of scientific fraud. We revisit these initial experiments, and show that the leave-one-out testing procedure they used likely leads to a slight over-estimate of the predictability, but also that simple models can outperform their proposed model by some margin. We go on to explore more abstract linguistic features, such as linguistic complexity and discourse structure, only to obtain negative results. Upon analyzing our models, we do see some interesting patterns, though: Scientific fraud, for examples, contains less comparison, as well as different types of hedging and ways of presenting logical reasoning.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04108",
        "title": "Do Convolutional Networks need to be Deep for Text Classification ?",
        "authors": [
            "Hoa T. Le",
            "Christophe Cerisara",
            "Alexandre Denis"
        ],
        "abstract": "We study in this work the importance of depth in convolutional models for text classification, either when character or word inputs are considered. We show on 5 standard text classification and sentiment analysis tasks that deep models indeed give better performances than shallow networks when the text input is represented as a sequence of characters. However, a simple shallow-and-wide network outperforms deep models such as DenseNet with word inputs. Our shallow word model further establishes new state-of-the-art performances on two datasets: Yelp Binary (95.9\\%) and Yelp Full (64.9\\%).\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04218",
        "title": "Learning Features from Co-occurrences: A Theoretical Analysis",
        "authors": [
            "Yanpeng Li"
        ],
        "abstract": "Representing a word by its co-occurrences with other words in context is an effective way to capture the meaning of the word. However, the theory behind remains a challenge. In this work, taking the example of a word classification task, we give a theoretical analysis of the approaches that represent a word X by a function f(P(C|X)), where C is a context feature, P(C|X) is the conditional probability estimated from a text corpus, and the function f maps the co-occurrence measure to a prediction score. We investigate the impact of context feature C and the function f. We also explain the reasons why using the co-occurrences with multiple context features may be better than just using a single one. In addition, some of the results shed light on the theory of feature learning and machine learning in general.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04221",
        "title": "Parsing with Traces: An $O(n^4)$ Algorithm and a Structural Representation",
        "authors": [
            "Jonathan K. Kummerfeld",
            "Dan Klein"
        ],
        "abstract": "General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proof-of-concept parser that recovers a range of null elements and trace types.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04227",
        "title": "Automatic Speech Recognition with Very Large Conversational Finnish and Estonian Vocabularies",
        "authors": [
            "Seppo Enarvi",
            "Peter Smit",
            "Sami Virpioja",
            "Mikko Kurimo"
        ],
        "abstract": "Today, the vocabulary size for language models in large vocabulary speech recognition is typically several hundreds of thousands of words. While this is already sufficient in some applications, the out-of-vocabulary words are still limiting the usability in others. In agglutinative languages the vocabulary for conversational speech should include millions of word forms to cover the spelling variations due to colloquial pronunciations, in addition to the word compounding and inflections. Very large vocabularies are also needed, for example, when the recognition of rare proper names is important.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04408",
        "title": "Developing a concept-level knowledge base for sentiment analysis in Singlish",
        "authors": [
            "Rajiv Bajpai",
            "Soujanya Poria",
            "Danyun Ho",
            "Erik Cambria"
        ],
        "abstract": "In this paper, we present Singlish sentiment lexicon, a concept-level knowledge base for sentiment analysis that associates multiword expressions to a set of emotion labels and a polarity value. Unlike many other sentiment analysis resources, this lexicon is not built by manually labeling pieces of knowledge coming from general NLP resources such as WordNet or DBPedia. Instead, it is automatically constructed by applying graph-mining and multi-dimensional scaling techniques on the affective common-sense knowledge collected from three different sources. This knowledge is represented redundantly at three levels: semantic network, matrix, and vector space. Subsequently, the concepts are labeled by emotions and polarity through the ensemble application of spreading activation, neural networks and an emotion categorization model.\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04412",
        "title": "Evaluating Semantic Parsing against a Simple Web-based Question Answering Model",
        "authors": [
            "Alon Talmor",
            "Mor Geva",
            "Jonathan Berant"
        ],
        "abstract": "Semantic parsing shines at analyzing complex natural language that involves composition and computation over multiple pieces of evidence. However, datasets for semantic parsing contain many factoid questions that can be answered from a single web document. In this paper, we propose to evaluate semantic parsing-based question answering models by comparing them to a question answering baseline that queries the web and extracts the answer only from web snippets, without access to the target knowledge-base. We investigate this approach on COMPLEXQUESTIONS, a dataset designed to focus on compositional language, and find that our model obtains reasonable performance (35 F1 compared to 41 F1 of state-of-the-art). We find in our analysis that our model performs well on complex questions involving conjunctions, but struggles on questions that involve relation composition and superlatives.\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04481",
        "title": "LIUM-CVC Submissions for WMT17 Multimodal Translation Task",
        "authors": [
            "Ozan Caglayan",
            "Walid Aransa",
            "Adrien Bardet",
            "Mercedes Garc\u00eda-Mart\u00ednez",
            "Fethi Bougares",
            "Lo\u00efc Barrault",
            "Marc Masana",
            "Luis Herranz",
            "Joost van de Weijer"
        ],
        "abstract": "This paper describes the monomodal and multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT17 Shared Task on Multimodal Translation. We mainly explored two multimodal architectures where either global visual features or convolutional feature maps are integrated in order to benefit from visual context. Our final systems ranked first for both En-De and En-Fr language pairs according to the automatic evaluation metrics METEOR and BLEU.\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04499",
        "title": "LIUM Machine Translation Systems for WMT17 News Translation Task",
        "authors": [
            "Mercedes Garc\u00eda-Mart\u00ednez",
            "Ozan Caglayan",
            "Walid Aransa",
            "Adrien Bardet",
            "Fethi Bougares",
            "Lo\u00efc Barrault"
        ],
        "abstract": "This paper describes LIUM submissions to WMT17 News Translation Task for English-German, English-Turkish, English-Czech and English-Latvian language pairs. We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework. Competitive scores were obtained by ensembling various systems and exploiting the availability of target monolingual corpora for back-translation. The impact of back-translation quantity and quality is also analyzed for English-Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04538",
        "title": "Cross-genre Document Retrieval: Matching between Conversational and Formal Writings",
        "authors": [
            "Tomasz Jurczyk",
            "Jinho D. Choi"
        ],
        "abstract": "This paper challenges a cross-genre document retrieval task, where the queries are in formal writing and the target documents are in conversational writing. In this task, a query, is a sentence extracted from either a summary or a plot of an episode in a TV show, and the target document consists of transcripts from the corresponding episode. To establish a strong baseline, we employ the current state-of-the-art search engine to perform document retrieval on the dataset collected for this work. We then introduce a structure reranking approach to improve the initial ranking by utilizing syntactic and semantic structures generated by NLP tools. Our evaluation shows an improvement of more than 4% when the structure reranking is applied, which is very promising.\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04546",
        "title": "Linguistic Markers of Influence in Informal Interactions",
        "authors": [
            "Shrimai Prabhumoye",
            "Samridhi Choudhary",
            "Evangelia Spiliopoulou",
            "Christopher Bogart",
            "Carolyn Penstein Rose",
            "Alan W Black"
        ],
        "abstract": "There has been a long standing interest in understanding `Social Influence' both in Social Sciences and in Computational Linguistics. In this paper, we present a novel approach to study and measure interpersonal influence in daily interactions. Motivated by the basic principles of influence, we attempt to identify indicative linguistic features of the posts in an online knitting community. We present the scheme used to operationalize and label the posts with indicator features. Experiments with the identified features show an improvement in the classification accuracy of influence by 3.15%. Our results illustrate the important correlation between the characteristics of the language and its potential to influence others.\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04550",
        "title": "CUNI System for the WMT17 Multimodal Translation Task",
        "authors": [
            "Jind\u0159ich Helcl",
            "Jind\u0159ich Libovick\u00fd"
        ],
        "abstract": "In this paper, we describe our submissions to the WMT17 Multimodal Translation Task. For Task 1 (multimodal translation), our best scoring system is a purely textual neural translation of the source image caption to the target language. The main feature of the system is the use of additional data that was acquired by selecting similar sentences from parallel corpora and by data synthesis with back-translation. For Task 2 (cross-lingual image captioning), our best submitted system generates an English caption which is then translated by the best system used in Task 1. We also present negative results, which are based on ideas that we believe have potential of making improvements, but did not prove to be useful in our particular setup.\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04596",
        "title": "DocTag2Vec: An Embedding Based Multi-label Learning Approach for Document Tagging",
        "authors": [
            "Sheng Chen",
            "Akshay Soni",
            "Aasish Pappu",
            "Yashar Mehdad"
        ],
        "abstract": "Tagging news articles or blog posts with relevant tags from a collection of predefined ones is coined as document tagging in this work. Accurate tagging of articles can benefit several downstream applications such as recommendation and search. In this work, we propose a novel yet simple approach called DocTag2Vec to accomplish this task. We substantially extend Word2Vec and Doc2Vec---two popular models for learning distributed representation of words and documents. In DocTag2Vec, we simultaneously learn the representation of words, documents, and tags in a joint vector space during training, and employ the simple $k$-nearest neighbor search to predict tags for unseen documents. In contrast to previous multi-label learning methods, DocTag2Vec directly deals with raw text instead of provided feature vector, and in addition, enjoys advantages like the learning of tag representation, and the ability of handling newly created tags. To demonstrate the effectiveness of our approach, we conduct experiments on several datasets and show promising results against state-of-the-art methods.\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04652",
        "title": "EmojiNet: An Open Service and API for Emoji Sense Discovery",
        "authors": [
            "Sanjaya Wijeratne",
            "Lakshika Balasuriya",
            "Amit Sheth",
            "Derek Doran"
        ],
        "abstract": "This paper presents the release of EmojiNet, the largest machine-readable emoji sense inventory that links Unicode emoji representations to their English meanings extracted from the Web. EmojiNet is a dataset consisting of: (i) 12,904 sense labels over 2,389 emoji, which were extracted from the web and linked to machine-readable sense definitions seen in BabelNet, (ii) context words associated with each emoji sense, which are inferred through word embedding models trained over Google News corpus and a Twitter message corpus for each emoji sense definition, and (iii) recognizing discrepancies in the presentation of emoji on different platforms, specification of the most likely platform-based emoji sense for a selected set of emoji. The dataset is hosted as an open service with a REST API and is available at ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04653",
        "title": "A Semantics-Based Measure of Emoji Similarity",
        "authors": [
            "Sanjaya Wijeratne",
            "Lakshika Balasuriya",
            "Amit Sheth",
            "Derek Doran"
        ],
        "abstract": "Emoji have grown to become one of the most important forms of communication on the web. With its widespread use, measuring the similarity of emoji has become an important problem for contemporary text processing since it lies at the heart of sentiment analysis, search, and interface design tasks. This paper presents a comprehensive analysis of the semantic similarity of emoji through embedding models that are learned over machine-readable emoji meanings in the EmojiNet knowledge base. Using emoji descriptions, emoji sense labels and emoji sense definitions, and with different training corpora obtained from Twitter and Google News, we develop and test multiple embedding models to measure emoji similarity. To evaluate our work, we create a new dataset called EmoSim508, which assigns human-annotated semantic similarity scores to a set of 508 carefully selected emoji pairs. After validation with EmoSim508, we present a real-world use-case of our emoji embedding models using a sentiment analysis task and show that our models outperform the previous best-performing emoji embedding model on this task. The EmoSim508 dataset and our emoji embedding models are publicly released with this paper and can be downloaded from ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04662",
        "title": "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language",
        "authors": [
            "Alexey Zobnin"
        ],
        "abstract": "Consider a continuous word embedding model. Usually, the cosines between word vectors are used as a measure of similarity of words. These cosines do not change under orthogonal transformations of the embedding space. We demonstrate that, using some canonical orthogonal transformations from SVD, it is possible both to increase the meaning of some components and to make the components more stable under re-learning. We study the interpretability of components for publicly available models for the Russian language (RusVectores, fastText, RDT).\n    ",
        "submission_date": "2017-07-14T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04817",
        "title": "Open-Set Language Identification",
        "authors": [
            "Shervin Malmasi"
        ],
        "abstract": "We present the first open-set language identification experiments using one-class classification. We first highlight the shortcomings of traditional feature extraction methods and propose a hashing-based feature vectorization approach as a solution. Using a dataset of 10 languages from different writing systems, we train a One- Class Support Vector Machine using only a monolingual corpus for each language. Each model is evaluated against a test set of data from all 10 languages and we achieve an average F-score of 0.99, highlighting the effectiveness of this approach for open-set language identification.\n    ",
        "submission_date": "2017-07-16T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04848",
        "title": "Do Neural Nets Learn Statistical Laws behind Natural Language?",
        "authors": [
            "Shuntaro Takahashi",
            "Kumiko Tanaka-Ishii"
        ],
        "abstract": "The performance of deep learning in natural language processing has been spectacular, but the reasons for this success remain unclear because of the inherent complexity of deep learning. This paper provides empirical evidence of its effectiveness and of a limitation of neural networks for language engineering. Precisely, we demonstrate that a neural language model based on long short-term memory (LSTM) effectively reproduces Zipf's law and Heaps' law, two representative statistical properties underlying natural language. We discuss the quality of reproducibility and the emergence of Zipf's law and Heaps' law as training progresses. We also point out that the neural language model has a limitation in reproducing long-range correlation, another statistical property of natural language. This understanding could provide a direction for improving the architectures of neural networks.\n    ",
        "submission_date": "2017-07-16T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04860",
        "title": "Automated Detection of Non-Relevant Posts on the Russian Imageboard \"2ch\": Importance of the Choice of Word Representations",
        "authors": [
            "Amir Bakarov",
            "Olga Gureenkova"
        ],
        "abstract": "This study considers the problem of automated detection of non-relevant posts on Web forums and discusses the approach of resolving this problem by approximation it with the task of detection of semantic relatedness between the given post and the opening post of the forum discussion thread. The approximated task could be resolved through learning the supervised classifier with a composed word embeddings of two posts. Considering that the success in this task could be quite sensitive to the choice of word representations, we propose a comparison of the performance of different word embedding models. We train 7 models (Word2Vec, Glove, Word2Vec-f, Wang2Vec, AdaGram, FastText, Swivel), evaluate embeddings produced by them on dataset of human judgements and compare their performance on the task of non-relevant posts detection. To make the comparison, we propose a dataset of semantic relatedness with posts from one of the most popular Russian Web forums, imageboard \"2ch\", which has challenging lexical and grammatical features.\n    ",
        "submission_date": "2017-07-16T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04879",
        "title": "Listening while Speaking: Speech Chain by Deep Learning",
        "authors": [
            "Andros Tjandra",
            "Sakriani Sakti",
            "Satoshi Nakamura"
        ],
        "abstract": "Despite the close relationship between speech perception and production, research in automatic speech recognition (ASR) and text-to-speech synthesis (TTS) has progressed more or less independently without exerting much mutual influence on each other. In human communication, on the other hand, a closed-loop speech chain mechanism with auditory feedback from the speaker's mouth to her ear is crucial. In this paper, we take a step further and develop a closed-loop speech chain model based on deep learning. The sequence-to-sequence model in close-loop architecture allows us to train our model on the concatenation of both labeled and unlabeled data. While ASR transcribes the unlabeled speech features, TTS attempts to reconstruct the original speech waveform based on the text from ASR. In the opposite direction, ASR also attempts to reconstruct the original text transcription given the synthesized speech. To the best of our knowledge, this is the first deep learning model that integrates human speech perception and production behaviors. Our experimental results show that the proposed approach significantly improved the performance more than separate systems that were only trained with labeled data.\n    ",
        "submission_date": "2017-07-16T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04913",
        "title": "End-to-End Information Extraction without Token-Level Supervision",
        "authors": [
            "Rasmus Berg Palm",
            "Dirk Hovy",
            "Florian Laws",
            "Ole Winther"
        ],
        "abstract": "Most state-of-the-art information extraction approaches rely on token-level labels to find the areas of interest in text. Unfortunately, these labels are time-consuming and costly to create, and consequently, not available for many real-life IE tasks. To make matters worse, token-level labels are usually not the desired output, but just an intermediary step. End-to-end (E2E) models, which take raw text as input and produce the desired output directly, need not depend on token-level labels. We propose an E2E model based on pointer networks, which can be trained directly on pairs of raw input and output text. We evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT movie corpus and compare to neural baselines that do use token-level labels. We achieve competitive results, within a few percentage points of the baselines, showing the feasibility of E2E information extraction without the need for token-level labels. This opens up new possibilities, as for many tasks currently addressed by human extractors, raw input and output data are available, but not token-level labels.\n    ",
        "submission_date": "2017-07-16T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05000",
        "title": "In-Order Transition-based Constituent Parsing",
        "authors": [
            "Jiangming Liu",
            "Yue Zhang"
        ],
        "abstract": "Both bottom-up and top-down strategies have been used for neural transition-based constituent parsing. The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree, where bottom-up strategies and top-down strategies take post-order and pre-order traversal over trees, respectively. Bottom-up parsers benefit from rich features from readily built partial parses, but lack lookahead guidance in the parsing process; top-down parsers benefit from non-local guidance for local decisions, but rely on a strong encoder over the input to predict a constituent hierarchy before its ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05114",
        "title": "Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation",
        "authors": [
            "Baosong Yang",
            "Derek F. Wong",
            "Tong Xiao",
            "Lidia S. Chao",
            "Jingbo Zhu"
        ],
        "abstract": "This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05116",
        "title": "To Normalize, or Not to Normalize: The Impact of Normalization on Part-of-Speech Tagging",
        "authors": [
            "Rob van der Goot",
            "Barbara Plank",
            "Malvina Nissim"
        ],
        "abstract": "Does normalization help Part-of-Speech (POS) tagging accuracy on noisy, non-canonical data? To the best of our knowledge, little is known on the actual impact of normalization in a real-world scenario, where gold error detection is not available. We investigate the effect of automatic normalization on POS tagging of tweets. We also compare normalization to strategies that leverage large amounts of unlabeled data kept in its raw form. Our results show that normalization helps, but does not add consistently beyond just word embedding layer initialization. The latter approach yields a tagging model that is competitive with a Twitter state-of-the-art tagger.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05118",
        "title": "LIG-CRIStAL System for the WMT17 Automatic Post-Editing Task",
        "authors": [
            "Alexandre Berard",
            "Olivier Pietquin",
            "Laurent Besacier"
        ],
        "abstract": "This paper presents the LIG-CRIStAL submission to the shared Automatic Post- Editing task of WMT 2017. We propose two neural post-editing models: a monosource model with a task-specific attention mechanism, which performs particularly well in a low-resource scenario; and a chained architecture which makes use of the source sentence to provide extra context. This latter architecture manages to slightly improve our results when more training data is available. We present and discuss our results on two datasets (en-de and de-en) that are made available for the task.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05127",
        "title": "Neural Reranking for Named Entity Recognition",
        "authors": [
            "Jie Yang",
            "Yue Zhang",
            "Fei Dong"
        ],
        "abstract": "We propose a neural reranking system for named entity recognition (NER). The basic idea is to leverage recurrent neural network models to learn sentence-level patterns that involve named entity mentions. In particular, given an output sentence produced by a baseline NER model, we replace all entity mentions, such as \\textit{Barack Obama}, into their entity types, such as \\textit{PER}. The resulting sentence patterns contain direct output information, yet is less sparse without specific named entities. For example, \"PER was born in LOC\" can be such a pattern. LSTM and CNN structures are utilised for learning deep representations of such sentences for reranking. Results show that our system can significantly improve the NER accuracies over two different baselines, giving the best reported results on a standard benchmark.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05227",
        "title": "Auxiliary Objectives for Neural Error Detection Models",
        "authors": [
            "Marek Rei",
            "Helen Yannakoudakis"
        ],
        "abstract": "We investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to error detection in learner writing. Auxiliary costs provide the model with additional linguistic information, allowing it to learn general-purpose compositional features that can then be exploited for other objectives. Our experiments show that a joint learning approach trained with parallel labels on in-domain data improves performance over the previous best error detection system. While the resulting model has the same number of parameters, the additional objectives allow it to be optimised more efficiently and achieve better performance.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05233",
        "title": "Detecting Off-topic Responses to Visual Prompts",
        "authors": [
            "Marek Rei"
        ],
        "abstract": "Automated methods for essay scoring have made great progress in recent years, achieving accuracies very close to human annotators. However, a known weakness of such automated scorers is not taking into account the semantic relevance of the submitted text. While there is existing work on detecting answer relevance given a textual prompt, very little previous research has been done to incorporate visual writing prompts. We propose a neural architecture and several extensions for detecting off-topic responses to visual prompts and evaluate it on a dataset of texts written by language learners.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05236",
        "title": "Artificial Error Generation with Machine Translation and Syntactic Patterns",
        "authors": [
            "Marek Rei",
            "Mariano Felice",
            "Zheng Yuan",
            "Ted Briscoe"
        ],
        "abstract": "Shortage of available training data is holding back progress in the area of automated error detection. This paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources. We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors. In addition, we explore a system for extracting textual patterns from an annotated corpus, which can then be used to insert errors into grammatically correct sentences. Our experiments show that the inclusion of artificially generated errors significantly improves error detection accuracy on both FCE and CoNLL 2014 datasets.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05246",
        "title": "Learning to select data for transfer learning with Bayesian Optimization",
        "authors": [
            "Sebastian Ruder",
            "Barbara Plank"
        ],
        "abstract": "Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to \\emph{learn} data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are -- to some degree -- transferable across models, domains, and even tasks.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05261",
        "title": "Exploring text datasets by visualizing relevant words",
        "authors": [
            "Franziska Horn",
            "Leila Arras",
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ],
        "abstract": "When working with a new dataset, it is important to first explore and familiarize oneself with it, before applying any advanced machine learning algorithms. However, to the best of our knowledge, no tools exist that quickly and reliably give insight into the contents of a selection of documents with respect to what distinguishes them from other documents belonging to different categories. In this paper we propose to extract `relevant words' from a collection of texts, which summarize the contents of documents belonging to a certain class (or discovered cluster in the case of unlabeled datasets), and visualize them in word clouds to allow for a survey of salient features at a glance. We compare three methods for extracting relevant words and demonstrate the usefulness of the resulting word clouds by providing an overview of the classes contained in a dataset of scientific publications as well as by discovering trending topics from recent New York Times article snippets.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05266",
        "title": "A Simple Language Model based on PMI Matrix Approximations",
        "authors": [
            "Oren Melamud",
            "Ido Dagan",
            "Jacob Goldberger"
        ],
        "abstract": "In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec's algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05288",
        "title": "MAG: A Multilingual, Knowledge-base Agnostic and Deterministic Entity Linking Approach",
        "authors": [
            "Diego Moussallem",
            "Ricardo Usbeck",
            "Michael R\u00f6der",
            "Axel-Cyrille Ngonga Ngomo"
        ],
        "abstract": "Entity linking has recently been the subject of a significant body of research. Currently, the best performing approaches rely on trained mono-lingual models. Porting these approaches to other languages is consequently a difficult endeavor as it requires corresponding training data and retraining of the models. We address this drawback by presenting a novel multilingual, knowledge-based agnostic and deterministic approach to entity linking, dubbed MAG. MAG is based on a combination of context-based retrieval on structured knowledge bases and graph algorithms. We evaluate MAG on 23 data sets and in 7 languages. Our results show that the best approach trained on English datasets (PBOH) achieves a micro F-measure that is up to 4 times worse on datasets in other languages. MAG, on the other hand, achieves state-of-the-art performance on English datasets and reaches a micro F-measure that is up to 0.6 higher than that of PBOH on non-English languages.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05315",
        "title": "Unsupervised Iterative Deep Learning of Speech Features and Acoustic Tokens with Applications to Spoken Term Detection",
        "authors": [
            "Cheng-Tao Chung",
            "Cheng-Yu Tsai",
            "Chia-Hsiang Liu",
            "Lin-Shan Lee"
        ],
        "abstract": "In this paper we aim to automatically discover high quality frame-level speech features and acoustic tokens directly from unlabeled speech data. A Multi-granular Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters describing the model configuration. These different sets of acoustic tokens carry different characteristics for the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on frame-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. The multi-granular acoustic token sets and the frame-level speech features can be iteratively optimized in the iterative deep learning framework. We call this framework the Multi-granular Acoustic Tokenizing Deep Neural Network (MATDNN). The results were evaluated using the metrics and corpora defined in the Zero Resource Speech Challenge organized at Interspeech 2015, and improved performance was obtained with a set of experiments of query-by-example spoken term detection on the same corpora. Visualization for the discovered tokens against the English phonemes was also shown.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05436",
        "title": "Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder",
        "authors": [
            "Huadong Chen",
            "Shujian Huang",
            "David Chiang",
            "Jiajun Chen"
        ],
        "abstract": "Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05438",
        "title": "Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation",
        "authors": [
            "Huadong Chen",
            "Shujian Huang",
            "David Chiang",
            "Xinyu Dai",
            "Jiajun Chen"
        ],
        "abstract": "Pairwise ranking methods are the basis of many widely used discriminative training approaches for structure prediction problems in natural language processing(NLP). Decomposing the problem of ranking hypotheses into pairwise comparisons enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder learning. We propose a listwise learning framework for structure prediction problems such as machine translation. Our framework directly models the entire translation list's ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05468",
        "title": "Detecting Intentional Lexical Ambiguity in English Puns",
        "authors": [
            "Elena Mikhalkova",
            "Yuri Karyakin"
        ],
        "abstract": "The article describes a model of automatic analysis of puns, where a word is intentionally used in two meanings at the same time (the target word). We employ Roget's Thesaurus to discover two groups of words which, in a pun, form around two abstract bits of meaning (semes). They become a semantic vector, based on which an SVM classifier learns to recognize puns, reaching a score 0.73 for F-measure. We apply several rule-based methods to locate intentionally ambiguous (target) words, based on structural and semantic criteria. It appears that the structural criterion is more effective, although it possibly characterizes only the tested dataset. The results we get correlate with the results of other teams at SemEval-2017 competition (Task 7 Detection and Interpretation of English Puns) considering effects of using supervised learning models and word statistics.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05479",
        "title": "PunFields at SemEval-2017 Task 7: Employing Roget's Thesaurus in Automatic Pun Recognition and Interpretation",
        "authors": [
            "Elena Mikhalkova",
            "Yuri Karyakin"
        ],
        "abstract": "The article describes a model of automatic interpretation of English puns, based on Roget's Thesaurus, and its implementation, PunFields. In a pun, the algorithm discovers two groups of words that belong to two main semantic fields. The fields become a semantic vector based on which an SVM classifier learns to recognize puns. A rule-based model is then applied for recognition of intentionally ambiguous (target) words and their definitions. In SemEval Task 7 PunFields shows a considerably good result in pun classification, but requires improvement in searching for the target word and its definition.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05481",
        "title": "A Comparative Analysis of Social Network Pages by Interests of Their Followers",
        "authors": [
            "Elena Mikhalkova",
            "Nadezhda Ganzherli",
            "Yuri Karyakin"
        ],
        "abstract": "Being a matter of cognition, user interests should be apt to classification independent of the language of users, social network and content of interest itself. To prove it, we analyze a collection of English and Russian Twitter and Vkontakte community pages by interests of their followers. First, we create a model of Major Interests (MaIs) with the help of expert analysis and then classify a set of pages using machine learning algorithms (SVM, Neural Network, Naive Bayes, and some other). We take three interest domains that are typical of both English and Russian-speaking communities: football, rock music, vegetarianism. The results of classification show a greater correlation between Russian-Vkontakte and Russian-Twitter pages while English-Twitterpages appear to provide the highest score.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05501",
        "title": "Story Generation from Sequence of Independent Short Descriptions",
        "authors": [
            "Parag Jain",
            "Priyanka Agrawal",
            "Abhijit Mishra",
            "Mohak Sukhwani",
            "Anirban Laha",
            "Karthik Sankaranarayanan"
        ],
        "abstract": "Existing Natural Language Generation (NLG) systems are weak AI systems and exhibit limited capabilities when language generation tasks demand higher levels of creativity, originality and brevity. Effective solutions or, at least evaluations of modern NLG paradigms for such creative tasks have been elusive, unfortunately. This paper introduces and addresses the task of coherent story generation from independent descriptions, describing a scene or an event. Towards this, we explore along two popular text-generation paradigms -- (1) Statistical Machine Translation (SMT), posing story generation as a translation problem and (2) Deep Learning, posing story generation as a sequence to sequence learning problem. In SMT, we chose two popular methods such as phrase based SMT (PB-SMT) and syntax based SMT (SYNTAX-SMT) to `translate' the incoherent input text into stories. We then implement a deep recurrent neural network (RNN) architecture that encodes sequence of variable length input descriptions to corresponding latent representations and decodes them to produce well formed comprehensive story like summaries. The efficacy of the suggested approaches is demonstrated on a publicly available dataset with the help of popular machine translation and summarization evaluation metrics.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05589",
        "title": "On the State of the Art of Evaluation in Neural Language Models",
        "authors": [
            "G\u00e1bor Melis",
            "Chris Dyer",
            "Phil Blunsom"
        ],
        "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05635",
        "title": "Spherical Paragraph Model",
        "authors": [
            "Ruqing Zhang",
            "Jiafeng Guo",
            "Yanyan Lan",
            "Jun Xu",
            "Xueqi Cheng"
        ],
        "abstract": "Representing texts as fixed-length vectors is central to many language processing tasks. Most traditional methods build text representations based on the simple Bag-of-Words (BoW) representation, which loses the rich semantic relations between words. Recent advances in natural language processing have shown that semantically meaningful representations of words can be efficiently acquired by distributed models, making it possible to build text representations based on a better foundation called the Bag-of-Word-Embedding (BoWE) representation. However, existing text representation methods using BoWE often lack sound probabilistic foundations or cannot well capture the semantic relatedness encoded in word vectors. To address these problems, we introduce the Spherical Paragraph Model (SPM), a probabilistic generative model based on BoWE, for text representation. SPM has good probabilistic interpretability and can fully leverage the rich semantics of words, the word co-occurrence information as well as the corpus-wide information to help the representation learning of texts. Experimental results on topical classification and sentiment analysis demonstrate that SPM can achieve new state-of-the-art performances on several benchmark datasets.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05850",
        "title": "A Short Survey of Biomedical Relation Extraction Techniques",
        "authors": [
            "Elham Shahab"
        ],
        "abstract": "Biomedical information is growing rapidly in the recent years and retrieving useful data through information extraction system is getting more attention. In the current research, we focus on different aspects of relation extraction techniques in biomedical domain and briefly describe the state-of-the-art for relation extraction between a variety of biological elements.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05853",
        "title": "Encoding Word Confusion Networks with Recurrent Neural Networks for Dialog State Tracking",
        "authors": [
            "Glorianna Jagfeld",
            "Ngoc Thang Vu"
        ],
        "abstract": "This paper presents our novel method to encode word confusion networks, which can represent a rich hypothesis space of automatic speech recognition systems, via recurrent neural networks. We demonstrate the utility of our approach for the task of dialog state tracking in spoken dialog systems that relies on automatic speech recognition output. Encoding confusion networks outperforms encoding the best hypothesis of the automatic speech recognition in a neural system for dialog state tracking on the well-known second Dialog State Tracking Challenge dataset.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05928",
        "title": "Deep Active Learning for Named Entity Recognition",
        "authors": [
            "Yanyao Shen",
            "Hyokun Yun",
            "Zachary C. Lipton",
            "Yakov Kronrod",
            "Animashree Anandkumar"
        ],
        "abstract": "Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2018-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05967",
        "title": "Measuring Thematic Fit with Distributional Feature Overlap",
        "authors": [
            "Enrico Santus",
            "Emmanuele Chersoni",
            "Alessandro Lenci",
            "Philippe Blache"
        ],
        "abstract": "In this paper, we introduce a new distributional method for modeling predicate-argument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems. Moreover, it provides an explicit representation of the features characterizing verb-specific semantic roles.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06002",
        "title": "Argotario: Computational Argumentation Meets Serious Games",
        "authors": [
            "Ivan Habernal",
            "Raffael Hannemann",
            "Christian Pollak",
            "Christopher Klamm",
            "Patrick Pauli",
            "Iryna Gurevych"
        ],
        "abstract": "An important skill in critical thinking and argumentation is the ability to spot and recognize fallacies. Fallacious arguments, omnipresent in argumentative discourse, can be deceptive, manipulative, or simply leading to `wrong moves' in a discussion. Despite their importance, argumentation scholars and NLP researchers with focus on argumentation quality have not yet investigated fallacies empirically. The nonexistence of resources dealing with fallacious argumentation calls for scalable approaches to data acquisition and annotation, for which the serious games methodology offers an appealing, yet unexplored, alternative. We present Argotario, a serious game that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06012",
        "title": "Modeling Target-Side Inflection in Neural Machine Translation",
        "authors": [
            "Ale\u0161 Tamchyna",
            "Marion Weller-Di Marco",
            "Alexander Fraser"
        ],
        "abstract": "NMT systems have problems with large vocabulary sizes. Byte-pair encoding (BPE) is a popular approach to solving this problem, but while BPE allows the system to generate any target-side word, it does not enable effective generalization over the rich vocabulary in morphologically rich languages with strong inflectional phenomena. We introduce a simple approach to overcome this problem by training a system to produce the lemma of a word and its morphologically rich POS tag, which is then followed by a deterministic generation step. We apply this strategy for English-Czech and English-German translation scenarios, obtaining improvements in both settings. We furthermore show that the improvement is not due to only adding explicit morphological information.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06065",
        "title": "Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition",
        "authors": [
            "Taesup Kim",
            "Inchul Song",
            "Yoshua Bengio"
        ],
        "abstract": "Layer normalization is a recently introduced technique for normalizing the activities of neurons in deep neural networks to improve the training speed and stability. In this paper, we introduce a new layer normalization technique called Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling in speech recognition. By dynamically generating the scaling and shifting parameters in layer normalization, DLN adapts neural acoustic models to the acoustic variability arising from various factors such as speakers, channel noises, and environments. Unlike other adaptive acoustic models, our proposed approach does not require additional adaptation data or speaker information such as i-vectors. Moreover, the model size is fixed as it dynamically generates adaptation parameters. We apply our proposed DLN to deep bidirectional LSTM acoustic models and evaluate them on two benchmark datasets for large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The experimental results show that our DLN improves neural acoustic models in terms of transcription accuracy by dynamically adapting to various speakers and environments.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06100",
        "title": "Discovering topics in text datasets by visualizing relevant words",
        "authors": [
            "Franziska Horn",
            "Leila Arras",
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ],
        "abstract": "When dealing with large collections of documents, it is imperative to quickly get an overview of the texts' contents. In this paper we show how this can be achieved by using a clustering algorithm to identify topics in the dataset and then selecting and visualizing relevant words, which distinguish a group of documents from the rest of the texts, to summarize the contents of the documents belonging to each topic. We demonstrate our approach by discovering trending topics in a collection of New York Times article snippets.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06130",
        "title": "Improving Language Modeling using Densely Connected Recurrent Neural Networks",
        "authors": [
            "Fr\u00e9deric Godin",
            "Joni Dambre",
            "Wesley De Neve"
        ],
        "abstract": "In this paper, we introduce the novel concept of densely connected layers into recurrent neural networks. We evaluate our proposed architecture on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al. 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06151",
        "title": "Expect the unexpected: Harnessing Sentence Completion for Sarcasm Detection",
        "authors": [
            "Aditya Joshi",
            "Samarth Agrawal",
            "Pushpak Bhattacharyya",
            "Mark Carman"
        ],
        "abstract": "The trigram `I love being' is expected to be followed by positive words such as `happy'. In a sarcastic sentence, however, the word `ignored' may be observed. The expected and the observed words are, thus, incongruous. We model sarcasm detection as the task of detecting incongruity between an observed and an expected word. In order to obtain the expected word, we use Context2Vec, a sentence completion library based on Bidirectional LSTM. However, since the exact word where such an incongruity occurs may not be known in advance, we present two approaches: an All-words approach (which consults sentence completion for every content word) and an Incongruous words-only approach (which consults sentence completion for the 50% most incongruous content words). The approaches outperform reported values for tweets but not for discussion forum posts. This is likely to be because of redundant consultation of sentence completion for discussion forum posts. Therefore, we consider an oracle case where the exact incongruous word is manually labeled in a corpus reported in past work. In this case, the performance is higher than the all-words approach. This sets up the promise for using sentence completion for sarcasm detection.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06167",
        "title": "Sentence-level quality estimation by predicting HTER as a multi-component metric",
        "authors": [
            "Eleftherios Avramidis"
        ],
        "abstract": "This submission investigates alternative machine learning models for predicting the HTER score on the sentence level. Instead of directly predicting the HTER score, we suggest a model that jointly predicts the amount of the 4 distinct post-editing operations, which are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. Without any feature exploration, a multi-layer perceptron with 4 outputs yields small but significant improvements over the baseline.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06195",
        "title": "Fast and Accurate OOV Decoder on High-Level Features",
        "authors": [
            "Yuri Khokhlov",
            "Natalia Tomashenko",
            "Ivan Medennikov",
            "Alexei Romanenko"
        ],
        "abstract": "This work proposes a novel approach to out-of-vocabulary (OOV) keyword search (KWS) task. The proposed approach is based on using high-level features from an automatic speech recognition (ASR) system, so called phoneme posterior based (PPB) features, for decoding. These features are obtained by calculating time-dependent phoneme posterior probabilities from word lattices, followed by their smoothing. For the PPB features we developed a special novel very fast, simple and efficient OOV decoder. Experimental results are presented on the Georgian language from the IARPA Babel Program, which was the test language in the OpenKWS 2016 evaluation campaign. The results show that in terms of maximum term weighted value (MTWV) metric and computational speed, for single ASR systems, the proposed approach significantly outperforms the state-of-the-art approach based on using in-vocabulary proxies for OOV keywords in the indexed database. The comparison of the two OOV KWS approaches on the fusion results of the nine different ASR systems demonstrates that the proposed OOV decoder outperforms the proxy-based approach in terms of MTWV metric given the comparable processing speed. Other important advantages of the OOV decoder include extremely low memory consumption and simplicity of its implementation and parameter optimization.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06226",
        "title": "The Role of Conversation Context for Sarcasm Detection in Online Interactions",
        "authors": [
            "Debanjan Ghosh",
            "Alexander Richard Fabbri",
            "Smaranda Muresan"
        ],
        "abstract": "Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, speaker's sarcastic intent is not always obvious without additional context. Focusing on social media discussions, we investigate two issues: (1) does modeling of conversation context help in sarcasm detection and (2) can we understand what part of conversation context triggered the sarcastic reply. To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the sarcastic response. We show that the conditional LSTM network (Rocktaschel et al., 2015) and LSTM networks with sentence level attention on context and response outperform the LSTM model that reads only the response. To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06265",
        "title": "Unsupervised Domain Adaptation for Robust Speech Recognition via Variational Autoencoder-Based Data Augmentation",
        "authors": [
            "Wei-Ning Hsu",
            "Yu Zhang",
            "James Glass"
        ],
        "abstract": "Domain mismatch between training and testing can lead to significant degradation in performance in many machine learning scenarios. Unfortunately, this is not a rare situation for automatic speech recognition deployments in real-world applications. Research on robust speech recognition can be regarded as trying to overcome this domain mismatch issue. In this paper, we address the unsupervised domain adaptation problem for robust speech recognition, where both source and target domain speech are presented, but word transcripts are only available for the source domain speech. We present novel augmentation-based methods that transform speech in a way that does not change the transcripts. Specifically, we first train a variational autoencoder on both source and target domain data (without supervision) to learn a latent representation of speech. We then transform nuisance attributes of speech that are irrelevant to recognition by modifying the latent representations, in order to augment labeled training data with additional data whose distribution is more similar to the target domain. The proposed method is evaluated on the CHiME-4 dataset and reduces the absolute word error rate (WER) by as much as 35% compared to the non-adapted baseline.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06299",
        "title": "Reward-Balancing for Statistical Spoken Dialogue Systems using Multi-objective Reinforcement Learning",
        "authors": [
            "Stefan Ultes",
            "Pawe\u0142 Budzianowski",
            "I\u00f1igo Casanueva",
            "Nikola Mrk\u0161i\u0107",
            "Lina Rojas-Barahona",
            "Pei-Hao Su",
            "Tsung-Hsien Wen",
            "Milica Ga\u0161i\u0107",
            "Steve Young"
        ],
        "abstract": "Reinforcement learning is widely used for dialogue policy optimization where the reward function often consists of more than one component, e.g., the dialogue success and the dialogue length. In this work, we propose a structured method for finding a good balance between these components by searching for the optimal reward component weighting. To render this search feasible, we use multi-objective reinforcement learning to significantly reduce the number of training dialogues required. We apply our proposed method to find optimized component weights for six domains and compare them to a default baseline.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06320",
        "title": "Learning Visually Grounded Sentence Representations",
        "authors": [
            "Douwe Kiela",
            "Alexis Conneau",
            "Allan Jabri",
            "Maximilian Nickel"
        ],
        "abstract": "We introduce a variety of models, trained on a supervised image captioning corpus to predict the image features for a given caption, to perform sentence representation grounding. We train a grounded sentence encoder that achieves good performance on COCO caption and image retrieval and subsequently show that this encoder can successfully be transferred to various NLP tasks, with improved performance over text-only models. Lastly, we analyze the contribution of grounding, and show that word embeddings learned by this system outperform non-grounded ones.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2018-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06341",
        "title": "A Sub-Character Architecture for Korean Language Processing",
        "authors": [
            "Karl Stratos"
        ],
        "abstract": "We introduce a novel sub-character architecture that exploits a unique compositional structure of the Korean language. Our method decomposes each character into a small set of primitive phonetic units called jamo letters from which character- and word-level representations are induced. The jamo letters divulge syntactic and semantic information that is difficult to access with conventional character-level units. They greatly alleviate the data sparsity problem, reducing the observation space to 1.6% of the original while increasing accuracy in our experiments. We apply our architecture to dependency parsing and achieve dramatic improvement over strong lexical baselines.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06357",
        "title": "Improving Discourse Relation Projection to Build Discourse Annotated Corpora",
        "authors": [
            "Majid Laali",
            "Leila Kosseim"
        ],
        "abstract": "The naive approach to annotation projection is not effective to project discourse annotations from one language to another because implicit discourse relations are often changed to explicit ones and vice-versa in the translation. In this paper, we propose a novel approach based on the intersection between statistical word-alignment models to identify unsupported discourse annotations. This approach identified 65% of the unsupported annotations in the English-French parallel sentences from Europarl. By filtering out these unsupported annotations, we induced the first PDTB-style discourse annotated corpus for French from Europarl. We then used this corpus to train a classifier to identify the discourse-usage of French discourse connectives and show a 15% improvement of F1-score compared to the classifier trained on the non-filtered annotations.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06378",
        "title": "Large-Scale Goodness Polarity Lexicons for Community Question Answering",
        "authors": [
            "Todor Mihaylov",
            "Daniel Belchev",
            "Yasen Kiprov",
            "Ivan Koychev",
            "Preslav Nakov"
        ],
        "abstract": "We transfer a key idea from the field of sentiment analysis to a new domain: community question answering (cQA). The cQA task we are interested in is the following: given a question and a thread of comments, we want to re-rank the comments so that the ones that are good answers to the question would be ranked higher than the bad ones. We notice that good vs. bad comments use specific vocabulary and that one can often predict the goodness/badness of a comment even ignoring the question, based on the comment contents only. This leads us to the idea to build a good/bad polarity lexicon as an analogy to the positive/negative sentiment polarity lexicons, commonly used in sentiment analysis. In particular, we use pointwise mutual information in order to build large-scale goodness polarity lexicons in a semi-supervised manner starting with a small number of initial seeds. The evaluation results show an improvement of 0.7 MAP points absolute over a very strong baseline and state-of-the art performance on SemEval-2016 Task 3.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06456",
        "title": "Revisiting Selectional Preferences for Coreference Resolution",
        "authors": [
            "Benjamin Heinzerling",
            "Nafise Sadat Moosavi",
            "Michael Strube"
        ],
        "abstract": "Selectional preferences have long been claimed to be essential for coreference resolution. However, they are mainly modeled only implicitly by current coreference resolvers. We propose a dependency-based embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. We show that the incorporation of our model improves coreference resolution performance on the CoNLL dataset, matching the state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile such improvements are.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06480",
        "title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones",
        "authors": [
            "Zhenisbek Assylbekov",
            "Rustem Takhanov",
            "Bagdat Myrzakhmetov",
            "Jonathan N. Washington"
        ],
        "abstract": "Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06519",
        "title": "Language Transfer of Audio Word2Vec: Learning Audio Segment Representations without Target Language Data",
        "authors": [
            "Chia-Hao Shen",
            "Janet Y. Sung",
            "Hung-Yi Lee"
        ],
        "abstract": "Audio Word2Vec offers vector representations of fixed dimensionality for variable-length audio segments using Sequence-to-sequence Autoencoder (SA). These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with real world applications such as query-by-example Spoken Term Detection (STD). This paper examines the capability of language transfer of Audio Word2Vec. We train SA from one language (source language) and use it to extract the vector representation of the audio segments of another language (target language). We found that SA can still catch phonetic structure from the audio segments of the target language if the source and target languages are similar. In query-by-example STD, we obtain the vector representations from the SA learned from a large amount of source language data, and found them surpass the representations from naive encoder and SA directly learned from a small amount of target language data. The result shows that it is possible to learn Audio Word2Vec model from high-resource languages and use it on low-resource languages. This further expands the usability of Audio Word2Vec.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06556",
        "title": "High-risk learning: acquiring new word vectors from tiny data",
        "authors": [
            "Aurelie Herbelot",
            "Marco Baroni"
        ],
        "abstract": "Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn 'a good vector' for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06690",
        "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning",
        "authors": [
            "Wenhan Xiong",
            "Thien Hoang",
            "William Yang Wang"
        ],
        "abstract": "We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2018-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06799",
        "title": "Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks",
        "authors": [
            "Nils Reimers",
            "Iryna Gurevych"
        ],
        "abstract": "Selecting optimal parameters for a neural network architecture can often make the difference between mediocre and state-of-the-art performance. However, little is published which parameters and design choices should be evaluated or selected making the correct hyperparameter optimization often a \"black art that requires expert experiences\" (Snoek et al., 2012). In this paper, we evaluate the importance of different network design choices and hyperparameters for five common linguistic sequence tagging tasks (POS, Chunking, NER, Entity Recognition, and Event Detection). We evaluated over 50.000 different setups and found, that some parameters, like the pre-trained word embeddings or the last layer of the network, have a large impact on the performance, while other parameters, for example the number of LSTM layers or the number of recurrent units, are of minor importance. We give a recommendation on a configuration that performs well among different tasks.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06806",
        "title": "Shallow reading with Deep Learning: Predicting popularity of online content using only its title",
        "authors": [
            "Wociech Stokowiec",
            "Tomasz Trzcinski",
            "Krzysztof Wolk",
            "Krzysztof Marasek",
            "Przemyslaw Rokita"
        ],
        "abstract": "With the ever decreasing attention span of contemporary Internet users, the title of online content (such as a news article or video) can be a major factor in determining its popularity. To take advantage of this phenomenon, we propose a new method based on a bidirectional Long Short-Term Memory (LSTM) neural network designed to predict the popularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small. To our knowledge, this is the first attempt of applying popularity prediction using only textual information from the title.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06841",
        "title": "An Error-Oriented Approach to Word Embedding Pre-Training",
        "authors": [
            "Youmna Farag",
            "Marek Rei",
            "Ted Briscoe"
        ],
        "abstract": "We propose a novel word embedding pre-training approach that exploits writing errors in learners' scripts. We compare our method to previous models that tune the embeddings based on script scores and the discrimination between correct and corrupt word contexts in addition to the generic commonly-used embeddings pre-trained on large corpora. The comparison is achieved by using the aforementioned models to bootstrap a neural network that learns to predict a holistic score for scripts. Furthermore, we investigate augmenting our model with error corrections and monitor the impact on performance. Our results show that our error-oriented approach outperforms other comparable ones which is further demonstrated when training on more data. Additionally, extending the model with corrections provides further performance gains when data sparsity is an issue.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06875",
        "title": "Why We Need New Evaluation Metrics for NLG",
        "authors": [
            "Jekaterina Novikova",
            "Ond\u0159ej Du\u0161ek",
            "Amanda Cercas Curry",
            "Verena Rieser"
        ],
        "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06878",
        "title": "Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",
        "authors": [
            "Alexander Panchenko",
            "Fide Marten",
            "Eugen Ruppert",
            "Stefano Faralli",
            "Dmitry Ustalov",
            "Simone Paolo Ponzetto",
            "Chris Biemann"
        ],
        "abstract": "Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our system, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06885",
        "title": "SGNMT -- A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies",
        "authors": [
            "Felix Stahlberg",
            "Eva Hasler",
            "Danielle Saunders",
            "Bill Byrne"
        ],
        "abstract": "This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices, $n$-best lists or other kinds of scores and constraints. Predictors can be combined with other predictors to form complex decoding tasks. SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for prototyping new research ideas. SGNMT is actively being used by students in the MPhil program in Machine Learning, Speech and Language Technology at the University of Cambridge for course work and theses, as well as for most of the research work in our group.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06932",
        "title": "A study on text-score disagreement in online reviews",
        "authors": [
            "Michela Fazzolari",
            "Vittoria Cozza",
            "Marinella Petrocchi",
            "Angelo Spognardi"
        ],
        "abstract": "In this paper, we focus on online reviews and employ artificial intelligence tools, taken from the cognitive computing field, to help understanding the relationships between the textual part of the review and the assigned numerical score. We move from the intuitions that 1) a set of textual reviews expressing different sentiments may feature the same score (and vice-versa); and 2) detecting and analyzing the mismatches between the review content and the actual score may benefit both service providers and consumers, by highlighting specific factors of satisfaction (and dissatisfaction) in texts.\n",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06945",
        "title": "Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation",
        "authors": [
            "Ivan Vuli\u0107",
            "Nikola Mrk\u0161i\u0107",
            "Anna Korhonen"
        ],
        "abstract": "Existing approaches to automatic VerbNet-style verb classification are heavily dependent on feature engineering and therefore limited to languages with mature NLP pipelines. In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages. To the best of our knowledge, this is the first study which demonstrates how the architectures for learning word embeddings can be applied to this challenging syntactic-semantic task. Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet. A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using vector dimensions as features for learning verb classes. Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06957",
        "title": "Reconstruction of Word Embeddings from Sub-Word Parameters",
        "authors": [
            "Karl Stratos"
        ],
        "abstract": "Pre-trained word embeddings improve the performance of a neural model at the cost of increasing the model size. We propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level. Our approach is quite simple: before task-specific training, we first optimize sub-word parameters to reconstruct pre-trained word embeddings using various distance measures. We report interesting results on a variety of tasks: word similarity, word analogy, and part-of-speech tagging.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06961",
        "title": "Mimicking Word Embeddings using Subword RNNs",
        "authors": [
            "Yuval Pinter",
            "Robert Guthrie",
            "Jacob Eisenstein"
        ],
        "abstract": "Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised character-based model in low-resource settings.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06971",
        "title": "Split and Rephrase",
        "authors": [
            "Shashi Narayan",
            "Claire Gardent",
            "Shay B. Cohen",
            "Anastasia Shimorina"
        ],
        "abstract": "We propose a new sentence simplification task (Split-and-Rephrase) where the aim is to split a complex sentence into a meaning preserving sequence of shorter sentences. Like sentence simplification, splitting-and-rephrasing has the potential of benefiting both natural language processing and societal applications. Because shorter sentences are generally better processed by NLP systems, it could be used as a preprocessing step which facilitates and improves the performance of parsers, semantic role labellers and machine translation systems. It should also be of use for people with reading disabilities because it allows the conversion of longer sentences into shorter ones. This paper makes two contributions towards this new task. First, we create and make available a benchmark consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning. Second, we propose five models (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06996",
        "title": "A Sentiment-and-Semantics-Based Approach for Emotion Detection in Textual Conversations",
        "authors": [
            "Umang Gupta",
            "Ankush Chatterjee",
            "Radhakrishnan Srikanth",
            "Puneet Agrawal"
        ],
        "abstract": "Emotions are physiological states generated in humans in reaction to internal or external events. They are complex and studied across numerous fields including computer science. As humans, on reading \"Why don't you ever text me!\" we can either interpret it as a sad or angry emotion and the same ambiguity exists for machines. Lack of facial expressions and voice modulations make detecting emotions from text a challenging problem. However, as humans increasingly communicate using text messaging applications, and digital agents gain popularity in our society, it is essential that these digital agents are emotion aware, and respond accordingly.\n",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2018-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07045",
        "title": "End-to-end Neural Coreference Resolution",
        "authors": [
            "Kenton Lee",
            "Luheng He",
            "Mike Lewis",
            "Luke Zettlemoyer"
        ],
        "abstract": "We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07048",
        "title": "Progressive Joint Modeling in Unsupervised Single-channel Overlapped Speech Recognition",
        "authors": [
            "Zhehuai Chen",
            "Jasha Droppo",
            "Jinyu Li",
            "Wayne Xiong"
        ],
        "abstract": "Unsupervised single-channel overlapped speech recognition is one of the hardest problems in automatic speech recognition (ASR). Permutation invariant training (PIT) is a state of the art model-based approach, which applies a single neural network to solve this single-input, multiple-output modeling problem. We propose to advance the current state of the art by imposing a modular structure on the neural network, applying a progressive pretraining regimen, and improving the objective function with transfer learning and a discriminative training criterion. The modular structure splits the problem into three sub-tasks: frame-wise interpreting, utterance-level speaker tracing, and speech recognition. The pretraining regimen uses these modules to solve progressively harder tasks. Transfer learning leverages parallel clean speech to improve the training targets for the network. Our discriminative training formulation is a modification of standard formulations, that also penalizes competing outputs of the system. Experiments are conducted on the artificial overlapped Switchboard and hub5e-swb dataset. The proposed framework achieves over 30% relative improvement of WER over both a strong jointly trained system, PIT for ASR, and a separately optimized system, PIT for speech separation with clean speech ASR model. The improvement comes from better model generalization, training efficiency and the sequence level linguistic knowledge integration.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07062",
        "title": "A Pilot Study of Domain Adaptation Effect for Neural Abstractive Summarization",
        "authors": [
            "Xinyu Hua",
            "Lu Wang"
        ],
        "abstract": "We study the problem of domain adaptation for neural abstractive summarization. We make initial efforts in investigating what information can be transferred to a new domain. Experimental results on news stories and opinion articles indicate that neural summarization model benefits from pre-training based on extractive summaries. We also find that the combination of in-domain and out-of-domain setup yields better summaries when in-domain data is insufficient. Further analysis shows that, the model is capable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style for a target domain.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07086",
        "title": "Identifying civilians killed by police with distantly supervised entity-event extraction",
        "authors": [
            "Katherine A. Keith",
            "Abram Handler",
            "Michael Pinkham",
            "Cara Magliozzi",
            "Joshua McDuffie",
            "Brendan O'Connor"
        ],
        "abstract": "We propose a new, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases.\n    ",
        "submission_date": "2017-07-22T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07129",
        "title": "Predicting the Gender of Indonesian Names",
        "authors": [
            "Ali Akbar Septiandri"
        ],
        "abstract": "We investigated a way to predict the gender of a name using character-level Long-Short Term Memory (char-LSTM). We compared our method with some conventional machine learning methods, namely Naive Bayes, logistic regression, and XGBoost with n-grams as the features. We evaluated the models on a dataset consisting of the names of Indonesian people. It is not common to use a family name as the surname in Indonesian culture, except in some ethnicities. Therefore, we inferred the gender from both full names and first names. The results show that we can achieve 92.25% accuracy from full names, while using first names only yields 90.65% accuracy. These results are better than the ones from applying the classical machine learning algorithms to n-grams.\n    ",
        "submission_date": "2017-07-22T00:00:00",
        "last_modified_date": "2017-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07167",
        "title": "Attention-Based End-to-End Speech Recognition on Voice Search",
        "authors": [
            "Changhao Shan",
            "Junbo Zhang",
            "Yujun Wang",
            "Lei Xie"
        ],
        "abstract": "Recently, there has been a growing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. In this paper, we explore the use of attention-based encoder-decoder model for Mandarin speech recognition on a voice search task. Previous attempts have shown that applying attention-based encoder-decoder to Mandarin speech recognition was quite difficult due to the logographic orthography of Mandarin, the large vocabulary and the conditional dependency of the attention model. In this paper, we use character embedding to deal with the large vocabulary. Several tricks are used for effective model training, including L2 regularization, Gaussian weight noise and frame skipping. We compare two attention mechanisms and use attention smoothing to cover long context in the attention model. Taken together, these tricks allow us to finally achieve a character error rate (CER) of 3.58% and a sentence error rate (SER) of 7.43% on the MiTV voice search dataset. While together with a trigram language model, CER and SER reach 2.81% and 5.77%, respectively.\n    ",
        "submission_date": "2017-07-22T00:00:00",
        "last_modified_date": "2018-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07182",
        "title": "Native Language Identification on Text and Speech",
        "authors": [
            "Marcos Zampieri",
            "Alina Maria Ciobanu",
            "Liviu P. Dinu"
        ],
        "abstract": "This paper presents an ensemble system combining the output of multiple SVM classifiers to native language identification (NLI). The system was submitted to the NLI Shared Task 2017 fusion track which featured students essays and spoken responses in form of audio transcriptions and iVectors by non-native English speakers of eleven native languages. Our system competed in the challenge under the team name ZCD and was based on an ensemble of SVM classifiers trained on character n-grams achieving 83.58% accuracy and ranking 3rd in the shared task.\n    ",
        "submission_date": "2017-07-22T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07191",
        "title": "MoodSwipe: A Soft Keyboard that Suggests Messages Based on User-Specified Emotions",
        "authors": [
            "Chieh-Yang Huang",
            "Tristan Labetoulle",
            "Ting-Hao Kenneth Huang",
            "Yi-Pei Chen",
            "Hung-Chen Chen",
            "Vallari Srivastava",
            "Lun-Wei Ku"
        ],
        "abstract": "We present MoodSwipe, a soft keyboard that suggests text messages given the user-specified emotions utilizing the real dialog data. The aim of MoodSwipe is to create a convenient user interface to enjoy the technology of emotion classification and text suggestion, and at the same time to collect labeled data automatically for developing more advanced technologies. While users select the MoodSwipe keyboard, they can type as usual but sense the emotion conveyed by their text and receive suggestions for their message as a benefit. In MoodSwipe, the detected emotions serve as the medium for suggested texts, where viewing the latter is the incentive to correcting the former. We conduct several experiments to show the superiority of the emotion classification models trained on the dialog data, and further to verify good emotion cues are important context for text suggestion.\n    ",
        "submission_date": "2017-07-22T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07212",
        "title": "\"i have a feeling trump will win..................\": Forecasting Winners and Losers from User Predictions on Twitter",
        "authors": [
            "Sandesh Swamy",
            "Alan Ritter",
            "Marie-Catherine de Marneffe"
        ],
        "abstract": "Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome:\"Leonardo DiCaprio will win Best Actor\" vs. \"Leonardo DiCaprio may win\" or \"No way Leonardo wins!\". Can popular beliefs on social media predict who will win? To answer this question, we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision. We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users' explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts' predictions and retrospectively identify surprise outcomes.\n    ",
        "submission_date": "2017-07-22T00:00:00",
        "last_modified_date": "2017-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07240",
        "title": "Language modeling with Neural trans-dimensional random fields",
        "authors": [
            "Bin Wang",
            "Zhijian Ou"
        ],
        "abstract": "Trans-dimensional random field language models (TRF LMs) have recently been introduced, where sentences are modeled as a collection of random fields. The TRF approach has been shown to have the advantages of being computationally more efficient in inference than LSTM LMs with close performance and being able to flexibly integrating rich features. In this paper we propose neural TRFs, beyond of the previous discrete TRFs that only use linear potentials with discrete features. The idea is to use nonlinear potentials with continuous features, implemented by neural networks (NNs), in the TRF framework. Neural TRFs combine the advantages of both NNs and TRFs. The benefits of word embedding, nonlinear feature learning and larger context modeling are inherited from the use of NNs. At the same time, the strength of efficient inference by avoiding expensive softmax is preserved. A number of technical contributions, including employing deep convolutional neural networks (CNNs) to define the potentials and incorporating the joint stochastic approximation (JSA) strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs. Various LMs are evaluated in terms of speech recognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The results show that neural TRF LMs not only improve over discrete TRF LMs, but also perform slightly better than LSTM LMs with only one fifth of parameters and 16x faster inference efficiency.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07250",
        "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
        "authors": [
            "Amir Zadeh",
            "Minghai Chen",
            "Soujanya Poria",
            "Erik Cambria",
            "Louis-Philippe Morency"
        ],
        "abstract": "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07265",
        "title": "Composing Distributed Representations of Relational Patterns",
        "authors": [
            "Sho Takase",
            "Naoaki Okazaki",
            "Kentaro Inui"
        ],
        "abstract": "Learning distributed representations for relation instances is a central technique in downstream NLP applications. In order to address semantic modeling of relational patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset. In addition, we conduct a comparative study of different encoders including additive composition, RNN, LSTM, and GRU for composing distributed representations of relational patterns. We also present Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in the relation classification task.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07273",
        "title": "Hierarchical Embeddings for Hypernymy Detection and Directionality",
        "authors": [
            "Kim Anh Nguyen",
            "Maximilian K\u00f6per",
            "Sabine Schulte im Walde",
            "Ngoc Thang Vu"
        ],
        "abstract": "We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym$-$hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state$-$of$-$the$-$art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07278",
        "title": "Fine Grained Citation Span for References in Wikipedia",
        "authors": [
            "Besnik Fetahu",
            "Katja Markert",
            "Avishek Anand"
        ],
        "abstract": "\\emph{Verifiability} is one of the core editing principles in Wikipedia, editors being encouraged to provide citations for the added content. For a Wikipedia article, determining the \\emph{citation span} of a citation, i.e. what content is covered by a citation, is important as it helps decide for which content citations are still missing.\n",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07279",
        "title": "Using Argument-based Features to Predict and Analyse Review Helpfulness",
        "authors": [
            "Haijing Liu",
            "Yang Gao",
            "Pin Lv",
            "Mengxue Li",
            "Shiqiang Geng",
            "Minglan Li",
            "Hao Wang"
        ],
        "abstract": "We study the helpful product reviews identification problem in this paper. We observe that the evidence-conclusion discourse relations, also known as arguments, often appear in product reviews, and we hypothesise that some argument-based features, e.g. the percentage of argumentative sentences, the evidences-conclusions ratios, are good indicators of helpful reviews. To validate this hypothesis, we manually annotate arguments in 110 hotel reviews, and investigate the effectiveness of several combinations of argument-based features. Experiments suggest that, when being used together with the argument-based features, the state-of-the-art baseline features can enjoy a performance boost (in terms of F1) of 11.01\\% in average.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07328",
        "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
        "authors": [
            "Robin Jia",
            "Percy Liang"
        ],
        "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to $7\\%$. We hope our insights will motivate the development of new models that understand language more precisely.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07331",
        "title": "Rule-Based Spanish Morphological Analyzer Built From Spell Checking Lexicon",
        "authors": [
            "Natalie Ahn"
        ],
        "abstract": "Preprocessing tools for automated text analysis have become more widely available in major languages, but non-English tools are often still limited in their functionality. When working with Spanish-language text, researchers can easily find tools for tokenization and stemming, but may not have the means to extract more complex word features like verb tense or mood. Yet Spanish is a morphologically rich language in which such features are often identifiable from word form. Conjugation rules are consistent, but many special verbs and nouns take on different rules. While building a complete dictionary of known words and their morphological rules would be labor intensive, resources to do so already exist, in spell checkers designed to generate valid forms of known words. This paper introduces a set of tools for Spanish-language morphological analysis, built using the COES spell checking tools, to label person, mood, tense, gender and number, derive a word's root noun or verb infinitive, and convert verbs to their nominal form.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07343",
        "title": "A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events",
        "authors": [
            "Prafulla Kumar Choubey",
            "Ruihong Huang"
        ],
        "abstract": "We present a sequential model for temporal relation classification between intra-sentence events. The key observation is that the overall syntactic structure and compositional meanings of the multi-word context between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The neural nets learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07344",
        "title": "Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events",
        "authors": [
            "Prafulla Kumar Choubey",
            "Ruihong Huang"
        ],
        "abstract": "We introduce a novel iterative approach for event coreference resolution that gradually builds event clusters by exploiting inter-dependencies among event mentions within the same chain as well as across event chains. Among event mentions in the same chain, we distinguish within- and cross-document event coreference links by using two distinct pairwise classifiers, trained separately to capture differences in feature distributions of within- and cross-document event clusters. Our event coreference approach alternates between WD and CD clustering and combines arguments from both event clusters after every merge, continuing till no more merge can be made. And then it performs further merging between event chains that are both closely related to a set of other chains of events. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods in joint task of WD and CD event coreference resolution.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07402",
        "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback",
        "authors": [
            "Khanh Nguyen",
            "Hal Daum\u00e9 III",
            "Jordan Boyd-Graber"
        ],
        "abstract": "Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07413",
        "title": "Exploring Neural Transducers for End-to-End Speech Recognition",
        "authors": [
            "Eric Battenberg",
            "Jitong Chen",
            "Rewon Child",
            "Adam Coates",
            "Yashesh Gaur",
            "Yi Li",
            "Hairong Liu",
            "Sanjeev Satheesh",
            "David Seetapun",
            "Anuroop Sriram",
            "Zhenyao Zhu"
        ],
        "abstract": "In this work, we perform an empirical comparison among the CTC, RNN-Transducer, and attention-based Seq2Seq models for end-to-end speech recognition. We show that, without any language model, Seq2Seq and RNN-Transducer models both outperform the best reported CTC models with a language model, on the popular Hub5'00 benchmark. On our internal diverse dataset, these trends continue - RNNTransducer models rescored with a language model after beam search outperform our best CTC models. These results simplify the speech recognition pipeline so that decoding can now be expressed purely as neural network operations. We also study how the choice of encoder architecture affects the performance of the three models - when all encoder layers are forward only, and when encoders downsample the input representation aggressively.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07469",
        "title": "Character-level Intra Attention Network for Natural Language Inference",
        "authors": [
            "Han Yang",
            "Marta R. Costa-juss\u00e0",
            "Jos\u00e9 A. R. Fonollosa"
        ],
        "abstract": "Natural language inference (NLI) is a central problem in language understanding. End-to-end artificial neural networks have reached state-of-the-art performance in NLI field recently.\n",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07499",
        "title": "Analysing Errors of Open Information Extraction Systems",
        "authors": [
            "Rudolf Schneider",
            "Tom Oberhauser",
            "Tobias Klatt",
            "Felix A. Gers",
            "Alexander L\u00f6ser"
        ],
        "abstract": "We report results on benchmarking Open Information Extraction (OIE) systems using RelVis, a toolkit for benchmarking Open Information Extraction systems. Our comprehensive benchmark contains three data sets from the news domain and one data set from Wikipedia with overall 4522 labeled sentences and 11243 binary or n-ary OIE relations. In our analysis on these data sets we compared the performance of four popular OIE systems, ClausIE, OpenIE 4.2, Stanford OpenIE and PredPatt. In addition, we evaluated the impact of five common error classes on a subset of 749 n-ary tuples. From our deep analysis we unreveal important research directions for a next generation of OIE systems.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07554",
        "title": "Learning Rare Word Representations using Semantic Bridging",
        "authors": [
            "Victor Prokhorov",
            "Mohammad Taher Pilehvar",
            "Dimitri Kartsaklis",
            "Pietro Li\u00f3",
            "Nigel Collier"
        ],
        "abstract": "We propose a methodology that adapts graph embedding techniques (DeepWalk (Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016)) as well as cross-lingual vector space mapping approaches (Least Squares and Canonical Correlation Analysis) in order to merge the corpus and ontological sources of lexical knowledge. We also perform comparative analysis of the used algorithms in order to identify the best combination for the proposed system. We then apply this to the task of enhancing the coverage of an existing word embedding's vocabulary with rare and unseen words. We show that our technique can provide considerable extra coverage (over 99%), leading to consistent performance gain (around 10% absolute gain is achieved with w2v-gn-500K cf.\u00a73.3) on the Rare Word Similarity dataset.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07568",
        "title": "CAp 2017 challenge: Twitter Named Entity Recognition",
        "authors": [
            "C\u00e9dric Lopez",
            "Ioannis Partalas",
            "Georgios Balikas",
            "Nadia Derbas",
            "Am\u00e9lie Martin",
            "Coralie Reutenauer",
            "Fr\u00e9d\u00e9rique Segond",
            "Massih-Reza Amini"
        ],
        "abstract": "The paper describes the CAp 2017 challenge. The challenge concerns the problem of Named Entity Recognition (NER) for tweets written in French. We first present the data preparation steps we followed for constructing the dataset released in the framework of the challenge. We begin by demonstrating why NER for tweets is a challenging problem especially when the number of entities increases. We detail the annotation process and the necessary decisions we made. We provide statistics on the inter-annotator agreement, and we conclude the data description part with examples and statistics for the data. We, then, describe the participation in the challenge, where 8 teams participated, with a focus on the methods employed by the challenge participants and the scores achieved in terms of F$_1$ measure. Importantly, the constructed dataset comprising $\\sim$6,000 tweets annotated for 13 types of entities, which to the best of our knowledge is the first such dataset in French, is publicly available at \\url{",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07591",
        "title": "Transition-Based Generation from Abstract Meaning Representations",
        "authors": [
            "Timo Schick"
        ],
        "abstract": "This work addresses the task of generating English sentences from Abstract Meaning Representation (AMR) graphs. To cope with this task, we transform each input AMR graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an algorithm that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Bleu score of 27.4 on the LDC2014T12 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07601",
        "title": "Image Pivoting for Learning Multilingual Multimodal Representations",
        "authors": [
            "Spandana Gella",
            "Rico Sennrich",
            "Frank Keller",
            "Mirella Lapata"
        ],
        "abstract": "In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07628",
        "title": "Improve Lexicon-based Word Embeddings By Word Sense Disambiguation",
        "authors": [
            "Yuanzhi Ke",
            "Masafumi Hagiwara"
        ],
        "abstract": "There have been some works that learn a lexicon together with the corpus to improve the word embeddings. However, they either model the lexicon separately but update the neural networks for both the corpus and the lexicon by the same likelihood, or minimize the distance between all of the synonym pairs in the lexicon. Such methods do not consider the relatedness and difference of the corpus and the lexicon, and may not be the best optimized. In this paper, we propose a novel method that considers the relatedness and difference of the corpus and the lexicon. It trains word embeddings by learning the corpus to predicate a word and its corresponding synonym under the context at the same time. For polysemous words, we use a word sense disambiguation filter to eliminate the synonyms that have different meanings for the context. To evaluate the proposed method, we compare the performance of the word embeddings trained by our proposed model, the control groups without the filter or the lexicon, and the prior works in the word similarity tasks and text classification task. The experimental results show that the proposed model provides better embeddings for polysemous words and improves the performance for text classification.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07631",
        "title": "Deep Architectures for Neural Machine Translation",
        "authors": [
            "Antonio Valerio Miceli Barone",
            "Jind\u0159ich Helcl",
            "Rico Sennrich",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "abstract": "It has been shown that increasing model depth improves the quality of neural machine translation. However, different architectural variants to increase model depth have been proposed, and so far, there has been no thorough comparative study.\n",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07719",
        "title": "Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification",
        "authors": [
            "Heike Adel",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We introduce globally normalized convolutional neural networks for joint entity classification and relation extraction. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2018-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07755",
        "title": "AMR Parsing using Stack-LSTMs",
        "authors": [
            "Miguel Ballesteros",
            "Yaser Al-Onaizan"
        ],
        "abstract": "We present a transition-based AMR parser that directly generates AMR parses from plain text. We use Stack-LSTMs to represent our parser state and make decisions greedily. In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07806",
        "title": "Macro Grammars and Holistic Triggering for Efficient Semantic Parsing",
        "authors": [
            "Yuchen Zhang",
            "Panupong Pasupat",
            "Percy Liang"
        ],
        "abstract": "To learn a semantic parser from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7% to 42.7%, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7%.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07911",
        "title": "Machine Translation at Booking.com: Journey and Lessons Learned",
        "authors": [
            "Pavel Levin",
            "Nishikant Dhanuka",
            "Maxim Khalilov"
        ],
        "abstract": "We describe our recently developed neural machine translation (NMT) system and benchmark it against our own statistical machine translation (SMT) system as well as two other general purpose online engines (statistical and neural). We present automatic and human evaluation results of the translation output provided by each system. We also analyze the effect of sentence length on the quality of output for SMT and NMT systems.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07922",
        "title": "Question Dependent Recurrent Entity Network for Question Answering",
        "authors": [
            "Andrea Madotto",
            "Giuseppe Attardi"
        ],
        "abstract": "Question Answering is a task which requires building models capable of providing answers to questions expressed in human language. Full question answering involves some form of reasoning ability. We introduce a neural network architecture for this task, which is a form of $Memory\\ Network$, that recognizes entities and their relations to answers through a focus attention mechanism. Our model is named $Question\\ Dependent\\ Recurrent\\ Entity\\ Network$ and extends $Recurrent\\ Entity\\ Network$ by exploiting aspects of the question during the memorization process. We validate the model on both synthetic and real datasets: the $bAbI$ question answering dataset and the $CNN\\ \\&\\ Daily\\ News$ $reading\\ comprehension$ dataset. In our experiments, the models achieved a State-of-The-Art in the former and competitive results in the latter.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08041",
        "title": "Synthesising Sign Language from semantics, approaching \"from the target and back\"",
        "authors": [
            "Michael Filhol",
            "Gilles Falquet"
        ],
        "abstract": "We present a Sign Language modelling approach allowing to build grammars and create linguistic input for Sign synthesis through avatars. We comment on the type of grammar it allows to build, and observe a resemblance between the resulting expressions and traditional semantic representations. Comparing the ways in which the paradigms are designed, we name and contrast two essentially different strategies for building higher-level linguistic input: \"source-and-forward\" vs. \"target-and-back\". We conclude by favouring the latter, acknowledging the power of being able to automatically generate output from semantically relevant input straight into articulations of the target language.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08052",
        "title": "Challenges in Data-to-Document Generation",
        "authors": [
            "Sam Wiseman",
            "Stuart M. Shieber",
            "Alexander M. Rush"
        ],
        "abstract": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08081",
        "title": "Learning Word Relatedness over Time",
        "authors": [
            "Guy D. Rosin",
            "Eytan Adar",
            "Kira Radinsky"
        ],
        "abstract": "Search systems are often focused on providing relevant results for the \"now\", assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the Web to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as query expansion, leverage the relationship between terms but cannot function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from longitudinal data collections. The model supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this task and show its application for the task of query expansion, achieving high gain.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08084",
        "title": "ShotgunWSD: An unsupervised algorithm for global word sense disambiguation inspired by DNA sequencing",
        "authors": [
            "Andrei M. Butnaru",
            "Radu Tudor Ionescu",
            "Florentina Hristea"
        ],
        "abstract": "In this paper, we present a novel unsupervised algorithm for word sense disambiguation (WSD) at the document level. Our algorithm is inspired by a widely-used approach in the field of genetics for whole genome sequencing, known as the Shotgun sequencing technique. The proposed WSD algorithm is based on three main steps. First, a brute-force WSD algorithm is applied to short context windows (up to 10 words) selected from the document in order to generate a short list of likely sense configurations for each window. In the second step, these local sense configurations are assembled into longer composite configurations based on suffix and prefix matching. The resulted configurations are ranked by their length, and the sense of each word is chosen based on a voting scheme that considers only the top k configurations in which the word appears. We compare our algorithm with other state-of-the-art unsupervised WSD algorithms and demonstrate better performance, sometimes by a very large margin. We also show that our algorithm can yield better performance than the Most Common Sense (MCS) baseline on one data set. Moreover, our algorithm has a very small number of parameters, is robust to parameter tuning, and, unlike other bio-inspired methods, it gives a deterministic solution (it does not involve random choices).\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08098",
        "title": "From Image to Text Classification: A Novel Approach based on Clustering Word Embeddings",
        "authors": [
            "Andrei M. Butnaru",
            "Radu Tudor Ionescu"
        ],
        "abstract": "In this paper, we propose a novel approach for text classification based on clustering word embeddings, inspired by the bag of visual words model, which is widely used in computer vision. After each word in a collection of documents is represented as word vector using a pre-trained word embeddings model, a k-means algorithm is applied on the word vectors in order to obtain a fixed-size set of clusters. The centroid of each cluster is interpreted as a super word embedding that embodies all the semantically related word vectors in a certain region of the embedding space. Every embedded word in the collection of documents is then assigned to the nearest cluster centroid. In the end, each document is represented as a bag of super word embeddings by computing the frequency of each super word embedding in the respective document. We also diverge from the idea of building a single vocabulary for the entire collection of documents, and propose to build class-specific vocabularies for better performance. Using this kind of representation, we report results on two text mining tasks, namely text categorization by topic and polarity classification. On both tasks, our model yields better performance than the standard bag of words.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08139",
        "title": "Analogs of Linguistic Structure in Deep Representations",
        "authors": [
            "Jacob Andreas",
            "Dan Klein"
        ],
        "abstract": "We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a \"syntax\" with functional analogues to qualitative properties of natural language.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08172",
        "title": "The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations",
        "authors": [
            "Nikita Nangia",
            "Adina Williams",
            "Angeliki Lazaridou",
            "Samuel R. Bowman"
        ],
        "abstract": "This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al. (2017). All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al.. The best single model used stacked BiLSTMs with residual connections to extract sentence features and reached 74.5% accuracy on the genre-matched test set. Surprisingly, the results of the competition were fairly consistent across the genre-matched and genre-mismatched test sets, and across subsets of the test data representing a variety of linguistic phenomena, suggesting that all of the submitted systems learned reasonably domain-independent representations for sentence meaning.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08214",
        "title": "Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation Functions in Quasi-Recurrent Neural Networks",
        "authors": [
            "Fr\u00e9deric Godin",
            "Jonas Degrave",
            "Joni Dambre",
            "Wesley De Neve"
        ],
        "abstract": "In this paper, we introduce a novel type of Rectified Linear Unit (ReLU), called a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an unbounded positive and negative image, can be used as a drop-in replacement for a tanh activation function in the recurrent step of Quasi-Recurrent Neural Networks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less prone to the vanishing gradient problem, they are noise robust, and they induce sparse activations.\n",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08290",
        "title": "Fast calculation of entropy with Zhang's estimator",
        "authors": [
            "Antoni Lozano",
            "Bernardino Casas",
            "Chris Bentz",
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Entropy is a fundamental property of a repertoire. Here, we present an efficient algorithm to estimate the entropy of types with the help of Zhang's estimator. The algorithm takes advantage of the fact that the number of different frequencies in a text is in general much smaller than the number of types. We justify the convenience of the algorithm by means of an analysis of the statistical properties of texts from more than 1000 languages. Our work opens up various possibilities for future research.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08349",
        "title": "Can string kernels pass the test of time in Native Language Identification?",
        "authors": [
            "Radu Tudor Ionescu",
            "Marius Popescu"
        ],
        "abstract": "We describe a machine learning approach for the 2017 shared task on Native Language Identification (NLI). The proposed approach combines several kernels using multiple kernel learning. While most of our kernels are based on character p-grams (also known as n-grams) extracted from essays or speech transcripts, we also use a kernel based on i-vectors, a low-dimensional representation of audio recordings, provided by the shared task organizers. For the learning stage, we choose Kernel Discriminant Analysis (KDA) over Kernel Ridge Regression (KRR), because the former classifier obtains better results than the latter one on the development set. In our previous work, we have used a similar machine learning approach to achieve state-of-the-art NLI results. The goal of this paper is to demonstrate that our shallow and simple approach based on string kernels (with minor improvements) can pass the test of time and reach state-of-the-art performance in the 2017 NLI shared task, despite the recent advances in natural language processing. We participated in all three tracks, in which the competitors were allowed to use only the essays (essay track), only the speech transcripts (speech track), or both (fusion track). Using only the data provided by the organizers for training our models, we have reached a macro F1 score of 86.95% in the closed essay track, a macro F1 score of 87.55% in the closed speech track, and a macro F1 score of 93.19% in the closed fusion track. With these scores, our team (UnibucKernel) ranked in the first group of teams in all three tracks, while attaining the best scores in the speech and the fusion tracks.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08435",
        "title": "SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set",
        "authors": [
            "William Havard",
            "Laurent Besacier",
            "Olivier Rosec"
        ],
        "abstract": "This paper presents an augmentation of MSCOCO dataset where speech is added to image and text. Speech captions are generated using text-to-speech (TTS) synthesis resulting in 616,767 spoken captions (more than 600h) paired with images. Disfluencies and speed perturbation are added to the signal in order to sound more natural. Each speech signal (WAV) is paired with a JSON file containing exact timecode for each word/syllable/phoneme in the spoken caption. Such a corpus could be used for Language and Vision (LaVi) tasks including speech input or output instead of text. Investigating multimodal learning schemes for unsupervised speech pattern discovery is also possible with this corpus, as demonstrated by a preliminary study conducted on a subset of the corpus (10h, 10k spoken captions). The dataset is available on Zenodo: ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2020-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08446",
        "title": "All that is English may be Hindi: Enhancing language identification through automatic ranking of likeliness of word borrowing in social media",
        "authors": [
            "Jasabanta Patro",
            "Bidisha Samanta",
            "Saurabh Singh",
            "Abhipsa Basu",
            "Prithwish Mukherjee",
            "Monojit Choudhury",
            "Animesh Mukherjee"
        ],
        "abstract": "In this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from social media. In terms of Spearman correlation coefficient values, our methods perform more than two times better (nearly 0.62) in predicting the borrowing likeliness compared to the best performing baseline (nearly 0.26) reported in literature. Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts. In 88 percent of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08458",
        "title": "Men Are from Mars, Women Are from Venus: Evaluation and Modelling of Verbal Associations",
        "authors": [
            "Ekaterina Vylomova",
            "Andrei Shcherbakov",
            "Yuriy Philippovich",
            "Galina Cherkasova"
        ],
        "abstract": "We present a quantitative analysis of human word association pairs and study the types of relations presented in the associations. We put our main focus on the correlation between response types and respondent characteristics such as occupation and gender by contrasting syntagmatic and paradigmatic associations. Finally, we propose a personalised distributed word association model and show the importance of incorporating demographic factors into the models commonly used in natural language processing.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08470",
        "title": "Implicit Entity Linking in Tweets",
        "authors": [
            "Sujan Perera",
            "Pablo N. Mendes",
            "Adarsh Alex",
            "Amit Sheth",
            "Krishnaprasad Thirunarayan"
        ],
        "abstract": "Over the years, Twitter has become one of the largest communication platforms providing key data to various applications such as brand monitoring, trend detection, among others. Entity linking is one of the major tasks in natural language understanding from tweets and it associates entity mentions in text to corresponding entries in knowledge bases in order to provide unambiguous interpretation and additional con- text. State-of-the-art techniques have focused on linking explicitly mentioned entities in tweets with reasonable success. However, we argue that in addition to explicit mentions i.e. The movie Gravity was more ex- pensive than the mars orbiter mission entities (movie Gravity) can also be mentioned implicitly i.e. This new space movie is crazy. you must watch it!. This paper introduces the problem of implicit entity linking in tweets. We propose an approach that models the entities by exploiting their factual and contextual knowledge. We demonstrate how to use these models to perform implicit entity linking on a ground truth dataset with 397 tweets from two domains, namely, Movie and Book. Specifically, we show: 1) the importance of linking implicit entities and its value addition to the standard entity linking task, and 2) the importance of exploiting contextual knowledge associated with an entity for linking their implicit mentions. We also make the ground truth dataset publicly available to foster the research in this new research area.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08559",
        "title": "Video Highlight Prediction Using Audience Chat Reactions",
        "authors": [
            "Cheng-Yang Fu",
            "Joon Lee",
            "Mohit Bansal",
            "Alexander C. Berg"
        ],
        "abstract": "Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08588",
        "title": "Self-organized Hierarchical Softmax",
        "authors": [
            "Yikang Shen",
            "Shawn Tan",
            "Chrisopher Pal",
            "Aaron Courville"
        ],
        "abstract": "We propose a new self-organizing hierarchical softmax formulation for neural-network-based language models over large vocabularies. Instead of using a predefined hierarchical structure, our approach is capable of learning word clusters with clear syntactical and semantic meaning during the language model training process. We provide experiments on standard benchmarks for language modeling and sentence compression tasks. We find that this approach is as fast as other efficient softmax approximations, while achieving comparable or even better performance relative to similar full softmax models.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08608",
        "title": "Gradient-based Inference for Networks with Output Constraints",
        "authors": [
            "Jay Yoon Lee",
            "Sanket Vaibhav Mehta",
            "Michael Wick",
            "Jean-Baptiste Tristan",
            "Jaime Carbonell"
        ],
        "abstract": "Practitioners apply neural networks to increasingly complex problems in natural language processing, such as syntactic parsing and semantic role labeling that have rich output structures. Many such structured-prediction problems require deterministic constraints on the output values; for example, in sequence-to-sequence syntactic parsing, we require that the sequential outputs encode valid trees. While hidden units might capture such properties, the network is not always able to learn such constraints from the training data alone, and practitioners must then resort to post-processing. In this paper, we present an inference method for neural networks that enforces deterministic constraints on outputs without performing rule-based post-processing or expensive discrete search. Instead, in the spirit of gradient-based training, we enforce constraints with gradient-based inference (GBI): for each input at test-time, we nudge continuous model weights until the network's unconstrained inference procedure generates an output that satisfies the constraints. We study the efficacy of GBI on three tasks with hard constraints: semantic role labeling, syntactic parsing, and sequence transduction. In each case, the algorithm not only satisfies constraints but improves accuracy, even when the underlying network is state-of-the-art.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2019-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08660",
        "title": "Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants",
        "authors": [
            "Andrey Kutuzov",
            "Erik Velldal",
            "Lilja \u00d8vrelid"
        ],
        "abstract": "This paper deals with using word embedding models to trace the temporal dynamics of semantic relations between pairs of words. The set-up is similar to the well-known analogies task, but expanded with a time dimension. To this end, we apply incremental updating of the models with new training texts, including incremental vocabulary expansion, coupled with learned transformation matrices that let us map between members of the relation. The proposed approach is evaluated on the task of predicting insurgent armed groups based on geographical locations. The gold standard data for the time span 1994--2010 is extracted from the UCDP Armed Conflicts dataset. The results show that the method is feasible and outperforms the baselines, but also that important work still remains to be done.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08713",
        "title": "Determining Semantic Textual Similarity using Natural Deduction Proofs",
        "authors": [
            "Hitomi Yanaka",
            "Koji Mineshima",
            "Pascual Martinez-Gomez",
            "Daisuke Bekki"
        ],
        "abstract": "Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logic-based systems and that features derived from the proofs are effective for learning textual similarity.\n    ",
        "submission_date": "2017-07-27T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08783",
        "title": "Analysis of Italian Word Embeddings",
        "authors": [
            "Rocco Tripodi",
            "Stefano Li Pira"
        ],
        "abstract": "In this work we analyze the performances of two of the most used word embeddings algorithms, skip-gram and continuous bag of words on Italian language. These algorithms have many hyper-parameter that have to be carefully tuned in order to obtain accurate word representation in vectorial space. We provide an accurate analysis and an evaluation, showing what are the best configuration of parameters for specific tasks.\n    ",
        "submission_date": "2017-07-27T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08852",
        "title": "Detecting and Explaining Causes From Text For a Time Series Event",
        "authors": [
            "Dongyeop Kang",
            "Varun Gangal",
            "Ang Lu",
            "Zheng Chen",
            "Eduard Hovy"
        ],
        "abstract": "Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between features extracted from text such as N-grams, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient reasoning. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.\n    ",
        "submission_date": "2017-07-27T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08866",
        "title": "Deep Residual Learning for Weakly-Supervised Relation Extraction",
        "authors": [
            "Yi Yao Huang",
            "William Yang Wang"
        ],
        "abstract": "Deep residual learning (ResNet) is a new method for training very deep neural networks using identity map-ping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.\n    ",
        "submission_date": "2017-07-27T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08939",
        "title": "Strawman: an Ensemble of Deep Bag-of-Ngrams for Sentiment Analysis",
        "authors": [
            "Kyunghyun Cho"
        ],
        "abstract": "This paper describes a builder entry, named \"strawman\", to the sentence-level sentiment analysis task of the \"Build It, Break It\" shared task of the First Workshop on Building Linguistically Generalizable NLP Systems. The goal of a builder is to provide an automated sentiment analyzer that would serve as a target for breakers whose goal is to find pairs of minimally-differing sentences that break the analyzer.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08976",
        "title": "Effective Inference for Generative Neural Parsing",
        "authors": [
            "Mitchell Stern",
            "Daniel Fried",
            "Dan Klein"
        ],
        "abstract": "Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.\n    ",
        "submission_date": "2017-07-27T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08998",
        "title": "ASDA : Analyseur Syntaxique du Dialecte Alg{\u00e9}rien dans un but d'analyse s{\u00e9}mantique",
        "authors": [
            "Im\u00e8ne Guellil",
            "Fai\u00e7al Azouaou"
        ],
        "abstract": "Opinion mining and sentiment analysis in social media is a research issue having a great interest in the scientific community. However, before begin this analysis, we are faced with a set of problems. In particular, the problem of the richness of languages and dialects within these media. To address this problem, we propose in this paper an approach of construction and implementation of Syntactic analyzer named ASDA. This tool represents a parser for the Algerian dialect that label the terms of a given corpus. Thus, we construct a labeling table containing for each term its stem,  different prefixes and suffixes, allowing us to determine the different grammatical parts a sort of POS tagging. This labeling will serve us later in the semantic processing of the Algerian dialect, like the automatic translation of this dialect or sentiment analysis\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09050",
        "title": "A Shared Task on Bandit Learning for Machine Translation",
        "authors": [
            "Artem Sokolov",
            "Julia Kreutzer",
            "Kellen Sunderland",
            "Pavel Danchenko",
            "Witold Szymaniak",
            "Hagen F\u00fcrstenau",
            "Stefan Riezler"
        ],
        "abstract": "We introduce and describe the results of a novel shared task on bandit learning for machine translation. The task was organized jointly by Amazon and Heidelberg University for the first time at the Second Conference on Machine Translation (WMT 2017). The goal of the task is to encourage research on learning machine translation from weak user feedback instead of human references or post-edits. On each of a sequence of rounds, a machine translation system is required to propose a translation for an input, and receives a real-valued estimate of the quality of the proposed translation for learning. This paper describes the shared task's learning and evaluation setup, using services hosted on Amazon Web Services (AWS), the data and evaluation metrics, and the results of various machine translation architectures and learning protocols.\n    ",
        "submission_date": "2017-07-27T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09067",
        "title": "Adapting Sequence Models for Sentence Correction",
        "authors": [
            "Allen Schmaltz",
            "Yoon Kim",
            "Alexander M. Rush",
            "Stuart M. Shieber"
        ],
        "abstract": "In a controlled experiment of sequence-to-sequence approaches for the task of sentence correction, we find that character-based models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.\n    ",
        "submission_date": "2017-07-27T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09168",
        "title": "Learning to Predict Charges for Criminal Cases with Legal Basis",
        "authors": [
            "Bingfeng Luo",
            "Yansong Feng",
            "Jianbo Xu",
            "Xiang Zhang",
            "Dongyan Zhao"
        ],
        "abstract": "The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.\n    ",
        "submission_date": "2017-07-28T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09231",
        "title": "Improving coreference resolution with automatically predicted prosodic information",
        "authors": [
            "Ina R\u00f6siger",
            "Sabrina Stehwien",
            "Arndt Riester",
            "Ngoc Thang Vu"
        ],
        "abstract": "Adding manually annotated prosodic information, specifically pitch accents and phrasing, to the typical text-based feature set for coreference resolution has previously been shown to have a positive effect on German data. Practical applications on spoken language, however, would rely on automatically predicted prosodic information. In this paper we predict pitch accents (and phrase boundaries) using a convolutional neural network (CNN) model from acoustic features extracted from the speech signal. After an assessment of the quality of these automatic prosodic annotations, we show that they also significantly improve coreference resolution.\n    ",
        "submission_date": "2017-07-28T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09406",
        "title": "Online Deception Detection Refueled by Real World Data Collection",
        "authors": [
            "Wenlin Yao",
            "Zeyu Dai",
            "Ruihong Huang",
            "James Caverlee"
        ],
        "abstract": "The lack of large realistic datasets presents a bottleneck in online deception detection studies. In this paper, we apply a data collection method based on social network analysis to quickly identify high-quality deceptive and truthful online reviews from Amazon. The dataset contains more than 10,000 deceptive reviews and is diverse in product domains and reviewers. Using this dataset, we explore effective general features for online deception detection that perform well across domains. We demonstrate that with generalized features - advertising speak and writing complexity scores - deception detection performance can be further improved by adding additional deceptive reviews from assorted domains in training. Finally, reviewer level evaluation gives an interesting insight into different deceptive reviewers' writing styles.\n    ",
        "submission_date": "2017-07-28T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09410",
        "title": "A Weakly Supervised Approach to Train Temporal Relation Classifiers and Acquire Regular Event Pairs Simultaneously",
        "authors": [
            "Wenlin Yao",
            "Saipravallika Nettyam",
            "Ruihong Huang"
        ],
        "abstract": "Capabilities of detecting temporal relations between two events can benefit many applications. Most of existing temporal relation classifiers were trained in a supervised manner. Instead, we explore the observation that regular event pairs show a consistent temporal relation despite of their various contexts, and these rich contexts can be used to train a contextual temporal relation classifier, which can further recognize new temporal relation contexts and identify new regular event pairs. We focus on detecting after and before temporal relations and design a weakly supervised learning approach that extracts thousands of regular event pairs and learns a contextual temporal relation classifier simultaneously. Evaluation shows that the acquired regular event pairs are of high quality and contain rich commonsense knowledge and domain specific knowledge. In addition, the weakly supervised trained temporal relation classifier achieves comparable performance with the state-of-the-art supervised systems.\n    ",
        "submission_date": "2017-07-28T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09443",
        "title": "Bilingual Document Alignment with Latent Semantic Indexing",
        "authors": [
            "Ulrich Germann"
        ],
        "abstract": "We apply cross-lingual Latent Semantic Indexing to the Bilingual Document Alignment Task at WMT16. Reduced-rank singular value decomposition of a bilingual term-document matrix derived from known English/French page pairs in the training data allows us to map monolingual documents into a joint semantic space. Two variants of cosine similarity between the vectors that place each document into the joint semantic space are combined with a measure of string similarity between corresponding URLs to produce 1:1 alignments of English/French web pages in a variety of domains. The system achieves a recall of ca. 88% if no in-domain data is used for building the latent semantic model, and 93% if such data is included.\n",
        "submission_date": "2017-07-29T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09448",
        "title": "Sentiment Analysis on Financial News Headlines using Training Dataset Augmentation",
        "authors": [
            "Vineet John",
            "Olga Vechtomova"
        ],
        "abstract": "This paper discusses the approach taken by the UWaterloo team to arrive at a solution for the Fine-Grained Sentiment Analysis problem posed by Task 5 of SemEval 2017. The paper describes the document vectorization and sentiment score prediction techniques used, as well as the design and implementation decisions taken while building the system for this task. The system uses text vectorization models, such as N-gram, TF-IDF and paragraph embeddings, coupled with regression model variants to predict the sentiment scores. Amongst the methods examined, unigrams and bigrams coupled with simple linear regression obtained the best baseline accuracy. The paper also explores data augmentation methods to supplement the training dataset. This system was designed for Subtask 2 (News Statements and Headlines).\n    ",
        "submission_date": "2017-07-29T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09468",
        "title": "Zero-Shot Activity Recognition with Verb Attribute Induction",
        "authors": [
            "Rowan Zellers",
            "Yejin Choi"
        ],
        "abstract": "In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb \"salute\" has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.\n    ",
        "submission_date": "2017-07-29T00:00:00",
        "last_modified_date": "2017-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09491",
        "title": "Topology Analysis of International Networks Based on Debates in the United Nations",
        "authors": [
            "Stefano Gurciullo",
            "Slava Mikhaylov"
        ],
        "abstract": "In complex, high dimensional and unstructured data it is often difficult to extract meaningful patterns. This is especially the case when dealing with textual data. Recent studies in machine learning, information theory and network science have developed several novel instruments to extract the semantics of unstructured data, and harness it to build a network of relations. Such approaches serve as an efficient tool for dimensionality reduction and pattern detection. This paper applies semantic network science to extract ideological proximity in the international arena, by focusing on the data from General Debates in the UN General Assembly on the topics of high salience to international community. UN General Debate corpus (UNGDC) covers all high-level debates in the UN General Assembly from 1970 to 2014, covering all UN member states. The research proceeds in three main steps. First, Latent Dirichlet Allocation (LDA) is used to extract the topics of the UN speeches, and therefore semantic information. Each country is then assigned a vector specifying the exposure to each of the topics identified. This intermediate output is then used in to construct a network of countries based on information theoretical metrics where the links capture similar vectorial patterns in the topic distributions. Topology of the networks is then analyzed through network properties like density, path length and clustering. Finally, we identify specific topological features of our networks using the map equation framework to detect communities in our networks of countries.\n    ",
        "submission_date": "2017-07-29T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09533",
        "title": "Curriculum Learning and Minibatch Bucketing in Neural Machine Translation",
        "authors": [
            "Tom Kocmi",
            "Ondrej Bojar"
        ],
        "abstract": "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \"curriculum learning\"). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \"curricula\" achieve a small improvement over the baseline.\n    ",
        "submission_date": "2017-07-29T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09569",
        "title": "Learning Language Representations for Typology Prediction",
        "authors": [
            "Chaitanya Malaviya",
            "Graham Neubig",
            "Patrick Littell"
        ],
        "abstract": "One central mystery of neural NLP is what neural models \"know\" about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages? Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one neural machine translation (NMT) system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages' geographic and phylogenetic neighbors.\n    ",
        "submission_date": "2017-07-29T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09611",
        "title": "Joint Named Entity Recognition and Stance Detection in Tweets",
        "authors": [
            "Dilek K\u00fc\u00e7\u00fck"
        ],
        "abstract": "Named entity recognition (NER) is a well-established task of information extraction which has been studied for decades. More recently, studies reporting NER experiments on social media texts have emerged. On the other hand, stance detection is a considerably new research topic usually considered within the scope of sentiment analysis. Stance detection studies are mostly applied to texts of online debates where the stance of the text owner for a particular target, either explicitly or implicitly mentioned in text, is explored. In this study, we investigate the possible contribution of named entities to the stance detection task in tweets. We report the evaluation results of NER experiments as well as that of the subsequent stance detection experiments using named entities, on a publicly-available stance-annotated data set of tweets. Our results indicate that named entities obtained with a high-performance NER system can contribute to stance detection performance on tweets.\n    ",
        "submission_date": "2017-07-30T00:00:00",
        "last_modified_date": "2017-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09751",
        "title": "Skill2vec: Machine Learning Approach for Determining the Relevant Skills from Job Description",
        "authors": [
            "Le Van-Duyet",
            "Vo Minh Quan",
            "Dang Quang An"
        ],
        "abstract": "Unsupervise learned word embeddings have seen tremendous success in numerous Natural Language Processing (NLP) tasks in recent years. The main contribution of this paper is to develop a technique called Skill2vec, which applies machine learning techniques in recruitment to enhance the search strategy to find candidates possessing the appropriate skills. Skill2vec is a neural network architecture inspired by Word2vec, developed by Mikolov et al. in 2013. It transforms skills to new vector space, which has the characteristics of calculation and presents skills relationships. We conducted an experiment evaluation manually by a recruitment company's domain experts to demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2019-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09769",
        "title": "Low-Resource Neural Headline Generation",
        "authors": [
            "Ottokar Tilk",
            "Tanel Alum\u00e4e"
        ],
        "abstract": "Recent neural headline generation models have shown great results, but are generally trained on very large datasets. We focus our efforts on improving headline quality on smaller datasets by the means of pretraining. We propose new methods that enable pre-training all the parameters of the model and utilize all available text, resulting in improvements by up to 32.4% relative in perplexity and 2.84 points in ROUGE.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09816",
        "title": "Combining Thesaurus Knowledge and Probabilistic Topic Models",
        "authors": [
            "Natalia Loukachevitch",
            "Michael Nokel",
            "Kirill Ivanov"
        ],
        "abstract": "In this paper we present the approach of introducing thesaurus knowledge into probabilistic topic models. The main idea of the approach is based on the assumption that the frequencies of semantically related words and phrases, which are met in the same texts, should be enhanced: this action leads to their larger contribution into topics found in these texts. We have conducted experiments with several thesauri and found that for improving topic models, it is useful to utilize domain-specific knowledge. If a general thesaurus, such as WordNet, is used, the thesaurus-based improvement of topic models can be achieved with excluding hyponymy relations in combined topic models.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09861",
        "title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging",
        "authors": [
            "Nils Reimers",
            "Iryna Gurevych"
        ],
        "abstract": "In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant (p < 10^-4) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F1-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09879",
        "title": "Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English",
        "authors": [
            "Duygu Ataman",
            "Matteo Negri",
            "Marco Turchi",
            "Marcello Federico"
        ],
        "abstract": "The necessity of using a fixed-size word vocabulary in order to control the model complexity in state-of-the-art neural machine translation (NMT) systems is an important bottleneck on performance, especially for morphologically rich languages. Conventional methods that aim to overcome this problem by using sub-word or character-level representations solely rely on statistics and disregard the linguistic properties of words, which leads to interruptions in the word structure and causes semantic and syntactic losses. In this paper, we propose a new vocabulary reduction method for NMT, which can reduce the vocabulary of a given input corpus at any rate while also considering the morphological properties of the language. Our method is based on unsupervised morphology learning and can be, in principle, used for pre-processing any language pair. We also present an alternative word segmentation method based on supervised morphological analysis, which aids us in measuring the accuracy of our model. We evaluate our method in Turkish-to-English NMT task where the input language is morphologically rich and agglutinative. We analyze different representation methods in terms of translation accuracy as well as the semantic and syntactic properties of the generated output. Our method obtains a significant improvement of 2.3 BLEU points over the conventional vocabulary reduction technique, showing that it can provide better accuracy in open vocabulary translation of morphologically rich languages.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09920",
        "title": "Regularization techniques for fine-tuning in neural machine translation",
        "authors": [
            "Antonio Valerio Miceli Barone",
            "Barry Haddow",
            "Ulrich Germann",
            "Rico Sennrich"
        ],
        "abstract": "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English->German and English->Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00055",
        "title": "SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation",
        "authors": [
            "Daniel Cer",
            "Mona Diab",
            "Eneko Agirre",
            "I\u00f1igo Lopez-Gazpio",
            "Lucia Specia"
        ],
        "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00098",
        "title": "The Code2Text Challenge: Text Generation in Source Code Libraries",
        "authors": [
            "Kyle Richardson",
            "Sina Zarrie\u00df",
            "Jonas Kuhn"
        ],
        "abstract": "We propose a new shared task for tactical data-to-text generation in the domain of source code libraries. Specifically, we focus on text generation of function descriptions from example software projects. Data is drawn from existing resources used for studying the related problem of semantic parser induction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a wide variety of both natural languages and programming languages. In this paper, we describe these existing resources, which will serve as training and development data for the task, and discuss plans for building new independent test sets.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00107",
        "title": "Learned in Translation: Contextualized Word Vectors",
        "authors": [
            "Bryan McCann",
            "James Bradbury",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2018-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00133",
        "title": "Grounding Language for Transfer in Deep Reinforcement Learning",
        "authors": [
            "Karthik Narasimhan",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ],
        "abstract": "In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. Specifically, by learning to ground the meaning of text to the dynamics of the environment such as transitions and rewards, an autonomous agent can effectively bootstrap policy learning on a new domain given its description. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized state representation to effectively use entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments. For instance, we achieve up to 14% and 11.5% absolute improvement over previously existing models in terms of average and initial rewards, respectively.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2018-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00154",
        "title": "Neural Rating Regression with Abstractive Tips Generation for Recommendation",
        "authors": [
            "Piji Li",
            "Zihao Wang",
            "Zhaochun Ren",
            "Lidong Bing",
            "Wai Lam"
        ],
        "abstract": "Recently, some E-commerce sites launch a new interaction box called Tips on their mobile apps. Users can express their experience and feelings or provide suggestions using short texts typically several words or one sentence. In essence, writing some tips and giving a numerical rating are two facets of a user's product assessment action, expressing the user experience and feelings. Jointly modeling these two facets is helpful for designing a better recommendation system. While some existing models integrate text information such as item specifications or user reviews into user and item latent factors for improving the rating prediction, no existing works consider tips for improving recommendation quality. We propose a deep learning based framework named NRT which can simultaneously predict precise ratings and generate abstractive tips with good linguistic quality simulating user experience and feelings. For abstractive tips generation, gated recurrent neural networks are employed to \"translate\" user and item latent representations into a concise sentence. Extensive experiments on benchmark datasets from different domains show that NRT achieves significant improvements over the state-of-the-art methods. Moreover, the generated tips can vividly predict the user experience and feelings.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00160",
        "title": "Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers",
        "authors": [
            "Nafise Sadat Moosavi",
            "Michael Strube"
        ],
        "abstract": "Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more generalizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization, our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2018-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00179",
        "title": "An Investigation into the Pedagogical Features of Documents",
        "authors": [
            "Emily Sheng",
            "Prem Natarajan",
            "Jonathan Gordon",
            "Gully Burns"
        ],
        "abstract": "Characterizing the content of a technical document in terms of its learning utility can be useful for applications related to education, such as generating reading lists from large collections of documents. We refer to this learning utility as the \"pedagogical value\" of the document to the learner. While pedagogical value is an important concept that has been studied extensively within the education domain, there has been little work exploring it from a computational, i.e., natural language processing (NLP), perspective. To allow a computational exploration of this concept, we introduce the notion of \"pedagogical roles\" of documents (e.g., Tutorial and Survey) as an intermediary component for the study of pedagogical value. Given the lack of available corpora for our exploration, we create the first annotated corpus of pedagogical roles and use it to test baseline techniques for automatic prediction of such roles.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00214",
        "title": "Natural Language Processing with Small Feed-Forward Networks",
        "authors": [
            "Jan A. Botha",
            "Emily Pitler",
            "Ji Ma",
            "Anton Bakalov",
            "Alex Salcianu",
            "David Weiss",
            "Ryan McDonald",
            "Slav Petrov"
        ],
        "abstract": "We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00241",
        "title": "Improving Part-of-Speech Tagging for NLP Pipelines",
        "authors": [
            "Vishaal Jatav",
            "Ravi Teja",
            "Srini Bharadwaj",
            "Venkat Srinivasan"
        ],
        "abstract": "This paper outlines the results of sentence level linguistics based rules for improving part-of-speech tagging. It is well known that the performance of complex NLP systems is negatively affected if one of the preliminary stages is less than perfect. Errors in the initial stages in the pipeline have a snowballing effect on the pipeline's end performance. We have created a set of linguistics based rules at the sentence level which adjust part-of-speech tags from state-of-the-art taggers. Comparison with state-of-the-art taggers on widely used benchmarks demonstrate significant improvements in tagging accuracy and consequently in the quality and accuracy of NLP systems.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00308",
        "title": "SenGen: Sentence Generating Neural Variational Topic Model",
        "authors": [
            "Ramesh Nallapati",
            "Igor Melnyk",
            "Abhishek Kumar",
            "Bowen Zhou"
        ],
        "abstract": "We present a new topic model that generates documents by sampling a topic for one whole sentence at a time, and generating the words in the sentence using an RNN decoder that is conditioned on the topic of the sentence. We argue that this novel formalism will help us not only visualize and model the topical discourse structure in a document better, but also potentially lead to more interpretable topics since we can now illustrate topics by sampling representative sentences instead of bag of words or phrases. We present a variational auto-encoder approach for learning in which we use a factorized variational encoder that independently models the posterior over topical mixture vectors of documents using a feed-forward network, and the posterior over topic assignments to sentences using an RNN. Our preliminary experiments on two different datasets indicate early promise, but also expose many challenges that remain to be addressed.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00391",
        "title": "A Continuously Growing Dataset of Sentential Paraphrases",
        "authors": [
            "Wuwei Lan",
            "Siyu Qiu",
            "Hua He",
            "Wei Xu"
        ],
        "abstract": "A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at ~70% precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00415",
        "title": "A Generative Parser with a Discriminative Recognition Algorithm",
        "authors": [
            "Jianpeng Cheng",
            "Adam Lopez",
            "Mirella Lapata"
        ],
        "abstract": "Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00416",
        "title": "Deriving Verb Predicates By Clustering Verbs with Arguments",
        "authors": [
            "Joao Sedoc",
            "Derry Wijaya",
            "Masoud Rouhizadeh",
            "Andy Schwartz",
            "Lyle Ungar"
        ],
        "abstract": "Hand-built verb clusters such as the widely used Levin classes (Levin, 1993) have proved useful, but have limited coverage. Verb classes automatically induced from corpus data such as those from VerbKB (Wijaya, 2016), on the other hand, can give clusters with much larger coverage, and can be adapted to specific corpora such as Twitter. We present a method for clustering the outputs of VerbKB: verbs with their multiple argument types, e.g. \"marry(person, person)\", \"feel(person, emotion).\" We make use of a novel low-dimensional embedding of verbs and their arguments to produce high quality clusters in which the same verb can be in different clusters depending on its argument type. The resulting verb clusters do a better job than hand-built clusters of predicting sarcasm, sentiment, and locus of control in tweets.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00481",
        "title": "A Lightweight Front-end Tool for Interactive Entity Population",
        "authors": [
            "Hidekazu Oiwa",
            "Yoshihiko Suhara",
            "Jiyu Komiya",
            "Andrei Lopatenko"
        ],
        "abstract": "Entity population, a task of collecting entities that belong to a particular category, has attracted attention from vertical domains. There is still a high demand for creating entity dictionaries in vertical domains, which are not covered by existing knowledge bases. We develop a lightweight front-end tool for facilitating interactive entity population. We implement key components necessary for effective interactive entity population: 1) GUI-based dashboards to quickly modify an entity dictionary, and 2) entity highlighting on documents for quickly viewing the current progress. We aim to reduce user cost from beginning to end, including package installation and maintenance. The implementation enables users to use this tool on their web browsers without any additional packages --- users can focus on their missions to create entity dictionaries. Moreover, an entity expansion module is implemented as external APIs. This design makes it easy to continuously improve interactive entity population pipelines. We are making our demo publicly available (",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00531",
        "title": "End-to-End Neural Segmental Models for Speech Recognition",
        "authors": [
            "Hao Tang",
            "Liang Lu",
            "Lingpeng Kong",
            "Kevin Gimpel",
            "Karen Livescu",
            "Chris Dyer",
            "Noah A. Smith",
            "Steve Renals"
        ],
        "abstract": "Segmental models are an alternative to frame-based models for sequence prediction, where hypothesized path weights are based on entire segment scores rather than a single frame at a time. Neural segmental models are segmental models that use neural network-based weight functions. Neural segmental models have achieved competitive results for speech recognition, and their end-to-end training has been explored in several studies. In this work, we review neural segmental models, which can be viewed as consisting of a neural network-based acoustic encoder and a finite-state transducer decoder. We study end-to-end segmental models with different weight functions, including ones based on frame-level neural classifiers and on segmental recurrent neural networks. We study how reducing the search space size impacts performance under different weight functions. We also compare several loss functions for end-to-end training. Finally, we explore training approaches, including multi-stage vs. end-to-end training and multitask training that combines segmental and frame-level losses.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00549",
        "title": "Improved Representation Learning for Predicting Commonsense Ontologies",
        "authors": [
            "Xiang Li",
            "Luke Vilnis",
            "Andrew McCallum"
        ],
        "abstract": "Recent work in learning ontologies (hierarchical and partially-ordered structures) has leveraged the intrinsic geometry of spaces of learned representations to make predictions that automatically obey complex structural constraints. We explore two extensions of one such model, the order-embedding model for hierarchical relation learning, with an aim towards improved performance on text data for commonsense knowledge representation. Our first model jointly learns ordering relations and non-hierarchical knowledge in the form of raw text. Our second extension exploits the partial order structure of the training data to find long-distance triplet constraints among embeddings which are poorly enforced by the pairwise training procedure. We find that both incorporating free text and augmented training constraints improve over the original order-embedding model and other strong baselines.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00553",
        "title": "Low-Rank Hidden State Embeddings for Viterbi Sequence Labeling",
        "authors": [
            "Dung Thai",
            "Shikhar Murty",
            "Trapit Bansal",
            "Luke Vilnis",
            "David Belanger",
            "Andrew McCallum"
        ],
        "abstract": "In textual information extraction and other sequence labeling tasks it is now common to use recurrent neural networks (such as LSTM) to form rich embedded representations of long-term input co-occurrence patterns. Representation of output co-occurrence patterns is typically limited to a hand-designed graphical model, such as a linear-chain CRF representing short-term Markov dependencies among successive labels. This paper presents a method that learns embedded representations of latent output structure in sequence data. Our model takes the form of a finite-state machine with a large number of latent states per label (a latent variable CRF), where the state-transition matrix is factorized---effectively forming an embedded representation of state-transitions capable of enforcing long-term label dependencies, while supporting exact Viterbi inference over output labels. We demonstrate accuracy improvements and interpretable latent structure in a synthetic but complex task based on CoNLL named entity recognition.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00563",
        "title": "Analyzing Neural MT Search and Model Performance",
        "authors": [
            "Jan Niehues",
            "Eunah Cho",
            "Thanh-Le Ha",
            "Alex Waibel"
        ],
        "abstract": "In this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex search algorithm is necessary. Furthermore, we investigate the question if more complex models which might only be applicable during rescoring are promising.\n",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00625",
        "title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization",
        "authors": [
            "Piji Li",
            "Wai Lam",
            "Lidong Bing",
            "Zihao Wang"
        ],
        "abstract": "We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN).\n",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00712",
        "title": "Dynamic Data Selection for Neural Machine Translation",
        "authors": [
            "Marlies van der Wees",
            "Arianna Bisazza",
            "Christof Monz"
        ],
        "abstract": "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00726",
        "title": "The University of Edinburgh's Neural MT Systems for WMT17",
        "authors": [
            "Rico Sennrich",
            "Alexandra Birch",
            "Anna Currey",
            "Ulrich Germann",
            "Barry Haddow",
            "Kenneth Heafield",
            "Antonio Valerio Miceli Barone",
            "Philip Williams"
        ],
        "abstract": "This paper describes the University of Edinburgh's submissions to the WMT17 shared news translation and biomedical translation tasks. We participated in 12 translation directions for news, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese. For the biomedical task we submitted systems for English to Czech, German, Polish and Romanian. Our systems are neural machine translation systems trained with Nematus, an attentional encoder-decoder. We follow our setup from last year and build BPE-based models with parallel and back-translated monolingual training data. Novelties this year include the use of deep architectures, layer normalization, and more compact models due to weight tying and improvements in BPE segmentations. We perform extensive ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00781",
        "title": "Dynamic Entity Representations in Neural Language Models",
        "authors": [
            "Yangfeng Ji",
            "Chenhao Tan",
            "Sebastian Martschat",
            "Yejin Choi",
            "Noah A. Smith"
        ],
        "abstract": "Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00790",
        "title": "Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition",
        "authors": [
            "Yong Jiang",
            "Wenjuan Han",
            "Kewei Tu"
        ],
        "abstract": "Unsupervised dependency parsing aims to learn a dependency parser from unannotated sentences. Existing work focuses on either learning generative models using the expectation-maximization algorithm and its variants, or learning discriminative models using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-of-the-art performance on thirty languages.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00801",
        "title": "Dependency Grammar Induction with Neural Lexicalization and Big Training Data",
        "authors": [
            "Wenjuan Han",
            "Yong Jiang",
            "Kewei Tu"
        ],
        "abstract": "We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence. We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00818",
        "title": "Enterprise to Computer: Star Trek chatbot",
        "authors": [
            "Grishma Jena",
            "Mansi Vashisht",
            "Abheek Basu",
            "Lyle Ungar",
            "Jo\u00e3o Sedoc"
        ],
        "abstract": "Human interactions and human-computer interactions are strongly influenced by style as well as content. Adding a persona to a chatbot makes it more human-like and contributes to a better and more engaging user experience. In this work, we propose a design for a chatbot that captures the \"style\" of Star Trek by incorporating references from the show along with peculiar tones of the fictional characters therein. Our Enterprise to Computer bot (E2Cbot) treats Star Trek dialog style and general dialog style differently, using two recurrent neural network Encoder-Decoder models. The Star Trek dialog style uses sequence to sequence (SEQ2SEQ) models (Sutskever et al., 2014; Bahdanau et al., 2014) trained on Star Trek dialogs. The general dialog style uses Word Graph to shift the response of the SEQ2SEQ model into the Star Trek domain. We evaluate the bot both in terms of perplexity and word overlap with Star Trek vocabulary and subjectively using human evaluators.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00850",
        "title": "Towards Semantic Modeling of Contradictions and Disagreements: A Case Study of Medical Guidelines",
        "authors": [
            "Wlodek Zadrozny",
            "Hossein Hematialam",
            "Luciana Garbayo"
        ],
        "abstract": "We introduce a formal distinction between contradictions and disagreements in natural language texts, motivated by the need to formally reason about contradictory medical guidelines. This is a novel and potentially very useful distinction, and has not been discussed so far in NLP and logic. We also describe a NLP system capable of automated finding contradictory medical guidelines; the system uses a combination of text analysis and information retrieval modules. We also report positive evaluation results on a small corpus of contradictory medical recommendations.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00897",
        "title": "Domain Aware Neural Dialog System",
        "authors": [
            "Sajal Choudhary",
            "Prerna Srivastava",
            "Lyle Ungar",
            "Jo\u00e3o Sedoc"
        ],
        "abstract": "We investigate the task of building a domain aware chat system which generates intelligent responses in a conversation comprising of different domains. The domain, in this case, is the topic or theme of the conversation. To achieve this, we present DOM-Seq2Seq, a domain aware neural network model based on the novel technique of using domain-targeted sequence-to-sequence models (Sutskever et al., 2014) and a domain classifier. The model captures features from current utterance and domains of the previous utterances to facilitate the formation of relevant responses. We evaluate our model on automatic metrics and compare our performance with the Seq2Seq model.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00993",
        "title": "Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning",
        "authors": [
            "Jan Niehues",
            "Eunah Cho"
        ],
        "abstract": "Linguistic resources such as part-of-speech (POS) tags have been extensively used in statistical machine translation (SMT) frameworks and have yielded better performances. However, usage of such linguistic annotations in neural machine translation (NMT) systems has been left under-explored.\n",
        "submission_date": "2017-08-03T00:00:00",
        "last_modified_date": "2017-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01009",
        "title": "Revisiting Activation Regularization for Language RNNs",
        "authors": [
            "Stephen Merity",
            "Bryan McCann",
            "Richard Socher"
        ],
        "abstract": "Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing. Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance. Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations. We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling. Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures. These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.\n    ",
        "submission_date": "2017-08-03T00:00:00",
        "last_modified_date": "2017-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01018",
        "title": "CRF Autoencoder for Unsupervised Dependency Parsing",
        "authors": [
            "Jiong Cai",
            "Yong Jiang",
            "Kewei Tu"
        ],
        "abstract": "Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.\n    ",
        "submission_date": "2017-08-03T00:00:00",
        "last_modified_date": "2017-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01065",
        "title": "Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset",
        "authors": [
            "Piji Li",
            "Lidong Bing",
            "Wai Lam"
        ],
        "abstract": "We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.\n    ",
        "submission_date": "2017-08-03T00:00:00",
        "last_modified_date": "2017-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01318",
        "title": "The UMD Neural Machine Translation Systems at WMT17 Bandit Learning Task",
        "authors": [
            "Amr Sharaf",
            "Shi Feng",
            "Khanh Nguyen",
            "Kiant\u00e9 Brantley",
            "Hal Daum\u00e9 III"
        ],
        "abstract": "We describe the University of Maryland machine translation systems submitted to the WMT17 German-English Bandit Learning Task. The task is to adapt a translation system to a new domain, using only bandit feedback: the system receives a German sentence to translate, produces an English sentence, and only gets a scalar score as feedback. Targeting these two challenges (adaptation and bandit learning), we built a standard neural machine translation system and extended it in two ways: (1) robust reinforcement learning techniques to learn effectively from the bandit feedback, and (2) domain adaptation using data selection from a large corpus of parallel data.\n    ",
        "submission_date": "2017-08-03T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01353",
        "title": "Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference",
        "authors": [
            "Qian Chen",
            "Xiaodan Zhu",
            "Zhen-Hua Ling",
            "Si Wei",
            "Hui Jiang",
            "Diana Inkpen"
        ],
        "abstract": "The RepEval 2017 Shared Task aims to evaluate natural language understanding models for sentence representation, in which a sentence is represented as a fixed-length vector with neural networks and the quality of the representation is tested with a natural language inference task. This paper describes our system (alpha) that is ranked among the top in the Shared Task, on both the in-domain test set (obtaining a 74.9% accuracy) and on the cross-domain test set (also attaining a 74.9% accuracy), demonstrating that the model generalizes well to the cross-domain data. Our model is equipped with intra-sentence gated-attention composition which helps achieve a better performance. In addition to submitting our model to the Shared Task, we have also tested it on the Stanford Natural Language Inference (SNLI) dataset. We obtain an accuracy of 85.5%, which is the best reported result on SNLI when cross-sentence attention is not allowed, the same condition enforced in RepEval 2017.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01372",
        "title": "Hashtag Healthcare: From Tweets to Mental Health Journals Using Deep Transfer Learning",
        "authors": [
            "Benjamin Shickel",
            "Martin Heesacker",
            "Sherry Benton",
            "Parisa Rashidi"
        ],
        "abstract": "As the popularity of social media platforms continues to rise, an ever-increasing amount of human communication and self- expression takes place online. Most recent research has focused on mining social media for public user opinion about external entities such as product reviews or sentiment towards political news. However, less attention has been paid to analyzing users' internalized thoughts and emotions from a mental health perspective. In this paper, we quantify the semantic difference between public Tweets and private mental health journals used in online cognitive behavioral therapy. We will use deep transfer learning techniques for analyzing the semantic gap between the two domains. We show that for the task of emotional valence prediction, social media can be successfully harnessed to create more accurate, robust, and personalized mental health models. Our results suggest that the semantic gap between public and private self-expression is small, and that utilizing the abundance of available social media is one way to overcome the small sample sizes of mental health data, which are commonly limited by availability and privacy concerns.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01425",
        "title": "The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants",
        "authors": [
            "Ivan Habernal",
            "Henning Wachsmuth",
            "Iryna Gurevych",
            "Benno Stein"
        ],
        "abstract": "Reasoning is a crucial part of natural language argumentation. To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. As arguments are highly contextualized, warrants are usually presupposed and left implicit. Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense. In this paper we develop a methodology for reconstructing warrants systematically. We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. A solution to this task will define a substantial step towards automatic warrant reconstruction. However, experiments with several neural attention and language models reveal that current approaches do not suffice.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2018-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01464",
        "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion",
        "authors": [
            "Ben Peters",
            "Jon Dehdari",
            "Josef van Genabith"
        ],
        "abstract": "Grapheme-to-phoneme conversion (g2p) is necessary for text-to-speech and automatic speech recognition systems. Most g2p systems are monolingual: they require language-specific data or handcrafting of rules. Such systems are difficult to extend to low resource languages, for which data and handcrafted rules are not available. As an alternative, we present a neural sequence-to-sequence approach to g2p which is trained on spelling--pronunciation pairs in hundreds of languages. The system shares a single encoder and decoder across all languages, allowing it to utilize the intrinsic similarities between different writing systems. We show an 11% improvement in phoneme error rate over an approach based on adapting high-resource monolingual g2p models to low-resource languages. Our model is also much more compact relative to previous approaches.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01525",
        "title": "Language Design as Information Renormalization",
        "authors": [
            "Angel J. Gallego",
            "Roman Orus"
        ],
        "abstract": "Here we consider some well-known facts in syntax from a physics perspective, allowing us to establish equivalences between both fields with many consequences. Mainly, we observe that the operation MERGE, put forward by N. Chomsky in 1995, can be interpreted as a physical information coarse-graining. Thus, MERGE in linguistics entails information renormalization in physics, according to different time scales. We make this point mathematically formal in terms of language models. In this setting, MERGE amounts to a probability tensor implementing a coarse-graining, akin to a probabilistic context-free grammar. The probability vectors of meaningful sentences are given by stochastic tensor networks (TN) built from diagonal tensors and which are mostly loop-free, such as Tree Tensor Networks and Matrix Product States, thus being computationally very efficient to manipulate. We show that this implies the polynomially-decaying (long-range) correlations experimentally observed in language, and also provides arguments in favour of certain types of neural networks for language processing. Moreover, we show how to obtain such language models from quantum states that can be efficiently prepared on a quantum computer, and use this to find bounds on the perplexity of the probability distribution of words in a sentence. Implications of our results are discussed across several ambits.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2022-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01681",
        "title": "Predicting the Law Area and Decisions of French Supreme Court Cases",
        "authors": [
            "Octavia-Maria Sulea",
            "Marcos Zampieri",
            "Mihaela Vela",
            "Josef van Genabith"
        ],
        "abstract": "In this paper, we investigate the application of text classification methods to predict the law area and the decision of cases judged by the French Supreme Court. We also investigate the influence of the time period in which a ruling was made over the textual form of the case description and the extent to which it is necessary to mask the judge's motivation for a ruling to emulate a real-world test scenario. We report results of 96% f1 score in predicting a case ruling, 90% f1 score in predicting the law area of a case, and 75.9% f1 score in estimating the time span when a ruling has been issued using a linear Support Vector Machine (SVM) classifier trained on lexical features.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01713",
        "title": "Automatic Question-Answering Using A Deep Similarity Neural Network",
        "authors": [
            "Shervin Minaee",
            "Zhu Liu"
        ],
        "abstract": "Automatic question-answering is a classical problem in natural language processing, which aims at designing systems that can automatically answer a question, in the same way as human does. In this work, we propose a deep learning based model for automatic question-answering. First the questions and answers are embedded using neural probabilistic modeling. Then a deep similarity neural network is trained to find the similarity score of a pair of answer and question. Then for each question, the best answer is found as the one with the highest similarity score. We first train this model on a large-scale public question-answering database, and then fine-tune it to transfer to the customer-care chat data. We have also tested our framework on a public question-answering database and achieved very good performance.\n    ",
        "submission_date": "2017-08-05T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01759",
        "title": "Referenceless Quality Estimation for Natural Language Generation",
        "authors": [
            "Ond\u0159ej Du\u0161ek",
            "Jekaterina Novikova",
            "Verena Rieser"
        ],
        "abstract": "Traditional automatic evaluation measures for natural language generation (NLG) use costly human-authored references to estimate the quality of a system output. In this paper, we propose a referenceless quality estimation (QE) approach based on recurrent neural networks, which predicts a quality score for a NLG system output by comparing it to the source meaning representation only. Our method outperforms traditional metrics and a constant baseline in most respects; we also show that synthetic data helps to increase correlation results by 21% compared to the base system. Our results are comparable to results obtained in similar QE tasks despite the more challenging setting.\n    ",
        "submission_date": "2017-08-05T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01766",
        "title": "A Syllable-based Technique for Word Embeddings of Korean Words",
        "authors": [
            "Sanghyuk Choi",
            "Taeuk Kim",
            "Jinseok Seol",
            "Sang-goo Lee"
        ],
        "abstract": "Word embedding has become a fundamental component to many NLP tasks such as named entity recognition and machine translation. However, popular models that learn such embeddings are unaware of the morphology of words, so it is not directly applicable to highly agglutinative languages such as Korean. We propose a syllable-based learning model for Korean using a convolutional neural network, in which word representation is composed of trained syllable vectors. Our model successfully produces morphologically meaningful representation of Korean words compared to the original Skip-gram embeddings. The results also show that it is quite robust to the Out-of-Vocabulary problem.\n    ",
        "submission_date": "2017-08-05T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01769",
        "title": "Extractive Multi Document Summarization using Dynamical Measurements of Complex Networks",
        "authors": [
            "Jorge V. Tohalino",
            "Diego R. Amancio"
        ],
        "abstract": "Due to the large amount of textual information available on Internet, it is of paramount relevance to use techniques that find relevant and concise content. A typical task devoted to the identification of informative sentences in documents is the so called extractive document summarization task. In this paper, we use complex network concepts to devise an extractive Multi Document Summarization (MDS) method, which extracts the most central sentences from several textual sources. In the proposed model, texts are represented as networks, where nodes represent sentences and the edges are established based on the number of shared words. Differently from previous works, the identification of relevant terms is guided by the characterization of nodes via dynamical measurements of complex networks, including symmetry, accessibility and absorption time. The evaluation of the proposed system revealed that excellent results were obtained with particular dynamical measurements, including those based on the exploration of networks via random walks.\n    ",
        "submission_date": "2017-08-05T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01771",
        "title": "Neural Machine Translation with Word Predictions",
        "authors": [
            "Rongxiang Weng",
            "Shujian Huang",
            "Zaixiang Zheng",
            "Xinyu Dai",
            "Jiajun Chen"
        ],
        "abstract": "In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the ",
        "submission_date": "2017-08-05T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01809",
        "title": "A Comparison of Neural Models for Word Ordering",
        "authors": [
            "Eva Hasler",
            "Felix Stahlberg",
            "Marcus Tomalin",
            "Adri`a de Gispert",
            "Bill Byrne"
        ],
        "abstract": "We compare several language models for the word-ordering task and propose a new bag-to-sequence neural model based on attention-based sequence-to-sequence models. We evaluate the model on a large German WMT data set where it significantly outperforms existing models. We also describe a novel search strategy for LM-based word ordering and report results on the English Penn Treebank. Our best model setup outperforms prior work both in terms of speed and quality.\n    ",
        "submission_date": "2017-08-05T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01980",
        "title": "Translating Phrases in Neural Machine Translation",
        "authors": [
            "Xing Wang",
            "Zhaopeng Tu",
            "Deyi Xiong",
            "Min Zhang"
        ],
        "abstract": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02005",
        "title": "Memory-augmented Neural Machine Translation",
        "authors": [
            "Yang Feng",
            "Shiyue Zhang",
            "Andi Zhang",
            "Dong Wang",
            "Andrew Abel"
        ],
        "abstract": "Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by $9.0$ and $2.7$ BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02043",
        "title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?",
        "authors": [
            "Marc Tanti",
            "Albert Gatt",
            "Kenneth P. Camilleri"
        ],
        "abstract": "In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary `generation' component. This view suggests that the image features should be `injected' into the RNN. This is in fact the dominant view in the literature. Alternatively, the RNN can instead be viewed as only encoding the previously generated words. This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be `merged' with the image features at a later stage. This paper compares these two architectures. We find that, in general, late merging outperforms injection, suggesting that RNNs are better viewed as encoders, rather than generators.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02099",
        "title": "Multimodal Classification for Analysing Social Media",
        "authors": [
            "Chi Thang Duong",
            "Remi Lebret",
            "Karl Aberer"
        ],
        "abstract": "Classification of social media data is an important approach in understanding user behavior on the Web. Although information on social media can be of different modalities such as texts, images, audio or videos, traditional approaches in classification usually leverage only one prominent modality. Techniques that are able to leverage multiple modalities are often complex and susceptible to the absence of some modalities. In this paper, we present simple models that combine information from different modalities to classify social media content and are able to handle the above problems with existing techniques. Our models combine information from different modalities using a pooling layer and an auxiliary learning task is used to learn a common feature space. We demonstrate the performance of our models and their robustness to the missing of some modalities in the emotion classification domain. Our approaches, although being simple, can not only achieve significantly higher accuracies than traditional fusion approaches but also have comparable results when only one modality is available.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02182",
        "title": "Regularizing and Optimizing LSTM Language Models",
        "authors": [
            "Stephen Merity",
            "Nitish Shirish Keskar",
            "Richard Socher"
        ],
        "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02210",
        "title": "Video Highlights Detection and Summarization with Lag-Calibration based on Concept-Emotion Mapping of Crowd-sourced Time-Sync Comments",
        "authors": [
            "Qing Ping",
            "Chaomei Chen"
        ],
        "abstract": "With the prevalence of video sharing, there are increasing demands for automatic video digestion such as highlight detection. Recently, platforms with crowdsourced time-sync video comments have emerged worldwide, providing a good opportunity for highlight detection. However, this task is non-trivial: (1) time-sync comments often lag behind their corresponding shot; (2) time-sync comments are semantically sparse and noisy; (3) to determine which shots are highlights is highly subjective. The present paper aims to tackle these challenges by proposing a framework that (1) uses concept-mapped lexical-chains for lag calibration; (2) models video highlights based on comment intensity and combination of emotion and concept concentration of each shot; (3) summarize each detected highlight using improved SumBasic with emotion and concept mapping. Experiments on large real-world datasets show that our highlight detection method and summarization method both outperform other benchmarks with considerable margins.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02254",
        "title": "Asking Too Much? The Rhetorical Role of Questions in Political Discourse",
        "authors": [
            "Justine Zhang",
            "Arthur Spirling",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Questions play a prominent role in social interactions, performing rhetorical functions that go beyond that of simple informational exchange. The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor. While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied.\n",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02267",
        "title": "ISS-MULT: Intelligent Sample Selection for Multi-Task Learning in Question Answering",
        "authors": [
            "Ali Ahmadvand",
            "Jinho D. Choi"
        ],
        "abstract": "Transferring knowledge from a source domain to another domain is useful, especially when gathering new data is very expensive and time-consuming. Deep networks have been well-studied for question answering tasks in recent years; however, no prominent research for transfer learning through deep neural networks exists in the question answering field. In this paper, two main methods (INIT and MULT) in this field are examined. Then, a new method named Intelligent sample selection (ISS-MULT) is proposed to improve the MULT method for question answering tasks. Different datasets, specificay SQuAD, SelQA, WikiQA, NewWikiQA and InforBoxQA, are used for evaluation. Moreover, two different tasks of question answering - answer selection and answer triggering - are evaluated to examine the effectiveness of transfer learning. The results show that using transfer learning generally improves the performance if the corpora are related and are based on the same policy. In addition, using ISS-MULT could finely improve the MULT method for question answering tasks, and these improvements prove more significant in the answer triggering task.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2019-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02275",
        "title": "Corpus-level Fine-grained Entity Typing",
        "authors": [
            "Yadollah Yaghoobzadeh",
            "Heike Adel",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class such as \"food\" or \"artist\". The application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding- based and combines (i) a global model that scores based on aggregated contextual information of an entity and (ii) a context model that first scores the individual occurrences of an entity and then aggregates the scores. Each of the two proposed models has some specific properties. For the global model, learning high quality entity representations is crucial because it is the only source used for the predictions. Therefore, we introduce representations using name and contexts of entities on the three levels of entity, word, and character. We show each has complementary information and a multi-level representation is the best. For the context model, we need to use distant supervision since the context-level labels are not available for entities. Distant supervised labels are noisy and this harms the performance of models. Therefore, we introduce and apply new algorithms for noise mitigation using multi-instance learning. We show the effectiveness of our models in a large entity typing dataset, built from Freebase.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2018-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02300",
        "title": "Reinforced Video Captioning with Entailment Rewards",
        "authors": [
            "Ramakanth Pasunuru",
            "Mohit Bansal"
        ],
        "abstract": "Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02312",
        "title": "Shortcut-Stacked Sentence Encoders for Multi-Domain Inference",
        "authors": [
            "Yixin Nie",
            "Mohit Bansal"
        ],
        "abstract": "We present a simple sequential sentence encoder for multi-domain natural language inference. Our encoder is based on stacked bidirectional LSTM-RNNs with shortcut connections and fine-tuning of word embeddings. The overall supervised model uses the above encoder to encode two input sentences into two vectors, and then uses a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural. Our Shortcut-Stacked sentence encoders achieve strong improvements over existing encoders on matched and mismatched multi-domain natural language inference (top non-ensemble single-model result in the EMNLP RepEval 2017 Shared Task (Nangia et al., 2017)). Moreover, they achieve the new state-of-the-art encoding result on the original SNLI dataset (Bowman et al., 2015).\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02383",
        "title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach",
        "authors": [
            "Meng Fang",
            "Yuan Li",
            "Trevor Cohn"
        ],
        "abstract": "Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.\n    ",
        "submission_date": "2017-08-08T00:00:00",
        "last_modified_date": "2017-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02420",
        "title": "Mining fine-grained opinions on closed captions of YouTube videos with an attention-RNN",
        "authors": [
            "Edison Marrese-Taylor",
            "Jorge A. Balazs",
            "Yutaka Matsuo"
        ],
        "abstract": "Video reviews are the natural evolution of written product reviews. In this paper we target this phenomenon and introduce the first dataset created from closed captions of YouTube product review videos as well as a new attention-RNN model for aspect extraction and joint aspect extraction and sentiment classification. Our model provides state-of-the-art performance on aspect extraction without requiring the usage of hand-crafted features on the SemEval ABSA corpus, while it outperforms the baseline on the joint task. In our dataset, the attention-RNN model outperforms the baseline for both tasks, but we observe important performance drops for all models in comparison to SemEval. These results, as well as further experiments on domain adaptation for aspect extraction, suggest that differences between speech and written text, which have been discussed extensively in the literature, also extend to the domain of product reviews, where they are relevant for fine-grained opinion mining.\n    ",
        "submission_date": "2017-08-08T00:00:00",
        "last_modified_date": "2017-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02561",
        "title": "Neural-based Context Representation Learning for Dialog Act Classification",
        "authors": [
            "Daniel Ortega",
            "Ngoc Thang Vu"
        ],
        "abstract": "We explore context representation learning methods in neural-based models for dialog act classification. We propose and compare extensively different methods which combine recurrent neural network architectures and attention mechanisms (AMs) at different context levels. Our experimental results on two benchmark datasets show consistent improvements compared to the models without contextual information and reveal that the most suitable AM in the architecture depends on the nature of the dataset.\n    ",
        "submission_date": "2017-08-08T00:00:00",
        "last_modified_date": "2017-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02657",
        "title": "Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?",
        "authors": [
            "Xiang Zhang",
            "Yann LeCun"
        ],
        "abstract": "This article offers an empirical study on the different ways of encoding Chinese, Japanese, Korean (CJK) and English languages for text classification. Different encoding levels are studied, including UTF-8 bytes, characters, words, romanized characters and romanized words. For all encoding levels, whenever applicable, we provide comparisons with linear models, fastText and convolutional networks. For convolutional networks, we compare between encoding mechanisms using character glyph images, one-hot (or one-of-n) encoding, and embedding. In total there are 473 models, using 14 large-scale text classification datasets in 4 languages including Chinese, English, Japanese and Korean. Some conclusions from these results include that byte-level one-hot encoding based on UTF-8 consistently produces competitive results for convolutional networks, that word-level n-grams linear models are competitive even without perfect word segmentation, and that fastText provides the best result using character-level n-gram encoding but can overfit when the features are overly rich.\n    ",
        "submission_date": "2017-08-08T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02709",
        "title": "Recent Trends in Deep Learning Based Natural Language Processing",
        "authors": [
            "Tom Young",
            "Devamanyu Hazarika",
            "Soujanya Poria",
            "Erik Cambria"
        ],
        "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.\n    ",
        "submission_date": "2017-08-09T00:00:00",
        "last_modified_date": "2018-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02912",
        "title": "KeyXtract Twitter Model - An Essential Keywords Extraction Model for Twitter Designed using NLP Tools",
        "authors": [
            "Tharindu Weerasooriya",
            "Nandula Perera",
            "S.R. Liyanage"
        ],
        "abstract": "Since a tweet is limited to 140 characters, it is ambiguous and difficult for traditional Natural Language Processing (NLP) tools to analyse. This research presents KeyXtract which enhances the machine learning based Stanford CoreNLP Part-of-Speech (POS) tagger with the Twitter model to extract essential keywords from a tweet. The system was developed using rule-based parsers and two corpora. The data for the research was obtained from a Twitter profile of a telecommunication company. The system development consisted of two stages. At the initial stage, a domain specific corpus was compiled after analysing the tweets. The POS tagger extracted the Noun Phrases and Verb Phrases while the parsers removed noise and extracted any other keywords missed by the POS tagger. The system was evaluated using the Turing Test. After it was tested and compared against Stanford CoreNLP, the second stage of the system was developed addressing the shortcomings of the first stage. It was enhanced using Named Entity Recognition and Lemmatization. The second stage was also tested using the Turing test and its pass rate increased from 50.00% to 83.33%. The performance of the final system output was measured using the F1 score. Stanford CoreNLP with the Twitter model had an average F1 of 0.69 while the improved system had a F1 of 0.77. The accuracy of the system could be improved by using a complete domain specific corpus. Since the system used linguistic features of a sentence, it could be applied to other NLP tools.\n    ",
        "submission_date": "2017-08-09T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02977",
        "title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling",
        "authors": [
            "Licheng Yu",
            "Mohit Bansal",
            "Tamara L. Berg"
        ],
        "abstract": "We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.\n    ",
        "submission_date": "2017-08-09T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02989",
        "title": "Identifying Reference Spans: Topic Modeling and Word Embeddings help IR",
        "authors": [
            "Luis Moraes",
            "Shahryar Baki",
            "Rakesh Verma",
            "Daniel Lee"
        ],
        "abstract": "The CL-SciSumm 2016 shared task introduced an interesting problem: given a document D and a piece of text that cites D, how do we identify the text spans of D being referenced by the piece of text? The shared task provided the first annotated dataset for studying this problem. We present an analysis of our continued work in improving our system's performance on this task. We demonstrate how topic models and word embeddings can be used to surpass the previously best performing system.\n    ",
        "submission_date": "2017-08-09T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03105",
        "title": "Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models",
        "authors": [
            "Hussein S. Al-Olimat",
            "Krishnaprasad Thirunarayan",
            "Valerie Shalin",
            "Amit Sheth"
        ],
        "abstract": "Extracting location names from informal and unstructured social media data requires the identification of referent boundaries and partitioning compound names. Variability, particularly systematic variability in location names (Carroll, 1983), challenges the identification task. Some of this variability can be anticipated as operations within a statistical language model, in this case drawn from gazetteers such as OpenStreetMap (OSM), Geonames, and DBpedia. This permits evaluation of an observed n-gram in Twitter targeted text as a legitimate location name variant from the same location-context. Using n-gram statistics and location-related dictionaries, our Location Name Extraction tool (LNEx) handles abbreviations and automatically filters and augments the location names in gazetteers (handling name contractions and auxiliary contents) to help detect the boundaries of multi-word location names and thereby delimit them in texts.\n",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2020-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03152",
        "title": "Towards Neural Speaker Modeling in Multi-Party Conversation: The Task, Dataset, and Models",
        "authors": [
            "Zhao Meng",
            "Lili Mou",
            "Zhi Jin"
        ],
        "abstract": "Neural network-based dialog systems are attracting increasing attention in both academia and industry. Recently, researchers have begun to realize the importance of speaker modeling in neural dialog systems, but there lacks established tasks and datasets. In this paper, we propose speaker classification as a surrogate task for general speaker modeling, and collect massive data to facilitate research in this direction. We further investigate temporal-based and content-based models of speakers, and propose several hybrids of them. Experiments show that speaker classification is feasible, and that hybrid models outperform each single component.\n    ",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2018-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03186",
        "title": "Neural and Statistical Methods for Leveraging Meta-information in Machine Translation",
        "authors": [
            "Shahram Khadivi",
            "Patrick Wilken",
            "Leonard Dahlmann",
            "Evgeny Matusov"
        ],
        "abstract": "In this paper, we discuss different methods which use meta information and richer context that may accompany source language input to improve machine translation quality. We focus on category information of input text as meta information, but the proposed methods can be extended to all textual and non-textual meta information that might be available for the input text or automatically predicted using the text content. The main novelty of this work is to use state-of-the-art neural network methods to tackle this problem within a statistical machine translation (SMT) framework. We observe translation quality improvements up to 3% in terms of BLEU score in some text categories.\n    ",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03246",
        "title": "SESA: Supervised Explicit Semantic Analysis",
        "authors": [
            "Dasha Bogdanova",
            "Majid Yazdani"
        ],
        "abstract": "In recent years supervised representation learning has provided state of the art or close to the state of the art results in semantic analysis tasks including ranking and information retrieval. The core idea is to learn how to embed items into a latent space such that they optimize a supervised objective in that latent space. The dimensions of the latent space have no clear semantics, and this reduces the interpretability of the system. For example, in personalization models, it is hard to explain why a particular item is ranked high for a given user profile. We propose a novel model of representation learning called Supervised Explicit Semantic Analysis (SESA) that is trained in a supervised fashion to embed items to a set of dimensions with explicit semantics. The model learns to compare two objects by representing them in this explicit space, where each dimension corresponds to a concept from a knowledge base. This work extends Explicit Semantic Analysis (ESA) with a supervised model for ranking problems. We apply this model to the task of Job-Profile relevance in LinkedIn in which a set of skills defines our explicit dimensions of the space. Every profile and job are encoded to this set of skills their similarity is calculated in this space. We use RNNs to embed text input into this space. In addition to interpretability, our model makes use of the web-scale collaborative skills data that is provided by users for each LinkedIn profile. Our model provides state of the art result while it remains interpretable.\n    ",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03271",
        "title": "Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search",
        "authors": [
            "Leonard Dahlmann",
            "Evgeny Matusov",
            "Pavel Petrushkov",
            "Shahram Khadivi"
        ],
        "abstract": "In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German->English news domain and English->Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong NMT baseline.\n    ",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03312",
        "title": "Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of Chinese and Japanese",
        "authors": [
            "Yuanzhi Ke",
            "Masafumi Hagiwara"
        ],
        "abstract": "The character vocabulary can be very large in non-alphabetic languages such as Chinese and Japanese, which makes neural network models huge to process such languages. We explored a model for sentiment classification that takes the embeddings of the radicals of the Chinese characters, i.e, hanzi of Chinese and kanji of Japanese. Our model is composed of a CNN word feature encoder and a bi-directional RNN document feature encoder. The results achieved are on par with the character embedding-based models, and close to the state-of-the-art word embedding-based models, with 90% smaller vocabulary, and at least 13% and 80% fewer parameters than the character embedding-based models and word embedding-based models respectively. The results suggest that the radical embedding-based approach is cost-effective for machine learning on Chinese and Japanese.\n    ",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03390",
        "title": "Making Sense of Word Embeddings",
        "authors": [
            "Maria Pelevina",
            "Nikolay Arefyev",
            "Chris Biemann",
            "Alexander Panchenko"
        ],
        "abstract": "We present a simple yet effective approach for learning word sense embeddings. In contrast to existing techniques, which either directly learn sense representations from corpora or rely on sense inventories from lexical resources, our approach can induce a sense inventory from existing word embeddings via clustering of ego-networks of related words. An integrated WSD mechanism enables labeling of words in context with learned sense vectors, which gives rise to downstream applications. Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems.\n    ",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03421",
        "title": "N-gram and Neural Language Models for Discriminating Similar Languages",
        "authors": [
            "Andre Cianflone",
            "Leila Kosseim"
        ],
        "abstract": "This paper describes our submission (named clac) to the 2016 Discriminating Similar Languages (DSL) shared task. We participated in the closed Sub-task 1 (Set A) with two separate machine learning techniques. The first approach is a character based Convolution Neural Network with a bidirectional long short term memory (BiLSTM) layer (CLSTM), which achieved an accuracy of 78.45% with minimal tuning. The second approach is a character-based n-gram model. This last approach achieved an accuracy of 88.45% which is close to the accuracy of 89.38% achieved by the best submission, and allowed us to rank #7 overall.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03425",
        "title": "Argument Labeling of Explicit Discourse Relations using LSTM Neural Networks",
        "authors": [
            "Sohail Hooda",
            "Leila Kosseim"
        ],
        "abstract": "Argument labeling of explicit discourse relations is a challenging task. The state of the art systems achieve slightly above 55% F-measure but require hand-crafted features. In this paper, we propose a Long Short Term Memory (LSTM) based model for argument labeling. We experimented with multiple configurations of our model. Using the PDTB dataset, our best model achieved an F1 measure of 23.05% without any feature engineering. This is significantly higher than the 20.52% achieved by the state of the art RNN approach, but significantly lower than the feature based state of the art systems. On the other hand, because our approach learns only from the raw dataset, it is more widely applicable to multiple textual genres and languages.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03446",
        "title": "What matters in a transferable neural network model for relation classification in the biomedical domain?",
        "authors": [
            "Sunil Kumar Sahu",
            "Ashish Anand"
        ],
        "abstract": "Lack of sufficient labeled data often limits the applicability of advanced machine learning algorithms to real life problems. However efficient use of Transfer Learning (TL) has been shown to be very useful across domains. TL utilizes valuable knowledge learned in one task (source task), where sufficient data is available, to the task of interest (target task). In biomedical and clinical domain, it is quite common that lack of sufficient training data do not allow to fully exploit machine learning models. In this work, we present two unified recurrent neural models leading to three transfer learning frameworks for relation classification tasks. We systematically investigate effectiveness of the proposed frameworks in transferring the knowledge under multiple aspects related to source and target tasks, such as, similarity or relatedness between source and target tasks, and size of training data for source task. Our empirical results show that the proposed frameworks in general improve the model performance, however these improvements do depend on aspects related to source and target tasks. This dependence then finally determine the choice of a particular TL framework.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03447",
        "title": "Unified Neural Architecture for Drug, Disease and Clinical Entity Recognition",
        "authors": [
            "Sunil Kumar Sahu",
            "Ashish Anand"
        ],
        "abstract": "Most existing methods for biomedical entity recognition task rely on explicit feature engineering where many features either are specific to a particular task or depends on output of other existing NLP tools. Neural architectures have been shown across various domains that efforts for explicit feature design can be reduced. In this work we propose an unified framework using bi-directional long short term memory network (BLSTM) for named entity recognition (NER) tasks in biomedical and clinical domains. Three important characteristics of the framework are as follows - (1) model learns contextual as well as morphological features using two different BLSTM in hierarchy, (2) model uses first order linear conditional random field (CRF) in its output layer in cascade of BLSTM to infer label or tag sequence, (3) model does not use any domain specific features or dictionary, i.e., in another words, same set of features are used in the three NER tasks, namely, disease name recognition (Disease NER), drug name recognition (Drug NER) and clinical entity recognition (Clinical NER). We compare performance of the proposed model with existing state-of-the-art models on the standard benchmark datasets of the three tasks. We show empirically that the proposed framework outperforms all existing models. Further our analysis of CRF layer and word-embedding obtained using character based embedding show their importance.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03492",
        "title": "Break it Down for Me: A Study in Automated Lyric Annotation",
        "authors": [
            "Lucas Sterckx",
            "Jason Naradowsky",
            "Bill Byrne",
            "Thomas Demeester",
            "Chris Develder"
        ],
        "abstract": "Comprehending lyrics, as found in songs and poems, can pose a challenge to human and machine readers alike. This motivates the need for systems that can understand the ambiguity and jargon found in such creative texts, and provide commentary to aid readers in reaching the correct interpretation. We introduce the task of automated lyric annotation (ALA). Like text simplification, a goal of ALA is to rephrase the original text in a more easily understandable manner. However, in ALA the system must often include additional information to clarify niche terminology and abstract concepts. To stimulate research on this task, we release a large collection of crowdsourced annotations for song lyrics. We analyze the performance of translation and retrieval models on this task, measuring performance with both automated and human evaluation. We find that each model captures a unique type of information important to the task.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03541",
        "title": "Automatic Identification of AltLexes using Monolingual Parallel Corpora",
        "authors": [
            "Elnaz Davoodi",
            "Leila Kosseim"
        ],
        "abstract": "The automatic identification of discourse relations is still a challenging task in natural language processing. Discourse connectives, such as \"since\" or \"but\", are the most informative cues to identify explicit relations; however discourse parsers typically use a closed inventory of such connectives. As a result, discourse relations signaled by markers outside these inventories (i.e. AltLexes) are not detected as effectively. In this paper, we propose a novel method to leverage parallel corpora in text simplification and lexical resources to automatically identify alternative lexicalizations that signal discourse relation. When applied to the Simple Wikipedia and Newsela corpora along with WordNet and the PPDB, the method allowed the automatic discovery of 91 AltLexes.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03629",
        "title": "Simple and Effective Dimensionality Reduction for Word Embeddings",
        "authors": [
            "Vikas Raunak"
        ],
        "abstract": "Word embeddings have become the basic building blocks for several natural language processing and information retrieval tasks. Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on further improving the pre-trained word vectors through post-processing algorithms. One such area of improvement is the dimensionality reduction of the word embeddings. Reducing the size of word embeddings through dimensionality reduction can improve their utility in memory constrained devices, benefiting several real-world applications. In this work, we present a novel algorithm that effectively combines PCA based dimensionality reduction with a recently proposed post-processing algorithm, to construct word embeddings of lower dimensions. Empirical evaluations on 12 standard word similarity benchmarks show that our algorithm reduces the embedding dimensionality by 50%, while achieving similar or (more often) better performance than the higher dimension embeddings.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03696",
        "title": "Emotion Intensities in Tweets",
        "authors": [
            "Saif M. Mohammad",
            "Felipe Bravo-Marquez"
        ],
        "abstract": "This paper examines the task of detecting intensity of emotion from text. We create the first datasets of tweets annotated for anger, fear, joy, and sadness intensities. We use a technique called best--worst scaling (BWS) that improves annotation consistency and obtains reliable fine-grained scores. We show that emotion-word hashtags often impact emotion intensity, usually conveying a more intense emotion. Finally, we create a benchmark regression system and conduct experiments to determine: which features are useful for detecting emotion intensity, and, the extent to which two emotions are similar in terms of how they manifest in language.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03699",
        "title": "Improved Abusive Comment Moderation with User Embeddings",
        "authors": [
            "John Pavlopoulos",
            "Prodromos Malakasiotis",
            "Juli Bakagianni",
            "Ion Androutsopoulos"
        ],
        "abstract": "Experimenting with a dataset of approximately 1.6M user comments from a Greek news sports portal, we explore how a state of the art RNN-based moderation method can be improved by adding user embeddings, user type embeddings, user biases, or user type biases. We observe improvements in all cases, with user embeddings leading to the biggest performance gains.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03700",
        "title": "WASSA-2017 Shared Task on Emotion Intensity",
        "authors": [
            "Saif M. Mohammad",
            "Felipe Bravo-Marquez"
        ],
        "abstract": "We present the first shared task on detecting the intensity of emotion felt by the speaker of a tweet. We create the first datasets of tweets annotated for anger, fear, joy, and sadness intensities using a technique called best--worst scaling (BWS). We show that the annotations lead to reliable fine-grained intensity scores (rankings of tweets by intensity). The data was partitioned into training, development, and test sets for the competition. Twenty-two teams participated in the shared task, with the best system obtaining a Pearson correlation of 0.747 with the gold intensity scores. We summarize the machine learning setups, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful for the task. The emotion intensity dataset and the shared task are helping improve our understanding of how we convey more or less intense emotions through language.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03743",
        "title": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs",
        "authors": [
            "Nanyun Peng",
            "Hoifung Poon",
            "Chris Quirk",
            "Kristina Toutanova",
            "Wen-tau Yih"
        ],
        "abstract": "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.\n    ",
        "submission_date": "2017-08-12T00:00:00",
        "last_modified_date": "2017-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03910",
        "title": "Semi-supervised emotion lexicon expansion with label propagation and specialized word embeddings",
        "authors": [
            "Mario Giulianelli"
        ],
        "abstract": "There exist two main approaches to automatically extract affective orientation: lexicon-based and corpus-based. In this work, we argue that these two methods are compatible and show that combining them can improve the accuracy of emotion classifiers. In particular, we introduce a novel variant of the Label Propagation algorithm that is tailored to distributed word representations, we apply batch gradient descent to accelerate the optimization of label propagation and to make the optimization feasible for large graphs, and we propose a reproducible method for emotion lexicon expansion. We conclude that label propagation can expand an emotion lexicon in a meaningful way and that the expanded emotion lexicon can be leveraged to improve the accuracy of an emotion classifier.\n    ",
        "submission_date": "2017-08-13T00:00:00",
        "last_modified_date": "2017-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03920",
        "title": "Towards Speech Emotion Recognition \"in the wild\" using Aggregated Corpora and Deep Multi-Task Learning",
        "authors": [
            "Jaebok Kim",
            "Gwenn Englebienne",
            "Khiet P. Truong",
            "Vanessa Evers"
        ],
        "abstract": "One of the challenges in Speech Emotion Recognition (SER) \"in the wild\" is the large mismatch between training and test data (e.g. speakers and tasks). In order to improve the generalisation capabilities of the emotion models, we propose to use Multi-Task Learning (MTL) and use gender and naturalness as auxiliary tasks in deep neural networks. This method was evaluated in within-corpus and various cross-corpus classification experiments that simulate conditions \"in the wild\". In comparison to Single-Task Learning (STL) based state of the art methods, we found that our MTL method proposed improved performance significantly. Particularly, models using both gender and naturalness achieved more gains than those using either gender or naturalness separately. This benefit was also found in the high-level representations of the feature space, obtained from our method proposed, where discriminative emotional clusters could be observed.\n    ",
        "submission_date": "2017-08-13T00:00:00",
        "last_modified_date": "2017-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03940",
        "title": "Leveraging Sparse and Dense Feature Combinations for Sentiment Classification",
        "authors": [
            "Tao Yu",
            "Christopher Hidey",
            "Owen Rambow",
            "Kathleen McKeown"
        ],
        "abstract": "Neural networks are one of the most popular approaches for many natural language processing tasks such as sentiment analysis. They often outperform traditional machine learning models and achieve the state-of-art results on most tasks. However, many existing deep learning models are complex, difficult to train and provide a limited improvement over simpler methods. We propose a simple, robust and powerful model for sentiment classification. This model outperforms many deep learning models and achieves comparable results to other deep learning models with complex architectures on sentiment analysis datasets. We publish the code online.\n    ",
        "submission_date": "2017-08-13T00:00:00",
        "last_modified_date": "2017-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03994",
        "title": "Data Sets: Word Embeddings Learned from Tweets and General Data",
        "authors": [
            "Quanzhi Li",
            "Sameena Shah",
            "Xiaomo Liu",
            "Armineh Nourbakhsh"
        ],
        "abstract": "A word embedding is a low-dimensional, dense and real- valued vector representation of a word. Word embeddings have been used in many NLP tasks. They are usually gener- ated from a large text corpus. The embedding of a word cap- tures both its syntactic and semantic aspects. Tweets are short, noisy and have unique lexical and semantic features that are different from other types of text. Therefore, it is necessary to have word embeddings learned specifically from tweets. In this paper, we present ten word embedding data sets. In addition to the data sets learned from just tweet data, we also built embedding sets from the general data and the combination of tweets with the general data. The general data consist of news articles, Wikipedia data and other web data. These ten embedding models were learned from about 400 million tweets and 7 billion words from the general text. In this paper, we also present two experiments demonstrating how to use the data sets in some NLP tasks, such as tweet sentiment analysis and tweet topic classification tasks.\n    ",
        "submission_date": "2017-08-14T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03995",
        "title": "Sentiment Analysis by Joint Learning of Word Embeddings and Classifier",
        "authors": [
            "Prathusha Kameswara Sarma",
            "Bill Sethares"
        ],
        "abstract": "Word embeddings are representations of individual words of a text document in a vector space and they are often use- ful for performing natural language pro- cessing tasks. Current state of the art al- gorithms for learning word embeddings learn vector representations from large corpora of text documents in an unsu- pervised fashion. This paper introduces SWESA (Supervised Word Embeddings for Sentiment Analysis), an algorithm for sentiment analysis via word embeddings. SWESA leverages document label infor- mation to learn vector representations of words from a modest corpus of text doc- uments by solving an optimization prob- lem that minimizes a cost function with respect to both word embeddings as well as classification accuracy. Analysis re- veals that SWESA provides an efficient way of estimating the dimension of the word embeddings that are to be learned. Experiments on several real world data sets show that SWESA has superior per- formance when compared to previously suggested approaches to word embeddings and sentiment analysis tasks.\n    ",
        "submission_date": "2017-08-14T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04134",
        "title": "A Measure for Dialog Complexity and its Application in Streamlining Service Operations",
        "authors": [
            "Q Vera Liao",
            "Biplav Srivastava",
            "Pavan Kapanipathi"
        ],
        "abstract": "Dialog is a natural modality for interaction between customers and businesses in the service industry. As customers call up the service provider, their interactions may be routine or extraordinary. We believe that these interactions, when seen as dialogs, can be analyzed to obtain a better understanding of customer needs and how to efficiently address them. We introduce the idea of a dialog complexity measure to characterize multi-party interactions, propose a general data-driven method to calculate it, use it to discover insights in public and enterprise dialog datasets, and demonstrate its beneficial usage in facilitating better handling of customer requests and evaluating service agents.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04299",
        "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks",
        "authors": [
            "Sayyed M. Zahiri",
            "Jinho D. Choi"
        ],
        "abstract": "While there have been significant advances in detecting emotions from speech and image recognition, emotion detection on text is still under-explored and remained as an active research field. This paper introduces a corpus for text-based emotion detection on multiparty dialogue as well as deep neural models that outperform the existing approaches for document classification. We first present a new corpus that provides annotation of seven emotions on consecutive utterances in dialogues extracted from the show, Friends. We then suggest four types of sequence-based convolutional neural network models with attention that leverage the sequence information encapsulated in dialogue. Our best model shows the accuracies of 37.9% and 54% for fine- and coarse-grained emotions, respectively. Given the difficulty of this task, this is promising.\n    ",
        "submission_date": "2017-08-14T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04358",
        "title": "Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks",
        "authors": [
            "Afshin Rahimi",
            "Timothy Baldwin",
            "Trevor Cohn"
        ],
        "abstract": "We propose a method for embedding two-dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over Twitter data, the proposed model outperforms conventional regression-based geolocation and provides a better estimate of uncertainty. We also show the effectiveness of the representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.\n    ",
        "submission_date": "2017-08-14T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04390",
        "title": "Fluency-Guided Cross-Lingual Image Captioning",
        "authors": [
            "Weiyu Lan",
            "Xirong Li",
            "Jianfeng Dong"
        ],
        "abstract": "Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual setting. Different from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of fluency in the translated sentences, we propose in this paper a fluency-guided learning framework. The framework comprises a module to automatically estimate the fluency of the sentences and another module to utilize the estimated fluency scores to effectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both fluency and relevance of the generated captions in Chinese, but without using any manually written sentences from the target language.\n    ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04439",
        "title": "Extractive Summarization using Deep Learning",
        "authors": [
            "Sukriti Verma",
            "Vagisha Nidhi"
        ],
        "abstract": "This paper proposes a text summarization approach for factual reports using a deep learning model. This approach consists of three phases: feature extraction, feature enhancement, and summary generation, which work together to assimilate core information and generate a coherent, understandable summary. We are exploring various features to improve the set of sentences selected for the summary, and are using a Restricted Boltzmann Machine to enhance and abstract those features to improve resultant accuracy without losing any important information. The sentences are scored based on those enhanced features and an extractive summary is constructed. Experimentation carried out on several articles demonstrates the effectiveness of the proposed approach. Source code available at: ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2019-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04469",
        "title": "Comparison of Decoding Strategies for CTC Acoustic Models",
        "authors": [
            "Thomas Zenkel",
            "Ramon Sanabria",
            "Florian Metze",
            "Jan Niehues",
            "Matthias Sperber",
            "Sebastian St\u00fcker",
            "Alex Waibel"
        ],
        "abstract": "Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR. For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string. We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area.\n    ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04557",
        "title": "Database of Parliamentary Speeches in Ireland, 1919-2013",
        "authors": [
            "Alexander Herzog",
            "Slava J. Mikhaylov"
        ],
        "abstract": "We present a database of parliamentary debates that contains the complete record of parliamentary speeches from D\u00e1il \u00c9ireann, the lower house and principal chamber of the Irish parliament, from 1919 to 2013. In addition, the database contains background information on all TDs (Teachta D\u00e1la, members of parliament), such as their party affiliations, constituencies and office positions. The current version of the database includes close to 4.5 million speeches from 1,178 TDs. The speeches were downloaded from the official parliament website and further processed and parsed with a Python script. Background information on TDs was collected from the member database of the parliament website. Data on cabinet positions (ministers and junior ministers) was collected from the official website of the government. A record linkage algorithm and human coders were used to match TDs and ministers.\n    ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04559",
        "title": "Statistical Vs Rule Based Machine Translation; A Case Study on Indian Language Perspective",
        "authors": [
            "Sreelekha S"
        ],
        "abstract": "In this paper we present our work on a case study between Statistical Machien Transaltion (SMT) and Rule-Based Machine Translation (RBMT) systems on English-Indian langugae and Indian to Indian langugae perspective. Main objective of our study is to make a five way performance compariosn; such as, a) SMT and RBMT b) SMT on English-Indian langugae c) RBMT on English-Indian langugae d) SMT on Indian to Indian langugae perspective e) RBMT on Indian to Indian langugae perspective. Through a detailed analysis we describe the Rule Based and the Statistical Machine Translation system developments and its evaluations. Through a detailed error analysis, we point out the relative strengths and weaknesses of both systems. The observations based on our study are: a) SMT systems outperforms RBMT b) In the case of SMT, English to Indian language MT systmes performs better than Indian to English langugae MT systems c) In the case of RBMT, English to Indian langugae MT systems perofrms better than Indian to Englsih Language MT systems d) SMT systems performs better for Indian to Indian language MT systems compared to RBMT. Effectively, we shall see that even with a small amount of training corpus a statistical machine translation system has many advantages for high quality domain specific machine translation over that of a rule-based counterpart.\n    ",
        "submission_date": "2017-08-12T00:00:00",
        "last_modified_date": "2017-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04587",
        "title": "Automatic Summarization of Online Debates",
        "authors": [
            "Nattapong Sanchan",
            "Ahmet Aker",
            "Kalina Bontcheva"
        ],
        "abstract": "Debate summarization is one of the novel and challenging research areas in automatic text summarization which has been largely unexplored. In this paper, we develop a debate summarization pipeline to summarize key topics which are discussed or argued in the two opposing sides of online debates. We view that the generation of debate summaries can be achieved by clustering, cluster labeling, and visualization. In our work, we investigate two different clustering approaches for the generation of the summaries. In the first approach, we generate the summaries by applying purely term-based clustering and cluster labeling. The second approach makes use of X-means for clustering and Mutual Information for labeling the clusters. Both approaches are driven by ontologies. We visualize the results using bar charts. We think that our results are a smooth entry for users aiming to receive the first impression about what is discussed within a debate topic containing waste number of argumentations.\n    ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04592",
        "title": "Gold Standard Online Debates Summaries and First Experiments Towards Automatic Summarization of Online Debate Data",
        "authors": [
            "Nattapong Sanchan",
            "Ahmet Aker",
            "Kalina Bontcheva"
        ],
        "abstract": "Usage of online textual media is steadily increasing. Daily, more and more news stories, blog posts and scientific articles are added to the online volumes. These are all freely accessible and have been employed extensively in multiple research areas, e.g. automatic text summarization, information retrieval, information extraction, etc. Meanwhile, online debate forums have recently become popular, but have remained largely unexplored. For this reason, there are no sufficient resources of annotated debate data available for conducting research in this genre. In this paper, we collected and annotated debate data for an automatic summarization task. Similar to extractive gold standard summary generation our data contains sentences worthy to include into a summary. Five human annotators performed this task. Inter-annotator agreement, based on semantic similarity, is 36% for Cohen's kappa and 48% for Krippendorff's alpha. Moreover, we also implement an extractive summarization system for online debates and discuss prominent features for the task of summarizing online debate data automatically.\n    ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04681",
        "title": "Identifying Harm Events in Clinical Care through Medical Narratives",
        "authors": [
            "Arman Cohan",
            "Allan Fong",
            "Raj Ratwani",
            "Nazli Goharian"
        ],
        "abstract": "Preventable medical errors are estimated to be among the leading causes of injury and death in the United States. To prevent such errors, healthcare systems have implemented patient safety and incident reporting systems. These systems enable clinicians to report unsafe conditions and cases where patients have been harmed due to errors in medical care. These reports are narratives in natural language and while they provide detailed information about the situation, it is non-trivial to perform large scale analysis for identifying common causes of errors and harm to the patients. In this work, we present a method based on attentive convolutional and recurrent networks for identifying harm events in patient care and categorize the harm based on its severity level. We demonstrate that our methods can significantly improve the performance over existing methods in identifying harm in clinical care.\n    ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04704",
        "title": "Evaluating Word Embeddings for Sentence Boundary Detection in Speech Transcripts",
        "authors": [
            "Marcos V. Treviso",
            "Christopher D. Shulby",
            "Sandra M. Aluisio"
        ],
        "abstract": "This paper is motivated by the automation of neuropsychological tests involving discourse analysis in the retellings of narratives by patients with potential cognitive impairment. In this scenario the task of sentence boundary detection in speech transcripts is important as discourse analysis involves the application of Natural Language Processing tools, such as taggers and parsers, which depend on the sentence as a processing unit. Our aim in this paper is to verify which embedding induction method works best for the sentence boundary detection task, specifically whether it be those which were proposed to capture semantic, syntactic or morphological similarities.\n    ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04729",
        "title": "Deconvolutional Paragraph Representation Learning",
        "authors": [
            "Yizhe Zhang",
            "Dinghan Shen",
            "Guoyin Wang",
            "Zhe Gan",
            "Ricardo Henao",
            "Lawrence Carin"
        ],
        "abstract": "Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.\n    ",
        "submission_date": "2017-08-16T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04755",
        "title": "Learning Chinese Word Representations From Glyphs Of Characters",
        "authors": [
            "Tzu-Ray Su",
            "Hung-Yi Lee"
        ],
        "abstract": "In this paper, we propose new methods to learn Chinese word representations. Chinese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by convolutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chinese and made them public.\n    ",
        "submission_date": "2017-08-16T00:00:00",
        "last_modified_date": "2017-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04765",
        "title": "Dialogue Act Segmentation for Vietnamese Human-Human Conversational Texts",
        "authors": [
            "Thi Lan Ngo",
            "Khac Linh Pham",
            "Minh Son Cao",
            "Son Bao Pham",
            "Xuan Hieu Phan"
        ],
        "abstract": "Dialog act identification plays an important role in understanding conversations. It has been widely applied in many fields such as dialogue systems, automatic machine translation, automatic speech recognition, and especially useful in systems with human-computer natural language dialogue interfaces such as virtual assistants and chatbots. The first step of identifying dialog act is identifying the boundary of the dialog act in utterances. In this paper, we focus on segmenting the utterance according to the dialog act boundaries, i.e. functional segments identification, for Vietnamese utterances. We investigate carefully functional segment identification in two approaches: (1) machine learning approach using maximum entropy (ME) and conditional random fields (CRFs); (2) deep learning approach using bidirectional Long Short-Term Memory (LSTM) with a CRF layer (Bi-LSTM-CRF) on two different conversational datasets: (1) Facebook messages (Message data); (2) transcription from phone conversations (Phone data). To the best of our knowledge, this is the first work that applies deep learning based approach to dialog act segmentation. As the results show, deep learning approach performs appreciably better as to compare with traditional machine learning approaches. Moreover, it is also the first study that tackles dialog act and functional segment identification for Vietnamese.\n    ",
        "submission_date": "2017-08-16T00:00:00",
        "last_modified_date": "2017-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04923",
        "title": "mAnI: Movie Amalgamation using Neural Imitation",
        "authors": [
            "Naveen Panwar",
            "Shreya Khare",
            "Neelamadhav Gantayat",
            "Rahul Aralikatte",
            "Senthil Mani",
            "Anush Sankaran"
        ],
        "abstract": "Cross-modal data retrieval has been the basis of various creative tasks performed by Artificial Intelligence (AI). One such highly challenging task for AI is to convert a book into its corresponding movie, which most of the creative film makers do as of today. In this research, we take the first step towards it by visualizing the content of a book using its corresponding movie visuals. Given a set of sentences from a book or even a fan-fiction written in the same universe, we employ deep learning models to visualize the input by stitching together relevant frames from the movie. We studied and compared three different types of setting to match the book with the movie content: (i) Dialog model: using only the dialog from the movie, (ii) Visual model: using only the visual content from the movie, and (iii) Hybrid model: using the dialog and the visual content from the movie. Experiments on the publicly available MovieBook dataset shows the effectiveness of the proposed models.\n    ",
        "submission_date": "2017-08-16T00:00:00",
        "last_modified_date": "2017-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05045",
        "title": "Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding",
        "authors": [
            "Zequn Sun",
            "Wei Hu",
            "Chengkai Li"
        ],
        "abstract": "Entity alignment is the task of finding entities in two knowledge bases (KBs) that represent the same real-world object. When facing KBs in different natural languages, conventional cross-lingual entity alignment methods rely on machine translation to eliminate the language barriers. These approaches often suffer from the uneven quality of translations between languages. While recent embedding-based techniques encode entities and relationships in KBs and do not need machine translation for cross-lingual entity alignment, a significant number of attributes remain largely unexplored. In this paper, we propose a joint attribute-preserving embedding model for cross-lingual entity alignment. It jointly embeds the structures of two KBs into a unified vector space and further refines it by leveraging attribute correlations in the KBs. Our experimental results on real-world datasets show that this approach significantly outperforms the state-of-the-art embedding approaches for cross-lingual entity alignment and could be complemented with methods based on machine translation.\n    ",
        "submission_date": "2017-08-16T00:00:00",
        "last_modified_date": "2017-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05071",
        "title": "Learning spectro-temporal features with 3D CNNs for speech emotion recognition",
        "authors": [
            "Jaebok Kim",
            "Khiet P. Truong",
            "Gwenn Englebienne",
            "Vanessa Evers"
        ],
        "abstract": "In this paper, we propose to use deep 3-dimensional convolutional networks (3D CNNs) in order to address the challenge of modelling spectro-temporal dynamics for speech emotion recognition (SER). Compared to a hybrid of Convolutional Neural Network and Long-Short-Term-Memory (CNN-LSTM), our proposed 3D CNNs simultaneously extract short-term and long-term spectral features with a moderate number of parameters. We evaluated our proposed and other state-of-the-art methods in a speaker-independent manner using aggregated corpora that give a large and diverse set of speakers. We found that 1) shallow temporal and moderately deep spectral kernels of a homogeneous architecture are optimal for the task; and 2) our 3D CNNs are more effective for spectro-temporal feature learning compared to other methods. Finally, we visualised the feature space obtained with our proposed method using t-distributed stochastic neighbour embedding (T-SNE) and could observe distinct clusters of emotions.\n    ",
        "submission_date": "2017-08-14T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05148",
        "title": "Natural Language Processing: State of The Art, Current Trends and Challenges",
        "authors": [
            "Diksha Khurana",
            "Aditya Koli",
            "Kiran Khatter",
            "Sukhdev Singh"
        ],
        "abstract": "Natural language processing (NLP) has recently gained much attention for representing and analysing human language computationally. It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. The paper distinguishes four phases by discussing different levels of NLP and components of Natural Language Generation (NLG) followed by presenting the history and evolution of NLP, state of the art presenting the various applications of NLP and current trends and challenges.\n    ",
        "submission_date": "2017-08-17T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05269",
        "title": "Towards Syntactic Iberian Polarity Classification",
        "authors": [
            "David Vilares",
            "Marcos Garcia",
            "Miguel A. Alonso",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "abstract": "Lexicon-based methods using syntactic rules for polarity classification rely on parsers that are dependent on the language and on treebank guidelines. Thus, rules are also dependent and require adaptation, especially in multilingual scenarios. We tackle this challenge in the context of the Iberian Peninsula, releasing the first symbolic syntax-based Iberian system with rules shared across five official languages: Basque, Catalan, Galician, Portuguese and Spanish. The model is made available.\n    ",
        "submission_date": "2017-08-17T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05286",
        "title": "Simple Open Stance Classification for Rumour Analysis",
        "authors": [
            "Ahmet Aker",
            "Leon Derczynski",
            "Kalina Bontcheva"
        ],
        "abstract": "Stance classification determines the attitude, or stance, in a (typically short) text. The task has powerful applications, such as the detection of fake news or the automatic extraction of attitudes toward entities or events in the media. This paper describes a surprisingly simple and efficient classification approach to open stance classification in Twitter, for rumour and veracity classification. The approach profits from a novel set of automatically identifiable problem-specific features, which significantly boost classifier accuracy and achieve above state-of-the-art results on recent benchmark datasets. This calls into question the value of using complex sophisticated models for stance classification without first doing informed feature extraction.\n    ",
        "submission_date": "2017-08-17T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05449",
        "title": "An Annotated Corpus of Relational Strategies in Customer Service",
        "authors": [
            "Ian Beaver",
            "Cynthia Freeman",
            "Abdullah Mueen"
        ],
        "abstract": "We create and release the first publicly available commercial customer service corpus with annotated relational segments. Human-computer data from three live customer service Intelligent Virtual Agents (IVAs) in the domains of travel and telecommunications were collected, and reviewers marked all text that was deemed unnecessary to the determination of user intention. After merging the selections of multiple reviewers to create highlighted texts, a second round of annotation was done to determine the classes of language present in the highlighted sections such as the presence of Greetings, Backstory, Justification, Gratitude, Rants, or Emotions. This resulting corpus is a valuable resource for improving the quality and relational abilities of IVAs. As well as discussing the corpus itself, we compare the usage of such language in human-human interactions on TripAdvisor forums. We show that removal of this language from task-based inputs has a positive effect on IVA understanding by both an increase in confidence and improvement in responses, demonstrating the need for automated methods of its discovery.\n    ",
        "submission_date": "2017-08-17T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05466",
        "title": "Large-Scale Domain Adaptation via Teacher-Student Learning",
        "authors": [
            "Jinyu Li",
            "Michael L. Seltzer",
            "Xi Wang",
            "Rui Zhao",
            "Yifan Gong"
        ],
        "abstract": "High accuracy speech recognition requires a large amount of transcribed data for supervised training. In the absence of such data, domain adaptation of a well-trained acoustic model can be performed, but even here, high accuracy usually requires significant labeled data from the target domain. In this work, we propose an approach to domain adaptation that does not require transcriptions but instead uses a corpus of unlabeled parallel data, consisting of pairs of samples from the source domain of the well-trained model and the desired target domain. To perform adaptation, we employ teacher/student (T/S) learning, in which the posterior probabilities generated by the source-domain model can be used in lieu of labels to train the target-domain model. We evaluate the proposed approach in two scenarios, adapting a clean acoustic model to noisy speech and adapting an adults speech acoustic model to children speech. Significant improvements in accuracy are obtained, with reductions in word error rate of up to 44% over the original source model without the need for transcribed data in the target domain. Moreover, we show that increasing the amount of unlabeled data results in additional model robustness, which is particularly beneficial when using simulated training data in the target-domain.\n    ",
        "submission_date": "2017-08-17T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05482",
        "title": "A Question Answering Approach to Emotion Cause Extraction",
        "authors": [
            "Lin Gui",
            "Jiannan Hu",
            "Yulan He",
            "Ruifeng Xu",
            "Qin Lu",
            "Jiachen Du"
        ],
        "abstract": "Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05515",
        "title": "Syllable-level Neural Language Model for Agglutinative Language",
        "authors": [
            "Seunghak Yu",
            "Nilesh Kulkarni",
            "Haejun Lee",
            "Jihie Kim"
        ],
        "abstract": "Language models for agglutinative languages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes. We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property. Our model outperforms character-level embedding in perplexity by 16.87 with 9.50M parameters. Proposed method achieves state of the art performance over existing input prediction methods in terms of Key Stroke Saving and has been commercialized.\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05521",
        "title": "EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity",
        "authors": [
            "Edison Marrese-Taylor",
            "Yutaka Matsuo"
        ],
        "abstract": "In this paper we describe a deep learning system that has been designed and built for the WASSA 2017 Emotion Intensity Shared Task. We introduce a representation learning approach based on inner attention on top of an RNN. Results show that our model offers good capabilities and is able to successfully identify emotion-bearing words to predict intensity without leveraging on lexicons, obtaining the 13th place among 22 shared task competitors.\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05536",
        "title": "Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution",
        "authors": [
            "E. Manjavacas",
            "J. de Gussem",
            "W. Daelemans",
            "M. Kestemont"
        ],
        "abstract": "Recent applications of neural language models have led to an increased interest in the automatic generation of natural language. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation: its ability to reproduce authorial writing styles. Using established models for authorship attribution, we empirically assess the stylistic qualities of neurally generated text. In comparison to conventional language models, neural models generate fuzzier text that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05582",
        "title": "Agree to Disagree: Improving Disagreement Detection with Dual GRUs",
        "authors": [
            "Sushant Hiray",
            "Venkatesh Duppada"
        ],
        "abstract": "This paper presents models for detecting agreement/disagreement in online discussions. In this work we show that by using a Siamese inspired architecture to encode the discussions, we no longer need to rely on hand-crafted features to exploit the meta thread structure. We evaluate our model on existing online discussion corpora - ABCD, IAC and AWTP. Experimental results on ABCD dataset show that by fusing lexical and word embedding features, our model achieves the state of the art performance of 0.804 average F1 score. We also show that the model trained on ABCD dataset performs competitively on relatively smaller annotated datasets (IAC and AWTP).\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05592",
        "title": "Future Word Contexts in Neural Network Language Models",
        "authors": [
            "Xie Chen",
            "Xunying Liu",
            "Anton Ragni",
            "Yu Wang",
            "Mark Gales"
        ],
        "abstract": "Recently, bidirectional recurrent network language models (bi-RNNLMs) have been shown to outperform standard, unidirectional, recurrent neural network language models (uni-RNNLMs) on a range of speech recognition tasks. This indicates that future word context information beyond the word history can be useful. However, bi-RNNLMs pose a number of challenges as they make use of the complete previous and future word context information. This impacts both training efficiency and their use within a lattice rescoring framework. In this paper these issues are addressed by proposing a novel neural network structure, succeeding word RNNLMs (su-RNNLMs). Instead of using a recurrent unit to capture the complete future word contexts, a feedforward unit is used to model a finite number of succeeding, future, words. This model can be trained much more efficiently than bi-RNNLMs and can also be used for lattice rescoring. Experimental results on a meeting transcription task (AMI) show the proposed model consistently outperformed uni-RNNLMs and yield only a slight degradation compared to bi-RNNLMs in N-best rescoring. Additionally, performance improvements can be obtained using lattice rescoring and subsequent confusion network decoding.\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05682",
        "title": "An Improved Residual LSTM Architecture for Acoustic Modeling",
        "authors": [
            "Lu Huang",
            "Jiasong Sun",
            "Ji Xu",
            "Yi Yang"
        ],
        "abstract": "Long Short-Term Memory (LSTM) is the primary recurrent neural networks architecture for acoustic modeling in automatic speech recognition systems. Residual learning is an efficient method to help neural networks converge easier and faster. In this paper, we propose several types of residual LSTM methods for our acoustic modeling. Our experiments indicate that, compared with classic LSTM, our architecture shows more than 8% relative reduction in Phone Error Rate (PER) on TIMIT tasks. At the same time, our residual fast LSTM approach shows 4% relative reduction in PER on the same task. Besides, we find that all this architecture could have good results on THCHS-30, Librispeech and Switchboard corpora.\n    ",
        "submission_date": "2017-08-17T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05719",
        "title": "Cross-Lingual Dependency Parsing for Closely Related Languages - Helsinki's Submission to VarDial 2017",
        "authors": [
            "J\u00f6rg Tiedemann"
        ],
        "abstract": "This paper describes the submission from the University of Helsinki to the shared task on cross-lingual dependency parsing at VarDial 2017. We present work on annotation projection and treebank translation that gave good results for all three target languages in the test set. In particular, Slovak seems to work well with information coming from the Czech treebank, which is in line with related work. The attachment scores for cross-lingual models even surpass the fully supervised models trained on the target language treebank. Croatian is the most difficult language in the test set and the improvements over the baseline are rather modest. Norwegian works best with information coming from Swedish whereas Danish contributes surprisingly little.\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05729",
        "title": "Neural machine translation for low-resource languages",
        "authors": [
            "Robert \u00d6stling",
            "J\u00f6rg Tiedemann"
        ],
        "abstract": "Neural machine translation (NMT) approaches have improved the state of the art in many machine translation settings over the last couple of years, but they require large amounts of training data to produce sensible output. We demonstrate that NMT can be used for low-resource languages as well, by introducing more local dependencies and using word alignments to learn sentence reordering during translation. In addition to our novel model, we also present an empirical evaluation of low-resource phrase-based statistical machine translation (SMT) and NMT to investigate the lower limits of the respective technologies. We find that while SMT remains the best option for low-resource settings, our method can produce acceptable translations with only 70000 tokens of training data, a level where the baseline NMT system fails completely.\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05763",
        "title": "The Natural Stories Corpus",
        "authors": [
            "Richard Futrell",
            "Edward Gibson",
            "Hal Tily",
            "Idan Blank",
            "Anastasia Vishnevetsky",
            "Steven T. Piantadosi",
            "Evelina Fedorenko"
        ],
        "abstract": "It is now a common practice to compare models of human language processing by predicting participant reactions (such as reading times) to corpora consisting of rich naturalistic linguistic materials. However, many of the corpora used in these studies are based on naturalistic text and thus do not contain many of the low-frequency syntactic constructions that are often required to distinguish processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected parse trees and includes self-paced reading time data. Here we give an overview of the content of the corpus and release the data.\n    ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05797",
        "title": "CLaC @ QATS: Quality Assessment for Text Simplification",
        "authors": [
            "Elnaz Davoodi",
            "Leila Kosseim"
        ],
        "abstract": "This paper describes our approach to the 2016 QATS quality assessment shared task. We trained three independent Random Forest classifiers in order to assess the quality of the simplified texts in terms of grammaticality, meaning preservation and simplicity. We used the language model of Google-Ngram as feature to predict the grammaticality. Meaning preservation is predicted using two complementary approaches based on word embedding and WordNet synonyms. A wider range of features including TF-IDF, sentence length and frequency of cue phrases are used to evaluate the simplicity aspect. Overall, the accuracy of the system ranges from 33.33% for the overall aspect to 58.73% for grammaticality.\n    ",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05798",
        "title": "The CLaC Discourse Parser at CoNLL-2016",
        "authors": [
            "Majid Laali",
            "Andre Cianflone",
            "Leila Kosseim"
        ],
        "abstract": "This paper describes our submission \"CLaC\" to the CoNLL-2016 shared task on shallow discourse parsing. We used two complementary approaches for the task. A standard machine learning approach for the parsing of explicit relations, and a deep learning approach for non-explicit relations. Overall, our parser achieves an F1-score of 0.2106 on the identification of discourse relations (0.3110 for explicit relations and 0.1219 for non-explicit relations) on the blind CoNLL-2016 test set.\n    ",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05800",
        "title": "On the Contribution of Discourse Structure on Text Complexity Assessment",
        "authors": [
            "Elnaz Davoodi",
            "Leila Kosseim"
        ],
        "abstract": "This paper investigates the influence of discourse features on text complexity assessment. To do so, we created two data sets based on the Penn Discourse Treebank and the Simple English Wikipedia corpora and compared the influence of coherence, cohesion, surface, lexical and syntactic features to assess text complexity.\n",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05801",
        "title": "ClaC: Semantic Relatedness of Words and Phrases",
        "authors": [
            "Reda Siblini",
            "Leila Kosseim"
        ],
        "abstract": "The measurement of phrasal semantic relatedness is an important metric for many natural language processing applications. In this paper, we present three approaches for measuring phrasal semantics, one based on a semantic network model, another on a distributional similarity model, and a hybrid between the two. Our hybrid approach achieved an F-measure of 77.4% on the task of evaluating the semantic similarity of words and compositional phrases.\n    ",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05803",
        "title": "Measuring the Effect of Discourse Relations on Blog Summarization",
        "authors": [
            "Shamima Mithun",
            "Leila Kosseim"
        ],
        "abstract": "The work presented in this paper attempts to evaluate and quantify the use of discourse relations in the context of blog summarization and compare their use to more traditional and factual texts. Specifically, we measured the usefulness of 6 discourse relations - namely comparison, contingency, illustration, attribution, topic-opinion, and attributive for the task of text summarization from blogs. We have evaluated the effect of each relation using the TAC 2008 opinion summarization dataset and compared them with the results with the DUC 2007 dataset. The results show that in both textual genres, contingency, comparison, and illustration relations provide a significant improvement on summarization content; while attribution, topic-opinion, and attributive relations do not provide a consistent and significant improvement. These results indicate that, at least for summarization, discourse relations are just as useful for informal and affective texts as for more traditional news articles.\n    ",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05857",
        "title": "The CLaC Discourse Parser at CoNLL-2015",
        "authors": [
            "Majid Laali",
            "Elnaz Davoodi",
            "Leila Kosseim"
        ],
        "abstract": "This paper describes our submission (kosseim15) to the CoNLL-2015 shared task on shallow discourse parsing. We used the UIMA framework to develop our parser and used ClearTK to add machine learning functionality to the UIMA framework. Overall, our parser achieves a result of 17.3 F1 on the identification of discourse relations on the blind CoNLL-2015 test set, ranking in sixth place.\n    ",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05873",
        "title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016",
        "authors": [
            "Alexander Baturo",
            "Niheer Dasandi",
            "Slava J. Mikhaylov"
        ],
        "abstract": "There is surprisingly little known about agenda setting for international development in the United Nations (UN) despite it having a significant influence on the process and outcomes of development efforts. This paper addresses this shortcoming using a novel approach that applies natural language processing techniques to countries' annual statements in the UN General Debate. Every year UN member states deliver statements during the General Debate on their governments' perspective on major issues in world politics. These speeches provide invaluable information on state preferences on a wide range of issues, including international development, but have largely been overlooked in the study of global politics. This paper identifies the main international development topics that states raise in these speeches between 1970 and 2016, and examine the country-specific drivers of international development rhetoric.\n    ",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05891",
        "title": "Arabic Multi-Dialect Segmentation: bi-LSTM-CRF vs. SVM",
        "authors": [
            "Mohamed Eldesouki",
            "Younes Samih",
            "Ahmed Abdelali",
            "Mohammed Attia",
            "Hamdy Mubarak",
            "Kareem Darwish",
            "Kallmeyer Laura"
        ],
        "abstract": "Arabic word segmentation is essential for a variety of NLP applications such as machine translation and information retrieval. Segmentation entails breaking words into their constituent stems, affixes and clitics. In this paper, we compare two approaches for segmenting four major Arabic dialects using only several thousand training examples for each dialect. The two approaches involve posing the problem as a ranking problem, where an SVM ranker picks the best segmentation, and as a sequence labeling problem, where a bi-LSTM RNN coupled with CRF determines where best to segment words. We are able to achieve solid segmentation results for all dialects using rather limited training data. We also show that employing Modern Standard Arabic data for domain adaptation and assuming context independence improve overall results.\n    ",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05942",
        "title": "The Helsinki Neural Machine Translation System",
        "authors": [
            "Robert \u00d6stling",
            "Yves Scherrer",
            "J\u00f6rg Tiedemann",
            "Gongbo Tang",
            "Tommi Nieminen"
        ],
        "abstract": "We introduce the Helsinki Neural Machine Translation system (HNMT) and how it is applied in the news translation task at WMT 2017, where it ranked first in both the human and automatic evaluations for English--Finnish. We discuss the success of English--Finnish translations and the overall advantage of NMT over a strong SMT baseline. We also discuss our submissions for English--Latvian, English--Chinese and Chinese--English.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05943",
        "title": "Neural Machine Translation with Extended Context",
        "authors": [
            "J\u00f6rg Tiedemann",
            "Yves Scherrer"
        ],
        "abstract": "We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05956",
        "title": "An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog",
        "authors": [
            "Bing Liu",
            "Ian Lane"
        ],
        "abstract": "We present a novel end-to-end trainable neural network model for task-oriented dialog systems. The model is able to track dialog state, issue API calls to knowledge base (KB), and incorporate structured KB query results into system responses to successfully complete task-oriented dialogs. The proposed model produces well-structured system responses by jointly learning belief tracking and KB result processing conditioning on the dialog history. We evaluate the model in a restaurant search domain using a dataset that is converted from the second Dialog State Tracking Challenge (DSTC2) corpus. Experiment results show that the proposed model can robustly track dialog state given the dialog history. Moreover, our model demonstrates promising results in producing appropriate system responses, outperforming prior end-to-end trainable neural network models using per-response accuracy evaluation metrics.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05992",
        "title": "Expanding Abbreviations in a Strongly Inflected Language: Are Morphosyntactic Tags Sufficient?",
        "authors": [
            "Piotr \u017belasko"
        ],
        "abstract": "In this paper, the problem of recovery of morphological information lost in abbreviated forms is addressed with a focus on highly inflected languages. Evidence is presented that the correct inflected form of an expanded abbreviation can in many cases be deduced solely from the morphosyntactic tags of the context. The prediction model is a deep bidirectional LSTM network with tag embedding. The training and evaluation data are gathered by finding the words which could have been abbreviated and using their corresponding morphosyntactic tags as the labels, while the tags of the context words are used as the input features for classification. The network is trained on over 10 million words from the Polish Sejm Corpus and achieves 74.2% prediction accuracy on a smaller, but more general National Corpus of Polish. The analysis of errors suggests that performance in this task may improve if some prior knowledge about the abbreviated word is incorporated into the model.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2018-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05997",
        "title": "A Batch Noise Contrastive Estimation Approach for Training Large Vocabulary Language Models",
        "authors": [
            "Youssef Oualil",
            "Dietrich Klakow"
        ],
        "abstract": "Training large vocabulary Neural Network Language Models (NNLMs) is a difficult task due to the explicit requirement of the output layer normalization, which typically involves the evaluation of the full softmax function over the complete vocabulary. This paper proposes a Batch Noise Contrastive Estimation (B-NCE) approach to alleviate this problem. This is achieved by reducing the vocabulary, at each time step, to the target words in the batch and then replacing the softmax by the noise contrastive estimation approach, where these words play the role of targets and noise samples at the same time. In doing so, the proposed approach can be fully formulated and implemented using optimal dense matrix operations. Applying B-NCE to train different NNLMs on the Large Text Compression Benchmark (LTCB) and the One Billion Word Benchmark (OBWB) shows a significant reduction of the training time with no noticeable degradation of the models performance. This paper also presents a new baseline comparative study of different standard NNLMs on the large OBWB on a single Titan-X GPU.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2017-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06022",
        "title": "Learning to Paraphrase for Question Answering",
        "authors": [
            "Li Dong",
            "Jonathan Mallinson",
            "Siva Reddy",
            "Mirella Lapata"
        ],
        "abstract": "Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-to-end using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06025",
        "title": "Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks",
        "authors": [
            "Nathan Hartmann",
            "Erick Fonseca",
            "Christopher Shulby",
            "Marcos Treviso",
            "Jessica Rodrigues",
            "Sandra Aluisio"
        ],
        "abstract": "Word embeddings have been found to provide meaningful representations for words in an efficient way; therefore, they have become common in Natural Language Processing sys- tems. In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants. We trained 31 word embedding models using FastText, GloVe, Wang2Vec and Word2Vec. We evaluated them intrinsically on syntactic and semantic analogies and extrinsically on POS tagging and sentence semantic similarity tasks. The obtained results suggest that word analogies are not appropriate for word embedding evaluation; task-specific evaluations appear to be a better option.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06068",
        "title": "Vector Space Model as Cognitive Space for Text Classification",
        "authors": [
            "Barathi Ganesh HB",
            "Anand Kumar M",
            "Soman KP"
        ],
        "abstract": "In this era of digitization, knowing the user's sociolect aspects have become essential features to build the user specific recommendation systems. These sociolect aspects could be found by mining the user's language sharing in the form of text in social media and reviews. This paper describes about the experiment that was performed in PAN Author Profiling 2017 shared task. The objective of the task is to find the sociolect aspects of the users from their tweets. The sociolect aspects considered in this experiment are user's gender and native language information. Here user's tweets written in a different language from their native language are represented as Document - Term Matrix with document frequency as the constraint. Further classification is done using the Support Vector Machine by taking gender and native language as target classes. This experiment attains the average accuracy of 73.42% in gender prediction and 76.26% in the native language identification task.\n    ",
        "submission_date": "2017-08-21T00:00:00",
        "last_modified_date": "2017-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06073",
        "title": "The Microsoft 2017 Conversational Speech Recognition System",
        "authors": [
            "W. Xiong",
            "L. Wu",
            "F. Alleva",
            "J. Droppo",
            "X. Huang",
            "A. Stolcke"
        ],
        "abstract": "We describe the 2017 version of Microsoft's conversational speech recognition system, in which we update our 2016 system with recent developments in neural-network-based acoustic and language modeling to further advance the state of the art on the Switchboard speech recognition task. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby subsets of acoustic models are first combined at the senone/frame level, followed by a word-level voting via confusion networks. We also added a confusion network rescoring step after system combination. The resulting system yields a 5.1\\% word error rate on the 2000 Switchboard evaluation set.\n    ",
        "submission_date": "2017-08-21T00:00:00",
        "last_modified_date": "2017-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06075",
        "title": "Scientific Information Extraction with Semi-supervised Neural Tagging",
        "authors": [
            "Yi Luan",
            "Mari Ostendorf",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.\n    ",
        "submission_date": "2017-08-21T00:00:00",
        "last_modified_date": "2017-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06185",
        "title": "Seernet at EmoInt-2017: Tweet Emotion Intensity Estimator",
        "authors": [
            "Venkatesh Duppada",
            "Sushant Hiray"
        ],
        "abstract": "The paper describes experiments on estimating emotion intensity in tweets using a generalized regressor system. The system combines lexical, syntactic and pre-trained word embedding features, trains them on general regressors and finally combines the best performing models to create an ensemble. The proposed system stood 3rd out of 22 systems in the leaderboard of WASSA-2017 Shared Task on Emotion Intensity.\n    ",
        "submission_date": "2017-08-21T00:00:00",
        "last_modified_date": "2017-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06426",
        "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models",
        "authors": [
            "Anuroop Sriram",
            "Heewoo Jun",
            "Sanjeev Satheesh",
            "Adam Coates"
        ],
        "abstract": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.\n    ",
        "submission_date": "2017-08-21T00:00:00",
        "last_modified_date": "2017-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06510",
        "title": "Handling Homographs in Neural Machine Translation",
        "authors": [
            "Frederick Liu",
            "Han Lu",
            "Graham Neubig"
        ],
        "abstract": "Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct translation based on the context. However, with the advent of neural machine translation (NMT) systems, which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words. We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense be- fore feeding it into the encoder. Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs.\n    ",
        "submission_date": "2017-08-22T00:00:00",
        "last_modified_date": "2018-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06550",
        "title": "Golden Years, Golden Shores: A Study of Elders in Online Travel Communities",
        "authors": [
            "Bart\u0142omiej Balcerzak",
            "Rados\u0142aw Nielek"
        ],
        "abstract": "In this paper we present our exploratory findings related to extracting knowledge and experiences from a community of senior tourists. By using tools of qualitative analysis as well as review of literature, we managed to verify a set of hypotheses related to the content created by senior tourists when participating in on-line communities. We also produced a codebook, representing various themes one may encounter in such communities. This codebook, derived from our own qualitative research, as well a literature review will serve as a basis for further development of automated tools of knowledge extraction. We also managed to find that older adults more often than other poster in tourists forums, mention their age in discussion, more often share their experiences and motivation to travel, however they do not differ in relation to describing barriers encountered while traveling.\n    ",
        "submission_date": "2017-08-22T00:00:00",
        "last_modified_date": "2017-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06555",
        "title": "Long-Short Range Context Neural Networks for Language Modeling",
        "authors": [
            "Youssef Oualil",
            "Mittul Singh",
            "Clayton Greenberg",
            "Dietrich Klakow"
        ],
        "abstract": "The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques.\n    ",
        "submission_date": "2017-08-22T00:00:00",
        "last_modified_date": "2017-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06708",
        "title": "A rule based algorithm for detecting negative words in Persian",
        "authors": [
            "Reza Takhshid",
            "Adel Rahimi"
        ],
        "abstract": "In this paper, we present a novel method for detecting negative words in Persian. We first used an algorithm to an exceptions list which was later modified by hand. We then used the mentioned lists and a Persian polarity corpus in our rule based algorithm to detect negative words.\n    ",
        "submission_date": "2017-08-19T00:00:00",
        "last_modified_date": "2017-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06828",
        "title": "Classification of Radiology Reports Using Neural Attention Models",
        "authors": [
            "Bonggun Shin",
            "Falgun H. Chokshi",
            "Timothy Lee",
            "Jinho D. Choi"
        ],
        "abstract": "The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure \"black-box\" models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies.\n    ",
        "submission_date": "2017-08-22T00:00:00",
        "last_modified_date": "2017-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06989",
        "title": "A Neural Network Approach for Mixing Language Models",
        "authors": [
            "Youssef Oualil",
            "Dietrich Klakow"
        ],
        "abstract": "The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.\n    ",
        "submission_date": "2017-08-23T00:00:00",
        "last_modified_date": "2017-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07104",
        "title": "Automatic Detection of Fake News",
        "authors": [
            "Ver\u00f3nica P\u00e9rez-Rosas",
            "Bennett Kleinberg",
            "Alexandra Lefevre",
            "Rada Mihalcea"
        ],
        "abstract": "The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analysis on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors. In addition, we provide comparative analyses of the automatic and manual identification of fake news.\n    ",
        "submission_date": "2017-08-23T00:00:00",
        "last_modified_date": "2017-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07149",
        "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses",
        "authors": [
            "Ryan Lowe",
            "Michael Noseworthy",
            "Iulian V. Serban",
            "Nicolas Angelard-Gontier",
            "Yoshua Bengio",
            "Joelle Pineau"
        ],
        "abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.\n    ",
        "submission_date": "2017-08-23T00:00:00",
        "last_modified_date": "2018-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07241",
        "title": "NNVLP: A Neural Network-Based Vietnamese Language Processing Toolkit",
        "authors": [
            "Thai-Hoang Pham",
            "Xuan-Khoai Pham",
            "Tuan-Anh Nguyen",
            "Phuong Le-Hong"
        ],
        "abstract": "This paper demonstrates neural network-based toolkit namely NNVLP for essential Vietnamese language processing tasks including part-of-speech (POS) tagging, chunking, named entity recognition (NER). Our toolkit is a combination of bidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network (CNN), Conditional Random Field (CRF), using pre-trained word embeddings as input, which achieves state-of-the-art results on these three tasks. We provide both API and web demo for this toolkit.\n    ",
        "submission_date": "2017-08-24T00:00:00",
        "last_modified_date": "2017-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07252",
        "title": "A Study on Neural Network Language Modeling",
        "authors": [
            "Dengliang Shi"
        ],
        "abstract": "An exhaustive study on neural network language modeling (NNLM) is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed.\n    ",
        "submission_date": "2017-08-24T00:00:00",
        "last_modified_date": "2017-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07265",
        "title": "An Image Analysis Approach to the Calligraphy of Books",
        "authors": [
            "Henrique F. de Arruda",
            "Vanessa Q. Marinho",
            "Thales S. Lima",
            "Diego R. Amancio",
            "Luciano da F. Costa"
        ],
        "abstract": "Text network analysis has received increasing attention as a consequence of its wide range of applications. In this work, we extend a previous work founded on the study of topological features of mesoscopic networks. Here, the geometrical properties of visualized networks are quantified in terms of several image analysis techniques and used as subsidies for authorship attribution. It was found that the visual features account for performance similar to that achieved by using topological measurements. In addition, the combination of these two types of features improved the performance.\n    ",
        "submission_date": "2017-08-24T00:00:00",
        "last_modified_date": "2017-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07279",
        "title": "Combining Discrete and Neural Features for Sequence Labeling",
        "authors": [
            "Jie Yang",
            "Zhiyang Teng",
            "Meishan Zhang",
            "Yue Zhang"
        ],
        "abstract": "Neural network models have recently received heated research attention in the natural language processing community. Compared with traditional models with discrete features, neural models have two main advantages. First, they take low-dimensional, real-valued embedding vectors as inputs, which can be trained over large raw data, thereby addressing the issue of feature sparsity in discrete models. Second, deep neural networks can be used to automatically combine input features, and including non-local features that capture semantic patterns that cannot be expressed using discrete indicator features. As a result, neural network models have achieved competitive accuracies compared with the best discrete models for a range of NLP tasks.\n",
        "submission_date": "2017-08-24T00:00:00",
        "last_modified_date": "2017-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07403",
        "title": "CloudScan - A configuration-free invoice analysis system using recurrent neural networks",
        "authors": [
            "Rasmus Berg Palm",
            "Ole Winther",
            "Florian Laws"
        ],
        "abstract": "We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation. In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts. The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely. We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system. We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788.\n    ",
        "submission_date": "2017-08-24T00:00:00",
        "last_modified_date": "2017-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07476",
        "title": "M2D: Monolog to Dialog Generation for Conversational Story Telling",
        "authors": [
            "Kevin K. Bowden",
            "Grace I. Lin",
            "Lena I. Reed",
            "Marilyn A. Walker"
        ],
        "abstract": "Storytelling serves many different social functions, e.g. stories are used to persuade, share troubles, establish shared values, learn social behaviors, and entertain. Moreover, stories are often told conversationally through dialog, and previous work suggests that information provided dialogically is more engaging than when provided in monolog. In this paper, we present algorithms for converting a deep representation of a story into a dialogic storytelling, that can vary aspects of the telling, including the personality of the storytellers. We conduct several experiments to test whether dialogic storytellings are more engaging, and whether automatically generated variants in linguistic form that correspond to personality differences can be recognized in an extended storytelling dialog.\n    ",
        "submission_date": "2017-08-24T00:00:00",
        "last_modified_date": "2017-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07524",
        "title": "Supervised Speech Separation Based on Deep Learning: An Overview",
        "authors": [
            "DeLiang Wang",
            "Jitong Chen"
        ],
        "abstract": "Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multi-talker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.\n    ",
        "submission_date": "2017-08-24T00:00:00",
        "last_modified_date": "2018-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07624",
        "title": "SPARQL as a Foreign Language",
        "authors": [
            "Tommaso Soru",
            "Edgard Marx",
            "Diego Moussallem",
            "Gustavo Publio",
            "Andr\u00e9 Valdestilhas",
            "Diego Esteves",
            "Ciro Baron Neto"
        ],
        "abstract": "In the last years, the Linked Data Cloud has achieved a size of more than 100 billion facts pertaining to a multitude of domains. However, accessing this information has been significantly challenging for lay users. Approaches to problems such as Question Answering on Linked Data and Link Discovery have notably played a role in increasing information access. These approaches are often based on handcrafted and/or statistical models derived from data observation. Recently, Deep Learning architectures based on Neural Networks called seq2seq have shown to achieve state-of-the-art results at translating sequences into sequences. In this direction, we propose Neural SPARQL Machines, end-to-end deep architectures to translate any natural language expression into sentences encoding SPARQL queries. Our preliminary results, restricted on selected DBpedia classes, show that Neural SPARQL Machines are a promising approach for Question Answering on Linked Data, as they can deal with known problems such as vocabulary mismatch and perform graph pattern composition.\n    ",
        "submission_date": "2017-08-25T00:00:00",
        "last_modified_date": "2020-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07690",
        "title": "Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization",
        "authors": [
            "Demian Gholipour Ghalandari"
        ],
        "abstract": "The centroid-based model for extractive document summarization is a simple and fast baseline that ranks sentences based on their similarity to a centroid vector. In this paper, we apply this ranking to possible summaries instead of sentences and use a simple greedy algorithm to find the best summary. Furthermore, we show possi- bilities to scale up to larger input docu- ment collections by selecting a small num- ber of sentences from each document prior to constructing the summary. Experiments were done on the DUC2004 dataset for multi-document summarization. We ob- serve a higher performance over the orig- inal model, on par with more complex state-of-the-art methods.\n    ",
        "submission_date": "2017-08-25T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07722",
        "title": "A dependency look at the reality of constituency",
        "authors": [
            "Xinying Chen",
            "Carlos G\u00f3mez-Rodr\u00edguez",
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "A comment on \"Neurophysiological dynamics of phrase-structure building during sentence processing\" by Nelson et al (2017), Proceedings of the National Academy of Sciences USA 114(18), E3669-E3678.\n    ",
        "submission_date": "2017-08-24T00:00:00",
        "last_modified_date": "2018-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07863",
        "title": "$k$-Nearest Neighbor Augmented Neural Networks for Text Classification",
        "authors": [
            "Zhiguo Wang",
            "Wael Hamza",
            "Linfeng Song"
        ],
        "abstract": "In recent years, many deep-learning based models are proposed for text classification. This kind of models well fits the training set from the statistical point of view. However, it lacks the capacity of utilizing instance-level information from individual instances in the training set. In this work, we propose to enhance neural network models by allowing them to leverage information from $k$-nearest neighbor (kNN) of the input text. Our model employs a neural network that encodes texts into text embeddings. Moreover, we also utilize $k$-nearest neighbor of the input text as an external memory, and utilize it to capture instance-level information from the training set. The final prediction is made based on features from both the neural network encoder and the kNN memory. Experimental results on several standard benchmark datasets show that our model outperforms the baseline model on all the datasets, and it even beats a very deep neural network model (with 29 layers) in several datasets. Our model also shows superior performance when training instances are scarce, and when the training set is severely unbalanced. Our model also leverages techniques such as semi-supervised training and transfer learning quite well.\n    ",
        "submission_date": "2017-08-25T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07950",
        "title": "Machine Translation in Indian Languages: Challenges and Resolution",
        "authors": [
            "Raj Nath Patel",
            "Prakash B. Pimpale",
            "M Sasikumar"
        ],
        "abstract": "English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using pre-ordering and suffix separation. The pre-ordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of pre-ordering and suffix separation helps in improving the quality of English to Indian Language machine translation.\n    ",
        "submission_date": "2017-08-26T00:00:00",
        "last_modified_date": "2018-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08484",
        "title": "Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank",
        "authors": [
            "Kai Zhao",
            "Liang Huang"
        ],
        "abstract": "Discourse parsing has long been treated as a stand-alone problem independent from constituency or dependency parsing. Most attempts at this problem are pipelined rather than end-to-end, sophisticated, and not self-contained: they assume gold-standard text segmentations (Elementary Discourse Units), and use external parsers for syntactic features. In this paper we propose the first end-to-end discourse parser that jointly parses in both syntax and discourse levels, as well as the first syntacto-discourse treebank by integrating the Penn Treebank with the RST Treebank. Built upon our recent span-based constituency parser, this joint syntacto-discourse parser requires no preprocessing whatsoever (such as segmentation or feature extraction), achieves the state-of-the-art end-to-end discourse parsing accuracy.\n    ",
        "submission_date": "2017-08-28T00:00:00",
        "last_modified_date": "2017-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08572",
        "title": "Really? Well. Apparently Bootstrapping Improves the Performance of Sarcasm and Nastiness Classifiers for Online Dialogue",
        "authors": [
            "Stephanie Lukin",
            "Marilyn Walker"
        ],
        "abstract": "More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic Natural Language Processing resources such as news, highly social dialogue is frequent in social media, making it a challenging context for NLP. This paper tests a bootstrapping method, originally proposed in a monologic domain, to train classifiers to identify two different types of subjective language in dialogue: sarcasm and nastiness. We explore two methods of developing linguistic indicators to be used in a first level classifier aimed at maximizing precision at the expense of recall. The best performing classifier for the first phase achieves 54% precision and 38% recall for sarcastic utterances. We then use general syntactic patterns from previous work to create more general sarcasm indicators, improving precision to 62% and recall to 52%. To further test the generality of the method, we then apply it to bootstrapping a classifier for nastiness dialogic acts. Our first phase, using crowdsourced nasty indicators, achieves 58% precision and 49% recall, which increases to 75% precision and 62% recall when we bootstrap over the first level with generalized syntactic patterns.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08573",
        "title": "Generating Different Story Tellings from Semantic Representations of Narrative",
        "authors": [
            "Elena Rishes",
            "Stephanie M. Lukin",
            "David K. Elson",
            "Marilyn A. Walker"
        ],
        "abstract": "In order to tell stories in different voices for different audiences, interactive story systems require: (1) a semantic representation of story structure, and (2) the ability to automatically generate story and dialogue from this semantic representation using some form of Natural Language Generation (NLG). However, there has been limited research on methods for linking story structures to narrative descriptions of scenes and story events. In this paper we present an automatic method for converting from Scheherazade's story intention graph, a semantic representation, to the input required by the Personage NLG engine. Using 36 Aesop Fables distributed in DramaBank, a collection of story encodings, we train translation rules on one story and then test these rules by generating text for the remaining 35. The results are measured in terms of the string similarity metrics Levenshtein Distance and BLEU score. The results show that we can generate the 35 stories with correct content: the test set stories on average are close to the output of the Scheherazade realizer, which was customized to this semantic representation. We provide some examples of story variations generated by personage. In future work, we will experiment with measuring the quality of the same stories generated in different voices, and with techniques for making storytelling interactive.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08575",
        "title": "Identifying Subjective and Figurative Language in Online Dialogue",
        "authors": [
            "Stephanie M. Lukin",
            "Luke Eisenberg",
            "Thomas Corcoran",
            "Marilyn A. Walker"
        ],
        "abstract": "More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic resources such as news, highly social dialogue is very frequent in social media. We aim to automatically identify sarcastic and nasty utterances in unannotated online dialogue, extending a bootstrapping method previously applied to the classification of monologic subjective sentences in Riloff and Weibe 2003. We have adapted the method to fit the sarcastic and nasty dialogic domain. Our method is as follows: 1) Explore methods for identifying sarcastic and nasty cue words and phrases in dialogues; 2) Use the learned cues to train a sarcastic (nasty) Cue-Based Classifier; 3) Learn general syntactic extraction patterns from the sarcastic (nasty) utterances and define fine-tuned sarcastic patterns to create a Pattern-Based Classifier; 4) Combine both Cue-Based and fine-tuned Pattern-Based Classifiers to maximize precision at the expense of recall and test on unannotated utterances.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08580",
        "title": "Generating Sentence Planning Variations for Story Telling",
        "authors": [
            "Stephanie M. Lukin",
            "Lena I. Reed",
            "Marilyn A. Walker"
        ],
        "abstract": "There has been a recent explosion in applications for dialogue interaction ranging from direction-giving and tourist information to interactive story systems. Yet the natural language generation (NLG) component for many of these systems remains largely handcrafted. This limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content. We propose that a solution to this problem lies in new methods for developing language generation resources. We describe the ES-Translator, a computational language generator that has previously been applied only to fables, and quantitatively evaluate the domain independence of the EST by applying it to personal narratives from weblogs. We then take advantage of recent work on language generation to create a parameterized sentence planner for story generation that provides aggregation operations, variations in discourse and in point of view. Finally, we present a user evaluation of different personal narrative retellings.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08585",
        "title": "Narrative Variations in a Virtual Storyteller",
        "authors": [
            "Stephanie M. Lukin",
            "Marilyn A. Walker"
        ],
        "abstract": "Research on storytelling over the last 100 years has distinguished at least two levels of narrative representation (1) story, or fabula; and (2) discourse, or sujhet. We use this distinction to create Fabula Tales, a computational framework for a virtual storyteller that can tell the same story in different ways through the implementation of general narratological variations, such as varying direct vs. indirect speech, character voice (style), point of view, and focalization. A strength of our computational framework is that it is based on very general methods for re-using existing story content, either from fables or from personal narratives collected from blogs. We first explain how a simple annotation tool allows naive annotators to easily create a deep representation of fabula called a story intention graph, and show how we use this representation to generate story tellings automatically. Then we present results of two studies testing our narratological parameters, and showing that different tellings affect the reader's perception of the story and characters.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08615",
        "title": "Comparing Human and Machine Errors in Conversational Speech Transcription",
        "authors": [
            "Andreas Stolcke",
            "Jasha Droppo"
        ],
        "abstract": "Recent work in automatic recognition of conversational telephone speech (CTS) has achieved accuracy levels comparable to human transcribers, although there is some debate how to precisely quantify human performance on this task, using the NIST 2000 CTS evaluation set. This raises the question what systematic differences, if any, may be found differentiating human from machine transcription errors. In this paper we approach this question by comparing the output of our most accurate CTS recognition system to that of a standard speech transcription vendor pipeline. We find that the most frequent substitution, deletion and insertion error types of both outputs show a high degree of overlap. The only notable exception is that the automatic recognizer tends to confuse filled pauses (\"uh\") and backchannel acknowledgments (\"uhhuh\"). Humans tend not to make this error, presumably due to the distinctive and opposing pragmatic functions attached to these words. Furthermore, we quantify the correlation between human and machine errors at the speaker level, and investigate the effect of speaker overlap between training and test data. Finally, we report on an informal \"Turing test\" asking humans to discriminate between automatic and human transcription error cases.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08712",
        "title": "Neural Machine Translation Training in a Multi-Domain Scenario",
        "authors": [
            "Hassan Sajjad",
            "Nadir Durrani",
            "Fahim Dalvi",
            "Yonatan Belinkov",
            "Stephan Vogel"
        ],
        "abstract": "In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and multi-model ensemble. Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning an already trained model.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2018-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08959",
        "title": "A Simple LSTM model for Transition-based Dependency Parsing",
        "authors": [
            "Mohab Elkaref",
            "Bernd Bohnet"
        ],
        "abstract": "We present a simple LSTM-based transition-based dependency parser. Our model is composed of a single LSTM hidden layer replacing the hidden layer in the usual feed-forward network architecture. We also propose a new initialization method that uses the pre-trained weights from a feed-forward neural network to initialize our LSTM-based model. We also show that using dropout on the input layer has a positive effect on performance. Our final parser achieves a 93.06% unlabeled and 91.01% labeled attachment score on the Penn Treebank. We additionally replace LSTMs with GRUs and Elman units in our model and explore the effectiveness of our initialization method on individual gates constituting all three types of RNN units.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09025",
        "title": "Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling",
        "authors": [
            "Xiaofeng Zhu",
            "Diego Klabjan",
            "Patrick Bless"
        ],
        "abstract": "In this paper, we present hierarchical relationbased latent Dirichlet allocation (hrLDA), a data-driven hierarchical topic model for extracting terminological ontologies from a large number of heterogeneous documents. In contrast to traditional topic models, hrLDA relies on noun phrases instead of unigrams, considers syntax and document structures, and enriches topic hierarchies with topic relations. Through a series of experiments, we demonstrate the superiority of hrLDA over existing topic models, especially for building hierarchies. Furthermore, we illustrate the robustness of hrLDA in the settings of noisy data sets, which are likely to occur in many practical scenarios. Our ontology evaluation results show that ontologies extracted from hrLDA are very competitive with the ontologies created by domain experts.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09082",
        "title": "PersonaBank: A Corpus of Personal Narratives and Their Story Intention Graphs",
        "authors": [
            "Stephanie M. Lukin",
            "Kevin Bowden",
            "Casey Barackman",
            "Marilyn A. Walker"
        ],
        "abstract": "We present a new corpus, PersonaBank, consisting of 108 personal stories from weblogs that have been annotated with their Story Intention Graphs, a deep representation of the fabula of a story. We describe the topics of the stories and the basis of the Story Intention Graph representation, as well as the process of annotating the stories to produce the Story Intention Graphs and the challenges of adapting the tool to this new personal narrative domain We also discuss how the corpus can be used in applications that retell the story using different styles of tellings, co-tellings, or as a content planner.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09085",
        "title": "Argument Strength is in the Eye of the Beholder: Audience Effects in Persuasion",
        "authors": [
            "Stephanie M. Lukin",
            "Pranav Anand",
            "Marilyn Walker",
            "Steve Whittaker"
        ],
        "abstract": "Americans spend about a third of their time online, with many participating in online conversations on social and political issues. We hypothesize that social media arguments on such issues may be more engaging and persuasive than traditional media summaries, and that particular types of people may be more or less convinced by particular styles of argument, e.g. emotional arguments may resonate with some personalities while factual arguments resonate with others. We report a set of experiments testing at large scale how audience variables interact with argument style to affect the persuasiveness of an argument, an under-researched topic within natural language processing. We show that belief change is affected by personality factors, with conscientious, open and agreeable people being more convinced by emotional arguments.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09090",
        "title": "Automating Direct Speech Variations in Stories and Games",
        "authors": [
            "Stephanie M. Lukin",
            "James O. Ryan",
            "Marilyn A. Walker"
        ],
        "abstract": "Dialogue authoring in large games requires not only content creation but the subtlety of its delivery, which can vary from character to character. Manually authoring this dialogue can be tedious, time-consuming, or even altogether infeasible. This paper utilizes a rich narrative representation for modeling dialogue and an expressive natural language generation engine for realizing it, and expands upon a translation tool that bridges the two. We add functionality to the translator to allow direct speech to be modeled by the narrative representation, whereas the original translator supports only narratives told by a third person narrator. We show that we can perform character substitution in dialogues. We implement and evaluate a potential application to dialogue implementation: generating dialogue for games with big, dynamic, or procedurally-generated open worlds. We present a pilot study on human perceptions of the personalities of characters using direct speech, assuming unknown personality types at the time of authoring.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09151",
        "title": "Paradigm Completion for Derivational Morphology",
        "authors": [
            "Ryan Cotterell",
            "Ekaterina Vylomova",
            "Huda Khayrallah",
            "Christo Kirov",
            "David Yarowsky"
        ],
        "abstract": "The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models, adapted from the inflection task, are able to learn a range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2025-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09157",
        "title": "Cross-lingual, Character-Level Neural Morphological Tagging",
        "authors": [
            "Ryan Cotterell",
            "Georg Heigold"
        ],
        "abstract": "Even for common NLP tasks, sufficient supervision is not available in many languages -- morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones, improving accuracy by up to 30% over a monolingual model.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2025-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09163",
        "title": "An Empirical Study of Discriminative Sequence Labeling Models for Vietnamese Text Processing",
        "authors": [
            "Phuong Le-Hong",
            "Minh Pham Quang Nhat",
            "Thai-Hoang Pham",
            "Tuan-Anh Tran",
            "Dang-Minh Nguyen"
        ],
        "abstract": "This paper presents an empirical study of two widely-used sequence prediction models, Conditional Random Fields (CRFs) and Long Short-Term Memory Networks (LSTMs), on two fundamental tasks for Vietnamese text processing, including part-of-speech tagging and named entity recognition. We show that a strong lower bound for labeling accuracy can be obtained by relying only on simple word-based features with minimal hand-crafted feature engineering, of 90.65\\% and 86.03\\% performance scores on the standard test sets for the two tasks respectively. In particular, we demonstrate empirically the surprising efficiency of word embeddings in both of the two tasks, with both of the two models. We point out that the state-of-the-art LSTMs model does not always outperform significantly the traditional CRFs model, especially on moderate-sized data sets. Finally, we give some suggestions and discussions for efficient use of sequence labeling models in practical applications.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09217",
        "title": "Look-ahead Attention for Generation in Neural Machine Translation",
        "authors": [
            "Long Zhou",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "The attention model has become a standard component in neural machine translation (NMT) and it guides translation process by selectively focusing on parts of the source sentence when predicting each target word. However, we find that the generation of a target word does not only depend on the source sentence, but also rely heavily on the previous generated target words, especially the distant words which are difficult to model by using recurrent neural networks. To solve this problem, we propose in this paper a novel look-ahead attention mechanism for generation in NMT, which aims at directly capturing the dependency relationship between target words. We further design three patterns to integrate our look-ahead attention into the conventional attention model. Experiments on NIST Chinese-to-English and WMT English-to-German translation tasks show that our proposed look-ahead attention mechanism achieves substantial improvements over state-of-the-art baselines.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09230",
        "title": "TANKER: Distributed Architecture for Named Entity Recognition and Disambiguation",
        "authors": [
            "Sandro A. Coelho",
            "Diego Moussallem",
            "Gustavo C. Publio",
            "Diego Esteves"
        ],
        "abstract": "Named Entity Recognition and Disambiguation (NERD) systems have recently been widely researched to deal with the significant growth of the Web. NERD systems are crucial for several Natural Language Processing (NLP) tasks such as summarization, understanding, and machine translation. However, there is no standard interface specification, i.e. these systems may vary significantly either for exporting their outputs or for processing the inputs. Thus, when a given company desires to implement more than one NERD system, the process is quite exhaustive and prone to failure. In addition, industrial solutions demand critical requirements, e.g., large-scale processing, completeness, versatility, and licenses. Commonly, these requirements impose a limitation, making good NERD models to be ignored by companies. This paper presents TANKER, a distributed architecture which aims to overcome scalability, reliability and failure tolerance limitations related to industrial needs by combining NERD systems. To this end, TANKER relies on a micro-services oriented architecture, which enables agile development and delivery of complex enterprise applications. In addition, TANKER provides a standardized API which makes possible to combine several NERD systems at once.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09234",
        "title": "Fighting with the Sparsity of Synonymy Dictionaries",
        "authors": [
            "Dmitry Ustalov",
            "Mikhail Chernoskutov",
            "Chris Biemann",
            "Alexander Panchenko"
        ],
        "abstract": "Graph-based synset induction methods, such as MaxMax and Watset, induce synsets by performing a global clustering of a synonymy graph. However, such methods are sensitive to the structure of the input synonymy graph: sparseness of the input dictionary can substantially reduce the quality of the extracted synsets. In this paper, we propose two different approaches designed to alleviate the incompleteness of the input dictionaries. The first one performs a pre-processing of the graph by adding missing edges, while the second one performs a post-processing by merging similar synset clusters. We evaluate these approaches on two datasets for the Russian language and discuss their impact on the performance of synset induction methods. Finally, we perform an extensive error analysis of each approach and discuss prominent alternative methods for coping with the problem of the sparsity of the synonymy dictionaries.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09403",
        "title": "Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set",
        "authors": [
            "Tianze Shi",
            "Liang Huang",
            "Lillian Lee"
        ],
        "abstract": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case O(n^3) exact decoders for arc-hybrid and arc-eager transition systems. With our minimal features, we also present O(n^3) global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the \"second-best-in-class\" result on the English Penn Treebank.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09417",
        "title": "LangPro: Natural Language Theorem Prover",
        "authors": [
            "Lasha Abzianidze"
        ],
        "abstract": "LangPro is an automated theorem prover for natural language (",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09450",
        "title": "Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events",
        "authors": [
            "Elahe Rahimtoroghi",
            "Ernesto Hernandez",
            "Marilyn A Walker"
        ],
        "abstract": "Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topic-specific datasets enables learning finer-grained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09453",
        "title": "Inference of Fine-Grained Event Causality from Blogs and Films",
        "authors": [
            "Zhichao Hu",
            "Elahe Rahimtoroghi",
            "Marilyn A Walker"
        ],
        "abstract": "Human understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning fine-grained causal relations between everyday events. We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80% of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09496",
        "title": "Inferring Narrative Causality between Event Pairs in Films",
        "authors": [
            "Zhichao Hu",
            "Marilyn A. Walker"
        ],
        "abstract": "To understand narrative, humans draw inferences about the underlying relations between narrative events. Cognitive theories of narrative understanding define these inferences as four different types of causality, that include pairs of events A, B where A physically causes B (X drop, X break), to pairs of events where A causes emotional state B (Y saw X, Y felt fear). Previous work on learning narrative relations from text has either focused on \"strict\" physical causality, or has been vague about what relation is being learned. This paper learns pairs of causal events from a corpus of film scene descriptions which are action rich and tend to be told in chronological order. We show that event pairs induced using our methods are of high quality and are judged to have a stronger causal relation than event pairs from Rel-grams.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09497",
        "title": "Unsupervised Induction of Contingent Event Pairs from Film Scenes",
        "authors": [
            "Zhichao Hu",
            "Elahe Rahimtoroghi",
            "Larissa Munishkina",
            "Reid Swanson",
            "Marilyn A. Walker"
        ],
        "abstract": "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the contingent discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be contingent on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09609",
        "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
        "authors": [
            "Greg Durrett",
            "Jonathan K. Kummerfeld",
            "Taylor Berg-Kirkpatrick",
            "Rebecca S. Portnoff",
            "Sadia Afroz",
            "Damon McCoy",
            "Kirill Levchenko",
            "Vern Paxson"
        ],
        "abstract": "One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own \"fine-grained domain\" in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09702",
        "title": "Human and Machine Judgements for Russian Semantic Relatedness",
        "authors": [
            "Alexander Panchenko",
            "Dmitry Ustalov",
            "Nikolay Arefyev",
            "Denis Paperno",
            "Natalia Konstantinova",
            "Natalia Loukachevitch",
            "Chris Biemann"
        ],
        "abstract": "Semantic relatedness of terms represents similarity of meaning by a numerical score. On the one hand, humans easily make judgments about semantic relatedness. On the other hand, this kind of information is useful in language processing systems. While semantic relatedness has been extensively studied for English using numerous language resources, such as associative norms, human judgments, and datasets generated from lexical databases, no evaluation resources of this kind have been available for Russian to date. Our contribution addresses this problem. We present five language resources of different scale and purpose for Russian semantic relatedness, each being a list of triples (word_i, word_j, relatedness_ij). Four of them are designed for evaluation of systems for computing semantic relatedness, complementing each other in terms of the semantic relation type they represent. These benchmarks were used to organize a shared task on Russian semantic relatedness, which attracted 19 teams. We use one of the best approaches identified in this competition to generate the fifth high-coverage resource, the first open distributional thesaurus of Russian. Multiple evaluations of this thesaurus, including a large-scale crowdsourcing study involving native speakers, indicate its high accuracy.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09789",
        "title": "Learning Lexico-Functional Patterns for First-Person Affect",
        "authors": [
            "Lena Reed",
            "Jiaqi Wu",
            "Shereen Oraby",
            "Pranav Anand",
            "Marilyn Walker"
        ],
        "abstract": "Informal first-person narratives are a unique resource for computational models of everyday events and people's affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate's arguments. We present a method to learn proxies for these functions from first-person narratives. We construct a novel fine-grained test set, and show that the patterns we learn improve our ability to predict first-person affective reactions to everyday events, from a Stanford sentiment baseline of .67F to .75F.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09803",
        "title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation",
        "authors": [
            "Toan Q. Nguyen",
            "David Chiang"
        ],
        "abstract": "We present a simple method to improve neural translation of a low-resource language pair using parallel data from a related, also low-resource, language pair. The method is based on the transfer method of Zoph et al., but whereas their method ignores any source vocabulary overlap, ours exploits it. First, we split words using Byte Pair Encoding (BPE) to increase vocabulary overlap. Then, we train a model on the first language pair and transfer its parameters, including its source word embeddings, to another model and continue training on the second language pair. Our experiments show that transfer learning helps word-based translation only slightly, but when used on top of a much stronger BPE baseline, it yields larger improvements of up to 4.3 BLEU.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00023",
        "title": "R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering",
        "authors": [
            "Shuohang Wang",
            "Mo Yu",
            "Xiaoxiao Guo",
            "Zhiguo Wang",
            "Tim Klinger",
            "Wei Zhang",
            "Shiyu Chang",
            "Gerald Tesauro",
            "Bowen Zhou",
            "Jing Jiang"
        ],
        "abstract": "In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that \"reads\" the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance. In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader $(R^3)$, based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker along with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00028",
        "title": "Glyph-aware Embedding of Chinese Characters",
        "authors": [
            "Falcon Z. Dai",
            "Zheng Cai"
        ],
        "abstract": "Given the advantage and recent success of English character-level and subword-unit models in several NLP tasks, we consider the equivalent modeling problem for Chinese. Chinese script is logographic and many Chinese logograms are composed of common substructures that provide semantic, phonetic and syntactic hints. In this work, we propose to explicitly incorporate the visual appearance of a character's glyph in its representation, resulting in a novel glyph-aware embedding of Chinese characters. Being inspired by the success of convolutional neural networks in computer vision, we use them to incorporate the spatio-structural patterns of Chinese glyphs as rendered in raw pixels. In the context of two basic Chinese NLP tasks of language modeling and word segmentation, the model learns to represent each character's task-relevant semantic and syntactic information in the character-level embedding.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00094",
        "title": "Linguistic Reflexes of Well-Being and Happiness in Echo",
        "authors": [
            "Jiaqi Wu",
            "Marilyn Walker",
            "Pranav Anand",
            "Steve Whittaker"
        ],
        "abstract": "Different theories posit different sources for feelings of well-being and happiness. Appraisal theory grounds our emotional responses in our goals and desires and their fulfillment, or lack of fulfillment. Self Determination theory posits that the basis for well-being rests on our assessment of our competence, autonomy, and social connection. And surveys that measure happiness empirically note that people require their basic needs to be met for food and shelter, but beyond that tend to be happiest when socializing, eating or having sex. We analyze a corpus of private microblogs from a well-being application called ECHO, where users label each written post about daily events with a happiness score between 1 and 9. Our goal is to ground the linguistic descriptions of events that users experience in theories of well-being and happiness, and then examine the extent to which different theoretical accounts can explain the variance in the happiness scores. We show that recurrent event types, such as OBLIGATION and INCOMPETENCE, which affect people's feelings of well-being are not captured in current lexical or semantic resources.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00103",
        "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning",
        "authors": [
            "Victor Zhong",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00155",
        "title": "Order-Planning Neural Text Generation From Structured Data",
        "authors": [
            "Lei Sha",
            "Lili Mou",
            "Tianyu Liu",
            "Pascal Poupart",
            "Sujian Li",
            "Baobao Chang",
            "Zhifang Sui"
        ],
        "abstract": "Generating texts from structured data (e.g., a table) is important for various natural language processing tasks such as question answering and dialog systems. In recent studies, researchers use neural language models and encoder-decoder frameworks for table-to-text generation. However, these neural network-based approaches do not model the order of contents during text generation. When a human writes a summary based on a given table, he or she would probably consider the content order before wording. In a biography, for example, the nationality of a person is typically mentioned before occupation in a biography. In this paper, we propose an order-planning text generation model to capture the relationship between different fields and use such relationship to make the generated text more fluent and smooth. We conducted experiments on the WikiBio dataset and achieve significantly higher performance than previous methods in terms of BLEU, ROUGE, and NIST scores.\n    ",
        "submission_date": "2017-09-01T00:00:00",
        "last_modified_date": "2017-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00224",
        "title": "Variational Inference for Logical Inference",
        "authors": [
            "Guy Emerson",
            "Ann Copestake"
        ],
        "abstract": "Functional Distributional Semantics is a framework that aims to learn, from text, semantic representations which can be interpreted in terms of truth. Here we make two contributions to this framework. The first is to show how a type of logical inference can be performed by evaluating conditional probabilities. The second is to make these calculations tractable by means of a variational approximation. This approximation also enables faster convergence during training, allowing us to close the gap with state-of-the-art vector space models when evaluating on semantic similarity. We demonstrate promising performance on two tasks.\n    ",
        "submission_date": "2017-09-01T00:00:00",
        "last_modified_date": "2017-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00226",
        "title": "Semantic Composition via Probabilistic Model Theory",
        "authors": [
            "Guy Emerson",
            "Ann Copestake"
        ],
        "abstract": "Semantic composition remains an open problem for vector space models of semantics. In this paper, we explain how the probabilistic graphical model used in the framework of Functional Distributional Semantics can be interpreted as a probabilistic version of model theory. Building on this, we explain how various semantic phenomena can be recast in terms of conditional probabilities in the graphical model. This connection between formal semantics and machine learning is helpful in both directions: it gives us an explicit mechanism for modelling context-dependent meanings (a challenge for formal semantics), and also gives us well-motivated techniques for composing distributed representations (a challenge for distributional semantics). We present results on two datasets that go beyond word similarity, showing how these semantically-motivated techniques improve on the performance of vector models.\n    ",
        "submission_date": "2017-09-01T00:00:00",
        "last_modified_date": "2017-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00345",
        "title": "Making \"fetch\" happen: The influence of social and linguistic context on nonstandard word growth and decline",
        "authors": [
            "Ian Stewart",
            "Jacob Eisenstein"
        ],
        "abstract": "In an online community, new words come and go: today's \"haha\" may be replaced by tomorrow's \"lol.\" Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a speech community. But unlike other types of innovation, language change is shaped and constrained by the system in which it takes part. To investigate the links between social and structural factors in language change, we undertake a large-scale analysis of nonstandard word growth in the online community Reddit. We find that dissemination across many linguistic contexts is a sign of growth: words that appear in more linguistic contexts grow faster and survive longer. We also find that social dissemination likely plays a less important role in explaining word growth and decline than previously hypothesized.\n    ",
        "submission_date": "2017-09-01T00:00:00",
        "last_modified_date": "2018-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00354",
        "title": "Query-by-example Spoken Term Detection using Attention-based Multi-hop Networks",
        "authors": [
            "Chia-Wei Ao",
            "Hung-yi Lee"
        ],
        "abstract": "Retrieving spoken content with spoken queries, or query-by- example spoken term detection (STD), is attractive because it makes possible the matching of signals directly on the acoustic level without transcribing them into text. Here, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network, whose input is a spoken query and an audio segment containing several utterances; the output states whether the audio segment includes the query. The model can be trained in either a supervised scenario using labeled data, or in an unsupervised fashion. In the supervised scenario, we find that the attention mechanism and multiple hops improve performance, and that the attention weights indicate the time span of the detected terms. In the unsupervised setting, the model mimics the behavior of the existing query-by-example STD system, yielding performance comparable to the existing system but with a lower search time complexity.\n    ",
        "submission_date": "2017-09-01T00:00:00",
        "last_modified_date": "2018-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00387",
        "title": "MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge",
        "authors": [
            "Suwon Shon",
            "Ahmed Ali",
            "James Glass"
        ],
        "abstract": "In order to successfully annotate the Arabic speech con- tent found in open-domain media broadcasts, it is essential to be able to process a diverse set of Arabic dialects. For the 2017 Multi-Genre Broadcast challenge (MGB-3) there were two possible tasks: Arabic speech recognition, and Arabic Dialect Identification (ADI). In this paper, we describe our efforts to create an ADI system for the MGB-3 challenge, with the goal of distinguishing amongst four major Arabic dialects, as well as Modern Standard Arabic. Our research fo- cused on dialect variability and domain mismatches between the training and test domain. In order to achieve a robust ADI system, we explored both Siamese neural network models to learn similarity and dissimilarities among Arabic dialects, as well as i-vector post-processing to adapt domain mismatches. Both Acoustic and linguistic features were used for the final MGB-3 submissions, with the best primary system achieving 75% accuracy on the official 10hr test set.\n    ",
        "submission_date": "2017-08-28T00:00:00",
        "last_modified_date": "2017-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00389",
        "title": "End-to-end Learning for Short Text Expansion",
        "authors": [
            "Jian Tang",
            "Yue Wang",
            "Kai Zheng",
            "Qiaozhu Mei"
        ],
        "abstract": "Effectively making sense of short texts is a critical task for many real world applications such as search engines, social media services, and recommender systems. The task is particularly challenging as a short text contains very sparse information, often too sparse for a machine learning algorithm to pick up useful signals. A common practice for analyzing short text is to first expand it with external information, which is usually harvested from a large collection of longer texts. In literature, short text expansion has been done with all kinds of heuristics. We propose an end-to-end solution that automatically learns how to expand short text to optimize a given learning task. A novel deep memory network is proposed to automatically find relevant information from a collection of longer documents and reformulate the short text through a gating mechanism. Using short text classification as a demonstrating task, we show that the deep memory network significantly outperforms classical text expansion methods with comprehensive experiments on real world data sets.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00489",
        "title": "Arc-Standard Spinal Parsing with Stack-LSTMs",
        "authors": [
            "Miguel Ballesteros",
            "Xavier Carreras"
        ],
        "abstract": "We present a neural transition-based parser for spinal trees, a dependency representation of constituent trees. The parser uses Stack-LSTMs that compose constituent nodes with dependency-based derivations. In experiments, we show that this model adapts to different styles of dependency relations, but this choice has little effect for predicting constituent structure, suggesting that LSTMs induce useful states by themselves.\n    ",
        "submission_date": "2017-09-01T00:00:00",
        "last_modified_date": "2017-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00541",
        "title": "Patterns versus Characters in Subword-aware Neural Language Modeling",
        "authors": [
            "Rustem Takhanov",
            "Zhenisbek Assylbekov"
        ],
        "abstract": "Words in some natural languages can have a composite structure. Elements of this structure include the root (that could also be composite), prefixes and suffixes with which various nuances and relations to other words can be expressed. Thus, in order to build a proper word representation one must take into account its internal structure. From a corpus of texts we extract a set of frequent subwords and from the latter set we select patterns, i.e. subwords which encapsulate information on character $n$-gram regularities. The selection is made using the pattern-based Conditional Random Field model with $l_1$ regularization. Further, for every word we construct a new sequence over an alphabet of patterns. The new alphabet's symbols confine a local statistical context stronger than the characters, therefore they allow better representations in ${\\mathbb{R}}^n$ and are better building blocks for word representation. In the task of subword-aware language modeling, pattern-based models outperform character-based analogues by 2-20 perplexity points. Also, a recurrent neural network in which a word is represented as a sum of embeddings of its patterns is on par with a competitive and significantly more sophisticated character-based convolutional architecture.\n    ",
        "submission_date": "2017-09-02T00:00:00",
        "last_modified_date": "2017-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00575",
        "title": "Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection",
        "authors": [
            "Marek Rei",
            "Luana Bulat",
            "Douwe Kiela",
            "Ekaterina Shutova"
        ],
        "abstract": "The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on hand-engineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.\n    ",
        "submission_date": "2017-09-02T00:00:00",
        "last_modified_date": "2017-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00616",
        "title": "Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging",
        "authors": [
            "Hassan Sajjad",
            "Fahim Dalvi",
            "Nadir Durrani",
            "Ahmed Abdelali",
            "Yonatan Belinkov",
            "Stephan Vogel"
        ],
        "abstract": "Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.\n    ",
        "submission_date": "2017-09-02T00:00:00",
        "last_modified_date": "2017-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00659",
        "title": "Investigating how well contextual features are captured by bi-directional recurrent neural network models",
        "authors": [
            "Kushal Chawla",
            "Sunil Kumar Sahu",
            "Ashish Anand"
        ],
        "abstract": "Learning algorithms for natural language processing (NLP) tasks traditionally rely on manually defined relevant contextual features. On the other hand, neural network models using an only distributional representation of words have been successfully applied for several NLP tasks. Such models learn features automatically and avoid explicit feature engineering. Across several domains, neural models become a natural choice specifically when limited characteristics of data are known. However, this flexibility comes at the cost of interpretability. In this paper, we define three different methods to investigate ability of bi-directional recurrent neural networks (RNNs) in capturing contextual features. In particular, we analyze RNNs for sequence tagging tasks. We perform a comprehensive analysis on general as well as biomedical domain datasets. Our experiments focus on important contextual words as features, which can easily be extended to analyze various other feature types. We also investigate positional effects of context words and show how the developed methods can be used for error analysis.\n    ",
        "submission_date": "2017-09-03T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00678",
        "title": "Disentangling ASR and MT Errors in Speech Translation",
        "authors": [
            "Ngoc-Tien Le",
            "Benjamin Lecouteux",
            "Laurent Besacier"
        ],
        "abstract": "The main aim of this paper is to investigate automatic quality assessment for spoken language translation (SLT). More precisely, we investigate SLT errors that can be due to transcription (ASR) or to translation (MT) modules. This paper investigates automatic detection of SLT errors using a single classifier based on joint ASR and MT features. We evaluate both 2-class (good/bad) and 3-class (good/badASR/badMT ) labeling tasks. The 3-class problem necessitates to disentangle ASR and MT errors in the speech translation output and we propose two label extraction methods for this non trivial step. This enables - as a by-product - qualitative analysis on the SLT errors and their origin (are they due to transcription or to translation step?) on our large in-house corpus for French-to-English speech translation.\n    ",
        "submission_date": "2017-09-03T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00770",
        "title": "Understanding the Logical and Semantic Structure of Large Documents",
        "authors": [
            "Muhammad Mahbubur Rahman",
            "Tim Finin"
        ],
        "abstract": "Current language understanding approaches focus on small documents, such as newswire articles, blog posts, product reviews and discussion forum entries. Understanding and extracting information from large documents like legal briefs, proposals, technical manuals and research articles is still a challenging task. We describe a framework that can analyze a large document and help people to know where a particular information is in that document. We aim to automatically identify and classify semantic sections of documents and assign consistent and human-understandable labels to similar sections across documents. A key contribution of our research is modeling the logical and semantic structure of an electronic document. We apply machine learning techniques, including deep learning, in our prototype system. We also make available a dataset of information about a collection of scholarly articles from the arXiv eprints collection that includes a wide range of metadata for each article, including a table of contents, section labels, section summarizations and more. We hope that this dataset will be a useful resource for the machine learning and NLP communities in information retrieval, content-based question answering and language modeling.\n    ",
        "submission_date": "2017-09-03T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00813",
        "title": "From Review to Rating: Exploring Dependency Measures for Text Classification",
        "authors": [
            "Samuel Cunningham-Nelson",
            "Mahsa Baktashmotlagh",
            "Wageeh Boles"
        ],
        "abstract": "Various text analysis techniques exist, which attempt to uncover unstructured information from text. In this work, we explore using statistical dependence measures for textual classification, representing text as word vectors. Student satisfaction scores on a 3-point scale and their free text comments written about university subjects are used as the dataset. We have compared two textual representations: a frequency word representation and term frequency relationship to word vectors, and found that word vectors provide a greater accuracy. However, these word vectors have a large number of features which aggravates the burden of computational complexity. Thus, we explored using a non-linear dependency measure for feature selection by maximizing the dependence between the text reviews and corresponding scores. Our quantitative and qualitative analysis on a student satisfaction dataset shows that our approach achieves comparable accuracy to the full feature vector, while being an order of magnitude faster in testing. These text analysis and feature reduction techniques can be used for other textual data applications such as sentiment analysis.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00831",
        "title": "Hypothesis Testing based Intrinsic Evaluation of Word Embeddings",
        "authors": [
            "Nishant Gurnani"
        ],
        "abstract": "We introduce the cross-match test - an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for word embeddings. We show that cross-match is an effective means of measuring distributional similarity between different vector representations and of evaluating the statistical significance of different vector embedding models. Additionally, we find that cross-match can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for machine translation. We demonstrate that the results of the hypothesis test align with our expectations and note that the framework of two sample hypothesis testing is not limited to word embeddings and can be extended to all vector representations.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00947",
        "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects",
        "authors": [
            "Pedro Saleiro",
            "Lu\u00eds Sarmento",
            "Eduarda Mendes Rodrigues",
            "Carlos Soares",
            "Eug\u00e9nio Oliveira"
        ],
        "abstract": "This paper describes a preliminary study for producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream. We start by experimenting with a relatively small sample and focusing on three challenges: volume of training data, vocabulary size and intrinsic evaluation metrics. Using a single GPU, we were able to scale up vocabulary size from 2048 words embedded and 500K training examples to 32768 words over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch. We also observed that using less than 50\\% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics suffer from over-sensitivity to their corresponding cosine similarity thresholds, indicating that a wider range of metrics need to be developed to track progress.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01042",
        "title": "Getting Reliable Annotations for Sarcasm in Online Dialogues",
        "authors": [
            "Reid Swanson",
            "Stephanie Lukin",
            "Luke Eisenberg",
            "Thomas Chase Corcoran",
            "Marilyn A.Walker"
        ],
        "abstract": "The language used in online forums differs in many ways from that of traditional language resources such as news. One difference is the use and frequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the aim is to develop a theory of sarcasm in dialogue, or engineer automatic methods for reliably detecting sarcasm, a major challenge is simply the difficulty of getting enough reliably labelled examples. In this paper we describe our work on methods for achieving highly reliable sarcasm annotations from untrained annotators on Mechanical Turk. We explore the use of a number of common statistical reliability measures, such as Kappa, Karger's, Majority Class, and EM. We show that more sophisticated measures do not appear to yield better results for our data than simple measures such as assuming that the correct label is the one that a majority of Turkers apply.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01058",
        "title": "A Unified Query-based Generative Model for Question Generation and Question Answering",
        "authors": [
            "Linfeng Song",
            "Zhiguo Wang",
            "Wael Hamza"
        ],
        "abstract": "We propose a query-based generative model for solving both tasks of question generation (QG) and question an- swering (QA). The model follows the classic encoder- decoder framework. The encoder takes a passage and a query as input then performs query understanding by matching the query with the passage from multiple per- spectives. The decoder is an attention-based Long Short Term Memory (LSTM) model with copy and coverage mechanisms. In the QG task, a question is generated from the system given the passage and the target answer, whereas in the QA task, the answer is generated given the question and the passage. During the training stage, we leverage a policy-gradient reinforcement learning algorithm to overcome exposure bias, a major prob- lem resulted from sequence learning with cross-entropy loss. For the QG task, our experiments show higher per- formances than the state-of-the-art results. When used as additional training data, the automatically generated questions even improve the performance of a strong ex- tractive QA system. In addition, our model shows bet- ter performance than the state-of-the-art baselines of the generative QA task.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2018-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01121",
        "title": "Do latent tree learning models identify meaningful structure in sentences?",
        "authors": [
            "Adina Williams",
            "Andrew Drozdov",
            "Samuel R. Bowman"
        ],
        "abstract": "Recent work on the problem of latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2018-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01186",
        "title": "Learning Neural Word Salience Scores",
        "authors": [
            "Krasen Samardzhiev",
            "Andrew Gargett",
            "Danushka Bollegala"
        ],
        "abstract": "Measuring the salience of a word is an essential step in numerous NLP tasks. Heuristic approaches such as tfidf have been used so far to estimate the salience of words. We propose \\emph{Neural Word Salience} (NWS) scores, unlike heuristics, are learnt from a corpus. Specifically, we learn word salience scores such that, using pre-trained word embeddings as the input, can accurately predict the words that appear in a sentence, given the words that appear in the sentences preceding or succeeding that sentence. Experimental results on sentence similarity prediction show that the learnt word salience scores perform comparably or better than some of the state-of-the-art approaches for representing sentences on benchmark datasets for sentence similarity, while using only a fraction of the training and prediction times required by prior methods. Moreover, our NWS scores positively correlate with psycholinguistic measures such as concreteness, and imageability implying a close connection to the salience as perceived by humans.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01189",
        "title": "Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features",
        "authors": [
            "Fan Yang",
            "Arjun Mukherjee",
            "Eduard Dragut"
        ],
        "abstract": "Satirical news is considered to be entertainment, but it is potentially deceptive and harmful. Despite the embedded genre in the article, not everyone can recognize the satirical cues and therefore believe the news as true news. We observe that satirical cues are often reflected in certain paragraphs rather than the whole document. Existing works only consider document-level features to detect the satire, which could be limited. We consider paragraph-level linguistic features to unveil the satire by incorporating neural network and attention mechanism. We investigate the difference between paragraph-level features and document-level features, and analyze them on a large satirical news dataset. The evaluation shows that the proposed model detects satirical news effectively and reveals what features are important at which level.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01193",
        "title": "Compositional Approaches for Representing Relations Between Words: A Comparative Study",
        "authors": [
            "Huda Hakami",
            "Danushka Bollegala"
        ],
        "abstract": "Identifying the relations that exist between words (or entities) is important for various natural language processing tasks such as, relational search, noun-modifier classification and analogy detection. A popular approach to represent the relations between a pair of words is to extract the patterns in which the words co-occur with from a corpus, and assign each word-pair a vector of pattern frequencies. Despite the simplicity of this approach, it suffers from data sparseness, information scalability and linguistic creativity as the model is unable to handle previously unseen word pairs in a corpus. In contrast, a compositional approach for representing relations between words overcomes these issues by using the attributes of each individual word to indirectly compose a representation for the common relations that hold between the two words. This study aims to compare different operations for creating relation representations from word-level representations. We investigate the performance of the compositional methods by measuring the relational similarities using several benchmark datasets for word analogy. Moreover, we evaluate the different relation representations in a knowledge base completion task.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01199",
        "title": "Using $k$-way Co-occurrences for Learning Word Embeddings",
        "authors": [
            "Danushka Bollegala",
            "Yuichi Yoshida",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "Co-occurrences between two words provide useful insights into the semantics of those words. Consequently, numerous prior work on word embedding learning have used co-occurrences between two words as the training signal for learning word embeddings. However, in natural language texts it is common for multiple words to be related and co-occurring in the same context. We extend the notion of co-occurrences to cover $k(\\geq\\!\\!2)$-way co-occurrences among a set of $k$-words. Specifically, we prove a theoretical relationship between the joint probability of $k(\\geq\\!\\!2)$ words, and the sum of $\\ell_2$ norms of their embeddings. Next, we propose a learning objective motivated by our theoretical result that utilises $k$-way co-occurrences for learning word embeddings. Our experimental results show that the derived theoretical relationship does indeed hold empirically, and despite data sparsity, for some smaller $k$ values, $k$-way embeddings perform comparably or better than $2$-way embeddings in a range of tasks.\n    ",
        "submission_date": "2017-09-05T00:00:00",
        "last_modified_date": "2017-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01562",
        "title": "Optimizing for Measure of Performance in Max-Margin Parsing",
        "authors": [
            "Alexander Bauer",
            "Shinichi Nakajima",
            "Nico G\u00f6rnitz",
            "Klaus-Robert M\u00fcller"
        ],
        "abstract": "Many statistical learning problems in the area of natural language processing including sequence tagging, sequence segmentation and syntactic parsing has been successfully approached by means of structured prediction methods. An appealing property of the corresponding discriminative learning algorithms is their ability to integrate the loss function of interest directly into the optimization process, which potentially can increase the resulting performance accuracy. Here, we demonstrate on the example of constituency parsing how to optimize for F1-score in the max-margin framework of structural SVM. In particular, the optimization is with respect to the original (not binarized) trees.\n    ",
        "submission_date": "2017-09-05T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01572",
        "title": "Sequence Prediction with Neural Segmental Models",
        "authors": [
            "Hao Tang"
        ],
        "abstract": "Segments that span contiguous parts of inputs, such as phonemes in speech, named entities in sentences, actions in videos, occur frequently in sequence prediction problems. Segmental models, a class of models that explicitly hypothesizes segments, have allowed the exploration of rich segment features for sequence prediction. However, segmental models suffer from slow decoding, hampering the use of computationally expensive features.\n",
        "submission_date": "2017-09-05T00:00:00",
        "last_modified_date": "2018-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01634",
        "title": "The Voynich Manuscript is Written in Natural Language: The Pahlavi Hypothesis",
        "authors": [
            "J. Michael Herrmann"
        ],
        "abstract": "The late medieval Voynich Manuscript (VM) has resisted decryption and was considered a meaningless hoax or an unsolvable cipher. Here, we provide evidence that the VM is written in natural language by establishing a relation of the Voynich alphabet and the Iranian Pahlavi script. Many of the Voynich characters are upside-down versions of their Pahlavi counterparts, which may be an effect of different writing directions. Other Voynich letters can be explained as ligatures or departures from Pahlavi with the intent to cope with known problems due to the stupendous ambiguity of Pahlavi text. While a translation of the VM text is not attempted here, we can confirm the Voynich-Pahlavi relation at the character level by the transcription of many words from the VM illustrations and from parts of the main text. Many of the transcribed words can be identified as terms from Zoroastrian cosmology which is in line with the use of Pahlavi script in Zoroastrian communities from medieval times.\n    ",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01679",
        "title": "A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse",
        "authors": [
            "Sosuke Kobayashi",
            "Naoaki Okazaki",
            "Kentaro Inui"
        ],
        "abstract": "This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities.\n    ",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01713",
        "title": "Spoken English Intelligibility Remediation with PocketSphinx Alignment and Feature Extraction Improves Substantially over the State of the Art",
        "authors": [
            "Yuan Gao",
            "Brij Mohan Lal Srivastava",
            "James Salsman"
        ],
        "abstract": "We use automatic speech recognition to assess spoken English learner pronunciation based on the authentic intelligibility of the learners' spoken responses determined from support vector machine (SVM) classifier or deep learning neural network model predictions of transcription correctness. Using numeric features produced by PocketSphinx alignment mode and many recognition passes searching for the substitution and deletion of each expected phoneme and insertion of unexpected phonemes in sequence, the SVM models achieve 82 percent agreement with the accuracy of Amazon Mechanical Turk crowdworker transcriptions, up from 75 percent reported by multiple independent researchers. Using such features with SVM classifier probability prediction models can help computer-aided pronunciation teaching (CAPT) systems provide intelligibility remediation.\n    ",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2018-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01766",
        "title": "Information-Propogation-Enhanced Neural Machine Translation by Relation Model",
        "authors": [
            "Wen Zhang",
            "Jiawei Hu",
            "Yang Feng",
            "Qun Liu"
        ],
        "abstract": "Even though sequence-to-sequence neural machine translation (NMT) model have achieved state-of-art performance in the recent fewer years, but it is widely concerned that the recurrent neural network (RNN) units are very hard to capture the long-distance state information, which means RNN can hardly find the feature with long term dependency as the sequence becomes longer. Similarly, convolutional neural network (CNN) is introduced into NMT for speeding recently, however, CNN focus on capturing the local feature of the sequence; To relieve this issue, we incorporate a relation network into the standard encoder-decoder framework to enhance information-propogation in neural network, ensuring that the information of the source sentence can flow into the decoder adequately. Experiments show that proposed framework outperforms the statistical MT model and the state-of-art NMT model significantly on two data sets with different scales.\n    ",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2018-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01848",
        "title": "Depression and Self-Harm Risk Assessment in Online Forums",
        "authors": [
            "Andrew Yates",
            "Arman Cohan",
            "Nazli Goharian"
        ],
        "abstract": "Users suffering from mental health conditions often turn to online resources for support, including specialized online support communities or general communities such as Twitter and Reddit. In this work, we present a neural framework for supporting and studying users in both types of communities. We propose methods for identifying posts in support communities that may indicate a risk of self-harm, and demonstrate that our approach outperforms strong previously proposed methods for identifying such posts. Self-harm is closely related to depression, which makes identifying depressed users on general forums a crucial related task. We introduce a large-scale general forum dataset (\"RSDD\") consisting of users with self-reported depression diagnoses matched with control users. We show how our method can be applied to effectively identify depressed users from their use of language alone. We demonstrate that our method outperforms strong baselines on this general forum dataset.\n    ",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2017-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01887",
        "title": "Measuring the Similarity of Sentential Arguments in Dialog",
        "authors": [
            "Amita Misra",
            "Brian Ecker",
            "Marilyn A. Walker"
        ],
        "abstract": "When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. Debate websites produce curated summaries of arguments on such topics; these summaries typically consist of lists of sentences that represent frequently paraphrased propositions, or labels capturing the essence of one particular aspect of an argument, e.g. Morality or Second Amendment. We call these frequently paraphrased propositions ARGUMENT FACETS. Like these curated sites, our goal is to induce and identify argument facets across multiple conversations, and produce summaries. However, we aim to do this automatically. We frame the problem as consisting of two steps: we first extract sentences that express an argument from raw social media dialogs, and then rank the extracted arguments in terms of their similarity to one another. Sets of similar arguments are used to represent argument facets. We show here that we can predict ARGUMENT FACET SIMILARITY with a correlation averaging 0.63 compared to a human topline averaging 0.68 over three debate topics, easily beating several reasonable baselines.\n    ",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2017-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01888",
        "title": "Language Modeling by Clustering with Word Embeddings for Text Readability Assessment",
        "authors": [
            "Miriam Cha",
            "Youngjune Gwon",
            "H.T. Kung"
        ],
        "abstract": "We present a clustering-based language model using word embeddings for text readability prediction. Presumably, an Euclidean semantic space hypothesis holds true for word embeddings whose training is done by observing word co-occurrences. We argue that clustering with word embeddings in the metric space should yield feature representations in a higher semantic space appropriate for text regression. Also, by representing features in terms of histograms, our approach can naturally address documents of varying lengths. An empirical evaluation using the Common Core Standards corpus reveals that the features formed on our clustering-based language model significantly improve the previously known results for the same corpus in readability prediction. We also evaluate the task of sentence matching based on semantic relatedness using the Wiki-SimpleWiki corpus and find that our features lead to superior matching performance.\n    ",
        "submission_date": "2017-09-05T00:00:00",
        "last_modified_date": "2017-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01895",
        "title": "A Semi-Supervised Approach to Detecting Stance in Tweets",
        "authors": [
            "Amita Misra",
            "Brian Ecker",
            "Theodore Handleman",
            "Nicolas Hahn",
            "Marilyn Walker"
        ],
        "abstract": "Stance classification aims to identify, for a particular issue under discussion, whether the speaker or author of a conversational turn has Pro (Favor) or Con (Against) stance on the issue. Detecting stance in tweets is a new task proposed for SemEval-2016 Task6, involving predicting stance for a dataset of tweets on the topics of abortion, atheism, climate change, feminism and Hillary Clinton. Given the small size of the dataset, our team created our own topic-specific training corpus by developing a set of high precision hashtags for each topic that were used to query the twitter API, with the aim of developing a large training corpus without additional human labeling of tweets for stance. The hashtags selected for each topic were predicted to be stance-bearing on their own. Experimental results demonstrate good performance for our features for opinion-target pairs based on generalizing dependency features using sentiment lexicons.\n    ",
        "submission_date": "2017-09-03T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01915",
        "title": "Towards Neural Machine Translation with Latent Tree Attention",
        "authors": [
            "James Bradbury",
            "Richard Socher"
        ],
        "abstract": "Building models that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in natural language processing. We introduce such a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline.\n    ",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2017-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01950",
        "title": "\"Having 2 hours to write a paper is fun!\": Detecting Sarcasm in Numerical Portions of Text",
        "authors": [
            "Lakshya Kumar",
            "Arpan Somani",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Sarcasm occurring due to the presence of numerical portions in text has been quoted as an error made by automatic sarcasm detection approaches in the past. We present a first study in detecting sarcasm in numbers, as in the case of the sentence 'Love waking up at 4 am'. We analyze the challenges of the problem, and present Rule-based, Machine Learning and Deep Learning approaches to detect sarcasm in numerical portions of text. Our Deep Learning approach outperforms four past works for sarcasm detection and Rule-based and Machine learning approaches on a dataset of tweets, obtaining an F1-score of 0.93. This shows that special attention to text containing numbers may be useful to improve state-of-the-art in sarcasm detection.\n    ",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2017-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02184",
        "title": "Translating Terminological Expressions in Knowledge Bases with Neural Machine Translation",
        "authors": [
            "Mihael Arcan",
            "Daniel Torregrosa",
            "Paul Buitelaar"
        ],
        "abstract": "Our work presented in this paper focuses on the translation of terminological expressions represented in semantically structured resources, like ontologies or knowledge graphs. The challenge of translating ontology labels or terminological expressions documented in knowledge bases lies in the highly specific vocabulary and the lack of contextual information, which can guide a machine translation system to translate ambiguous words into the targeted domain. Due to these challenges, we evaluate the translation quality of domain-specific expressions in the medical and financial domain with statistical as well as with neural machine translation methods and experiment domain adaptation of the translation models with terminological expressions only. Furthermore, we perform experiments on the injection of external terminological expressions into the translation systems. Through these experiments, we observed a significant advantage in domain adaptation for the domain-specific resource in the medical and financial domain and the benefit of subword models over word-based neural machine translation models for terminology translation.\n    ",
        "submission_date": "2017-09-07T00:00:00",
        "last_modified_date": "2019-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02271",
        "title": "Leveraging Discourse Information Effectively for Authorship Attribution",
        "authors": [
            "Su Wang",
            "Elisa Ferracane",
            "Raymond J. Mooney"
        ],
        "abstract": "We explore techniques to maximize the effectiveness of discourse information in the task of authorship attribution. We present a novel method to embed discourse features in a Convolutional Neural Network text classifier, which achieves a state-of-the-art result by a substantial margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.\n    ",
        "submission_date": "2017-09-07T00:00:00",
        "last_modified_date": "2017-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02279",
        "title": "Cynical Selection of Language Model Training Data",
        "authors": [
            "Amittai Axelrod"
        ],
        "abstract": "The Moore-Lewis method of \"intelligent selection of language model training data\" is very effective, cheap, efficient... and also has structural problems. (1) The method defines relevance by playing language models trained on the in-domain and the out-of-domain (or data pool) corpora against each other. This powerful idea-- which we set out to preserve-- treats the two corpora as the opposing ends of a single spectrum. This lack of nuance does not allow for the two corpora to be very similar. In the extreme case where the come from the same distribution, all of the sentences have a Moore-Lewis score of zero, so there is no resulting ranking. (2) The selected sentences are not guaranteed to be able to model the in-domain data, nor to even cover the in-domain data. They are simply well-liked by the in-domain model; this is necessary, but not sufficient. (3) There is no way to tell what is the optimal number of sentences to select, short of picking various thresholds and building the systems.\n",
        "submission_date": "2017-09-07T00:00:00",
        "last_modified_date": "2017-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02349",
        "title": "A Deep Reinforcement Learning Chatbot",
        "authors": [
            "Iulian V. Serban",
            "Chinnadhurai Sankar",
            "Mathieu Germain",
            "Saizheng Zhang",
            "Zhouhan Lin",
            "Sandeep Subramanian",
            "Taesup Kim",
            "Michael Pieper",
            "Sarath Chandar",
            "Nan Rosemary Ke",
            "Sai Rajeshwar",
            "Alexandre de Brebisson",
            "Jose M. R. Sotelo",
            "Dendi Suhubdy",
            "Vincent Michalski",
            "Alexandre Nguyen",
            "Joelle Pineau",
            "Yoshua Bengio"
        ],
        "abstract": "We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.\n    ",
        "submission_date": "2017-09-07T00:00:00",
        "last_modified_date": "2017-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02755",
        "title": "Simple Recurrent Units for Highly Parallelizable Recurrence",
        "authors": [
            "Tao Lei",
            "Yu Zhang",
            "Sida I. Wang",
            "Hui Dai",
            "Yoav Artzi"
        ],
        "abstract": "Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.\n    ",
        "submission_date": "2017-09-08T00:00:00",
        "last_modified_date": "2018-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02783",
        "title": "A Statistical Comparison of Some Theories of NP Word Order",
        "authors": [
            "Richard Futrell",
            "Roger Levy",
            "Matthew Dryer"
        ],
        "abstract": "A frequent object of study in linguistic typology is the order of elements {demonstrative, adjective, numeral, noun} in the noun phrase. The goal is to predict the relative frequencies of these orders across languages. Here we use Poisson regression to statistically compare some prominent accounts of this variation. We compare feature systems derived from Cinque (2005) to feature systems given in Cysouw (2010) and Dryer (in prep). In this setting, we do not find clear reasons to prefer the model of Cinque (2005) or Dryer (in prep), but we find both of these models have substantially better fit to the typological data than the model from Cysouw (2010).\n    ",
        "submission_date": "2017-09-08T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02828",
        "title": "Globally Normalized Reader",
        "authors": [
            "Jonathan Raiman",
            "John Miller"
        ],
        "abstract": "Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answer's sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type. This method improves the performance of all models considered in this work and is of independent interest for a variety of NLP tasks.\n    ",
        "submission_date": "2017-09-08T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02842",
        "title": "Combining LSTM and Latent Topic Modeling for Mortality Prediction",
        "authors": [
            "Yohan Jo",
            "Lisa Lee",
            "Shruti Palaskar"
        ],
        "abstract": "There is a great need for technologies that can predict the mortality of patients in intensive care units with both high accuracy and accountability. We present joint end-to-end neural network architectures that combine long short-term memory (LSTM) and a latent topic model to simultaneously train a classifier for mortality prediction and learn latent topics indicative of mortality from textual clinical notes. For topic interpretability, the topic modeling layer has been carefully designed as a single-layer network with constraints inspired by LDA. Experiments on the MIMIC-III dataset show that our models significantly outperform prior models that are based on LDA topics in mortality prediction. However, we achieve limited success with our method for interpreting topics from the trained models by looking at the neural network weights.\n    ",
        "submission_date": "2017-09-08T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02843",
        "title": "CLaC at SemEval-2016 Task 11: Exploring linguistic and psycho-linguistic Features for Complex Word Identification",
        "authors": [
            "Elnaz Davoodi",
            "Leila Kosseim"
        ],
        "abstract": "This paper describes the system deployed by the CLaC-EDLK team to the \"SemEval 2016, Complex Word Identification task\". The goal of the task is to identify if a given word in a given context is \"simple\" or \"complex\". Our system relies on linguistic features and cognitive complexity. We used several supervised models, however the Random Forest model outperformed the others. Overall our best configuration achieved a G-score of 68.8% in the task, ranking our system 21 out of 45.\n    ",
        "submission_date": "2017-09-08T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02911",
        "title": "Semi-Supervised Instance Population of an Ontology using Word Vector Embeddings",
        "authors": [
            "Vindula Jayawardana",
            "Dimuthu Lakmal",
            "Nisansa de Silva",
            "Amal Shehan Perera",
            "Keet Sugathadasa",
            "Buddhi Ayesha",
            "Madhavi Perera"
        ],
        "abstract": "In many modern day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the field of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study, we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models.\n    ",
        "submission_date": "2017-09-09T00:00:00",
        "last_modified_date": "2017-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03010",
        "title": "Steering Output Style and Topic in Neural Response Generation",
        "authors": [
            "Di Wang",
            "Nebojsa Jojic",
            "Chris Brockett",
            "Eric Nyberg"
        ],
        "abstract": "We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to restrict style and topic without degrading output quality in conversational tasks.\n    ",
        "submission_date": "2017-09-09T00:00:00",
        "last_modified_date": "2017-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03036",
        "title": "Abductive Matching in Question Answering",
        "authors": [
            "Kedar Dhamdhere",
            "Kevin S. McCurley",
            "Mukund Sundararajan",
            "Ankur Taly"
        ],
        "abstract": "We study question-answering over semi-structured data. We introduce a new way to apply the technique of semantic parsing by applying machine learning only to provide annotations that the system infers to be missing; all the other parsing logic is in the form of manually authored rules. In effect, the machine learning is used to provide non-syntactic matches, a step that is ill-suited to manual rules. The advantage of this approach is in its debuggability and in its transparency to the end-user. We demonstrate the effectiveness of the approach by achieving state-of-the-art performance of 40.42% accuracy on a standard benchmark dataset over tables from Wikipedia.\n    ",
        "submission_date": "2017-09-10T00:00:00",
        "last_modified_date": "2017-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03064",
        "title": "AppTechMiner: Mining Applications and Techniques from Scientific Articles",
        "authors": [
            "Mayank Singh",
            "Soham Dan",
            "Sanyam Agarwal",
            "Pawan Goyal",
            "Animesh Mukherjee"
        ],
        "abstract": "This paper presents AppTechMiner, a rule-based information extraction framework that automatically constructs a knowledge base of all application areas and problem solving techniques. Techniques include tools, methods, datasets or evaluation metrics. We also categorize individual research articles based on their application areas and the techniques proposed/improved in the article. Our system achieves high average precision (~82%) and recall (~84%) in knowledge base creation. It also performs well in application and technique assignment to an individual article (average accuracy ~66%). In the end, we further present two use cases presenting a trivial information retrieval system and an extensive temporal analysis of the usage of techniques and application areas. At present, we demonstrate the framework for the domain of computational linguistics but this can be easily generalized to any other field of research.\n    ",
        "submission_date": "2017-09-10T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03167",
        "title": "Debbie, the Debate Bot of the Future",
        "authors": [
            "Geetanjali Rakshit",
            "Kevin K. Bowden",
            "Lena Reed",
            "Amita Misra",
            "Marilyn Walker"
        ],
        "abstract": "Chatbots are a rapidly expanding application of dialogue systems with companies switching to bot services for customer support, and new applications for users interested in casual conversation. One style of casual conversation is argument, many people love nothing more than a good argument. Moreover, there are a number of existing corpora of argumentative dialogues, annotated for agreement and disagreement, stance, sarcasm and argument quality. This paper introduces Debbie, a novel arguing bot, that selects arguments from conversational corpora, and aims to use them appropriately in context. We present an initial working prototype of Debbie, with some preliminary evaluation and describe future work.\n    ",
        "submission_date": "2017-09-10T00:00:00",
        "last_modified_date": "2017-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03190",
        "title": "Data-Driven Dialogue Systems for Social Agents",
        "authors": [
            "Kevin K. Bowden",
            "Shereen Oraby",
            "Amita Misra",
            "Jiaqi Wu",
            "Stephanie Lukin"
        ],
        "abstract": "In order to build dialogue systems to tackle the ambitious task of holding social conversations, we argue that we need a data driven approach that includes insight into human conversational chit chat, and which incorporates different natural language processing modules. Our strategy is to analyze and index large corpora of social media data, including Twitter conversations, online debates, dialogues between friends, and blog posts, and then to couple this data retrieval with modules that perform tasks such as sentiment and style analysis, topic modeling, and summarization. We aim for personal assistants that can learn more nuanced human language, and to grow from task-oriented agents to more personable social bots.\n    ",
        "submission_date": "2017-09-10T00:00:00",
        "last_modified_date": "2017-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03544",
        "title": "KnowNER: Incremental Multilingual Knowledge in Named Entity Recognition",
        "authors": [
            "Dominic Seyler",
            "Tatiana Dembelova",
            "Luciano Del Corro",
            "Johannes Hoffart",
            "Gerhard Weikum"
        ],
        "abstract": "KnowNER is a multilingual Named Entity Recognition (NER) system that leverages different degrees of external knowledge. A novel modular framework divides the knowledge into four categories according to the depth of knowledge they convey. Each category consists of a set of features automatically generated from different information sources (such as a knowledge-base, a list of names or document-specific semantic annotations) and is used to train a conditional random field (CRF). Since those information sources are usually multilingual, KnowNER can be easily trained for a wide range of languages. In this paper, we show that the incorporation of deeper knowledge systematically boosts accuracy and compare KnowNER with state-of-the-art NER approaches across three languages (i.e., English, German and Spanish) performing amongst state-of-the art systems in all of them.\n    ",
        "submission_date": "2017-09-11T00:00:00",
        "last_modified_date": "2017-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03637",
        "title": "Capturing Long-range Contextual Dependencies with Memory-enhanced Conditional Random Fields",
        "authors": [
            "Fei Liu",
            "Timothy Baldwin",
            "Trevor Cohn"
        ],
        "abstract": "Despite successful applications across a broad range of NLP tasks, conditional random fields (\"CRFs\"), in particular the linear-chain variant, are only able to model local features. While this has important benefits in terms of inference tractability, it limits the ability of the model to capture long-range dependencies between items. Attempts to extend CRFs to capture long-range dependencies have largely come at the cost of computational complexity and approximate inference. In this work, we propose an extension to CRFs by integrating external memory, taking inspiration from memory networks, thereby allowing CRFs to incorporate information far beyond neighbouring steps. Experiments across two tasks show substantial improvements over strong CRF and LSTM baselines.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03665",
        "title": "Small-footprint Keyword Spotting Using Deep Neural Network and Connectionist Temporal Classifier",
        "authors": [
            "Zhiming Wang",
            "Xiaolong Li",
            "Jun Zhou"
        ],
        "abstract": "Mainly for the sake of solving the lack of keyword-specific data, we propose one Keyword Spotting (KWS) system using Deep Neural Network (DNN) and Connectionist Temporal Classifier (CTC) on power-constrained small-footprint mobile devices, taking full advantage of general corpus from continuous speech recognition which is of great amount. DNN is to directly predict the posterior of phoneme units of any personally customized key-phrase, and CTC to produce a confidence score of the given phoneme sequence as responsive decision-making mechanism. The CTC-KWS has competitive performance in comparison with purely DNN based keyword specific KWS, but not increasing any computational complexity.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03756",
        "title": "Cross-lingual Word Segmentation and Morpheme Segmentation as Sequence Labelling",
        "authors": [
            "Yan Shao"
        ],
        "abstract": "This paper presents our segmentation system developed for the MLP 2017 shared tasks on cross-lingual word segmentation and morpheme segmentation. We model both word and morpheme segmentation as character-level sequence labelling tasks. The prevalent bidirectional recurrent neural network with conditional random fields as the output interface is adapted as the baseline system, which is further improved via ensemble decoding. Our universal system is applied to and extensively evaluated on all the official data sets without any language-specific adjustment. The official evaluation results indicate that the proposed model achieves outstanding accuracies both for word and morpheme segmentation on all the languages in various types when compared to the other participating systems.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03759",
        "title": "Language Models of Spoken Dutch",
        "authors": [
            "Lyan Verwimp",
            "Joris Pelemans",
            "Marieke Lycke",
            "Hugo Van hamme",
            "Patrick Wambacq"
        ],
        "abstract": "In Flanders, all TV shows are subtitled. However, the process of subtitling is a very time-consuming one and can be sped up by providing the output of a speech recognizer run on the audio of the TV show, prior to the subtitling. Naturally, this speech recognition will perform much better if the employed language model is adapted to the register and the topic of the program. We present several language models trained on subtitles of television shows provided by the Flemish public-service broadcaster VRT. This data was gathered in the context of the project STON which has as purpose to facilitate the process of subtitling TV shows. One model is trained on all available data (46M word tokens), but we also trained models on a specific type of TV show or domain/topic. Language models of spoken language are quite rare due to the lack of training data. The size of this corpus is relatively large for a corpus of spoken language (compare with e.g. CGN which has 9M words), but still rather small for a language model. Thus, in practice it is advised to interpolate these models with a large background language model trained on written language. The models can be freely downloaded on ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03814",
        "title": "SYSTRAN Purely Neural MT Engines for WMT2017",
        "authors": [
            "Yongchao Deng",
            "Jungi Kim",
            "Guillaume Klein",
            "Catherine Kobus",
            "Natalia Segal",
            "Christophe Servan",
            "Bo Wang",
            "Dakun Zhang",
            "Josep Crego",
            "Jean Senellart"
        ],
        "abstract": "This paper describes SYSTRAN's systems submitted to the WMT 2017 shared news translation task for English-German, in both translation directions. Our systems are built using OpenNMT, an open-source neural machine translation system, implementing sequence-to-sequence models with LSTM encoder/decoders and attention. We experimented using monolingual data automatically back-translated. Our resulting models are further hyper-specialised with an adaptation technique that finely tunes models according to the evaluation test sentences.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03815",
        "title": "OpenNMT: Open-source Toolkit for Neural Machine Translation",
        "authors": [
            "Guillaume Klein",
            "Yoon Kim",
            "Yuntian Deng",
            "Josep Crego",
            "Jean Senellart",
            "Alexander M. Rush"
        ],
        "abstract": "We introduce an open-source toolkit for neural machine translation (NMT) to support research into model architectures, feature representations, and source modalities, while maintaining competitive performance, modularity and reasonable training requirements.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03856",
        "title": "StarSpace: Embed All The Things!",
        "authors": [
            "Ledell Wu",
            "Adam Fisch",
            "Sumit Chopra",
            "Keith Adams",
            "Antoine Bordes",
            "Jason Weston"
        ],
        "abstract": "We present StarSpace, a general-purpose neural embedding model that can solve a wide variety of problems: labeling tasks such as text classification, ranking tasks such as information retrieval/web search, collaborative filtering-based or content-based recommendation, embedding of multi-relational graphs, and learning word, sentence or document level embeddings. In each case the model works by embedding those entities comprised of discrete features and comparing them against each other -- learning similarities dependent on the task. Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03925",
        "title": "Human Associations Help to Detect Conventionalized Multiword Expressions",
        "authors": [
            "Natalia Loukachevitch",
            "Anastasia Gerasimova"
        ],
        "abstract": "In this paper we show that if we want to obtain human evidence about conventionalization of some phrases, we should ask native speakers about associations they have to a given phrase and its component words. We have shown that if component words of a phrase have each other as frequent associations, then this phrase can be considered as conventionalized. Another type of conventionalized phrases can be revealed using two factors: low entropy of phrase associations and low intersection of component word and phrase associations. The association experiments were performed for the Russian language.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03933",
        "title": "Hash Embeddings for Efficient Word Representations",
        "authors": [
            "Dan Svenstrup",
            "Jonas Meinertz Hansen",
            "Ole Winther"
        ],
        "abstract": "We present hash embeddings, an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by $k$ $d$-dimensional embeddings vectors and one $k$ dimensional weight vector. The final $d$ dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of $B$ embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions of tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03968",
        "title": "Affective Neural Response Generation",
        "authors": [
            "Nabiha Asghar",
            "Pascal Poupart",
            "Jesse Hoey",
            "Xin Jiang",
            "Lili Mou"
        ],
        "abstract": "Existing neural conversational models process natural language primarily on a lexico-syntactic level, thereby ignoring one of the most crucial components of human-to-human dialogue: its affective content. We take a step in this direction by proposing three novel ways to incorporate affective/emotional aspects into long short term memory (LSTM) encoder-decoder neural conversation models: (1) affective word embeddings, which are cognitively engineered, (2) affect-based objective functions that augment the standard cross-entropy loss, and (3) affectively diverse beam search for decoding. Experiments show that these techniques improve the open-domain conversational prowess of encoder-decoder networks by enabling them to produce emotionally rich responses that are more interesting and natural.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03980",
        "title": "Refining Source Representations with Relation Networks for Neural Machine Translation",
        "authors": [
            "Wen Zhang",
            "Jiawei Hu",
            "Yang Feng",
            "Qun Liu"
        ],
        "abstract": "Although neural machine translation (NMT) with the encoder-decoder framework has achieved great success in recent times, it still suffers from some drawbacks: RNNs tend to forget old information which is often useful and the encoder only operates through words without considering word relationship. To solve these problems, we introduce a relation networks (RN) into NMT to refine the encoding representations of the source. In our method, the RN first augments the representation of each source word with its neighbors and reasons all the possible pairwise relations between them. Then the source representations and all the relations are fed to the attention module and the decoder together, keeping the main encoder-decoder architecture unchanged. Experiments on two Chinese-to-English data sets in different scales both show that our method can outperform the competitive baselines significantly.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2018-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04005",
        "title": "Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs",
        "authors": [
            "Rui Zhang",
            "Honglak Lee",
            "Lazaros Polymenakos",
            "Dragomir Radev"
        ],
        "abstract": "In this paper, we study the problem of addressee and response selection in multi-party conversations. Understanding multi-party conversations is challenging because of complex speaker interactions: multiple speakers exchange messages with each other, playing different roles (sender, addressee, observer), and these roles vary across turns. To tackle this challenge, we propose the Speaker Interaction Recurrent Neural Network (SI-RNN). Whereas the previous state-of-the-art system updated speaker embeddings only for the sender, SI-RNN uses a novel dialog encoder to update speaker embeddings in a role-sensitive way. Additionally, unlike the previous work that selected the addressee and response separately, SI-RNN selects them jointly by viewing the task as a sequence prediction problem. Experimental results show that SI-RNN significantly improves the accuracy of addressee and response selection, particularly in complex conversations with many speakers and responses to distant messages many turns in the past.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04109",
        "title": "Empower Sequence Labeling with Task-Aware Neural Language Model",
        "authors": [
            "Liyuan Liu",
            "Jingbo Shang",
            "Frank F. Xu",
            "Xiang Ren",
            "Huan Gui",
            "Jian Peng",
            "Jiawei Han"
        ],
        "abstract": "Linguistic sequence labeling is a general modeling approach that encompasses a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks (NNs) make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a novel neural framework to extract abundant knowledge hidden in raw texts to empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F1 score of 91.71$\\pm$0.10 without using any extra annotation.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04219",
        "title": "Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets",
        "authors": [
            "Jeremy Barnes",
            "Roman Klinger",
            "Sabine Schulte im Walde"
        ],
        "abstract": "There has been a good amount of progress in sentiment analysis over the past 10 years, including the proposal of new methods and the creation of benchmark datasets. In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task. Accordingly, it is hard to understand how well a certain model generalizes across different tasks and datasets. In this paper, we contribute to this situation by comparing several models on six different benchmarks, which belong to different domains and additionally have different levels of granularity (binary, 3-class, 4-class and 5-class). We show that Bi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are particularly good at fine-grained sentiment tasks (i. e., with more than two classes). Incorporating sentiment information into word embeddings during training gives good results for datasets that are lexically similar to the training data. With our experiments, we contribute to a better understanding of the performance of different model architectures on different data sets. Consequently, we detect novel state-of-the-art results on the SenTube datasets.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04250",
        "title": "Dialogue Act Sequence Labeling using Hierarchical encoder with CRF",
        "authors": [
            "Harshit Kumar",
            "Arvind Agarwal",
            "Riddhiman Dasgupta",
            "Sachindra Joshi",
            "Arun Kumar"
        ],
        "abstract": "Dialogue Act recognition associate dialogue acts (i.e., semantic labels) to utterances in a conversation. The problem of associating semantic labels to utterances can be treated as a sequence labeling problem. In this work, we build a hierarchical recurrent neural network using bidirectional LSTM as a base unit and the conditional random field (CRF) as the top layer to classify each utterance into its corresponding dialogue act. The hierarchical network learns representations at multiple levels, i.e., word level, utterance level, and conversation level. The conversation level representations are input to the CRF layer, which takes into account not only all previous utterances but also their dialogue acts, thus modeling the dependency among both, labels and utterances, an important consideration of natural dialogue. We validate our approach on two different benchmark data sets, Switchboard and Meeting Recorder Dialogue Act, and show performance improvement over the state-of-the-art methods by $2.2\\%$ and $4.1\\%$ absolute points, respectively. It is worth noting that the inter-annotator agreement on Switchboard data set is $84\\%$, and our method is able to achieve the accuracy of about $79\\%$ despite being trained on the noisy data.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04264",
        "title": "Flexible End-to-End Dialogue System for Knowledge Grounded Conversation",
        "authors": [
            "Wenya Zhu",
            "Kaixiang Mo",
            "Yu Zhang",
            "Zhangbin Zhu",
            "Xuezheng Peng",
            "Qiang Yang"
        ],
        "abstract": "In knowledge grounded conversation, domain knowledge plays an important role in a special domain such as Music. The response of knowledge grounded conversation might contain multiple answer entities or no entity at all. Although existing generative question answering (QA) systems can be applied to knowledge grounded conversation, they either have at most one entity in a response or cannot deal with out-of-vocabulary entities. We propose a fully data-driven generative dialogue system GenDS that is capable of generating responses based on input message and related knowledge base (KB). To generate arbitrary number of answer entities even when these entities never appear in the training set, we design a dynamic knowledge enquirer which selects different answer entities at different positions in a single response, according to different local context. It does not rely on the representations of entities, enabling our model deal with out-of-vocabulary entities. We collect a human-human conversation data (ConversMusic) with knowledge annotations. The proposed method is evaluated on CoversMusic and a public question answering dataset. Our proposed GenDS system outperforms baseline methods significantly in terms of the BLEU, entity accuracy, entity recall and human evaluation. Moreover,the experiments also demonstrate that GenDS works better even on small datasets.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04348",
        "title": "Natural Language Inference over Interaction Space",
        "authors": [
            "Yichen Gong",
            "Heng Luo",
            "Jian Zhang"
        ],
        "abstract": "Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2018-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04359",
        "title": "Linguistic Features of Genre and Method Variation in Translation: A Computational Perspective",
        "authors": [
            "Ekaterina Lapshninova-Koltunski",
            "Marcos Zampieri"
        ],
        "abstract": "In this paper we describe the use of text classification methods to investigate genre and method variation in an English - German translation corpus. For this purpose we use linguistically motivated features representing texts using a combination of part-of-speech tags arranged in bigrams, trigrams, and 4-grams. The classification method used in this paper is a Bayesian classifier with Laplace smoothing. We use the output of the classifiers to carry out an extensive feature analysis on the main difference between genres and methods of translation.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04409",
        "title": "A Review of Evaluation Techniques for Social Dialogue Systems",
        "authors": [
            "Amanda Cercas Curry",
            "Helen Hastie",
            "Verena Rieser"
        ],
        "abstract": "In contrast with goal-oriented dialogue, social dialogue has no clear measure of task success. Consequently, evaluation of these systems is notoriously hard. In this paper, we review current evaluation methods, focusing on automatic metrics. We conclude that turn-based metrics often ignore the context and do not account for the fact that several replies are valid, while end-of-dialogue rewards are mainly hand-crafted. Both lack grounding in human perceptions.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04482",
        "title": "Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems",
        "authors": [
            "Yonatan Belinkov",
            "James Glass"
        ],
        "abstract": "Neural models have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04491",
        "title": "Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis",
        "authors": [
            "\u0141ukasz Augustyniak",
            "Krzysztof Rajda",
            "Tomasz Kajdanowicz"
        ],
        "abstract": "This paper fills a gap in aspect-based sentiment analysis and aims to present a new method for preparing and analysing texts concerning opinion and generating user-friendly descriptive reports in natural language. We present a comprehensive set of techniques derived from Rhetorical Structure Theory and sentiment analysis to extract aspects from textual opinions and then build an abstractive summary of a set of opinions. Moreover, we propose aspect-aspect graphs to evaluate the importance of aspects and to filter out unimportant ones from the summary. Additionally, the paper presents a prototype solution of data flow with interesting and valuable results. The proposed method's results proved the high accuracy of aspect detection when applied to the gold standard dataset.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04558",
        "title": "Using NLU in Context for Question Answering: Improving on Facebook's bAbI Tasks",
        "authors": [
            "John S. Ball"
        ],
        "abstract": "For the next step in human to machine interaction, Artificial Intelligence (AI) should interact predominantly using natural language because, if it worked, it would be the fastest way to communicate. Facebook's toy tasks (bAbI) provide a useful benchmark to compare implementations for conversational AI. While the published experiments so far have been based on exploiting the distributional hypothesis with machine learning, our model exploits natural language understanding (NLU) with the decomposition of language based on Role and Reference Grammar (RRG) and the brain-based Patom theory. Our combinatorial system for conversational AI based on linguistics has many advantages: passing bAbI task tests without parsing or statistics while increasing scalability. Our model validates both the training and test data to find 'garbage' input and output (GIGO). It is not rules-based, nor does it use parts of speech, but instead relies on meaning. While Deep Learning is difficult to debug and fix, every step in our model can be understood and changed like any non-statistical computer program. Deep Learning's lack of explicable reasoning has raised opposition to AI, partly due to fear of the unknown. To support the goals of AI, we propose extended tasks to use human-level statements with tense, aspect and voice, and embedded clauses with junctures: and answers to be natural language generation (NLG) instead of keywords. While machine learning permits invalid training data to produce incorrect test responses, our system cannot because the context tracking would need to be intentionally broken. We believe no existing learning systems can currently solve these extended natural language tests. There appears to be a knowledge gap between NLP researchers and linguists, but ongoing competitive results such as these promise to narrow that gap.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04682",
        "title": "Towards an Arabic-English Machine-Translation Based on Semantic Web",
        "authors": [
            "Neama Abdulaziz Dahan",
            "Fadl Mutaher Ba-Alwi",
            "Ibrahim Ahmed Al-Baltah",
            "Ghaleb H. Al-gapheri"
        ],
        "abstract": "Communication tools make the world like a small village and as a consequence people can contact with others who are from different societies or who speak different languages. This communication cannot happen effectively without Machine Translation because they can be found anytime and everywhere. There are a number of studies that have developed Machine Translation for the English language with so many other languages except the Arabic it has not been considered yet. Therefore we aim to highlight a roadmap for our proposed translation machine to provide an enhanced Arabic English translation based on Semantic.\n    ",
        "submission_date": "2017-09-14T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04685",
        "title": "Machine-Translation History and Evolution: Survey for Arabic-English Translations",
        "authors": [
            "Nabeel T. Alsohybe",
            "Neama Abdulaziz Dahan",
            "Fadl Mutaher Ba-Alwi"
        ],
        "abstract": "As a result of the rapid changes in information and communication technology (ICT), the world has become a small village where people from all over the world connect with each other in dialogue and communication via the Internet. Also, communications have become a daily routine activity due to the new globalization where companies and even universities become global residing cross countries borders. As a result, translation becomes a needed activity in this connected world. ICT made it possible to have a student in one country take a course or even a degree from a different country anytime anywhere easily. The resulted communication still needs a language as a means that helps the receiver understands the contents of the sent message. People need an automated translation application because human translators are hard to find all the times, and the human translations are very expensive comparing to the translations automated process. Several types of research describe the electronic process of the Machine-Translation. In this paper, the authors are going to study some of these previous researches, and they will explore some of the needed tools for the Machine-Translation. This research is going to contribute to the Machine-Translation area by helping future researchers to have a summary for the Machine-Translation groups of research and to let lights on the importance of the translation mechanism.\n    ",
        "submission_date": "2017-09-14T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04696",
        "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding",
        "authors": [
            "Tao Shen",
            "Tianyi Zhou",
            "Guodong Long",
            "Jing Jiang",
            "Shirui Pan",
            "Chengqi Zhang"
        ],
        "abstract": "Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely used on NLP tasks to capture the long-term and local dependencies, respectively. Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation, significantly less training time, and flexibility in modeling dependencies. We propose a novel attention mechanism in which the attention between elements from input sequence(s) is directional and multi-dimensional (i.e., feature-wise). A light-weight neural net, \"Directional Self-Attention Network (DiSAN)\", is then proposed to learn sentence embedding, based solely on the proposed attention without any RNN/CNN structure. DiSAN is only composed of a directional self-attention with temporal order encoded, followed by a multi-dimensional attention that compresses the sequence into a vector representation. Despite its simple form, DiSAN outperforms complicated RNN models on both prediction quality and time efficiency. It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02% on the Stanford Natural Language Inference (SNLI) dataset, and shows state-of-the-art test accuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language inference (MultiNLI), Sentences Involving Compositional Knowledge (SICK), Customer Review, MPQA, TREC question-type classification and Subjectivity (SUBJ) datasets.\n    ",
        "submission_date": "2017-09-14T00:00:00",
        "last_modified_date": "2017-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04820",
        "title": "Synapse at CAp 2017 NER challenge: Fasttext CRF",
        "authors": [
            "Damien Sileo",
            "Camille Pradel",
            "Philippe Muller",
            "Tim Van de Cruys"
        ],
        "abstract": "We present our system for the CAp 2017 NER challenge which is about named entity recognition on French tweets. Our system leverages unsupervised learning on a larger dataset of French tweets to learn features feeding a CRF model. It was ranked first without using any gazetteer or structured external data, with an F-measure of 58.89\\%. To the best of our knowledge, it is the first system to use fasttext embeddings (which include subword representations) and an embedding-based sentence representation for NER.\n    ",
        "submission_date": "2017-09-14T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04849",
        "title": "Self-Attentive Residual Decoder for Neural Machine Translation",
        "authors": [
            "Lesly Miculicich Werlen",
            "Nikolaos Pappas",
            "Dhananjay Ram",
            "Andrei Popescu-Belis"
        ],
        "abstract": "Neural sequence-to-sequence networks with attention have achieved remarkable performance for machine translation. One of the reasons for their effectiveness is their ability to capture relevant source-side contextual information at each time-step prediction through an attention mechanism. However, the target-side context is solely based on the sequence model which, in practice, is prone to a recency bias and lacks the ability to capture effectively non-sequential dependencies among words. To address this limitation, we propose a target-side-attentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. The residual learning facilitates the flow of information from the distant past and is able to emphasize any of the previously translated words, hence it gains access to a wider context. The proposed model outperforms a neural MT baseline as well as a memory and self-attention network on three language pairs. The analysis of the attention learned by the decoder confirms that it emphasizes a wider context, and that it captures syntactic-like structures.\n    ",
        "submission_date": "2017-09-14T00:00:00",
        "last_modified_date": "2018-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04857",
        "title": "A New Semantic Theory of Natural Language",
        "authors": [
            "Kun Xing"
        ],
        "abstract": "Formal Semantics and Distributional Semantics are two important semantic frameworks in Natural Language Processing (NLP). Cognitive Semantics belongs to the movement of Cognitive Linguistics, which is based on contemporary cognitive science. Each framework could deal with some meaning phenomena, but none of them fulfills all requirements proposed by applications. A unified semantic theory characterizing all important language phenomena has both theoretical and practical significance; however, although many attempts have been made in recent years, no existing theory has achieved this goal yet.\n",
        "submission_date": "2017-09-10T00:00:00",
        "last_modified_date": "2017-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04969",
        "title": "Cross-Platform Emoji Interpretation: Analysis, a Solution, and Applications",
        "authors": [
            "Fred Morstatter",
            "Kai Shu",
            "Suhang Wang",
            "Huan Liu"
        ],
        "abstract": "Most social media platforms are largely based on text, and users often write posts to describe where they are, what they are seeing, and how they are feeling. Because written text lacks the emotional cues of spoken and face-to-face dialogue, ambiguities are common in written language. This problem is exacerbated in the short, informal nature of many social media posts. To bypass this issue, a suite of special characters called \"emojis,\" which are small pictograms, are embedded within the text. Many emojis are small depictions of facial expressions designed to help disambiguate the emotional meaning of the text. However, a new ambiguity arises in the way that emojis are rendered. Every platform (Windows, Mac, and Android, to name a few) renders emojis according to their own style. In fact, it has been shown that some emojis can be rendered so differently that they look \"happy\" on some platforms, and \"sad\" on others. In this work, we use real-world data to verify the existence of this problem. We verify that the usage of the same emoji can be significantly different across platforms, with some emojis exhibiting different sentiment polarities on different platforms. We propose a solution to identify the intended emoji based on the platform-specific nature of the emoji used by the author of a social media post. We apply our solution to sentiment analysis, a task that can benefit from the emoji calibration technique we use in this work. We conduct experiments to evaluate the effectiveness of the mapping in this task.\n    ",
        "submission_date": "2017-09-14T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05014",
        "title": "WOAH: Preliminaries to Zero-shot Ontology Learning for Conversational Agents",
        "authors": [
            "Gonzalo Estr\u00e1n Buyo"
        ],
        "abstract": "The present paper presents the Weighted Ontology Approximation Heuristic (WOAH), a novel zero-shot approach to ontology estimation for conversational agents development environments. This methodology extracts verbs and nouns separately from data by distilling the dependencies obtained and applying similarity and sparsity metrics to generate an ontology estimation configurable in terms of the level of generalization.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05074",
        "title": "A Deep Generative Framework for Paraphrase Generation",
        "authors": [
            "Ankush Gupta",
            "Arvind Agarwal",
            "Prawaan Singh",
            "Piyush Rai"
        ],
        "abstract": "Paraphrase generation is an important problem in NLP, especially in question answering, information retrieval, information extraction, conversation systems, to name a few. In this paper, we address the problem of generating paraphrases automatically. Our proposed method is based on a combination of deep generative models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases, given an input sentence. Traditional VAEs when combined with recurrent neural networks can generate free text but they are not suitable for paraphrase generation for a given sentence. We address this problem by conditioning the both, encoder and decoder sides of VAE, on the original sentence, so that it can generate the given sentence's paraphrases. Unlike most existing models, our model is simple, modular and can generate multiple paraphrases, for a given sentence. Quantitative evaluation of the proposed method on a benchmark paraphrase dataset demonstrates its efficacy, and its performance improvement over the state-of-the-art methods by a significant margin, whereas qualitative human evaluation indicate that the generated paraphrases are well-formed, grammatically correct, and are relevant to the input sentence. Furthermore, we evaluate our method on a newly released question paraphrase dataset, and establish a new baseline for future research.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05094",
        "title": "Unsupervised Aspect Term Extraction with B-LSTM & CRF using Automatically Labelled Datasets",
        "authors": [
            "Athanasios Giannakopoulos",
            "Claudiu Musat",
            "Andreea Hossmann",
            "Michael Baeriswyl"
        ],
        "abstract": "Aspect Term Extraction (ATE) identifies opinionated aspect terms in texts and is one of the tasks in the SemEval Aspect Based Sentiment Analysis (ABSA) contest. The small amount of available datasets for supervised ATE and the costly human annotation for aspect term labelling give rise to the need for unsupervised ATE. In this paper, we introduce an architecture that achieves top-ranking performance for supervised ATE. Moreover, it can be used efficiently as feature extractor and classifier for unsupervised ATE. Our second contribution is a method to automatically construct datasets for ATE. We train a classifier on our automatically labelled datasets and evaluate it on the human annotated SemEval ABSA test sets. Compared to a strong rule-based baseline, we obtain a dramatically higher F-score and attain precision values above 80%. Our unsupervised method beats the supervised ABSA baseline from SemEval, while preserving high precision scores.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05227",
        "title": "Transcribing Against Time",
        "authors": [
            "Matthias Sperber",
            "Graham Neubig",
            "Jan Niehues",
            "Satoshi Nakamura",
            "Alex Waibel"
        ],
        "abstract": "We investigate the problem of manually correcting errors from an automatic speech transcript in a cost-sensitive fashion. This is done by specifying a fixed time budget, and then automatically choosing location and size of segments for correction such that the number of corrected errors is maximized. The core components, as suggested by previous research [1], are a utility model that estimates the number of errors in a particular segment, and a cost model that estimates annotation effort for the segment. In this work we propose a dynamic updating framework that allows for the training of cost models during the ongoing transcription process. This removes the need for transcriber enrollment prior to the actual transcription, and improves correction efficiency by allowing highly transcriber-adaptive cost modeling. We first confirm and analyze the improvements afforded by this method in a simulated study. We then conduct a realistic user study, observing efficiency improvements of 15% relative on average, and 42% for the participants who deviated most strongly from our initial, transcriber-agnostic cost model. Moreover, we find that our updating framework can capture dynamically changing factors, such as transcriber fatigue and topic familiarity, which we observe to have a large influence on the transcriber's working behavior.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05295",
        "title": "And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue",
        "authors": [
            "Shereen Oraby",
            "Lena Reed",
            "Ryan Compton",
            "Ellen Riloff",
            "Marilyn Walker",
            "Steve Whittaker"
        ],
        "abstract": "We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of \"factual\" and \"feeling\" debate forum posts, we extract patterns that are highly correlated with factual and emotional arguments, and then apply a bootstrapping methodology to find new patterns in a larger pool of unannotated forum posts. This process automatically produces a large set of patterns representing linguistic expressions that are highly correlated with factual and emotional language. Finally, we analyze the most discriminating patterns to better understand the defining characteristics of factual and emotional arguments.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05305",
        "title": "Are you serious?: Rhetorical Questions and Sarcasm in Social Media Dialog",
        "authors": [
            "Shereen Oraby",
            "Vrindavan Harrison",
            "Amita Misra",
            "Ellen Riloff",
            "Marilyn Walker"
        ],
        "abstract": "Effective models of social dialog must understand a broad range of rhetorical and figurative devices. Rhetorical questions (RQs) are a type of figurative language whose aim is to achieve a pragmatic goal, such as structuring an argument, being persuasive, emphasizing a point, or being ironic. While there are computational models for other forms of figurative language, rhetorical questions have received little attention to date. We expand a small dataset from previous work, presenting a corpus of 10,270 RQs from debate forums and Twitter that represent different discourse functions. We show that we can clearly distinguish between RQs and sincere questions (0.76 F1). We then show that RQs can be used both sarcastically and non-sarcastically, observing that non-sarcastic (other) uses of RQs are frequently argumentative in forums, and persuasive in tweets. We present experiments to distinguish between these uses of RQs using SVM and LSTM models that represent linguistic features and post-level context, achieving results as high as 0.76 F1 for \"sarcastic\" and 0.77 F1 for \"other\" in forums, and 0.83 F1 for both \"sarcastic\" and \"other\" in tweets. We supplement our quantitative experiments with an in-depth characterization of the linguistic variation in RQs.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05308",
        "title": "Harvesting Creative Templates for Generating Stylistically Varied Restaurant Reviews",
        "authors": [
            "Shereen Oraby",
            "Sheideh Homayon",
            "Marilyn Walker"
        ],
        "abstract": "Many of the creative and figurative elements that make language exciting are lost in translation in current natural language generation engines. In this paper, we explore a method to harvest templates from positive and negative reviews in the restaurant domain, with the goal of vastly expanding the types of stylistic variation available to the natural language generator. We learn hyperbolic adjective patterns that are representative of the strongly-valenced expressive language commonly used in either positive or negative reviews. We then identify and delexicalize entities, and use heuristics to extract generation templates from review sentences. We evaluate the learned templates against more traditional review templates, using subjective measures of \"convincingness\", \"interestingness\", and \"naturalness\". Our results show that the learned templates score highly on these measures. Finally, we analyze the linguistic categories that characterize the learned positive and negative templates. We plan to use the learned templates to improve the conversational style of dialogue systems in the restaurant domain.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05404",
        "title": "Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue",
        "authors": [
            "Shereen Oraby",
            "Vrindavan Harrison",
            "Lena Reed",
            "Ernesto Hernandez",
            "Ellen Riloff",
            "Marilyn Walker"
        ],
        "abstract": "The use of irony and sarcasm in social media allows us to study them at scale for the first time. However, their diversity has made it difficult to construct a high-quality corpus of sarcasm in dialogue. Here, we describe the process of creating a large- scale, highly-diverse corpus of online debate forums dialogue, and our novel methods for operationalizing classes of sarcasm in the form of rhetorical questions and hyperbole. We show that we can use lexico-syntactic cues to reliably retrieve sarcastic utterances with high accuracy. To demonstrate the properties and quality of our corpus, we conduct supervised learning experiments with simple features, and show that we achieve both higher precision and F than previous work on sarcasm in debate forums dialogue. We apply a weakly-supervised linguistic pattern learner and qualitatively analyze the linguistic differences in each class.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05411",
        "title": "Combining Search with Structured Data to Create a More Engaging User Experience in Open Domain Dialogue",
        "authors": [
            "Kevin K. Bowden",
            "Shereen Oraby",
            "Jiaqi Wu",
            "Amita Misra",
            "Marilyn Walker"
        ],
        "abstract": "The greatest challenges in building sophisticated open-domain conversational agents arise directly from the potential for ongoing mixed-initiative multi-turn dialogues, which do not follow a particular plan or pursue a particular fixed information need. In order to make coherent conversational contributions in this context, a conversational agent must be able to track the types and attributes of the entities under discussion in the conversation and know how they are related. In some cases, the agent can rely on structured information sources to help identify the relevant semantic relations and produce a turn, but in other cases, the only content available comes from search, and it may be unclear which semantic relations hold between the search results and the discourse context. A further constraint is that the system must produce its contribution to the ongoing conversation in real-time. This paper describes our experience building SlugBot for the 2017 Alexa Prize, and discusses how we leveraged search and structured data from different sources to help SlugBot produce dialogic turns and carry on conversations whose length over the semi-finals user evaluation period averaged 8:17 minutes.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05413",
        "title": "\"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts",
        "authors": [
            "Shereen Oraby",
            "Pritam Gundecha",
            "Jalal Mahmud",
            "Mansurul Bhuiyan",
            "Rama Akkiraju"
        ],
        "abstract": "Given the increasing popularity of customer service dialogue on Twitter, analysis of conversation data is essential to understand trends in customer and agent behavior for the purpose of automating customer service interactions. In this work, we develop a novel taxonomy of fine-grained \"dialogue acts\" frequently observed in customer service, showcasing acts that are more suited to the domain than the more generic existing taxonomies. Using a sequential SVM-HMM model, we model conversation flow, predicting the dialogue act of a given turn in real-time. We characterize differences between customer and agent behavior in Twitter customer service conversations, and investigate the effect of testing our system on different customer service industries. Finally, we use a data-driven approach to predict important conversation outcomes: customer satisfaction, customer frustration, and overall problem resolution. We show that the type and location of certain dialogue acts in a conversation have a significant effect on the probability of desirable and undesirable outcomes, and present actionable rules based on our findings. The patterns and rules we derive can be used as guidelines for outcome-driven automated customer service platforms.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05467",
        "title": "Acquiring Background Knowledge to Improve Moral Value Prediction",
        "authors": [
            "Ying Lin",
            "Joe Hoover",
            "Morteza Dehghani",
            "Marlon Mooijman",
            "Heng Ji"
        ],
        "abstract": "In this paper, we address the problem of detecting expressions of moral values in tweets using content analysis. This is a particularly challenging problem because moral values are often only implicitly signaled in language, and tweets contain little contextual information due to length constraints. To address these obstacles, we present a novel approach to automatically acquire background knowledge from an external knowledge base to enrich input texts and thus improve moral value prediction. By combining basic text features with background knowledge, our overall context-aware framework achieves performance comparable to a single human annotator. To the best of our knowledge, this is the first attempt to incorporate background knowledge for the prediction of implicit psychological variables in the area of computational social science.\n    ",
        "submission_date": "2017-09-16T00:00:00",
        "last_modified_date": "2017-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05475",
        "title": "Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification",
        "authors": [
            "Bo-Ru Lu",
            "Frank Shyu",
            "Yun-Nung Chen",
            "Hung-Yi Lee",
            "Lin-shan Lee"
        ],
        "abstract": "Connectionist temporal classification (CTC) is a powerful approach for sequence-to-sequence learning, and has been popularly used in speech recognition. The central ideas of CTC include adding a label \"blank\" during training. With this mechanism, CTC eliminates the need of segment alignment, and hence has been applied to various sequence-to-sequence learning problems. In this work, we applied CTC to abstractive summarization for spoken content. The \"blank\" in this case implies the corresponding input data are less important or noisy; thus it can be ignored. This approach was shown to outperform the existing methods in term of ROUGE scores over Chinese Gigaword and MATBN corpora. This approach also has the nice property that the ordering of words or characters in the input documents can be better preserved in the generated summaries.\n    ",
        "submission_date": "2017-09-16T00:00:00",
        "last_modified_date": "2017-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05487",
        "title": "Role of Morphology Injection in Statistical Machine Translation",
        "authors": [
            "Sreelekha S",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Phrase-based Statistical models are more commonly used as they perform optimally in terms of both, translation quality and complexity of the system. Hindi and in general all Indian languages are morphologically richer than English. Hence, even though Phrase-based systems perform very well for the less divergent language pairs, for English to Indian language translation, we need more linguistic information (such as morphology, parse tree, parts of speech tags, etc.) on the source side. Factored models seem to be useful in this case, as Factored models consider word as a vector of factors. These factors can contain any information about the surface word and use it while translating. Hence, the objective of this work is to handle morphological inflections in Hindi and Marathi using Factored translation models while translating from English. SMT approaches face the problem of data sparsity while translating into a morphologically rich language. It is very unlikely for a parallel corpus to contain all morphological forms of words. We propose a solution to generate these unseen morphological forms and inject them into original training corpora. In this paper, we study factored models and the problem of sparseness in context of translation to morphologically rich languages. We propose a simple and effective solution which is based on enriching the input with various morphological forms of words. We observe that morphology injection improves the quality of translation in terms of both adequacy and fluency. We verify this with the experiments on two morphologically rich languages: Hindi and Marathi, while translating from English.\n    ",
        "submission_date": "2017-09-16T00:00:00",
        "last_modified_date": "2017-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05522",
        "title": "AISHELL-1: An Open-Source Mandarin Speech Corpus and A Speech Recognition Baseline",
        "authors": [
            "Hui Bu",
            "Jiayu Du",
            "Xingyu Na",
            "Bengu Wu",
            "Hao Zheng"
        ],
        "abstract": "An open-source Mandarin speech corpus called AISHELL-1 is released. It is by far the largest corpus which is suitable for conducting the speech recognition research and building speech recognition systems for Mandarin. The recording procedure, including audio capturing devices and environments are presented in details. The preparation of the related resources, including transcriptions and lexicon are described. The corpus is released with a Kaldi recipe. Experimental results implies that the quality of audio recordings and transcriptions are promising.\n    ",
        "submission_date": "2017-09-16T00:00:00",
        "last_modified_date": "2017-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05563",
        "title": "Data Innovation for International Development: An overview of natural language processing for qualitative data analysis",
        "authors": [
            "Philipp Broniecki",
            "Anna Hanchar",
            "Slava J. Mikhaylov"
        ],
        "abstract": "Availability, collection and access to quantitative data, as well as its limitations, often make qualitative data the resource upon which development programs heavily rely. Both traditional interview data and social media analysis can provide rich contextual information and are essential for research, appraisal, monitoring and evaluation. These data may be difficult to process and analyze both systematically and at scale. This, in turn, limits the ability of timely data driven decision-making which is essential in fast evolving complex social systems. In this paper, we discuss the potential of using natural language processing to systematize analysis of qualitative data, and to inform quick decision-making in the development context. We illustrate this with interview data generated in a format of micro-narratives for the UNDP Fragments of Impact project.\n    ",
        "submission_date": "2017-09-16T00:00:00",
        "last_modified_date": "2017-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05587",
        "title": "Character Distributions of Classical Chinese Literary Texts: Zipf's Law, Genres, and Epochs",
        "authors": [
            "Chao-Lin Liu",
            "Shuhua Zhang",
            "Yuanli Geng",
            "Huei-ling Lai",
            "Hongsu Wang"
        ],
        "abstract": "We collect 14 representative corpora for major periods in Chinese history in this study. These corpora include poetic works produced in several dynasties, novels of the Ming and Qing dynasties, and essays and news reports written in modern Chinese. The time span of these corpora ranges between 1046 BCE and 2007 CE. We analyze their character and word distributions from the viewpoint of the Zipf's law, and look for factors that affect the deviations and similarities between their Zipfian curves. Genres and epochs demonstrated their influences in our analyses. Specifically, the character distributions for poetic works of between 618 CE and 1644 CE exhibit striking similarity. In addition, although texts of the same dynasty may tend to use the same set of characters, their character distributions still deviate from each other.\n    ",
        "submission_date": "2017-09-17T00:00:00",
        "last_modified_date": "2017-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05599",
        "title": "Hierarchical Gated Recurrent Neural Tensor Network for Answer Triggering",
        "authors": [
            "Wei Li",
            "Yunfang Wu"
        ],
        "abstract": "In this paper, we focus on the problem of answer triggering ad-dressed by Yang et al. (2015), which is a critical component for a real-world question answering system. We employ a hierarchical gated recurrent neural tensor (HGRNT) model to capture both the context information and the deep in-teractions between the candidate answers and the question. Our result on F val-ue achieves 42.6%, which surpasses the baseline by over 10 %.\n    ",
        "submission_date": "2017-09-17T00:00:00",
        "last_modified_date": "2017-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05631",
        "title": "Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models",
        "authors": [
            "Marcely Zanon Boito",
            "Alexandre Berard",
            "Aline Villavicencio",
            "Laurent Besacier"
        ],
        "abstract": "Word discovery is the task of extracting words from unsegmented text. In this paper we examine to what extent neural networks can be applied to this task in a realistic unwritten language scenario, where only small corpora and limited annotations are available. We investigate two scenarios: one with no supervision and another with limited supervision with access to the most frequent words. Obtained results show that it is possible to retrieve at least 27% of the gold standard vocabulary by training an encoder-decoder neural machine translation system with only 5,157 sentences. This result is close to those obtained with a task-specific Bayesian nonparametric model. Moreover, our approach has the advantage of generating translation alignments, which could be used to create a bilingual lexicon. As a future perspective, this approach is also well suited to work directly from speech.\n    ",
        "submission_date": "2017-09-17T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05729",
        "title": "Flexible Computing Services for Comparisons and Analyses of Classical Chinese Poetry",
        "authors": [
            "Chao-Lin Liu"
        ],
        "abstract": "We collect nine corpora of representative Chinese poetry for the time span of 1046 BCE and 1644 CE for studying the history of Chinese words, collocations, and patterns. By flexibly integrating our own tools, we are able to provide new perspectives for approaching our goals. We illustrate the ideas with two examples. The first example show a new way to compare word preferences of poets, and the second example demonstrates how we can utilize our corpora in historical studies of the Chinese words. We show the viability of the tools for academic research, and we wish to make it helpful for enriching existing Chinese dictionary as well.\n    ",
        "submission_date": "2017-09-18T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05778",
        "title": "Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model for Short Text Multi-class Classification Problems",
        "authors": [
            "Bradford Heap",
            "Michael Bain",
            "Wayne Wobcke",
            "Alfred Krzywicki",
            "Susanne Schmeidl"
        ],
        "abstract": "The bag-of-words model is a standard representation of text for many linear classifier learners. In many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions. However in settings where there is a large vocabulary, large variance in the frequency of terms in the training corpus, many classes and very short text (e.g., single sentences or document titles) the bag-of-words representation becomes extremely sparse, and this can reduce the accuracy of classifiers. A particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence. In this work we introduce a method for enriching the bag-of-words model by complementing such rare term information with related terms from both general and domain-specific Word Vector models. By reducing sparseness in the bag-of-words models, our enrichment approach achieves improved classification over several baseline classifiers in a variety of text classification problems. Our approach is also efficient because it requires no change to the linear classifier before or during training, since bag-of-words enrichment applies only to text being classified.\n    ",
        "submission_date": "2017-09-18T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05820",
        "title": "Toward a full-scale neural machine translation in production: the Booking.com use case",
        "authors": [
            "Pavel Levin",
            "Nishikant Dhanuka",
            "Talaat Khalil",
            "Fedor Kovalev",
            "Maxim Khalilov"
        ],
        "abstract": "While some remarkable progress has been made in neural machine translation (NMT) research, there have not been many reports on its development and evaluation in practice. This paper tries to fill this gap by presenting some of our findings from building an in-house travel domain NMT system in a large scale E-commerce setting. The three major topics that we cover are optimization and training (including different optimization strategies and corpus sizes), handling real-world content and evaluating results.\n    ",
        "submission_date": "2017-09-18T00:00:00",
        "last_modified_date": "2017-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05914",
        "title": "Limitations of Cross-Lingual Learning from Image Search",
        "authors": [
            "Mareike Hartmann",
            "Anders Soegaard"
        ],
        "abstract": "Cross-lingual representation learning is an important step in making NLP scale to all the world's languages. Recent work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused on the translation of selected nouns only. In our work, we investigate whether the meaning of other parts-of-speech, in particular adjectives and verbs, can be learned in the same way. We also experiment with combining the representations learned from visual data with embeddings learned from textual data. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.\n    ",
        "submission_date": "2017-09-18T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06033",
        "title": "Sequence to Sequence Learning for Event Prediction",
        "authors": [
            "Dai Quoc Nguyen",
            "Dat Quoc Nguyen",
            "Cuong Xuan Chu",
            "Stefan Thater",
            "Manfred Pinkal"
        ],
        "abstract": "This paper presents an approach to the task of predicting an event description from a preceding sentence in a text. Our approach explores sequence-to-sequence learning using a bidirectional multi-layer recurrent neural network. Our approach substantially outperforms previous work in terms of the BLEU score on two datasets derived from WikiHow and DeScript respectively. Since the BLEU score is not easy to interpret as a measure of event prediction, we complement our study with a second evaluation that exploits the rich linguistic annotation of gold paraphrase sets of events.\n    ",
        "submission_date": "2017-09-18T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06136",
        "title": "Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural Dialog Models",
        "authors": [
            "Bing Liu",
            "Ian Lane"
        ],
        "abstract": "In this paper, we present a deep reinforcement learning (RL) framework for iterative dialog policy optimization in end-to-end task-oriented dialog systems. Popular approaches in learning dialog policy with RL include letting a dialog agent to learn against a user simulator. Building a reliable user simulator, however, is not trivial, often as difficult as building a good dialog agent. We address this challenge by jointly optimizing the dialog agent and the user simulator with deep RL by simulating dialogs between the two agents. We first bootstrap a basic dialog agent and a basic user simulator by learning directly from dialog corpora with supervised training. We then improve them further by letting the two agents to conduct task-oriented dialogs and iteratively optimizing their policies with deep RL. Both the dialog agent and the user simulator are designed with neural network models that can be trained end-to-end. Our experiment results show that the proposed method leads to promising improvements on task success rate and total task reward comparing to supervised training and single-agent RL training baseline models.\n    ",
        "submission_date": "2017-09-18T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06162",
        "title": "Paraphrasing verbal metonymy through computational methods",
        "authors": [
            "Alberto Mor\u00f3n Hern\u00e1ndez"
        ],
        "abstract": "Verbal metonymy has received relatively scarce attention in the field of computational linguistics despite the fact that a model to accurately paraphrase metonymy has applications both in academia and the technology sector. The method described in this paper makes use of data from the British National Corpus in order to create word vectors, find instances of verbal metonymy and generate potential paraphrases. Two different ways of creating word vectors are evaluated in this study: Continuous bag of words and Skip-grams. Skip-grams are found to outperform the Continuous bag of words approach. Furthermore, the Skip-gram model is found to operate with better-than-chance accuracy and there is a strong positive relationship (phi coefficient = 0.61) between the model's classification and human judgement of the ranked paraphrases. This study lends credence to the viability of modelling verbal metonymy through computational methods based on distributional semantics.\n    ",
        "submission_date": "2017-09-18T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06265",
        "title": "Dynamic Oracle for Neural Machine Translation in Decoding Phase",
        "authors": [
            "Zi-Yi Dou",
            "Hao Zhou",
            "Shu-Jian Huang",
            "Xin-Yu Dai",
            "Jia-Jun Chen"
        ],
        "abstract": "The past several years have witnessed the rapid progress of end-to-end Neural Machine Translation (NMT). However, there exists discrepancy between training and inference in NMT when decoding, which may lead to serious problems since the model might be in a part of the state space it has never seen during training. To address the issue, Scheduled Sampling has been proposed. However, there are certain limitations in Scheduled Sampling and we propose two dynamic oracle-based methods to improve it. We manage to mitigate the discrepancy by changing the training process towards a less guided scheme and meanwhile aggregating the oracle's demonstrations. Experimental results show that the proposed approaches improve translation quality over standard NMT system.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06307",
        "title": "A Fast and Accurate Vietnamese Word Segmenter",
        "authors": [
            "Dat Quoc Nguyen",
            "Dai Quoc Nguyen",
            "Thanh Vu",
            "Mark Dras",
            "Mark Johnson"
        ],
        "abstract": "We propose a novel approach to Vietnamese word segmentation. Our approach is based on the Single Classification Ripple Down Rules methodology (Compton and Jansen, 1990), where rules are stored in an exception structure and new rules are only added to correct segmentation errors given by existing rules. Experimental results on the benchmark Vietnamese treebank show that our approach outperforms previous state-of-the-art approaches JVnSegmenter, vnTokenizer, DongDu and UETsegmenter in terms of both accuracy and performance speed. Our code is open-source and available at: ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06309",
        "title": "Aspect-Based Relational Sentiment Analysis Using a Stacked Neural Network Architecture",
        "authors": [
            "Soufian Jebbara",
            "Philipp Cimiano"
        ],
        "abstract": "Sentiment analysis can be regarded as a relation extraction problem in which the sentiment of some opinion holder towards a certain aspect of a product, theme or event needs to be extracted. We present a novel neural architecture for sentiment analysis as a relation extraction problem that addresses this problem by dividing it into three subtasks: i) identification of aspect and opinion terms, ii) labeling of opinion terms with a sentiment, and iii) extraction of relations between opinion terms and aspect terms. For each subtask, we propose a neural network based component and combine all of them into a complete system for relational sentiment analysis. The component for aspect and opinion term extraction is a hybrid architecture consisting of a recurrent neural network stacked on top of a convolutional neural network. This approach outperforms a standard convolutional deep neural architecture as well as a recurrent network architecture and performs competitively compared to other methods on two datasets of annotated customer reviews. To extract sentiments for individual opinion terms, we propose a recurrent architecture in combination with word distance features and achieve promising results, outperforming a majority baseline by 18% accuracy and providing the first results for the USAGE dataset. Our relation extraction component outperforms the current state-of-the-art in aspect-opinion relation extraction by 15% F-Measure.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06311",
        "title": "Aspect-Based Sentiment Analysis Using a Two-Step Neural Network Architecture",
        "authors": [
            "Soufian Jebbara",
            "Philipp Cimiano"
        ],
        "abstract": "The World Wide Web holds a wealth of information in the form of unstructured texts such as customer reviews for products, events and more. By extracting and analyzing the expressed opinions in customer reviews in a fine-grained way, valuable opportunities and insights for customers and businesses can be gained. We propose a neural network based system to address the task of Aspect-Based Sentiment Analysis to compete in Task 2 of the ESWC-2016 Challenge on Semantic Sentiment Analysis. Our proposed architecture divides the task in two subtasks: aspect term extraction and aspect-specific sentiment extraction. This approach is flexible in that it allows to address each subtask independently. As a first step, a recurrent neural network is used to extract aspects from a text by framing the problem as a sequence labeling task. In a second step, a recurrent network processes each extracted aspect with respect to its context and predicts a sentiment label. The system uses pretrained semantic word embedding features which we experimentally enhance with semantic knowledge extracted from WordNet. Further features extracted from SenticNet prove to be beneficial for the extraction of sentiment labels. As the best performing system in its category, our proposed system proves to be an effective approach for the Aspect-Based Sentiment Analysis.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06317",
        "title": "Improving Opinion-Target Extraction with Character-Level Word Embeddings",
        "authors": [
            "Soufian Jebbara",
            "Philipp Cimiano"
        ],
        "abstract": "Fine-grained sentiment analysis is receiving increasing attention in recent years. Extracting opinion target expressions (OTE) in reviews is often an important step in fine-grained, aspect-based sentiment analysis. Retrieving this information from user-generated text, however, can be difficult. Customer reviews, for instance, are prone to contain misspelled words and are difficult to process due to their domain-specific language. In this work, we investigate whether character-level models can improve the performance for the identification of opinion target expressions. We integrate information about the character structure of a word into a sequence labeling system using character-level word embeddings and show their positive impact on the system's performance. Specifically, we obtain an increase by 3.3 points F1-score with respect to our baseline model. In further experiments, we reveal encoded character patterns of the learned embeddings and give a nuanced view of the performance differences of both models.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06365",
        "title": "MetaLDA: a Topic Model that Efficiently Incorporates Meta information",
        "authors": [
            "He Zhao",
            "Lan Du",
            "Wray Buntine",
            "Gang Liu"
        ],
        "abstract": "Besides the text content, documents and their associated words usually come with rich sets of meta informa- tion, such as categories of documents and semantic/syntactic features of words, like those encoded in word embeddings. Incorporating such meta information directly into the generative process of topic models can improve modelling accuracy and topic quality, especially in the case where the word-occurrence information in the training data is insufficient. In this paper, we present a topic model, called MetaLDA, which is able to leverage either document or word meta information, or both of them jointly. With two data argumentation techniques, we can derive an efficient Gibbs sampling algorithm, which benefits from the fully local conjugacy of the model. Moreover, the algorithm is favoured by the sparsity of the meta information. Extensive experiments on several real world datasets demonstrate that our model achieves comparable or improved performance in terms of both perplexity and topic quality, particularly in handling sparse texts. In addition, compared with other models using meta information, our model runs significantly faster.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06429",
        "title": "Neural Networks for Text Correction and Completion in Keyboard Decoding",
        "authors": [
            "Shaona Ghosh",
            "Per Ola Kristensson"
        ],
        "abstract": "Despite the ubiquity of mobile and wearable text messaging applications, the problem of keyboard text decoding is not tackled sufficiently in the light of the enormous success of the deep learning Recurrent Neural Network (RNN) and Convolutional Neural Networks (CNN) for natural language understanding. In particular, considering that the keyboard decoders should operate on devices with memory and processor resource constraints, makes it challenging to deploy industrial scale deep neural network (DNN) models. This paper proposes a sequence-to-sequence neural attention network system for automatic text correction and completion. Given an erroneous sequence, our model encodes character level hidden representations and then decodes the revised sequence thus enabling auto-correction and completion. We achieve this by a combination of character level CNN and gated recurrent unit (GRU) encoder along with and a word level gated recurrent unit (GRU) attention decoder. Unlike traditional language models that learn from billions of words, our corpus size is only 12 million words; an order of magnitude smaller. The memory footprint of our learnt model for inference and prediction is also an order of magnitude smaller than the conventional language model based text decoders. We report baseline performance for neural keyboard decoders in such limited domain. Our models achieve a word level accuracy of $90\\%$ and a character error rate CER of $2.4\\%$ over the Twitter typo dataset. We present a novel dataset of noisy to corrected mappings by inducing the noise distribution from the Twitter data over the OpenSubtitles 2009 dataset; on which our model predicts with a word level accuracy of $98\\%$ and sequence accuracy of $68.9\\%$. In our user study, our model achieved an average CER of $2.6\\%$ with the state-of-the-art non-neural touch-screen keyboard decoder at CER of $1.6\\%$.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06436",
        "title": "Language Modeling with Highway LSTM",
        "authors": [
            "Gakuto Kurata",
            "Bhuvana Ramabhadran",
            "George Saon",
            "Abhinav Sethy"
        ],
        "abstract": "Language models (LMs) based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks. In this paper, we extend an LSTM by adding highway networks inside an LSTM and use the resulting Highway LSTM (HW-LSTM) model for language modeling. The added highway networks increase the depth in the time dimension. Since a typical LSTM has two internal states, a memory cell and a hidden state, we compare various types of HW-LSTM by adding highway networks onto the memory cell and/or the hidden state. Experimental results on English broadcast news and conversational telephone speech recognition show that the proposed HW-LSTM LM improves speech recognition accuracy on top of a strong LSTM LM baseline. We report 5.1% and 9.9% on the Switchboard and CallHome subsets of the Hub5 2000 evaluation, which reaches the best performance numbers reported on these tasks to date.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06438",
        "title": "A Recorded Debating Dataset",
        "authors": [
            "Shachar Mirkin",
            "Michal Jacovi",
            "Tamar Lavee",
            "Hong-Kwang Kuo",
            "Samuel Thomas",
            "Leslie Sager",
            "Lili Kotlerman",
            "Elad Venezian",
            "Noam Slonim"
        ],
        "abstract": "This paper describes an English audio and textual dataset of debating speeches, a unique resource for the growing research field of computational argumentation and debating technologies. We detail the process of speech recording by professional debaters, the transcription of the speeches with an Automatic Speech Recognition (ASR) system, their consequent automatic processing to produce a text that is more \"NLP-friendly\", and in parallel -- the manual transcription of the speeches in order to produce gold-standard \"reference\" transcripts. We release 60 speeches on various controversial topics, each in five formats corresponding to the different stages in the production of the data. The intention is to allow utilizing this resource for multiple research purposes, be it the addition of in-domain training data for a debate-specific ASR system, or applying argumentation mining on either noisy or clean debate transcripts. We intend to make further releases of this data in the future.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2018-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06671",
        "title": "Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words",
        "authors": [
            "Danushka Bollegala",
            "Kohei Hayashi",
            "Ken-ichi Kawarabayashi"
        ],
        "abstract": "Distributed word embeddings have shown superior performances in numerous Natural Language Processing (NLP) tasks. However, their performances vary significantly across different tasks, implying that the word embeddings learnt by those methods capture complementary aspects of lexical semantics. Therefore, we believe that it is important to combine the existing word embeddings to produce more accurate and complete \\emph{meta-embeddings} of words. For this purpose, we propose an unsupervised locally linear meta-embedding learning method that takes pre-trained word embeddings as the input, and produces more accurate meta embeddings. Unlike previously proposed meta-embedding learning methods that learn a global projection over all words in a vocabulary, our proposed method is sensitive to the differences in local neighbourhoods of the individual source word embeddings. Moreover, we show that vector concatenation, a previously proposed highly competitive baseline approach for integrating word embeddings, can be derived as a special case of the proposed method. Experimental results on semantic similarity, word analogy, relation classification, and short-text classification tasks show that our meta-embeddings to significantly outperform prior methods in several benchmark datasets, establishing a new state of the art for meta-embeddings.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06673",
        "title": "Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational Compositional Operators for Analogy Detection",
        "authors": [
            "Huda Hakami",
            "Danushka Bollegala",
            "Hayashi Kohei"
        ],
        "abstract": "Representing the semantic relations that exist between two given words (or entities) is an important first step in a wide-range of NLP applications such as analogical reasoning, knowledge base completion and relational information retrieval. A simple, yet surprisingly accurate method for representing a relation between two words is to compute the vector offset (\\PairDiff) between their corresponding word embeddings. Despite the empirical success, it remains unclear as to whether \\PairDiff is the best operator for obtaining a relational representation from word embeddings. We conduct a theoretical analysis of generalised bilinear operators that can be used to measure the $\\ell_{2}$ relational distance between two word-pairs. We show that, if the word embeddings are standardised and uncorrelated, such an operator will be independent of bilinear terms, and can be simplified to a linear form, where \\PairDiff is a special case. For numerous word embedding types, we empirically verify the uncorrelation assumption, demonstrating the general applicability of our theoretical result. Moreover, we experimentally discover \\PairDiff from the bilinear relation composition operator on several benchmark analogy datasets.\n    ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06818",
        "title": "Updating the silent speech challenge benchmark with deep learning",
        "authors": [
            "Yan Ji",
            "Licheng Liu",
            "Hongcui Wang",
            "Zhilei Liu",
            "Zhibin Niu",
            "Bruce Denby"
        ],
        "abstract": "The 2010 Silent Speech Challenge benchmark is updated with new results obtained in a Deep Learning strategy, using the same input features and decoding strategy as in the original article. A Word Error Rate of 6.4% is obtained, compared to the published value of 17.4%. Additional results comparing new auto-encoder-based features with the original features at reduced dimensionality, as well as decoding scenarios on two different language models, are also presented. The Silent Speech Challenge archive has been updated to contain both the original and the new auto-encoder features, in addition to the original raw data.\n    ",
        "submission_date": "2017-09-20T00:00:00",
        "last_modified_date": "2017-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06901",
        "title": "De-identification of medical records using conditional random fields and long short-term memory networks",
        "authors": [
            "Zhipeng Jiang",
            "Chao Zhao",
            "Bin He",
            "Yi Guan",
            "Jingchi Jiang"
        ],
        "abstract": "The CEGS N-GRID 2016 Shared Task 1 in Clinical Natural Language Processing focuses on the de-identification of psychiatric evaluation records. This paper describes two participating systems of our team, based on conditional random fields (CRFs) and long short-term memory networks (LSTMs). A pre-processing module was introduced for sentence detection and tokenization before de-identification. For CRFs, manually extracted rich features were utilized to train the model. For LSTMs, a character-level bi-directional LSTM network was applied to represent tokens and classify tags for each token, following which a decoding layer was stacked to decode the most probable protected health information (PHI) terms. The LSTM-based system attained an i2b2 strict micro-F_1 measure of 89.86%, which was higher than that of the CRF-based system.\n    ",
        "submission_date": "2017-09-20T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06918",
        "title": "Constructing a Hierarchical User Interest Structure based on User Profiles",
        "authors": [
            "Chao Zhao",
            "Min Zhao",
            "Yi Guan"
        ],
        "abstract": "The interests of individual internet users fall into a hierarchical structure which is useful in regards to building personalized searches and recommendations. Most studies on this subject construct the interest hierarchy of a single person from the document perspective. In this study, we constructed the user interest hierarchy via user profiles. We organized 433,397 user interests, referred to here as \"attentions\", into a user attention network (UAN) from 200 million user profiles; we then applied the Louvain algorithm to detect hierarchical clusters in these attentions. Finally, a 26-level hierarchy with 34,676 clusters was obtained. We found that these attention clusters were aggregated according to certain topics as opposed to the hyponymy-relation based conceptual ontologies. The topics can be entities or concepts, and the relations were not restrained by hyponymy. The concept relativity encapsulated in the user's interest can be captured by labeling the attention clusters with corresponding concepts.\n    ",
        "submission_date": "2017-09-20T00:00:00",
        "last_modified_date": "2017-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07104",
        "title": "On the Use of Machine Translation-Based Approaches for Vietnamese Diacritic Restoration",
        "authors": [
            "Thai-Hoang Pham",
            "Xuan-Khoai Pham",
            "Phuong Le-Hong"
        ],
        "abstract": "This paper presents an empirical study of two machine translation-based approaches for Vietnamese diacritic restoration problem, including phrase-based and neural-based machine translation models. This is the first work that applies neural-based machine translation method to this problem and gives a thorough comparison to the phrase-based machine translation method which is the current state-of-the-art method for this problem. On a large dataset, the phrase-based approach has an accuracy of 97.32% while that of the neural-based approach is 96.15%. While the neural-based method has a slightly lower accuracy, it is about twice faster than the phrase-based method in terms of inference speed. Moreover, neural-based machine translation method has much room for future improvement such as incorporating pre-trained word embeddings and collecting more training data.\n    ",
        "submission_date": "2017-09-20T00:00:00",
        "last_modified_date": "2017-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07109",
        "title": "Deconvolutional Latent-Variable Model for Text Sequence Matching",
        "authors": [
            "Dinghan Shen",
            "Yizhe Zhang",
            "Ricardo Henao",
            "Qinliang Su",
            "Lawrence Carin"
        ],
        "abstract": "A latent-variable model is introduced for text matching, inferring sentence representations by jointly optimizing generative and discriminative objectives. To alleviate typical optimization challenges in latent-variable models for text, we employ deconvolutional networks as the sequence decoder (generator), providing learned latent codes with more semantic information and better generalization. Our model, trained in an unsupervised manner, yields stronger empirical predictive performance than a decoder based on Long Short-Term Memory (LSTM), with less parameters and considerably faster training. Further, we apply it to text sequence-matching problems. The proposed model significantly outperforms several strong sentence-encoding baselines, especially in the semi-supervised setting.\n    ",
        "submission_date": "2017-09-21T00:00:00",
        "last_modified_date": "2017-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07276",
        "title": "Speech Recognition Challenge in the Wild: Arabic MGB-3",
        "authors": [
            "Ahmed Ali",
            "Stephan Vogel",
            "Steve Renals"
        ],
        "abstract": "This paper describes the Arabic MGB-3 Challenge - Arabic Speech Recognition in the Wild. Unlike last year's Arabic MGB-2 Challenge, for which the recognition task was based on more than 1,200 hours broadcast TV news recordings from Aljazeera Arabic TV programs, MGB-3 emphasises dialectal Arabic using a multi-genre collection of Egyptian YouTube videos. Seven genres were used for the data collection: comedy, cooking, family/kids, fashion, drama, sports, and science (TEDx). A total of 16 hours of videos, split evenly across the different genres, were divided into adaptation, development and evaluation data sets. The Arabic MGB-Challenge comprised two tasks: A) Speech transcription, evaluated on the MGB-3 test set, along with the 10 hour MGB-2 test set to report progress on the MGB-2 evaluation; B) Arabic dialect identification, introduced this year in order to distinguish between four major Arabic dialects - Egyptian, Levantine, North African, Gulf, as well as Modern Standard Arabic. Two hours of audio per dialect were released for development and a further two hours were used for evaluation. For dialect identification, both lexical features and i-vector bottleneck features were shared with participants in addition to the raw audio recordings. Overall, thirteen teams submitted ten systems to the challenge. We outline the approaches adopted in each system, and summarise the evaluation results.\n    ",
        "submission_date": "2017-09-21T00:00:00",
        "last_modified_date": "2017-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07357",
        "title": "Retrofitting Concept Vector Representations of Medical Concepts to Improve Estimates of Semantic Similarity and Relatedness",
        "authors": [
            "Zhiguo Yu",
            "Byron C. Wallace",
            "Todd Johnson",
            "Trevor Cohen"
        ],
        "abstract": "Estimation of semantic similarity and relatedness between biomedical concepts has utility for many informatics applications. Automated methods fall into two categories: methods based on distributional statistics drawn from text corpora, and methods using the structure of existing knowledge resources. Methods in the former category disregard taxonomic structure, while those in the latter fail to consider semantically relevant empirical information. In this paper, we present a method that retrofits distributional context vector representations of biomedical concepts using structural information from the UMLS Metathesaurus, such that the similarity between vector representations of linked concepts is augmented. We evaluated it on the UMNSRS benchmark. Our results demonstrate that retrofitting of concept vector representations leads to better correlation with human raters for both similarity and relatedness, surpassing the best results reported to date. They also demonstrate a clear improvement in performance on this reference standard for retrofitted vector representations, as compared to those without retrofitting.\n    ",
        "submission_date": "2017-09-21T00:00:00",
        "last_modified_date": "2017-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07403",
        "title": "Inducing Distant Supervision in Suggestion Mining through Part-of-Speech Embeddings",
        "authors": [
            "Sapna Negi",
            "Paul Buitelaar"
        ],
        "abstract": "Mining suggestion expressing sentences from a given text is a less investigated sentence classification task, and therefore lacks hand labeled benchmark datasets. In this work, we propose and evaluate two approaches for distant supervision in suggestion mining. The distant supervision is obtained through a large silver standard dataset, constructed using the text from wikiHow and Wikipedia. Both the approaches use a LSTM based neural network architecture to learn a classification model for suggestion mining, but vary in their method to use the silver standard dataset. The first approach directly trains the classifier using this dataset, while the second approach only learns word embeddings from this dataset. In the second approach, we also learn POS embeddings, which interestingly gives the best classification accuracy.\n    ",
        "submission_date": "2017-09-21T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07434",
        "title": "Analyzing users' sentiment towards popular consumer industries and brands on Twitter",
        "authors": [
            "Guoning Hu",
            "Preeti Bhargava",
            "Saul Fuhrmann",
            "Sarah Ellinger",
            "Nemanja Spasojevic"
        ],
        "abstract": "Social media serves as a unified platform for users to express their thoughts on subjects ranging from their daily lives to their opinion on consumer brands and products. These users wield an enormous influence in shaping the opinions of other consumers and influence brand perception, brand loyalty and brand advocacy. In this paper, we analyze the opinion of 19M Twitter users towards 62 popular industries, encompassing 12,898 enterprise and consumer brands, as well as associated subject matter topics, via sentiment analysis of 330M tweets over a period spanning a month. We find that users tend to be most positive towards manufacturing and most negative towards service industries. In addition, they tend to be more positive or negative when interacting with brands than generally on Twitter. We also find that sentiment towards brands within an industry varies greatly and we demonstrate this using two industries as use cases. In addition, we discover that there is no strong correlation between topic sentiments of different industries, demonstrating that topic sentiments are highly dependent on the context of the industry that they are mentioned in. We demonstrate the value of such an analysis in order to assess the impact of brands on social media. We hope that this initial study will prove valuable for both researchers and companies in understanding users' perception of industries, brands and associated topics and encourage more research in this field.\n    ",
        "submission_date": "2017-09-21T00:00:00",
        "last_modified_date": "2017-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07470",
        "title": "Learning Domain-Specific Word Embeddings from Sparse Cybersecurity Texts",
        "authors": [
            "Arpita Roy",
            "Youngja Park",
            "SHimei Pan"
        ],
        "abstract": "Word embedding is a Natural Language Processing (NLP) technique that automatically maps words from a vocabulary to vectors of real numbers in an embedding space. It has been widely used in recent years to boost the performance of a vari-ety of NLP tasks such as Named Entity Recognition, Syntac-tic Parsing and Sentiment Analysis. Classic word embedding methods such as Word2Vec and GloVe work well when they are given a large text corpus. When the input texts are sparse as in many specialized domains (e.g., cybersecurity), these methods often fail to produce high-quality vectors. In this pa-per, we describe a novel method to train domain-specificword embeddings from sparse texts. In addition to domain texts, our method also leverages diverse types of domain knowledge such as domain vocabulary and semantic relations. Specifi-cally, we first propose a general framework to encode diverse types of domain knowledge as text annotations. Then we de-velop a novel Word Annotation Embedding (WAE) algorithm to incorporate diverse types of text annotations in word em-bedding. We have evaluated our method on two cybersecurity text corpora: a malware description corpus and a Common Vulnerability and Exposure (CVE) corpus. Our evaluation re-sults have demonstrated the effectiveness of our method in learning domain-specific word embeddings.\n    ",
        "submission_date": "2017-09-21T00:00:00",
        "last_modified_date": "2017-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07484",
        "title": "WERd: Using Social Text Spelling Variants for Evaluating Dialectal Speech Recognition",
        "authors": [
            "Ahmed Ali",
            "Preslav Nakov",
            "Peter Bell",
            "Steve Renals"
        ],
        "abstract": "We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Such a situation is typical for machine translation (MT), and thus we borrow ideas from an MT evaluation metric, namely TERp, an extension of translation error rate which is closely-related to WER. In particular, in the process of comparing a hypothesis to a reference, we make use of spelling variants for words and phrases, which we mine from Twitter in an unsupervised fashion. Our experiments with evaluating ASR output for Egyptian Arabic, and further manual analysis, show that the resulting WERd (i.e., WER for dialects) metric, a variant of TERp, is more adequate than WER for evaluating dialectal ASR.\n    ",
        "submission_date": "2017-09-21T00:00:00",
        "last_modified_date": "2017-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07758",
        "title": "Improving Language Modelling with Noise-contrastive estimation",
        "authors": [
            "Farhana Ferdousi Liza",
            "Marek Grzes"
        ],
        "abstract": "Neural language models do not scale well when the vocabulary is large. Noise-contrastive estimation (NCE) is a sampling-based method that allows for fast learning with large vocabularies. Although NCE has shown promising performance in neural machine translation, it was considered to be an unsuccessful approach for language modelling. A sufficient investigation of the hyperparameters in the NCE-based neural language models was also missing. In this paper, we showed that NCE can be a successful approach in neural language modelling when the hyperparameters of a neural network are tuned appropriately. We introduced the 'search-then-converge' learning rate schedule for NCE and designed a heuristic that specifies how to use this schedule. The impact of the other important hyperparameters, such as the dropout rate and the weight initialisation range, was also demonstrated. We showed that appropriate tuning of NCE-based neural language models outperforms the state-of-the-art single-model methods on a popular benchmark.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07777",
        "title": "Sentence Correction Based on Large-scale Language Modelling",
        "authors": [
            "Ji Wen"
        ],
        "abstract": "With the further development of informatization, more and more data is stored in the form of text. There are some loss of text during their generation and transmission. The paper aims to establish a language model based on the large-scale corpus to complete the restoration of missing text. In this paper, we introduce a novel measurement to find the missing words, and a way of establishing a comprehensive candidate lexicon to insert the correct choice of words. The paper also introduces some effective optimization methods, which largely improve the efficiency of the text restoration and shorten the time of dealing with 1000 sentences into 3.6 seconds. \\keywords{ language model, sentence correction, word imputation, parallel optimization\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07809",
        "title": "Neural Machine Translation",
        "authors": [
            "Philipp Koehn"
        ],
        "abstract": "Draft of textbook chapter on neural machine translation. a comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges. Written as chapter for the textbook Statistical Machine Translation. Used in the JHU Fall 2017 class on machine translation.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07814",
        "title": "Attention-based Wav2Text with Feature Transfer Learning",
        "authors": [
            "Andros Tjandra",
            "Sakriani Sakti",
            "Satoshi Nakamura"
        ],
        "abstract": "Conventional automatic speech recognition (ASR) typically performs multi-level pattern recognition tasks that map the acoustic speech waveform into a hierarchy of speech units. But, it is widely known that information loss in the earlier stage can propagate through the later stages. After the resurgence of deep learning, interest has emerged in the possibility of developing a purely end-to-end ASR system from the raw waveform to the transcription without any predefined alignments and hand-engineered models. However, the successful attempts in end-to-end architecture still used spectral-based features, while the successful attempts in using raw waveform were still based on the hybrid deep neural network - Hidden Markov model (DNN-HMM) framework. In this paper, we construct the first end-to-end attention-based encoder-decoder model to process directly from raw speech waveform to the text transcription. We called the model as \"Attention-based Wav2Text\". To assist the training process of the end-to-end model, we propose to utilize a feature transfer learning. Experimental results also reveal that the proposed Attention-based Wav2Text model directly with raw waveform could achieve a better result in comparison with the attentional encoder-decoder model trained on standard front-end filterbank features.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07840",
        "title": "Challenging Neural Dialogue Models with Natural Data: Memory Networks Fail on Incremental Phenomena",
        "authors": [
            "Igor Shalyminov",
            "Arash Eshghi",
            "Oliver Lemon"
        ],
        "abstract": "Natural, spontaneous dialogue proceeds incrementally on a word-by-word basis; and it contains many sorts of disfluency such as mid-utterance/sentence hesitations, interruptions, and self-corrections. But training data for machine learning approaches to dialogue processing is often either cleaned-up or wholly synthetic in order to avoid such phenomena. The question then arises of how well systems trained on such clean data generalise to real spontaneous dialogue, or indeed whether they are trainable at all on naturally occurring dialogue data. To answer this question, we created a new corpus called bAbI+ by systematically adding natural spontaneous incremental dialogue phenomena such as restarts and self-corrections to the Facebook AI Research's bAbI dialogues dataset. We then explore the performance of a state-of-the-art retrieval model, MemN2N, on this more natural dataset. Results show that the semantic accuracy of the MemN2N model drops drastically; and that although it is in principle able to learn to process the constructions in bAbI+, it needs an impractical amount of training data to do so. Finally, we go on to show that an incremental, semantic parser -- DyLan -- shows 100% semantic accuracy on both bAbI and bAbI+, highlighting the generalisation properties of linguistically informed dialogue models.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07858",
        "title": "Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars",
        "authors": [
            "Arash Eshghi",
            "Igor Shalyminov",
            "Oliver Lemon"
        ],
        "abstract": "We investigate an end-to-end method for automatically inducing task-based dialogue systems from small amounts of unannotated dialogue data. It combines an incremental semantic grammar - Dynamic Syntax and Type Theory with Records (DS-TTR) - with Reinforcement Learning (RL), where language generation and dialogue management are a joint decision problem. The systems thus produced are incremental: dialogues are processed word-by-word, shown previously to be essential in supporting natural, spontaneous dialogue. We hypothesised that the rich linguistic knowledge within the grammar should enable a combinatorially large number of dialogue variations to be processed, even when trained on very few dialogues. Our experiments show that our model can process 74% of the Facebook AI bAbI dataset even when trained on only 0.13% of the data (5 dialogues). It can in addition process 65% of bAbI+, a corpus we created by systematically adding incremental dialogue phenomena such as restarts and self-corrections to bAbI. We compare our model with a state-of-the-art retrieval model, MemN2N. We find that, in terms of semantic accuracy, MemN2N shows very poor robustness to the bAbI+ transformations even when trained on the full bAbI dataset.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07862",
        "title": "Mitigating the Impact of Speech Recognition Errors on Chatbot using Sequence-to-Sequence Model",
        "authors": [
            "Pin-Jung Chen",
            "I-Hung Hsu",
            "Yi-Yao Huang",
            "Hung-Yi Lee"
        ],
        "abstract": "We apply sequence-to-sequence model to mitigate the impact of speech recognition errors on open domain end-to-end dialog generation. We cast the task as a domain adaptation problem where ASR transcriptions and original text are in two different domains. In this paper, our proposed model includes two individual encoders for each domain data and make their hidden states similar to ensure the decoder predict the same dialog text. The method shows that the sequence-to-sequence model can learn the ASR transcriptions and original text pair having the same meaning and eliminate the speech recognition errors. Experimental results on Cornell movie dialog dataset demonstrate that the domain adaption system help the spoken dialog system generate more similar responses with the original text answers.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08011",
        "title": "Long Short-Term Memory for Japanese Word Segmentation",
        "authors": [
            "Yoshiaki Kitagawa",
            "Mamoru Komachi"
        ],
        "abstract": "This study presents a Long Short-Term Memory (LSTM) neural network approach to Japanese word segmentation (JWS). Previous studies on Chinese word segmentation (CWS) succeeded in using recurrent neural networks such as LSTM and gated recurrent units (GRU). However, in contrast to Chinese, Japanese includes several character types, such as hiragana, katakana, and kanji, that produce orthographic variations and increase the difficulty of word segmentation. Additionally, it is important for JWS tasks to consider a global context, and yet traditional JWS approaches rely on local features. In order to address this problem, this study proposes employing an LSTM-based approach to JWS. The experimental results indicate that the proposed model achieves state-of-the-art accuracy with respect to various Japanese corpora.\n    ",
        "submission_date": "2017-09-23T00:00:00",
        "last_modified_date": "2018-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08074",
        "title": "Language Independent Acquisition of Abbreviations",
        "authors": [
            "Michael R. Glass",
            "Md Faisal Mahbub Chowdhury",
            "Alfio M. Gliozzo"
        ],
        "abstract": "This paper addresses automatic extraction of abbreviations (encompassing acronyms and initialisms) and corresponding long-form expansions from plain unstructured text. We create and are going to release a multilingual resource for abbreviations and their corresponding expansions, built automatically by exploiting Wikipedia redirect and disambiguation pages, that can be used as a benchmark for evaluation. We address a shortcoming of previous work where only the redirect pages were used, and so every abbreviation had only a single expansion, even though multiple different expansions are possible for many of the abbreviations. We also develop a principled machine learning based approach to scoring expansion candidates using different techniques such as indicators of near synonymy, topical relatedness, and surface similarity. We show improved performance over seven languages, including two with a non-Latin alphabet, relative to strong baselines.\n    ",
        "submission_date": "2017-09-23T00:00:00",
        "last_modified_date": "2017-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08196",
        "title": "Identifying Phrasemes via Interlingual Association Measures -- A Data-driven Approach on Dependency-parsed and Word-aligned Parallel Corpora",
        "authors": [
            "Johannes Gra\u00ebn"
        ],
        "abstract": "This is a preprint of the article \"Identifying Phrasemes via Interlingual Association Measures\" that was presented in February 2016 at the LeKo (Lexical combinations and typified speech in a multilingual context) conference in Innsbruck.\n    ",
        "submission_date": "2017-09-24T00:00:00",
        "last_modified_date": "2017-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08294",
        "title": "Learning Context-Sensitive Convolutional Filters for Text Processing",
        "authors": [
            "Dinghan Shen",
            "Martin Renqiang Min",
            "Yitong Li",
            "Lawrence Carin"
        ],
        "abstract": "Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-sensitive convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-sensitive filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-sensitive filters, we further validate and rationalize the effectiveness of proposed framework.\n    ",
        "submission_date": "2017-09-25T00:00:00",
        "last_modified_date": "2018-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08299",
        "title": "Dataset for the First Evaluation on Chinese Machine Reading Comprehension",
        "authors": [
            "Yiming Cui",
            "Ting Liu",
            "Zhipeng Chen",
            "Wentao Ma",
            "Shijin Wang",
            "Guoping Hu"
        ],
        "abstract": "Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading comprehension datasets, in this paper we propose a new Chinese reading comprehension dataset for accelerating related research in the community. The proposed dataset contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set. Along with this dataset, we also hosted the first Evaluation on Chinese Machine Reading Comprehension (CMRC-2017) and successfully attracted tens of participants, which suggest the potential impact of this dataset.\n    ",
        "submission_date": "2017-09-25T00:00:00",
        "last_modified_date": "2018-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08521",
        "title": "Using objective words in the reviews to improve the colloquial arabic sentiment analysis",
        "authors": [
            "Omar Al-Harbi"
        ],
        "abstract": "One of the main difficulties in sentiment analysis of the Arabic language is the presence of the colloquialism. In this paper, we examine the effect of using objective words in conjunction with sentimental words on sentiment classification for the colloquial Arabic reviews, specifically Jordanian colloquial reviews. The reviews often include both sentimental and objective words, however, the most existing sentiment analysis models ignore the objective words as they are considered useless. In this work, we created two lexicons: the first includes the colloquial sentimental words and compound phrases, while the other contains the objective words associated with values of sentiment tendency based on a particular estimation method. We used these lexicons to extract sentiment features that would be training input to the Support Vector Machines (SVM) to classify the sentiment polarity of the reviews. The reviews dataset have been collected manually from JEERAN website. The results of the experiments show that the proposed approach improves the polarity classification in comparison to two baseline models, with accuracy 95.6%.\n    ",
        "submission_date": "2017-09-25T00:00:00",
        "last_modified_date": "2017-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08600",
        "title": "EZLearn: Exploiting Organic Supervision in Large-Scale Data Annotation",
        "authors": [
            "Maxim Grechkin",
            "Hoifung Poon",
            "Bill Howe"
        ],
        "abstract": "Many real-world applications require automated data annotation, such as identifying tissue origins based on gene expressions and classifying images into semantic categories. Annotation classes are often numerous and subject to changes over time, and annotating examples has become the major bottleneck for supervised learning methods. In science and other high-value domains, large repositories of data samples are often available, together with two sources of organic supervision: a lexicon for the annotation classes, and text descriptions that accompany some data samples. Distant supervision has emerged as a promising paradigm for exploiting such indirect supervision by automatically annotating examples where the text description contains a class mention in the lexicon. However, due to linguistic variations and ambiguities, such training data is inherently noisy, which limits the accuracy of this approach. In this paper, we introduce an auxiliary natural language processing system for the text modality, and incorporate co-training to reduce noise and augment signal in distant supervision. Without using any manually labeled data, our EZLearn system learned to accurately annotate data samples in functional genomics and scientific figure comprehension, substantially outperforming state-of-the-art supervised methods trained on tens of thousands of annotated examples.\n    ",
        "submission_date": "2017-09-25T00:00:00",
        "last_modified_date": "2018-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08624",
        "title": "Long Text Generation via Adversarial Training with Leaked Information",
        "authors": [
            "Jiaxian Guo",
            "Sidi Lu",
            "Han Cai",
            "Weinan Zhang",
            "Yong Yu",
            "Jun Wang"
        ],
        "abstract": "Automatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets (GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional Manager module, which takes the extracted features of current generated words and outputs a latent vector to guide the Worker module for next-word generation. Our extensive experiments on synthetic data and various real-world tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between Manager and Worker.\n    ",
        "submission_date": "2017-09-24T00:00:00",
        "last_modified_date": "2017-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08694",
        "title": "Methodology and Results for the Competition on Semantic Similarity Evaluation and Entailment Recognition for PROPOR 2016",
        "authors": [
            "Luciano Barbosa",
            "Paulo R. Cavalin",
            "Victor Guimaraes",
            "Matthias Kormaksson"
        ],
        "abstract": "In this paper, we present the methodology and the results obtained by our teams, dubbed Blue Man Group, in the ASSIN (from the Portuguese {\\it Avalia\u00e7\u00e3o de Similaridade Sem\u00e2ntica e Infer\u00eancia Textual}) competition, held at PROPOR 2016\\footnote{International Conference on the Computational Processing of the Portuguese Language - ",
        "submission_date": "2017-09-19T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08698",
        "title": "Identifying Restaurant Features via Sentiment Analysis on Yelp Reviews",
        "authors": [
            "Boya Yu",
            "Jiaxu Zhou",
            "Yi Zhang",
            "Yunong Cao"
        ],
        "abstract": "Many people use Yelp to find a good restaurant. Nonetheless, with only an overall rating for each restaurant, Yelp offers not enough information for independently judging its various aspects such as environment, service or flavor. In this paper, we introduced a machine learning based method to characterize such aspects for particular types of restaurants. The main approach used in this paper is to use a support vector machine (SVM) model to decipher the sentiment tendency of each review from word frequency. Word scores generated from the SVM models are further processed into a polarity index indicating the significance of each word for special types of restaurant. Customers overall tend to express more sentiment regarding service. As for the distinction between different cuisines, results that match the common sense are obtained: Japanese cuisines are usually fresh, some French cuisines are overpriced while Italian Restaurants are often famous for their pizzas.\n    ",
        "submission_date": "2017-09-20T00:00:00",
        "last_modified_date": "2017-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08716",
        "title": "DOC: Deep Open Classification of Text Documents",
        "authors": [
            "Lei Shu",
            "Hu Xu",
            "Bing Liu"
        ],
        "abstract": "Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.\n    ",
        "submission_date": "2017-09-25T00:00:00",
        "last_modified_date": "2017-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08878",
        "title": "Generating Sentences by Editing Prototypes",
        "authors": [
            "Kelvin Guu",
            "Tatsunori B. Hashimoto",
            "Yonatan Oren",
            "Percy Liang"
        ],
        "abstract": "We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2018-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08898",
        "title": "Improving a Multi-Source Neural Machine Translation Model with Corpus Extension for Low-Resource Languages",
        "authors": [
            "Gyu-Hyeon Choi",
            "Jong-Hun Shin",
            "Young-Kil Kim"
        ],
        "abstract": "In machine translation, we often try to collect resources to improve performance. However, most of the language pairs, such as Korean-Arabic and Korean-Vietnamese, do not have enough resources to train machine translation systems. In this paper, we propose the use of synthetic methods for extending a low-resource corpus and apply it to a multi-source neural machine translation model. We showed the improvement of machine translation performance through corpus extension using the synthetic method. We specifically focused on how to create source sentences that can make better target sentences, including the use of synthetic methods. We found that the corpus extension could also improve the performance of multi-source neural machine translation. We showed the corpus extension and multi-source model to be efficient methods for a low-resource language pair. Furthermore, when both methods were used together, we found better machine translation performance.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2018-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08907",
        "title": "Input-to-Output Gate to Improve RNN Language Models",
        "authors": [
            "Sho Takase",
            "Jun Suzuki",
            "Masaaki Nagata"
        ],
        "abstract": "This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed method as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09119",
        "title": "Integration of Japanese Papers Into the DBLP Data Set",
        "authors": [
            "Paul Christian Sommerhoff"
        ],
        "abstract": "If someone is looking for a certain publication in the field of computer science, the searching person is likely to use the DBLP to find the desired publication. The DBLP data set is continuously extended with new publications, or rather their metadata, for example the names of involved authors, the title and the publication date. While the size of the data set is already remarkable, specific areas can still be improved. The DBLP offers a huge collection of English papers because most papers concerning computer science are published in English. Nevertheless, there are official publications in other languages which are supposed to be added to the data set. One kind of these are Japanese papers. This diploma thesis will show a way to automatically process publication lists of Japanese papers and to make them ready for an import into the DBLP data set. Especially important are the problems along the way of processing, such as transcription handling and Personal Name Matching with Japanese names.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2017-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09220",
        "title": "Dataset Construction via Attention for Aspect Term Extraction with Distant Supervision",
        "authors": [
            "Athanasios Giannakopoulos",
            "Diego Antognini",
            "Claudiu Musat",
            "Andreea Hossmann",
            "Michael Baeriswyl"
        ],
        "abstract": "Aspect Term Extraction (ATE) detects opinionated aspect terms in sentences or text spans, with the end goal of performing aspect-based sentiment analysis. The small amount of available datasets for supervised ATE and the fact that they cover only a few domains raise the need for exploiting other data sources in new and creative ways. Publicly available review corpora contain a plethora of opinionated aspect terms and cover a larger domain spectrum. In this paper, we first propose a method for using such review corpora for creating a new dataset for ATE. Our method relies on an attention mechanism to select sentences that have a high likelihood of containing actual opinionated aspects. We thus improve the quality of the extracted aspects. We then use the constructed dataset to train a model and perform ATE with distant supervision. By evaluating on human annotated datasets, we prove that our method achieves a significantly improved performance over various unsupervised and supervised baselines. Finally, we prove that sentence selection matters when it comes to creating new datasets for ATE. Specifically, we show that, using a set of selected sentences leads to higher ATE performance compared to using the whole sentence set.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2017-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09239",
        "title": "Predicting Disease-Gene Associations using Cross-Document Graph-based Features",
        "authors": [
            "Hendrik ter Horst",
            "Matthias Hartung",
            "Roman Klinger",
            "Matthias Zwick",
            "Philipp Cimiano"
        ],
        "abstract": "In the context of personalized medicine, text mining methods pose an interesting option for identifying disease-gene associations, as they can be used to generate novel links between diseases and genes which may complement knowledge from structured databases. The most straightforward approach to extract such links from text is to rely on a simple assumption postulating an association between all genes and diseases that co-occur within the same document. However, this approach (i) tends to yield a number of spurious associations, (ii) does not capture different relevant types of associations, and (iii) is incapable of aggregating knowledge that is spread across documents. Thus, we propose an approach in which disease-gene co-occurrences and gene-gene interactions are represented in an RDF graph. A machine learning-based classifier is trained that incorporates features extracted from the graph to separate disease-gene pairs into valid disease-gene associations and spurious ones. On the manually curated Genetic Testing Registry, our approach yields a 30 points increase in F1 score over a plain co-occurrence baseline.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2017-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09250",
        "title": "Lexical Disambiguation in Natural Language Questions (NLQs)",
        "authors": [
            "Omar Al-Harbi",
            "Shaidah Jusoh",
            "Norita Md Norwawi"
        ],
        "abstract": "Question processing is a fundamental step in a question answering (QA) application, and its quality impacts the performance of QA application. The major challenging issue in processing question is how to extract semantic of natural language questions (NLQs). A human language is ambiguous. Ambiguity may occur at two levels; lexical and syntactic. In this paper, we propose a new approach for resolving lexical ambiguity problem by integrating context knowledge and concepts knowledge of a domain, into shallow natural language processing (SNLP) techniques. Concepts knowledge is modeled using ontology, while context knowledge is obtained from WordNet, and it is determined based on neighborhood words in a question. The approach will be applied to a university QA system.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2017-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09254",
        "title": "Learning to Explain Non-Standard English Words and Phrases",
        "authors": [
            "Ke Ni",
            "William Yang Wang"
        ],
        "abstract": "We describe a data-driven approach for automatically explaining new, non-standard English expressions in a given sentence, building on a large dataset that includes 15 years of crowdsourced examples from ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2017-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09360",
        "title": "Learning of Colors from Color Names: Distribution and Point Estimation",
        "authors": [
            "Lyndon White",
            "Roberto Togneri",
            "Wei Liu",
            "Mohammed Bennamoun"
        ],
        "abstract": "Color names are often made up of multiple words. As a task in natural language understanding we investigate in depth the capacity of neural networks based on sums of word embeddings (SOWE), recurrence (LSTM and GRU based RNNs) and convolution (CNN), to estimate colors from sequences of terms. We consider both point and distribution estimates of color. We argue that the latter has a particular value as there is no clear agreement between people as to what a particular color describes -- different people have a different idea of what it means to be ``very dark orange'', for example. Surprisingly, despite it's simplicity, the sum of word embeddings generally performs the best on almost all evaluations.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2020-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09373",
        "title": "A Bimodal Network Approach to Model Topic Dynamics",
        "authors": [
            "Luigi Di Caro",
            "Marco Guerzoni",
            "Massimiliano Nuccio",
            "Giovanni Siragusa"
        ],
        "abstract": "This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Allocation (LDA). The main contribution is the conceptualization of the topic dynamics and its formalization and codification into an algorithm. To benchmark the effectiveness of this approach, we propose three indexes which track the transformation of topics over time, their rate of birth and death, and the novelty of their content. Applying the LDA, we test the algorithm both on a controlled experiment and on a corpus of several thousands of scientific papers over a period of more than 100 years which account for the history of the economic thought.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09404",
        "title": "A Preliminary Study for Building an Arabic Corpus of Pair Questions-Texts from the Web: AQA-Webcorp",
        "authors": [
            "Wided Bakari",
            "Patrice Bellot",
            "Mahmoud Neji"
        ],
        "abstract": "With the development of electronic media and the heterogeneity of Arabic data on the Web, the idea of building a clean corpus for certain applications of natural language processing, including machine translation, information retrieval, question answer, become more and more pressing. In this manuscript, we seek to create and develop our own corpus of pair's questions-texts. This constitution then will provide a better base for our experimentation step. Thus, we try to model this constitution by a method for Arabic insofar as it recovers texts from the web that could prove to be answers to our factual questions. To do this, we had to develop a java script that can extract from a given query a list of html pages. Then clean these pages to the extent of having a data base of texts and a corpus of pair's question-texts. In addition, we give preliminary results of our proposal method. Some investigations for the construction of Arabic corpus are also presented in this document.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09443",
        "title": "Prosodic Features from Large Corpora of Child-Directed Speech as Predictors of the Age of Acquisition of Words",
        "authors": [
            "Lea Frermann",
            "Michael C. Frank"
        ],
        "abstract": "The impressive ability of children to acquire language is a widely studied phenomenon, and the factors influencing the pace and patterns of word learning remains a subject of active research. Although many models predicting the age of acquisition of words have been proposed, little emphasis has been directed to the raw input children achieve. In this work we present a comparatively large-scale multi-modal corpus of prosody-text aligned child directed speech. Our corpus contains automatically extracted word-level prosodic features, and we investigate the utility of this information as predictors of age of acquisition. We show that prosody features boost predictive power in a regularized regression, and demonstrate their utility in the context of a multi-modal factorized language models trained and tested on child-directed speech.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09500",
        "title": "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets",
        "authors": [
            "Rotem Dror",
            "Gili Baumer",
            "Marina Bogomolov",
            "Roi Reichart"
        ],
        "abstract": "With the ever-growing amounts of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper, we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09587",
        "title": "Multi-Label Classification of Patient Notes a Case Study on ICD Code Assignment",
        "authors": [
            "Tal Baumel",
            "Jumana Nassour-Kassis",
            "Raphael Cohen",
            "Michael Elhadad",
            "No`emie Elhadad"
        ],
        "abstract": "In the context of the Electronic Health Record, automated diagnosis coding of patient notes is a useful task, but a challenging one due to the large number of codes and the length of patient notes. We investigate four models for assigning multiple ICD codes to discharge summaries taken from both MIMIC II and III. We present Hierarchical Attention-GRU (HA-GRU), a hierarchical approach to tag a document by identifying the sentences relevant for each label. HA-GRU achieves state-of-the art results. Furthermore, the learned sentence-level attention layer highlights the model decision process, allows easier error analysis, and suggests future directions for improvement.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2017-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09590",
        "title": "An attentive neural architecture for joint segmentation and parsing and its application to real estate ads",
        "authors": [
            "Giannis Bekoulis",
            "Johannes Deleu",
            "Thomas Demeester",
            "Chris Develder"
        ],
        "abstract": "In processing human produced text using natural language processing (NLP) techniques, two fundamental subtasks that arise are (i) segmentation of the plain text into meaningful subunits (e.g., entities), and (ii) dependency parsing, to establish relations between subunits. In this paper, we develop a relatively simple and effective neural joint model that performs both segmentation and dependency parsing together, instead of one after the other as in most state-of-the-art works. We will focus in particular on the real estate ad setting, aiming to convert an ad to a structured description, which we name property tree, comprising the tasks of (1) identifying important entities of a property (e.g., rooms) from classifieds and (2) structuring them into a tree format. In this work, we propose a new joint model that is able to tackle the two tasks simultaneously and construct the property tree by (i) avoiding the error propagation that would arise from the subtasks one after the other in a pipelined fashion, and (ii) exploiting the interactions between the subtasks. For this purpose, we perform an extensive comparative study of the pipeline methods and the new proposed joint model, reporting an improvement of over three percentage points in the overall edge F1 score of the property tree. Also, we propose attention methods, to encourage our model to focus on salient tokens during the construction of the property tree. Thus we experimentally demonstrate the usefulness of attentive neural architectures for the proposed joint model, showcasing a further improvement of two percentage points in edge F1 score for our application.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2018-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09686",
        "title": "Application of a Hybrid Bi-LSTM-CRF model to the task of Russian Named Entity Recognition",
        "authors": [
            "L. T. Anh",
            "M. Y. Arkhipov",
            "M. S. Burtsev"
        ],
        "abstract": "Named Entity Recognition (NER) is one of the most common tasks of the natural language processing. The purpose of NER is to find and classify tokens in text documents into predefined categories called tags, such as person names, quantity expressions, percentage expressions, names of locations, organizations, as well as expression of time, currency and others. Although there is a number of approaches have been proposed for this task in Russian language, it still has a substantial potential for the better solutions. In this work, we studied several deep neural network models starting from vanilla Bi-directional Long Short-Term Memory (Bi-LSTM) then supplementing it with Conditional Random Fields (CRF) as well as highway networks and finally adding external word embeddings. All models were evaluated across three datasets: Gareev's dataset, Person-1000, FactRuEval-2016. We found that extension of Bi-LSTM model with CRF significantly increased the quality of predictions. Encoding input tokens with external word embeddings reduced training time and allowed to achieve state of the art for the Russian NER task.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2017-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09749",
        "title": "KeyVec: Key-semantics Preserving Document Representations",
        "authors": [
            "Bin Bi",
            "Hao Ma"
        ],
        "abstract": "Previous studies have demonstrated the empirical success of word embeddings in various applications. In this paper, we investigate the problem of learning distributed representations for text documents which many machine learning algorithms take as input for a number of NLP tasks.\n",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09783",
        "title": "A Deep Neural Network Approach To Parallel Sentence Extraction",
        "authors": [
            "Francis Gr\u00e9goire",
            "Philippe Langlais"
        ],
        "abstract": "Parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. We propose an end-to-end deep neural network approach to detect translational equivalence between sentences in two different languages. In contrast to previous approaches, which typically rely on multiples models and various word alignment features, by leveraging continuous vector representation of sentences we remove the need of any domain specific feature engineering. Using a siamese bidirectional recurrent neural networks, our results against a strong baseline based on a state-of-the-art parallel sentence extraction system show a significant improvement in both the quality of the extracted parallel sentences and the translation performance of statistical machine translation systems. We believe this study is the first one to investigate deep learning for the parallel sentence extraction task.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09816",
        "title": "Edina: Building an Open Domain Socialbot with Self-dialogues",
        "authors": [
            "Ben Krause",
            "Marco Damonte",
            "Mihai Dobre",
            "Daniel Duma",
            "Joachim Fainberg",
            "Federico Fancellu",
            "Emmanuel Kahembwe",
            "Jianpeng Cheng",
            "Bonnie Webber"
        ],
        "abstract": "We present Edina, the University of Edinburgh's social bot for the Amazon Alexa Prize competition. Edina is a conversational agent whose responses utilize data harvested from Amazon Mechanical Turk (AMT) through an innovative new technique we call self-dialogues. These are conversations in which a single AMT Worker plays both participants in a dialogue. Such dialogues are surprisingly natural, efficient to collect and reflective of relevant and/or trending topics. These self-dialogues provide training data for a generative neural network as well as a basis for soft rules used by a matching score component. Each match of a soft rule against a user utterance is associated with a confidence score which we show is strongly indicative of reply quality, allowing this component to self-censor and be effectively integrated with other components. Edina's full architecture features a rule-based system backing off to a matching score, backing off to a generative neural network. Our hybrid data-driven methodology thus addresses both coverage limitations of a strictly rule-based approach and the lack of guarantees of a strictly machine-learning approach.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09885",
        "title": "Sentiment Classification with Word Attention based on Weakly Supervised Learning with a Convolutional Neural Network",
        "authors": [
            "Gichang Lee",
            "Jaeyun Jeong",
            "Seungwan Seo",
            "CzangYeob Kim",
            "Pilsung Kang"
        ],
        "abstract": "In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is no information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. In order to verify the proposed methodology, we evaluated the classification accuracy and inclusion rate of polarity words using two movie review datasets. Experimental result show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10053",
        "title": "Graph Convolutional Networks for Named Entity Recognition",
        "authors": [
            "A. Cetoli",
            "S. Bragaglia",
            "A.D. O'Harney",
            "M. Sloan"
        ],
        "abstract": "In this paper we investigate the role of the dependency tree in a named entity recognizer upon using a set of GCN. We perform a comparison among different NER architectures and show that the grammar of a sentence positively influences the results. Experiments on the ontonotes dataset demonstrate consistent performance improvements, without requiring heavy feature engineering nor additional language-specific knowledge.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2018-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10159",
        "title": "A Web of Hate: Tackling Hateful Speech in Online Social Spaces",
        "authors": [
            "Haji Mohammad Saleem",
            "Kelly P Dillon",
            "Susan Benesch",
            "Derek Ruths"
        ],
        "abstract": "Online social platforms are beset with hateful speech - content that expresses hatred for a person or group of people. Such content can frighten, intimidate, or silence platform users, and some of it can inspire other users to commit violence. Despite widespread recognition of the problems posed by such content, reliable solutions even for detecting hateful speech are lacking. In the present work, we establish why keyword-based methods are insufficient for detection. We then propose an approach to detecting hateful speech that uses content produced by self-identifying hateful communities as training data. Our approach bypasses the expensive annotation process often required to train keyword systems and performs well across several established platforms, making substantial improvements over current state-of-the-art approaches.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10191",
        "title": "Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks",
        "authors": [
            "Mingbo Ma",
            "Kai Zhao",
            "Liang Huang",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "Sentence-level classification and sequential labeling are two fundamental tasks in language understanding. While these two tasks are usually modeled separately, in reality, they are often correlated, for example in intent classification and slot filling, or in topic classification and named-entity recognition. In order to utilize the potential benefits from their correlations, we propose a jointly trained model for learning the two tasks simultaneously via Long Short-Term Memory (LSTM) networks. This model predicts the sentence-level category and the word-level label sequence from the stepwise output hidden representations of LSTM. We also introduce a novel mechanism of \"sparse attention\" to weigh words differently based on their semantic relevance to sentence-level classification. The proposed method outperforms baseline models on ATIS and TREC datasets.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10204",
        "title": "A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering",
        "authors": [
            "Bin Bi",
            "Hao Ma"
        ],
        "abstract": "This paper proposes a novel neural machine reading model for open-domain question answering at scale. Existing machine comprehension models typically assume that a short piece of relevant text containing answers is already identified and given to the models, from which the models are designed to extract answers. This assumption, however, is not realistic for building a large-scale open-domain question answering system which requires both deep text understanding and identifying relevant text from corpus simultaneously.\n",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2017-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10217",
        "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
        "authors": [
            "Wei-Nan Zhang",
            "Zhigang Chen",
            "Wanxiang Che",
            "Guoping Hu",
            "Ting Liu"
        ],
        "abstract": "In this paper, we introduce the first evaluation of Chinese human-computer dialogue technology. We detail the evaluation scheme, tasks, metrics and how to collect and annotate the data for training, developing and test. The evaluation includes two tasks, namely user intent classification and online testing of task-oriented dialogue. To consider the different sources of the data for training and developing, the first task can also be divided into two sub tasks. Both the two tasks are coming from the real problems when using the applications developed by industry. The evaluation data is provided by the iFLYTEK Corporation. Meanwhile, in this paper, we publish the evaluation results to present the current performance of the participants in the two tasks of Chinese human-computer dialogue technology. Moreover, we analyze the existing problems of human-computer dialogue as well as the evaluation scheme itself.\n    ",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2019-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10367",
        "title": "Structured Embedding Models for Grouped Data",
        "authors": [
            "Maja Rudolph",
            "Francisco Ruiz",
            "Susan Athey",
            "David Blei"
        ],
        "abstract": "Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S-EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the co-purchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how S-EFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10381",
        "title": "Towards Universal Semantic Tagging",
        "authors": [
            "Lasha Abzianidze",
            "Johan Bos"
        ],
        "abstract": "The paper proposes the task of universal semantic tagging---tagging word tokens with language-neutral, semantically informative tags. We argue that the task, with its independent nature, contributes to better semantic analysis for wide-coverage multilingual text. We present the initial version of the semantic tagset and show that (a) the tags provide semantically fine-grained information, and (b) they are suitable for cross-lingual semantic parsing. An application of the semantic tagging in the Parallel Meaning Bank supports both of these points as the tags contribute to formal lexical semantics and their cross-lingual projection. As a part of the application, we annotate a small corpus with the semantic tags and present new baseline result for universal semantic tagging.\n    ",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10423",
        "title": "Learning how to learn: an adaptive dialogue agent for incrementally learning visually grounded word meanings",
        "authors": [
            "Yanchao Yu",
            "Arash Eshghi",
            "Oliver Lemon"
        ],
        "abstract": "We present an optimised multi-modal dialogue agent for interactive learning of visually grounded word meanings from a human tutor, trained on real human-human tutoring data. Within a life-long interactive learning period, the agent, trained using Reinforcement Learning (RL), must be able to handle natural conversations with human users and achieve good learning performance (accuracy) while minimising human effort in the learning process. We train and evaluate this system in interaction with a simulated human tutor, which is built on the BURCHAK corpus -- a Human-Human Dialogue dataset for the visual learning task. The results show that: 1) The learned policy can coherently interact with the simulated user to achieve the goal of the task (i.e. learning visual attributes of objects, e.g. colour and shape); and 2) it finds a better trade-off between classifier accuracy and tutoring costs than hand-crafted rule-based policies, including ones with dynamic policies.\n    ",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10426",
        "title": "Training an adaptive dialogue policy for interactive learning of visually grounded word meanings",
        "authors": [
            "Yanchao Yu",
            "Arash Eshghi",
            "Oliver Lemon"
        ],
        "abstract": "We present a multi-modal dialogue system for interactive learning of perceptually grounded word meanings from a human tutor. The system integrates an incremental, semantic parsing/generation framework - Dynamic Syntax and Type Theory with Records (DS-TTR) - with a set of visual classifiers that are learned throughout the interaction and which ground the meaning representations that it produces. We use this system in interaction with a simulated human tutor to study the effects of different dialogue policies and capabilities on the accuracy of learned meanings, learning rates, and efforts/costs to the tutor. We show that the overall performance of the learning agent is affected by (1) who takes initiative in the dialogues; (2) the ability to express/use their confidence level about visual attributes; and (3) the ability to process elliptical and incrementally constructed dialogue turns. Ultimately, we train an adaptive dialogue policy which optimises the trade-off between classifier accuracy and tutoring costs.\n    ",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10431",
        "title": "The BURCHAK corpus: a Challenge Data Set for Interactive Learning of Visually Grounded Word Meanings",
        "authors": [
            "Yanchao Yu",
            "Arash Eshghi",
            "Gregory Mills",
            "Oliver Joseph Lemon"
        ],
        "abstract": "We motivate and describe a new freely available human-human dialogue dataset for interactive learning of visually grounded word meanings through ostensive definition by a tutor to a learner. The data has been collected using a novel, character-by-character variant of the DiET chat tool (Healey et al., 2003; Mills and Healey, submitted) with a novel task, where a Learner needs to learn invented visual attribute words (such as \" burchak \" for square) from a tutor. As such, the text-based interactions closely resemble face-to-face conversation and thus contain many of the linguistic phenomena encountered in natural, spontaneous dialogue. These include self-and other-correction, mid-sentence continuations, interruptions, overlaps, fillers, and hedges. We also present a generic n-gram framework for building user (i.e. tutor) simulations from this type of incremental data, which is freely available to researchers. We show that the simulations produce outputs that are similar to the original data (e.g. 78% turn match similarity). Finally, we train and evaluate a Reinforcement Learning dialogue control agent for learning visually grounded word meanings, trained from the BURCHAK corpus. The learned policy shows comparable performance to a rule-based system built previously.\n    ",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10445",
        "title": "Synonym Discovery with Etymology-based Word Embeddings",
        "authors": [
            "Seunghyun Yoon",
            "Pablo Estrada",
            "Kyomin Jung"
        ],
        "abstract": "We propose a novel approach to learn word embeddings based on an extended version of the distributional hypothesis. Our model derives word embedding vectors using the etymological composition of words, rather than the context in which they appear. It has the strength of not requiring a large text corpus, but instead it requires reliable access to etymological roots of words, making it specially fit for languages with logographic writing systems. The model consists on three steps: (1) building an etymological graph, which is a bipartite network of words and etymological roots, (2) obtaining the biadjacency matrix of the etymological graph and reducing its dimensionality, (3) using columns/rows of the resulting matrices as embedding vectors. We test our model in the Chinese and Sino-Korean vocabularies. Our graphs are formed by a set of 117,000 Chinese words, and a set of 135,000 Sino-Korean words. In both cases we show that our model performs well in the task of synonym discovery.\n    ",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2017-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.10486",
        "title": "Symbol, Conversational, and Societal Grounding with a Toy Robot",
        "authors": [
            "Casey Kennington",
            "Sarah Plane"
        ],
        "abstract": "Essential to meaningful interaction is grounding at the symbolic, conversational, and societal levels. We present ongoing work with Anki's Cozmo toy robot as a research platform where we leverage the recent words-as-classifiers model of lexical semantics in interactive reference resolution tasks for language grounding.\n    ",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00164",
        "title": "Speaker Role Contextual Modeling for Language Understanding and Dialogue Policy Learning",
        "authors": [
            "Ta-Chung Chi",
            "Po-Chun Chen",
            "Shang-Yu Su",
            "Yun-Nung Chen"
        ],
        "abstract": "Language understanding (LU) and dialogue policy learning are two essential components in conversational systems. Human-human dialogues are not well-controlled and often random and unpredictable due to their own goals and speaking habits. This paper proposes a role-based contextual model to consider different speaker roles independently based on the various speaking patterns in the multi-turn dialogues. The experiments on the benchmark dataset show that the proposed role-based model successfully learns role-specific behavioral patterns for contextual encoding and then significantly improves language understanding and dialogue policy learning tasks.\n    ",
        "submission_date": "2017-09-30T00:00:00",
        "last_modified_date": "2017-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00165",
        "title": "Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding",
        "authors": [
            "Po-Chun Chen",
            "Ta-Chung Chi",
            "Shang-Yu Su",
            "Yun-Nung Chen"
        ],
        "abstract": "Spoken language understanding (SLU) is an essential component in conversational systems. Most SLU component treats each utterance independently, and then the following components aggregate the multi-turn information in the separate phases. In order to avoid error propagation and effectively utilize contexts, prior work leveraged history for contextual SLU. However, the previous model only paid attention to the content in history utterances without considering their temporal information and speaker roles. In the dialogues, the most recent utterances should be more important than the least recent ones. Furthermore, users usually pay attention to 1) self history for reasoning and 2) others' utterances for listening, the speaker of the utterances may provides informative cues to help understanding. Therefore, this paper proposes an attention-based network that additionally leverages temporal information and speaker role for better SLU, where the attention to contexts and speaker roles can be automatically learned in an end-to-end manner. The experiments on the benchmark Dialogue State Tracking Challenge 4 (DSTC4) dataset show that the time-aware dynamic role attention networks significantly improve the understanding performance.\n    ",
        "submission_date": "2017-09-30T00:00:00",
        "last_modified_date": "2017-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00205",
        "title": "Bag-of-Vector Embeddings of Dependency Graphs for Semantic Induction",
        "authors": [
            "Diana Nicoleta Popa",
            "James Henderson"
        ],
        "abstract": "Vector-space models, from word embeddings to neural network parsers, have many advantages for NLP. But how to generalise from fixed-length word vectors to a vector space for arbitrary linguistic structures is still unclear. In this paper we propose bag-of-vector embeddings of arbitrary linguistic graphs. A bag-of-vector space is the minimal nonparametric extension of a vector space, allowing the representation to grow with the size of the graph, but not tying the representation to any specific tree or graph structure. We propose efficient training and inference algorithms based on tensor factorisation for embedding arbitrary graphs in a bag-of-vector space. We demonstrate the usefulness of this representation by training bag-of-vector embeddings of dependency graphs and evaluating them on unsupervised semantic induction for the Semantic Textual Similarity and Natural Language Inference tasks.\n    ",
        "submission_date": "2017-09-30T00:00:00",
        "last_modified_date": "2017-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00273",
        "title": "What Words Do We Use to Lie?: Word Choice in Deceptive Messages",
        "authors": [
            "Jason Xiaotian Dou",
            "Michelle Liu",
            "Haaris Muneer",
            "Adam Schlussel"
        ],
        "abstract": "Text messaging is the most widely used form of computer-mediated communication (CMC). Previous findings have shown that linguistic factors can reliably indicate messages as deceptive. For example, users take longer and use more words to craft deceptive messages than they do truthful messages. Existing research has also examined how factors, such as student status and gender, affect rates of deception and word choice in deceptive messages. However, this research has been limited by small sample sizes and has returned contradicting findings. This paper aims to address these issues by using a dataset of text messages collected from a large and varied set of participants using an Android messaging application. The results of this paper show significant differences in word choice and frequency of deceptive messages between male and female participants, as well as between students and non-students.\n    ",
        "submission_date": "2017-10-01T00:00:00",
        "last_modified_date": "2022-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00341",
        "title": "Fully Automated Fact Checking Using External Sources",
        "authors": [
            "Georgi Karadzhov",
            "Preslav Nakov",
            "Lluis Marquez",
            "Alberto Barron-Cedeno",
            "Ivan Koychev"
        ],
        "abstract": "Given the constantly growing proliferation of false claims online in recent years, there has been also a growing research interest in automatically distinguishing false rumors from factually true claims. Here, we propose a general-purpose framework for fully-automatic fact checking using external sources, tapping the potential of the entire Web as a knowledge source to confirm or reject a claim. Our framework uses a deep neural network with LSTM text encoding to combine semantic kernels with task-specific embeddings that encode a claim together with pieces of potentially-relevant text fragments from the Web, taking the source reliability into account. The evaluation results show good performance on two different tasks and datasets: (i) rumor detection and (ii) fact checking of the answers to a question in community question answering forums.\n    ",
        "submission_date": "2017-10-01T00:00:00",
        "last_modified_date": "2017-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00346",
        "title": "Robust Tuning Datasets for Statistical Machine Translation",
        "authors": [
            "Preslav Nakov",
            "Stephan Vogel"
        ],
        "abstract": "We explore the idea of automatically crafting a tuning dataset for Statistical Machine Translation (SMT) that makes the hyper-parameters of the SMT system more robust with respect to some specific deficiencies of the parameter tuning algorithms. This is an under-explored research direction, which can allow better parameter tuning. In this paper, we achieve this goal by selecting a subset of the available sentence pairs, which are more suitable for specific combinations of optimizers, objective functions, and evaluation measures. We demonstrate the potential of the idea with the pairwise ranking optimization (PRO) optimizer, which is known to yield too short translations. We show that the learning problem can be alleviated by tuning on a subset of the development set, selected based on sentence length. In particular, using the longest 50% of the tuning sentences, we achieve two-fold tuning speedup, and improvements in BLEU score that rival those of alternatives, which fix BLEU+1's smoothing instead.\n    ",
        "submission_date": "2017-10-01T00:00:00",
        "last_modified_date": "2017-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00372",
        "title": "Mathematical foundations of matrix syntax",
        "authors": [
            "Roman Orus",
            "Roger Martin",
            "Juan Uriagereka"
        ],
        "abstract": "Matrix syntax is a formal model of syntactic relations in language. The purpose of this paper is to explain its mathematical foundations, for an audience with some formal background. We make an axiomatic presentation, motivating each axiom on linguistic and practical grounds. The resulting mathematical structure resembles some aspects of quantum mechanics. Matrix syntax allows us to describe a number of language phenomena that are otherwise very difficult to explain, such as linguistic chains, and is arguably a more economical theory of language than most of the theories proposed in the context of the minimalist program in linguistics. In particular, sentences are naturally modelled as vectors in a Hilbert space with a tensor product structure, built from 2x2 matrices belonging to some specific group.\n    ",
        "submission_date": "2017-10-01T00:00:00",
        "last_modified_date": "2019-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00453",
        "title": "Visual Reasoning with Natural Language",
        "authors": [
            "Stephanie Zhou",
            "Alane Suhr",
            "Yoav Artzi"
        ],
        "abstract": "Natural language provides a widely accessible and expressive interface for robotic agents. To understand language in complex environments, agents must reason about the full range of language inputs and their correspondence to the world. Such reasoning over language and vision is an open problem that is receiving increasing attention. While existing data sets focus on visual diversity, they do not display the full range of natural language expressions, such as counting, set reasoning, and comparisons.\n",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2017-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00477",
        "title": "A Crowd-Annotated Spanish Corpus for Humor Analysis",
        "authors": [
            "Santiago Castro",
            "Luis Chiruzzo",
            "Aiala Ros\u00e1",
            "Diego Garat",
            "Guillermo Moncecchi"
        ],
        "abstract": "Computational Humor involves several tasks, such as humor recognition, humor generation, and humor scoring, for which it is useful to have human-curated data. In this work we present a corpus of 27,000 tweets written in Spanish and crowd-annotated by their humor value and funniness score, with about four annotations per tweet, tagged by 1,300 people over the Internet. It is equally divided between tweets coming from humorous and non-humorous accounts. The inter-annotator agreement Krippendorff's alpha value is 0.5710. The dataset is available for general use and can serve as a basis for humor detection and as a first step to tackle subjectivity.\n    ",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2018-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00519",
        "title": "Attentive Convolution: Equipping CNNs with RNN-style Attention Mechanisms",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "In NLP, convolutional neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that this is because the attention in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into convolution). Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in the input text t^x. In this work, we propose an attentive convolution network, ATTCONV. It extends the context scope of the convolution operation, deriving higher-level features for a word not only from local context, but also information extracted from nonlocal context by the attention mechanism commonly used in RNNs. This nonlocal context can come (i) from parts of the input text t^x that are distant or (ii) from extra (i.e., external) contexts t^y. Experiments on sentence modeling with zero-context (sentiment analysis), single-context (textual entailment) and multiple-context (claim verification) demonstrate the effectiveness of ATTCONV in sentence representation learning with the incorporation of context. In particular, attentive convolution outperforms attentive pooling and is a strong competitor to popular attentive RNNs.\n    ",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2018-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00641",
        "title": "Improving speech recognition by revising gated recurrent units",
        "authors": [
            "Mirco Ravanelli",
            "Philemon Brakel",
            "Maurizio Omologo",
            "Yoshua Bengio"
        ],
        "abstract": "Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just two multiplicative gates.\n",
        "submission_date": "2017-09-29T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00683",
        "title": "The Dependence of Frequency Distributions on Multiple Meanings of Words, Codes and Signs",
        "authors": [
            "Xiaoyong Yan",
            "Petter Minnhagen"
        ],
        "abstract": "The dependence of the frequency distributions due to multiple meanings of words in a text is investigated by deleting letters. By coding the words with fewer letters the number of meanings per coded word increases. This increase is measured and used as an input in a predictive theory. For a text written in English, the word-frequency distribution is broad and fat-tailed, whereas if the words are only represented by their first letter the distribution becomes exponential. Both distribution are well predicted by the theory, as is the whole sequence obtained by consecutively representing the words by the first L=6,5,4,3,2,1 letters. Comparisons of texts written by Chinese characters and the same texts written by letter-codes are made and the similarity of the corresponding frequency-distributions are interpreted as a consequence of the multiple meanings of Chinese characters. This further implies that the difference of the shape for word-frequencies for an English text written by letters and a Chinese text written by Chinese characters is due to the coding and not to the language per se.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00689",
        "title": "Building Chatbots from Forum Data: Model Selection Using Question Answering Metrics",
        "authors": [
            "Martin Boyanov",
            "Ivan Koychev",
            "Preslav Nakov",
            "Alessandro Moschitti",
            "Giovanni Da San Martino"
        ],
        "abstract": "We propose to use question answering (QA) data from Web forums to train chatbots from scratch, i.e., without dialog training data. First, we extract pairs of question and answer sentences from the typically much longer texts of questions and answers in a forum. We then use these shorter texts to train seq2seq models in a more efficient way. We further improve the parameter optimization using a new model selection strategy based on QA measures. Finally, we propose to use extrinsic evaluation with respect to a QA task as an automatic evaluation method for chatbots. The evaluation shows that the model achieves a MAP of 63.5% on the extrinsic task. Moreover, it can answer correctly 49.5% of the questions when they are similar to questions asked in the forum, and 47.3% of the questions when they are more conversational in style.\n    ",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2017-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00803",
        "title": "Compiling and Processing Historical and Contemporary Portuguese Corpora",
        "authors": [
            "Marcos Zampieri"
        ],
        "abstract": "This technical report describes the framework used for processing three large Portuguese corpora. Two corpora contain texts from newspapers, one published in Brazil and the other published in Portugal. The third corpus is Colonia, a historical Portuguese collection containing texts written between the 16th and the early 20th century. The report presents pre-processing methods, segmentation, and annotation of the corpora as well as indexing and querying methods. Finally, it presents published research papers using the corpora.\n    ",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2017-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00880",
        "title": "Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection",
        "authors": [
            "Haw-Shiuan Chang",
            "ZiYun Wang",
            "Luke Vilnis",
            "Andrew McCallum"
        ],
        "abstract": "Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, coreference, relation extraction, and question answering. Supervised learning from labeled hypernym sources, such as WordNet, limits the coverage of these models, which can be addressed by learning hypernyms from unlabeled text. Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts in a low-dimensional and interpretable space. In experimental evaluations more comprehensive than any previous literature of which we are aware-evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions-we find that our method provides up to double the precision of previous unsupervised embeddings, and the highest average performance, using a much more compact word representation, and yielding many new state-of-the-art results.\n    ",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2018-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00923",
        "title": "Minimal Dependency Translation: a Framework for Computer-Assisted Translation for Under-Resourced Languages",
        "authors": [
            "Michael Gasser"
        ],
        "abstract": "This paper introduces Minimal Dependency Translation (MDT), an ongoing project to develop a rule-based framework for the creation of rudimentary bilingual lexicon-grammars for machine translation and computer-assisted translation into and out of under-resourced languages as well as initial steps towards an implementation of MDT for English-to-Amharic translation. The basic units in MDT, called groups, are headed multi-item sequences. In addition to wordforms, groups may contain lexemes, syntactic-semantic categories, and grammatical features. Each group is associated with one or more translations, each of which is a group in a target language. During translation, constraint satisfaction is used to select a set of source-language groups for the input sentence and to sequence the words in the associated target-language groups.\n    ",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2017-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00936",
        "title": "Identifying Nominals with No Head Match Co-references Using Deep Learning",
        "authors": [
            "M. Stone",
            "R. Arora"
        ],
        "abstract": "Identifying nominals with no head match is a long-standing challenge in coreference resolution with current systems performing significantly worse than humans. In this paper we present a new neural network architecture which outperforms the current state-of-the-art system on the English portion of the CoNLL 2012 Shared Task. This is done by using a logistic regression on features produced by two submodels, one of which is has the architecture proposed in [CM16a] while the other combines domain specific embeddings of the antecedent and the mention. We also propose some simple additional features which seem to improve performance for all models substantially, increasing F1 by almost 4% on basic logistic regression and other complex models.\n    ",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2017-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00969",
        "title": "Event Identification as a Decision Process with Non-linear Representation of Text",
        "authors": [
            "Yukun Yan",
            "Daqi Zheng",
            "Zhengdong Lu",
            "Sen Song"
        ],
        "abstract": "We propose scale-free Identifier Network(sfIN), a novel model for event identification in documents. In general, sfIN first encodes a document into multi-scale memory stacks, then extracts special events via conducting multi-scale actions, which can be considered as a special type of sequence labelling. The design of large scale actions makes it more efficient processing a long document. The whole model is trained with both supervised learning and reinforcement learning.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00987",
        "title": "Annotation and Detection of Emotion in Text-based Dialogue Systems with CNN",
        "authors": [
            "Jialiang Zhao",
            "Qi Gao"
        ],
        "abstract": "Knowledge of users' emotion states helps improve human-computer interaction. In this work, we presented EmoNet, an emotion detector of Chinese daily dialogues based on deep convolutional neural networks. In order to maintain the original linguistic features, such as the order, commonly used methods like segmentation and keywords extraction were not adopted, instead we increased the depth of CNN and tried to let CNN learn inner linguistic relationships. Our main contribution is that we presented a new model and a new pipeline which can be used in multi-language environment to solve sentimental problems. Experimental results shows EmoNet has a great capacity in learning the emotion of dialogues and achieves a better result than other state of art detectors do.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00998",
        "title": "Is Structure Necessary for Modeling Argument Expectations in Distributional Semantics?",
        "authors": [
            "Emmanuele Chersoni",
            "Enrico Santus",
            "Philippe Blache",
            "Alessandro Lenci"
        ],
        "abstract": "Despite the number of NLP studies dedicated to thematic fit estimation, little attention has been paid to the related task of composing and updating verb argument expectations. The few exceptions have mostly modeled this phenomenon with structured distributional models, implicitly assuming a similarly structured representation of events. Recent experimental evidence, however, suggests that human processing system could also exploit an unstructured \"bag-of-arguments\" type of event representation to predict upcoming input. In this paper, we re-implement a traditional structured model and adapt it to compare the different hypotheses concerning the degree of structure in our event knowledge, evaluating their relative performance in the task of the argument expectations update.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01025",
        "title": "MMCR4NLP: Multilingual Multiway Corpora Repository for Natural Language Processing",
        "authors": [
            "Raj Dabre",
            "Sadao Kurohashi"
        ],
        "abstract": "Multilinguality is gradually becoming ubiquitous in the sense that more and more researchers have successfully shown that using additional languages help improve the results in many Natural Language Processing tasks. Multilingual Multiway Corpora (MMC) contain the same sentence in multiple languages. Such corpora have been primarily used for Multi-Source and Pivot Language Machine Translation but are also useful for developing multilingual sequence taggers by transfer learning. While these corpora are available, they are not organized for multilingual experiments and researchers need to write boilerplate code every time they want to use said corpora. Moreover, because there is no official MMC collection it becomes difficult to compare against existing approaches. As such we present our work on creating a unified and systematically organized repository of MMC spanning a large number of languages. We also provide training, development and test splits for corpora where official splits are unavailable. We hope that this will help speed up the pace of multilingual NLP research and ensure that NLP researchers obtain results that are more trustable since they can be compared easily. We indicate corpora sources, extraction procedures if any and relevant statistics. We also make our collection public for research purposes.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2019-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01095",
        "title": "Towards an Inferential Lexicon of Event Selecting Predicates for French",
        "authors": [
            "Ingrid Falk",
            "Fabienne Martin"
        ],
        "abstract": "We present a manually constructed seed lexicon encoding the inferential profiles of French event selecting predicates across different uses. The inferential profile (Karttunen, 1971a) of a verb is designed to capture the inferences triggered by the use of this verb in context. It reflects the influence of the clause-embedding verb on the factuality of the event described by the embedded clause. The resource developed provides evidence for the following three hypotheses: (i) French implicative verbs have an aspect dependent profile (their inferential profile varies with outer aspect), while factive verbs have an aspect independent profile (they keep the same inferential profile with both imperfective and perfective aspect); (ii) implicativity decreases with imperfective aspect: the inferences triggered by French implicative verbs combined with perfective aspect are often weakened when the same verbs are combined with imperfective aspect; (iii) implicativity decreases with an animate (deep) subject: the inferences triggered by a verb which is implicative with an inanimate subject are weakened when the same verb is used with an animate subject. The resource additionally shows that verbs with different inferential profiles display clearly distinct sub-categorisation patterns. In particular, verbs that have both factive and implicative readings are shown to prefer infinitival clauses in their implicative reading, and tensed clauses in their factive reading.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01329",
        "title": "Improving Lexical Choice in Neural Machine Translation",
        "authors": [
            "Toan Q. Nguyen",
            "David Chiang"
        ],
        "abstract": "We explore two solutions to the problem of mistranslating rare words in neural machine translation. First, we argue that the standard output layer, which computes the inner product of a vector representing the context with all possible output word embeddings, rewards frequent words disproportionately, and we propose to fix the norms of both vectors to a constant value. Second, we integrate a simple lexical module which is jointly trained with the rest of the model. We evaluate our approaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrase-based translation in nearly all settings.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2018-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01411",
        "title": "Transferring Semantic Roles Using Translation and Syntactic Information",
        "authors": [
            "Maryam Aminian",
            "Mohammad Sadegh Rasooli",
            "Mona Diab"
        ],
        "abstract": "Our paper addresses the problem of annotation projection for semantic role labeling for resource-poor languages using supervised annotations from a resource-rich language through parallel data. We propose a transfer method that employs information from source and target syntactic dependencies as well as word alignment density to improve the quality of an iterative bootstrapping method. Our experiments yield a $3.5$ absolute labeled F-score improvement over a standard annotation projection method.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01487",
        "title": "Cross-Language Question Re-Ranking",
        "authors": [
            "Giovanni Da San Martino",
            "Salvatore Romeo",
            "Alberto Barron-Cedeno",
            "Shafiq Joty",
            "Lluis Marquez",
            "Alessandro Moschitti",
            "Preslav Nakov"
        ],
        "abstract": "We study how to find relevant questions in community forums when the language of the new questions is different from that of the existing questions in the forum. In particular, we explore the Arabic-English language pair. We compare a kernel-based system with a feed-forward neural network in a scenario where a large parallel corpus is available for training a machine translation system, bilingual dictionaries, and cross-language word embeddings. We observe that both approaches degrade the performance of the system when working on the translated text, especially the kernel-based system, which depends heavily on a syntactic kernel. We address this issue using a cross-language tree kernel, which compares the original Arabic tree to the English trees of the related questions. We show that this kernel almost closes the performance gap with respect to the monolingual system. On the neural network side, we use the parallel corpus to train cross-language embeddings, which we then use to represent the Arabic input and the English related questions in the same space. The results also improve to close to those of the monolingual neural network. Overall, the kernel system shows a better performance compared to the neural network in all cases.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01492",
        "title": "Semantic Sentiment Analysis of Twitter Data",
        "authors": [
            "Preslav Nakov"
        ],
        "abstract": "Internet and the proliferation of smart mobile devices have changed the way information is created, shared, and spreads, e.g., microblogs such as Twitter, weblogs such as LiveJournal, social networks such as Facebook, and instant messengers such as Skype and WhatsApp are now commonly used to share thoughts and opinions about anything in the surrounding world. This has resulted in the proliferation of social media content, thus creating new opportunities to study public opinion at a scale that was never possible before. Naturally, this abundance of data has quickly attracted business and research interest from various fields including marketing, political science, and social studies, among many others, which are interested in questions like these: Do people like the new Apple Watch? Do Americans support ObamaCare? How do Scottish feel about the Brexit? Answering these questions requires studying the sentiment of opinions people express in social media, which has given rise to the fast growth of the field of sentiment analysis in social media, with Twitter being especially popular for research due to its scale, representativeness, variety of topics discussed, as well as ease of public access to its messages. Here we present an overview of work on sentiment analysis on Twitter.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01504",
        "title": "Discourse Structure in Machine Translation Evaluation",
        "authors": [
            "Shafiq Joty",
            "Francisco Guzm\u00e1n",
            "Llu\u00eds M\u00e0rquez",
            "Preslav Nakov"
        ],
        "abstract": "In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation. We first design discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST). Then, we show that a simple linear combination with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment- and at the system-level. This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could be taken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DiscoTKparty. We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse trees for machine translation evaluation. In particular we show that: (i) all aspects of the RST tree are relevant, (ii) nuclearity is more useful than relation type, and (iii) the similarity of the translation RST tree to the reference tree is positively correlated with translation quality.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01779",
        "title": "Building a Web-Scale Dependency-Parsed Corpus from CommonCrawl",
        "authors": [
            "Alexander Panchenko",
            "Eugen Ruppert",
            "Stefano Faralli",
            "Simone Paolo Ponzetto",
            "Chris Biemann"
        ],
        "abstract": "We present DepCC, the largest-to-date linguistically analyzed corpus in English including 365 million documents, composed of 252 billion tokens and 7.5 billion of named entity occurrences in 14.3 billion sentences from a web-scale crawl of the \\textsc{Common Crawl} project. The sentences are processed with a dependency parser and with a named entity tagger and contain provenance information, enabling various applications ranging from training syntax-based word embeddings to open information extraction and question answering. We built an index of all sentences and their linguistic meta-data enabling quick search across the corpus. We demonstrate the utility of this corpus on the verb similarity task by showing that a distributional model trained on our corpus yields better results than models trained on smaller corpora, like Wikipedia. This distributional model outperforms the state of art models of verb similarity trained on smaller corpora on the SimVerb3500 dataset.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2018-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01789",
        "title": "Enhanced Neural Machine Translation by Learning from Draft",
        "authors": [
            "Aodong Li",
            "Shiyue Zhang",
            "Dong Wang",
            "Thomas Fang Zheng"
        ],
        "abstract": "Neural machine translation (NMT) has recently achieved impressive results. A potential problem of the existing NMT algorithm, however, is that the decoding is conducted from left to right, without considering the right context. This paper proposes an two-stage approach to solve the problem. In the first stage, a conventional attention-based NMT system is used to produce a draft translation, and in the second stage, a novel double-attention NMT system is used to refine the translation, by looking at the original input as well as the draft translation. This drafting-and-refinement can obtain the right-context information from the draft, hence producing more consistent translations. We evaluated this approach using two Chinese-English translation tasks, one with 44k pairs and 1M pairs respectively. The experiments showed that our approach achieved positive improvements over the conventional NMT system: the improvements are 2.4 and 0.9 BLEU points on the small-scale and large-scale tasks, respectively.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01799",
        "title": "Counterfactual Language Model Adaptation for Suggesting Phrases",
        "authors": [
            "Kenneth C. Arnold",
            "Kai-Wei Chang",
            "Adam T. Kalai"
        ],
        "abstract": "Mobile devices use language models to suggest words and phrases for use in text entry. Traditional language models are based on contextual word frequency in a static corpus of text. However, certain types of phrases, when offered to writers as suggestions, may be systematically chosen more often than their frequency would predict. In this paper, we propose the task of generating suggestions that writers accept, a related but distinct task to making accurate predictions. Although this task is fundamentally interactive, we propose a counterfactual setting that permits offline training and evaluation. We find that even a simple language model can capture text characteristics that improve acceptability.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01809",
        "title": "Syntactic and Semantic Features For Code-Switching Factored Language Models",
        "authors": [
            "Heike Adel",
            "Ngoc Thang Vu",
            "Katrin Kirchhoff",
            "Dominic Telaar",
            "Tanja Schultz"
        ],
        "abstract": "This paper presents our latest investigations on different features for factored language models for Code-Switching speech and their effect on automatic speech recognition (ASR) performance. We focus on syntactic and semantic features which can be extracted from Code-Switching text data and integrate them into factored language models. Different possible factors, such as words, part-of-speech tags, Brown word clusters, open class words and clusters of open class word embeddings are explored. The experimental results reveal that Brown word clusters, part-of-speech tags and open-class words are the most effective at reducing the perplexity of factored language models on the Mandarin-English Code-Switching corpus SEAME. In ASR experiments, the model containing Brown word clusters and part-of-speech tags and the model also including clusters of open class word embeddings yield the best mixed error rate results. In summary, the best language model can significantly reduce the perplexity on the SEAME evaluation set by up to 10.8% relative and the mixed error rate by up to 3.4% relative.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01949",
        "title": "Semantic speech retrieval with a visually grounded model of untranscribed speech",
        "authors": [
            "Herman Kamper",
            "Gregory Shakhnarovich",
            "Karen Livescu"
        ],
        "abstract": "There is growing interest in models that can learn from unlabelled speech paired with visual context. This setting is relevant for low-resource speech processing, robotics, and human language acquisition research. Here we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics. We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to (semantic) keyword labels. We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals. Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches. We perform an extensive analysis of the model and its resulting representations.\n    ",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2018-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01977",
        "title": "Machine Learning Based Detection of Clickbait Posts in Social Media",
        "authors": [
            "Xinyue Cao",
            "Thai Le",
            "Jason",
            "Zhang"
        ],
        "abstract": "Clickbait (headlines) make use of misleading titles that hide critical information from or exaggerate the content on the landing target pages to entice clicks. As clickbaits often use eye-catching wording to attract viewers, target contents are often of low quality. Clickbaits are especially widespread on social media such as Twitter, adversely impacting user experience by causing immense dissatisfaction. Hence, it has become increasingly important to put forward a widely applicable approach to identify and detect clickbaits. In this paper, we make use of a dataset from the clickbait challenge 2017 (",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2017-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02076",
        "title": "On the Effective Use of Pretraining for Natural Language Inference",
        "authors": [
            "Ignacio Cases",
            "Minh-Thang Luong",
            "Christopher Potts"
        ],
        "abstract": "Neural networks have excelled at many NLP tasks, but there remain open questions about the performance of pretrained distributed word representations and their interaction with weight initialization and other hyperparameters. We address these questions empirically using attention-based sequence-to-sequence models for natural language inference (NLI). Specifically, we compare three types of embeddings: random, pretrained (GloVe, word2vec), and retrofitted (pretrained plus WordNet information). We show that pretrained embeddings outperform both random and retrofitted ones in a large NLI corpus. Further experiments on more controlled data sets shed light on the contexts for which retrofitted embeddings can be useful. We also explore two principled approaches to initializing the rest of the model parameters, Gaussian and orthogonal, showing that the latter yields gains of up to 2.9% in the NLI task.\n    ",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2017-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02086",
        "title": "Indowordnets help in Indian Language Machine Translation",
        "authors": [
            "Sreelekha S",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Being less resource languages, Indian-Indian and English-Indian language MT system developments faces the difficulty to translate various lexical phenomena. In this paper, we present our work on a comparative study of 440 phrase-based statistical trained models for 110 language pairs across 11 Indian languages. We have developed 110 baseline Statistical Machine Translation systems. Then we have augmented the training corpus with Indowordnet synset word entries of lexical database and further trained 110 models on top of the baseline system. We have done a detailed performance comparison using various evaluation metrics such as BLEU score, METEOR and TER. We observed significant improvement in evaluations of translation quality across all the 440 models after using the Indowordnet. These experiments give a detailed insight in two ways : (1) usage of lexical database with synset mapping for resource poor languages (2) efficient usage of Indowordnet sysnset mapping. More over, synset mapped lexical entries helped the SMT system to handle the ambiguity to a great extent during the translation.\n    ",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02093",
        "title": "Morphology Generation for Statistical Machine Translation",
        "authors": [
            "Sreelekha S",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "When translating into morphologically rich languages, Statistical MT approaches face the problem of data sparsity. The severity of the sparseness problem will be high when the corpus size of morphologically richer language is less. Even though we can use factored models to correctly generate morphological forms of words, the problem of data sparseness limits their performance. In this paper, we describe a simple and effective solution which is based on enriching the input corpora with various morphological forms of words. We use this method with the phrase-based and factor-based experiments on two morphologically rich languages: Hindi and Marathi when translating from English. We evaluate the performance of our experiments both in terms automatic evaluation and subjective evaluation such as adequacy and fluency. We observe that the morphology injection method helps in improving the quality of translation. We further analyze that the morph injection method helps in handling the data sparseness problem to a great level.\n    ",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02095",
        "title": "Machine Translation Evaluation with Neural Networks",
        "authors": [
            "Francisco Guzm\u00e1n",
            "Shafiq R. Joty",
            "Llu\u00eds M\u00e0rquez",
            "Preslav Nakov"
        ],
        "abstract": "We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is embedded into compact distributed vector representations, and fed into a multi-layer neural network that models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses. We experiment with the benchmark datasets from the WMT Metrics shared task, on which we obtain the best results published so far, with the basic network configuration. We also perform a series of experiments to analyze and understand the contribution of the different components of the network. We evaluate variants and extensions, including fine-tuning of the semantic embeddings, and sentence-based representations modeled with convolutional and recurrent neural networks. In summary, the proposed framework is flexible and generalizable, allows for efficient learning and scoring, and provides an MT evaluation metric that correlates with human judgments, and is on par with the state of the art.\n    ",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2017-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02100",
        "title": "Phrase Pair Mappings for Hindi-English Statistical Machine Translation",
        "authors": [
            "Sreelekha S",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "In this paper, we present our work on the creation of lexical resources for the Machine Translation between English and Hindi. We describes the development of phrase pair mappings for our experiments and the comparative performance evaluation between different trained models on top of the baseline Statistical Machine Translation system. We focused on augmenting the parallel corpus with more vocabulary as well as with various inflected forms by exploring different ways. We have augmented the training corpus with various lexical resources such as lexical words, synset words, function words and verb phrases. We have described the case studies, automatic and subjective evaluations, detailed error analysis for both the English to Hindi and Hindi to English machine translation systems. We further analyzed that, there is an incremental growth in the quality of machine translation with the usage of various lexical resources. Thus lexical resources do help uplift the translation quality of resource poor langugaes.\n    ",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02187",
        "title": "BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages",
        "authors": [
            "Benjamin Heinzerling",
            "Michael Strube"
        ],
        "abstract": "We present BPEmb, a collection of pre-trained subword unit embeddings in 275 languages, based on Byte-Pair Encoding (BPE). In an evaluation using fine-grained entity typing as testbed, BPEmb performs competitively, and for some languages bet- ter than alternative subword approaches, while requiring vastly fewer resources and no tokenization. BPEmb is available at ",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2017-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02318",
        "title": "A Semantic Relevance Based Neural Network for Text Summarization and Text Simplification",
        "authors": [
            "Shuming Ma",
            "Xu Sun"
        ],
        "abstract": "Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.\n    ",
        "submission_date": "2017-10-06T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02365",
        "title": "Czech Text Document Corpus v 2.0",
        "authors": [
            "Pavel Kr\u00e1l",
            "Ladislav Lenc"
        ],
        "abstract": "This paper introduces \"Czech Text Document Corpus v 2.0\", a collection of text documents for automatic document classification in Czech language. It is composed of the text documents provided by the Czech News Agency and is freely available for research purposes at ",
        "submission_date": "2017-10-06T00:00:00",
        "last_modified_date": "2018-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02398",
        "title": "Bilingual Words and Phrase Mappings for Marathi and Hindi SMT",
        "authors": [
            "Sreelekha S",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Lack of proper linguistic resources is the major challenges faced by the Machine Translation system developments when dealing with the resource poor languages. In this paper, we describe effective ways to utilize the lexical resources to improve the quality of statistical machine translation. Our research on the usage of lexical resources mainly focused on two ways, such as; augmenting the parallel corpus with more vocabulary and to provide various word forms. We have augmented the training corpus with various lexical resources such as lexical words, function words, kridanta pairs and verb phrases. We have described the case studies, evaluations and detailed error analysis for both Marathi to Hindi and Hindi to Marathi machine translation systems. From the evaluations we observed that, there is an incremental growth in the quality of machine translation as the usage of various lexical resources increases. Moreover, usage of various lexical resources helps to improve the coverage and quality of machine translation where limited parallel corpus is available.\n    ",
        "submission_date": "2017-10-05T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02437",
        "title": "Learning Word Embeddings for Hyponymy with Entailment-Based Distributional Semantics",
        "authors": [
            "James Henderson"
        ],
        "abstract": "Lexical entailment, such as hyponymy, is a fundamental issue in the semantics of natural language. This paper proposes distributional semantic models which efficiently learn word embeddings for entailment, using a recently-proposed framework for modelling entailment in a vector-space. These models postulate a latent vector for a pseudo-phrase containing two neighbouring word vectors. We investigate both modelling words as the evidence they contribute about this phrase vector, or as the posterior distribution of a one-word phrase vector, and find that the posterior vectors perform better. The resulting word embeddings outperform the best previous results on predicting hyponymy between words, in unsupervised and semi-supervised experiments.\n    ",
        "submission_date": "2017-10-06T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02514",
        "title": "On the Challenges of Sentiment Analysis for Dynamic Events",
        "authors": [
            "Monireh Ebrahimi",
            "Amir Hossein Yazdavar",
            "Amit Sheth"
        ],
        "abstract": "With the proliferation of social media over the last decade, determining people's attitude with respect to a specific topic, document, interaction or events has fueled research interest in natural language processing and introduced a new channel called sentiment and emotion analysis. For instance, businesses routinely look to develop systems to automatically understand their customer conversations by identifying the relevant content to enhance marketing their products and managing their reputations. Previous efforts to assess people's sentiment on Twitter have suggested that Twitter may be a valuable resource for studying political sentiment and that it reflects the offline political landscape. According to a Pew Research Center report, in January 2016 44 percent of US adults stated having learned about the presidential election through social media. Furthermore, 24 percent reported use of social media posts of the two candidates as a source of news and information, which is more than the 15 percent who have used both candidates' websites or emails combined. The first presidential debate between Trump and Hillary was the most tweeted debate ever with 17.1 million tweets.\n    ",
        "submission_date": "2017-10-06T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02569",
        "title": "Low-resource bilingual lexicon extraction using graph based word embeddings",
        "authors": [
            "Ximena Gutierrez-Vasques",
            "Victor Mijangos"
        ],
        "abstract": "In this work we focus on the task of automatically extracting bilingual lexicon for the language pair Spanish-Nahuatl. This is a low-resource setting where only a small amount of parallel corpus is available. Most of the downstream methods do not work well under low-resources conditions. This is specially true for the approaches that use vectorial representations like Word2Vec. Our proposal is to construct bilingual word vectors from a graph. This graph is generated using translation pairs obtained from an unsupervised word alignment method.\n",
        "submission_date": "2017-10-06T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02603",
        "title": "Low-Rank RNN Adaptation for Context-Aware Language Modeling",
        "authors": [
            "Aaron Jaech",
            "Mari Ostendorf"
        ],
        "abstract": "A context-aware language model uses location, user and/or domain metadata (context) to adapt its predictions. In neural language models, context information is typically represented as an embedding and it is given to the RNN as an additional input, which has been shown to be useful in many applications. We introduce a more powerful mechanism for using context to adapt an RNN by letting the context vector control a low-rank transformation of the recurrent layer weight matrix. Experiments show that allowing a greater fraction of the model parameters to be adjusted has benefits in terms of perplexity and classification for several different types of context.\n    ",
        "submission_date": "2017-10-06T00:00:00",
        "last_modified_date": "2018-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02650",
        "title": "Topic Modeling based on Keywords and Context",
        "authors": [
            "Johannes Schneider"
        ],
        "abstract": "Current topic models often suffer from discovering topics not matching human intuition, unnatural switching of topics within documents and high computational demands. We address these concerns by proposing a topic model and an inference algorithm based on automatically identifying characteristic keywords for topics. Keywords influence topic-assignments of nearby words. Our algorithm learns (key)word-topic scores and it self-regulates the number of topics. Inference is simple and easily parallelizable. Qualitative analysis yields comparable results to state-of-the-art models (eg. LDA), but with different strengths and weaknesses. Quantitative analysis using 9 datasets shows gains in terms of classification accuracy, PMI score, computational performance and consistency of topic assignments within documents, while most often using less topics.\n    ",
        "submission_date": "2017-10-07T00:00:00",
        "last_modified_date": "2018-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02717",
        "title": "Group Sparse CNNs for Question Classification with Answer Sets",
        "authors": [
            "Mingbo Ma",
            "Liang Huang",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "Question classification is an important task with wide applications. However, traditional techniques treat questions as general sentences, ignoring the corresponding answer data. In order to consider answer information into question modeling, we first introduce novel group sparse autoencoders which refine question representation by utilizing group information in the answer set. We then propose novel group sparse CNNs which naturally learn question representation with respect to their answers by implanting group sparse autoencoders into traditional CNNs. The proposed model significantly outperform strong baselines on four datasets.\n    ",
        "submission_date": "2017-10-07T00:00:00",
        "last_modified_date": "2017-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02718",
        "title": "OSU Multimodal Machine Translation System Report",
        "authors": [
            "Mingbo Ma",
            "Dapeng Li",
            "Kai Zhao",
            "Liang Huang"
        ],
        "abstract": "This paper describes Oregon State University's submissions to the shared WMT'17 task \"multimodal translation task I\". In this task, all the sentence pairs are image captions in different languages. The key difference between this task and conventional machine translation is that we have corresponding images as additional information for each sentence pair. In this paper, we introduce a simple but effective system which takes an image shared between different languages, feeding it into the both encoding and decoding side. We report our system's performance for English-French and English-German with Flickr30K (in-domain) and MSCOCO (out-of-domain) datasets. Our system achieves the best performance in TER for English-German for MSCOCO dataset.\n    ",
        "submission_date": "2017-10-07T00:00:00",
        "last_modified_date": "2017-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02745",
        "title": "Multi-Document Summarization using Distributed Bag-of-Words Model",
        "authors": [
            "Kaustubh Mani",
            "Ishan Verma",
            "Hardik Meisheri",
            "Lipika Dey"
        ],
        "abstract": "As the number of documents on the web is growing exponentially, multi-document summarization is becoming more and more important since it can provide the main ideas in a document set in short time. In this paper, we present an unsupervised centroid-based document-level reconstruction framework using distributed bag of words model. Specifically, our approach selects summary sentences in order to minimize the reconstruction error between the summary and the documents. We apply sentence selection and beam search, to further improve the performance of our model. Experimental results on two different datasets show significant performance gains compared with the state-of-the-art baselines.\n    ",
        "submission_date": "2017-10-07T00:00:00",
        "last_modified_date": "2018-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02772",
        "title": "Smarnet: Teaching Machines to Read and Comprehend Like Human",
        "authors": [
            "Zheqian Chen",
            "Rongqin Yang",
            "Bin Cao",
            "Zhou Zhao",
            "Deng Cai",
            "Xiaofei He"
        ],
        "abstract": "Machine Comprehension (MC) is a challenging task in Natural Language Processing field, which aims to guide the machine to comprehend a passage and answer the given question. Many existing approaches on MC task are suffering the inefficiency in some bottlenecks, such as insufficient lexical understanding, complex question-passage interaction, incorrect answer extraction and so on. In this paper, we address these problems from the viewpoint of how humans deal with reading tests in a scientific way. Specifically, we first propose a novel lexical gating mechanism to dynamically combine the words and characters representations. We then guide the machines to read in an interactive way with attention mechanism and memory network. Finally we add a checking layer to refine the answer for insurance. The extensive experiments on two popular datasets SQuAD and TriviaQA show that our method exceeds considerable performance than most state-of-the-art solutions at the time of submission.\n    ",
        "submission_date": "2017-10-08T00:00:00",
        "last_modified_date": "2017-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02855",
        "title": "The IIT Bombay English-Hindi Parallel Corpus",
        "authors": [
            "Anoop Kunchukuttan",
            "Pratik Mehta",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "We present the IIT Bombay English-Hindi Parallel Corpus. The corpus is a compilation of parallel corpora previously available in the public domain as well as new parallel corpora we collected. The corpus contains 1.49 million parallel segments, of which 694k segments were not previously available in the public domain. The corpus has been pre-processed for machine translation, and we report baseline phrase-based SMT and NMT translation results on this corpus. This corpus has been used in two editions of shared tasks at the Workshop on Asian Language Translation (2016 and 2017). The corpus is freely available for non-commercial research. To the best of our knowledge, this is the largest publicly available English-Hindi parallel corpus.\n    ",
        "submission_date": "2017-10-08T00:00:00",
        "last_modified_date": "2018-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02861",
        "title": "Clickbait detection using word embeddings",
        "authors": [
            "Vijayasaradhi Indurthi",
            "Subba Reddy Oota"
        ],
        "abstract": "Clickbait is a pejorative term describing web content that is aimed at generating online advertising revenue, especially at the expense of quality or accuracy, relying on sensationalist headlines or eye-catching thumbnail pictures to attract click-throughs and to encourage forwarding of the material over online social networks. We use distributed word representations of the words in the title as features to identify clickbaits in online news media. We train a machine learning model using linear regression to predict the cickbait score of a given tweet. Our methods achieve an F1-score of 64.98\\% and an MSE of 0.0791. Compared to other methods, our method is simple, fast to train, does not require extensive feature engineering and yet moderately effective.\n    ",
        "submission_date": "2017-10-08T00:00:00",
        "last_modified_date": "2017-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02925",
        "title": "Natural Language Inference from Multiple Premises",
        "authors": [
            "Alice Lai",
            "Yonatan Bisk",
            "Julia Hockenmaier"
        ],
        "abstract": "We define a novel textual entailment task that requires inference over multiple premise sentences. We present a new dataset for this task that minimizes trivial lexical inferences, emphasizes knowledge of everyday events, and presents a more challenging setting for textual entailment. We evaluate several strong neural baselines and analyze how the multiple premise task differs from standard textual entailment.\n    ",
        "submission_date": "2017-10-09T00:00:00",
        "last_modified_date": "2017-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03006",
        "title": "Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features",
        "authors": [
            "Gregor Wiedemann",
            "Gerhard Heyer"
        ],
        "abstract": "In recent years, (retro-)digitizing paper-based files became a major undertaking for private and public archives as well as an important task in electronic mailroom applications. As a first step, the workflow involves scanning and Optical Character Recognition (OCR) of documents. Preservation of document contexts of single page scans is a major requirement in this context. To facilitate workflows involving very large amounts of paper scans, page stream segmentation (PSS) is the task to automatically separate a stream of scanned images into multi-page documents. In a digitization project together with a German federal archive, we developed a novel approach based on convolutional neural networks (CNN) combining image and text features to achieve optimal document separation results. Evaluation shows that our PSS architecture achieves an accuracy up to 93 % which can be regarded as a new state-of-the-art for this task.\n    ",
        "submission_date": "2017-10-09T00:00:00",
        "last_modified_date": "2019-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03203",
        "title": "Deep Learning Paradigm with Transformed Monolingual Word Embeddings for Multilingual Sentiment Analysis",
        "authors": [
            "Yujie Lu",
            "Tatsunori Mori"
        ],
        "abstract": "The surge of social media use brings huge demand of multilingual sentiment analysis (MSA) for unveiling cultural difference. So far, traditional methods resorted to machine translation---translating texts in other languages to English, and then adopt the methods once worked in English. However, this paradigm is conditioned by the quality of machine translation. In this paper, we propose a new deep learning paradigm to assimilate the differences between languages for MSA. We first pre-train monolingual word embeddings separately, then map word embeddings in different spaces into a shared embedding space, and then finally train a parameter-sharing deep neural network for MSA. The experimental results show that our paradigm is effective. Especially, our CNN model outperforms a state-of-the-art baseline by around 2.1% in terms of classification accuracy.\n    ",
        "submission_date": "2017-10-09T00:00:00",
        "last_modified_date": "2017-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03255",
        "title": "Multitask training with unlabeled data for end-to-end sign language fingerspelling recognition",
        "authors": [
            "Bowen Shi",
            "Karen Livescu"
        ],
        "abstract": "We address the problem of automatic American Sign Language fingerspelling recognition from video. Prior work has largely relied on frame-level labels, hand-crafted features, or other constraints, and has been hampered by the scarcity of data for this task. We introduce a model for fingerspelling recognition that addresses these issues. The model consists of an auto-encoder-based feature extractor and an attention-based neural encoder-decoder, which are trained jointly. The model receives a sequence of image frames and outputs the fingerspelled word, without relying on any frame-level training labels or hand-crafted features. In addition, the auto-encoder subcomponent makes it possible to leverage unlabeled data to improve the feature learning. The model achieves 11.6% and 4.4% absolute letter accuracy improvement respectively in signer-independent and signer-adapted fingerspelling recognition over previous approaches that required frame-level training labels.\n    ",
        "submission_date": "2017-10-09T00:00:00",
        "last_modified_date": "2019-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03348",
        "title": "What does Attention in Neural Machine Translation Pay Attention to?",
        "authors": [
            "Hamidreza Ghader",
            "Christof Monz"
        ],
        "abstract": "Attention in neural machine translation provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, attention is considered to be an alignment model as well. However, there is no work that specifically studies attention and provides analysis of what is being learned by attention models. Thus, the question still remains that how attention is similar or different from the traditional alignment. In this paper, we provide detailed analysis of attention and compare it to traditional alignment. We answer the question of whether attention is only capable of modelling translational equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.\n    ",
        "submission_date": "2017-10-09T00:00:00",
        "last_modified_date": "2017-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03430",
        "title": "Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering",
        "authors": [
            "Seunghyun Yoon",
            "Joongbo Shin",
            "Kyomin Jung"
        ],
        "abstract": "In this paper, we propose a novel end-to-end neural architecture for ranking candidate answers, that adapts a hierarchical recurrent neural network and a latent topic clustering module. With our proposed model, a text is encoded to a vector representation from an word-level to a chunk-level to effectively capture the entire meaning. In particular, by adapting the hierarchical structure, our model shows very small performance degradations in longer text comprehension while other state-of-the-art recurrent neural network models suffer from it. Additionally, the latent topic clustering module extracts semantic information from target samples. This clustering module is useful for any text related tasks by allowing each data sample to find its nearest topic cluster, thus helping the neural network model analyze the entire data. We evaluate our models on the Ubuntu Dialogue Corpus and consumer electronic domain question answering dataset, which is related to Samsung products. The proposed model shows state-of-the-art results for ranking question-answer pairs.\n    ",
        "submission_date": "2017-10-10T00:00:00",
        "last_modified_date": "2018-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03476",
        "title": "MoNoise: Modeling Noise Using a Modular Normalization System",
        "authors": [
            "Rob van der Goot",
            "Gertjan van Noord"
        ],
        "abstract": "We propose MoNoise: a normalization model focused on generalizability and efficiency, it aims at being easily reusable and adaptable. Normalization is the task of translating texts from a non- canonical domain to a more canonical domain, in our case: from social media data to standard language. Our proposed model is based on a modular candidate generation in which each module is responsible for a different type of normalization action. The most important generation modules are a spelling correction system and a word embeddings module. Depending on the definition of the normalization task, a static lookup list can be crucial for performance. We train a random forest classifier to rank the candidates, which generalizes well to all different types of normaliza- tion actions. Most features for the ranking originate from the generation modules; besides these features, N-gram features prove to be an important source of information. We show that MoNoise beats the state-of-the-art on different normalization benchmarks for English and Dutch, which all define the task of normalization slightly different.\n    ",
        "submission_date": "2017-10-10T00:00:00",
        "last_modified_date": "2017-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03501",
        "title": "A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments",
        "authors": [
            "P. Godard",
            "G. Adda",
            "M. Adda-Decker",
            "J. Benjumea",
            "L. Besacier",
            "J. Cooper-Leavitt",
            "G-N. Kouarata",
            "L. Lamel",
            "H. Maynard",
            "M. Mueller",
            "A. Rialland",
            "S. Stueker",
            "F. Yvon",
            "M. Zanon-Boito"
        ],
        "abstract": "Most speech and language technologies are trained with massive amounts of speech and text information. However, most of the world languages do not have such resources or stable orthography. Systems constructed under these almost zero resource conditions are not only promising for speech technology but also for computational language documentation. The goal of computational language documentation is to help field linguists to (semi-)automatically analyze and annotate audio recordings of endangered and unwritten languages. Example tasks are automatic phoneme discovery or lexicon discovery from the speech signal. This paper presents a speech corpus collected during a realistic language documentation process. It is made up of 5k speech utterances in Mboshi (Bantu C25) aligned to French text translations. Speech transcriptions are also made available: they correspond to a non-standard graphemic form close to the language phonology. We present how the data was collected, cleaned and processed and we illustrate its use through a zero-resource task: spoken term discovery. The dataset is made available to the community for reproducible computational language documentation experiments and their evaluation.\n    ",
        "submission_date": "2017-10-10T00:00:00",
        "last_modified_date": "2018-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03743",
        "title": "Confidence through Attention",
        "authors": [
            "Mat\u012bss Rikters",
            "Mark Fishel"
        ],
        "abstract": "Attention distributions of the generated translations are a useful bi-product of attention-based recurrent neural network translation models and can be treated as soft alignments between the input and output tokens. In this work, we use attention distributions as a confidence metric for output translations. We present two strategies of using the attention distributions: filtering out bad translations from a large back-translated corpus, and selecting the best translation in a hybrid setup of two different translation systems. While manual evaluation indicated only a weak correlation between our confidence score and human judgments, the use-cases showed improvements of up to 2.22 BLEU points for filtering and 0.99 points for hybrid translation, tested on English<->German and English<->Latvian translation.\n    ",
        "submission_date": "2017-10-10T00:00:00",
        "last_modified_date": "2017-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03838",
        "title": "The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages",
        "authors": [
            "Dingquan Wang",
            "Jason Eisner"
        ],
        "abstract": "We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a \"nearby\" source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.\n    ",
        "submission_date": "2017-10-10T00:00:00",
        "last_modified_date": "2017-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03877",
        "title": "Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure with Supervised Learning",
        "authors": [
            "Dingquan Wang",
            "Jason Eisner"
        ],
        "abstract": "We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin.\n    ",
        "submission_date": "2017-10-11T00:00:00",
        "last_modified_date": "2017-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03954",
        "title": "Decision support from financial disclosures with deep neural networks and transfer learning",
        "authors": [
            "Mathias Kraus",
            "Stefan Feuerriegel"
        ],
        "abstract": "Company disclosures greatly aid in the process of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives.\n    ",
        "submission_date": "2017-10-11T00:00:00",
        "last_modified_date": "2017-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03957",
        "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
        "authors": [
            "Yanran Li",
            "Hui Su",
            "Xiaoyu Shen",
            "Wenjie Li",
            "Ziqiang Cao",
            "Shuzi Niu"
        ],
        "abstract": "We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems.\n    ",
        "submission_date": "2017-10-11T00:00:00",
        "last_modified_date": "2017-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04087",
        "title": "Word Translation Without Parallel Data",
        "authors": [
            "Alexis Conneau",
            "Guillaume Lample",
            "Marc'Aurelio Ranzato",
            "Ludovic Denoyer",
            "Herv\u00e9 J\u00e9gou"
        ],
        "abstract": "State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.\n    ",
        "submission_date": "2017-10-11T00:00:00",
        "last_modified_date": "2018-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04203",
        "title": "Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion Lexicon",
        "authors": [
            "Giannis Haralabopoulos",
            "Elena Simperl"
        ],
        "abstract": "Sentiment analysis aims to uncover emotions conveyed through information. In its simplest form, it is performed on a polarity basis, where the goal is to classify information with positive or negative emotion. Recent research has explored more nuanced ways to capture emotions that go beyond polarity. For these methods to work, they require a critical resource: a lexicon that is appropriate for the task at hand, in terms of the range of emotions it captures diversity. In the past, sentiment analysis lexicons have been created by experts, such as linguists and behavioural scientists, with strict rules. Lexicon evaluation was also performed by experts or gold standards. In our paper, we propose a crowdsourcing method for lexicon acquisition, which is scalable, cost-effective, and doesn't require experts or gold standards. We also compare crowd and expert evaluations of the lexicon, to assess the overall lexicon quality, and the evaluation capabilities of the crowd.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04334",
        "title": "DisSent: Sentence Representation Learning from Explicit Discourse Relations",
        "authors": [
            "Allen Nie",
            "Erin D. Bennett",
            "Noah D. Goodman"
        ],
        "abstract": "Learning effective representations of sentences is one of the core missions of natural language understanding. Existing models either train on a vast amount of text, or require costly, manually curated sentence relation datasets. We show that with dependency parsing and rule-based rubrics, we can curate a high quality sentence relation task by leveraging explicit discourse relations. We show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning, representing relations that can only be determined when the meanings of two sentences are combined. We demonstrate that the automatically curated corpus allows a bidirectional LSTM sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as BERT. Our fixed sentence embeddings achieve high performance on a variety of transfer tasks, including SentEval, and we achieve state-of-the-art results on Penn Discourse Treebank's implicit relation prediction task.\n    ",
        "submission_date": "2017-10-12T00:00:00",
        "last_modified_date": "2019-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04344",
        "title": "Using Context Events in Neural Network Models for Event Temporal Status Identification",
        "authors": [
            "Zeyu Dai",
            "Wenlin Yao",
            "Ruihong Huang"
        ],
        "abstract": "Focusing on the task of identifying event temporal status, we find that events directly or indirectly governing the target event in a dependency tree are most important contexts. Therefore, we extract dependency chains containing context events and use them as input in neural network models, which consistently outperform previous models using local context words as input. Visualization verifies that the dependency chain representation can effectively capture the context events which are closely related to the target event and play key roles in predicting event temporal status.\n    ",
        "submission_date": "2017-10-12T00:00:00",
        "last_modified_date": "2017-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04437",
        "title": "Revisiting the Design Issues of Local Models for Japanese Predicate-Argument Structure Analysis",
        "authors": [
            "Yuichiroh Matsubayashi",
            "Kentaro Inui"
        ],
        "abstract": "The research trend in Japanese predicate-argument structure (PAS) analysis is shifting from pointwise prediction models with local features to global models designed to search for globally optimal solutions. However, the existing global models tend to employ only relatively simple local features; therefore, the overall performance gains are rather limited. The importance of designing a local model is demonstrated in this study by showing that the performance of a sophisticated local model can be considerably improved with recent feature embedding methods and a feature combination learning based on a neural network, outperforming the state-of-the-art global models in $F_1$ on a common benchmark dataset.\n    ",
        "submission_date": "2017-10-12T00:00:00",
        "last_modified_date": "2017-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04515",
        "title": "Convolutional Attention-based Seq2Seq Neural Network for End-to-End ASR",
        "authors": [
            "Dan Lim"
        ],
        "abstract": "This thesis introduces the sequence to sequence model with Luong's attention mechanism for end-to-end ASR. It also describes various neural network algorithms including Batch normalization, Dropout and Residual network which constitute the convolutional attention-based seq2seq neural network. Finally the proposed model proved its effectiveness for speech recognition achieving 15.8% phoneme error rate on TIMIT dataset.\n    ",
        "submission_date": "2017-10-12T00:00:00",
        "last_modified_date": "2017-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04600",
        "title": "Auto Analysis of Customer Feedback using CNN and GRU Network",
        "authors": [
            "Deepak Gupta",
            "Pabitra Lenka",
            "Harsimran Bedi",
            "Asif Ekbal",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Analyzing customer feedback is the best way to channelize the data into new marketing strategies that benefit entrepreneurs as well as customers. Therefore an automated system which can analyze the customer behavior is in great demand. Users may write feedbacks in any language, and hence mining appropriate information often becomes intractable. Especially in a traditional feature-based supervised model, it is difficult to build a generic system as one has to understand the concerned language for finding the relevant features. In order to overcome this, we propose deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) based approaches that do not require handcrafting of features. We evaluate these techniques for analyzing customer feedback sentences in four languages, namely English, French, Japanese and Spanish. Our empirical analysis shows that our models perform well in all the four languages on the setups of IJCNLP Shared Task on Customer Feedback Analysis. Our model achieved the second rank in French, with an accuracy of 71.75% and third ranks for all the other languages.\n    ",
        "submission_date": "2017-10-12T00:00:00",
        "last_modified_date": "2017-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04802",
        "title": "End-to-end Network for Twitter Geolocation Prediction and Hashing",
        "authors": [
            "Jey Han Lau",
            "Lianhua Chi",
            "Khoi-Nguyen Tran",
            "Trevor Cohn"
        ],
        "abstract": "We propose an end-to-end neural network to predict the geolocation of a tweet. The network takes as input a number of raw Twitter metadata such as the tweet message and associated user account information. Our model is language independent, and despite minimal feature engineering, it is interpretable and capable of learning location indicative words and timing patterns. Compared to state-of-the-art systems, our model outperforms them by 2%-6%. Additionally, we propose extensions to the model to compress representation learnt by the network into binary codes. Experiments show that it produces compact codes compared to benchmark hashing algorithms. An implementation of the model is released publicly.\n    ",
        "submission_date": "2017-10-13T00:00:00",
        "last_modified_date": "2017-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04989",
        "title": "Complex Word Identification: Challenges in Data Annotation and System Performance",
        "authors": [
            "Marcos Zampieri",
            "Shervin Malmasi",
            "Gustavo Paetzold",
            "Lucia Specia"
        ],
        "abstract": "This paper revisits the problem of complex word identification (CWI) following up the SemEval CWI shared task. We use ensemble classifiers to investigate how well computational methods can discriminate between complex and non-complex words. Furthermore, we analyze the classification performance to understand what makes lexical complexity challenging. Our findings show that most systems performed poorly on the SemEval CWI dataset, and one of the reasons for that is the way in which human annotation was performed.\n    ",
        "submission_date": "2017-10-13T00:00:00",
        "last_modified_date": "2017-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05094",
        "title": "Learning Phrase Embeddings from Paraphrases with GRUs",
        "authors": [
            "Zhihao Zhou",
            "Lifu Huang",
            "Heng Ji"
        ],
        "abstract": "Learning phrase representations has been widely explored in many Natural Language Processing (NLP) tasks (e.g., Sentiment Analysis, Machine Translation) and has shown promising improvements. Previous studies either learn non-compositional phrase representations with general word embedding learning techniques or learn compositional phrase representations based on syntactic structures, which either require huge amounts of human annotations or cannot be easily generalized to all phrases. In this work, we propose to take advantage of large-scaled paraphrase database and present a pair-wise gated recurrent units (pairwise-GRU) framework to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks.\n    ",
        "submission_date": "2017-10-13T00:00:00",
        "last_modified_date": "2017-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05364",
        "title": "Clickbait Detection in Tweets Using Self-attentive Network",
        "authors": [
            "Yiwei Zhou"
        ],
        "abstract": "Clickbait detection in tweets remains an elusive challenge. In this paper, we describe the solution for the Zingel Clickbait Detector at the Clickbait Challenge 2017, which is capable of evaluating each tweet's level of click baiting. We first reformat the regression problem as a multi-classification problem, based on the annotation scheme. To perform multi-classification, we apply a token-level, self-attentive mechanism on the hidden states of bi-directional Gated Recurrent Units (biGRU), which enables the model to generate tweets' task-specific vector representations by attending to important tokens. The self-attentive neural network can be trained end-to-end, without involving any manual feature engineering. Our detector ranked first in the final evaluation of Clickbait Challenge 2017.\n    ",
        "submission_date": "2017-10-15T00:00:00",
        "last_modified_date": "2017-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05370",
        "title": "NoReC: The Norwegian Review Corpus",
        "authors": [
            "Erik Velldal",
            "Lilja \u00d8vrelid",
            "Eivind Alexander Bergem",
            "Cathrine Stadsnes",
            "Samia Touileb",
            "Fredrik J\u00f8rgensen"
        ],
        "abstract": "This paper presents the Norwegian Review Corpus (NoReC), created for training and evaluating models for document-level sentiment analysis. The full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review is labeled with a manually assigned score of 1-6, as provided by the rating of the original author. This first release of the corpus comprises more than 35,000 reviews. It is distributed using the CoNLL-U format, pre-processed using UDPipe, along with a rich set of metadata. The work reported in this paper forms part of the SANT initiative (Sentiment Analysis for Norwegian Text), a project seeking to provide resources and tools for sentiment analysis and opinion mining for Norwegian. As resources for sentiment analysis have so far been unavailable for Norwegian, NoReC represents a highly valuable and sought-after addition to Norwegian language technology.\n    ",
        "submission_date": "2017-10-15T00:00:00",
        "last_modified_date": "2017-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05429",
        "title": "Semi-Supervised Approach to Monitoring Clinical Depressive Symptoms in Social Media",
        "authors": [
            "Amir Hossein Yazdavar",
            "Hussein S. Al-Olimat",
            "Monireh Ebrahimi",
            "Goonmeet Bajaj",
            "Tanvi Banerjee",
            "Krishnaprasad Thirunarayan",
            "Jyotishman Pathak",
            "Amit Sheth"
        ],
        "abstract": "With the rise of social media, millions of people are routinely expressing their moods, feelings, and daily struggles with mental health issues on social media platforms like Twitter. Unlike traditional observational cohort studies conducted through questionnaires and self-reported surveys, we explore the reliable detection of clinical depression from tweets obtained unobtrusively. Based on the analysis of tweets crawled from users with self-reported depressive symptoms in their Twitter profiles, we demonstrate the potential for detecting clinical depression symptoms which emulate the PHQ-9 questionnaire clinicians use today. Our study uses a semi-supervised statistical model to evaluate how the duration of these symptoms and their expression on Twitter (in terms of word usage patterns and topical preferences) align with the medical findings reported via the PHQ-9. Our proactive and automatic screening tool is able to identify clinical depressive symptoms with an accuracy of 68% and precision of 72%.\n    ",
        "submission_date": "2017-10-16T00:00:00",
        "last_modified_date": "2017-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05519",
        "title": "BKTreebank: Building a Vietnamese Dependency Treebank",
        "authors": [
            "Kiem-Hieu Nguyen"
        ],
        "abstract": "Dependency treebank is an important resource in any language. In this paper, we present our work on building BKTreebank, a dependency treebank for Vietnamese. Important points on designing POS tagset, dependency relations, and annotation guidelines are discussed. We describe experiments on POS tagging and dependency parsing on the treebank. Experimental results show that the treebank is a useful resource for Vietnamese language processing.\n    ",
        "submission_date": "2017-10-16T00:00:00",
        "last_modified_date": "2018-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05709",
        "title": "Aligning Script Events with Narrative Texts",
        "authors": [
            "Simon Ostermann",
            "Michael Roth",
            "Stefan Thater",
            "Manfred Pinkal"
        ],
        "abstract": "Script knowledge plays a central role in text understanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible.\n    ",
        "submission_date": "2017-10-16T00:00:00",
        "last_modified_date": "2019-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05780",
        "title": "A retrieval-based dialogue system utilizing utterance and context embeddings",
        "authors": [
            "Alexander Bartl",
            "Gerasimos Spanakis"
        ],
        "abstract": "Finding semantically rich and computer-understandable representations for textual dialogues, utterances and words is crucial for dialogue systems (or conversational agents), as their performance mostly depends on understanding the context of conversations. Recent research aims at finding distributed vector representations (embeddings) for words, such that semantically similar words are relatively close within the vector-space. Encoding the \"meaning\" of text into vectors is a current trend, and text can range from words, phrases and documents to actual human-to-human conversations. In recent research approaches, responses have been generated utilizing a decoder architecture, given the vector representation of the current conversation. In this paper, the utilization of embeddings for answer retrieval is explored by using Locality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor (ANN) model, to find similar conversations in a corpus and rank possible candidates. Experimental results on the well-known Ubuntu Corpus (in English) and a customer service chat dataset (in Dutch) show that, in combination with a candidate selection method, retrieval-based approaches outperform generative ones and reveal promising future research directions towards the usability of such a system.\n    ",
        "submission_date": "2017-10-16T00:00:00",
        "last_modified_date": "2017-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05978",
        "title": "Convolutional Neural Networks for Sentiment Classification on Business Reviews",
        "authors": [
            "Andreea Salinca"
        ],
        "abstract": "Recently Convolutional Neural Networks (CNNs) models have proven remarkable results for text classification and sentiment analysis. In this paper, we present our approach on the task of classifying business reviews using word embeddings on a large-scale dataset provided by Yelp: Yelp 2017 challenge dataset. We compare word-based CNN using several pre-trained word embeddings and end-to-end vector representations for text reviews classification. We conduct several experiments to capture the semantic relationship between business reviews and we use deep learning techniques that prove that the obtained results are competitive with traditional methods.\n    ",
        "submission_date": "2017-10-16T00:00:00",
        "last_modified_date": "2017-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06071",
        "title": "PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee"
        ],
        "abstract": "We present PubMed 200k RCT, a new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: we hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06112",
        "title": "CASICT Tibetan Word Segmentation System for MLWS2017",
        "authors": [
            "Jiawei Hu",
            "Qun Liu"
        ],
        "abstract": "We participated in the MLWS 2017 on Tibetan word segmentation task, our system is trained in a unrestricted way, by introducing a baseline system and 76w tibetan segmented sentences of ours. In the system character sequence is processed by the baseline system into word sequence, then a subword unit (BPE algorithm) split rare words into subwords with its corresponding features, after that a neural network classifier is adopted to token each subword into \"B,M,E,S\" label, in decoding step a simple rule is used to recover a final word sequence. The candidate system for submition is selected by evaluating the F-score in dev set pre-extracted from the 76w sentences. Experiment shows that this method can fix segmentation errors of baseline system and result in a significant performance gain.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06313",
        "title": "Paying Attention to Multi-Word Expressions in Neural Machine Translation",
        "authors": [
            "Mat\u012bss Rikters",
            "Ond\u0159ej Bojar"
        ],
        "abstract": "Processing of multi-word expressions (MWEs) is a known problem for any natural language processing task. Even neural machine translation (NMT) struggles to overcome it. This paper presents results of experiments on investigating NMT attention allocation to the MWEs and improving automated translation of sentences that contain MWEs in English->Latvian and English->Czech NMT systems. Two improvement strategies were explored -(1) bilingual pairs of automatically extracted MWE candidates were added to the parallel corpus used to train the NMT system, and (2) full sentences containing the automatically extracted MWE candidates were added to the parallel corpus. Both approaches allowed to increase automated evaluation results. The best result - 0.99 BLEU point increase - has been reached with the first approach, while with the second approach minimal improvements achieved. We also provide open-source software and tools used for MWE extraction and alignment inspection.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2019-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06371",
        "title": "Specialising Word Vectors for Lexical Entailment",
        "authors": [
            "Ivan Vuli\u0107",
            "Nikola Mrk\u0161i\u0107"
        ],
        "abstract": "We present LEAR (Lexical Entailment Attract-Repel), a novel post-processing method that transforms any input word vector space to emphasise the asymmetric relation of lexical entailment (LE), also known as the IS-A or hyponymy-hypernymy relation. By injecting external linguistic constraints (e.g., WordNet links) into the initial vector space, the LE specialisation procedure brings true hyponymy-hypernymy pairs closer together in the transformed Euclidean space. The proposed asymmetric distance measure adjusts the norms of word vectors to reflect the actual WordNet-style hierarchy of concepts. Simultaneously, a joint objective enforces semantic similarity using the symmetric cosine distance, yielding a vector space specialised for both lexical relations at once. LEAR specialisation achieves state-of-the-art performance in the tasks of hypernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2018-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06393",
        "title": "RETUYT in TASS 2017: Sentiment Analysis for Spanish Tweets using SVM and CNN",
        "authors": [
            "Aiala Ros\u00e1",
            "Luis Chiruzzo",
            "Mathias Etcheverry",
            "Santiago Castro"
        ],
        "abstract": "This article presents classifiers based on SVM and Convolutional Neural Networks (CNN) for the TASS 2017 challenge on tweets sentiment analysis. The classifier with the best performance in general uses a combination of SVM and CNN. The use of word embeddings was particularly useful for improving the classifiers performance.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06406",
        "title": "Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz Interface for Collecting Human-Robot Dialogue",
        "authors": [
            "Claire Bonial",
            "Matthew Marge",
            "Ron artstein",
            "Ashley Foots",
            "Felix Gervits",
            "Cory J. Hayes",
            "Cassidy Henry",
            "Susan G. Hill",
            "Anton Leuski",
            "Stephanie M. Lukin",
            "Pooja Moolchandani",
            "Kimberly A. Pollard",
            "David Traum",
            "Clare R. Voss"
        ],
        "abstract": "We describe the adaptation and refinement of a graphical user interface designed to facilitate a Wizard-of-Oz (WoZ) approach to collecting human-robot dialogue data. The data collected will be used to develop a dialogue system for robot navigation. Building on an interface previously used in the development of dialogue systems for virtual agents and video playback, we add templates with open parameters which allow the wizard to quickly produce a wide variety of utterances. Our research demonstrates that this approach to data collection is viable as an intermediate step in developing a dialogue system for physical robots in remote locations from their users - a domain in which the human and robot need to regularly verify and update a shared understanding of the physical environment. We show that our WoZ interface and the fixed set of utterances and templates therein provide for a natural pace of dialogue with good coverage of the navigation domain.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06481",
        "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents",
        "authors": [
            "Johannes Welbl",
            "Pontus Stenetorp",
            "Sebastian Riedel"
        ],
        "abstract": "Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently there exist no resources to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence - effectively performing multi-hop (alias multi-step) inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information, as providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 42.9% compared to human performance at 74.0% - leaving ample room for improvement.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2018-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06524",
        "title": "Unsupervised Sentence Representations as Word Information Series: Revisiting TF--IDF",
        "authors": [
            "Ignacio Arroyo-Fern\u00e1ndez",
            "Carlos-Francisco M\u00e9ndez-Cruz",
            "Gerardo Sierra",
            "Juan-Manuel Torres-Moreno",
            "Grigori Sidorov"
        ],
        "abstract": "Sentence representation at the semantic level is a challenging task for Natural Language Processing and Artificial Intelligence. Despite the advances in word embeddings (i.e. word vector representations), capturing sentence meaning is an open question due to complexities of semantic interactions among words. In this paper, we present an embedding method, which is aimed at learning unsupervised sentence representations from unlabeled text. We propose an unsupervised method that models a sentence as a weighted series of word embeddings. The weights of the word embeddings are fitted by using Shannon's word entropies provided by the Term Frequency--Inverse Document Frequency (TF--IDF) transform. The hyperparameters of the model can be selected according to the properties of data (e.g. sentence length and textual gender). Hyperparameter selection involves word embedding methods and dimensionalities, as well as weighting schemata. Our method offers advantages over existing methods: identifiable modules, short-term training, online inference of (unseen) sentence representations, as well as independence from domain, external knowledge and language resources. Results showed that our model outperformed the state of the art in well-known Semantic Textual Similarity (STS) benchmarks. Moreover, our model reached state-of-the-art performance when compared to supervised and knowledge-based STS systems.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06536",
        "title": "Basic tasks of sentiment analysis",
        "authors": [
            "Iti Chaturvedi",
            "Soujanya Poria",
            "Erik Cambria"
        ],
        "abstract": "Subjectivity detection is the task of identifying objective and subjective sentences. Objective sentences are those which do not exhibit any sentiment. So, it is desired for a sentiment analysis engine to find and separate the objective sentences for further analysis, e.g., polarity detection. In subjective sentences, opinions can often be expressed on one or multiple topics. Aspect extraction is a subtask of sentiment analysis that consists in identifying opinion targets in opinionated text, i.e., in detecting the specific aspects of a product or service the opinion holder is either praising or complaining about.\n    ",
        "submission_date": "2017-10-18T00:00:00",
        "last_modified_date": "2017-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06554",
        "title": "Honk: A PyTorch Reimplementation of Convolutional Neural Networks for Keyword Spotting",
        "authors": [
            "Raphael Tang",
            "Jimmy Lin"
        ],
        "abstract": "We describe Honk, an open-source PyTorch reimplementation of convolutional neural networks for keyword spotting that are included as examples in TensorFlow. These models are useful for recognizing \"command triggers\" in speech-based interfaces (e.g., \"Hey Siri\"), which serve as explicit cues for audio recordings of utterances that are sent to the cloud for full speech recognition. Evaluation on Google's recently released Speech Commands Dataset shows that our reimplementation is comparable in accuracy and provides a starting point for future work on the keyword spotting task.\n    ",
        "submission_date": "2017-10-18T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06632",
        "title": "Towards a Seamless Integration of Word Senses into Downstream NLP Applications",
        "authors": [
            "Mohammad Taher Pilehvar",
            "Jose Camacho-Collados",
            "Roberto Navigli",
            "Nigel Collier"
        ],
        "abstract": "Lexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into NLP systems has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.\n    ",
        "submission_date": "2017-10-18T00:00:00",
        "last_modified_date": "2017-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06700",
        "title": "Build Fast and Accurate Lemmatization for Arabic",
        "authors": [
            "Hamdy Mubarak"
        ],
        "abstract": "In this paper we describe the complexity of building a lemmatizer for Arabic which has a rich and complex derivational morphology, and we discuss the need for a fast and accurate lammatization to enhance Arabic Information Retrieval (IR) results. We also introduce a new data set that can be used to test lemmatization accuracy, and an efficient lemmatization algorithm that outperforms state-of-the-art Arabic lemmatization in terms of accuracy and speed. We share the data set and the code for public.\n    ",
        "submission_date": "2017-10-18T00:00:00",
        "last_modified_date": "2017-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06917",
        "title": "Annotating High-Level Structures of Short Stories and Personal Anecdotes",
        "authors": [
            "Boyang Li",
            "Beth Cardier",
            "Tong Wang",
            "Florian Metze"
        ],
        "abstract": "Stories are a vital form of communication in human culture; they are employed daily to persuade, to elicit sympathy, or to convey a message. Computational understanding of human narratives, especially high-level narrative structures, remain limited to date. Multiple literary theories for narrative structures exist, but operationalization of the theories has remained a challenge. We developed an annotation scheme by consolidating and extending existing narratological theories, including Labov and Waletsky's (1967) functional categorization scheme and Freytag's (1863) pyramid of dramatic tension, and present 360 annotated short stories collected from online sources. In the future, this research will support an approach that enables systems to intelligently sustain complex communications with humans.\n    ",
        "submission_date": "2017-10-08T00:00:00",
        "last_modified_date": "2018-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06922",
        "title": "Emergent Translation in Multi-Agent Communication",
        "authors": [
            "Jason Lee",
            "Kyunghyun Cho",
            "Jason Weston",
            "Douwe Kiela"
        ],
        "abstract": "While most machine translation systems to date are trained on large parallel corpora, humans learn language in a different way: by being grounded in an environment and interacting with other humans. In this work, we propose a communication game where two agents, native speakers of their own respective languages, jointly learn to solve a visual referential task. We find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals. The emergent translation is interactive and multimodal, and crucially does not require parallel corpora, but only monolingual, independent text and corresponding images. Our proposed translation model achieves this by grounding the source and target languages into a shared visual modality, and outperforms several baselines on both word-level and sentence-level translation tasks. Furthermore, we show that agents in a multilingual community learn to translate better and faster than in a bilingual communication setting.\n    ",
        "submission_date": "2017-10-12T00:00:00",
        "last_modified_date": "2018-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06923",
        "title": "Adapting general-purpose speech recognition engine output for domain-specific natural language question answering",
        "authors": [
            "C. Anantaram",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "Speech-based natural language question-answering interfaces to enterprise systems are gaining a lot of attention. General-purpose speech engines can be integrated with NLP systems to provide such interfaces. Usually, general-purpose speech engines are trained on large `general' corpus. However, when such engines are used for specific domains, they may not recognize domain-specific words well, and may produce erroneous output. Further, the accent and the environmental conditions in which the speaker speaks a sentence may induce the speech engine to inaccurately recognize certain words. The subsequent natural language question-answering does not produce the requisite results as the question does not accurately represent what the speaker intended. Thus, the speech engine's output may need to be adapted for a domain before further natural language processing is carried out. We present two mechanisms for such an adaptation, one based on evolutionary development and the other based on machine learning, and show how we can repair the speech-output to make the subsequent natural language question-answering better.\n    ",
        "submission_date": "2017-10-12T00:00:00",
        "last_modified_date": "2017-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06931",
        "title": "OhioState at IJCNLP-2017 Task 4: Exploring Neural Architectures for Multilingual Customer Feedback Analysis",
        "authors": [
            "Dushyanta Dhyani"
        ],
        "abstract": "This paper describes our systems for IJCNLP 2017 Shared Task on Customer Feedback Analysis. We experimented with simple neural architectures that gave competitive performance on certain tasks. This includes shallow CNN and Bi-Directional LSTM architectures with Facebook's Fasttext as a baseline model. Our best performing model was in the Top 5 systems using the Exact-Accuracy and Micro-Average-F1 metrics for the Spanish (85.28% for both) and French (70% and 73.17% respectively) task, and outperformed all the other models on comment (87.28%) and meaningless (51.85%) tags using Micro Average F1 by Tags metric for the French task.\n    ",
        "submission_date": "2017-10-18T00:00:00",
        "last_modified_date": "2017-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06937",
        "title": "Embedding-Based Speaker Adaptive Training of Deep Neural Networks",
        "authors": [
            "Xiaodong Cui",
            "Vaibhava Goel",
            "George Saon"
        ],
        "abstract": "An embedding-based speaker adaptive training (SAT) approach is proposed and investigated in this paper for deep neural network acoustic modeling. In this approach, speaker embedding vectors, which are a constant given a particular speaker, are mapped through a control network to layer-dependent element-wise affine transformations to canonicalize the internal feature representations at the output of hidden layers of a main network. The control network for generating the speaker-dependent mappings is jointly estimated with the main network for the overall speaker adaptive acoustic modeling. Experiments on large vocabulary continuous speech recognition (LVCSR) tasks show that the proposed SAT scheme can yield superior performance over the widely-used speaker-aware training using i-vectors with speaker-adapted input features.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07032",
        "title": "SLING: A framework for frame semantic parsing",
        "authors": [
            "Michael Ringgaard",
            "Rahul Gupta",
            "Fernando C. N. Pereira"
        ],
        "abstract": "We describe SLING, a framework for parsing natural language into semantic frames. SLING supports general transition-based, neural-network parsing with bidirectional LSTM input encoding and a Transition Based Recurrent Unit (TBRU) for output decoding. The parsing model is trained end-to-end using only the text tokens as input. The transition system has been designed to output frame graphs directly without any intervening symbolic representation. The SLING framework includes an efficient and scalable frame store implementation as well as a neural network JIT compiler for fast inference during parsing. SLING is implemented in C++ and it is available for download on GitHub.\n    ",
        "submission_date": "2017-10-19T00:00:00",
        "last_modified_date": "2017-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07045",
        "title": "Unsupervised Context-Sensitive Spelling Correction of English and Dutch Clinical Free-Text with Word and Character N-Gram Embeddings",
        "authors": [
            "Pieter Fivez",
            "Simon \u0160uster",
            "Walter Daelemans"
        ],
        "abstract": "We present an unsupervised context-sensitive spelling correction method for clinical free-text that uses word and character n-gram embeddings. Our method generates misspelling replacement candidates and ranks them according to their semantic fit, by calculating a weighted cosine similarity between the vectorized representation of a candidate and the misspelling context. To tune the parameters of this model, we generate self-induced spelling error corpora. We perform our experiments for two languages. For English, we greatly outperform off-the-shelf spelling correction tools on a manually annotated MIMIC-III test set, and counter the frequency bias of a noisy channel model, showing that neural embeddings can be successfully exploited to improve upon the state-of-the-art. For Dutch, we also outperform an off-the-shelf spelling correction tool on manually annotated clinical records from the Antwerp University Hospital, but can offer no empirical evidence that our method counters the frequency bias of a noisy channel model in this case as well. However, both our context-sensitive model and our implementation of the noisy channel model obtain high scores on the test set, establishing a state-of-the-art for Dutch clinical spelling correction with the noisy channel model.\n    ",
        "submission_date": "2017-10-19T00:00:00",
        "last_modified_date": "2017-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07177",
        "title": "Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description",
        "authors": [
            "Desmond Elliott",
            "Stella Frank",
            "Lo\u00efc Barrault",
            "Fethi Bougares",
            "Lucia Specia"
        ],
        "abstract": "We present the results from the second shared task on multimodal machine translation and multilingual image description. Nine teams submitted 19 systems to two tasks. The multimodal translation task, in which the source sentence is supplemented by an image, was extended with a new language (French) and two new test sets. The multilingual image description task was changed such that at test time, only the image is given. Compared to last year, multimodal systems improved, but text-only systems remain competitive.\n    ",
        "submission_date": "2017-10-19T00:00:00",
        "last_modified_date": "2017-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07210",
        "title": "Multi-Task Label Embedding for Text Classification",
        "authors": [
            "Honglun Zhang",
            "Liqiang Xiao",
            "Wenqing Chen",
            "Yongkun Wang",
            "Yaohui Jin"
        ],
        "abstract": "Multi-task learning in text classification leverages implicit correlations among related tasks to extract common features and yield performance gains. However, most previous works treat labels of each task as independent and meaningless one-hot vectors, which cause a loss of potential information and makes it difficult for these models to jointly learn three or more tasks. In this paper, we propose Multi-Task Label Embedding to convert labels in text classification into semantic vectors, thereby turning the original tasks into vector matching tasks. We implement unsupervised, supervised and semi-supervised models of Multi-Task Label Embedding, all utilizing semantic correlations among tasks and making it particularly convenient to scale and transfer as more tasks are involved. Extensive experiments on five benchmark datasets for text classification show that our models can effectively improve performances of related tasks with semantic representations of labels and additional information from each other.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07388",
        "title": "Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation Models",
        "authors": [
            "Yi Luan",
            "Chris Brockett",
            "Bill Dolan",
            "Jianfeng Gao",
            "Michel Galley"
        ],
        "abstract": "Building a persona-based conversation agent is challenging owing to the lack of large amounts of speaker-specific conversation data for model training. This paper addresses the problem by proposing a multi-task learning approach to training neural conversation models that leverages both conversation data across speakers and other types of data pertaining to the speaker and speaker roles to be modeled. Experiments show that our approach leads to significant improvements over baseline model quality, generating responses that capture more precisely speakers' traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2017-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07394",
        "title": "Recognizing Explicit and Implicit Hate Speech Using a Weakly Supervised Two-path Bootstrapping Approach",
        "authors": [
            "Lei Gao",
            "Alexis Kuppersmith",
            "Ruihong Huang"
        ],
        "abstract": "In the wake of a polarizing election, social media is laden with hateful content. To address various limitations of supervised hate speech classification methods including corpus bias and huge cost of annotation, we propose a weakly supervised two-path bootstrapping approach for an online hate speech detection model leveraging large-scale unlabeled data. This system significantly outperforms hate speech detection systems that are trained in a supervised manner using manually annotated data. Applying this model on a large quantity of tweets collected before, after, and on election day reveals motivations and patterns of inflammatory language.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2018-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07395",
        "title": "Detecting Online Hate Speech Using Context Aware Models",
        "authors": [
            "Lei Gao",
            "Ruihong Huang"
        ],
        "abstract": "In the wake of a polarizing election, the cyber world is laden with hate speech. Context accompanying a hate speech text is useful for identifying hate speech, which however has been largely overlooked in existing datasets and hate speech detection models. In this paper, we provide an annotated corpus of hate speech with context information well kept. Then we propose two types of hate speech detection models that incorporate context information, a logistic regression model with context features and a neural network model with learning components for context. Our evaluation shows that both models outperform a strong baseline by around 3% to 4% in F1 score and combining these two models further improve the performance by another 7% in F1 score.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2018-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07441",
        "title": "A Semantically Motivated Approach to Compute ROUGE Scores",
        "authors": [
            "Elaheh ShafieiBavani",
            "Mohammad Ebrahimi",
            "Raymond Wong",
            "Fang Chen"
        ],
        "abstract": "ROUGE is one of the first and most widely used evaluation metrics for text summarization. However, its assessment merely relies on surface similarities between peer and model summaries. Consequently, ROUGE is unable to fairly evaluate abstractive summaries including lexical variations and paraphrasing. Exploring the effectiveness of lexical resource-based models to address this issue, we adopt a graph-based algorithm into ROUGE to capture the semantic similarities between peer and model summaries. Our semantically motivated approach computes ROUGE scores based on both lexical and semantic similarities. Experiment results over TAC AESOP datasets indicate that exploiting the lexico-semantic similarity of the words used in summaries would significantly help ROUGE correlate better with human judgments.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2017-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07503",
        "title": "Local Word Vectors Guiding Keyphrase Extraction",
        "authors": [
            "Eirini Papagiannopoulou",
            "Grigorios Tsoumakas"
        ],
        "abstract": "Automated keyphrase extraction is a fundamental textual information processing task concerned with the selection of representative phrases from a document that summarize its content. This work presents a novel unsupervised method for keyphrase extraction, whose main innovation is the use of local word embeddings (in particular GloVe vectors), i.e., embeddings trained from the single document under consideration. We argue that such local representation of words and keyphrases are able to accurately capture their semantics in the context of the document they are part of, and therefore can help in improving keyphrase extraction quality. Empirical results offer evidence that indeed local representations lead to better keyphrase extraction results compared to both embeddings trained on very large third corpora or larger corpora consisting of several documents of the same scientific field and to other state-of-the-art unsupervised keyphrase extraction methods.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2018-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07695",
        "title": "Verb Pattern: A Probabilistic Semantic Representation on Verbs",
        "authors": [
            "Wanyun Cui",
            "Xiyou Zhou",
            "Hangyu Lin",
            "Yanghua Xiao",
            "Haixun Wang",
            "Seung-won Hwang",
            "Wei Wang"
        ],
        "abstract": "Verbs are important in semantic understanding of natural language. Traditional verb representations, such as FrameNet, PropBank, VerbNet, focus on verbs' roles. These roles are too coarse to represent verbs' semantics. In this paper, we introduce verb patterns to represent verbs' semantics, such that each pattern corresponds to a single semantic of the verb. First we analyze the principles for verb patterns: generality and specificity. Then we propose a nonparametric model based on description length. Experimental results prove the high effectiveness of verb patterns. We further apply verb patterns to context-aware conceptualization, to show that verb patterns are helpful in semantic-related tasks.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2017-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07729",
        "title": "Is space a word, too?",
        "authors": [
            "Jake Ryland Williams",
            "Giovanni C. Santia"
        ],
        "abstract": "For words, rank-frequency distributions have long been heralded for adherence to a potentially-universal phenomenon known as Zipf's law. The hypothetical form of this empirical phenomenon was refined by Ben\u00eeot Mandelbrot to that which is presently referred to as the Zipf-Mandelbrot law. Parallel to this, Herbet Simon proposed a selection model potentially explaining Zipf's law. However, a significant dispute between Simon and Mandelbrot, notable empirical exceptions, and the lack of a strong empirical connection between Simon's model and the Zipf-Mandelbrot law have left the questions of universality and mechanistic generation open. We offer a resolution to these issues by exhibiting how the dark matter of word segmentation, i.e., space, punctuation, etc., connect the Zipf-Mandelbrot law to Simon's mechanistic process. This explains Mandelbrot's refinement as no more than a fudge factor, accommodating the effects of the exclusion of the rank-frequency dark matter. Thus, integrating these non-word objects resolves a more-generalized rank-frequency law. Since this relies upon the integration of space, etc., we find support for the hypothesis that $all$ are generated by common processes, indicating from a physical perspective that space is a word, too.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2017-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07770",
        "title": "Text Coherence Analysis Based on Deep Neural Network",
        "authors": [
            "Baiyun Cui",
            "Yingming Li",
            "Yaqing Zhang",
            "Zhongfei Zhang"
        ],
        "abstract": "In this paper, we propose a novel deep coherence model (DCM) using a convolutional neural network architecture to capture the text coherence. The text coherence problem is investigated with a new perspective of learning sentence distributional representation and text coherence modeling simultaneously. In particular, the model captures the interactions between sentences by computing the similarities of their distributional representations. Further, it can be easily trained in an end-to-end fashion. The proposed model is evaluated on a standard Sentence Ordering task. The experimental results demonstrate its effectiveness and promise in coherence assessment showing a significant improvement over the state-of-the-art by a wide margin.\n    ",
        "submission_date": "2017-10-21T00:00:00",
        "last_modified_date": "2017-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07960",
        "title": "How big is big enough? Unsupervised word sense disambiguation using a very large corpus",
        "authors": [
            "Piotr Przyby\u0142a"
        ],
        "abstract": "In this paper, the problem of disambiguating a target word for Polish is approached by searching for related words with known meaning. These relatives are used to build a training corpus from unannotated text. This technique is improved by proposing new rich sources of replacements that substitute the traditional requirement of monosemy with heuristics based on wordnet relations. The na\u00efve Bayesian classifier has been modified to account for an unknown distribution of senses. A corpus of 600 million web documents (594 billion tokens), gathered by the NEKST search engine allows us to assess the relationship between training set size and disambiguation accuracy. The classifier is evaluated using both a wordnet baseline and a corpus with 17,314 manually annotated occurrences of 54 ambiguous words.\n    ",
        "submission_date": "2017-10-22T00:00:00",
        "last_modified_date": "2017-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08015",
        "title": "Bringing Semantic Structures to User Intent Detection in Online Medical Queries",
        "authors": [
            "Chenwei Zhang",
            "Nan Du",
            "Wei Fan",
            "Yaliang Li",
            "Chun-Ta Lu",
            "Philip S. Yu"
        ],
        "abstract": "The Internet has revolutionized healthcare by offering medical information ubiquitously to patients via web search. The healthcare status, complex medical information needs of patients are expressed diversely and implicitly in their medical text queries. Aiming to better capture a focused picture of user's medical-related information search and shed insights on their healthcare information access strategies, it is challenging yet rewarding to detect structured user intentions from their diversely expressed medical text queries. We introduce a graph-based formulation to explore structured concept transitions for effective user intent detection in medical queries, where each node represents a medical concept mention and each directed edge indicates a medical concept transition. A deep model based on multi-task learning is introduced to extract structured semantic transitions from user queries, where the model extracts word-level medical concept mentions as well as sentence-level concept transitions collectively. A customized graph-based mutual transfer loss function is designed to impose explicit constraints and further exploit the contribution of mentioning a medical concept word to the implication of a semantic transition. We observe an 8% relative improvement in AUC and 23% relative reduction in coverage error by comparing the proposed model with the best baseline model for the concept transition inference task on real-world medical text queries.\n    ",
        "submission_date": "2017-10-22T00:00:00",
        "last_modified_date": "2017-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08048",
        "title": "A First Step in Combining Cognitive Event Features and Natural Language Representations to Predict Emotions",
        "authors": [
            "Andres Campero",
            "Bjarke Felbo",
            "Joshua B. Tenenbaum",
            "Rebecca Saxe"
        ],
        "abstract": "We explore the representational space of emotions by combining methods from different academic fields. Cognitive science has proposed appraisal theory as a view on human emotion with previous research showing how human-rated abstract event features can predict fine-grained emotions and capture the similarity space of neural patterns in mentalizing brain regions. At the same time, natural language processing (NLP) has demonstrated how transfer and multitask learning can be used to cope with scarcity of annotated data for text modeling.\n",
        "submission_date": "2017-10-23T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08246",
        "title": "Testing the limits of unsupervised learning for semantic similarity",
        "authors": [
            "Richa Sharma",
            "Muktabh Mayank Srivastava"
        ],
        "abstract": "Semantic Similarity between two sentences can be defined as a way to determine how related or unrelated two sentences are. The task of Semantic Similarity in terms of distributed representations can be thought to be generating sentence embeddings (dense vectors) which take both context and meaning of sentence in account. Such embeddings can be produced by multiple methods, in this paper we try to evaluate LSTM auto encoders for generating these embeddings. Unsupervised algorithms (auto encoders to be specific) just try to recreate their inputs, but they can be forced to learn order (and some inherent meaning to some extent) by creating proper bottlenecks. We try to evaluate how properly can algorithms trained just on plain English Sentences learn to figure out Semantic Similarity, without giving them any sense of what meaning of a sentence is.\n    ",
        "submission_date": "2017-10-23T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08312",
        "title": "Attending to All Mention Pairs for Full Abstract Biological Relation Extraction",
        "authors": [
            "Patrick Verga",
            "Emma Strubell",
            "Ofer Shai",
            "Andrew McCallum"
        ],
        "abstract": "Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. However, many relation types, particularly in biomedical text, are expressed across sentences or require a large context to disambiguate. We propose a model to consider all mention and entity pairs simultaneously in order to make a prediction. We encode full paper abstracts using an efficient self-attention encoder and form pairwise predictions between all mentions with a bi-affine operation. An entity-pair wise pooling aggregates mention pair scores to make a final prediction while alleviating training noise by performing within document multi-instance learning. We improve our model's performance by jointly training the model to predict named entities and adding an additional corpus of weakly labeled data. We demonstrate our model's effectiveness by achieving the state of the art on the Biocreative V Chemical Disease Relation dataset for models without KB resources, outperforming ensembles of models which use hand-crafted features and additional linguistic resources.\n    ",
        "submission_date": "2017-10-23T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08321",
        "title": "Content Based Document Recommender using Deep Learning",
        "authors": [
            "Nishant Nikhil",
            "Muktabh Mayank Srivastava"
        ],
        "abstract": "With the recent advancements in information technology there has been a huge surge in amount of data available. But information retrieval technology has not been able to keep up with this pace of information generation resulting in over spending of time for retrieving relevant information. Even though systems exist for assisting users to search a database along with filtering and recommending relevant information, but recommendation system which uses content of documents for recommendation still have a long way to mature. Here we present a Deep Learning based supervised approach to recommend similar documents based on the similarity of content. We combine the C-DSSM model with Word2Vec distributed representations of words to create a novel model to classify a document pair as relevant/irrelavant by assigning a score to it. Using our model retrieval of documents can be done in O(1) time and the memory complexity is O(n), where n is number of documents.\n    ",
        "submission_date": "2017-10-23T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08396",
        "title": "Deep Health Care Text Classification",
        "authors": [
            "Vinayakumar R",
            "Barathi Ganesh HB",
            "Anand Kumar M",
            "Soman KP"
        ],
        "abstract": "Health related social media mining is a valuable apparatus for the early recognition of the diverse antagonistic medicinal conditions. Mostly, the existing methods are based on machine learning with knowledge-based learning. This working note presents the Recurrent neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media mining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories. The experiments are conducted on 2nd Social Media Mining for Health Applications Shared Task at AMIA 2017. The experiment results are considerable; however the proposed method is appropriate for the health text classification. This is primarily due to the reason that, it doesn't rely on any feature engineering mechanisms.\n    ",
        "submission_date": "2017-10-23T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08451",
        "title": "Combining Lexical Features and a Supervised Learning Approach for Arabic Sentiment Analysis",
        "authors": [
            "Samhaa R. El-Beltagy",
            "Talaat Khalil",
            "Amal Halaby",
            "Muhammad Hammad"
        ],
        "abstract": "The importance of building sentiment analysis tools for Arabic social media has been recognized during the past couple of years, especially with the rapid increase in the number of Arabic social media users. One of the main difficulties in tackling this problem is that text within social media is mostly colloquial, with many dialects being used within social media platforms. In this paper, we present a set of features that were integrated with a machine learning based sentiment analysis model and applied on Egyptian, Saudi, Levantine, and MSA Arabic social media datasets. Many of the proposed features were derived through the use of an Arabic Sentiment Lexicon. The model also presents emoticon based features, as well as input text related features such as the number of segments within the text, the length of the text, whether the text ends with a question mark or not, etc. We show that the presented features have resulted in an increased accuracy across six of the seven datasets we've experimented with and which are all benchmarked. Since the developed model out-performs all existing Arabic sentiment analysis systems that have publicly available datasets, we can state that this model presents state-of-the-art in Arabic sentiment analysis.\n    ",
        "submission_date": "2017-10-23T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08458",
        "title": "NileTMRG at SemEval-2017 Task 4: Arabic Sentiment Analysis",
        "authors": [
            "Samhaa R. El-Beltagy",
            "Mona El Kalamawy",
            "Abu Bakr Soliman"
        ],
        "abstract": "This paper describes two systems that were used by the authors for addressing Arabic Sentiment Analysis as part of SemEval-2017, task 4. The authors participated in three Arabic related subtasks which are: Subtask A (Message Polarity Classification), Sub-task B (Topic-Based Message Polarity classification) and Subtask D (Tweet quantification) using the team name of NileTMRG. For subtask A, we made use of our previously developed sentiment analyzer which we augmented with a scored lexicon. For subtasks B and D, we used an ensemble of three different classifiers. The first classifier was a convolutional neural network for which we trained (word2vec) word embeddings. The second classifier consisted of a MultiLayer Perceptron, while the third classifier was a Logistic regression model that takes the same input as the second classifier. Voting between the three classifiers was used to determine the final outcome. The output from task B, was quantified to produce the results for task D. In all three Arabic related tasks in which NileTMRG participated, the team ranked at number one.\n    ",
        "submission_date": "2017-10-23T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08691",
        "title": "BENGAL: An Automatic Benchmark Generator for Entity Recognition and Linking",
        "authors": [
            "Axel-Cyrille Ngonga Ngomo",
            "Michael R\u00f6der",
            "Diego Moussallem",
            "Ricardo Usbeck",
            "Ren\u00e9 Speck"
        ],
        "abstract": "The manual creation of gold standards for named entity recognition and entity linking is time- and resource-intensive. Moreover, recent works show that such gold standards contain a large proportion of mistakes in addition to being difficult to maintain. We hence present BENGAL, a novel automatic generation of such gold standards as a complement to manually created benchmarks. The main advantage of our benchmarks is that they can be readily generated at any time. They are also cost-effective while being guaranteed to be free of annotation errors. We compare the performance of 11 tools on benchmarks in English generated by BENGAL and on 16benchmarks created manually. We show that our approach can be ported easily across languages by presenting results achieved by 4 tools on both Brazilian Portuguese and Spanish. Overall, our results suggest that our automatic benchmark generation approach can create varied benchmarks that have characteristics similar to those of existing benchmarks. Our approach is open-source. Our experimental results are available at ",
        "submission_date": "2017-10-24T00:00:00",
        "last_modified_date": "2018-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08721",
        "title": "Clickbait Identification using Neural Networks",
        "authors": [
            "Philippe Thomas"
        ],
        "abstract": "This paper presents the results of our participation in the Clickbait Detection Challenge 2017. The system relies on a fusion of neural networks, incorporating different types of available informations. It does not require any linguistic preprocessing, and hence generalizes more easily to new domains and languages. The final combined model achieves a mean squared error of 0.0428, an accuracy of 0.826, and a F1 score of 0.564. According to the official evaluation metric the system ranked 6th of the 13 participating teams.\n    ",
        "submission_date": "2017-10-24T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09137",
        "title": "Linking Tweets with Monolingual and Cross-Lingual News using Transformed Word Embeddings",
        "authors": [
            "Aditya Mogadala",
            "Dominik Jung",
            "Achim Rettinger"
        ],
        "abstract": "Social media platforms have grown into an important medium to spread information about an event published by the traditional media, such as news articles. Grouping such diverse sources of information that discuss the same topic in varied perspectives provide new insights. But the gap in word usage between informal social media content such as tweets and diligently written content (e.g. news articles) make such assembling difficult. In this paper, we propose a transformation framework to bridge the word usage gap between tweets and online news articles across languages by leveraging their word embeddings. Using our framework, word embeddings extracted from tweets and news articles are aligned closer to each other across languages, thus facilitating the identification of similarity between news articles and tweets. Experimental results show a notable improvement over baselines for monolingual tweets and news articles comparison, while new findings are reported for cross-lingual comparison.\n    ",
        "submission_date": "2017-10-25T00:00:00",
        "last_modified_date": "2017-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09233",
        "title": "A Simple Text Analytics Model To Assist Literary Criticism: comparative approach and example on James Joyce against Shakespeare and the Bible",
        "authors": [
            "Renato Fabbri",
            "Luis Henrique Garcia"
        ],
        "abstract": "Literary analysis, criticism or studies is a largely valued field with dedicated journals and researchers which remains mostly within the humanities scope. Text analytics is the computer-aided process of deriving information from texts. In this article we describe a simple and generic model for performing literary analysis using text analytics. The method relies on statistical measures of: 1) token and sentence sizes and 2) Wordnet synset features. These measures are then used in Principal Component Analysis where the texts to be analyzed are observed against Shakespeare and the Bible, regarded as reference literature. The model is validated by analyzing selected works from James Joyce (1882-1941), one of the most important writers of the 20th century. We discuss the consistency of this approach, the reasons why we did not use other techniques (e.g. part-of-speech tagging) and the ways by which the analysis model might be adapted and enhanced.\n    ",
        "submission_date": "2017-10-24T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09306",
        "title": "Exploring the Use of Text Classification in the Legal Domain",
        "authors": [
            "Octavia-Maria Sulea",
            "Marcos Zampieri",
            "Shervin Malmasi",
            "Mihaela Vela",
            "Liviu P. Dinu",
            "Josef van Genabith"
        ],
        "abstract": "In this paper, we investigate the application of text classification methods to support law professionals. We present several experiments applying machine learning techniques to predict with high accuracy the ruling of the French Supreme Court and the law area to which a case belongs to. We also investigate the influence of the time period in which a ruling was made on the form of the case description and the extent to which we need to mask information in a full case ruling to automatically obtain training and test data that resembles case descriptions. We developed a mean probability ensemble system combining the output of multiple SVM classifiers. We report results of 98% average F1 score in predicting a case ruling, 96% F1 score for predicting the law area of a case, and 87.07% F1 score on estimating the date of a ruling.\n    ",
        "submission_date": "2017-10-25T00:00:00",
        "last_modified_date": "2017-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09340",
        "title": "Non-Projective Dependency Parsing with Non-Local Transitions",
        "authors": [
            "Daniel Fern\u00e1ndez-Gonz\u00e1lez",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "abstract": "We present a novel transition system, based on the Covington non-projective parser, introducing non-local transitions that can directly create arcs involving nodes to the left of the current focus positions. This avoids the need for long sequences of No-Arc transitions to create long-distance arcs, thus alleviating error propagation. The resulting parser outperforms the original version and achieves the best accuracy on the Stanford Dependencies conversion of the Penn Treebank among greedy transition-based algorithms.\n    ",
        "submission_date": "2017-10-25T00:00:00",
        "last_modified_date": "2018-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09589",
        "title": "ALL-IN-1: Short Text Classification with One Model for All Languages",
        "authors": [
            "Barbara Plank"
        ],
        "abstract": "We present ALL-IN-1, a simple model for multilingual text classification that does not require any parallel data. It is based on a traditional Support Vector Machine classifier exploiting multilingual word embeddings and character n-grams. Our model is simple, easily extendable yet very effective, overall ranking 1st (out of 12 teams) in the IJCNLP 2017 shared task on customer feedback analysis in four languages: English, French, Japanese and Spanish.\n    ",
        "submission_date": "2017-10-26T00:00:00",
        "last_modified_date": "2017-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09617",
        "title": "Streaming Small-Footprint Keyword Spotting using Sequence-to-Sequence Models",
        "authors": [
            "Yanzhang He",
            "Rohit Prabhavalkar",
            "Kanishka Rao",
            "Wei Li",
            "Anton Bakhtin",
            "Ian McGraw"
        ],
        "abstract": "We develop streaming keyword spotting systems using a recurrent neural network transducer (RNN-T) model: an all-neural, end-to-end trained, sequence-to-sequence model which jointly learns acoustic and language model components. Our models are trained to predict either phonemes or graphemes as subword units, thus allowing us to detect arbitrary keyword phrases, without any out-of-vocabulary words. In order to adapt the models to the requirements of keyword spotting, we propose a novel technique which biases the RNN-T system towards a specific keyword of interest.\n",
        "submission_date": "2017-10-26T00:00:00",
        "last_modified_date": "2017-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09753",
        "title": "Impact of Coreference Resolution on Slot Filling",
        "authors": [
            "Heike Adel",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "In this paper, we demonstrate the importance of coreference resolution for natural language processing on the example of the TAC Slot Filling shared task. We illustrate the strengths and weaknesses of automatic coreference resolution systems and provide experimental results to show that they improve performance in the slot filling end-to-end setting. Finally, we publish KBPchains, a resource containing automatically extracted coreference chains from the TAC source corpus in order to support other researchers working on this topic.\n    ",
        "submission_date": "2017-10-26T00:00:00",
        "last_modified_date": "2017-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09867",
        "title": "Understanding Early Word Learning in Situated Artificial Agents",
        "authors": [
            "Felix Hill",
            "Stephen Clark",
            "Karl Moritz Hermann",
            "Phil Blunsom"
        ],
        "abstract": "Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome challenges that infants face when learning their first words. While it is notable that models with no meaningful prior knowledge overcome these obstacles, researchers currently lack a clear understanding of how they do so, a problem that we attempt to address in this paper. For maximum control and generality, we focus on a simple neural network-based language learning agent, trained via policy-gradient methods, which can interpret single-word instructions in a simulated 3D world. Whilst the goal is not to explicitly model infant word learning, we take inspiration from experimental paradigms in developmental psychology and apply some of these to the artificial agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel method for visualising semantic representations in the agent.\n    ",
        "submission_date": "2017-10-26T00:00:00",
        "last_modified_date": "2019-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09942",
        "title": "CANDiS: Coupled & Attention-Driven Neural Distant Supervision",
        "authors": [
            "Tushar Nagarajan",
            "Sharmistha",
            "Partha Talukdar"
        ],
        "abstract": "Distant Supervision for Relation Extraction uses heuristically aligned text data with an existing knowledge base as training data. The unsupervised nature of this technique allows it to scale to web-scale relation extraction tasks, at the expense of noise in the training data. Previous work has explored relationships among instances of the same entity-pair to reduce this noise, but relationships among instances across entity-pairs have not been fully exploited. We explore the use of inter-instance couplings based on verb-phrase and entity type similarities. We propose a novel technique, CANDiS, which casts distant supervision using inter-instance coupling into an end-to-end neural network model. CANDiS incorporates an attention module at the instance-level to model the multi-instance nature of this problem. CANDiS outperforms existing state-of-the-art techniques on a standard benchmark dataset.\n    ",
        "submission_date": "2017-10-26T00:00:00",
        "last_modified_date": "2017-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10224",
        "title": "BridgeNets: Student-Teacher Transfer Learning Based on Recursive Neural Networks and its Application to Distant Speech Recognition",
        "authors": [
            "Jaeyoung Kim",
            "Mostafa El-Khamy",
            "Jungwon Lee"
        ],
        "abstract": "Despite the remarkable progress achieved on automatic speech recognition, recognizing far-field speeches mixed with various noise sources is still a challenging task. In this paper, we introduce novel student-teacher transfer learning, BridgeNet which can provide a solution to improve distant speech recognition. There are two key features in BridgeNet. First, BridgeNet extends traditional student-teacher frameworks by providing multiple hints from a teacher network. Hints are not limited to the soft labels from a teacher network. Teacher's intermediate feature representations can better guide a student network to learn how to denoise or dereverberate noisy input. Second, the proposed recursive architecture in the BridgeNet can iteratively improve denoising and recognition performance. The experimental results of BridgeNet showed significant improvements in tackling the distant speech recognition problem, where it achieved up to 13.24% relative WER reductions on AMI corpus compared to a baseline neural network without teacher's hints.\n    ",
        "submission_date": "2017-10-27T00:00:00",
        "last_modified_date": "2018-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10248",
        "title": "Tensor network language model",
        "authors": [
            "Vasily Pestun",
            "Yiannis Vlassopoulos"
        ],
        "abstract": "We propose a new statistical model suitable for machine learning of systems with long distance correlations such as natural languages. The model is based on directed acyclic graph decorated by multi-linear tensor maps in the vertices and vector spaces in the edges, called tensor network. Such tensor networks have been previously employed for effective numerical computation of the renormalization group flow on the space of effective quantum field theories and lattice models of statistical mechanics. We provide explicit algebro-geometric analysis of the parameter moduli space for tree graphs, discuss model properties and applications such as statistical translation.\n    ",
        "submission_date": "2017-10-27T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10280",
        "title": "One-shot and few-shot learning of word embeddings",
        "authors": [
            "Andrew K. Lampinen",
            "James L. McClelland"
        ],
        "abstract": "Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.\n    ",
        "submission_date": "2017-10-27T00:00:00",
        "last_modified_date": "2018-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10361",
        "title": "Deep Residual Learning for Small-Footprint Keyword Spotting",
        "authors": [
            "Raphael Tang",
            "Jimmy Lin"
        ],
        "abstract": "We explore the application of deep residual learning and dilated convolutions to the keyword spotting task, using the recently-released Google Speech Commands Dataset as our benchmark. Our best residual network (ResNet) implementation significantly outperforms Google's previous convolutional neural networks in terms of accuracy. By varying model depth and width, we can achieve compact models that also outperform previous small-footprint variants. To our knowledge, we are the first to examine these approaches for keyword spotting, and our results establish an open-source state-of-the-art reference to support the development of future speech-based interfaces.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2018-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10398",
        "title": "A Study of All-Convolutional Encoders for Connectionist Temporal Classification",
        "authors": [
            "Kalpesh Krishna",
            "Liang Lu",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "abstract": "Connectionist temporal classification (CTC) is a popular sequence prediction approach for automatic speech recognition that is typically used with models based on recurrent neural networks (RNNs). We explore whether deep convolutional neural networks (CNNs) can be used effectively instead of RNNs as the \"encoder\" in CTC. CNNs lack an explicit representation of the entire sequence, but have the advantage that they are much faster to train. We present an exploration of CNNs as encoders for CTC models, in the context of character-based (lexicon-free) automatic speech recognition. In particular, we explore a range of one-dimensional convolutional layers, which are particularly efficient. We compare the performance of our CNN-based models against typical RNNbased models in terms of training time, decoding time, model size and word error rate (WER) on the Switchboard Eval2000 corpus. We find that our CNN-based models are close in performance to LSTMs, while not matching them, and are much faster to train and decode.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2018-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10453",
        "title": "Inducing Regular Grammars Using Recurrent Neural Networks",
        "authors": [
            "Mor Cohen",
            "Avi Caciularu",
            "Idan Rejwan",
            "Jonathan Berant"
        ],
        "abstract": "Grammar induction is the task of learning a grammar from a set of examples. Recently, neural networks have been shown to be powerful learning machines that can identify patterns in streams of data. In this work we investigate their effectiveness in inducing a regular grammar from data, without any assumptions about the grammar. We train a recurrent neural network to distinguish between strings that are in or outside a regular language, and utilize an algorithm for extracting the learned finite-state automaton. We apply this method to several regular languages and find unexpected results regarding the connections between the network's states that may be regarded as evidence for generalization.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2018-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10498",
        "title": "Topic Based Sentiment Analysis Using Deep Learning",
        "authors": [
            "Sharath T. S.",
            "Shubhangi Tandon"
        ],
        "abstract": "In this paper , we tackle Sentiment Analysis conditioned on a Topic in Twitter data using Deep Learning . We propose a 2-tier approach : In the first phase we create our own Word Embeddings and see that they do perform better than state-of-the-art embeddings when used with standard classifiers. We then perform inference on these embeddings to learn more about a word with respect to all the topics being considered, and also the top n-influencing words for each topic. In the second phase we use these embeddings to predict the sentiment of the tweet with respect to a given topic, and all other topics under discussion.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2017-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10504",
        "title": "Phase Conductor on Multi-layered Attentions for Machine Comprehension",
        "authors": [
            "Rui Liu",
            "Wei Wei",
            "Weiguang Mao",
            "Maria Chikina"
        ],
        "abstract": "Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and self-matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10520",
        "title": "A Dual Encoder Sequence to Sequence Model for Open-Domain Dialogue Modeling",
        "authors": [
            "Sharath T.S.",
            "Shubhangi Tandon",
            "Ryan Bauer"
        ],
        "abstract": "Ever since the successful application of sequence to sequence learning for neural machine translation systems, interest has surged in its applicability towards language generation in other problem domains. Recent work has investigated the use of these neural architectures towards modeling open-domain conversational dialogue, where it has been found that although these models are capable of learning a good distributional language model, dialogue coherence is still of concern. Unlike translation, conversation is much more a one-to-many mapping from utterance to a response, and it is even more pressing that the model be aware of the preceding flow of conversation. In this paper we propose to tackle this problem by introducing previous conversational context in terms of latent representations of dialogue acts over time. We inject the latent context representations into a sequence to sequence neural network in the form of dialog acts using a second encoder to enhance the quality and the coherence of the conversations generated. The main task of this research work is to show that adding latent variables that capture discourse relations does indeed result in more coherent responses when compared to conventional sequence to sequence models.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2017-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10574",
        "title": "Personalized word representations Carrying Personalized Semantics Learned from Social Network Posts",
        "authors": [
            "Zih-Wei Lin",
            "Tzu-Wei Sung",
            "Hung-Yi Lee",
            "Lin-Shan Lee"
        ],
        "abstract": "Distributed word representations have been shown to be very useful in various natural language processing (NLP) application tasks. These word vectors learned from huge corpora very often carry both semantic and syntactic information of words. However, it is well known that each individual user has his own language patterns because of different factors such as interested topics, friend groups, social activities, wording habits, etc., which may imply some kind of personalized semantics. With such personalized semantics, the same word may imply slightly differently for different users. For example, the word \"Cappuccino\" may imply \"Leisure\", \"Joy\", \"Excellent\" for a user enjoying coffee, by only a kind of drink for someone else. Such personalized semantics of course cannot be carried by the standard universal word vectors trained with huge corpora produced by many people. In this paper, we propose a framework to train different personalized word vectors for different users based on the very successful continuous skip-gram model using the social network data posted by many individual users. In this framework, universal background word vectors are first learned from the background corpora, and then adapted by the personalized corpus for each individual user to learn the personalized word vectors. We use two application tasks to evaluate the quality of the personalized word vectors obtained in this way, the user prediction task and the sentence completion task. These personalized word vectors were shown to carry some personalized semantics and offer improved performance on these two evaluation tasks.\n    ",
        "submission_date": "2017-10-29T00:00:00",
        "last_modified_date": "2017-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10585",
        "title": "Path-Based Attention Neural Model for Fine-Grained Entity Typing",
        "authors": [
            "Denghui Zhang",
            "Pengshan Cai",
            "Yantao Jia",
            "Manling Li",
            "Yuanzhuo Wang",
            "Xueqi Cheng"
        ],
        "abstract": "Fine-grained entity typing aims to assign entity mentions in the free text with types arranged in a hierarchical structure. Traditional distant supervision based methods employ a structured data source as a weak supervision and do not need hand-labeled data, but they neglect the label noise in the automatically labeled training corpus. Although recent studies use many features to prune wrong data ahead of training, they suffer from error propagation and bring much complexity. In this paper, we propose an end-to-end typing model, called the path-based attention neural model (PAN), to learn a noise- robust performance by leveraging the hierarchical structure of types. Experiments demonstrate its effectiveness.\n    ",
        "submission_date": "2017-10-29T00:00:00",
        "last_modified_date": "2018-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10586",
        "title": "Evaluation of Automatic Video Captioning Using Direct Assessment",
        "authors": [
            "Yvette Graham",
            "George Awad",
            "Alan Smeaton"
        ],
        "abstract": "We present Direct Assessment, a method for manually assessing the quality of automatically-generated captions for video. Evaluating the accuracy of video captions is particularly difficult because for any given video clip there is no definitive ground truth or correct answer against which to measure. Automatic metrics for comparing automatic video captions against a manual caption such as BLEU and METEOR, drawn from techniques used in evaluating machine translation, were used in the TRECVid video captioning task in 2016 but these are shown to have weaknesses. The work presented here brings human assessment into the evaluation by crowdsourcing how well a caption describes a video. We automatically degrade the quality of some sample captions which are assessed manually and from this we are able to rate the quality of the human assessors, a factor we take into account in the evaluation. Using data from the TRECVid video-to-text task in 2016, we show how our direct assessment method is replicable and robust and should scale to where there many caption-generation techniques to be evaluated.\n    ",
        "submission_date": "2017-10-29T00:00:00",
        "last_modified_date": "2017-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10609",
        "title": "Finding Dominant User Utterances And System Responses in Conversations",
        "authors": [
            "Dhiraj Madan",
            "Sachindra Joshi"
        ],
        "abstract": "There are several dialog frameworks which allow manual specification of intents and rule based dialog flow. The rule based framework provides good control to dialog designers at the expense of being more time consuming and laborious. The job of a dialog designer can be reduced if we could identify pairs of user intents and corresponding responses automatically from prior conversations between users and agents. In this paper we propose an approach to find these frequent user utterances (which serve as examples for intents) and corresponding agent responses. We propose a novel SimCluster algorithm that extends standard K-means algorithm to simultaneously cluster user utterances and agent utterances by taking their adjacency information into account. The method also aligns these clusters to provide pairs of intents and response groups. We compare our results with those produced by using simple Kmeans clustering on a real dataset and observe upto 10% absolute improvement in F1-scores. Through our experiments on synthetic dataset, we show that our algorithm gains more advantage over K-means algorithm when the data has large variance.\n    ",
        "submission_date": "2017-10-29T00:00:00",
        "last_modified_date": "2017-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10639",
        "title": "JESC: Japanese-English Subtitle Corpus",
        "authors": [
            "Reid Pryzant",
            "Yongjoo Chung",
            "Dan Jurafsky",
            "Denny Britz"
        ],
        "abstract": "In this paper we describe the Japanese-English Subtitle Corpus (JESC). JESC is a large Japanese-English parallel corpus covering the underrepresented domain of conversational dialogue. It consists of more than 3.2 million examples, making it the largest freely available dataset of its kind. The corpus was assembled by crawling and aligning subtitles found on the web. The assembly process incorporates a number of novel preprocessing elements to ensure high monolingual fluency and accurate bilingual alignments. We summarize its contents and evaluate its quality using human experts and baseline machine translation (MT) systems.\n    ",
        "submission_date": "2017-10-29T00:00:00",
        "last_modified_date": "2018-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10723",
        "title": "Simple and Effective Multi-Paragraph Reading Comprehension",
        "authors": [
            "Christopher Clark",
            "Matt Gardner"
        ],
        "abstract": "We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.\n    ",
        "submission_date": "2017-10-29T00:00:00",
        "last_modified_date": "2017-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10739",
        "title": "Learning neural trans-dimensional random field language models with noise-contrastive estimation",
        "authors": [
            "Bin Wang",
            "Zhijian Ou"
        ],
        "abstract": "Trans-dimensional random field language models (TRF LMs) where sentences are modeled as a collection of random fields, have shown close performance with LSTM LMs in speech recognition and are computationally more efficient in inference. However, the training efficiency of neural TRF LMs is not satisfactory, which limits the scalability of TRF LMs on large training corpus. In this paper, several techniques on both model formulation and parameter estimation are proposed to improve the training efficiency and the performance of neural TRF LMs. First, TRFs are reformulated in the form of exponential tilting of a reference distribution. Second, noise-contrastive estimation (NCE) is introduced to jointly estimate the model parameters and normalization constants. Third, we extend the neural TRF LMs by marrying the deep convolutional neural network (CNN) and the bidirectional LSTM into the potential function to extract the deep hierarchical features and bidirectionally sequential features. Utilizing all the above techniques enables the successful and efficient training of neural TRF LMs on a 40x larger training set with only 1/3 training time and further reduces the WER with relative reduction of 4.7% on top of a strong LSTM LM baseline.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10774",
        "title": "Sequence-to-Sequence ASR Optimization via Reinforcement Learning",
        "authors": [
            "Andros Tjandra",
            "Sakriani Sakti",
            "Satoshi Nakamura"
        ],
        "abstract": "Despite the success of sequence-to-sequence approaches in automatic speech recognition (ASR) systems, the models still suffer from several problems, mainly due to the mismatch between the training and inference conditions. In the sequence-to-sequence architecture, the model is trained to predict the grapheme of the current time-step given the input of speech signal and the ground-truth grapheme history of the previous time-steps. However, it remains unclear how well the model approximates real-world speech during inference. Thus, generating the whole transcription from scratch based on previous predictions is complicated and errors can propagate over time. Furthermore, the model is optimized to maximize the likelihood of training data instead of error rate evaluation metrics that actually quantify recognition quality. This paper presents an alternative strategy for training sequence-to-sequence ASR models by adopting the idea of reinforcement learning (RL). Unlike the standard training scheme with maximum likelihood estimation, our proposed approach utilizes the policy gradient algorithm. We can (1) sample the whole transcription based on the model's prediction in the training process and (2) directly optimize the model with negative Levenshtein distance as the reward. Experimental results demonstrate that we significantly improved the performance compared to a model trained only with maximum likelihood estimation.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2018-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10777",
        "title": "Understanding Hidden Memories of Recurrent Neural Networks",
        "authors": [
            "Yao Ming",
            "Shaozu Cao",
            "Ruixiang Zhang",
            "Zhen Li",
            "Yuanzhe Chen",
            "Yangqiu Song",
            "Huamin Qu"
        ],
        "abstract": "Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10994",
        "title": "Conceptual Text Summarizer: A new model in continuous vector space",
        "authors": [
            "Mohammad Ebrahim Khademi",
            "Mohammad Fakhredanesh",
            "Seyed Mojtaba Hoseini"
        ],
        "abstract": "Traditional methods of summarization are not cost-effective and possible today. Extractive summarization is a process that helps to extract the most important sentences from a text automatically and generates a short informative summary. In this work, we propose an unsupervised method to summarize Persian texts. This method is a novel hybrid approach that clusters the concepts of the text using deep learning and traditional statistical methods. First we produce a word embedding based on Hamshahri2 corpus and a dictionary of word frequencies. Then the proposed algorithm extracts the keywords of the document, clusters its concepts, and finally ranks the sentences to produce the summary. We evaluated the proposed method on Pasokh single-document corpus using the ROUGE evaluation measure. Without using any hand-crafted features, our proposed method achieves state-of-the-art results. We compared our unsupervised method with the best supervised Persian methods and we achieved an overall improvement of ROUGE-2 recall score of 7.5%.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2018-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11035",
        "title": "Machine Translation of Low-Resource Spoken Dialects: Strategies for Normalizing Swiss German",
        "authors": [
            "Pierre-Edouard Honnet",
            "Andrei Popescu-Belis",
            "Claudiu Musat",
            "Michael Baeriswyl"
        ],
        "abstract": "The goal of this work is to design a machine translation (MT) system for a low-resource family of dialects, collectively known as Swiss German, which are widely spoken in Switzerland but seldom written. We collected a significant number of parallel written resources to start with, up to a total of about 60k words. Moreover, we identified several other promising data sources for Swiss German. Then, we designed and compared three strategies for normalizing Swiss German input in order to address the regional diversity. We found that character-based neural MT was the best solution for text normalization. In combination with phrase-based statistical MT, our solution reached 36% BLEU score when translating from the Bernese dialect. This value, however, decreases as the testing data becomes more remote from the training one, geographically and topically. These resources and normalization techniques are a first step towards full MT of Swiss German dialects.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2018-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11041",
        "title": "Unsupervised Neural Machine Translation",
        "authors": [
            "Mikel Artetxe",
            "Gorka Labaka",
            "Eneko Agirre",
            "Kyunghyun Cho"
        ],
        "abstract": "In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2018-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11154",
        "title": "Creation of an Annotated Corpus of Spanish Radiology Reports",
        "authors": [
            "Viviana Cotik",
            "Dar\u00edo Filippo",
            "Roland Roller",
            "Hans Uszkoreit",
            "Feiyu Xu"
        ],
        "abstract": "This paper presents a new annotated corpus of 513 anonymized radiology reports written in Spanish. Reports were manually annotated with entities, negation and uncertainty terms and relations. The corpus was conceived as an evaluation resource for named entity recognition and relation extraction algorithms, and as input for the use of supervised methods. Biomedical annotated resources are scarce due to confidentiality issues and associated costs. This work provides some guidelines that could help other researchers to undertake similar tasks.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11169",
        "title": "Indirect Supervision for Relation Extraction using Question-Answer Pairs",
        "authors": [
            "Zeqiu Wu",
            "Xiang Ren",
            "Frank F. Xu",
            "Ji Li",
            "Jiawei Han"
        ],
        "abstract": "Automatic relation extraction (RE) for types of interest is of great importance for interpreting massive text corpora in an efficient manner. Traditional RE models have heavily relied on human-annotated corpus for training, which can be costly in generating labeled data and become obstacles when dealing with more relation types. Thus, more RE extraction systems have shifted to be built upon training data automatically acquired by linking to knowledge bases (distant supervision). However, due to the incompleteness of knowledge bases and the context-agnostic labeling, the training data collected via distant supervision (DS) can be very noisy. In recent years, as increasing attention has been brought to tackling question-answering (QA) tasks, user feedback or datasets of such tasks become more accessible. In this paper, we propose a novel framework, ReQuest, to leverage question-answer pairs as an indirect source of supervision for relation extraction, and study how to use such supervision to reduce noise induced from DS. Our model jointly embeds relation mentions, types, QA entity mention pairs and text features in two low-dimensional spaces (RE and QA), where objects with same relation types or semantically similar question-answer pairs have similar representations. Shared features connect these two spaces, carrying clearer semantic knowledge from both sources. ReQuest, then use these learned embeddings to estimate the types of test relation mentions. We formulate a global objective function and adopt a novel margin-based QA loss to reduce noise in DS by exploiting semantic evidence from the QA dataset. Our experimental results achieve an average of 11% improvement in F1 score on two public RE datasets combined with TREC QA dataset.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2017-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11277",
        "title": "Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning",
        "authors": [
            "Baolin Peng",
            "Xiujun Li",
            "Jianfeng Gao",
            "Jingjing Liu",
            "Yun-Nung Chen",
            "Kam-Fai Wong"
        ],
        "abstract": "This paper presents a new method --- adversarial advantage actor-critic (Adversarial A2C), which significantly improves the efficiency of dialogue policy learning in task-completion dialogue systems. Inspired by generative adversarial networks (GAN), we train a discriminator to differentiate responses/actions generated by dialogue agents from responses/actions by experts. Then, we incorporate the discriminator as another critic into the advantage actor-critic (A2C) framework, to encourage the dialogue agent to explore state-action within the regions where the agent takes actions similar to those of the experts. Experimental results in a movie-ticket booking domain show that the proposed Adversarial A2C can accelerate policy exploration efficiently.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2018-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11301",
        "title": "A generalized parsing framework for Abstract Grammars",
        "authors": [
            "Daniel Harasim",
            "Chris Bruno",
            "Eva Portelance",
            "Martin Rohrmeier",
            "Timothy J. O'Donnell"
        ],
        "abstract": "This technical report presents a general framework for parsing a variety of grammar formalisms. We develop a grammar formalism, called an Abstract Grammar, which is general enough to represent grammars at many levels of the hierarchy, including Context Free Grammars, Minimalist Grammars, and Generalized Context-free Grammars. We then develop a single parsing framework which is capable of parsing grammars which are at least up to GCFGs on the hierarchy. Our parsing framework exposes a grammar interface, so that it can parse any particular grammar formalism that can be reduced to an Abstract Grammar.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2018-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11332",
        "title": "Improving Social Media Text Summarization by Learning Sentence Weight Distribution",
        "authors": [
            "Jingjing Xu"
        ],
        "abstract": "Recently, encoder-decoder models are widely used in social media text summarization. However, these models sometimes select noise words in irrelevant sentences as part of a summary by error, thus declining the performance. In order to inhibit irrelevant sentences and focus on key information, we propose an effective approach by learning sentence weight distribution. In our model, we build a multi-layer perceptron to predict sentence weights. During training, we use the ROUGE score as an alternative to the estimated sentence weight, and try to minimize the gap between estimated weights and predicted weights. In this way, we encourage our model to focus on the key sentences, which have high relevance with the summary. Experimental results show that our approach outperforms baselines on a large-scale social media corpus.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2017-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11334",
        "title": "Shallow Discourse Parsing with Maximum Entropy Model",
        "authors": [
            "Jingjing Xu"
        ],
        "abstract": "In recent years, more research has been devoted to studying the subtask of the complete shallow discourse parsing, such as indentifying discourse connective and arguments of connective. There is a need to design a full discourse parser to pull these subtasks together. So we develop a discourse parser turning the free text into discourse relations. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. Each component applies the maximum entropy model with abundant lexical and syntax features extracted from the Penn Discourse Tree-bank. The head-based representation of the PDTB is adopted in the arguments identifier, which turns the problem of indentifying the arguments of discourse connective into finding the head and end of the arguments. In the non-explicit identifier, the contextual type features like words which have high frequency and can reflect the discourse relation are introduced to improve the performance of non-explicit identifier. Compared with other methods, experimental results achieve the considerable performance.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2017-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11344",
        "title": "A Sequential Matching Framework for Multi-turn Response Selection in Retrieval-based Chatbots",
        "authors": [
            "Yu Wu",
            "Wei Wu",
            "Chen Xing",
            "Can Xu",
            "Zhoujun Li",
            "Ming Zhou"
        ],
        "abstract": "We study the problem of response selection for multi-turn conversation in retrieval-based chatbots. The task requires matching a response candidate with a conversation context, whose challenges include how to recognize important parts of the context, and how to model the relationships among utterances in the context. Existing matching methods may lose important information in contexts as we can interpret them with a unified framework in which contexts are transformed to fixed-length vectors without any interaction with responses before matching. The analysis motivates us to propose a new matching framework that can sufficiently carry the important information in contexts to matching and model the relationships among utterances at the same time. The new framework, which we call a sequential matching framework (SMF), lets each utterance in a context interacts with a response candidate at the first step and transforms the pair to a matching vector. The matching vectors are then accumulated following the order of the utterances in the context with a recurrent neural network (RNN) which models the relationships among the utterances. The context-response matching is finally calculated with the hidden states of the RNN. Under SMF, we propose a sequential convolutional network and sequential attention network and conduct experiments on two public data sets to test their performance. Experimental results show that both models can significantly outperform the state-of-the-art matching methods. We also show that the models are interpretable with visualizations that provide us insights on how they capture and leverage the important information in contexts for matching.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2017-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11350",
        "title": "Grammar Induction for Minimalist Grammars using Variational Bayesian Inference : A Technical Report",
        "authors": [
            "Eva Portelance",
            "Amelia Bruno",
            "Daniel Harasim",
            "Leon Bergen",
            "Timothy J. O'Donnell"
        ],
        "abstract": "The following technical report presents a formal approach to probabilistic minimalist grammar parameter estimation. We describe a formalization of a minimalist grammar. We then present an algorithm for the application of variational Bayesian inference to this formalization.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2019-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11475",
        "title": "A Neural-Symbolic Approach to Design of CAPTCHA",
        "authors": [
            "Qiuyuan Huang",
            "Paul Smolensky",
            "Xiaodong He",
            "Li Deng",
            "Dapeng Wu"
        ],
        "abstract": "CAPTCHAs based on reading text are susceptible to machine-learning-based attacks due to recent significant advances in deep learning (DL). To address this, this paper promotes image/visual captioning based CAPTCHAs, which is robust against machine-learning-based attacks. To develop image/visual-captioning-based CAPTCHAs, this paper proposes a new image captioning architecture by exploiting tensor product representations (TPR), a structured neural-symbolic framework developed in cognitive science over the past 20 years, with the aim of integrating DL with explicit language structures and rules. We call it the Tensor Product Generation Network (TPGN). The key ideas of TPGN are: 1) unsupervised learning of role-unbinding vectors of words via a TPR-based deep neural network, and 2) integration of TPR with typical DL architectures including Long Short-Term Memory (LSTM) models. The novelty of our approach lies in its ability to generate a sentence and extract partial grammatical structure of the sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. Experimental results demonstrate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2017-10-29T00:00:00",
        "last_modified_date": "2018-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11601",
        "title": "Whodunnit? Crime Drama as a Case for Natural Language Understanding",
        "authors": [
            "Lea Frermann",
            "Shay B. Cohen",
            "Mirella Lapata"
        ],
        "abstract": "In this paper we argue that crime drama exemplified in television programs such as CSI:Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it. We propose to treat crime drama as a new inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed. We develop a new dataset based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2017-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00043",
        "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
        "authors": [
            "Guillaume Lample",
            "Alexis Conneau",
            "Ludovic Denoyer",
            "Marc'Aurelio Ranzato"
        ],
        "abstract": "Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2018-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00092",
        "title": "Summarizing Dialogic Arguments from Social Media",
        "authors": [
            "Amita Misra",
            "Shereen Oraby",
            "Shubhangi Tandon",
            "Sharath TS",
            "Pranav Anand",
            "Marilyn Walker"
        ],
        "abstract": "Online argumentative dialog is a rich source of information on popular beliefs and opinions that could be useful to companies as well as governmental or public policy agencies. Compact, easy to read, summaries of these dialogues would thus be highly valuable. A priori, it is not even clear what form such a summary should take. Previous work on summarization has primarily focused on summarizing written texts, where the notion of an abstract of the text is well defined. We collect gold standard training data consisting of five human summaries for each of 161 dialogues on the topics of Gay Marriage, Gun Control and Abortion. We present several different computational models aimed at identifying segments of the dialogues whose content should be used for the summary, using linguistic features and Word2vec features with both SVMs and Bidirectional LSTMs. We show that we can identify the most important arguments by using the dialog context with a best F-measure of 0.74 for gun control, 0.71 for gay marriage, and 0.67 for abortion.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2017-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00106",
        "title": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering",
        "authors": [
            "Caiming Xiong",
            "Victor Zhong",
            "Richard Socher"
        ],
        "abstract": "Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00155",
        "title": "Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples",
        "authors": [
            "Pavlos Vougiouklis",
            "Hady Elsahar",
            "Lucie-Aim\u00e9e Kaffee",
            "Christoph Gravier",
            "Frederique Laforest",
            "Jonathon Hare",
            "Elena Simperl"
        ],
        "abstract": "Most people do not interact with Semantic Web data directly. Unless they have the expertise to understand the underlying technology, they need textual or visual interfaces to help them make sense of it. We explore the problem of generating natural language summaries for Semantic Web data. This is non-trivial, especially in an open-domain context. To address this problem, we explore the use of neural networks. Our system encodes the information from a set of triples into a vector of fixed dimensionality and generates a textual summary by conditioning the output on the encoded vector. We train and evaluate our models on two corpora of loosely aligned Wikipedia snippets and DBpedia and Wikidata triples with promising results.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00179",
        "title": "Keyword-based Query Comprehending via Multiple Optimized-Demand Augmentation",
        "authors": [
            "Boyuan Pan",
            "Hao Li",
            "Zhou Zhao",
            "Deng Cai",
            "Xiaofei He"
        ],
        "abstract": "In this paper, we consider the problem of machine reading task when the questions are in the form of keywords, rather than natural language. In recent years, researchers have achieved significant success on machine reading comprehension tasks, such as SQuAD and TriviaQA. These datasets provide a natural language question sentence and a pre-selected passage, and the goal is to answer the question according to the passage. However, in the situation of interacting with machines by means of text, people are more likely to raise a query in form of several keywords rather than a complete sentence. The keyword-based query comprehension is a new challenge, because small variations to a question may completely change its semantical information, thus yield different answers. In this paper, we propose a novel neural network system that consists a Demand Optimization Model based on a passage-attention neural machine translation and a Reader Model that can find the answer given the optimized question. The Demand Optimization Model optimizes the original query and output multiple reconstructed questions, then the Reader Model takes the new questions as input and locate the answers from the passage. To make predictions robust, an evaluation mechanism will score the reconstructed questions so the final answer strike a good balance between the quality of both the Demand Optimization Model and the Reader Model. Experimental results on several datasets show that our framework significantly improves multiple strong baselines on this challenging task.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00247",
        "title": "Improved Text Language Identification for the South African Languages",
        "authors": [
            "Bernardt Duvenhage",
            "Mfundo Ntini",
            "Phala Ramonyai"
        ],
        "abstract": "Virtual assistants and text chatbots have recently been gaining popularity. Given the short message nature of text-based chat interactions, the language identification systems of these bots might only have 15 or 20 characters to make a prediction. However, accurate text language identification is important, especially in the early stages of many multilingual natural language processing pipelines.\n",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00279",
        "title": "Paraphrase Generation with Deep Reinforcement Learning",
        "authors": [
            "Zichao Li",
            "Xin Jiang",
            "Lifeng Shang",
            "Hang Li"
        ],
        "abstract": "Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP), and plays a key role in a number of applications such as question answering, search, and dialogue. In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a \\textit{generator} and an \\textit{evaluator}, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Empirical study shows that the learned evaluator can guide the generator to produce more accurate paraphrases. Experimental results demonstrate the proposed models (the generators) outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2018-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00294",
        "title": "Towards Automatic Generation of Entertaining Dialogues in Chinese Crosstalks",
        "authors": [
            "Shikang Du",
            "Xiaojun Wan",
            "Yajie Ye"
        ],
        "abstract": "Crosstalk, also known by its Chinese name xiangsheng, is a traditional Chinese comedic performing art featuring jokes and funny dialogues, and one of China's most popular cultural elements. It is typically in the form of a dialogue between two performers for the purpose of bringing laughter to the audience, with one person acting as the leading comedian and the other as the supporting role. Though general dialogue generation has been widely explored in previous studies, it is unknown whether such entertaining dialogues can be automatically generated or not. In this paper, we for the first time investigate the possibility of automatic generation of entertaining dialogues in Chinese crosstalks. Given the utterance of the leading comedian in each dialogue, our task aims to generate the replying utterance of the supporting role. We propose a humor-enhanced translation model to address this task and human evaluation results demonstrate the efficacy of our proposed model. The feasibility of automatic entertaining dialogue generation is also verified.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00309",
        "title": "Improving Neural Machine Translation through Phrase-based Forced Decoding",
        "authors": [
            "Jingyi Zhang",
            "Masao Utiyama",
            "Eiichro Sumita",
            "Graham Neubig",
            "Satoshi Nakamura"
        ],
        "abstract": "Compared to traditional statistical machine translation (SMT), neural machine translation (NMT) often sacrifices adequacy for the sake of fluency. We propose a method to combine the advantages of traditional SMT and NMT by exploiting an existing phrase-based SMT model to compute the phrase-based decoding cost for an NMT output and then using this cost to rerank the n-best NMT outputs. The main challenge in implementing this approach is that NMT outputs may not be in the search space of the standard phrase-based decoding algorithm, because the search space of phrase-based SMT is limited by the phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00331",
        "title": "Semantic Structure and Interpretability of Word Embeddings",
        "authors": [
            "Lutfi Kerem Senel",
            "Ihsan Utlu",
            "Veysel Yucesoy",
            "Aykut Koc",
            "Tolga Cukur"
        ],
        "abstract": "Dense word embeddings, which encode semantic meanings of words to low dimensional vector spaces have become very popular in natural language processing (NLP) research due to their state-of-the-art performances in many NLP tasks. Word embeddings are substantially successful in capturing semantic relations among words, so a meaningful semantic structure must be present in the respective vector spaces. However, in many cases, this semantic structure is broadly and heterogeneously distributed across the embedding dimensions, which makes interpretation a big challenge. In this study, we propose a statistical method to uncover the latent semantic structure in the dense word embeddings. To perform our analysis we introduce a new dataset (SEMCAT) that contains more than 6500 words semantically grouped under 110 categories. We further propose a method to quantify the interpretability of the word embeddings; the proposed method is a practical alternative to the classical word intrusion test that requires human intervention.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2018-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00350",
        "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
        "authors": [
            "Brenden M. Lake",
            "Marco Baroni"
        ],
        "abstract": "Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2018-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00354",
        "title": "JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis",
        "authors": [
            "Ryosuke Sonobe",
            "Shinnosuke Takamichi",
            "Hiroshi Saruwatari"
        ],
        "abstract": "Thanks to improvements in machine learning techniques including deep learning, a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist. In this paper, we designed a novel Japanese speech corpus, named the \"JSUT corpus,\" that is aimed at achieving end-to-end speech synthesis. The corpus consists of 10 hours of reading-style speech data and its transcription and covers all of the main pronunciations of daily-use Japanese characters. In this paper, we describe how we designed and analyzed the corpus. The corpus is freely available online.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2017-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00482",
        "title": "Learning with Latent Language",
        "authors": [
            "Jacob Andreas",
            "Dan Klein",
            "Sergey Levine"
        ],
        "abstract": "The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter's loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00513",
        "title": "Evaluating Discourse Phenomena in Neural Machine Translation",
        "authors": [
            "Rachel Bawden",
            "Rico Sennrich",
            "Alexandra Birch",
            "Barry Haddow"
        ],
        "abstract": "For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models' ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50% accuracy on our coreference test set and 53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5% for coreference and 57% for coherence/cohesion), highlighting the importance of target-side context.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2018-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00520",
        "title": "Uncovering Latent Style Factors for Expressive Speech Synthesis",
        "authors": [
            "Yuxuan Wang",
            "RJ Skerry-Ryan",
            "Ying Xiao",
            "Daisy Stanton",
            "Joel Shor",
            "Eric Battenberg",
            "Rob Clark",
            "Rif A. Saurous"
        ],
        "abstract": "Prosodic modeling is a core problem in speech synthesis. The key challenge is producing desirable prosody from textual input containing only phonetic information. In this preliminary study, we introduce the concept of \"style tokens\" in Tacotron, a recently proposed end-to-end neural speech synthesis model. Using style tokens, we aim to extract independent prosodic styles from training data. We show that without annotation data or an explicit supervision signal, our approach can automatically learn a variety of prosodic variations in a purely data-driven way. Importantly, each style token corresponds to a fixed style factor regardless of the given text sequence. As a result, we can control the prosodic style of synthetic speech in a somewhat predictable and globally consistent way.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2017-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00529",
        "title": "Text Annotation Graphs: Annotating Complex Natural Language Phenomena",
        "authors": [
            "Angus G. Forbes",
            "Kristine Lee",
            "Gus Hahn-Powell",
            "Marco A. Valenzuela-Esc\u00e1rcega",
            "Mihai Surdeanu"
        ],
        "abstract": "This paper introduces a new web-based software tool for annotating text, Text Annotation Graphs, or TAG. It provides functionality for representing complex relationships between words and word phrases that are not available in other software tools, including the ability to define and visualize relationships between the relationships themselves (semantic hypergraphs). Additionally, we include an approach to representing text annotations in which annotation subgraphs, or semantic summaries, are used to show relationships outside of the sequential context of the text itself. Users can use these subgraphs to quickly find similar structures within the current document or external annotated documents. Initially, TAG was developed to support information extraction tasks on a large database of biomedical articles. However, our software is flexible enough to support a wide range of annotation tasks for any domain. Examples are provided that showcase TAG's capabilities on morphological parsing and event extraction tasks. The TAG software is available at: ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2018-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00549",
        "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding",
        "authors": [
            "Anjishnu Kumar",
            "Arpit Gupta",
            "Julian Chan",
            "Sam Tucker",
            "Bjorn Hoffmeister",
            "Markus Dreyer",
            "Stanislav Peshterliev",
            "Ankur Gandhe",
            "Denis Filiminov",
            "Ariya Rastrow",
            "Christian Monson",
            "Agnika Kumar"
        ],
        "abstract": "This paper presents the design of the machine learning architecture that underlies the Alexa Skills Kit (ASK) a large scale Spoken Language Understanding (SLU) Software Development Kit (SDK) that enables developers to extend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the infrastructure powers over 25,000 skills deployed through the ASK, as well as AWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability and a rapid iteration cycle for third party developers. It imposes inductive biases that allow it to learn robust SLU models from extremely small and sparse datasets and, in doing so, removes significant barriers to entry for software developers and dialogue systems researchers.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2018-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00681",
        "title": "Extracting an English-Persian Parallel Corpus from Comparable Corpora",
        "authors": [
            "Akbar Karimi",
            "Ebrahim Ansari",
            "Bahram Sadeghi Bigham"
        ],
        "abstract": "Parallel data are an important part of a reliable Statistical Machine Translation (SMT) system. The more of these data are available, the better the quality of the SMT system. However, for some language pairs such as Persian-English, parallel sources of this kind are scarce. In this paper, a bidirectional method is proposed to extract parallel sentences from English and Persian document aligned Wikipedia. Two machine translation systems are employed to translate from Persian to English and the reverse after which an IR system is used to measure the similarity of the translated sentences. Adding the extracted sentences to the training data of the existing SMT systems is shown to improve the quality of the translation. Furthermore, the proposed method slightly outperforms the one-directional approach. The extracted corpus consists of about 200,000 sentences which have been sorted by their degree of similarity calculated by the IR system and is freely available for public access on the Web.\n    ",
        "submission_date": "2017-11-02T00:00:00",
        "last_modified_date": "2019-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00768",
        "title": "SRL4ORL: Improving Opinion Role Labeling using Multi-task Learning with Semantic Role Labeling",
        "authors": [
            "Ana Marasovi\u0107",
            "Anette Frank"
        ],
        "abstract": "For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question \"Who expressed what kind of sentiment towards what?\". Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.\n    ",
        "submission_date": "2017-11-02T00:00:00",
        "last_modified_date": "2018-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00894",
        "title": "Multi-Mention Learning for Reading Comprehension with Neural Cascades",
        "authors": [
            "Swabha Swayamdipta",
            "Ankur P. Parikh",
            "Tom Kwiatkowski"
        ],
        "abstract": "Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal, since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information at the representation level from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.\n    ",
        "submission_date": "2017-11-02T00:00:00",
        "last_modified_date": "2018-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00938",
        "title": "A Comparison of Feature-Based and Neural Scansion of Poetry",
        "authors": [
            "Manex Agirrezabal",
            "I\u00f1aki Alegria",
            "Mans Hulden"
        ],
        "abstract": "Automatic analysis of poetic rhythm is a challenging task that involves linguistics, literature, and computer science. When the language to be analyzed is known, rule-based systems or data-driven methods can be used. In this paper, we analyze poetic rhythm in English and Spanish. We show that the representations of data learned from character-based neural models are more informative than the ones from hand-crafted features, and that a Bi-LSTM+CRF-model produces state-of-the art accuracy on scansion of poetry in two languages. Results also show that the information about whole word structure, and not just independent syllables, is highly informative for performing scansion.\n    ",
        "submission_date": "2017-11-02T00:00:00",
        "last_modified_date": "2017-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01006",
        "title": "Towards Neural Machine Translation with Partially Aligned Corpora",
        "authors": [
            "Yining Wang",
            "Yang Zhao",
            "Jiajun Zhang",
            "Chengqing Zong",
            "Zhengshan Xue"
        ],
        "abstract": "While neural machine translation (NMT) has become the new paradigm, the parameter optimization requires large-scale parallel data which is scarce in many domains and language pairs. In this paper, we address a new translation scenario in which there only exists monolingual corpora and phrase pairs. We propose a new method towards translation with partially aligned sentence pairs which are derived from the phrase pairs and monolingual corpora. To make full use of the partially aligned corpora, we adapt the conventional NMT training method in two aspects. On one hand, different generation strategies are designed for aligned and unaligned target words. On the other hand, a different objective function is designed to model the partially aligned parts. The experiments demonstrate that our method can achieve a relatively good result in such a translation scenario, and tiny bitexts can boost translation quality to a large extent.\n    ",
        "submission_date": "2017-11-03T00:00:00",
        "last_modified_date": "2017-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01048",
        "title": "Dual Language Models for Code Switched Speech Recognition",
        "authors": [
            "Saurabh Garg",
            "Tanmay Parekh",
            "Preethi Jyothi"
        ],
        "abstract": "In this work, we present a simple and elegant approach to language modeling for bilingual code-switched text. Since code-switching is a blend of two or more different languages, a standard bilingual language model can be improved upon by using structures of the monolingual language models. We propose a novel technique called dual language models, which involves building two complementary monolingual language models and combining them using a probabilistic model for switching between the two. We evaluate the efficacy of our approach using a conversational Mandarin-English speech corpus. We prove the robustness of our model by showing significant improvements in perplexity measures over the standard bilingual language model without the use of any external information. Similar consistent improvements are also reflected in automatic speech recognition error rates.\n    ",
        "submission_date": "2017-11-03T00:00:00",
        "last_modified_date": "2018-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01068",
        "title": "Compressing Word Embeddings via Deep Compositional Code Learning",
        "authors": [
            "Raphael Shu",
            "Hideki Nakayama"
        ],
        "abstract": "Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.\n    ",
        "submission_date": "2017-11-03T00:00:00",
        "last_modified_date": "2017-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01100",
        "title": "One Model to Rule them all: Multitask and Multilingual Modelling for Lexical Analysis",
        "authors": [
            "Johannes Bjerva"
        ],
        "abstract": "When learning a new skill, you take advantage of your preexisting skills and knowledge. For instance, if you are a skilled violinist, you will likely have an easier time learning to play cello. Similarly, when learning a new language you take advantage of the languages you already speak. For instance, if your native language is Norwegian and you decide to learn Dutch, the lexical overlap between these two languages will likely benefit your rate of language acquisition. This thesis deals with the intersection of learning multiple tasks and learning multiple languages in the context of Natural Language Processing (NLP), which can be defined as the study of computational processing of human language. Although these two types of learning may seem different on the surface, we will see that they share many similarities.\n",
        "submission_date": "2017-11-03T00:00:00",
        "last_modified_date": "2017-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01161",
        "title": "Learning Filterbanks from Raw Speech for Phone Recognition",
        "authors": [
            "Neil Zeghidour",
            "Nicolas Usunier",
            "Iasonas Kokkinos",
            "Thomas Schatz",
            "Gabriel Synnaeve",
            "Emmanuel Dupoux"
        ],
        "abstract": "We train a bank of complex filters that operates on the raw waveform and is fed into a convolutional neural network for end-to-end phone recognition. These time-domain filterbanks (TD-filterbanks) are initialized as an approximation of mel-filterbanks, and then fine-tuned jointly with the remaining convolutional architecture. We perform phone recognition experiments on TIMIT and show that for several architectures, models trained on TD-filterbanks consistently outperform their counterparts trained on comparable mel-filterbanks. We get our best performance by learning all front-end steps, from pre-emphasis up to averaging. Finally, we observe that the filters at convergence have an asymmetric impulse response, and that some of them remain almost analytic.\n    ",
        "submission_date": "2017-11-03T00:00:00",
        "last_modified_date": "2018-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01362",
        "title": "\"Attention\" for Detecting Unreliable News in the Information Age",
        "authors": [
            "Venkatesh Duppada"
        ],
        "abstract": "An Unreliable news is any piece of information which is false or misleading, deliberately spread to promote political, ideological and financial agendas. Recently the problem of unreliable news has got a lot of attention as the number instances of using news and social media outlets for propaganda have increased rapidly. This poses a serious threat to society, which calls for technology to automatically and reliably identify unreliable news sources. This paper is an effort made in this direction to build systems for detecting unreliable news articles. In this paper, various NLP algorithms were built and evaluated on Unreliable News Data 2017 dataset. Variants of hierarchical attention networks (HAN) are presented for encoding and classifying news articles which achieve the best results of 0.944 ROC-AUC. Finally, Attention layer weights are visualized to understand and give insight into the decisions made by HANs. The results obtained are very promising and encouraging to deploy and use these systems in the real world to mitigate the problem of unreliable news.\n    ",
        "submission_date": "2017-11-03T00:00:00",
        "last_modified_date": "2017-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01386",
        "title": "Predicting Discharge Medications at Admission Time Based on Deep Learning",
        "authors": [
            "Yuan Yang",
            "Pengtao Xie",
            "Xin Gao",
            "Carol Cheng",
            "Christy Li",
            "Hongbao Zhang",
            "Eric Xing"
        ],
        "abstract": "Predicting discharge medications right after a patient being admitted is an important clinical decision, which provides physicians with guidance on what type of medication regimen to plan for and what possible changes on initial medication may occur during an inpatient stay. It also facilitates medication reconciliation process with easy detection of medication discrepancy at discharge time to improve patient safety. However, since the information available upon admission is limited and patients' condition may evolve during an inpatient stay, these predictions could be a difficult decision for physicians to make. In this work, we investigate how to leverage deep learning technologies to assist physicians in predicting discharge medications based on information documented in the admission note. We build a convolutional neural network which takes an admission note as input and predicts the medications placed on the patient at discharge time. Our method is able to distill semantic patterns from unstructured and noisy texts, and is capable of capturing the pharmacological correlations among medications. We evaluate our method on 25K patient visits and compare with 4 strong baselines. Our methods demonstrate a 20% increase in macro-averaged F1 score than the best baseline.\n    ",
        "submission_date": "2017-11-04T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01416",
        "title": "Language as a matrix product state",
        "authors": [
            "Vasily Pestun",
            "John Terilla",
            "Yiannis Vlassopoulos"
        ],
        "abstract": "We propose a statistical model for natural language that begins by considering language as a monoid, then representing it in complex matrices with a compatible translation invariant probability measure. We interpret the probability measure as arising via the Born rule from a translation invariant matrix product state.\n    ",
        "submission_date": "2017-11-04T00:00:00",
        "last_modified_date": "2017-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01427",
        "title": "Deep Stacking Networks for Low-Resource Chinese Word Segmentation with Transfer Learning",
        "authors": [
            "Jingjing Xu",
            "Xu Sun",
            "Sujian Li",
            "Xiaoyan Cai",
            "Bingzhen Wei"
        ],
        "abstract": "In recent years, neural networks have proven to be effective in Chinese word segmentation. However, this promising performance relies on large-scale training data. Neural networks with conventional architectures cannot achieve the desired results in low-resource datasets due to the lack of labelled training data. In this paper, we propose a deep stacking framework to improve the performance on word segmentation tasks with insufficient data by integrating datasets from diverse domains. Our framework consists of two parts, domain-based models and deep stacking networks. The domain-based models are used to learn knowledge from different datasets. The deep stacking networks are designed to integrate domain-based models. To reduce model conflicts, we innovatively add communication paths among models and design various structures of deep stacking networks, including Gaussian-based Stacking Networks, Concatenate-based Stacking Networks, Sequence-based Stacking Networks and Tree-based Stacking Networks. We conduct experiments on six low-resource datasets from various domains. Our proposed framework shows significant performance improvements on all datasets compared with several strong baselines.\n    ",
        "submission_date": "2017-11-04T00:00:00",
        "last_modified_date": "2017-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01505",
        "title": "Towards Linguistically Generalizable NLP Systems: A Workshop and Shared Task",
        "authors": [
            "Allyson Ettinger",
            "Sudha Rao",
            "Hal Daum\u00e9 III",
            "Emily M. Bender"
        ],
        "abstract": "This paper presents a summary of the first Workshop on Building Linguistically Generalizable Natural Language Processing Systems, and the associated Build It Break It, The Language Edition shared task. The goal of this workshop was to bring together researchers in NLP and linguistics with a shared task aimed at testing the generalizability of NLP systems beyond the distributions of their training data. We describe the motivation, setup, and participation of the shared task, provide discussion of some highlighted results, and discuss lessons learned.\n    ",
        "submission_date": "2017-11-04T00:00:00",
        "last_modified_date": "2017-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01515",
        "title": "Learning Word Embeddings from Speech",
        "authors": [
            "Yu-An Chung",
            "James Glass"
        ],
        "abstract": "In this paper, we propose a novel deep neural network architecture, Sequence-to-Sequence Audio2Vec, for unsupervised learning of fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the segments, and are close to other vectors in the embedding space if their corresponding segments are semantically similar. The design of the proposed model is based on the RNN Encoder-Decoder framework, and borrows the methodology of continuous skip-grams for training. The learned vector representations are evaluated on 13 widely used word similarity benchmarks, and achieved competitive results to that of GloVe. The biggest advantage of the proposed model is its capability of extracting semantic information of audio segments taken directly from raw speech, without relying on any other modalities such as text or images, which are challenging and expensive to collect and annotate.\n    ",
        "submission_date": "2017-11-05T00:00:00",
        "last_modified_date": "2017-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01567",
        "title": "Robust Speech Recognition Using Generative Adversarial Networks",
        "authors": [
            "Anuroop Sriram",
            "Heewoo Jun",
            "Yashesh Gaur",
            "Sanjeev Satheesh"
        ],
        "abstract": "This paper describes a general, scalable, end-to-end framework that uses the generative adversarial network (GAN) objective to enable robust speech recognition. Encoders trained with the proposed approach enjoy improved invariance by learning to map noisy audio to the same embedding space as that of clean audio. Unlike previous methods, the new framework does not rely on domain expertise or simplifying assumptions as are often needed in signal processing, and directly encourages robustness in a data-driven way. We show the new approach improves simulated far-field speech recognition of vanilla sequence-to-sequence models without specialized front-ends or preprocessing.\n    ",
        "submission_date": "2017-11-05T00:00:00",
        "last_modified_date": "2017-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01684",
        "title": "Authorship Analysis of Xenophon's Cyropaedia",
        "authors": [
            "Anjalie Field"
        ],
        "abstract": "In the past several decades, many authorship attribution studies have used computational methods to determine the authors of disputed texts. Disputed authorship is a common problem in Classics, since little information about ancient documents has survived the centuries. Many scholars have questioned the authenticity of the final chapter of Xenophon's Cyropaedia, a 4th century B.C. historical text. In this study, we use N-grams frequency vectors with a cosine similarity function and word frequency vectors with Naive Bayes Classifiers (NBC) and Support Vector Machines (SVM) to analyze the authorship of the Cyropaedia. Although the N-gram analysis shows that the epilogue of the Cyropaedia differs slightly from the rest of the work, comparing the analysis of Xenophon with analyses of Aristotle and Plato suggests that this difference is not significant. Both NBC and SVM analyses of word frequencies show that the final chapter of the Cyropaedia is closely related to the other chapters of the Cyropaedia. Therefore, this analysis suggests that the disputed chapter was written by Xenophon. This information can help scholars better understand the Cyropaedia and also demonstrates the usefulness of applying modern authorship analysis techniques to classical literature.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01701",
        "title": "Distributed Representation for Traditional Chinese Medicine Herb via Deep Learning Models",
        "authors": [
            "Wei Li",
            "Zheng Yang"
        ],
        "abstract": "Traditional Chinese Medicine (TCM) has accumulated a big amount of precious resource in the long history of development. TCM prescriptions that consist of TCM herbs are an important form of TCM treatment, which are similar to natural language documents, but in a weakly ordered fashion. Directly adapting language modeling style methods to learn the embeddings of the herbs can be problematic as the herbs are not strictly in order, the herbs in the front of the prescription can be connected to the very last ones. In this paper, we propose to represent TCM herbs with distributed representations via Prescription Level Language Modeling (PLLM). In one of our experiments, the correlation between our calculated similarity between medicines and the judgment of professionals achieves a Spearman score of 55.35 indicating a strong correlation, which surpasses human beginners (TCM related field bachelor student) by a big margin (over 10%).\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01731",
        "title": "A Survey on Dialogue Systems: Recent Advances and New Frontiers",
        "authors": [
            "Hongshen Chen",
            "Xiaorui Liu",
            "Dawei Yin",
            "Jiliang Tang"
        ],
        "abstract": "Dialogue systems have attracted more and more attention. Recent advances on dialogue systems are overwhelmingly contributed by deep learning techniques, which have been employed to enhance a wide range of big data applications such as computer vision, natural language processing, and recommender systems. For dialogue systems, deep learning can leverage a massive amount of data to learn meaningful feature representations and response generation strategies, while requiring a minimum amount of hand-crafting. In this article, we give an overview to these recent advances on dialogue systems from various perspectives and discuss some possible research directions. In particular, we generally divide existing dialogue systems into task-oriented and non-task-oriented models, then detail how deep learning techniques help them with representative algorithms and finally discuss some appealing research directions that can bring the dialogue system research into a new frontier.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2018-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01804",
        "title": "Evaluation of Croatian Word Embeddings",
        "authors": [
            "Lukas Svoboda",
            "Slobodan Beliga"
        ],
        "abstract": "Croatian is poorly resourced and highly inflected language from Slavic language family. Nowadays, research is focusing mostly on English. We created a new word analogy corpus based on the original English Word2vec word analogy corpus and added some of the specific linguistic aspects from Croatian language. Next, we created Croatian WordSim353 and RG65 corpora for a basic evaluation of word similarities. We compared created corpora on two popular word representation models, based on Word2Vec tool and fastText tool. Models has been trained on 1.37B tokens training data corpus and tested on a new robust Croatian word analogy corpus. Results show that models are able to create meaningful word representation. This research has shown that free word order and the higher morphological complexity of Croatian language influences the quality of resulting word embeddings.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2017-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01985",
        "title": "Fine-tuning Tree-LSTM for phrase-level sentiment classification on a Polish dependency treebank. Submission to PolEval task 2",
        "authors": [
            "Tomasz Korbak",
            "Paulina \u017bak"
        ],
        "abstract": "We describe a variant of Child-Sum Tree-LSTM deep neural network (Tai et al, 2015) fine-tuned for working with dependency trees and morphologically rich languages using the example of Polish. Fine-tuning included applying a custom regularization technique (zoneout, described by (Krueger et al., 2016), and further adapted for Tree-LSTMs) as well as using pre-trained word embeddings enhanced with sub-word information (Bojanowski et al., 2016). The system was implemented in PyTorch and evaluated on phrase-level sentiment labeling task as part of the PolEval competition.\n    ",
        "submission_date": "2017-11-03T00:00:00",
        "last_modified_date": "2017-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02012",
        "title": "Hi, how can I help you?: Automating enterprise IT support help desks",
        "authors": [
            "Senthil Mani",
            "Neelamadhav Gantayat",
            "Rahul Aralikatte",
            "Monika Gupta",
            "Sampath Dechu",
            "Anush Sankaran",
            "Shreya Khare",
            "Barry Mitchell",
            "Hemamalini Subramanian",
            "Hema Venkatarangan"
        ],
        "abstract": "Question answering is one of the primary challenges of natural language understanding. In realizing such a system, providing complex long answers to questions is a challenging task as opposed to factoid answering as the former needs context disambiguation. The different methods explored in the literature can be broadly classified into three categories namely: 1) classification based, 2) knowledge graph based and 3) retrieval based. Individually, none of them address the need of an enterprise wide assistance system for an IT support and maintenance domain. In this domain the variance of answers is large ranging from factoid to structured operating procedures; the knowledge is present across heterogeneous data sources like application specific documentation, ticket management systems and any single technique for a general purpose assistance is unable to scale for such a landscape. To address this, we have built a cognitive platform with capabilities adopted for this domain. Further, we have built a general purpose question answering system leveraging the platform that can be instantiated for multiple products, technologies in the support domain. The system uses a novel hybrid answering model that orchestrates across a deep learning classifier, a knowledge graph based context disambiguation module and a sophisticated bag-of-words search system. This orchestration performs context switching for a provided question and also does a smooth hand-off of the question to a human expert if none of the automated techniques can provide a confident answer. This system has been deployed across 675 internal enterprise IT support and maintenance projects.\n    ",
        "submission_date": "2017-11-02T00:00:00",
        "last_modified_date": "2017-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02013",
        "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
        "authors": [
            "Yikang Shen",
            "Zhouhan Lin",
            "Chin-Wei Huang",
            "Aaron Courville"
        ],
        "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.\n    ",
        "submission_date": "2017-11-02T00:00:00",
        "last_modified_date": "2018-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02085",
        "title": "Neural Speed Reading via Skim-RNN",
        "authors": [
            "Minjoon Seo",
            "Sewon Min",
            "Ali Farhadi",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models. In our experiments, we show that Skim-RNN can achieve significantly reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also shows that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2018-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02162",
        "title": "TAMU at KBP 2017: Event Nugget Detection and Coreference Resolution",
        "authors": [
            "Prafulla Kumar Choubey",
            "Ruihong Huang"
        ],
        "abstract": "In this paper, we describe TAMU's system submitted to the TAC KBP 2017 event nugget detection and coreference resolution task. Our system builds on the statistical and empirical observations made on training and development data. We found that modifiers of event nuggets tend to have unique syntactic distribution. Their parts-of-speech tags and dependency relations provides them essential characteristics that are useful in identifying their span and also defining their types and realis status. We further found that the joint modeling of event span detection and realis status identification performs better than the individual models for both tasks. Our simple system designed using minimal features achieved the micro-average F1 scores of 57.72, 44.27 and 42.47 for event span detection, type identification and realis status classification tasks respectively. Also, our system achieved the CoNLL F1 score of 27.20 in event coreference resolution task.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2018-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02173",
        "title": "Synthetic and Natural Noise Both Break Neural Machine Translation",
        "authors": [
            "Yonatan Belinkov",
            "Yonatan Bisk"
        ],
        "abstract": "Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2018-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02207",
        "title": "Towards Language-Universal End-to-End Speech Recognition",
        "authors": [
            "Suyoun Kim",
            "Michael L. Seltzer"
        ],
        "abstract": "Building speech recognizers in multiple languages typically involves replicating a monolingual training recipe for each language, or utilizing a multi-task learning approach where models for different languages have separate output labels but share some internal parameters. In this work, we exploit recent progress in end-to-end speech recognition to create a single multilingual speech recognition system capable of recognizing any of the languages seen in training. To do so, we propose the use of a universal character set that is shared among all languages. We also create a language-specific gating mechanism within the network that can modulate the network's internal representations in a language-specific way. We evaluate our proposed approach on the Microsoft Cortana task across three languages and show that our system outperforms both the individual monolingual systems and systems built with a multi-task learning approach. We also show that this model can be used to initialize a monolingual speech recognizer, and can be used to create a bilingual model for use in code-switching scenarios.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02212",
        "title": "Improved training for online end-to-end speech recognition systems",
        "authors": [
            "Suyoun Kim",
            "Michael L. Seltzer",
            "Jinyu Li",
            "Rui Zhao"
        ],
        "abstract": "Achieving high accuracy with end-to-end speech recognizers requires careful parameter initialization prior to training. Otherwise, the networks may fail to find a good local optimum. This is particularly true for online networks, such as unidirectional LSTMs. Currently, the best strategy to train such systems is to bootstrap the training from a tied-triphone system. However, this is time consuming, and more importantly, is impossible for languages without a high-quality pronunciation lexicon. In this work, we propose an initialization strategy that uses teacher-student learning to transfer knowledge from a large, well-trained, offline end-to-end speech recognition model to an online end-to-end model, eliminating the need for a lexicon or any other linguistic resources. We also explore curriculum learning and label smoothing and show how they can be combined with the proposed teacher-student learning for further improvements. We evaluate our methods on a Microsoft Cortana personal assistant task and show that the proposed method results in a 19 % relative improvement in word error rate compared to a randomly-initialized baseline system.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2018-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02281",
        "title": "Non-Autoregressive Neural Machine Translation",
        "authors": [
            "Jiatao Gu",
            "James Bradbury",
            "Caiming Xiong",
            "Victor O.K. Li",
            "Richard Socher"
        ],
        "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.\n    ",
        "submission_date": "2017-11-07T00:00:00",
        "last_modified_date": "2018-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02509",
        "title": "Structure Regularized Bidirectional Recurrent Convolutional Neural Network for Relation Classification",
        "authors": [
            "Ji Wen"
        ],
        "abstract": "Relation classification is an important semantic processing task in the field of natural language processing (NLP). In this paper, we present a novel model, Structure Regularized Bidirectional Recurrent Convolutional Neural Network(SR-BRCNN), to classify the relation of two entities in a sentence, and the new dataset of Chinese Sanwen for named entity recognition and relation classification. Some state-of-the-art systems concentrate on modeling the shortest dependency path (SDP) between two entities leveraging convolutional or recurrent neural networks. We further explore how to make full use of the dependency relations information in the SDP and how to improve the model by the method of structure regularization. We propose a structure regularized model to learn relation representations along the SDP extracted from the forest formed by the structure regularized dependency tree, which benefits reducing the complexity of the whole model and helps improve the $F_{1}$ score by 10.3. Experimental results show that our method outperforms the state-of-the-art approaches on the Chinese Sanwen task and performs as well on the SemEval-2010 Task 8 dataset\\footnote{The Chinese Sanwen corpus this paper developed and used will be released in the further.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02608",
        "title": "Extractive Multi-document Summarization Using Multilayer Networks",
        "authors": [
            "Jorge V. Tohalino",
            "Diego R. Amancio"
        ],
        "abstract": "Huge volumes of textual information has been produced every single day. In order to organize and understand such large datasets, in recent years, summarization techniques have become popular. These techniques aims at finding relevant, concise and non-redundant content from such a big data. While network methods have been adopted to model texts in some scenarios, a systematic evaluation of multilayer network models in the multi-document summarization task has been limited to a few studies. Here, we evaluate the performance of a multilayer-based method to select the most relevant sentences in the context of an extractive multi document summarization (MDS) task. In the adopted model, nodes represent sentences and edges are created based on the number of shared words between sentences. Differently from previous studies in multi-document summarization, we make a distinction between edges linking sentences from different documents (inter-layer) and those connecting sentences from the same document (intra-layer). As a proof of principle, our results reveal that such a discrimination between intra- and inter-layer in a multilayered representation is able to improve the quality of the generated summaries. This piece of information could be used to improve current statistical methods and related textual models.\n    ",
        "submission_date": "2017-11-07T00:00:00",
        "last_modified_date": "2017-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02781",
        "title": "RubyStar: A Non-Task-Oriented Mixture Model Dialog System",
        "authors": [
            "Huiting Liu",
            "Tao Lin",
            "Hanfei Sun",
            "Weijian Lin",
            "Chih-Wei Chang",
            "Teng Zhong",
            "Alexander Rudnicky"
        ],
        "abstract": "RubyStar is a dialog system designed to create \"human-like\" conversation by combining different response generation strategies. RubyStar conducts a non-task-oriented conversation on general topics by using an ensemble of rule-based, retrieval-based and generative methods. Topic detection, engagement monitoring, and context tracking are used for managing interaction. Predictable elements of conversation, such as the bot's backstory and simple question answering are handled by separate modules. We describe a rating scheme we developed for evaluating response generation. We find that character-level RNN is an effective generation model for general responses, with proper parameter settings; however other kinds of conversation topics might benefit from using other models.\n    ",
        "submission_date": "2017-11-08T00:00:00",
        "last_modified_date": "2017-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02918",
        "title": "Improving Hypernymy Extraction with Distributional Semantic Classes",
        "authors": [
            "Alexander Panchenko",
            "Dmitry Ustalov",
            "Stefano Faralli",
            "Simone P. Ponzetto",
            "Chris Biemann"
        ],
        "abstract": "In this paper, we show how distributionally-induced semantic classes can be helpful for extracting hypernyms. We present methods for inducing sense-aware semantic classes using distributional semantics and using these induced semantic classes for filtering noisy hypernymy relations. Denoising of hypernyms is performed by labeling each semantic class with its hypernyms. On the one hand, this allows us to filter out wrong extractions using the global structure of distributionally similar senses. On the other hand, we infer missing hypernyms via label propagation to cluster terms. We conduct a large-scale crowdsourcing study showing that processing of automatically extracted hypernyms using our approach improves the quality of the hypernymy extraction in terms of both precision and recall. Furthermore, we show the utility of our method in the domain taxonomy induction task, achieving the state-of-the-art results on a SemEval'16 task on taxonomy induction.\n    ",
        "submission_date": "2017-11-08T00:00:00",
        "last_modified_date": "2018-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03225",
        "title": "Large-scale Cloze Test Dataset Created by Teachers",
        "authors": [
            "Qizhe Xie",
            "Guokun Lai",
            "Zihang Dai",
            "Eduard Hovy"
        ],
        "abstract": "Cloze tests are widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset CLOTH, containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, CLOTH requires a deeper language understanding and a wider attention span than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a language model trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending the long-term context to be the key bottleneck.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2018-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03226",
        "title": "Weakly-supervised Relation Extraction by Pattern-enhanced Embedding Learning",
        "authors": [
            "Meng Qu",
            "Xiang Ren",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "abstract": "Extracting relations from text corpora is an important task in text mining. It becomes particularly challenging when focusing on weakly-supervised relation extraction, that is, utilizing a few relation instances (i.e., a pair of entities and their relation) as seeds to extract more instances from corpora. Existing distributional approaches leverage the corpus-level co-occurrence statistics of entities to predict their relations, and require large number of labeled instances to learn effective relation classifiers. Alternatively, pattern-based approaches perform bootstrapping or apply neural networks to model the local contexts, but still rely on large number of labeled instances to build reliable models. In this paper, we study integrating the distributional and pattern-based methods in a weakly-supervised setting, such that the two types of methods can provide complementary supervision for each other to build an effective, unified model. We propose a novel co-training framework with a distributional module and a pattern module. During training, the distributional module helps the pattern module discriminate between the informative patterns and other patterns, and the pattern module generates some highly-confident instances to improve the distributional module. The whole framework can be effectively optimized by iterating between improving the pattern module and updating the distributional module. We conduct experiments on two tasks: knowledge base completion with text corpora and corpus-level relation extraction. Experimental results prove the effectiveness of our framework in the weakly-supervised setting.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2017-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03230",
        "title": "An Empirical Analysis of Multiple-Turn Reasoning Strategies in Reading Comprehension Tasks",
        "authors": [
            "Yelong Shen",
            "Xiaodong Liu",
            "Kevin Duh",
            "Jianfeng Gao"
        ],
        "abstract": "Reading comprehension (RC) is a challenging task that requires synthesis of information across sentences and multiple turns of reasoning. Using a state-of-the-art RC model, we empirically investigate the performance of single-turn and multiple-turn reasoning on the SQuAD and MS MARCO datasets. The RC model is an end-to-end neural network with iterative attention, and uses reinforcement learning to dynamically control the number of turns. We find that multiple-turn reasoning outperforms single-turn reasoning for all question and answer types; further, we observe that enabling a flexible number of turns generally improves upon a fixed multiple-turn strategy. %across all question types, and is particularly beneficial to questions with lengthy, descriptive answers. We achieve results competitive to the state-of-the-art on these two datasets.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2017-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03381",
        "title": "Tracking of enriched dialog states for flexible conversational information access",
        "authors": [
            "Yinpei Dai",
            "Zhijian Ou",
            "Dawei Ren",
            "Pengfei Yu"
        ],
        "abstract": "Dialog state tracking (DST) is a crucial component in a task-oriented dialog system for conversational information access. A common practice in current dialog systems is to define the dialog state by a set of slot-value pairs. Such representation of dialog states and the slot-filling based DST have been widely employed, but suffer from three drawbacks. (1) The dialog state can contain only a single value for a slot, and (2) can contain only users' affirmative preference over the values for a slot. (3) Current task-based dialog systems mainly focus on the searching task, while the enquiring task is also very common in practice. The above observations motivate us to enrich current representation of dialog states and collect a brand new dialog dataset about movies, based upon which we build a new DST, called enriched DST (EDST), for flexible accessing movie information. The EDST supports the searching task, the enquiring task and their mixed task. We show that the new EDST method not only achieves good results on Iqiyi dataset, but also outperforms other state-of-the-art DST methods on the traditional dialog datasets, WOZ2.0 and DSTC2.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2018-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03483",
        "title": "Learning Multi-Modal Word Representation Grounded in Visual Context",
        "authors": [
            "\u00c9loi Zablocki",
            "Benjamin Piwowarski",
            "Laure Soulier",
            "Patrick Gallinari"
        ],
        "abstract": "Representing the semantics of words is a long-standing problem for the natural language processing community. Most methods compute word semantics given their textual context in large corpora. More recently, researchers attempted to integrate perceptual and visual features. Most of these works consider the visual appearance of objects to enhance word representations but they ignore the visual environment and context in which objects appear. We propose to unify text-based techniques with vision-based techniques by simultaneously leveraging textual and visual context to learn multimodal word embeddings. We explore various choices for what can serve as a visual context and present an end-to-end method to integrate visual context elements in a multimodal skip-gram model. We provide experiments and extensive analysis of the obtained results.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2017-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03541",
        "title": "Language Modeling for Code-Switched Data: Challenges and Approaches",
        "authors": [
            "Ganji Sreeram",
            "Rohit Sinha"
        ],
        "abstract": "Lately, the problem of code-switching has gained a lot of attention and has emerged as an active area of research. In bilingual communities, the speakers commonly embed the words and phrases of a non-native language into the syntax of a native language in their day-to-day communications. The code-switching is a global phenomenon among multilingual communities, still very limited acoustic and linguistic resources are available as yet. For developing effective speech based applications, the ability of the existing language technologies to deal with the code-switched data can not be over emphasized. The code-switching is broadly classified into two modes: inter-sentential and intra-sentential code-switching. In this work, we have studied the intra-sentential problem in the context of code-switching language modeling task. The salient contributions of this paper includes: (i) the creation of Hindi-English code-switching text corpus by crawling a few blogging sites educating about the usage of the Internet (ii) the exploration of the parts-of-speech features towards more effective modeling of Hindi-English code-switched data by the monolingual language model (LM) trained on native (Hindi) language data, and (iii) the proposal of a novel textual factor referred to as the code-switch factor (CS-factor), which allows the LM to predict the code-switching instances. In the context of recognition of the code-switching data, the substantial reduction in the PPL is achieved with the use of POS factors and also the proposed CS-factor provides independent as well as additive gain in the PPL.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2017-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03602",
        "title": "The Lifted Matrix-Space Model for Semantic Composition",
        "authors": [
            "WooJin Chung",
            "Sheng-Fu Wang",
            "Samuel R. Bowman"
        ],
        "abstract": "Tree-structured neural network architectures for sentence encoding draw inspiration from the approach to semantic composition generally seen in formal linguistics, and have shown empirical improvements over comparable sequence models by doing so. Moreover, adding multiplicative interaction terms to the composition functions in these models can yield significant further improvements. However, existing compositional approaches that adopt such a powerful composition function scale poorly, with parameter counts exploding as model dimension or vocabulary size grows. We introduce the Lifted Matrix-Space model, which uses a global transformation to map vector word embeddings to matrices, which can then be composed via an operation based on matrix-matrix multiplication. Its composition function effectively transmits a larger number of activations across layers with relatively few model parameters. We evaluate our model on the Stanford NLI corpus, the Multi-Genre NLI corpus, and the Stanford Sentiment Treebank and find that it consistently outperforms TreeLSTM (Tai et al., 2015), the previous best known composition function for tree-structured models.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2019-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03688",
        "title": "Document Context Neural Machine Translation with Memory Networks",
        "authors": [
            "Sameen Maruf",
            "Gholamreza Haffari"
        ],
        "abstract": "We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2018-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03689",
        "title": "Reinforcement Learning of Speech Recognition System Based on Policy Gradient and Hypothesis Selection",
        "authors": [
            "Taku Kato",
            "Takahiro Shinozaki"
        ],
        "abstract": "Speech recognition systems have achieved high recognition performance for several tasks. However, the performance of such systems is dependent on the tremendously costly development work of preparing vast amounts of task-matched transcribed speech data for supervised training. The key problem here is the cost of transcribing speech data. The cost is repeatedly required to support new languages and new tasks. Assuming broad network services for transcribing speech data for many users, a system would become more self-sufficient and more useful if it possessed the ability to learn from very light feedback from the users without annoying them. In this paper, we propose a general reinforcement learning framework for speech recognition systems based on the policy gradient method. As a particular instance of the framework, we also propose a hypothesis selection-based reinforcement learning method. The proposed framework provides a new view for several existing training and adaptation methods. The experimental results show that the proposed method improves the recognition performance compared to unsupervised adaptation.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03697",
        "title": "Integrating User and Agent Models: A Deep Task-Oriented Dialogue System",
        "authors": [
            "Weiyan Wang",
            "Yuxiang WU",
            "Yu Zhang",
            "Zhongqi Lu",
            "Kaixiang Mo",
            "Qiang Yang"
        ],
        "abstract": "Task-oriented dialogue systems can efficiently serve a large number of customers and relieve people from tedious works. However, existing task-oriented dialogue systems depend on handcrafted actions and states or extra semantic labels, which sometimes degrades user experience despite the intensive human intervention. Moreover, current user simulators have limited expressive ability so that deep reinforcement Seq2Seq models have to rely on selfplay and only work in some special cases. To address those problems, we propose a uSer and Agent Model IntegrAtion (SAMIA) framework inspired by an observation that the roles of the user and agent models are asymmetric. Firstly, this SAMIA framework model the user model as a Seq2Seq learning problem instead of ranking or designing rules. Then the built user model is used as a leverage to train the agent model by deep reinforcement learning. In the test phase, the output of the agent model is filtered by the user model to enhance the stability and robustness. Experiments on a real-world coffee ordering dataset verify the effectiveness of the proposed SAMIA framework.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03736",
        "title": "Joint Sentiment/Topic Modeling on Text Data Using Boosted Restricted Boltzmann Machine",
        "authors": [
            "Masoud Fatemi",
            "Mehran Safayani"
        ],
        "abstract": "Recently by the development of the Internet and the Web, different types of social media such as web blogs become an immense source of text data. Through the processing of these data, it is possible to discover practical information about different topics, individuals opinions and a thorough understanding of the society. Therefore, applying models which can automatically extract the subjective information from the documents would be efficient and helpful. Topic modeling methods, also sentiment analysis are the most raised topics in the natural language processing and text mining fields. In this paper a new structure for joint sentiment-topic modeling based on Restricted Boltzmann Machine (RBM) which is a type of neural networks is proposed. By modifying the structure of RBM as well as appending a layer which is analogous to sentiment of text data to it, we propose a generative structure for joint sentiment topic modeling based on neutral networks. The proposed method is supervised and trained by the Contrastive Divergence algorithm. The new attached layer in the proposed model is a layer with the multinomial probability distribution which can be used in text data sentiment classification or any other supervised application. The proposed model is compared with existing models in the experiments such as evaluating as a generative model, sentiment classification, information retrieval and the corresponding results demonstrate the efficiency of the method.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03754",
        "title": "Neural Skill Transfer from Supervised Language Tasks to Reading Comprehension",
        "authors": [
            "Todor Mihaylov",
            "Zornitsa Kozareva",
            "Anette Frank"
        ],
        "abstract": "Reading comprehension is a challenging task in natural language processing and requires a set of skills to be solved. While current approaches focus on solving the task as a whole, in this paper, we propose to use a neural network `skill' transfer approach. We transfer knowledge from several lower-level language tasks (skills) including textual entailment, named entity recognition, paraphrase detection and question type classification into the reading comprehension model.\n",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03759",
        "title": "YEDDA: A Lightweight Collaborative Text Span Annotation Tool",
        "authors": [
            "Jie Yang",
            "Yue Zhang",
            "Linwei Li",
            "Xingxuan Li"
        ],
        "abstract": "In this paper, we introduce \\textsc{Yedda}, a lightweight but efficient and comprehensive open-source tool for text span annotation. \\textsc{Yedda} provides a systematic solution for text span annotation, ranging from collaborative user annotation to administrator evaluation and analysis. It overcomes the low efficiency of traditional text annotation tools by annotating entities through both command line and shortcut keys, which are configurable with custom labels. \\textsc{Yedda} also gives intelligent recommendations by learning the up-to-date annotated text. An administrator client is developed to evaluate annotation quality of multiple annotators and generate detailed comparison report for each annotator pair. Experiments show that the proposed system can reduce the annotation time by half compared with existing annotation tools. And the annotation time can be further compressed by 16.47\\% through intelligent recommendation.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2018-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03859",
        "title": "Towards the Use of Deep Reinforcement Learning with Global Policy For Query-based Extractive Summarisation",
        "authors": [
            "Diego Molla"
        ],
        "abstract": "Supervised approaches for text summarisation suffer from the problem of mismatch between the target labels/scores of individual sentences and the evaluation score of the final summary. Reinforcement learning can solve this problem by providing a learning mechanism that uses the score of the final summary as a guide to determine the decisions made at the time of selection of each sentence. In this paper we present a proof-of-concept approach that applies a policy-gradient algorithm to learn a stochastic policy using an undiscounted reward. The method has been applied to a policy consisting of a simple neural network and simple features. The resulting deep reinforcement learning system is able to learn a global policy and obtain encouraging results.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03946",
        "title": "Bayesian Paragraph Vectors",
        "authors": [
            "Geng Ji",
            "Robert Bamler",
            "Erik B. Sudderth",
            "Stephan Mandt"
        ],
        "abstract": "Word2vec (Mikolov et al., 2013) has proven to be successful in natural language processing by capturing the semantic relationships between different words. Built on top of single-word embeddings, paragraph vectors (Le and Mikolov, 2014) find fixed-length representations for pieces of text with arbitrary lengths, such as documents, paragraphs, and sentences. In this work, we propose a novel interpretation for neural-network-based paragraph vectors by developing an unsupervised generative model whose maximum likelihood solution corresponds to traditional paragraph vectors. This probabilistic formulation allows us to go beyond point estimates of parameters and to perform Bayesian posterior inference. We find that the entropy of paragraph vectors decreases with the length of documents, and that information about posterior uncertainty improves performance in supervised learning tasks such as sentiment analysis and paraphrase detection.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03953",
        "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
        "authors": [
            "Zhilin Yang",
            "Zihang Dai",
            "Ruslan Salakhutdinov",
            "William W. Cohen"
        ],
        "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2018-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04044",
        "title": "Kernelized Hashcode Representations for Relation Extraction",
        "authors": [
            "Sahil Garg",
            "Aram Galstyan",
            "Greg Ver Steeg",
            "Irina Rish",
            "Guillermo Cecchi",
            "Shuyang Gao"
        ],
        "abstract": "Kernel methods have produced state-of-the-art results for a number of NLP tasks such as relation extraction, but suffer from poor scalability due to the high cost of computing kernel similarities between natural language structures. A recently proposed technique, kernelized locality-sensitive hashing (KLSH), can significantly reduce the computational cost, but is only applicable to classifiers operating on kNN graphs. Here we propose to use random subspaces of KLSH codes for efficiently constructing an explicit representation of NLP structures suitable for general classification methods. Further, we propose an approach for optimizing the KLSH model for classification problems by maximizing an approximation of mutual information between the KLSH codes (feature vectors) and the class labels. We evaluate the proposed approach on biomedical relation extraction datasets, and observe significant and robust improvements in accuracy w.r.t. state-of-the-art classifiers, along with drastic (orders-of-magnitude) speedup compared to conventional kernel methods.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2019-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04071",
        "title": "KBGAN: Adversarial Learning for Knowledge Graph Embeddings",
        "authors": [
            "Liwei Cai",
            "William Yang Wang"
        ],
        "abstract": "We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models. Because knowledge graphs typically only contain positive facts, sampling useful negative training examples is a non-trivial task. Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training. Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs. This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks. In experiments, we adversarially train two translation-based models, TransE and TransD, each with assistance from one of the two probability-based models, DistMult and ComplEx. We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2018-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04075",
        "title": "Towards Automated ICD Coding Using Deep Learning",
        "authors": [
            "Haoran Shi",
            "Pengtao Xie",
            "Zhiting Hu",
            "Ming Zhang",
            "Eric P. Xing"
        ],
        "abstract": "International Classification of Diseases(ICD) is an authoritative health care classification system of different diseases and conditions for clinical and management purposes. Considering the complicated and dedicated process to assign correct codes to each patient admission based on overall diagnosis, we propose a hierarchical deep learning model with attention mechanism which can automatically assign ICD diagnostic codes given written diagnosis. We utilize character-aware neural language models to generate hidden representations of written diagnosis descriptions and ICD codes, and design an attention mechanism to address the mismatch between the numbers of descriptions and corresponding codes. Our experimental results show the strong potential of automated ICD coding from diagnosis descriptions. Our best model achieves 0.53 and 0.90 of F1 score and area under curve of receiver operating characteristic respectively. The result outperforms those achieved using character-unaware encoding method or without attention mechanism. It indicates that our proposed deep learning model can code automatically in a reasonable way and provide a framework for computer-auxiliary ICD coding.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04079",
        "title": "Fine Grained Knowledge Transfer for Personalized Task-oriented Dialogue Systems",
        "authors": [
            "Kaixiang Mo",
            "Yu Zhang",
            "Qiang Yang",
            "Pascale Fung"
        ],
        "abstract": "Training a personalized dialogue system requires a lot of data, and the data collected for a single user is usually insufficient. One common practice for this problem is to share training dialogues between different users and train multiple sequence-to-sequence dialogue models together with transfer learning. However, current sequence-to-sequence transfer learning models operate on the entire sentence, which might cause negative transfer if different personal information from different users is mixed up. We propose a personalized decoder model to transfer finer granularity phrase-level knowledge between different users while keeping personal preferences of each user intact. A novel personal control gate is introduced, enabling the personalized decoder to switch between generating personalized phrases and shared phrases. The proposed personalized decoder model can be easily combined with various deep models and can be trained with reinforcement learning. Real-world experimental results demonstrate that the phrase-level personalized decoder improves the BLEU over multiple sentence-level transfer baseline models by as much as 7.5%.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2017-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04090",
        "title": "MojiTalk: Generating Emotional Responses at Scale",
        "authors": [
            "Xianda Zhou",
            "William Yang Wang"
        ],
        "abstract": "Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis. More specifically, we collect a large corpus of Twitter conversations that include emojis in the response, and assume the emojis convey the underlying emotions of the sentence. We then introduce a reinforced conditional variational encoder approach to train a deep generative model on these conversations, which allows us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2018-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04115",
        "title": "Discovering conversational topics and emotions associated with Demonetization tweets in India",
        "authors": [
            "Mitodru Niyogi",
            "Asim K. Pal"
        ],
        "abstract": "Social media platforms contain great wealth of information which provides us opportunities explore hidden patterns or unknown correlations, and understand people's satisfaction with what they are discussing. As one showcase, in this paper, we summarize the data set of Twitter messages related to recent demonetization of all Rs. 500 and Rs. 1000 notes in India and explore insights from Twitter's data. Our proposed system automatically extracts the popular latent topics in conversations regarding demonetization discussed in Twitter via the Latent Dirichlet Allocation (LDA) based topic model and also identifies the correlated topics across different categories. Additionally, it also discovers people's opinions expressed through their tweets related to the event under consideration via the emotion analyzer. The system also employs an intuitive and informative visualization to show the uncovered insight. Furthermore, we use an evaluation measure, Normalized Mutual Information (NMI), to select the best LDA models. The obtained LDA results show that the tool can be effectively used to extract discussion topics and summarize them for further manual analysis.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2017-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04154",
        "title": "Interpretable probabilistic embeddings: bridging the gap between topic models and neural networks",
        "authors": [
            "Anna Potapenko",
            "Artem Popov",
            "Konstantin Vorontsov"
        ],
        "abstract": "We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online EM-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling (SGNS) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models (ARTM) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2017-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04168",
        "title": "Unsupervised Document Embedding With CNNs",
        "authors": [
            "Chundi Liu",
            "Shunan Zhao",
            "Maksims Volkovs"
        ],
        "abstract": "We propose a new model for unsupervised document embedding. Leading existing approaches either require complex inference or use recurrent neural networks (RNN) that are difficult to parallelize. We take a different route and develop a convolutional neural network (CNN) embedding model. Our CNN architecture is fully parallelizable resulting in over 10x speedup in inference time over RNN models. Parallelizable architecture enables to train deeper models where each successive layer has increasingly larger receptive field and models longer range semantic structure within the document. We additionally propose a fully unsupervised learning algorithm to train this model based on stochastic forward prediction. Empirical results on two public benchmarks show that our approach produces comparable to state-of-the-art accuracy at a fraction of computational cost.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2018-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04204",
        "title": "Automatic Extraction of Commonsense LocatedNear Knowledge",
        "authors": [
            "Frank F. Xu",
            "Bill Yuchen Lin",
            "Kenny Q. Zhu"
        ],
        "abstract": "LocatedNear relation is a kind of commonsense knowledge describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus. Also, we release two benchmark datasets for evaluation and future research.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2018-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04231",
        "title": "Syntax-Directed Attention for Neural Machine Translation",
        "authors": [
            "Kehai Chen",
            "Rui Wang",
            "Masao Utiyama",
            "Eiichiro Sumita",
            "Tiejun Zhao"
        ],
        "abstract": "Attention mechanism, including global attention and local attention, plays a key role in neural machine translation (NMT). Global attention attends to all source words for word prediction. In comparison, local attention selectively looks at fixed-window source words. However, alignment weights for the current target word often decrease to the left and right by linear distance centering on the aligned source position and neglect syntax-directed distance constraints. In this paper, we extend local attention with syntax-distance constraint, to focus on syntactically related source words with the predicted target word, thus learning a more effective context vector for word prediction. Moreover, we further propose a double context NMT architecture, which consists of a global context vector and a syntax-directed context vector over the global attention, to provide more translation performance for NMT from source representation. The experiments on the large-scale Chinese-to-English and English-to-Germen translation tasks show that the proposed approach achieves a substantial and significant improvement over the baseline system.\n    ",
        "submission_date": "2017-11-12T00:00:00",
        "last_modified_date": "2019-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04289",
        "title": "Neural Natural Language Inference Models Enhanced with External Knowledge",
        "authors": [
            "Qian Chen",
            "Xiaodan Zhu",
            "Zhen-Hua Ling",
            "Diana Inkpen",
            "Si Wei"
        ],
        "abstract": "Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.\n    ",
        "submission_date": "2017-11-12T00:00:00",
        "last_modified_date": "2018-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04352",
        "title": "Fast Reading Comprehension with ConvNets",
        "authors": [
            "Felix Wu",
            "Ni Lao",
            "John Blitzer",
            "Guandao Yang",
            "Kilian Weinberger"
        ],
        "abstract": "State-of-the-art deep reading comprehension models are dominated by recurrent neural nets. Their sequential nature is a natural fit for language, but it also precludes parallelization within an instances and often becomes the bottleneck for deploying such models to latency critical scenarios. This is particularly problematic for longer texts. Here we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.\n    ",
        "submission_date": "2017-11-12T00:00:00",
        "last_modified_date": "2017-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04411",
        "title": "Convolutional Neural Network with Word Embeddings for Chinese Word Segmentation",
        "authors": [
            "Chunqi Wang",
            "Bo Xu"
        ],
        "abstract": "Character-based sequence labeling framework is flexible and efficient for Chinese word segmentation (CWS). Recently, many character-based neural models have been applied to CWS. While they obtain good performance, they have two obvious weaknesses. The first is that they heavily rely on manually designed bigram feature, i.e. they are not good at capturing n-gram features automatically. The second is that they make no use of full word information. For the first weakness, we propose a convolutional neural model, which is able to capture rich n-gram features without any feature engineering. For the second one, we propose an effective approach to integrate the proposed model with word embeddings. We evaluate the model on two benchmark datasets: PKU and MSR. Without any feature engineering, the model obtains competitive performance -- 95.7% on PKU and 97.3% on MSR. Armed with word embeddings, the model achieves state-of-the-art performance on both datasets -- 96.5% on PKU and 98.0% on MSR, without using any external labeled resource.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04436",
        "title": "SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning",
        "authors": [
            "Xiaojun Xu",
            "Chang Liu",
            "Dawn Song"
        ],
        "abstract": "Synthesizing SQL queries from natural language is a long-standing open problem and has been attracting considerable interest recently. Toward solving the problem, the de facto approach is to employ a sequence-to-sequence-style model. Such an approach will necessarily require the SQL queries to be serialized. Since the same SQL query may have multiple equivalent serializations, training a sequence-to-sequence-style model is sensitive to the choice from one of them. This phenomenon is documented as the \"order-matters\" problem. Existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations. However, we observe that the improvement from reinforcement learning is limited.\n",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04452",
        "title": "Digitising Cultural Complexity: Representing Rich Cultural Data in a Big Data environment",
        "authors": [
            "Jennifer Edmond",
            "Georgina Nugent Folan"
        ],
        "abstract": "One of the major terminological forces driving ICT integration in research today is that of \"big data.\" While the phrase sounds inclusive and integrative, \"big data\" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that human activity produces, but the technological imperative to enhance signal through the reduction of noise does not accommodate this richness. Data and the computational approaches that facilitate \"big data\" have acquired a perceived objectivity that belies their curated, malleable, reactive, and performative nature. In an input environment where anything can \"be data\" once it is entered into the system as \"data,\" data cleaning and processing, together with the metadata and information architectures that structure and facilitate our cultural archives acquire a capacity to delimit what data are. This engenders a process of simplification that has major implications for the potential for future innovation within research environments that depend on rich material yet are increasingly mediated by digital technologies. This paper presents the preliminary findings of the European-funded KPLEX (Knowledge Complexity) project which investigates the delimiting effect digital mediation and datafication has on rich, complex cultural data. The paper presents a systematic review of existing implicit definitions of data, elaborating on the implications of these definitions and highlighting the ways in which metadata and computational technologies can restrict the interpretative potential of data. It sheds light on the gap between analogue or augmented digital practices and fully computational ones, and the strategies researchers have developed to deal with this gap. The paper proposes a reconceptualisation of data as it is functionally employed within digitally-mediated research so as to incorporate and acknowledge the richness and complexity of our source materials.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04457",
        "title": "Word, Subword or Character? An Empirical Study of Granularity in Chinese-English NMT",
        "authors": [
            "Yining Wang",
            "Long Zhou",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "Neural machine translation (NMT), a new approach to machine translation, has been proved to outperform conventional statistical machine translation (SMT) across a variety of language pairs. Translation is an open-vocabulary problem, but most existing NMT systems operate with a fixed vocabulary, which causes the incapability of translating rare words. This problem can be alleviated by using different translation granularities, such as character, subword and hybrid word-character. Translation involving Chinese is one of the most difficult tasks in machine translation, however, to the best of our knowledge, there has not been any other work exploring which translation granularity is most suitable for Chinese in NMT. In this paper, we conduct an extensive comparison using Chinese-English NMT as a case study. Furthermore, we discuss the advantages and disadvantages of various translation granularities in detail. Our experiments show that subword model performs best for Chinese-to-English translation with the vocabulary which is not so big while hybrid word-character model is most suitable for English-to-Chinese translation. Moreover, experiments of different granularities show that Hybrid_BPE method can achieve best result on Chinese-to-English translation task.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04731",
        "title": "Evaluating prose style transfer with the Bible",
        "authors": [
            "Keith Carlson",
            "Allen Riddell",
            "Daniel Rockmore"
        ],
        "abstract": "In the prose style transfer task a system, provided with text input and a target prose style, produces output which preserves the meaning of the input text but alters the style. These systems require parallel data for evaluation of results and usually make use of parallel data for training. Currently, there are few publicly available corpora for this task. In this work, we identify a high-quality source of aligned, stylistically distinct text in different versions of the Bible. We provide a standardized split, into training, development and testing data, of the public domain versions in our corpus. This corpus is highly parallel since many Bible versions are included. Sentences are aligned due to the presence of chapter and verse numbers within all versions of the text. In addition to the corpus, we present the results, as measured by the BLEU and PINC metrics, of several models trained on our data which can serve as baselines for future research. While we present these data as a style transfer corpus, we believe that it is of unmatched quality and may be useful for other natural language tasks as well.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2018-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04805",
        "title": "QuickEdit: Editing Text & Translations by Crossing Words Out",
        "authors": [
            "David Grangier",
            "Michael Auli"
        ],
        "abstract": "We propose a framework for computer-assisted text editing. It applies to translation post-editing and to paraphrasing. Our proposal relies on very simple interactions: a human editor modifies a sentence by marking tokens they would like the system to change. Our model then generates a new sentence which reformulates the initial sentence by avoiding marked words. The approach builds upon neural sequence-to-sequence modeling and introduces a neural network which takes as input a sentence along with change markers. Our model is trained on translation bitext by simulating post-edits. We demonstrate the advantage of our approach for translation post-editing through simulated post-edits. We also evaluate our model for paraphrasing through a user study.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2018-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04903",
        "title": "Robust Multilingual Part-of-Speech Tagging via Adversarial Training",
        "authors": [
            "Michihiro Yasunaga",
            "Jungo Kasai",
            "Dragomir Radev"
        ],
        "abstract": "Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04951",
        "title": "From Word Segmentation to POS Tagging for Vietnamese",
        "authors": [
            "Dat Quoc Nguyen",
            "Thanh Vu",
            "Dai Quoc Nguyen",
            "Mark Dras",
            "Mark Johnson"
        ],
        "abstract": "This paper presents an empirical comparison of two strategies for Vietnamese Part-of-Speech (POS) tagging from unsegmented text: (i) a pipeline strategy where we consider the output of a word segmenter as the input of a POS tagger, and (ii) a joint strategy where we predict a combined segmentation and POS tag for each syllable. We also make a comparison between state-of-the-art (SOTA) feature-based and neural network-based models. On the benchmark Vietnamese treebank (Nguyen et al., 2009), experimental results show that the pipeline strategy produces better scores of POS tagging from unsegmented text than the joint strategy, and the highest accuracy is obtained by using a feature-based model.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2017-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04956",
        "title": "Classical Structured Prediction Losses for Sequence to Sequence Learning",
        "authors": [
            "Sergey Edunov",
            "Myle Ott",
            "Michael Auli",
            "David Grangier",
            "Marc'Aurelio Ranzato"
        ],
        "abstract": "There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT'14 German-English translation as well as Gigaword abstractive summarization. On the larger WMT'14 English-French translation task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04964",
        "title": "Dynamic Fusion Networks for Machine Reading Comprehension",
        "authors": [
            "Yichong Xu",
            "Jingjing Liu",
            "Jianfeng Gao",
            "Yelong Shen",
            "Xiaodong Liu"
        ],
        "abstract": "This paper presents a novel neural model - Dynamic Fusion Network (DFN), for machine reading comprehension (MRC). DFNs differ from most state-of-the-art models in their use of a dynamic multi-strategy attention process, in which passages, questions and answer candidates are jointly fused into attention vectors, along with a dynamic multi-step reasoning module for generating answers. With the use of reinforcement learning, for each input sample that consists of a question, a passage and a list of candidate answers, an instance of DFN with a sample-specific network architecture can be dynamically constructed by determining what attention strategy to apply and how many reasoning steps to take. Experiments show that DFNs achieve the best result reported on RACE, a challenging MRC dataset that contains real human reading questions in a wide variety of types. A detailed empirical analysis also demonstrates that DFNs can produce attention vectors that summarize information from questions, passages and answer candidates more effectively than other popular MRC models.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04987",
        "title": "Unified Pragmatic Models for Generating and Following Instructions",
        "authors": [
            "Daniel Fried",
            "Jacob Andreas",
            "Dan Klein"
        ],
        "abstract": "We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these models to tasks with sequential structure. Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05066",
        "title": "Learning an Executable Neural Semantic Parser",
        "authors": [
            "Jianpeng Cheng",
            "Siva Reddy",
            "Vijay Saraswat",
            "Mirella Lapata"
        ],
        "abstract": "This paper describes a neural semantic parser that maps natural language utterances onto logical forms which can be executed against a task-specific environment, such as a knowledge base or a database, to produce a response. The parser generates tree-structured logical forms with a transition-based approach which combines a generic tree-generation algorithm with domain-general operations defined by the logical language. The generation process is modeled by structured recurrent neural networks, which provide a rich encoding of the sentential context and generation history for making predictions. To tackle mismatches between natural language and logical form tokens, various attention mechanisms are explored. Finally, we consider different training settings for the neural semantic parser, including a fully supervised training where annotated logical forms are given, weakly-supervised training where denotations are provided, and distant supervision where only unlabeled sentences and a knowledge base are available. Experiments across a wide range of datasets demonstrate the effectiveness of our parser.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05073",
        "title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications",
        "authors": [
            "Wei He",
            "Kai Liu",
            "Jing Liu",
            "Yajuan Lyu",
            "Shiqi Zhao",
            "Xinyan Xiao",
            "Yuan Liu",
            "Yizhong Wang",
            "Hua Wu",
            "Qiaoqiao She",
            "Xuan Liu",
            "Tian Wu",
            "Haifeng Wang"
        ],
        "abstract": "This paper introduces DuReader, a new large-scale, open-domain Chinese ma- chine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated. (2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community. (3) scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far. Experiments show that human performance is well above current state-of-the-art baseline systems, leaving plenty of room for the community to make improvements. To help the community make these improvements, both DuReader and baseline systems have been posted online. We also organize a shared competition to encourage the exploration of more models. Since the release of the task, there are significant improvements over the baselines.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05116",
        "title": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering",
        "authors": [
            "Shuohang Wang",
            "Mo Yu",
            "Jing Jiang",
            "Wei Zhang",
            "Xiaoxiao Guo",
            "Shiyu Chang",
            "Zhiguo Wang",
            "Tim Klinger",
            "Gerald Tesauro",
            "Murray Campbell"
        ],
        "abstract": "A popular recent approach to answering open-domain questions is to first search for question-related passages and then apply reading comprehension models to extract answers. Existing methods usually extract answers from single passages independently. But some questions require a combination of evidence from across different sources to answer correctly. In this paper, we propose two models which make use of multiple passages to generate their answers. Both use an answer-reranking approach which reorders the answer candidates generated by an existing state-of-the-art QA model. We propose two methods, namely, strength-based re-ranking and coverage-based re-ranking, to make use of the aggregated evidence from different passages to better determine the answer. Our models have achieved state-of-the-art results on three public open-domain QA datasets: Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8 percentage points of improvement over the former two datasets.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05170",
        "title": "On Extending Neural Networks with Loss Ensembles for Text Classification",
        "authors": [
            "Hamideh Hajiabadi",
            "Diego Molla-Aliod",
            "Reza Monsefi"
        ],
        "abstract": "Ensemble techniques are powerful approaches that combine several weak learners to build a stronger one. As a meta learning framework, ensemble techniques can easily be applied to many machine learning techniques. In this paper we propose a neural network extended with an ensemble loss function for text classification. The weight of each weak loss function is tuned within the training phase through the gradient propagation optimization method of the neural network. The approach is evaluated on several text classification datasets. We also evaluate its performance in various environments with several degrees of label noise. Experimental results indicate an improvement of the results and strong resilience against label noise in comparison with other methods.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2017-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05186",
        "title": "False Positive and Cross-relation Signals in Distant Supervision Data",
        "authors": [
            "Anca Dumitrache",
            "Lora Aroyo",
            "Chris Welty"
        ],
        "abstract": "Distant supervision (DS) is a well-established method for relation extraction from text, based on the assumption that when a knowledge-base contains a relation between a term pair, then sentences that contain that pair are likely to express the relation. In this paper, we use the results of a crowdsourcing relation extraction task to identify two problems with DS data quality: the widely varying degree of false positives across different relations, and the observed causal connection between relations that are not considered by the DS method. The crowdsourcing data aggregation is performed using ambiguity-aware CrowdTruth metrics, that are used to capture and interpret inter-annotator disagreement. We also present preliminary results of using the crowd to enhance DS training data for a relation classification model, without requiring the crowd to annotate the entire set.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05198",
        "title": "Unsupervised patient representations from clinical notes with interpretable classification decisions",
        "authors": [
            "Madhumita Sushil",
            "Simon \u0160uster",
            "Kim Luyckx",
            "Walter Daelemans"
        ],
        "abstract": "We have two main contributions in this work: 1. We explore the usage of a stacked denoising autoencoder, and a paragraph vector model to learn task-independent dense patient representations directly from clinical notes. We evaluate these representations by using them as features in multiple supervised setups, and compare their performance with those of sparse representations. 2. To understand and interpret the representations, we explore the best encoded features within the patient representations obtained from the autoencoder model. Further, we calculate the significance of the input features of the trained classifiers when we use these pretrained representations as input.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2017-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05217",
        "title": "Controllable Abstractive Summarization",
        "authors": [
            "Angela Fan",
            "David Grangier",
            "Michael Auli"
        ],
        "abstract": "Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically. On the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 and human evaluation).\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05240",
        "title": "Weakly-supervised Semantic Parsing with Abstract Examples",
        "authors": [
            "Omer Goldman",
            "Veronica Latcinnik",
            "Udi Naveh",
            "Amir Globerson",
            "Jonathan Berant"
        ],
        "abstract": "Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2019-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05294",
        "title": "Modeling Semantic Relatedness using Global Relation Vectors",
        "authors": [
            "Shoaib Jameel",
            "Zied Bouraoui",
            "Steven Schockaert"
        ],
        "abstract": "Word embedding models such as GloVe rely on co-occurrence statistics from a large corpus to learn vector representations of word meaning. These vectors have proven to capture surprisingly fine-grained semantic and syntactic information. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships have mostly relied on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2017-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05313",
        "title": "Simulating Action Dynamics with Neural Process Networks",
        "authors": [
            "Antoine Bosselut",
            "Omer Levy",
            "Ari Holtzman",
            "Corin Ennis",
            "Dieter Fox",
            "Yejin Choi"
        ],
        "abstract": "Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics. Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05345",
        "title": "Supervised and Unsupervised Transfer Learning for Question Answering",
        "authors": [
            "Yu-An Chung",
            "Hung-Yi Lee",
            "James Glass"
        ],
        "abstract": "Although transfer learning has been shown to be successful for tasks like object and speech recognition, its applicability to question answering (QA) has yet to be well-studied. In this paper, we conduct extensive experiments to investigate the transferability of knowledge learned from a source QA dataset to a target dataset using two QA models. The performance of both models on a TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) is significantly improved via a simple transfer learning technique from MovieQA (Tapaswi et al., 2016). In particular, one of the models achieves the state-of-the-art on all target datasets; for the TOEFL listening comprehension test, it outperforms the previous best model by 7%. Finally, we show that transfer learning is helpful even in unsupervised scenarios when correct answers for target QA dataset examples are not available.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2018-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05350",
        "title": "A Deep Learning Approach for Expert Identification in Question Answering Communities",
        "authors": [
            "Chen Zheng",
            "Shuangfei Zhai",
            "Zhongfei Zhang"
        ],
        "abstract": "In this paper, we describe an effective convolutional neural network framework for identifying the expert in question answering community. This approach uses the convolutional neural network and combines user feature representations with question feature representations to compute scores that the user who gets the highest score is the expert on this question. Unlike prior work, this method does not measure expert based on measure answer content quality to identify the expert but only require question sentence and user embedding feature to identify the expert. Remarkably, Our model can be applied to different languages and different domains. The proposed framework is trained on two datasets, The first dataset is Stack Overflow and the second one is Zhihu. The Top-1 accuracy results of our experiments show that our framework outperforms the best baseline framework for expert identification.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2017-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05380",
        "title": "Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings",
        "authors": [
            "Shaohui Kuang",
            "Junhui Li",
            "Ant\u00f3nio Branco",
            "Weihua Luo",
            "Deyi Xiong"
        ],
        "abstract": "In neural machine translation, a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase. Differently from statistical machine translation, the associations between source words and their possible target counterparts are not explicitly stored. Source and target words are at the two ends of a long information processing procedure, mediated by hidden states at both the source encoding and the target decoding phases. This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language.\n",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2018-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05433",
        "title": "A Sequential Neural Encoder with Latent Structured Description for Modeling Sentences",
        "authors": [
            "Yu-Ping Ruan",
            "Qian Chen",
            "Zhen-Hua Ling"
        ],
        "abstract": "In this paper, we propose a sequential neural encoder with latent structured description (SNELSD) for modeling sentences. This model introduces latent chunk-level representations into conventional sequential neural encoders, i.e., recurrent neural networks (RNNs) with long short-term memory (LSTM) units, to consider the compositionality of languages in semantic modeling. An SNELSD model has a hierarchical structure that includes a detection layer and a description layer. The detection layer predicts the boundaries of latent word chunks in an input sentence and derives a chunk-level vector for each word. The description layer utilizes modified LSTM units to process these chunk-level vectors in a recurrent manner and produces sequential encoding outputs. These output vectors are further concatenated with word vectors or the outputs of a chain LSTM encoder to obtain the final sentence representation. All the model parameters are learned in an end-to-end manner without a dependency on additional text chunking or syntax parsing. A natural language inference (NLI) task and a sentiment analysis (SA) task are adopted to evaluate the performance of our proposed model. The experimental results demonstrate the effectiveness of the proposed SNELSD model on exploring task-dependent chunking patterns during the semantic modeling of sentences. Furthermore, the proposed method achieves better performance than conventional chain LSTMs and tree-structured LSTMs on both tasks.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05467",
        "title": "Aicyber's System for NLPCC 2017 Shared Task 2: Voting of Baselines",
        "authors": [
            "Du Steven",
            "Xi Zhang"
        ],
        "abstract": "This paper presents Aicyber's system for NLPCC 2017 shared task 2. It is formed by a voting of three deep learning based system trained on character-enhanced word vectors and a well known bag-of-word model.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05468",
        "title": "Tracking Typological Traits of Uralic Languages in Distributed Language Representations",
        "authors": [
            "Johannes Bjerva",
            "Isabelle Augenstein"
        ],
        "abstract": "Although linguistic typology has a long history, computational approaches have only recently gained popularity. The use of distributed representations in computational linguistics has also become increasingly popular. A recent development is to learn distributed representations of language, such that typologically similar languages are spatially close to one another. Although empirical successes have been shown for such language representations, they have not been subjected to much typological probing. In this paper, we first look at whether this type of language representations are empirically useful for model transfer between Uralic languages in deep neural networks. We then investigate which typological features are encoded in these representations by attempting to predict features in the World Atlas of Language Structures, at various stages of fine-tuning of the representations. We focus on Uralic languages, and find that some typological traits can be automatically inferred with accuracies well above a strong baseline.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05516",
        "title": "Investigating Inner Properties of Multimodal Representation and Semantic Compositionality with Brain-based Componential Semantics",
        "authors": [
            "Shaonan Wang",
            "Jiajun Zhang",
            "Nan Lin",
            "Chengqing Zong"
        ],
        "abstract": "Multimodal models have been proven to outperform text-based approaches on learning semantic representations. However, it still remains unclear what properties are encoded in multimodal representations, in what aspects do they outperform the single-modality representations, and what happened in the process of semantic compositionality in different input modalities. Considering that multimodal models are originally motivated by human concept representations, we assume that correlating multimodal representations with brain-based semantics would interpret their inner properties to answer the above questions. To that end, we propose simple interpretation methods based on brain-based componential semantics. First we investigate the inner properties of multimodal representations by correlating them with corresponding brain-based property vectors. Then we map the distributed vector space to the interpretable brain-based componential space to explore the inner properties of semantic compositionality. Ultimately, the present paper sheds light on the fundamental questions of natural language understanding, such as how to represent the meaning of words and how to combine word meanings into larger units.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05538",
        "title": "Detecting and assessing contextual change in diachronic text documents using context volatility",
        "authors": [
            "Christian Kahmann",
            "Andreas Niekler",
            "Gerhard Heyer"
        ],
        "abstract": "Terms in diachronic text corpora may exhibit a high degree of semantic dynamics that is only partially captured by the common notion of semantic change. The new measure of context volatility that we propose models the degree by which terms change context in a text collection over time. The computation of context volatility for a word relies on the significance-values of its co-occurrent terms and the corresponding co-occurrence ranks in sequential time spans. We define a baseline and present an efficient computational approach in order to overcome problems related to computational issues in the data structure. Results are evaluated both, on synthetic documents that are used to simulate contextual changes, and a real example based on British newspaper texts.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05568",
        "title": "Dialogue Act Recognition via CRF-Attentive Structured Network",
        "authors": [
            "Zheqian Chen",
            "Rongqin Yang",
            "Zhou Zhao",
            "Deng Cai",
            "Xiaofei He"
        ],
        "abstract": "Dialogue Act Recognition (DAR) is a challenging problem in dialogue interpretation, which aims to attach semantic labels to utterances and characterize the speaker's intention. Currently, many existing approaches formulate the DAR problem ranging from multi-classification to structured prediction, which suffer from handcrafted feature extensions and attentive contextual structural dependencies. In this paper, we consider the problem of DAR from the viewpoint of extending richer Conditional Random Field (CRF) structural dependencies without abandoning end-to-end training. We incorporate hierarchical semantic inference with memory mechanism on the utterance modeling. We then extend structured attention network to the linear-chain conditional random field layer which takes into account both contextual utterances and corresponding dialogue acts. The extensive experiments on two major benchmark datasets Switchboard Dialogue Act (SWDA) and Meeting Recorder Dialogue Act (MRDA) datasets show that our method achieves better performance than other state-of-the-art solutions to the problem. It is a remarkable fact that our method is nearly close to the human annotator's performance on SWDA within 2% gap.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05603",
        "title": "Words are Malleable: Computing Semantic Shifts in Political and Media Discourse",
        "authors": [
            "Hosein Azarbonyad",
            "Mostafa Dehghani",
            "Kaspar Beelen",
            "Alexandra Arkut",
            "Maarten Marx",
            "Jaap Kamps"
        ],
        "abstract": "Recently, researchers started to pay attention to the detection of temporal shifts in the meaning of words. However, most (if not all) of these approaches restricted their efforts to uncovering change over time, thus neglecting other valuable dimensions such as social or political variability. We propose an approach for detecting semantic shifts between different viewpoints--broadly defined as a set of texts that share a specific metadata feature, which can be a time-period, but also a social entity such as a political party. For each viewpoint, we learn a semantic space in which each word is represented as a low dimensional neural embedded vector. The challenge is to compare the meaning of a word in one space to its meaning in another space and measure the size of the semantic shifts. We compare the effectiveness of a measure based on optimal transformations between the two spaces with a measure based on the similarity of the neighbors of the word in the respective spaces. Our experiments demonstrate that the combination of these two performs best. We show that the semantic shifts not only occur over time, but also along different viewpoints in a short period of time. For evaluation, we demonstrate how this approach captures meaningful semantic shifts and can help improve other tasks such as the contrastive viewpoint summarization and ideology detection (measured as classification accuracy) in political texts. We also show that the two laws of semantic change which were empirically shown to hold for temporal shifts also hold for shifts across viewpoints. These laws state that frequent words are less likely to shift meaning while words with many senses are more likely to do so.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05626",
        "title": "Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time",
        "authors": [
            "Pankaj Gupta",
            "Subburam Rajaram",
            "Hinrich Sch\u00fctze",
            "Bernt Andrassy"
        ],
        "abstract": "Dynamic topic modeling facilitates the identification of topical trends over time in temporal collections of unstructured documents. We introduce a novel unsupervised neural dynamic topic model named as Recurrent Neural Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each time influence the topic discovery in the subsequent time steps. We account for the temporal ordering of documents by explicitly modeling a joint distribution of latent topical dependencies over time, using distributional estimators with temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP research, we demonstrate that compared to state-of-the art topic models, RNNRSM shows better generalization, topic interpretation, evolution and trends. We also introduce a metric (named as SPAN) to quantify the capability of dynamic topic model to capture word evolution in topics over time.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2018-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05678",
        "title": "Unsupervised Morphological Expansion of Small Datasets for Improving Word Embeddings",
        "authors": [
            "Syed Sarfaraz Akhtar",
            "Arihant Gupta",
            "Avijit Vajpayee",
            "Arjit Srivastava",
            "Manish Shrivastava"
        ],
        "abstract": "We present a language independent, unsupervised method for building word embeddings using morphological expansion of text. Our model handles the problem of data sparsity and yields improved word embeddings by relying on training word embeddings on artificially generated sentences. We evaluate our method using small sized training sets on eleven test sets for the word similarity task across seven languages. Further, for English, we evaluated the impacts of our approach using a large training set on three standard test sets. Our method improved results across all languages.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05680",
        "title": "An Unsupervised Approach for Mapping between Vector Spaces",
        "authors": [
            "Syed Sarfaraz Akhtar",
            "Arihant Gupta",
            "Avijit Vajpayee",
            "Arjit Srivastava",
            "Madan Gopal Jhawar",
            "Manish Shrivastava"
        ],
        "abstract": "We present a language independent, unsupervised approach for transforming word embeddings from source language to target language using a transformation matrix. Our model handles the problem of data scarcity which is faced by many languages in the world and yields improved word embeddings for words in the target language by relying on transformed embeddings of words of the source language. We initially evaluate our approach via word similarity tasks on a similar language pair - Hindi as source and Urdu as the target language, while we also evaluate our method on French and German as target languages and English as source language. Our approach improves the current state of the art results - by 13% for French and 19% for German. For Urdu, we saw an increment of 16% over our initial baseline score. We further explore the prospects of our approach by applying it on multiple models of the same language and transferring words between the two models, thus solving the problem of missing words in a model. We evaluate this on word similarity and word analogy tasks.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05732",
        "title": "ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations",
        "authors": [
            "John Wieting",
            "Kevin Gimpel"
        ],
        "abstract": "We describe PARANMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2018-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05780",
        "title": "Detecting Egregious Conversations between Customers and Virtual Agents",
        "authors": [
            "Tommy Sandbank",
            "Michal Shmueli-Scheuer",
            "Jonathan Herzig",
            "David Konopnicki",
            "John Richards",
            "David Piorkowski"
        ],
        "abstract": "Virtual agents are becoming a prominent channel of interaction in customer service. Not all customer interactions are smooth, however, and some can become almost comically bad. In such instances, a human agent might need to step in and salvage the conversation. Detecting bad conversations is important since disappointing customer service may threaten customer loyalty and impact revenue. In this paper, we outline an approach to detecting such egregious conversations, using behavioral cues from the user, patterns in agent responses, and user-agent interaction. Using logs of two commercial systems, we show that using these features improves the detection F1-score by around 20% over using textual features alone. In addition, we show that those features are common across two quite different domains and, arguably, universal.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2018-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05789",
        "title": "CMU LiveMedQA at TREC 2017 LiveQA: A Consumer Health Question Answering System",
        "authors": [
            "Yuan Yang",
            "Jingcheng Yu",
            "Ye Hu",
            "Xiaoyao Xu",
            "Eric Nyberg"
        ],
        "abstract": "In this paper, we present LiveMedQA, a question answering system that is optimized for consumer health question. On top of the general QA system pipeline, we introduce several new features that aim to exploit domain-specific knowledge and entity structures for better performance. This includes a question type/focus analyzer based on deep text classification model, a tree-based knowledge graph for answer generation and a complementary structure-aware searcher for answer retrieval. LiveMedQA system is evaluated in the TREC 2017 LiveQA medical subtask, where it received an average score of 0.356 on a 3 point scale. Evaluation results revealed 3 substantial drawbacks in current LiveMedQA system, based on which we provide a detailed discussion and propose a few solutions that constitute the main focus of our subsequent work.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05795",
        "title": "Finer Grained Entity Typing with TypeNet",
        "authors": [
            "Shikhar Murty",
            "Patrick Verga",
            "Luke Vilnis",
            "Andrew McCallum"
        ],
        "abstract": "We consider the challenging problem of entity typing over an extremely fine grained set of types, wherein a single mention or entity can have many simultaneous and often hierarchically-structured types. Despite the importance of the problem, there is a relative lack of resources in the form of fine-grained, deep type hierarchies aligned to existing knowledge bases. In response, we introduce TypeNet, a dataset of entity types consisting of over 1941 types organized in a hierarchy, obtained by manually annotating a mapping from 1081 Freebase types to WordNet. We also experiment with several models comparable to state-of-the-art systems and explore techniques to incorporate a structure loss on the hierarchy with the standard mention typing loss, as a first step towards future research on this dataset.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05851",
        "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
        "authors": [
            "Rajarshi Das",
            "Shehzaad Dhuliawala",
            "Manzil Zaheer",
            "Luke Vilnis",
            "Ishan Durugkar",
            "Akshay Krishnamurthy",
            "Alex Smola",
            "Andrew McCallum"
        ],
        "abstract": "Knowledge bases (KB), both automatically and manually constructed, are often incomplete --- many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with combinatorially many destinations from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths. Empirically, this approach obtains state-of-the-art results on several datasets, significantly outperforming prior methods.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2018-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05885",
        "title": "Crowdsourcing Question-Answer Meaning Representations",
        "authors": [
            "Julian Michael",
            "Gabriel Stanovsky",
            "Luheng He",
            "Ido Dagan",
            "Luke Zettlemoyer"
        ],
        "abstract": "We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We also develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions. A detailed qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including PropBank, NomBank, QA-SRL, and AMR) along with many previously under-resourced ones, including implicit arguments and relations. The QAMR data and annotation code is made publicly available to enable future work on how best to model these complex phenomena.\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2017-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06061",
        "title": "An Encoder-Decoder Framework Translating Natural Language to Database Queries",
        "authors": [
            "Ruichu Cai",
            "Boyan Xu",
            "Xiaoyan Yang",
            "Zhenjie Zhang",
            "Zijian Li",
            "Zhihao Liang"
        ],
        "abstract": "Machine translation is going through a radical revolution, driven by the explosive development of deep learning techniques using Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). In this paper, we consider a special case in machine translation problems, targeting to convert natural language into Structured Query Language (SQL) for data retrieval over relational database. Although generic CNN and RNN learn the grammar structure of SQL when trained with sufficient samples, the accuracy and training efficiency of the model could be dramatically improved, when the translation model is deeply integrated with the grammar rules of SQL. We present a new encoder-decoder framework, with a suite of new approaches, including new semantic features fed into the encoder, grammar-aware states injected into the memory of decoder, as well as recursive state management for sub-queries. These techniques help the neural network better focus on understanding semantics of operations in natural language and save the efforts on SQL grammar learning. The empirical evaluation on real world database and queries show that our approach outperform state-of-the-art solution by a significant margin.\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2018-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06141",
        "title": "ConvAMR: Abstract meaning representation parsing for legal document",
        "authors": [
            "Lai Dac Viet",
            "Vu Trong Sinh",
            "Nguyen Le Minh",
            "Ken Satoh"
        ],
        "abstract": "Convolutional neural networks (CNN) have recently achieved remarkable performance in a wide range of applications. In this research, we equip convolutional sequence-to-sequence (seq2seq) model with an efficient graph linearization technique for abstract meaning representation parsing. Our linearization method is better than the prior method at signaling the turn of graph traveling. Additionally, convolutional seq2seq model is more appropriate and considerably faster than the recurrent neural network models in this task. Our method outperforms previous methods by a large margin on both the standard dataset LDC2014T12. Our result indicates that future works still have a room for improving parsing model using graph linearization approach.\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2017-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06196",
        "title": "Addressing Cross-Lingual Word Sense Disambiguation on Low-Density Languages: Application to Persian",
        "authors": [
            "Navid Rekabsaz",
            "Mihai Lupu",
            "Allan Hanbury",
            "Andres Duque"
        ],
        "abstract": "We explore the use of unsupervised methods in Cross-Lingual Word Sense Disambiguation (CL-WSD) with the application of English to Persian. Our proposed approach targets the languages with scarce resources (low-density) by exploiting word embedding and semantic similarity of the words in context. We evaluate the approach on a recent evaluation benchmark and compare it with the state-of-the-art unsupervised system (CO-Graph). The results show that our approach outperforms both the standard baseline and the CO-Graph system in both of the task evaluation metrics (Out-Of-Five and Best result).\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2018-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06238",
        "title": "A Generative Approach to Question Answering",
        "authors": [
            "Rajarshee Mitra"
        ],
        "abstract": "Question Answering has come a long way from answer sentence selection, relational QA to reading and comprehension. We shift our attention to generative question answering (gQA) by which we facilitate machine to read passages and answer questions by learning to generate the answers. We frame the problem as a generative task where the encoder being a network that models the relationship between question and passage and encoding them to a vector thus facilitating the decoder to directly form an abstraction of the answer. Not being able to retain facts and making repetitions are common mistakes that affect the overall legibility of answers. To counter these issues, we employ copying mechanism and maintenance of coverage vector in our model respectively. Our results on MS-MARCO demonstrate it's superiority over baselines and we also show qualitative examples where we improved in terms of correctness and readability\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2018-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06351",
        "title": "Question Asking as Program Generation",
        "authors": [
            "Anselm Rothe",
            "Brenden M. Lake",
            "Todd M. Gureckis"
        ],
        "abstract": "A hallmark of human intelligence is the ability to ask rich, creative, and revealing questions. Here we introduce a cognitive model capable of constructing human-like questions. Our approach treats questions as formal programs that, when executed on the state of the world, output an answer. The model specifies a probability distribution over a complex, compositional space of programs, favoring concise programs that help the agent learn in the current context. We evaluate our approach by modeling the types of open-ended questions generated by humans who were attempting to learn about an ambiguous situation in a game. We find that our model predicts what questions people will ask, and can creatively produce novel questions that were not present in the training set. In addition, we compare a number of model variants, finding that both question informativeness and complexity are important for producing human-like questions.\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2017-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06729",
        "title": "Phonological (un)certainty weights lexical activation",
        "authors": [
            "Laura Gwilliams",
            "David Poeppel",
            "Alec Marantz",
            "Tal Linzen"
        ],
        "abstract": "Spoken word recognition involves at least two basic computations. First is matching acoustic input to phonological categories (e.g. /b/, /p/, /d/). Second is activating words consistent with those phonological categories. Here we test the hypothesis that the listener's probability distribution over lexical items is weighted by the outcome of both computations: uncertainty about phonological discretisation and the frequency of the selected word(s). To test this, we record neural responses in auditory cortex using magnetoencephalography, and model this activity as a function of the size and relative activation of lexical candidates. Our findings indicate that towards the beginning of a word, the processing system indeed weights lexical candidates by both phonological certainty and lexical frequency; however, later into the word, activation is weighted by frequency alone.\n    ",
        "submission_date": "2017-11-17T00:00:00",
        "last_modified_date": "2017-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06744",
        "title": "Learning to Organize Knowledge and Answer Questions with N-Gram Machines",
        "authors": [
            "Fan Yang",
            "Jiazhong Nie",
            "William W. Cohen",
            "Ni Lao"
        ],
        "abstract": "Though deep neural networks have great success in natural language processing, they are limited at more knowledge intensive AI tasks, such as open-domain Question Answering (QA). Existing end-to-end deep QA models need to process the entire text after observing the question, and therefore their complexity in responding a question is linear in the text size. This is prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web. We propose to solve this scalability issue by using symbolic meaning representations, which can be indexed and retrieved efficiently with complexity that is independent of the text size. We apply our approach, called the N-Gram Machine (NGM), to three representative tasks. First as proof-of-concept, we demonstrate that NGM successfully solves the bAbI tasks of synthetic text. Second, we show that NGM scales to large corpus by experimenting on \"life-long bAbI\", a special version of bAbI that contains millions of sentences. Lastly on the WikiMovies dataset, we use NGM to induce latent structure (i.e. schema) and answer questions from natural language Wikipedia text, with only QA pairs as weak supervision.\n    ",
        "submission_date": "2017-11-17T00:00:00",
        "last_modified_date": "2019-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06826",
        "title": "Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference",
        "authors": [
            "Moontae Lee",
            "David Mimno"
        ],
        "abstract": "The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space. However, the existing greedy algorithm often selects poor anchor words, reducing topic quality and interpretability. Rather than finding an approximate convex hull in a high-dimensional space, we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space. Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words.\n    ",
        "submission_date": "2017-11-18T00:00:00",
        "last_modified_date": "2017-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06861",
        "title": "Style Transfer in Text: Exploration and Evaluation",
        "authors": [
            "Zhenxin Fu",
            "Xiaoye Tan",
            "Nanyun Peng",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "abstract": "Style transfer is an important problem in natural language processing (NLP). However, the progress in language style transfer is lagged behind other domains, such as computer vision, mainly because of the lack of parallel data and principle evaluation metrics. In this paper, we propose to learn style transfer with non-parallel data. We explore two models to achieve this goal, and the key idea behind the proposed models is to learn separate content representations and style representations using adversarial networks. We also propose novel evaluation metrics which measure two aspects of style transfer: transfer strength and content preservation. We access our models and the evaluation metrics on two tasks: paper-news title transfer, and positive-negative review transfer. Results show that the proposed content preservation metric is highly correlate to human judgments, and the proposed models are able to generate sentences with higher style transfer strength and similar content preservation score comparing to auto-encoder.\n    ",
        "submission_date": "2017-11-18T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06872",
        "title": "Automatically Extracting Action Graphs from Materials Science Synthesis Procedures",
        "authors": [
            "Sheshera Mysore",
            "Edward Kim",
            "Emma Strubell",
            "Ao Liu",
            "Haw-Shiuan Chang",
            "Srikrishna Kompella",
            "Kevin Huang",
            "Andrew McCallum",
            "Elsa Olivetti"
        ],
        "abstract": "Computational synthesis planning approaches have achieved recent success in organic chemistry, where tabulated synthesis procedures are readily available for supervised learning. The syntheses of inorganic materials, however, exist primarily as natural language narratives contained within scientific journal articles. This synthesis information must first be extracted from the text in order to enable analogous synthesis planning methods for inorganic materials. In this work, we present a system for automatically extracting structured representations of synthesis procedures from the texts of materials science journal articles that describe explicit, experimental syntheses of inorganic compounds. We define the structured representation as a set of linked events made up of extracted scientific entities and evaluate two unsupervised approaches for extracting these structures on expert-annotated articles: a strong heuristic baseline and a generative model of procedural text. We also evaluate a variety of supervised models for extracting scientific entities. Our results provide insight into the nature of the data and directions for further work in this exciting new area of research.\n    ",
        "submission_date": "2017-11-18T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06895",
        "title": "Is China Entering WTO or shijie maoyi zuzhi--a Corpus Study of English Acronyms in Chinese Newspapers",
        "authors": [
            "Hai Hu"
        ],
        "abstract": "This is one of the first studies that quantitatively examine the usage of English acronyms (e.g. WTO) in Chinese texts. Using newspaper corpora, I try to answer 1) for all instances of a concept that has an English acronym (e.g. World Trade Organization), what percentage is expressed in the English acronym (WTO), and what percentage in its Chinese translation (shijie maoyi zuzhi), and 2) what factors are at play in language users' choice between the English and Chinese forms? Results show that different concepts have different percentage for English acronyms (PercentOfEn), ranging from 2% to 98%. Linear models show that PercentOfEn for individual concepts can be predicted by language economy (how long the Chinese translation is), concept frequency, and whether the first appearance of the concept in Chinese newspapers is the English acronym or its Chinese translation (all p < .05).\n    ",
        "submission_date": "2017-11-18T00:00:00",
        "last_modified_date": "2017-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07010",
        "title": "A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text",
        "authors": [
            "Jingjing Xu",
            "Ji Wen",
            "Xu Sun",
            "Qi Su"
        ],
        "abstract": "Named Entity Recognition and Relation Extraction for Chinese literature text is regarded as the highly difficult problem, partially because of the lack of tagging sets. In this paper, we build a discourse-level dataset from hundreds of Chinese literature articles for improving this task. To build a high quality dataset, we propose two tagging methods to solve the problem of data inconsistency, including a heuristic tagging method and a machine auxiliary tagging method. Based on this corpus, we also introduce several widely used models to conduct experiments. Experimental results not only show the usefulness of the proposed dataset, but also provide baselines for further research. The dataset is available at ",
        "submission_date": "2017-11-19T00:00:00",
        "last_modified_date": "2019-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07019",
        "title": "Incorporating Syntactic Uncertainty in Neural Machine Translation with Forest-to-Sequence Model",
        "authors": [
            "Poorya Zaremoodi",
            "Gholamreza Haffari"
        ],
        "abstract": "Incorporating syntactic information in Neural Machine Translation models is a method to compensate their requirement for a large amount of parallel training text, especially for low-resource language pairs. Previous works on using syntactic information provided by (inevitably error-prone) parsers has been promising. In this paper, we propose a forest-to-sequence Attentional Neural Machine Translation model to make use of exponentially many parse trees of the source sentence to compensate for the parser errors. Our method represents the collection of parse trees as a packed forest, and learns a neural attentional transduction model from the forest to the target sentence. Experiments on English to German, Chinese and Persian translation show the superiority of our method over the tree-to-sequence and vanilla sequence-to-sequence neural translation models.\n    ",
        "submission_date": "2017-11-19T00:00:00",
        "last_modified_date": "2017-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07065",
        "title": "Prior-aware Dual Decomposition: Document-specific Topic Inference for Spectral Topic Models",
        "authors": [
            "Moontae Lee",
            "David Bindel",
            "David Mimno"
        ],
        "abstract": "Spectral topic modeling algorithms operate on matrices/tensors of word co-occurrence statistics to learn topic-specific word distributions. This approach removes the dependence on the original documents and produces substantial gains in efficiency and provable topic inference, but at a cost: the model can no longer provide information about the topic composition of individual documents. Recently Thresholded Linear Inverse (TLI) is proposed to map the observed words of each document back to its topic composition. However, its linear characteristics limit the inference quality without considering the important prior information over topics. In this paper, we evaluate Simple Probabilistic Inverse (SPI) method and novel Prior-aware Dual Decomposition (PADD) that is capable of learning document-specific topic compositions in parallel. Experiments show that PADD successfully leverages topic correlations as a prior, notably outperforming TLI and learning quality topic compositions comparable to Gibbs sampling on various data.\n    ",
        "submission_date": "2017-11-19T00:00:00",
        "last_modified_date": "2017-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07265",
        "title": "Fast BTG-Forest-Based Hierarchical Sub-sentential Alignment",
        "authors": [
            "Hao Wang",
            "Yves Lepage"
        ],
        "abstract": "In this paper, we propose a novel BTG-forest-based alignment method. Based on a fast unsupervised initialization of parameters using variational IBM models, we synchronously parse parallel sentences top-down and align hierarchically under the constraint of BTG. Our two-step method can achieve the same run-time and comparable translation performance as fast_align while it yields smaller phrase tables. Final SMT results show that our method even outperforms in the experiment of distantly related languages, e.g., English-Japanese.\n    ",
        "submission_date": "2017-11-20T00:00:00",
        "last_modified_date": "2017-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07274",
        "title": "Speech recognition for medical conversations",
        "authors": [
            "Chung-Cheng Chiu",
            "Anshuman Tripathi",
            "Katherine Chou",
            "Chris Co",
            "Navdeep Jaitly",
            "Diana Jaunzeikare",
            "Anjuli Kannan",
            "Patrick Nguyen",
            "Hasim Sak",
            "Ananth Sankar",
            "Justin Tansuwan",
            "Nathan Wan",
            "Yonghui Wu",
            "Xuedong Zhang"
        ],
        "abstract": "In this work we explored building automatic speech recognition models for transcribing doctor patient conversation. We collected a large scale dataset of clinical conversations ($14,000$ hr), designed the task to represent the real word scenario, and explored several alignment approaches to iteratively improve data quality. We explored both CTC and LAS systems for building speech recognition models. The LAS was more resilient to noisy data and CTC required more data clean up. A detailed analysis is provided for understanding the performance for clinical tasks. Our analysis showed the speech recognition models performed well on important medical utterances, while errors occurred in causal conversations. Overall we believe the resulting models can provide reasonable quality in practice.\n    ",
        "submission_date": "2017-11-20T00:00:00",
        "last_modified_date": "2018-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07341",
        "title": "FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension",
        "authors": [
            "Hsin-Yuan Huang",
            "Chenguang Zhu",
            "Yelong Shen",
            "Weizhu Chen"
        ],
        "abstract": "This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"history of word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the \"history of word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2018-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07404",
        "title": "Non-Contextual Modeling of Sarcasm using a Neural Network Benchmark",
        "authors": [
            "N. Dianna Radpour",
            "Vinay Ashokkumar"
        ],
        "abstract": "One of the most crucial components of natural human-robot interaction is artificial intuition and its influence on dialog systems. The intuitive capability that humans have is undeniably extraordinary, and so remains one of the greatest challenges for natural communicative dialogue between humans and robots. In this paper, we introduce a novel probabilistic modeling framework of identifying, classifying and learning features of sarcastic text via training a neural network with human-informed sarcastic benchmarks. This is necessary for establishing a comprehensive sentiment analysis schema that is sensitive to the nuances of sarcasm-ridden text by being trained on linguistic cues. We show that our model provides a good fit for this type of real-world informed data, with potential to achieve as accurate, if not more, than alternatives. Though the implementation and benchmarking is an extensive task, it can be extended via the same method that we present to capture different forms of nuances in communication and making for much more natural and engaging dialogue systems.\n    ",
        "submission_date": "2017-11-20T00:00:00",
        "last_modified_date": "2017-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07611",
        "title": "Event Representations with Tensor-based Compositions",
        "authors": [
            "Noah Weber",
            "Niranjan Balasubramanian",
            "Nathanael Chambers"
        ],
        "abstract": "Robust and flexible event representations are important to many core areas in language understanding. Scripts were proposed early on as a way of representing sequences of events for such understanding, and has recently attracted renewed attention. However, obtaining effective representations for modeling script-like event sequences is challenging. It requires representations that can capture event-level and scenario-level semantics. We propose a new tensor-based composition method for creating event representations. The method captures more subtle semantic interactions between an event and its entities and yields representations that are effective at multiple event-related tasks. With the continuous representations, we also devise a simple schema generation method which produces better schemas compared to a prior discrete representation based method. Our analysis shows that the tensors capture distinct usages of a predicate even when there are only subtle differences in their surface realizations.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07632",
        "title": "Generating Thematic Chinese Poetry using Conditional Variational Autoencoders with Hybrid Decoders",
        "authors": [
            "Xiaopeng Yang",
            "Xiaowen Lin",
            "Shunda Suo",
            "Ming Li"
        ],
        "abstract": "Computer poetry generation is our first step towards computer writing. Writing must have a theme. The current approaches of using sequence-to-sequence models with attention often produce non-thematic poems. We present a novel conditional variational autoencoder with a hybrid decoder adding the deconvolutional neural networks to the general recurrent neural networks to fully learn topic information via latent variables. This approach significantly improves the relevance of the generated poems by representing each line of the poem not only in a context-sensitive manner but also in a holistic way that is highly related to the given keyword and the learned topic. A proposed augmented word2vec model further improves the rhythm and symmetry. Tests show that the generated poems by our approach are mostly satisfying with regulated rules and consistent themes, and 73.42% of them receive an Overall score no less than 3 (the highest score is 5).\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2020-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07646",
        "title": "Evaluating Machine Translation Performance on Chinese Idioms with a Blacklist Method",
        "authors": [
            "Yutong Shao",
            "Rico Sennrich",
            "Bonnie Webber",
            "Federico Fancellu"
        ],
        "abstract": "Idiom translation is a challenging problem in machine translation because the meaning of idioms is non-compositional, and a literal (word-by-word) translation is likely to be wrong. In this paper, we focus on evaluating the quality of idiom translation of MT systems. We introduce a new evaluation method based on an idiom-specific blacklist of literal translations, based on the insight that the occurrence of any blacklisted words in the translation output indicates a likely translation error. We introduce a dataset, CIBB (Chinese Idioms Blacklists Bank), and perform an evaluation of a state-of-the-art Chinese-English neural MT system. Our evaluation confirms that a sizable number of idioms in our test set are mistranslated (46.1%), that literal translation error is a common error type, and that our blacklist method is effective at identifying literal translation errors.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2018-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07656",
        "title": "Cross Temporal Recurrent Networks for Ranking Question Answer Pairs",
        "authors": [
            "Yi Tay",
            "Luu Anh Tuan",
            "Siu Cheung Hui"
        ],
        "abstract": "Temporal gates play a significant role in modern recurrent-based neural encoders, enabling fine-grained control over recursive compositional operations over time. In recurrent models such as the long short-term memory (LSTM), temporal gates control the amount of information retained or discarded over time, not only playing an important role in influencing the learned representations but also serving as a protection against vanishing gradients. This paper explores the idea of learning temporal gates for sequence pairs (question and answer), jointly influencing the learned representations in a pairwise manner. In our approach, temporal gates are learned via 1D convolutional layers and then subsequently cross applied across question and answer for joint learning. Empirically, we show that this conceptually simple sharing of temporal gates can lead to competitive performance across multiple benchmarks. Intuitively, what our network achieves can be interpreted as learning representations of question and answer pairs that are aware of what each other is remembering or forgetting, i.e., pairwise temporal gating. Via extensive experiments, we show that our proposed model achieves state-of-the-art performance on two community-based QA datasets and competitive performance on one factoid-based QA dataset.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07798",
        "title": "Visual and Textual Sentiment Analysis Using Deep Fusion Convolutional Neural Networks",
        "authors": [
            "Xingyue Chen",
            "Yunhong Wang",
            "Qingjie Liu"
        ],
        "abstract": "Sentiment analysis is attracting more and more attentions and has become a very hot research topic due to its potential applications in personalized recommendation, opinion mining, etc. Most of the existing methods are based on either textual or visual data and can not achieve satisfactory results, as it is very hard to extract sufficient information from only one single modality data. Inspired by the observation that there exists strong semantic correlation between visual and textual data in social medias, we propose an end-to-end deep fusion convolutional neural network to jointly learn textual and visual sentiment representations from training examples. The two modality information are fused together in a pooling layer and fed into fully-connected layers to predict the sentiment polarity. We evaluate the proposed approach on two widely used data sets. Results show that our method achieves promising result compared with the state-of-the-art methods which clearly demonstrate its competency.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07893",
        "title": "Effective Strategies in Zero-Shot Neural Machine Translation",
        "authors": [
            "Thanh-Le Ha",
            "Jan Niehues",
            "Alexander Waibel"
        ],
        "abstract": "In this paper, we proposed two strategies which can be applied to a multilingual neural machine translation system in order to better tackle zero-shot scenarios despite not having any parallel corpus. The experiments show that they are effective in terms of both performance and computing resources, especially in multilingual translation of unbalanced data in real zero-resourced condition when they alleviate the language bias problem.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07908",
        "title": "Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition",
        "authors": [
            "Devendra Singh Sachan",
            "Pengtao Xie",
            "Mrinmaya Sachan",
            "Eric P Xing"
        ],
        "abstract": "Biomedical named entity recognition (NER) is a fundamental task in text mining of medical documents and has many applications. Deep learning based approaches to this task have been gaining increasing attention in recent years as their parameters can be learned end-to-end without the need for hand-engineered features. However, these approaches rely on high-quality labeled data, which is expensive to obtain. To address this issue, we investigate how to use unlabeled text data to improve the performance of NER models. Specifically, we train a bidirectional language model (BiLM) on unlabeled data and transfer its weights to \"pretrain\" an NER model with the same architecture as the BiLM, which results in a better parameter initialization of the NER model. We evaluate our approach on four benchmark datasets for biomedical NER and show that it leads to a substantial improvement in the F1 scores compared with the state-of-the-art approaches. We also show that BiLM weight transfer leads to a faster model training and the pretrained model requires fewer training examples to achieve a particular F1 score.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2018-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07915",
        "title": "10Sent: A Stable Sentiment Analysis Method Based on the Combination of Off-The-Shelf Approaches",
        "authors": [
            "Philipe F. Melo",
            "Daniel H. Dalip",
            "Manoel M. Junior",
            "Marcos A. Gon\u00e7alves",
            "Fabr\u00edcio Benevenuto"
        ],
        "abstract": "Sentiment analysis has become a very important tool for analysis of social media data. There are several methods developed for this research field, many of them working very differently from each other, covering distinct aspects of the problem and disparate strategies. Despite the large number of existent techniques, there is no single one which fits well in all cases or for all data sources. Supervised approaches may be able to adapt to specific situations but they require manually labeled training, which is very cumbersome and expensive to acquire, mainly for a new application. In this context, in here, we propose to combine several very popular and effective state-of-the-practice sentiment analysis methods, by means of an unsupervised bootstrapped strategy for polarity classification. One of our main goals is to reduce the large variability (lack of stability) of the unsupervised methods across different domains (datasets). Our solution was thoroughly tested considering thirteen different datasets in several domains such as opinions, comments, and social media. The experimental results demonstrate that our combined method (aka, 10SENT) improves the effectiveness of the classification task, but more importantly, it solves a key problem in the field. It is consistently among the best methods in many data types, meaning that it can produce the best (or close to best) results in almost all considered contexts, without any additional costs (e.g., manual labeling). Our self-learning approach is also very independent of the base methods, which means that it is highly extensible to incorporate any new additional method that can be envisioned in the future. Finally, we also investigate a transfer learning approach for sentiment analysis as a means to gather additional (unsupervised) information for the proposed approach and we show the potential of this technique to improve our results.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07950",
        "title": "Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent",
        "authors": [
            "Zhilin Yang",
            "Saizheng Zhang",
            "Jack Urbanek",
            "Will Feng",
            "Alexander H. Miller",
            "Arthur Szlam",
            "Douwe Kiela",
            "Jason Weston"
        ],
        "abstract": "Contrary to most natural language processing research, which makes use of static datasets, humans learn language interactively, grounded in an environment. In this work we propose an interactive learning procedure called Mechanical Turker Descent (MTD) and use it to train agents to execute natural language commands grounded in a fantasy text adventure game. In MTD, Turkers compete to train better agents in the short term, and collaborate by sharing their agents' skills in the long term. This results in a gamified, engaging experience for the Turkers and a better quality teaching signal for the agents compared to static datasets, as the Turkers naturally adapt the training data to the agent's abilities.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2018-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08010",
        "title": "Unsupervised Adaptation with Domain Separation Networks for Robust Speech Recognition",
        "authors": [
            "Zhong Meng",
            "Zhuo Chen",
            "Vadim Mazalov",
            "Jinyu Li",
            "Yifan Gong"
        ],
        "abstract": "Unsupervised domain adaptation of speech signal aims at adapting a well-trained source-domain acoustic model to the unlabeled data from target domain. This can be achieved by adversarial training of deep neural network (DNN) acoustic models to learn an intermediate deep representation that is both senone-discriminative and domain-invariant. Specifically, the DNN is trained to jointly optimize the primary task of senone classification and the secondary task of domain classification with adversarial objective functions. In this work, instead of only focusing on learning a domain-invariant feature (i.e. the shared component between domains), we also characterize the difference between the source and target domain distributions by explicitly modeling the private component of each domain through a private component extractor DNN. The private component is trained to be orthogonal with the shared component and thus implicitly increases the degree of domain-invariance of the shared component. A reconstructor DNN is used to reconstruct the original speech feature from the private and shared components as a regularization. This domain separation framework is applied to the unsupervised environment adaptation task and achieved 11.08% relative WER reduction from the gradient reversal layer training, a representative adversarial training method, for automatic speech recognition on CHiME-3 dataset.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2019-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08083",
        "title": "Application of Natural Language Processing to Determine User Satisfaction in Public Services",
        "authors": [
            "Radoslaw Kowalski",
            "Marc Esteve",
            "Slava J. Mikhaylov"
        ],
        "abstract": "Research on customer satisfaction has increased substantially in recent years. However, the relative importance and relationships between different determinants of satisfaction remains uncertain. Moreover, quantitative studies to date tend to test for significance of pre-determined factors thought to have an influence with no scalable means to identify other causes of user satisfaction. The gaps in knowledge make it difficult to use available knowledge on user preference for public service improvement. Meanwhile, digital technology development has enabled new methods to collect user feedback, for example through online forums where users can comment freely on their experience. New tools are needed to analyze large volumes of such feedback. Use of topic models is proposed as a feasible solution to aggregate open-ended user opinions that can be easily deployed in the public sector. Generated insights can contribute to a more inclusive decision-making process in public service provision. This novel methodological approach is applied to a case of service reviews of publicly-funded primary care practices in England. Findings from the analysis of 145,000 reviews covering almost 7,700 primary care centers indicate that the quality of interactions with staff and bureaucratic exigencies are the key issues driving user satisfaction across England.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08195",
        "title": "On the Automatic Generation of Medical Imaging Reports",
        "authors": [
            "Baoyu Jing",
            "Pengtao Xie",
            "Eric Xing"
        ],
        "abstract": "Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time- consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the re- ports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the pre- diction of tags and the generation of para- graphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.\n    ",
        "submission_date": "2017-11-22T00:00:00",
        "last_modified_date": "2018-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08231",
        "title": "Does Higher Order LSTM Have Better Accuracy for Segmenting and Labeling Sequence Data?",
        "authors": [
            "Yi Zhang",
            "Xu Sun",
            "Shuming Ma",
            "Yang Yang",
            "Xuancheng Ren"
        ],
        "abstract": "Existing neural models usually predict the tag of the current token independent of the neighboring tags. The popular LSTM-CRF model considers the tag dependencies between every two consecutive tags. However, it is hard for existing neural models to take longer distance dependencies of tags into consideration. The scalability is mainly limited by the complex model structures and the cost of dynamic programming during training. In our work, we first design a new model called \"high order LSTM\" to predict multiple tags for the current token which contains not only the current tag but also the previous several tags. We call the number of tags in one prediction as \"order\". Then we propose a new method called Multi-Order BiLSTM (MO-BiLSTM) which combines low order and high order LSTMs together. MO-BiLSTM keeps the scalability to high order models with a pruning technique. We evaluate MO-BiLSTM on all-phrase chunking and NER datasets. Experiment results show that MO-BiLSTM achieves the state-of-the-art result in chunking and highly competitive results in two NER datasets.\n    ",
        "submission_date": "2017-11-22T00:00:00",
        "last_modified_date": "2018-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08412",
        "title": "Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes",
        "authors": [
            "Nikhil Garg",
            "Londa Schiebinger",
            "Dan Jurafsky",
            "James Zou"
        ],
        "abstract": "Word embeddings use vectors to represent words such that the geometry between vectors captures semantic relationship between the words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding can be leveraged to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 years of text data with the U.S. Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures global social shifts -- e.g., the women's movement in the 1960s and Asian immigration into the U.S -- and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a powerful new intersection between machine learning and quantitative social science.\n    ",
        "submission_date": "2017-11-22T00:00:00",
        "last_modified_date": "2017-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08493",
        "title": "Customized Nonlinear Bandits for Online Response Selection in Neural Conversation Models",
        "authors": [
            "Bing Liu",
            "Tong Yu",
            "Ian Lane",
            "Ole J. Mengshoel"
        ],
        "abstract": "Dialog response selection is an important step towards natural response generation in conversational agents. Existing work on neural conversational models mainly focuses on offline supervised learning using a large set of context-response pairs. In this paper, we focus on online learning of response selection in retrieval-based dialog systems. We propose a contextual multi-armed bandit model with a nonlinear reward function that uses distributed representation of text for online response selection. A bidirectional LSTM is used to produce the distributed representations of dialog context and responses, which serve as the input to a contextual bandit. In learning the bandit, we propose a customized Thompson sampling method that is applied to a polynomial feature space in approximating the reward. Experimental results on the Ubuntu Dialogue Corpus demonstrate significant performance gains of the proposed method over conventional linear contextual bandits. Moreover, we report encouraging response selection performance of the proposed neural bandit model using the Recall@k metric for a small set of online training samples.\n    ",
        "submission_date": "2017-11-22T00:00:00",
        "last_modified_date": "2017-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08609",
        "title": "Improving the Accuracy of Pre-trained Word Embeddings for Sentiment Analysis",
        "authors": [
            "Seyed Mahdi Rezaeinia",
            "Ali Ghodsi",
            "Rouhollah Rahmani"
        ],
        "abstract": "Sentiment analysis is one of the well-known tasks and fast growing research areas in natural language processing (NLP) and text classifications. This technique has become an essential part of a wide range of applications including politics, business, advertising and marketing. There are various techniques for sentiment analysis, but recently word embeddings methods have been widely used in sentiment classification tasks. Word2Vec and GloVe are currently among the most accurate and usable word embedding methods which can convert words into meaningful vectors. However, these methods ignore sentiment information of texts and need a huge corpus of texts for training and generating exact vectors which are used as inputs of deep learning models. As a result, because of the small size of some corpuses, researcher often have to use pre-trained word embeddings which were trained on other large text corpus such as Google News with about 100 billion words. The increasing accuracy of pre-trained word embeddings has a great impact on sentiment analysis research. In this paper we propose a novel method, Improved Word Vectors (IWV), which increases the accuracy of pre-trained word embeddings in sentiment analysis. Our method is based on Part-of-Speech (POS) tagging techniques, lexicon-based approaches and Word2Vec/GloVe methods. We tested the accuracy of our method via different deep learning models and sentiment datasets. Our experiment results show that Improved Word Vectors (IWV) are very effective for sentiment analysis.\n    ",
        "submission_date": "2017-11-23T00:00:00",
        "last_modified_date": "2017-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08726",
        "title": "Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce",
        "authors": [
            "Jianfei Yu",
            "Minghui Qiu",
            "Jing Jiang",
            "Jun Huang",
            "Shuangyong Song",
            "Wei Chu",
            "Haiqing Chen"
        ],
        "abstract": "In this paper, we study transfer learning for the PI and NLI problems, aiming to propose a general framework, which can effectively and efficiently adapt the shared knowledge learned from a resource-rich source domain to a resource- poor target domain. Specifically, since most existing transfer learning methods only focus on learning a shared feature space across domains while ignoring the relationship between the source and target domains, we propose to simultaneously learn shared representations and domain relationships in a unified framework. Furthermore, we propose an efficient and effective hybrid model by combining a sentence encoding- based method and a sentence interaction-based method as our base model. Extensive experiments on both paraphrase identification and natural language inference demonstrate that our base model is efficient and has promising performance compared to the competing models, and our transfer learning method can help to significantly boost the performance. Further analysis shows that the inter-domain and intra-domain relationship captured by our model are insightful. Last but not least, we deploy our transfer learning model for PI into our online chatbot system, which can bring in significant improvements over our existing system. Finally, we launch our new system on the chatbot platform Eva in our E-commerce site AliExpress.\n    ",
        "submission_date": "2017-11-23T00:00:00",
        "last_modified_date": "2017-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08792",
        "title": "SPINE: SParse Interpretable Neural Embeddings",
        "authors": [
            "Anant Subramanian",
            "Danish Pruthi",
            "Harsh Jhamtani",
            "Taylor Berg-Kirkpatrick",
            "Eduard Hovy"
        ],
        "abstract": "Prediction without justification has limited utility. Much of the success of neural models can be attributed to their ability to learn rich, dense and expressive representations. While these representations capture the underlying complexity and latent trends in the data, they are far from being interpretable. We propose a novel variant of denoising k-sparse autoencoders that generates highly efficient and interpretable distributed word representations (word embeddings), beginning with existing word representations from state-of-the-art methods like GloVe and word2vec. Through large scale human evaluation, we report that our resulting word embedddings are much more interpretable than the original GloVe and word2vec embeddings. Moreover, our embeddings outperform existing popular word embeddings on a diverse suite of benchmark downstream tasks.\n    ",
        "submission_date": "2017-11-23T00:00:00",
        "last_modified_date": "2017-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09050",
        "title": "Ethical Challenges in Data-Driven Dialogue Systems",
        "authors": [
            "Peter Henderson",
            "Koustuv Sinha",
            "Nicolas Angelard-Gontier",
            "Nan Rosemary Ke",
            "Genevieve Fried",
            "Ryan Lowe",
            "Joelle Pineau"
        ],
        "abstract": "The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.\n    ",
        "submission_date": "2017-11-24T00:00:00",
        "last_modified_date": "2017-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09160",
        "title": "An Exploration of Word Embedding Initialization in Deep-Learning Tasks",
        "authors": [
            "Tom Kocmi",
            "Ond\u0159ej Bojar"
        ],
        "abstract": "Word embeddings are the interface between the world of discrete units of text processing and the continuous, differentiable world of neural networks. In this work, we examine various random and pretrained initialization methods for embeddings used in deep networks and their effect on the performance on four NLP tasks with both recurrent and convolutional architectures. We confirm that pretrained embeddings are a little better than random initialization, especially considering the speed of learning. On the other hand, we do not see any significant difference between various methods of random initialization, as long as the variance is kept reasonably low. High-variance initialization prevents the network to use the space of embeddings and forces it to use other free parameters to accomplish the task. We support this hypothesis by observing the performance in learning lexical relations and by the fact that the network can learn to perform reasonably in its task even with fixed random embeddings.\n    ",
        "submission_date": "2017-11-24T00:00:00",
        "last_modified_date": "2017-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09181",
        "title": "Towards Accurate Deceptive Opinion Spam Detection based on Word Order-preserving CNN",
        "authors": [
            "Siyuan Zhao",
            "Zhiwei Xu",
            "Limin Liu",
            "Mengjie Guo"
        ],
        "abstract": "Nowadays, deep learning has been widely used. In natural language learning, the analysis of complex semantics has been achieved because of its high degree of flexibility. The deceptive opinions detection is an important application area in deep learning model, and related mechanisms have been given attention and researched. On-line opinions are quite short, varied types and content. In order to effectively identify deceptive opinions, we need to comprehensively study the characteristics of deceptive opinions, and explore novel characteristics besides the textual semantics and emotional polarity that have been widely used in text analysis. The detection mechanism based on deep learning has better self-adaptability and can effectively identify all kinds of deceptive opinions. In this paper, we optimize the convolution neural network model by embedding the word order characteristics in its convolution layer and pooling layer, which makes convolution neural network more suitable for various text classification and deceptive opinions detection. The TensorFlow-based experiments demonstrate that the detection mechanism proposed in this paper achieve more accurate deceptive opinion detection results.\n    ",
        "submission_date": "2017-11-25T00:00:00",
        "last_modified_date": "2018-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09271",
        "title": "Acronym Disambiguation: A Domain Independent Approach",
        "authors": [
            "Aditya Thakker",
            "Suhail Barot",
            "Sudhir Bagul"
        ],
        "abstract": "Acronyms are omnipresent. They usually express information that is repetitive and well known. But acronyms can also be ambiguous because there can be multiple expansions for the same acronym. In this paper, we propose a general system for acronym disambiguation that can work on any acronym given some context information. We present methods for retrieving all the possible expansions of an acronym from Wikipedia and ",
        "submission_date": "2017-11-25T00:00:00",
        "last_modified_date": "2017-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09285",
        "title": "Experiential, Distributional and Dependency-based Word Embeddings have Complementary Roles in Decoding Brain Activity",
        "authors": [
            "Samira Abnar",
            "Rasyan Ahmed",
            "Max Mijnheer",
            "Willem Zuidema"
        ],
        "abstract": "We evaluate 8 different word embedding models on their usefulness for predicting the neural activation patterns associated with concrete nouns. The models we consider include an experiential model, based on crowd-sourced association data, several popular neural and distributional models, and a model that reflects the syntactic context of words (based on dependency parses). Our goal is to assess the cognitive plausibility of these various embedding models, and understand how we can further improve our methods for interpreting brain imaging data.\n",
        "submission_date": "2017-11-25T00:00:00",
        "last_modified_date": "2017-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09357",
        "title": "Generative Adversarial Network for Abstractive Text Summarization",
        "authors": [
            "Linqing Liu",
            "Yao Lu",
            "Min Yang",
            "Qiang Qu",
            "Jia Zhu",
            "Hongyan Li"
        ],
        "abstract": "In this paper, we propose an adversarial process for abstractive text summarization, in which we simultaneously train a generative model G and a discriminative model D. In particular, we build the generator G as an agent of reinforcement learning, which takes the raw text as input and predicts the abstractive summarization. We also build a discriminator which attempts to distinguish the generated summary from the ground truth summary. Extensive experiments demonstrate that our model achieves competitive ROUGE scores with the state-of-the-art methods on CNN/Daily Mail dataset. Qualitatively, we show that our model is able to generate more abstractive, readable and diverse summaries.\n    ",
        "submission_date": "2017-11-26T00:00:00",
        "last_modified_date": "2017-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09367",
        "title": "Learning to Remember Translation History with a Continuous Cache",
        "authors": [
            "Zhaopeng Tu",
            "Yang Liu",
            "Shuming Shi",
            "Tong Zhang"
        ],
        "abstract": "Existing neural machine translation (NMT) models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose to augment NMT models with a very light-weight cache-like memory network, which stores recent hidden representations as translation history. The probability distribution over generated words is updated online depending on the translation history retrieved from the memory, endowing NMT models with the capability to dynamically adapt over time. Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the computational cost.\n    ",
        "submission_date": "2017-11-26T00:00:00",
        "last_modified_date": "2017-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09395",
        "title": "Improved Neural Text Attribute Transfer with Non-parallel Data",
        "authors": [
            "Igor Melnyk",
            "Cicero Nogueira dos Santos",
            "Kahini Wadhawan",
            "Inkit Padhi",
            "Abhishek Kumar"
        ],
        "abstract": "Text attribute transfer using non-parallel data requires methods that can perform disentanglement of content and linguistic attributes. In this work, we propose multiple improvements over the existing approaches that enable the encoder-decoder framework to cope with the text attribute transfer from non-parallel data. We perform experiments on the sentiment transfer task using two datasets. For both datasets, our proposed method outperforms a strong baseline in two of the three employed evaluation metrics.\n    ",
        "submission_date": "2017-11-26T00:00:00",
        "last_modified_date": "2017-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09476",
        "title": "Machine Translation using Semantic Web Technologies: A Survey",
        "authors": [
            "Diego Moussallem",
            "Matthias Wauer",
            "Axel-Cyrille Ngonga Ngomo"
        ],
        "abstract": "A large number of machine translation approaches have recently been developed to facilitate the fluid migration of content across languages. However, the literature suggests that many obstacles must still be dealt with to achieve better automatic translations. One of these obstacles is lexical and syntactic ambiguity. A promising way of overcoming this problem is using Semantic Web technologies. This article presents the results of a systematic review of machine translation approaches that rely on Semantic Web technologies for translating texts. Overall, our survey suggests that while Semantic Web technologies can enhance the quality of machine translation outputs for various problems, the combination of both is still in its infancy.\n    ",
        "submission_date": "2017-11-26T00:00:00",
        "last_modified_date": "2018-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09502",
        "title": "Modeling Past and Future for Neural Machine Translation",
        "authors": [
            "Zaixiang Zheng",
            "Hao Zhou",
            "Shujian Huang",
            "Lili Mou",
            "Xinyu Dai",
            "Jiajun Chen",
            "Zhaopeng Tu"
        ],
        "abstract": "Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase. To address this problem, we propose a novel mechanism that separates the source information into two parts: translated Past contents and untranslated Future contents, which are modeled by two additional recurrent layers. The Past and Future contents are fed to both the attention model and the decoder states, which offers NMT systems the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves translation performance in Chinese-English, German-English and English-German translation tasks. Specifically, the proposed model outperforms the conventional coverage model in both of the translation quality and the alignment error rate.\n    ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2017-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09534",
        "title": "Neural Text Generation: A Practical Guide",
        "authors": [
            "Ziang Xie"
        ],
        "abstract": "Deep learning methods have recently achieved great empirical success on machine translation, dialogue response generation, summarization, and other text generation tasks. At a high level, the technique has been to train end-to-end neural network models consisting of an encoder model to produce a hidden representation of the source text, followed by a decoder model to generate the target. While such models have significantly fewer pieces than earlier systems, significant tuning is still required to achieve good performance. For text generation models in particular, the decoder can behave in undesired ways, such as by generating truncated or repetitive outputs, outputting bland and generic responses, or in some cases producing ungrammatical gibberish. This paper is intended as a practical guide for resolving such undesired behavior in text generation models, with the aim of helping enable real-world applications.\n    ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09573",
        "title": "Code Completion with Neural Attention and Pointer Networks",
        "authors": [
            "Jian Li",
            "Yue Wang",
            "Michael R. Lyu",
            "Irwin King"
        ],
        "abstract": "Intelligent code completion has become an essential research task to accelerate modern software development. To facilitate effective code completion for dynamically-typed programming languages, we apply neural language models by learning from large codebases, and develop a tailored attention mechanism for code completion. However, standard neural language models even with attention mechanism cannot correctly predict the out-of-vocabulary (OoV) words that restrict the code completion performance. In this paper, inspired by the prevalence of locally repeated terms in program source code, and the recently proposed pointer copy mechanism, we propose a pointer mixture network for better predicting OoV words in code completion. Based on the context, the pointer mixture network learns to either generate a within-vocabulary word through an RNN component, or regenerate an OoV word from local context through a pointer component. Experiments on two benchmarked datasets demonstrate the effectiveness of our attention mechanism and pointer mixture network on the code completion task.\n    ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2018-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09645",
        "title": "Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis",
        "authors": [
            "Stefanos Angelidis",
            "Mirella Lapata"
        ],
        "abstract": "We consider the task of fine-grained sentiment analysis from the perspective of multiple instance learning (MIL). Our neural model is trained on document sentiment labels, and learns to predict the sentiment of text segments, i.e. sentences or elementary discourse units (EDUs), without segment-level supervision. We introduce an attention-based polarity scoring method for identifying positive and negative text snippets and a new dataset which we call SPOT (as shorthand for Segment-level POlariTy annotations) for evaluating MIL-style sentiment models like ours. Experimental results demonstrate superior performance against multiple baselines, whereas a judgement elicitation study shows that EDU-level opinion extraction produces more informative summaries than sentence-based alternatives.\n    ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2018-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09684",
        "title": "Production Ready Chatbots: Generate if not Retrieve",
        "authors": [
            "Aniruddha Tammewar",
            "Monik Pamecha",
            "Chirag Jain",
            "Apurva Nagvenkar",
            "Krupal Modi"
        ],
        "abstract": "In this paper, we present a hybrid model that combines a neural conversational model and a rule-based graph dialogue system that assists users in scheduling reminders through a chat conversation. The graph based system has high precision and provides a grammatically accurate response but has a low recall. The neural conversation model can cater to a variety of requests, as it generates the responses word by word as opposed to using canned responses. The hybrid system shows significant improvements over the existing baseline system of rule based approach and caters to complex queries with a domain-restricted neural model. Restricting the conversation topic and combination of graph based retrieval system with a neural generative model makes the final system robust enough for a real world application.\n    ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09724",
        "title": "Table-to-text Generation by Structure-aware Seq2seq Learning",
        "authors": [
            "Tianyu Liu",
            "Kexiang Wang",
            "Lei Sha",
            "Baobao Chang",
            "Zhifang Sui"
        ],
        "abstract": "Table-to-text generation aims to generate a description for a factual table which can be viewed as a set of field-value records. To encode both the content and the structure of a table, we propose a novel structure-aware seq2seq architecture which consists of field-gating encoder and description generator with dual attention. In the encoding phase, we update the cell memory of the LSTM unit by a field gate and its corresponding field value in order to incorporate field information into table representation. In the decoding phase, dual attention mechanism which contains word level attention and field level attention is proposed to model the semantic relevance between the generated description and the table. We conduct experiments on the \\texttt{WIKIBIO} dataset which contains over 700k biographies and corresponding infoboxes from Wikipedia. The attention visualizations and case studies show that our model is capable of generating coherent and informative descriptions based on the comprehensive understanding of both the content and the structure of a table. Automatic evaluations also show our model outperforms the baselines by a great margin. Code for this work is available on ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09824",
        "title": "Lexical-semantic resources: yet powerful resources for automatic personality classification",
        "authors": [
            "Xuan-Son Vu",
            "Lucie Flekova",
            "Lili Jiang",
            "Iryna Gurevych"
        ],
        "abstract": "In this paper, we aim to reveal the impact of lexical-semantic resources, used in particular for word sense disambiguation and sense-level semantic categorization, on automatic personality classification task. While stylistic features (e.g., part-of-speech counts) have been shown their power in this task, the impact of semantics beyond targeted word lists is relatively unexplored. We propose and extract three types of lexical-semantic features, which capture high-level concepts and emotions, overcoming the lexical gap of word n-grams. Our experimental results are comparable to state-of-the-art methods, while no personality-specific resources are required.\n    ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09873",
        "title": "Slim Embedding Layers for Recurrent Neural Language Models",
        "authors": [
            "Zhongliang Li",
            "Raymond Kulhanek",
            "Shaojun Wang",
            "Yunxin Zhao",
            "Shuang Wu"
        ],
        "abstract": "Recurrent neural language models are the state-of-the-art models for language modeling. When the vocabulary size is large, the space taken to store the model parameters becomes the bottleneck for the use of recurrent neural language models. In this paper, we introduce a simple space compression method that randomly shares the structured parameters at both the input and output embedding layers of the recurrent neural language models to significantly reduce the size of model parameters, but still compactly represent the original input and output embedding layers. The method is easy to implement and tune. Experiments on several data sets show that the new method can get similar perplexity and BLEU score results while only using a very tiny fraction of parameters.\n    ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10093",
        "title": "Surfacing contextual hate speech words within social media",
        "authors": [
            "Jherez Taylor",
            "Melvyn Peignon",
            "Yi-Shin Chen"
        ],
        "abstract": "Social media platforms have recently seen an increase in the occurrence of hate speech discourse which has led to calls for improved detection methods. Most of these rely on annotated data, keywords, and a classification technique. While this approach provides good coverage, it can fall short when dealing with new terms produced by online extremist communities which act as original sources of words which have alternate hate speech meanings. These code words (which can be both created and adopted words) are designed to evade automatic detection and often have benign meanings in regular discourse. As an example, \"skypes\", \"googles\", and \"yahoos\" are all instances of words which have an alternate meaning that can be used for hate speech. This overlap introduces additional challenges when relying on keywords for both the collection of data that is specific to hate speech, and downstream classification. In this work, we develop a community detection approach for finding extremist hate speech communities and collecting data from their members. We also develop a word embedding model that learns the alternate hate speech meaning of words and demonstrate the candidacy of our code words with several annotation experiments, designed to determine if it is possible to recognize a word as being used for hate speech without knowing its alternate meaning. We report an inter-annotator agreement rate of K=0.871, and K=0.676 for data drawn from our extremist community and the keyword approach respectively, supporting our claim that hate speech detection is a contextual task and does not depend on a fixed list of keywords. Our goal is to advance the domain by providing a high quality hate speech dataset in addition to learned code words that can be fed into existing classification approaches, thus improving the accuracy of automated detection.\n    ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10122",
        "title": "End-to-end Adversarial Learning for Generative Conversational Agents",
        "authors": [
            "Oswaldo Ludwig"
        ],
        "abstract": "This paper presents a new adversarial learning method for generative conversational agents (GCA) besides a new model of GCA. Similar to previous works on adversarial learning for dialogue generation, our method assumes the GCA as a generator that aims at fooling a discriminator that labels dialogues as human-generated or machine-generated; however, in our approach, the discriminator performs token-level classification, i.e. it indicates whether the current token was generated by humans or machines. To do so, the discriminator also receives the context utterances (the dialogue history) and the incomplete answer up to the current token as input. This new approach makes possible the end-to-end training by backpropagation. A self-conversation process enables to produce a set of generated data with more diversity for the adversarial training. This approach improves the performance on questions not related to the training data. Experimental results with human and adversarial evaluations show that the adversarial method yields significant performance gains over the usual teacher forcing training.\n    ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2018-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10124",
        "title": "Vietnamese Semantic Role Labelling",
        "authors": [
            "Phuong Le-Hong",
            "Thai Hoang Pham",
            "Xuan Khoai Pham",
            "Thi Minh Huyen Nguyen",
            "Thi Luong Nguyen",
            "Minh Hiep Nguyen"
        ],
        "abstract": "In this paper, we study semantic role labelling (SRL), a subtask of semantic parsing of natural language sentences and its application for the Vietnamese language. We present our effort in building Vietnamese PropBank, the first Vietnamese SRL corpus and a software system for labelling semantic roles of Vietnamese texts. In particular, we present a novel constituent extraction algorithm in the argument candidate identification step which is more suitable and more accurate than the common node-mapping method. In the machine learning part, our system integrates distributed word features produced by two recent unsupervised learning models in two learned statistical classifiers and makes use of integer linear programming inference procedure to improve the accuracy. The system is evaluated in a series of experiments and achieves a good result, an $F_1$ score of 74.77%. Our system, including corpus and software, is available as an open source project for free research and we believe that it is a good baseline for the development of future Vietnamese SRL systems.\n    ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10133",
        "title": "Unsupervised Discovery of Structured Acoustic Tokens with Applications to Spoken Term Detection",
        "authors": [
            "Cheng-Tao Chung",
            "Lin-Shan Lee"
        ],
        "abstract": "In this paper, we compare two paradigms for unsupervised discovery of structured acoustic tokens directly from speech corpora without any human annotation. The Multigranular Paradigm seeks to capture all available information in the corpora with multiple sets of tokens for different model granularities. The Hierarchical Paradigm attempts to jointly learn several levels of signal representations in a hierarchical structure. The two paradigms are unified within a theoretical framework in this paper. Query-by-Example Spoken Term Detection (QbE-STD) experiments on the QUESST dataset of MediaEval 2015 verifies the competitiveness of the acoustic tokens. The Enhanced Relevance Score (ERS) proposed in this work improves both paradigms for the task of QbE-STD. We also list results on the ABX evaluation task of the Zero Resource Challenge 2015 for comparison of the Paradigms.\n    ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10136",
        "title": "Acoustic-To-Word Model Without OOV",
        "authors": [
            "Jinyu Li",
            "Guoli Ye",
            "Rui Zhao",
            "Jasha Droppo",
            "Yifan Gong"
        ],
        "abstract": "Recently, the acoustic-to-word model based on the Connectionist Temporal Classification (CTC) criterion was shown as a natural end-to-end model directly targeting words as output units. However, this type of word-based CTC model suffers from the out-of-vocabulary (OOV) issue as it can only model limited number of words in the output layer and maps all the remaining words into an OOV output node. Therefore, such word-based CTC model can only recognize the frequent words modeled by the network output nodes. It also cannot easily handle the hot-words which emerge after the model is trained. In this study, we improve the acoustic-to-word model with a hybrid CTC model which can predict both words and characters at the same time. With a shared-hidden-layer structure and modular design, the alignments of words generated from the word-based CTC and the character-based CTC are synchronized. Whenever the acoustic-to-word model emits an OOV token, we back off that OOV segment to the word output generated from the character-based CTC, hence solving the OOV or hot-words issue. Evaluated on a Microsoft Cortana voice assistant task, the proposed model can reduce the errors introduced by the OOV output token in the acoustic-to-word model by 30%.\n    ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10163",
        "title": "Hybrid Oracle: Making Use of Ambiguity in Transition-based Chinese Dependency Parsing",
        "authors": [
            "Xuancheng Ren",
            "Xu Sun"
        ],
        "abstract": "In the training of transition-based dependency parsers, an oracle is used to predict a transition sequence for a sentence and its gold tree. However, the transition system may exhibit ambiguity, that is, there can be multiple correct transition sequences that form the gold tree. We propose to make use of the property in the training of neural dependency parsers, and present the Hybrid Oracle. The new oracle gives all the correct transitions for a parsing state, which are used in the cross entropy loss function to provide better supervisory signal. It is also used to generate different transition sequences for a sentence to better explore the training data and improve the generalization ability of the parser. Evaluations show that the parsers trained using the hybrid oracle outperform the parsers using the traditional oracle in Chinese dependency parsing. We provide analysis from a linguistic view. The code is available at ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2018-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10203",
        "title": "Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure",
        "authors": [
            "Dieuwke Hupkes",
            "Sara Veldhoen",
            "Willem Zuidema"
        ],
        "abstract": "We investigate how neural networks can learn and process languages with hierarchical, compositional semantics. To this end, we define the artificial task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. We find that recursive neural networks can find a generalising solution to this problem, and we visualise this solution by breaking it up in three steps: project, sum and squash. As a next step, we investigate recurrent neural networks, and show that a gated recurrent unit, that processes its input incrementally, also performs very well on this task. To develop an understanding of what the recurrent network encodes, visualisation techniques alone do not suffice. Therefore, we develop an approach where we formulate and test multiple hypotheses on the information encoded and processed by the network. For each hypothesis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. Our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. This is turn shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. We argue that diagnostic classification, unlike most visualisation techniques, does scale up from small networks in a toy domain, to larger and deeper recurrent networks dealing with real-life data, and may therefore contribute to a better understanding of the internal dynamics of current state-of-the-art models in natural language processing.\n    ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2018-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10705",
        "title": "Speaker-Sensitive Dual Memory Networks for Multi-Turn Slot Tagging",
        "authors": [
            "Young-Bum Kim",
            "Sungjin Lee",
            "Ruhi Sarikaya"
        ],
        "abstract": "In multi-turn dialogs, natural language understanding models can introduce obvious errors by being blind to contextual information. To incorporate dialog history, we present a neural architecture with Speaker-Sensitive Dual Memory Networks which encode utterances differently depending on the speaker. This addresses the different extents of information available to the system - the system knows only the surface form of user utterances while it has the exact semantics of system output. We performed experiments on real user data from Microsoft Cortana, a commercial personal assistant. The result showed a significant performance improvement over the state-of-the-art slot tagging models using contextual information.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10712",
        "title": "End-to-End Optimization of Task-Oriented Dialogue Model with Deep Reinforcement Learning",
        "authors": [
            "Bing Liu",
            "Gokhan Tur",
            "Dilek Hakkani-Tur",
            "Pararth Shah",
            "Larry Heck"
        ],
        "abstract": "In this paper, we present a neural network based task-oriented dialogue system that can be optimized end-to-end with deep reinforcement learning (RL). The system is able to track dialogue state, interface with knowledge bases, and incorporate query results into agent's responses to successfully complete task-oriented dialogues. Dialogue policy learning is conducted with a hybrid supervised and deep RL methods. We first train the dialogue agent in a supervised manner by learning directly from task-oriented dialogue corpora, and further optimize it with deep RL during its interaction with users. In the experiments on two different dialogue task domains, our model demonstrates robust performance in tracking dialogue state and producing reasonable system responses. We show that deep RL based optimization leads to significant improvement on task success rate and reduction in dialogue length comparing to supervised training model. We further show benefits of training task-oriented dialogue model end-to-end comparing to component-wise optimization with experiment results on dialogue simulations and human evaluations.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10837",
        "title": "Curriculum Q-Learning for Visual Vocabulary Acquisition",
        "authors": [
            "Ahmed H. Zaidi",
            "Russell Moore",
            "Ted Briscoe"
        ],
        "abstract": "The structure of curriculum plays a vital role in our learning process, both as children and adults. Presenting material in ascending order of difficulty that also exploits prior knowledge can have a significant impact on the rate of learning. However, the notion of difficulty and prior knowledge differs from person to person. Motivated by the need for a personalised curriculum, we present a novel method of curriculum learning for vocabulary words in the form of visual prompts. We employ a reinforcement learning model grounded in pedagogical theories that emulates the actions of a tutor. We simulate three students with different levels of vocabulary knowledge in order to evaluate the how well our model adapts to the environment. The results of the simulation reveal that through interaction, the model is able to identify the areas of weakness, as well as push students to the edge of their ZPD. We hypothesise that these methods can also be effective in training agents to learn language representations in a simulated environment where it has previously been shown that order of words and prior knowledge play an important role in the efficacy of language learning.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10960",
        "title": "Identifying Patterns of Associated-Conditions through Topic Models of Electronic Medical Records",
        "authors": [
            "Moumita Bhattacharya",
            "Claudine Jurkovitz",
            "Hagit Shatkay"
        ],
        "abstract": "Multiple adverse health conditions co-occurring in a patient are typically associated with poor prognosis and increased office or hospital visits. Developing methods to identify patterns of co-occurring conditions can assist in diagnosis. Thus identifying patterns of associations among co-occurring conditions is of growing interest. In this paper, we report preliminary results from a data-driven study, in which we apply a machine learning method, namely, topic modeling, to electronic medical records, aiming to identify patterns of associated conditions. Specifically, we use the well established latent dirichlet allocation, a method based on the idea that documents can be modeled as a mixture of latent topics, where each topic is a distribution over words. In our study, we adapt the LDA model to identify latent topics in patients' EMRs. We evaluate the performance of our method both qualitatively, and show that the obtained topics indeed align well with distinct medical phenomena characterized by co-occurring conditions.\n    ",
        "submission_date": "2017-11-17T00:00:00",
        "last_modified_date": "2017-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11027",
        "title": "Embedding Words as Distributions with a Bayesian Skip-gram Model",
        "authors": [
            "Arthur Bra\u017einskas",
            "Serhii Havrylov",
            "Ivan Titov"
        ],
        "abstract": "We introduce a method for embedding words as probability densities in a low-dimensional space. Rather than assuming that a word embedding is fixed across the entire text collection, as in standard word embedding methods, in our Bayesian model we generate it from a word-specific prior density for each occurrence of a given word. Intuitively, for each word, the prior density encodes the distribution of its potential 'meanings'. These prior densities are conceptually similar to Gaussian embeddings. Interestingly, unlike the Gaussian embeddings, we can also obtain context-specific densities: they encode uncertainty about the sense of a word given its context and correspond to posterior distributions within our model. The context-dependent densities have many potential applications: for example, we show that they can be directly used in the lexical substitution task. We describe an effective estimation method based on the variational autoencoding framework. We also demonstrate that our embeddings achieve competitive results on standard benchmarks.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2018-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11081",
        "title": "Improved Twitter Sentiment Analysis Using Naive Bayes and Custom Language Model",
        "authors": [
            "Angela Lin"
        ],
        "abstract": "In the last couple decades, social network services like Twitter have generated large volumes of data about users and their interests, providing meaningful business intelligence so organizations can better understand and engage their customers. All businesses want to know who is promoting their products, who is complaining about them, and how are these opinions bringing or diminishing value to a company. Companies want to be able to identify their high-value customers and quantify the value each user brings. Many businesses use social media metrics to calculate the user contribution score, which enables them to quantify the value that influential users bring on social media, so the businesses can offer them more differentiated services. However, the score calculation can be refined to provide a better illustration of a user's contribution. Using Microsoft Azure as a case study, we conducted Twitter sentiment analysis to develop a machine learning classification model that identifies tweet contents and sentiments most illustrative of positive-value user contribution. Using data mining and AI-powered cognitive tools, we analyzed factors of social influence and specifically, promotional language in the developer community. Our predictive model was a combination of a traditional supervised machine learning algorithm and a custom-developed natural language model for identifying promotional tweets, that identifies a product-specific promotion on Twitter with a 90% accuracy rate.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11118",
        "title": "Multimodal Attribute Extraction",
        "authors": [
            "Robert L. Logan IV",
            "Samuel Humeau",
            "Sameer Singh"
        ],
        "abstract": "The broad goal of information extraction is to derive structured information from unstructured data. However, most existing methods focus solely on text, ignoring other types of unstructured data such as images, video and audio which comprise an increasing portion of the information on the web. To address this shortcoming, we propose the task of multimodal attribute extraction. Given a collection of unstructured and semi-structured contextual information about an entity (such as a textual description, or visual depictions) the task is to extract the entity's underlying attributes. In this paper, we provide a dataset containing mixed-media data for over 2 million product items along with 7 million attribute-value pairs describing the items which can be used to train attribute extractors in a weakly supervised manner. We provide a variety of baselines which demonstrate the relative effectiveness of the individual modes of information towards solving the task, as well as study human performance.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11125",
        "title": "Predicting and Explaining Human Semantic Search in a Cognitive Model",
        "authors": [
            "Filip Miscevic",
            "Aida Nematzadeh",
            "Suzanne Stevenson"
        ],
        "abstract": "Recent work has attempted to characterize the structure of semantic memory and the search algorithms which, together, best approximate human patterns of search revealed in a semantic fluency task. There are a number of models that seek to capture semantic search processes over networks, but they vary in the cognitive plausibility of their implementation. Existing work has also neglected to consider the constraints that the incremental process of language acquisition must place on the structure of semantic memory. Here we present a model that incrementally updates a semantic network, with limited computational steps, and replicates many patterns found in human semantic fluency using a simple random walk. We also perform thorough analyses showing that a combination of both structural and semantic features are correlated with human performance patterns.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11191",
        "title": "Neural Response Generation with Dynamic Vocabularies",
        "authors": [
            "Yu Wu",
            "Wei Wu",
            "Dejian Yang",
            "Can Xu",
            "Zhoujun Li",
            "Ming Zhou"
        ],
        "abstract": "We study response generation for open domain conversation in chatbots. Existing methods assume that words in responses are generated from an identical vocabulary regardless of their inputs, which not only makes them vulnerable to generic patterns and irrelevant noise, but also causes a high cost in decoding. We propose a dynamic vocabulary sequence-to-sequence (DVS2S) model which allows each input to possess their own vocabulary in decoding. In training, vocabulary construction and response generation are jointly learned by maximizing a lower bound of the true objective with a Monte Carlo sampling method. In inference, the model dynamically allocates a small vocabulary for an input with the word prediction model, and conducts decoding only with the small vocabulary. Because of the dynamic vocabulary mechanism, DVS2S eludes many generic patterns and irrelevant words in generation, and enjoys efficient decoding at the same time. Experimental results on both automatic metrics and human annotations show that DVS2S can significantly outperform state-of-the-art methods in terms of response quality, but only requires 60% decoding time compared to the most efficient baseline.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11221",
        "title": "Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches",
        "authors": [
            "Shaohui Kuang",
            "Deyi Xiong",
            "Weihua Luo",
            "Guodong Zhou"
        ],
        "abstract": "Sentences in a well-formed text are connected to each other via various links to form the cohesive structure of the text. Current neural machine translation (NMT) systems translate a text in a conventional sentence-by-sentence fashion, ignoring such cross-sentence links and dependencies. This may lead to generate an incoherent target text for a coherent source text. In order to handle this issue, we propose a cache-based approach to modeling coherence for neural machine translation by capturing contextual information either from recently translated sentences or the entire document. Particularly, we explore two types of caches: a dynamic cache, which stores words from the best translation hypotheses of preceding sentences, and a topic cache, which maintains a set of target-side topical words that are semantically related to the document to be translated. On this basis, we build a new layer to score target words in these two caches with a cache-based neural model. Here the estimated probabilities from the cache-based neural model are combined with NMT probabilities into the final word prediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2018-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11310",
        "title": "Multi-Domain Adversarial Learning for Slot Filling in Spoken Language Understanding",
        "authors": [
            "Bing Liu",
            "Ian Lane"
        ],
        "abstract": "The goal of this paper is to learn cross-domain representations for slot filling task in spoken language understanding (SLU). Most of the recently published SLU models are domain-specific ones that work on individual task domains. Annotating data for each individual task domain is both financially costly and non-scalable. In this work, we propose an adversarial training method in learning common features and representations that can be shared across multiple domains. Model that produces such shared representations can be combined with models trained on individual domain SLU data to reduce the amount of training samples required for developing a new domain. In our experiments using data sets from multiple domains, we show that adversarial training helps in learning better domain-general SLU models, leading to improved slot filling F1 scores. We further show that applying adversarial learning on domain-general model also helps in achieving higher slot filling performance when the model is jointly optimized with domain-specific models.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11508",
        "title": "Calculating Semantic Similarity between Academic Articles using Topic Event and Ontology",
        "authors": [
            "Ming Liu",
            "Bo Lang",
            "Zepeng Gu"
        ],
        "abstract": "Determining semantic similarity between academic documents is crucial to many tasks such as plagiarism detection, automatic technical survey and semantic search. Current studies mostly focus on semantic similarity between concepts, sentences and short text fragments. However, document-level semantic matching is still based on statistical information in surface level, neglecting article structures and global semantic meanings, which may cause the deviation in document understanding. In this paper, we focus on the document-level semantic similarity issue for academic literatures with a novel method. We represent academic articles with topic events that utilize multiple information profiles, such as research purposes, methodologies and domains to integrally describe the research work, and calculate the similarity between topic events based on the domain ontology to acquire the semantic similarity between articles. Experiments show that our approach achieves significant performance compared to state-of-the-art methods.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11513",
        "title": "Lexical and Derivational Meaning in Vector-Based Models of Relativisation",
        "authors": [
            "Michael Moortgat",
            "Gijs Wijnholds"
        ],
        "abstract": "Sadrzadeh et al (2013) present a compositional distributional analysis of relative clauses in English in terms of the Frobenius algebraic structure of finite dimensional vector spaces. The analysis relies on distinct type assignments and lexical recipes for subject vs object relativisation. The situation for Dutch is different: because of the verb final nature of Dutch, relative clauses are ambiguous between a subject vs object relativisation reading. Using an extended version of Lambek calculus, we present a compositional distributional framework that accounts for this derivational ambiguity, and that allows us to give a single meaning recipe for the relative pronoun reconciling the Frobenius semantics with the demands of Dutch derivational syntax.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00044",
        "title": "Graph Centrality Measures for Boosting Popularity-Based Entity Linking",
        "authors": [
            "Hussam Hamdan",
            "Jean-Gabriel Ganascia"
        ],
        "abstract": "Many Entity Linking systems use collective graph-based methods to disambiguate the entity mentions within a document. Most of them have focused on graph construction and initial weighting of the candidate entities, less attention has been devoted to compare the graph ranking algorithms. In this work, we focus on the graph-based ranking algorithms, therefore we propose to apply five centrality measures: Degree, HITS, PageRank, Betweenness and Closeness. A disambiguation graph of candidate entities is constructed for each document using the popularity method, then centrality measures are applied to choose the most relevant candidate to boost the results of entity popularity method. We investigate the effectiveness of each centrality measure on the performance across different domains and datasets. Our experiments show that a simple and fast centrality measure such as Degree centrality can outperform other more time-consuming measures.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00069",
        "title": "On the importance of normative data in speech-based assessment",
        "authors": [
            "Zeinab Noorian",
            "Chlo\u00e9 Pou-Prom",
            "Frank Rudzicz"
        ],
        "abstract": "Data sets for identifying Alzheimer's disease (AD) are often relatively sparse, which limits their ability to train generalizable models. Here, we augment such a data set, DementiaBank, with each of two normative data sets, the Wisconsin Longitudinal Study and Talk2Me, each of which employs a speech-based picture-description assessment. Through minority class oversampling with ADASYN, we outperform state-of-the-art results in binary classification of people with and without AD in DementiaBank. This work highlights the effectiveness of combining sparse and difficult-to-acquire patient data with relatively large and easily accessible normative datasets.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00170",
        "title": "Text Generation Based on Generative Adversarial Nets with Latent Variable",
        "authors": [
            "Heng Wang",
            "Zengchang Qin",
            "Tao Wan"
        ],
        "abstract": "In this paper, we propose a model using generative adversarial net (GAN) to generate realistic text. Instead of using standard GAN, we combine variational autoencoder (VAE) with generative adversarial net. The use of high-level latent random variables is helpful to learn the data distribution and solve the problem that generative adversarial net always emits the similar data. We propose the VGAN model where the generative model is composed of recurrent neural network and VAE. The discriminative model is a convolutional neural network. We train the model via policy gradient. We apply the proposed model to the task of text generation and compare it to other recent neural network based models, such as recurrent neural network language model and SeqGAN. We evaluate the performance of the model by calculating negative log-likelihood and the BLEU score. We conduct experiments on three benchmark datasets, and results show that our model outperforms other previous models.\n    ",
        "submission_date": "2017-12-01T00:00:00",
        "last_modified_date": "2018-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00489",
        "title": "Visual Features for Context-Aware Speech Recognition",
        "authors": [
            "Abhinav Gupta",
            "Yajie Miao",
            "Leonardo Neves",
            "Florian Metze"
        ],
        "abstract": "Automatic transcriptions of consumer-generated multi-media content such as \"Youtube\" videos still exhibit high word error rates. Such data typically occupies a very broad domain, has been recorded in challenging conditions, with cheap hardware and a focus on the visual modality, and may have been post-processed or edited. In this paper, we extend our earlier work on adapting the acoustic model of a DNN-based speech recognition system to an RNN language model and show how both can be adapted to the objects and scenes that can be automatically detected in the video. We are working on a corpus of \"how-to\" videos from the web, and the idea is that an object that can be seen (\"car\"), or a scene that is being detected (\"kitchen\") can be used to condition both models on the \"context\" of the recording, thereby reducing perplexity and improving transcription. We achieve good improvements in both cases and compare and analyze the respective reductions in word error rate. We expect that our results can be used for any type of speech processing in which \"context\" information is available, for example in robotics, man-machine interaction, or when indexing large audio-visual archives, and should ultimately help to bring together the \"video-to-text\" and \"speech-to-text\" communities.\n    ",
        "submission_date": "2017-12-01T00:00:00",
        "last_modified_date": "2017-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00609",
        "title": "Improving Visually Grounded Sentence Representations with Self-Attention",
        "authors": [
            "Kang Min Yoo",
            "Youhyun Shin",
            "Sang-goo Lee"
        ],
        "abstract": "Sentence representation models trained only on language could potentially suffer from the grounding problem. Recent work has shown promising results in improving the qualities of sentence representations by jointly training them with associated image features. However, the grounding capability is limited due to distant connection between input sentences and image features by the design of the architecture. In order to further close the gap, we propose applying self-attention mechanism to the sentence encoder to deepen the grounding effect. Our results on transfer tasks show that self-attentive encoders are better for visual grounding, as they exploit specific words with strong visual associations.\n    ",
        "submission_date": "2017-12-02T00:00:00",
        "last_modified_date": "2017-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00725",
        "title": "Sentiment Classification using Images and Label Embeddings",
        "authors": [
            "Laura Graesser",
            "Abhinav Gupta",
            "Lakshay Sharma",
            "Evelina Bakhturina"
        ],
        "abstract": "In this project we analysed how much semantic information images carry, and how much value image data can add to sentiment analysis of the text associated with the images. To better understand the contribution from images, we compared models which only made use of image data, models which only made use of text data, and models which combined both data types. We also analysed if this approach could help sentiment classifiers generalize to unknown sentiments.\n    ",
        "submission_date": "2017-12-03T00:00:00",
        "last_modified_date": "2017-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00991",
        "title": "Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals",
        "authors": [
            "Girish Keshav Palshikar",
            "Sachin Pawar",
            "Saheb Chourasia",
            "Nitin Ramrakhiyani"
        ],
        "abstract": "Performance appraisal (PA) is an important HR process to periodically measure and evaluate every employee's performance vis-a-vis the goals established by the organization. A PA process involves purposeful multi-step multi-modal communication between employees, their supervisors and their peers, such as self-appraisal, supervisor assessment and peer feedback. Analysis of the structured data and text produced in PA is crucial for measuring the quality of appraisals and tracking actual improvements. In this paper, we apply text mining techniques to produce insights from PA text. First, we perform sentence classification to identify strengths, weaknesses and suggestions of improvements found in the supervisor assessments and then use clustering to discover broad categories among them. Next we use multi-class multi-label classification techniques to match supervisor assessments to predefined broad perspectives on performance. Finally, we propose a short-text summarization technique to produce a summary of peer feedback comments for a given employee and compare it with manual summaries. All techniques are illustrated using a real-life dataset of supervisor assessment and peer feedback text produced during the PA of 4528 employees in a large multi-national IT company.\n    ",
        "submission_date": "2017-12-04T00:00:00",
        "last_modified_date": "2017-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01097",
        "title": "Generalized Grounding Graphs: A Probabilistic Framework for Understanding Grounded Commands",
        "authors": [
            "Thomas Kollar",
            "Stefanie Tellex",
            "Matthew Walter",
            "Albert Huang",
            "Abraham Bachrach",
            "Sachi Hemachandra",
            "Emma Brunskill",
            "Ashis Banerjee",
            "Deb Roy",
            "Seth Teller",
            "Nicholas Roy"
        ],
        "abstract": "Many task domains require robots to interpret and act upon natural language commands which are given by people and which refer to the robot's physical surroundings. Such interpretation is known variously as the symbol grounding problem, grounded semantics and grounded language acquisition. This problem is challenging because people employ diverse vocabulary and grammar, and because robots have substantial uncertainty about the nature and contents of their surroundings, making it difficult to associate the constitutive language elements (principally noun phrases and spatial relations) of the command text to elements of those surroundings. Symbolic models capture linguistic structure but have not scaled successfully to handle the diverse language produced by untrained users. Existing statistical approaches can better handle diversity, but have not to date modeled complex linguistic structure, limiting achievable accuracy. Recent hybrid approaches have addressed limitations in scaling and complexity, but have not effectively associated linguistic and perceptual features. Our framework, called Generalized Grounding Graphs (G^3), addresses these issues by defining a probabilistic graphical model dynamically according to the linguistic parse structure of a natural language command. This approach scales effectively, handles linguistic diversity, and enables the system to associate parts of a command with the specific objects, places, and events in the external world to which they refer. We show that robots can learn word meanings and use those learned meanings to robustly follow natural language commands produced by untrained users. We demonstrate our approach for both mobility commands and mobile manipulation commands involving a variety of semi-autonomous robotic platforms, including a wheelchair, a micro-air vehicle, a forklift, and the Willow Garage PR2.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01213",
        "title": "An Encoder-Decoder Model for ICD-10 Coding of Death Certificates",
        "authors": [
            "Elena Tutubalina",
            "Zulfat Miftahutdinov"
        ],
        "abstract": "Information extraction from textual documents such as hospital records and healthrelated user discussions has become a topic of intense interest. The task of medical concept coding is to map a variable length text to medical concepts and corresponding classification codes in some external system or ontology. In this work, we utilize recurrent neural networks to automatically assign ICD-10 codes to fragments of death certificates written in English. We develop end-to-end neural architectures directly tailored to the task, including basic encoder-decoder architecture for statistical translation. In order to incorporate prior knowledge, we concatenate cosine similarities vector among the text and dictionary entry to the encoded state. Being applied to a standard benchmark from CLEF eHealth 2017 challenge, our model achieved F-measure of 85.01% on a full test set with significant improvement as compared to the average score of 62.2% for all official participants approaches.\n    ",
        "submission_date": "2017-12-04T00:00:00",
        "last_modified_date": "2017-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01411",
        "title": "#anorexia, #anarexia, #anarexyia: Characterizing Online Community Practices with Orthographic Variation",
        "authors": [
            "Ian Stewart",
            "Stevie Chancellor",
            "Munmun De Choudhury",
            "Jacob Eisenstein"
        ],
        "abstract": "Distinctive linguistic practices help communities build solidarity and differentiate themselves from outsiders. In an online community, one such practice is variation in orthography, which includes spelling, punctuation, and capitalization. Using a dataset of over two million Instagram posts, we investigate orthographic variation in a community that shares pro-eating disorder (pro-ED) content. We find that not only does orthographic variation grow more frequent over time, it also becomes more profound or deep, with variants becoming increasingly distant from the original: as, for example, #anarexyia is more distant than #anarexia from the original spelling #anorexia. These changes are driven by newcomers, who adopt the most extreme linguistic practices as they enter the community. Moreover, this behavior correlates with engagement: the newcomers who adopt deeper orthographic variants tend to remain active for longer in the community, and the posts that contain deeper variation receive more positive feedback in the form of \"likes.\" Previous work has linked community membership change with language change, and our work casts this connection in a new light, with newcomers driving an evolving practice, rather than adapting to it. We also demonstrate the utility of orthographic variation as a new lens to study sociolinguistic change in online communities, particularly when the change results from an exogenous force such as a content ban.\n    ",
        "submission_date": "2017-12-04T00:00:00",
        "last_modified_date": "2017-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01460",
        "title": "AWE-CM Vectors: Augmenting Word Embeddings with a Clinical Metathesaurus",
        "authors": [
            "Willie Boag",
            "Hassan Kan\u00e9"
        ],
        "abstract": "In recent years, word embeddings have been surprisingly effective at capturing intuitive characteristics of the words they represent. These vectors achieve the best results when training corpora are extremely large, sometimes billions of words. Clinical natural language processing datasets, however, tend to be much smaller. Even the largest publicly-available dataset of medical notes is three orders of magnitude smaller than the dataset of the oft-used \"Google News\" word vectors. In order to make up for limited training data sizes, we encode expert domain knowledge into our embeddings. Building on a previous extension of word2vec, we show that generalizing the notion of a word's \"context\" to include arbitrary features creates an avenue for encoding domain knowledge into word embeddings. We show that the word vectors produced by this method outperform their text-only counterparts across the board in correlation with clinical experts.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01476",
        "title": "Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing",
        "authors": [
            "J\u00falio Hoffimann",
            "Youli Mao",
            "Avinash Wesley",
            "Aimee Taylor"
        ],
        "abstract": "Drilling activities in the oil and gas industry have been reported over decades for thousands of wells on a daily basis, yet the analysis of this text at large-scale for information retrieval, sequence mining, and pattern analysis is very challenging. Drilling reports contain interpretations written by drillers from noting measurements in downhole sensors and surface equipment, and can be used for operation optimization and accident mitigation. In this initial work, a methodology is proposed for automatic classification of sentences written in drilling reports into three relevant labels (EVENT, SYMPTOM and ACTION) for hundreds of wells in an actual field. Some of the main challenges in the text corpus were overcome, which include the high frequency of technical symbols, mistyping/abbreviation of technical terms, and the presence of incomplete sentences in the drilling reports. We obtain state-of-the-art classification accuracy within this technical language and illustrate advanced queries enabled by the tool.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01562",
        "title": "EmTaggeR: A Word Embedding Based Novel Method for Hashtag Recommendation on Twitter",
        "authors": [
            "Kuntal Dey",
            "Ritvik Shrivastava",
            "Saroj Kaushik",
            "L. Venkata Subramaniam"
        ],
        "abstract": "The hashtag recommendation problem addresses recommending (suggesting) one or more hashtags to explicitly tag a post made on a given social network platform, based upon the content and context of the post. In this work, we propose a novel methodology for hashtag recommendation for microblog posts, specifically Twitter. The methodology, EmTaggeR, is built upon a training-testing framework that builds on the top of the concept of word embedding. The training phase comprises of learning word vectors associated with each hashtag, and deriving a word embedding for each hashtag. We provide two training procedures, one in which each hashtag is trained with a separate word embedding model applicable in the context of that hashtag, and another in which each hashtag obtains its embedding from a global context. The testing phase constitutes computing the average word embedding of the test post, and finding the similarity of this embedding with the known embeddings of the hashtags. The tweets that contain the most-similar hashtag are extracted, and all the hashtags that appear in these tweets are ranked in terms of embedding similarity scores. The top-K hashtags that appear in this ranked list, are recommended for the given test post. Our system produces F1 score of 50.83%, improving over the LDA baseline by around 6.53 times, outperforming the best-performing system known in the literature that provides a lift of 6.42 times. EmTaggeR is a fast, scalable and lightweight system, which makes it practical to deploy in real-life applications.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01586",
        "title": "Deep Semantic Role Labeling with Self-Attention",
        "authors": [
            "Zhixing Tan",
            "Mingxuan Wang",
            "Jun Xie",
            "Yidong Chen",
            "Xiaodong Shi"
        ],
        "abstract": "Semantic Role Labeling (SRL) is believed to be a crucial step towards natural language understanding and has been widely studied. Recent years, end-to-end SRL with recurrent neural networks (RNN) has gained increasing attention. However, it remains a major challenge for RNNs to handle structural information and long range dependencies. In this paper, we present a simple and effective architecture for SRL which aims to address these problems. Our model is based on self-attention which can directly capture the relationships between two tokens regardless of their distance. Our single model achieves F$_1=83.4$ on the CoNLL-2005 shared task dataset and F$_1=82.7$ on the CoNLL-2012 shared task dataset, which outperforms the previous state-of-the-art results by $1.8$ and $1.0$ F$_1$ score respectively. Besides, our model is computationally efficient, and the parsing speed is 50K tokens per second on a single Titan X GPU.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01719",
        "title": "Phylogenetics of Indo-European Language families via an Algebro-Geometric Analysis of their Syntactic Structures",
        "authors": [
            "Kevin Shu",
            "Andrew Ortegaray",
            "Robert Berwick",
            "Matilde Marcolli"
        ],
        "abstract": "Using Phylogenetic Algebraic Geometry, we analyze computationally the phylogenetic tree of subfamilies of the Indo-European language family, using data of syntactic structures. The two main sources of syntactic data are the SSWL database and Longobardi's recent data of syntactic parameters. We compute phylogenetic invariants and likelihood functions for two sets of Germanic languages, a set of Romance languages, a set of Slavic languages and a set of early Indo-European languages, and we compare the results with what is known through historical linguistics.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2019-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01741",
        "title": "Capturing Reliable Fine-Grained Sentiment Associations by Crowdsourcing and Best-Worst Scaling",
        "authors": [
            "Svetlana Kiritchenko",
            "Saif M. Mohammad"
        ],
        "abstract": "Access to word-sentiment associations is useful for many applications, including sentiment analysis, stance detection, and linguistic analysis. However, manually assigning fine-grained sentiment association scores to words has many challenges with respect to keeping annotations consistent. We apply the annotation technique of Best-Worst Scaling to obtain real-valued sentiment association scores for words and phrases in three different domains: general English, English Twitter, and Arabic Twitter. We show that on all three domains the ranking of words by sentiment remains remarkably consistent even when the annotation process is repeated with a different set of annotators. We also, for the first time, determine the minimum difference in sentiment association that is perceptible to native speakers of a language.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01765",
        "title": "Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation",
        "authors": [
            "Svetlana Kiritchenko",
            "Saif M. Mohammad"
        ],
        "abstract": "Rating scales are a widely used method for data annotation; however, they present several challenges, such as difficulty in maintaining inter- and intra-annotator consistency. Best-worst scaling (BWS) is an alternative method of annotation that is claimed to produce high-quality annotations while keeping the required number of annotations similar to that of rating scales. However, the veracity of this claim has never been systematically established. Here for the first time, we set up an experiment that directly compares the rating scale method with BWS. We show that with the same total number of annotations, BWS produces significantly more reliable results than the rating scale.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01769",
        "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models",
        "authors": [
            "Chung-Cheng Chiu",
            "Tara N. Sainath",
            "Yonghui Wu",
            "Rohit Prabhavalkar",
            "Patrick Nguyen",
            "Zhifeng Chen",
            "Anjuli Kannan",
            "Ron J. Weiss",
            "Kanishka Rao",
            "Ekaterina Gonina",
            "Navdeep Jaitly",
            "Bo Li",
            "Jan Chorowski",
            "Michiel Bacchiani"
        ],
        "abstract": "Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2% to 5.6%, while the best conventional system achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to 5% for the conventional system.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2018-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01794",
        "title": "The Effect of Negators, Modals, and Degree Adverbs on Sentiment Composition",
        "authors": [
            "Svetlana Kiritchenko",
            "Saif M. Mohammad"
        ],
        "abstract": "Negators, modals, and degree adverbs can significantly affect the sentiment of the words they modify. Often, their impact is modeled with simple heuristics; although, recent work has shown that such heuristics do not capture the true sentiment of multi-word phrases. We created a dataset of phrases that include various negators, modals, and degree adverbs, as well as their combinations. Both the phrases and their constituent content words were annotated with real-valued scores of sentiment association. Using phrasal terms in the created dataset, we analyze the impact of individual modifiers and the average effect of the groups of modifiers on overall sentiment. We find that the effect of modifiers varies substantially among the members of the same group. Furthermore, each individual modifier can affect sentiment words in different ways. Therefore, solutions based on statistical learning seem more promising than fixed hand-crafted rules on the task of automatic sentiment prediction.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01797",
        "title": "One for All: Towards Language Independent Named Entity Linking",
        "authors": [
            "Avirup Sil",
            "Radu Florian"
        ],
        "abstract": "Entity linking (EL) is the task of disambiguating mentions in text by associating them with entries in a predefined database of mentions (persons, organizations, etc). Most previous EL research has focused mainly on one language, English, with less attention being paid to other languages, such as Spanish or Chinese. In this paper, we introduce LIEL, a Language Independent Entity Linking system, which provides an EL framework which, once trained on one language, works remarkably well on a number of different languages without change. LIEL makes a joint global prediction over the entire document, employing a discriminative reranking framework with many domain and language-independent feature functions. Experiments on numerous benchmark datasets, show that the proposed system, once trained on one language, English, outperforms several state-of-the-art systems in English (by 4 points) and the trained model also works very well on Spanish (14 points better than a competitor system), demonstrating the viability of the approach.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01807",
        "title": "Improving the Performance of Online Neural Transducer Models",
        "authors": [
            "Tara N. Sainath",
            "Chung-Cheng Chiu",
            "Rohit Prabhavalkar",
            "Anjuli Kannan",
            "Yonghui Wu",
            "Patrick Nguyen",
            "Zhifeng Chen"
        ],
        "abstract": "Having a sequence-to-sequence model which can operate in an online fashion is important for streaming applications such as Voice Search. Neural transducer is a streaming sequence-to-sequence model, but has shown a significant degradation in performance compared to non-streaming models such as Listen, Attend and Spell (LAS). In this paper, we present various improvements to NT. Specifically, we look at increasing the window over which NT computes attention, mainly by looking backwards in time so the model still remains online. In addition, we explore initializing a NT model from a LAS-trained model so that it is guided with a better alignment. Finally, we explore including stronger language models such as using wordpiece models, and applying an external LM during the beam search. On a Voice Search task, we find with these improvements we can get NT to match the performance of LAS.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01813",
        "title": "Neural Cross-Lingual Entity Linking",
        "authors": [
            "Avirup Sil",
            "Gourab Kundu",
            "Radu Florian",
            "Wael Hamza"
        ],
        "abstract": "A major challenge in Entity Linking (EL) is making effective use of contextual information to disambiguate mentions to Wikipedia that might refer to different entities in different contexts. The problem exacerbates with cross-lingual EL which involves linking mentions written in non-English documents to entries in the English Wikipedia: to compare textual clues across languages we need to compute similarity between textual fragments across languages. In this paper, we propose a neural EL model that trains fine-grained similarities and dissimilarities between the query and candidate document from multiple perspectives, combined with convolution and tensor networks. Further, we show that this English-trained system can be applied, in zero-shot learning, to other languages by making surprisingly effective use of multi-lingual embeddings. The proposed system has strong empirical evidence yielding state-of-the-art results in English as well as cross-lingual: Spanish and Chinese TAC 2015 datasets.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01818",
        "title": "Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models",
        "authors": [
            "Rohit Prabhavalkar",
            "Tara N. Sainath",
            "Yonghui Wu",
            "Patrick Nguyen",
            "Zhifeng Chen",
            "Chung-Cheng Chiu",
            "Anjuli Kannan"
        ],
        "abstract": "Sequence-to-sequence models, such as attention-based models in automatic speech recognition (ASR), are typically trained to optimize the cross-entropy criterion which corresponds to improving the log-likelihood of the data. However, system performance is usually measured in terms of word error rate (WER), not log-likelihood. Traditional ASR systems benefit from discriminative sequence training which optimizes criteria such as the state-level minimum Bayes risk (sMBR) which are more closely related to WER. In the present work, we explore techniques to train attention-based models to directly minimize expected word error rate. We consider two loss functions which approximate the expected number of word errors: either by sampling from the model, or by using N-best lists of decoded hypotheses, which we find to be more effective than the sampling-based method. In experimental evaluations, we find that the proposed training procedure improves performance by up to 8.2% relative to the baseline system. This allows us to train grapheme-based, uni-directional attention-based models which match the performance of a traditional, state-of-the-art, discriminative sequence-trained system on a mobile voice-search task.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01821",
        "title": "Neural Machine Translation by Generating Multiple Linguistic Factors",
        "authors": [
            "Mercedes Garc\u00eda-Mart\u00ednez",
            "Lo\u00efc Barrault",
            "Fethi Bougares"
        ],
        "abstract": "Factored neural machine translation (FNMT) is founded on the idea of using the morphological and grammatical decomposition of the words (factors) at the output side of the neural network. This architecture addresses two well-known problems occurring in MT, namely the size of target language vocabulary and the number of unknown tokens produced in the translation. FNMT system is designed to manage larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). Moreover, we can produce grammatically correct words that are not part of the vocabulary. FNMT model is evaluated on IWSLT'15 English to French task and compared to the baseline word-based and BPE-based NMT systems. Promising qualitative and quantitative results (in terms of BLEU and METEOR) are reported.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01864",
        "title": "No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models",
        "authors": [
            "Tara N. Sainath",
            "Rohit Prabhavalkar",
            "Shankar Kumar",
            "Seungji Lee",
            "Anjuli Kannan",
            "David Rybach",
            "Vlad Schogol",
            "Patrick Nguyen",
            "Bo Li",
            "Yonghui Wu",
            "Zhifeng Chen",
            "Chung-Cheng Chiu"
        ],
        "abstract": "For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units.\n",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01969",
        "title": "Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks",
        "authors": [
            "Salman Mohammed",
            "Peng Shi",
            "Jimmy Lin"
        ],
        "abstract": "We examine the problem of question answering over knowledge graphs, focusing on simple questions that can be answered by the lookup of a single fact. Adopting a straightforward decomposition of the problem into entity detection, entity linking, relation prediction, and evidence combination, we explore simple yet strong baselines. On the popular SimpleQuestions dataset, we find that basic LSTMs and GRUs plus a few heuristics yield accuracies that approach the state of the art, and techniques that do not use neural networks also perform reasonably well. These results show that gains from sophisticated deep learning techniques proposed in the literature are quite modest and that some previous models exhibit unnecessary complexity.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2018-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02016",
        "title": "Dual Attention Network for Product Compatibility and Function Satisfiability Analysis",
        "authors": [
            "Hu Xu",
            "Sihong Xie",
            "Lei Shu",
            "Philip S. Yu"
        ],
        "abstract": "Product compatibility and their functionality are of utmost importance to customers when they purchase products, and to sellers and manufacturers when they sell products. Due to the huge number of products available online, it is infeasible to enumerate and test the compatibility and functionality of every product. In this paper, we address two closely related problems: product compatibility analysis and function satisfiability analysis, where the second problem is a generalization of the first problem (e.g., whether a product works with another product can be considered as a special function). We first identify a novel question and answering corpus that is up-to-date regarding product compatibility and functionality information. To allow automatic discovery product compatibility and functionality, we then propose a deep learning model called Dual Attention Network (DAN). Given a QA pair for a to-be-purchased product, DAN learns to 1) discover complementary products (or functions), and 2) accurately predict the actual compatibility (or satisfiability) of the discovered products (or functions). The challenges addressed by the model include the briefness of QAs, linguistic patterns indicating compatibility, and the appropriate fusion of questions and answers. We conduct experiments to quantitatively and qualitatively show that the identified products and functions have both high coverage and accuracy, compared with a wide spectrum of baselines.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02047",
        "title": "Distance-based Self-Attention Network for Natural Language Inference",
        "authors": [
            "Jinbae Im",
            "Sungzoon Cho"
        ],
        "abstract": "Attention mechanism has been used as an ancillary means to help RNN or CNN. However, the Transformer (Vaswani et al., 2017) recently recorded the state-of-the-art performance in machine translation with a dramatic reduction in training time by solely using attention. Motivated by the Transformer, Directional Self Attention Network (Shen et al., 2017), a fully attention-based sentence encoder, was proposed. It showed good performance with various data by using forward and backward directional information in a sentence. But in their study, not considered at all was the distance between words, an important feature when learning the local dependency to help understand the context of input text. We propose Distance-based Self-Attention Network, which considers the word distance by using a simple distance mask in order to model the local dependency without losing the ability of modeling global dependency which attention has inherent. Our model shows good performance with NLI data, and it records the new state-of-the-art result with SNLI data. Additionally, we show that our model has a strength in long sentences or documents.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02109",
        "title": "Multi-channel Encoder for Neural Machine Translation",
        "authors": [
            "Hao Xiong",
            "Zhongjun He",
            "Xiaoguang Hu",
            "Hua Wu"
        ],
        "abstract": "Attention-based Encoder-Decoder has the effective architecture for neural machine translation (NMT), which typically relies on recurrent neural networks (RNN) to build the blocks that will be lately called by attentive reader during the decoding process. This design of encoder yields relatively uniform composition on source sentence, despite the gating mechanism employed in encoding RNN. On the other hand, we often hope the decoder to take pieces of source sentence at varying levels suiting its own linguistic structure: for example, we may want to take the entity name in its raw form while taking an idiom as a perfectly composed unit. Motivated by this demand, we propose Multi-channel Encoder (MCE), which enhances encoding components with different levels of composition. More specifically, in addition to the hidden state of encoding RNN, MCE takes 1) the original word embedding for raw encoding with no composition, and 2) a particular design of external memory in Neural Turing Machine (NTM) for more complex composition, while all three encoding strategies are properly blended during decoding. Empirical study on Chinese-English translation shows that our model can improve by 6.52 BLEU points upon a strong open source NMT system: DL4MT1. On the WMT14 English- French task, our single shallow system achieves BLEU=38.8, comparable with the state-of-the-art deep models.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02121",
        "title": "A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network",
        "authors": [
            "Dai Quoc Nguyen",
            "Tu Dinh Nguyen",
            "Dat Quoc Nguyen",
            "Dinh Phung"
        ],
        "abstract": "In this paper, we propose a novel embedding model, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a convolutional neural network, so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3-column matrix where each column vector represents a triple element. This 3-column matrix is then fed to a convolution layer where multiple filters are operated on the matrix to generate different feature maps. These feature maps are then concatenated into a single feature vector representing the input triple. The feature vector is multiplied with a weight vector via a dot product to return a score. This score is then used to predict whether the triple is valid or not. Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2018-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02186",
        "title": "Product Function Need Recognition via Semi-supervised Attention Network",
        "authors": [
            "Hu Xu",
            "Sihong Xie",
            "Lei Shu",
            "Philip S. Yu"
        ],
        "abstract": "Functionality is of utmost importance to customers when they purchase products. However, it is unclear to customers whether a product can really satisfy their needs on functions. Further, missing functions may be intentionally hidden by the manufacturers or the sellers. As a result, a customer needs to spend a fair amount of time before purchasing or just purchase the product on his/her own risk. In this paper, we first identify a novel QA corpus that is dense on product functionality information \\footnote{The annotated corpus can be found at \\url{",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02223",
        "title": "Discourse-Aware Rumour Stance Classification in Social Media Using Sequential Classifiers",
        "authors": [
            "Arkaitz Zubiaga",
            "Elena Kochkina",
            "Maria Liakata",
            "Rob Procter",
            "Michal Lukasik",
            "Kalina Bontcheva",
            "Trevor Cohn",
            "Isabelle Augenstein"
        ],
        "abstract": "Rumour stance classification, defined as classifying the stance of specific social media posts into one of supporting, denying, querying or commenting on an earlier post, is becoming of increasing interest to researchers. While most previous work has focused on using individual tweets as classifier inputs, here we report on the performance of sequential classifiers that exploit the discourse features inherent in social media interactions or 'conversational threads'. Testing the effectiveness of four sequential classifiers -- Hawkes Processes, Linear-Chain Conditional Random Fields (Linear CRF), Tree-Structured Conditional Random Fields (Tree CRF) and Long Short Term Memory networks (LSTM) -- on eight datasets associated with breaking news stories, and looking at different types of local and contextual features, our work sheds new light on the development of accurate stance classifiers. We show that sequential classifiers that exploit the use of discourse properties in social media conversations while using only local features, outperform non-sequential classifiers. Furthermore, we show that LSTM using a reduced set of features can outperform the other sequential classifiers; this performance is consistent across datasets and across types of stances. To conclude, our work also analyses the different features under study, identifying those that best help characterise and distinguish between stances, such as supporting tweets being more likely to be accompanied by evidence than denying tweets. We also set forth a number of directions for future research.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02250",
        "title": "Why Do Neural Dialog Systems Generate Short and Meaningless Replies? A Comparison between Dialog and Translation",
        "authors": [
            "Bolin Wei",
            "Shuai Lu",
            "Lili Mou",
            "Hao Zhou",
            "Pascal Poupart",
            "Ge Li",
            "Zhi Jin"
        ],
        "abstract": "This paper addresses the question: Why do neural dialog systems generate short and meaningless replies? We conjecture that, in a dialog system, an utterance may have multiple equally plausible replies, causing the deficiency of neural networks in the dialog application. We propose a systematic way to mimic the dialog scenario in a machine translation system, and manage to reproduce the phenomenon of generating short and less meaningful sentences in the translation setting, showing evidence of our conjecture.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02480",
        "title": "A Corpus of Deep Argumentative Structures as an Explanation to Argumentative Relations",
        "authors": [
            "Paul Reisert",
            "Naoya Inoue",
            "Naoaki Okazaki",
            "Kentaro Inui"
        ],
        "abstract": "In this paper, we compose a new task for deep argumentative structure analysis that goes beyond shallow discourse structure analysis. The idea is that argumentative relations can reasonably be represented with a small set of predefined patterns. For example, using value judgment and bipolar causality, we can explain a support relation between two argumentative segments as follows: Segment 1 states that something is good, and Segment 2 states that it is good because it promotes something good when it happens. We are motivated by the following questions: (i) how do we formulate the task?, (ii) can a reasonable pattern set be created?, and (iii) do the patterns work? To examine the task feasibility, we conduct a three-stage, detailed annotation study using 357 argumentative relations from the argumentative microtext corpus, a small, but highly reliable corpus. We report the coverage of explanations captured by our patterns on a test set composed of 270 relations. Our coverage result of 74.6% indicates that argumentative relations can reasonably be explained by our small pattern set. Our agreement result of 85.9% shows that a reasonable inter-annotator agreement can be achieved. To assist with future work in computational argumentation, the annotated corpus is made publicly available.\n    ",
        "submission_date": "2017-12-07T00:00:00",
        "last_modified_date": "2017-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02555",
        "title": "Hungarian Layer: Logics Empowered Neural Architecture",
        "authors": [
            "Han Xiao",
            "Yidong Chen",
            "Xiaodong Shi"
        ],
        "abstract": "Neural architecture is a purely numeric framework, which fits the data as a continuous function. However, lacking of logic flow (e.g. \\textit{if, for, while}), traditional algorithms (e.g. \\textit{Hungarian algorithm, A$^*$ searching, decision tress algorithm}) could not be embedded into this paradigm, which limits the theories and applications. In this paper, we reform the calculus graph as a dynamic process, which is guided by logic flow. Within our novel methodology, traditional algorithms could empower numerical neural network. Specifically, regarding the subject of sentence matching, we reformulate this issue as the form of task-assignment, which is solved by Hungarian algorithm. First, our model applies BiLSTM to parse the sentences. Then Hungarian layer aligns the matching positions. Last, we transform the matching results for soft-max regression by another BiLSTM. Extensive experiments show that our model outperforms other state-of-the-art baselines substantially.\n    ",
        "submission_date": "2017-12-07T00:00:00",
        "last_modified_date": "2018-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02767",
        "title": "Topics and Label Propagation: Best of Both Worlds for Weakly Supervised Text Classification",
        "authors": [
            "Sachin Pawar",
            "Nitin Ramrakhiyani",
            "Swapnil Hingmire",
            "Girish K. Palshikar"
        ],
        "abstract": "We propose a Label Propagation based algorithm for weakly supervised text classification. We construct a graph where each document is represented by a node and edge weights represent similarities among the documents. Additionally, we discover underlying topics using Latent Dirichlet Allocation (LDA) and enrich the document graph by including the topics in the form of additional nodes. The edge weights between a topic and a text document represent level of \"affinity\" between them. Our approach does not require document level labelling, instead it expects manual labels only for topic nodes. This significantly minimizes the level of supervision needed as only a few topics are observed to be enough for achieving sufficiently high accuracy. The Label Propagation Algorithm is employed on this enriched graph to propagate labels among the nodes. Our approach combines the advantages of Label Propagation (through document-document similarities) and Topic Modelling (for minimal but smart supervision). We demonstrate the effectiveness of our approach on various datasets and compare with state-of-the-art weakly supervised text classification approaches.\n    ",
        "submission_date": "2017-12-04T00:00:00",
        "last_modified_date": "2017-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02768",
        "title": "Convolutional Neural Networks for Medical Diagnosis from Admission Notes",
        "authors": [
            "Christy Li",
            "Dimitris Konomis",
            "Graham Neubig",
            "Pengtao Xie",
            "Carol Cheng",
            "Eric Xing"
        ],
        "abstract": "$\\textbf{Objective}$ Develop an automatic diagnostic system which only uses textual admission information from Electronic Health Records (EHRs) and assist clinicians with a timely and statistically proved decision tool. The hope is that the tool can be used to reduce mis-diagnosis.\n",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02856",
        "title": "Effective Neural Solution for Multi-Criteria Word Segmentation",
        "authors": [
            "Han He",
            "Lei Wu",
            "Hua Yan",
            "Zhimin Gao",
            "Yi Feng",
            "George Townsend"
        ],
        "abstract": "We present a simple yet elegant solution to train a single joint model on multi-criteria corpora for Chinese Word Segmentation (CWS). Our novel design requires no private layers in model architecture, instead, introduces two artificial tokens at the beginning and ending of input sentence to specify the required target criteria. The rest of the model including Long Short-Term Memory (LSTM) layer and Conditional Random Fields (CRFs) layer remains unchanged and is shared across all datasets, keeping the size of parameter collection minimal and constant. On Bakeoff 2005 and Bakeoff 2008 datasets, our innovative design has surpassed both single-criterion and multi-criteria state-of-the-art learning results. To the best knowledge, our design is the first one that has achieved the latest high performance on such large scale datasets. Source codes and corpora of this paper are available on GitHub.\n    ",
        "submission_date": "2017-12-07T00:00:00",
        "last_modified_date": "2018-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02959",
        "title": "Sequence to Sequence Networks for Roman-Urdu to Urdu Transliteration",
        "authors": [
            "Mehreen Alam",
            "Sibt ul Hussain"
        ],
        "abstract": "Neural Machine Translation models have replaced the conventional phrase based statistical translation methods since the former takes a generic, scalable, data-driven approach rather than relying on manual, hand-crafted features. The neural machine translation system is based on one neural network that is composed of two parts, one that is responsible for input language sentence and other part that handles the desired output language sentence. This model based on encoder-decoder architecture also takes as input the distributed representations of the source language which enriches the learnt dependencies and gives a warm start to the network. In this work, we transform Roman-Urdu to Urdu transliteration into sequence to sequence learning problem. To this end, we make the following contributions. We create the first ever parallel corpora of Roman-Urdu to Urdu, create the first ever distributed representation of Roman-Urdu and present the first neural machine translation model that transliterates text from Roman-Urdu to Urdu language. Our model has achieved the state-of-the-art results using BLEU as the evaluation metric. Precisely, our model is able to correctly predict sentences up to length 10 while achieving BLEU score of 48.6 on the test set. We are hopeful that our model and our results shall serve as the baseline for further work in the domain of neural machine translation for Roman-Urdu to Urdu using distributed representation.\n    ",
        "submission_date": "2017-12-08T00:00:00",
        "last_modified_date": "2017-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03133",
        "title": "Building competitive direct acoustics-to-word models for English conversational speech recognition",
        "authors": [
            "Kartik Audhkhasi",
            "Brian Kingsbury",
            "Bhuvana Ramabhadran",
            "George Saon",
            "Michael Picheny"
        ],
        "abstract": "Direct acoustics-to-word (A2W) models in the end-to-end paradigm have received increasing attention compared to conventional sub-word based automatic speech recognition models using phones, characters, or context-dependent hidden Markov model states. This is because A2W models recognize words from speech without any decoder, pronunciation lexicon, or externally-trained language model, making training and decoding with such models simple. Prior work has shown that A2W models require orders of magnitude more training data in order to perform comparably to conventional models. Our work also showed this accuracy gap when using the English Switchboard-Fisher data set. This paper describes a recipe to train an A2W model that closes this gap and is at-par with state-of-the-art sub-word based models. We achieve a word error rate of 8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder or language model. We find that model initialization, training data order, and regularization have the most impact on the A2W model performance. Next, we present a joint word-character A2W model that learns to first spell the word and then recognize it. This model provides a rich output to the user instead of simple word hypotheses, making it especially useful in the case of words unseen or rarely-seen during training.\n    ",
        "submission_date": "2017-12-08T00:00:00",
        "last_modified_date": "2017-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03199",
        "title": "Characterizing the hyper-parameter space of LSTM language models for mixed context applications",
        "authors": [
            "Victor Akinwande",
            "Sekou L. Remy"
        ],
        "abstract": "Applying state of the art deep learning models to novel real world datasets gives a practical evaluation of the generalizability of these models. Of importance in this process is how sensitive the hyper parameters of such models are to novel datasets as this would affect the reproducibility of a model. We present work to characterize the hyper parameter space of an LSTM for language modeling on a code-mixed corpus. We observe that the evaluated model shows minimal sensitivity to our novel dataset bar a few hyper parameters.\n    ",
        "submission_date": "2017-12-08T00:00:00",
        "last_modified_date": "2017-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03376",
        "title": "Word Sense Disambiguation with LSTM: Do We Really Need 100 Billion Words?",
        "authors": [
            "Minh Le",
            "Marten Postma",
            "Jacopo Urbani"
        ],
        "abstract": "Recently, Yuan et al. (2016) have shown the effectiveness of using Long Short-Term Memory (LSTM) for performing Word Sense Disambiguation (WSD). Their proposed technique outperformed the previous state-of-the-art with several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study of this technique using only openly available datasets (GigaWord, SemCore, OMSTI) and software (TensorFlow). From them, it emerged that state-of-the-art results can be obtained with much less data than hinted by Yuan et al. All code and trained models are made freely available.\n    ",
        "submission_date": "2017-12-09T00:00:00",
        "last_modified_date": "2017-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03430",
        "title": "Aspect Extraction and Sentiment Classification of Mobile Apps using App-Store Reviews",
        "authors": [
            "Sharmistha Dey"
        ],
        "abstract": "Understanding of customer sentiment can be useful for product development. On top of that if the priorities for the development order can be known, then development procedure become simpler. This work has tried to address this issue in the mobile app domain. Along with aspect and opinion extraction this work has also categorized the extracted aspects ac-cording to their importance. This can help developers to focus their time and energy at the right place.\n    ",
        "submission_date": "2017-12-09T00:00:00",
        "last_modified_date": "2017-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03449",
        "title": "Modulating and attending the source image during encoding improves Multimodal Translation",
        "authors": [
            "Jean-Benoit Delbrouck",
            "St\u00e9phane Dupont"
        ],
        "abstract": "We propose a new and fully end-to-end approach for multimodal translation where the source text encoder modulates the entire visual input processing using conditional batch normalization, in order to compute the most informative image features for our task. Additionally, we propose a new attention mechanism derived from this original idea, where the attention model for the visual input is conditioned on the source text encoder representations. In the paper, we detail our models as well as the image analysis pipeline. Finally, we report experimental results. They are, as far as we know, the new state of the art on three different test sets.\n    ",
        "submission_date": "2017-12-09T00:00:00",
        "last_modified_date": "2017-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03463",
        "title": "Learning Interpretable Spatial Operations in a Rich 3D Blocks World",
        "authors": [
            "Yonatan Bisk",
            "Kevin J. Shih",
            "Yejin Choi",
            "Daniel Marcu"
        ],
        "abstract": "In this paper, we study the problem of mapping natural language instructions to complex spatial actions in a 3D blocks world. We first introduce a new dataset that pairs complex 3D spatial operations to rich natural language descriptions that require complex spatial and pragmatic interpretations such as \"mirroring\", \"twisting\", and \"balancing\". This dataset, built on the simulation environment of Bisk, Yuret, and Marcu (2016), attains language that is significantly richer and more complex, while also doubling the size of the original dataset in the 2D environment with 100 new world configurations and 250,000 tokens. In addition, we propose a new neural architecture that achieves competitive results while automatically discovering an inventory of interpretable spatial operations (Figure 5)\n    ",
        "submission_date": "2017-12-10T00:00:00",
        "last_modified_date": "2017-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03538",
        "title": "Multi-Task Learning for Mental Health using Social Media Text",
        "authors": [
            "Adrian Benton",
            "Margaret Mitchell",
            "Dirk Hovy"
        ],
        "abstract": "We introduce initial groundwork for estimating suicide risk and mental health in a deep learning framework. By modeling multiple conditions, the system learns to make predictions about suicide risk and mental health at a low false positive rate. Conditions are modeled as tasks in a multi-task learning (MTL) framework, with gender prediction as an additional auxiliary task. We demonstrate the effectiveness of multi-task learning by comparison to a well-tuned single-task baseline with the same number of parameters. Our best MTL model predicts potential suicide attempt, as well as the presence of atypical mental health, with AUC > 0.8. We also find additional large improvements using multi-task learning on mental health tasks with limited training data.\n    ",
        "submission_date": "2017-12-10T00:00:00",
        "last_modified_date": "2017-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03547",
        "title": "Inducing Interpretability in Knowledge Graph Embeddings",
        "authors": [
            "Chandrahas",
            "Tathagata Sengupta",
            "Cibi Pragadeesh",
            "Partha Pratim Talukdar"
        ],
        "abstract": "We study the problem of inducing interpretability in KG embeddings. Specifically, we explore the Universal Schema (Riedel et al., 2013) and propose a method to induce interpretability. There have been many vector space models proposed for the problem, however, most of these methods don't address the interpretability (semantics) of individual dimensions. In this work, we study this problem and propose a method for inducing interpretability in KG embeddings using entity co-occurrence statistics. The proposed method significantly improves the interpretability, while maintaining comparable performance in other KG tasks.\n    ",
        "submission_date": "2017-12-10T00:00:00",
        "last_modified_date": "2017-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03556",
        "title": "Stochastic Answer Networks for Machine Reading Comprehension",
        "authors": [
            "Xiaodong Liu",
            "Yelong Shen",
            "Kevin Duh",
            "Jianfeng Gao"
        ],
        "abstract": "We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the neural network during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).\n    ",
        "submission_date": "2017-12-10T00:00:00",
        "last_modified_date": "2018-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03609",
        "title": "Contextualized Word Representations for Reading Comprehension",
        "authors": [
            "Shimi Salant",
            "Jonathan Berant"
        ],
        "abstract": "Reading a document and extracting an answer to a question about its content has attracted substantial attention recently. While most work has focused on the interaction between the question and the document, in this work we evaluate the importance of context when the question and document are processed independently. We take a standard neural architecture for this task, and show that by providing rich contextualized word representations from a large pre-trained language model as well as allowing the model to choose between context-dependent and context-independent word representations, we can obtain dramatic improvements and reach performance comparable to state-of-the-art on the competitive SQuAD dataset.\n    ",
        "submission_date": "2017-12-10T00:00:00",
        "last_modified_date": "2018-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03645",
        "title": "Long-Range Correlation Underlying Childhood Language and Generative Models",
        "authors": [
            "Kumiko Tanaka-Ishii"
        ],
        "abstract": "Long-range correlation, a property of time series exhibiting long-term memory, is mainly studied in the statistical physics domain and has been reported to exist in natural language. Using a state-of-the-art method for such analysis, long-range correlation is first shown to occur in long CHILDES data sets. To understand why, Bayesian generative models of language, originally proposed in the cognitive scientific domain, are investigated. Among representative models, the Simon model was found to exhibit surprisingly good long-range correlation, but not the Pitman-Yor model. Since the Simon model is known not to correctly reflect the vocabulary growth of natural language, a simple new model is devised as a conjunct of the Simon and Pitman-Yor models, such that long-range correlation holds with a correct vocabulary growth rate. The investigation overall suggests that uniform sampling is one cause of long-range correlation and could thus have a relation with actual linguistic processes.\n    ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2017-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03665",
        "title": "Scale Up Event Extraction Learning via Automatic Training Data Generation",
        "authors": [
            "Ying Zeng",
            "Yansong Feng",
            "Rong Ma",
            "Zheng Wang",
            "Rui Yan",
            "Chongde Shi",
            "Dongyan Zhao"
        ],
        "abstract": "The task of event extraction has long been investigated in a supervised learning paradigm, which is bound by the number and the quality of the training instances. Existing training data must be manually generated through a combination of expert domain knowledge and extensive human involvement. However, due to drastic efforts required in annotating text, the resultant datasets are usually small, which severally affects the quality of the learned model, making it hard to generalize. Our work develops an automatic approach for generating training data for event extraction. Our approach allows us to scale up event extraction training instances from thousands to hundreds of thousands, and it does this at a much lower cost than a manual approach. We achieve this by employing distant supervision to automatically create event annotations from unlabelled text using existing structured knowledge bases or ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2017-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03903",
        "title": "A Novel Way of Identifying Cyber Predators",
        "authors": [
            "Dan Liu",
            "Ching Yee Suen",
            "Olga Ormandjieva"
        ],
        "abstract": "Recurrent Neural Networks with Long Short-Term Memory cell (LSTM-RNN) have impressive ability in sequence data processing, particularly for language model building and text classification. This research proposes the combination of sentiment analysis, new approach of sentence vectors and LSTM-RNN as a novel way for Sexual Predator Identification (SPI). LSTM-RNN language model is applied to generate sentence vectors which are the last hidden states in the language model. Sentence vectors are fed into another LSTM-RNN classifier, so as to capture suspicious conversations. Hidden state enables to generate vectors for sentences never seen before. Fasttext is used to filter the contents of conversations and generate a sentiment score so as to identify potential predators. The experiment achieves a record-breaking accuracy and precision of 100% with recall of 81.10%, exceeding the top-ranked result in the SPI competition.\n    ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2017-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03935",
        "title": "On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification",
        "authors": [
            "Gaurav Bhatt",
            "Aman Sharma",
            "Shivam Sharma",
            "Ankush Nagpal",
            "Balasubramanian Raman",
            "Ankush Mittal"
        ],
        "abstract": "Identifying the veracity of a news article is an interesting problem while automating this process can be a challenging task. Detection of a news article as fake is still an open question as it is contingent on many factors which the current state-of-the-art models fail to incorporate. In this paper, we explore a subtask to fake news identification, and that is stance detection. Given a news article, the task is to determine the relevance of the body and its claim. We present a novel idea that combines the neural, statistical and external features to provide an efficient solution to this problem. We compute the neural embedding from the deep recurrent model, statistical features from the weighted n-gram bag-of-words model and handcrafted external features with the help of feature engineering heuristics. Finally, using deep neural layer all the features are combined, thereby classifying the headline-body news pair as agree, disagree, discuss, or unrelated. We compare our proposed technique with the current state-of-the-art models on the fake news challenge dataset. Through extensive experiments, we find that the proposed model outperforms all the state-of-the-art techniques including the submissions to the fake news challenge.\n    ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2017-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04034",
        "title": "Learning Robust Dialog Policies in Noisy Environments",
        "authors": [
            "Maryam Fazel-Zarandi",
            "Shang-Wen Li",
            "Jin Cao",
            "Jared Casale",
            "Peter Henderson",
            "David Whitney",
            "Alborz Geramifard"
        ],
        "abstract": "Modern virtual personal assistants provide a convenient interface for completing daily tasks via voice commands. An important consideration for these assistants is the ability to recover from automatic speech recognition (ASR) and natural language understanding (NLU) errors. In this paper, we focus on learning robust dialog policies to recover from these errors. To this end, we develop a user simulator which interacts with the assistant through voice commands in realistic scenarios with noisy audio, and use it to learn dialog policies through deep reinforcement learning. We show that dialogs generated by our simulator are indistinguishable from human generated dialogs, as determined by human evaluators. Furthermore, preliminary experimental results show that the learned policies in noisy environments achieve the same execution success rate with fewer dialog turns compared to fixed rule-based policies.\n    ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2017-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04116",
        "title": "A Novel Document Generation Process for Topic Detection based on Hierarchical Latent Tree Models",
        "authors": [
            "Peixian Chen",
            "Zhourong Chen",
            "Nevin L. Zhang"
        ],
        "abstract": "We propose a novel document generation process based on hierarchical latent tree models (HLTMs) learned from data. An HLTM has a layer of observed word variables at the bottom and multiple layers of latent variables on top. For each document, we first sample values for the latent variables layer by layer via logic sampling, then draw relative frequencies for the words conditioned on the values of the latent variables, and finally generate words for the document using the relative word frequencies. The motivation for the work is to take word counts into consideration with HLTMs. In comparison with LDA-based hierarchical document generation processes, the new process achieves drastically better model fit with much fewer parameters. It also yields more meaningful topics and topic hierarchies. It is the new state-of-the-art for the hierarchical topic detection.\n    ",
        "submission_date": "2017-12-12T00:00:00",
        "last_modified_date": "2019-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04158",
        "title": "Tracing a Loose Wordhood for Chinese Input Method Engine",
        "authors": [
            "Xihu Zhang",
            "Chu Wei",
            "Hai Zhao"
        ],
        "abstract": "Chinese input methods are used to convert pinyin sequence or other Latin encoding systems into Chinese character sentences. For more effective pinyin-to-character conversion, typical Input Method Engines (IMEs) rely on a predefined vocabulary that demands manually maintenance on schedule. For the purpose of removing the inconvenient vocabulary setting, this work focuses on automatic wordhood acquisition by fully considering that Chinese inputting is a free human-computer interaction procedure. Instead of strictly defining words, a loose word likelihood is introduced for measuring how likely a character sequence can be a user-recognized word with respect to using IME. Then an online algorithm is proposed to adjust the word likelihood or generate new words by comparing user true choice for inputting and the algorithm prediction. The experimental results show that the proposed solution can agilely adapt to diverse typings and demonstrate performance approaching highly-optimized IME with fixed vocabulary.\n    ",
        "submission_date": "2017-12-12T00:00:00",
        "last_modified_date": "2017-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04313",
        "title": "The Zero Resource Speech Challenge 2017",
        "authors": [
            "Ewan Dunbar",
            "Xuan Nga Cao",
            "Juan Benjumea",
            "Julien Karadayi",
            "Mathieu Bernard",
            "Laurent Besacier",
            "Xavier Anguera",
            "Emmanuel Dupoux"
        ],
        "abstract": "We describe a new challenge aimed at discovering subword and word units from raw speech. This challenge is the followup to the Zero Resource Speech Challenge 2015. It aims at constructing systems that generalize across languages and adapt to new speakers. The design features and evaluation metrics of the challenge are presented and the results of seventeen models are discussed.\n    ",
        "submission_date": "2017-12-12T00:00:00",
        "last_modified_date": "2017-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04708",
        "title": "Differentiable lower bound for expected BLEU score",
        "authors": [
            "Vlad Zhukov",
            "Eugene Golikov",
            "Maksim Kretov"
        ],
        "abstract": "In natural language processing tasks performance of the models is often measured with some non-differentiable metric, such as BLEU score. To use efficient gradient-based methods for optimization, it is a common workaround to optimize some surrogate loss function. This approach is effective if optimization of such loss also results in improving target metric. The corresponding problem is referred to as loss-evaluation mismatch. In the present work we propose a method for calculation of differentiable lower bound of expected BLEU score that does not involve computationally expensive sampling procedure such as the one required when using REINFORCE rule from reinforcement learning (RL) framework.\n    ",
        "submission_date": "2017-12-13T00:00:00",
        "last_modified_date": "2018-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04762",
        "title": "Social Media Writing Style Fingerprint",
        "authors": [
            "Himank Yadav",
            "Juliang Li"
        ],
        "abstract": "We present our approach for computer-aided social media text authorship attribution based on recent advances in short text authorship verification. We use various natural language techniques to create word-level and character-level models that act as hidden layers to simulate a simple neural network. The choice of word-level and character-level models in each layer was informed through validation performance. The output layer of our system uses an unweighted majority vote vector to arrive at a conclusion. We also considered writing bias in social media posts while collecting our training dataset to increase system robustness. Our system achieved a precision, recall, and F-measure of 0.82, 0.926 and 0.869 respectively.\n    ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2017-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04787",
        "title": "Creating New Language and Voice Components for the Updated MaryTTS Text-to-Speech Synthesis Platform",
        "authors": [
            "Ingmar Steiner",
            "S\u00e9bastien Le Maguer"
        ],
        "abstract": "We present a new workflow to create components for the MaryTTS text-to-speech synthesis platform, which is popular with researchers and developers, extending it to support new languages and custom synthetic voices. This workflow replaces the previous toolkit with an efficient, flexible process that leverages modern build automation and cloud-hosted infrastructure. Moreover, it is compatible with the updated MaryTTS architecture, enabling new features and state-of-the-art paradigms such as synthesis based on deep neural networks (DNNs). Like MaryTTS itself, the new tools are free, open source software (FOSS), and promote the use of open data.\n    ",
        "submission_date": "2017-12-13T00:00:00",
        "last_modified_date": "2018-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04853",
        "title": "A User-Study on Online Adaptation of Neural Machine Translation to Human Post-Edits",
        "authors": [
            "Sariya Karimova",
            "Patrick Simianer",
            "Stefan Riezler"
        ],
        "abstract": "The advantages of neural machine translation (NMT) have been extensively validated for offline translation of several language pairs for different domains of spoken and written language. However, research on interactive learning of NMT by adaptation to human post-edits has so far been confined to simulation experiments. We present the first user study on online adaptation of NMT to user post-edits in the domain of patent translation. Our study involves 29 human subjects (translation students) whose post-editing effort and translation quality were measured on about 4,500 interactions of a human post-editor and a machine translation system integrating an online adaptive learning algorithm. Our experimental results show a significant reduction of human post-editing effort due to online adaptation in NMT according to several evaluation metrics, including hTER, hBLEU, and KSMR. Furthermore, we found significant improvements in BLEU/TER between NMT outputs and professional translations in granted patents, providing further evidence for the advantages of online adaptive NMT in an interactive setup.\n    ",
        "submission_date": "2017-12-13T00:00:00",
        "last_modified_date": "2018-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05128",
        "title": "Passing the Brazilian OAB Exam: data preparation and some experiments",
        "authors": [
            "Pedro Delfino",
            "Bruno Cuconato",
            "Edward Hermann Haeusler",
            "Alexandre Rademaker"
        ],
        "abstract": "In Brazil, all legal professionals must demonstrate their knowledge of the law and its application by passing the OAB exams, the national bar exams. The OAB exams therefore provide an excellent benchmark for the performance of legal information systems since passing the exam would arguably signal that the system has acquired capacity of legal reasoning comparable to that of a human lawyer. This article describes the construction of a new data set and some preliminary experiments on it, treating the problem of finding the justification for the answers to questions. The results provide a baseline performance measure against which to evaluate future improvements. We discuss the reasons to the poor performance and propose next steps.\n    ",
        "submission_date": "2017-12-14T00:00:00",
        "last_modified_date": "2017-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05181",
        "title": "Rasa: Open Source Language Understanding and Dialogue Management",
        "authors": [
            "Tom Bocklisch",
            "Joey Faulkner",
            "Nick Pawlowski",
            "Alan Nichol"
        ],
        "abstract": "We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source python libraries for building conversational software. Their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers. In terms of design philosophy, we aim for ease of use, and bootstrapping from minimal (or no) initial training data. Both packages are extensively documented and ship with a comprehensive suite of tests. The code is available at ",
        "submission_date": "2017-12-14T00:00:00",
        "last_modified_date": "2017-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05191",
        "title": "Relation Extraction : A Survey",
        "authors": [
            "Sachin Pawar",
            "Girish K. Palshikar",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "With the advent of the Internet, large amount of digital text is generated everyday in the form of news articles, research publications, blogs, question answering forums and social media. It is important to develop techniques for extracting information automatically from these documents, as lot of important information is hidden within them. This extracted information can be used to improve access and management of knowledge hidden in large text corpora. Several applications such as Question Answering, Information Retrieval would benefit from this information. Entities like persons and organizations, form the most basic unit of the information. Occurrences of entities in a sentence are often linked through well-defined relations; e.g., occurrences of person and organization in a sentence may be linked through relations such as employed at. The task of Relation Extraction (RE) is to identify such relations automatically. In this paper, we survey several important supervised, semi-supervised and unsupervised RE techniques. We also cover the paradigms of Open Information Extraction (OIE) and Distant Supervision. Finally, we describe some of the recent trends in the RE techniques and possible future research directions. This survey would be useful for three kinds of readers - i) Newcomers in the field who want to quickly learn about RE; ii) Researchers who want to know how the various RE techniques evolved over time and what are possible future research directions and iii) Practitioners who just need to know which RE technique works best in various settings.\n    ",
        "submission_date": "2017-12-14T00:00:00",
        "last_modified_date": "2017-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05382",
        "title": "Monotonic Chunkwise Attention",
        "authors": [
            "Chung-Cheng Chiu",
            "Colin Raffel"
        ],
        "abstract": "Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of a model using an offline soft attention mechanism. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention-based model.\n    ",
        "submission_date": "2017-12-14T00:00:00",
        "last_modified_date": "2018-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05403",
        "title": "Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis",
        "authors": [
            "Yi Tay",
            "Anh Tuan Luu",
            "Siu Cheung Hui"
        ],
        "abstract": "Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a given document with respect to a given aspect entity. While neural network architectures have been successful in predicting the overall polarity of sentences, aspect-specific sentiment analysis still remains as an open problem. In this paper, we propose a novel method for integrating aspect information into the neural model. More specifically, we incorporate aspect information into the neural model by modeling word-aspect relationships. Our novel model, \\textit{Aspect Fusion LSTM} (AF-LSTM) learns to attend based on associative relationships between sentence words and aspect which allows our model to adaptively focus on the correct words given an aspect term. This ameliorates the flaws of other state-of-the-art models that utilize naive concatenations to model word-aspect similarity. Instead, our model adopts circular convolution and circular correlation to model the similarity between aspect and words and elegantly incorporates this within a differentiable neural attention framework. Finally, our model is end-to-end differentiable and highly related to convolution-correlation (holographic like) memories. Our proposed neural model achieves state-of-the-art performance on benchmark datasets, outperforming ATAE-LSTM by $4\\%-5\\%$ on average across multiple datasets.\n    ",
        "submission_date": "2017-12-14T00:00:00",
        "last_modified_date": "2017-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05483",
        "title": "Learning when to skim and when to read",
        "authors": [
            "Alexander Rosenberg Johansen",
            "Richard Socher"
        ],
        "abstract": "Many recent advances in deep learning for natural language processing have come at increasing computational cost, but the power of these state-of-the-art models is not needed for every example in a dataset. We demonstrate two approaches to reducing unnecessary computation in cases where a fast but weak baseline classier and a stronger, slower model are both available. Applying an AUC-based metric to the task of sentiment classification, we find significant efficiency gains with both a probability-threshold method for reducing computational cost and one that uses a secondary decision network.\n    ",
        "submission_date": "2017-12-15T00:00:00",
        "last_modified_date": "2017-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05608",
        "title": "A Novel Approach for Effective Learning in Low Resourced Scenarios",
        "authors": [
            "Sri Harsha Dumpala",
            "Rupayan Chakraborty",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "Deep learning based discriminative methods, being the state-of-the-art machine learning techniques, are ill-suited for learning from lower amounts of data. In this paper, we propose a novel framework, called simultaneous two sample learning (s2sL), to effectively learn the class discriminative characteristics, even from very low amount of data. In s2sL, more than one sample (here, two samples) are simultaneously considered to both, train and test the classifier. We demonstrate our approach for speech/music discrimination and emotion classification through experiments. Further, we also show the effectiveness of s2sL approach for classification in low-resource scenario, and for imbalanced data.\n    ",
        "submission_date": "2017-12-15T00:00:00",
        "last_modified_date": "2017-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05626",
        "title": "Avoiding Echo-Responses in a Retrieval-Based Conversation System",
        "authors": [
            "Denis Fedorenko",
            "Nikita Smetanin",
            "Artem Rodichev"
        ],
        "abstract": "Retrieval-based conversation systems generally tend to highly rank responses that are semantically similar or even identical to the given conversation context. While the system's goal is to find the most appropriate response, rather than the most semantically similar one, this tendency results in low-quality responses. We refer to this challenge as the echoing problem. To mitigate this problem, we utilize a hard negative mining approach at the training stage. The evaluation shows that the resulting model reduces echoing and achieves better results in terms of Average Precision and Recall@N metrics, compared to the models trained without the proposed approach.\n    ",
        "submission_date": "2017-12-15T00:00:00",
        "last_modified_date": "2018-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05690",
        "title": "Sockeye: A Toolkit for Neural Machine Translation",
        "authors": [
            "Felix Hieber",
            "Tobias Domhan",
            "Michael Denkowski",
            "David Vilar",
            "Artem Sokolov",
            "Ann Clifton",
            "Matt Post"
        ],
        "abstract": "We describe Sockeye (version 1.12), an open-source sequence-to-sequence toolkit for Neural Machine Translation (NMT). Sockeye is a production-ready framework for training and applying models as well as an experimental platform for researchers. Written in Python and built on MXNet, the toolkit offers scalable training and inference for the three most prominent encoder-decoder architectures: attentional recurrent neural networks, self-attentional transformers, and fully convolutional networks. Sockeye also supports a wide range of optimizers, normalization and regularization techniques, and inference improvements from current NMT literature. Users can easily run standard training recipes, explore different model settings, and incorporate new ideas. In this paper, we highlight Sockeye's features and benchmark it against other NMT toolkits on two language arcs from the 2017 Conference on Machine Translation (WMT): English-German and Latvian-English. We report competitive BLEU scores across all three architectures, including an overall best score for Sockeye's transformer implementation. To facilitate further comparison, we release all system outputs and training scripts used in our experiments. The Sockeye toolkit is free software released under the Apache 2.0 license.\n    ",
        "submission_date": "2017-12-15T00:00:00",
        "last_modified_date": "2018-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05785",
        "title": "Sentiment Predictability for Stocks",
        "authors": [
            "Jordan Prosky",
            "Xingyou Song",
            "Andrew Tan",
            "Michael Zhao"
        ],
        "abstract": "In this work, we present our findings and experiments for stock-market prediction using various textual sentiment analysis tools, such as mood analysis and event extraction, as well as prediction models, such as LSTMs and specific convolutional architectures.\n    ",
        "submission_date": "2017-12-15T00:00:00",
        "last_modified_date": "2018-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05846",
        "title": "Hierarchical Text Generation and Planning for Strategic Dialogue",
        "authors": [
            "Denis Yarats",
            "Mike Lewis"
        ],
        "abstract": "End-to-end models for goal-orientated dialogue are challenging to train, because linguistic and strategic aspects are entangled in latent state vectors. We introduce an approach to learning representations of messages in dialogues by maximizing the likelihood of subsequent sentences and actions, which decouples the semantics of the dialogue utterance from its linguistic realization. We then use these latent sentence representations for hierarchical language generation, planning and reinforcement learning. Experiments show that our approach increases the end-task reward achieved by the model, improves the effectiveness of long-term planning using rollouts, and allows self-play reinforcement learning to improve decision making without diverging from human language. Our hierarchical latent-variable model outperforms previous work both linguistically and strategically.\n    ",
        "submission_date": "2017-12-15T00:00:00",
        "last_modified_date": "2018-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05884",
        "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions",
        "authors": [
            "Jonathan Shen",
            "Ruoming Pang",
            "Ron J. Weiss",
            "Mike Schuster",
            "Navdeep Jaitly",
            "Zongheng Yang",
            "Zhifeng Chen",
            "Yu Zhang",
            "Yuxuan Wang",
            "RJ Skerry-Ryan",
            "Rif A. Saurous",
            "Yannis Agiomyrgiannakis",
            "Yonghui Wu"
        ],
        "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.\n    ",
        "submission_date": "2017-12-16T00:00:00",
        "last_modified_date": "2018-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05898",
        "title": "NegBio: a high-performance tool for negation and uncertainty detection in radiology reports",
        "authors": [
            "Yifan Peng",
            "Xiaosong Wang",
            "Le Lu",
            "Mohammadhadi Bagheri",
            "Ronald Summers",
            "Zhiyong Lu"
        ],
        "abstract": "Negative and uncertain medical findings are frequent in radiology reports, but discriminating them from positive findings remains challenging for information extraction. Here, we propose a new algorithm, NegBio, to detect negative and uncertain findings in radiology reports. Unlike previous rule-based methods, NegBio utilizes patterns on universal dependencies to identify the scope of triggers that are indicative of negation or uncertainty. We evaluated NegBio on four datasets, including two public benchmarking corpora of radiology reports, a new radiology corpus that we annotated for this work, and a public corpus of general clinical texts. Evaluation on these datasets demonstrates that NegBio is highly accurate for detecting negative and uncertain findings and compares favorably to a widely-used state-of-the-art system NegEx (an average of 9.5% improvement in precision and 5.1% in F1-score).\n    ",
        "submission_date": "2017-12-16T00:00:00",
        "last_modified_date": "2017-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05972",
        "title": "Train Once, Test Anywhere: Zero-Shot Learning for Text Classification",
        "authors": [
            "Pushpankar Kumar Pushp",
            "Muktabh Mayank Srivastava"
        ],
        "abstract": "Zero-shot Learners are models capable of predicting unseen classes. In this work, we propose a Zero-shot Learning approach for text categorization. Our method involves training model on a large corpus of sentences to learn the relationship between a sentence and embedding of sentence's tags. Learning such relationship makes the model generalize to unseen sentences, tags, and even new datasets provided they can be put into same embedding space. The model learns to predict whether a given sentence is related to a tag or not; unlike other classifiers that learn to classify the sentence as one of the possible classes. We propose three different neural networks for the task and report their accuracy on the test set of the dataset used for training them as well as two other standard datasets for which no retraining was done. We show that our models generalize well across new unseen classes in both cases. Although the models do not achieve the accuracy level of the state of the art supervised models, yet it evidently is a step forward towards general intelligence in natural language processing.\n    ",
        "submission_date": "2017-12-16T00:00:00",
        "last_modified_date": "2017-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05999",
        "title": "Characterizing Political Fake News in Twitter by its Meta-Data",
        "authors": [
            "Julio Amador",
            "Axel Oehmichen",
            "Miguel Molina-Solana"
        ],
        "abstract": "This article presents a preliminary approach towards characterizing political fake news on Twitter through the analysis of their meta-data. In particular, we focus on more than 1.5M tweets collected on the day of the election of Donald Trump as 45th president of the United States of America. We use the meta-data embedded within those tweets in order to look for differences between tweets containing fake news and tweets not containing them. Specifically, we perform our analysis only on tweets that went viral, by studying proxies for users' exposure to the tweets, by characterizing accounts spreading fake news, and by looking at their polarization. We found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users.\n    ",
        "submission_date": "2017-12-16T00:00:00",
        "last_modified_date": "2017-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06074",
        "title": "Benford's Law and First Letter of Word",
        "authors": [
            "Xiaoyong Yan",
            "Seong-Gyu Yang",
            "Beom Jun Kim",
            "Petter Minnhagen"
        ],
        "abstract": "A universal First-Letter Law (FLL) is derived and described. It predicts the percentages of first letters for words in novels. The FLL is akin to Benford's law (BL) of first digits, which predicts the percentages of first digits in a data collection of numbers. Both are universal in the sense that FLL only depends on the numbers of letters in the alphabet, whereas BL only depends on the number of digits in the base of the number system. The existence of these types of universal laws appears counter-intuitive. Nonetheless both describe data very well. Relations to some earlier works are given. FLL predicts that an English author on the average starts about 16 out of 100 words with the English letter `t'. This is corroborated by data, yet an author can freely write anything. Fuller implications and the applicability of FLL remain for the future.\n    ",
        "submission_date": "2017-12-17T00:00:00",
        "last_modified_date": "2017-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06086",
        "title": "Deep Learning for Distant Speech Recognition",
        "authors": [
            "Mirco Ravanelli"
        ],
        "abstract": "Deep learning is an emerging technology that is considered one of the most promising directions for reaching higher levels of artificial intelligence. Among the other achievements, building computers that understand speech represents a crucial leap towards intelligent machines. Despite the great efforts of the past decades, however, a natural and robust human-machine speech interaction still appears to be out of reach, especially when users interact with a distant microphone in noisy and reverberant environments. The latter disturbances severely hamper the intelligibility of a speech signal, making Distant Speech Recognition (DSR) one of the major open challenges in the field.\n",
        "submission_date": "2017-12-17T00:00:00",
        "last_modified_date": "2017-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06100",
        "title": "Query-Based Abstractive Summarization Using Neural Networks",
        "authors": [
            "Johan Hasselqvist",
            "Niklas Helmertz",
            "Mikael K\u00e5geb\u00e4ck"
        ],
        "abstract": "In this paper, we present a model for generating summaries of text documents with respect to a query. This is known as query-based summarization. We adapt an existing dataset of news article summaries for the task and train a pointer-generator model using this dataset. The generated summaries are evaluated by measuring similarity to reference summaries. Our results show that a neural network summarization model, similar to existing neural network models for abstractive summarization, can be constructed to make use of queries to produce targeted summaries.\n    ",
        "submission_date": "2017-12-17T00:00:00",
        "last_modified_date": "2017-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06163",
        "title": "Towards a science of human stories: using sentiment analysis and emotional arcs to understand the building blocks of complex social systems",
        "authors": [
            "Andrew J. Reagan"
        ],
        "abstract": "Given the growing assortment of sentiment measuring instruments, it is imperative to understand which aspects of sentiment dictionaries contribute to both their classification accuracy and their ability to provide richer understanding of texts. Here, we perform detailed, quantitative tests and qualitative assessments of 6 dictionary-based methods applied, and briefly examine a further 20 methods. We show that while inappropriate for sentences, dictionary-based methods are generally robust in their classification accuracy for longer texts.\n",
        "submission_date": "2017-12-17T00:00:00",
        "last_modified_date": "2017-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06273",
        "title": "Low Resourced Machine Translation via Morpho-syntactic Modeling: The Case of Dialectal Arabic",
        "authors": [
            "Alexander Erdmann",
            "Nizar Habash",
            "Dima Taji",
            "Houda Bouamor"
        ],
        "abstract": "We present the second ever evaluated Arabic dialect-to-dialect machine translation effort, and the first to leverage external resources beyond a small parallel corpus. The subject has not previously received serious attention due to lack of naturally occurring parallel data; yet its importance is evidenced by dialectal Arabic's wide usage and breadth of inter-dialect variation, comparable to that of Romance languages. Our results suggest that modeling morphology and syntax significantly improves dialect-to-dialect translation, though optimizing such data-sparse models requires consideration of the linguistic differences between dialects and the nature of available data and resources. On a single-reference blind test set where untranslated input scores 6.5 BLEU and a model trained only on parallel data reaches 14.6, pivot techniques and morphosyntactic modeling significantly improve performance to 17.5.\n    ",
        "submission_date": "2017-12-18T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06289",
        "title": "A Chinese Dataset with Negative Full Forms for General Abbreviation Prediction",
        "authors": [
            "Yi Zhang",
            "Xu Sun"
        ],
        "abstract": "Abbreviation is a common phenomenon across languages, especially in Chinese. In most cases, if an expression can be abbreviated, its abbreviation is used more often than its fully expanded forms, since people tend to convey information in a most concise way. For various language processing tasks, abbreviation is an obstacle to improving the performance, as the textual form of an abbreviation does not express useful information, unless it's expanded to the full form. Abbreviation prediction means associating the fully expanded forms with their abbreviations. However, due to the deficiency in the abbreviation corpora, such a task is limited in current studies, especially considering general abbreviation prediction should also include those full form expressions that do not have valid abbreviations, namely the negative full forms (NFFs). Corpora incorporating negative full forms for general abbreviation prediction are few in number. In order to promote the research in this area, we build a dataset for general Chinese abbreviation prediction, which needs a few preprocessing steps, and evaluate several different models on the built dataset. The dataset is available at ",
        "submission_date": "2017-12-18T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06427",
        "title": "Detecting Hate Speech in Social Media",
        "authors": [
            "Shervin Malmasi",
            "Marcos Zampieri"
        ],
        "abstract": "In this paper we examine methods to detect hate speech in social media, while distinguishing this from general profanity. We aim to establish lexical baselines for this task by applying supervised classification methods using a recently released dataset annotated for this purpose. As features, our system uses character n-grams, word n-grams and word skip-grams. We obtain results of 78% accuracy in identifying posts across three classes. Results demonstrate that the main challenge lies in discriminating profanity and hate speech from each other. A number of directions for future work are discussed.\n    ",
        "submission_date": "2017-12-18T00:00:00",
        "last_modified_date": "2017-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06674",
        "title": "word representation or word embedding in Persian text",
        "authors": [
            "Siamak Sarmady",
            "Erfan Rahmani"
        ],
        "abstract": "Text processing is one of the sub-branches of natural language processing. Recently, the use of machine learning and neural networks methods has been given greater consideration. For this reason, the representation of words has become very important. This article is about word representation or converting words into vectors in Persian text. In this research GloVe, CBOW and skip-gram methods are updated to produce embedded vectors for Persian words. In order to train a neural networks, Bijankhan corpus, Hamshahri corpus and UPEC corpus have been compound and used. Finally, we have 342,362 words that obtained vectors in all three models for this words. These vectors have many usage for Persian natural language processing.\n    ",
        "submission_date": "2017-12-18T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06751",
        "title": "HotFlip: White-Box Adversarial Examples for Text Classification",
        "authors": [
            "Javid Ebrahimi",
            "Anyi Rao",
            "Daniel Lowd",
            "Dejing Dou"
        ],
        "abstract": "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.\n    ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2018-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06855",
        "title": "Subword and Crossword Units for CTC Acoustic Models",
        "authors": [
            "Thomas Zenkel",
            "Ramon Sanabria",
            "Florian Metze",
            "Alex Waibel"
        ],
        "abstract": "This paper proposes a novel approach to create an unit set for CTC based speech recognition systems. By using Byte Pair Encoding we learn an unit set of an arbitrary size on a given training text. In contrast to using characters or words as units this allows us to find a good trade-off between the size of our unit set and the available training data. We evaluate both Crossword units, that may span multiple word, and Subword units. By combining this approach with decoding methods using a separate language model we are able to achieve state of the art results for grapheme based CTC systems.\n    ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2018-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06880",
        "title": "Analogy Mining for Specific Design Needs",
        "authors": [
            "Karni Gilon",
            "Felicia Y Ng",
            "Joel Chan",
            "Hila Lifshitz Assaf",
            "Aniket Kittur",
            "Dafna Shahaf"
        ],
        "abstract": "Finding analogical inspirations in distant domains is a powerful way of solving problems. However, as the number of inspirations that could be matched and the dimensions on which that matching could occur grow, it becomes challenging for designers to find inspirations relevant to their needs. Furthermore, designers are often interested in exploring specific aspects of a product-- for example, one designer might be interested in improving the brewing capability of an outdoor coffee maker, while another might wish to optimize for portability. In this paper we introduce a novel system for targeting analogical search for specific needs. Specifically, we contribute a novel analogical search engine for expressing and abstracting specific design needs that returns more distant yet relevant inspirations than alternate approaches.\n    ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06961",
        "title": "Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings",
        "authors": [
            "Hanan Aldarmaki",
            "Mahesh Mohan",
            "Mona Diab"
        ],
        "abstract": "Most existing methods for automatic bilingual dictionary induction rely on prior alignments between the source and target languages, such as parallel corpora or seed dictionaries. For many language pairs, such supervised alignments are not readily available. We propose an unsupervised approach for learning a bilingual dictionary for a pair of languages given their independently-learned monolingual word embeddings. The proposed method exploits local and global structures in monolingual vector spaces to align them such that similar words are mapped to each other. We show empirically that the performance of bilingual correspondents learned using our proposed unsupervised method is comparable to that of using supervised bilingual correspondents from a seed dictionary.\n    ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2018-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06994",
        "title": "DeepNorm-A Deep Learning Approach to Text Normalization",
        "authors": [
            "Maryam Zare",
            "Shaurya Rohatgi"
        ],
        "abstract": "This paper presents an simple yet sophisticated approach to the challenge by Sproat and Jaitly (2016)- given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. Text normalization for a token seems very straightforward without it's context. But given the context of the used token and then normalizing becomes tricky for some classes. We present a novel approach in which the prediction of our classification algorithm is used by our sequence to sequence model to predict the normalized text of the input token. Our approach takes very less time to learn and perform well unlike what has been reported by Google (5 days on their GPU cluster). We have achieved an accuracy of 97.62 which is impressive given the resources we use. Our approach is using the best of both worlds, gradient boosting - state of the art in most classification tasks and sequence to sequence learning - state of the art in machine translation. We present our experiments and report results with various parameter settings.\n    ",
        "submission_date": "2017-12-17T00:00:00",
        "last_modified_date": "2017-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07004",
        "title": "Any-gram Kernels for Sentence Classification: A Sentiment Analysis Case Study",
        "authors": [
            "Rasoul Kaljahi",
            "Jennifer Foster"
        ],
        "abstract": "Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram features when learning from textual data. They are also compatible with the use of word embeddings so that word similarities can be accounted for. While the original any-gram kernels are implemented on top of tree kernels, we propose a new approach which is independent of tree kernels and is more efficient. We also propose a more effective way to make use of word embeddings than the original any-gram formulation. When applied to the task of sentiment classification, our new formulation achieves significantly better performance.\n    ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07040",
        "title": "The NarrativeQA Reading Comprehension Challenge",
        "authors": [
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Jonathan Schwarz",
            "Phil Blunsom",
            "Chris Dyer",
            "Karl Moritz Hermann",
            "G\u00e1bor Melis",
            "Edward Grefenstette"
        ],
        "abstract": "Reading comprehension (RC)---in contrast to information retrieval---requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.\n    ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07101",
        "title": "Improving End-to-End Speech Recognition with Policy Learning",
        "authors": [
            "Yingbo Zhou",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "Connectionist temporal classification (CTC) is widely used for maximum likelihood learning in end-to-end speech recognition models. However, there is usually a disparity between the negative maximum likelihood and the performance metric used in speech recognition, e.g., word error rate (WER). This results in a mismatch between the objective function and metric during training. We show that the above problem can be mitigated by jointly training with maximum likelihood and policy gradient. In particular, with policy learning we are able to directly optimize on the (otherwise non-differentiable) performance metric. We show that joint training improves relative performance by 4% to 13% for our end-to-end model as compared to the same model learned through maximum likelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and 5.42% and 14.70% on Librispeech test-clean and test-other set, respectively.\n    ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07108",
        "title": "Improved Regularization Techniques for End-to-End Speech Recognition",
        "authors": [
            "Yingbo Zhou",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "Regularization is important for end-to-end speech models, since the models are highly flexible and easy to overfit. Data augmentation and dropout has been important for improving end-to-end models in other domains. However, they are relatively under explored for end-to-end speech models. Therefore, we investigate the effectiveness of both methods for end-to-end trainable, deep speech recognition models. We augment audio data through random perturbations of tempo, pitch, volume, temporal alignment, and adding random ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07229",
        "title": "Attentive Memory Networks: Efficient Machine Reading for Conversational Search",
        "authors": [
            "Tom Kenter",
            "Maarten de Rijke"
        ],
        "abstract": "Recent advances in conversational systems have changed the search paradigm. Traditionally, a user poses a query to a search engine that returns an answer based on its index, possibly leveraging external knowledge bases and conditioning the response on earlier interactions in the search session. In a natural conversation, there is an additional source of information to take into account: utterances produced earlier in a conversation can also be referred to and a conversational IR system has to keep track of information conveyed by the user during the conversation, even if it is implicit.\n",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07316",
        "title": "A Flexible Approach to Automated RNN Architecture Generation",
        "authors": [
            "Martin Schrimpf",
            "Stephen Merity",
            "James Bradbury",
            "Richard Socher"
        ],
        "abstract": "The process of designing neural architectures requires expert knowledge and extensive trial and error. While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components. We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width. The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization. Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains. The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.\n    ",
        "submission_date": "2017-12-20T00:00:00",
        "last_modified_date": "2017-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07473",
        "title": "Differentially Private Distributed Learning for Language Modeling Tasks",
        "authors": [
            "Vadim Popov",
            "Mikhail Kudinov",
            "Irina Piontkovskaya",
            "Petr Vytovtov",
            "Alex Nevidomsky"
        ],
        "abstract": "One of the big challenges in machine learning applications is that training data can be different from the real-world data faced by the algorithm. In language modeling, users' language (e.g. in private messaging) could change in a year and be completely different from what we observe in publicly available data. At the same time, public data can be used for obtaining general knowledge (i.e. general model of English). We study approaches to distributed fine-tuning of a general model on user private data with the additional requirements of maintaining the quality on the general data and minimization of communication costs. We propose a novel technique that significantly improves prediction quality on users' language compared to a general model and outperforms gradient compression methods in terms of communication efficiency. The proposed procedure is fast and leads to an almost 70% perplexity reduction and 8.7 percentage point improvement in keystroke saving rate on informal English texts. We also show that the range of tasks our approach is applicable to is not limited by language modeling only. Finally, we propose an experimental framework for evaluating differential privacy of distributed training of language models and show that our approach has good privacy guarantees.\n    ",
        "submission_date": "2017-12-20T00:00:00",
        "last_modified_date": "2018-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07512",
        "title": "Ethical Questions in NLP Research: The (Mis)-Use of Forensic Linguistics",
        "authors": [
            "Anil Kumar Singh",
            "Akhilesh Sudhakar"
        ],
        "abstract": "Ideas from forensic linguistics are now being used frequently in Natural Language Processing (NLP), using machine learning techniques. While the role of forensic linguistics was more benign earlier, it is now being used for purposes which are questionable. Certain methods from forensic linguistics are employed, without considering their scientific limitations and ethical concerns. While we take the specific case of forensic linguistics as an example of such trends in NLP and machine learning, the issue is a larger one and present in many other scientific and data-driven domains. We suggest that such trends indicate that some of the applied sciences are exceeding their legal and scientific briefs. We highlight how carelessly implemented practices are serving to short-circuit the due processes of law as well breach ethical codes.\n    ",
        "submission_date": "2017-12-20T00:00:00",
        "last_modified_date": "2017-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07558",
        "title": "An Ensemble Model with Ranking for Social Dialogue",
        "authors": [
            "Ioannis Papaioannou",
            "Amanda Cercas Curry",
            "Jose L. Part",
            "Igor Shalyminov",
            "Xinnuo Xu",
            "Yanchao Yu",
            "Ond\u0159ej Du\u0161ek",
            "Verena Rieser",
            "Oliver Lemon"
        ],
        "abstract": "Open-domain social dialogue is one of the long-standing goals of Artificial Intelligence. This year, the Amazon Alexa Prize challenge was announced for the first time, where real customers get to rate systems developed by leading universities worldwide. The aim of the challenge is to converse \"coherently and engagingly with humans on popular topics for 20 minutes\". We describe our Alexa Prize system (called 'Alana') consisting of an ensemble of bots, combining rule-based and machine learning systems, and using a contextual ranking mechanism to choose a system response. The ranker was trained on real user feedback received during the competition, where we address the problem of how to train on the noisy and sparse feedback obtained during the competition.\n    ",
        "submission_date": "2017-12-20T00:00:00",
        "last_modified_date": "2017-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07745",
        "title": "Context-aware Path Ranking for Knowledge Base Completion",
        "authors": [
            "Sahisnu Mazumder",
            "Bing Liu"
        ],
        "abstract": "Knowledge base (KB) completion aims to infer missing facts from existing ones in a KB. Among various approaches, path ranking (PR) algorithms have received increasing attention in recent years. PR algorithms enumerate paths between entity pairs in a KB and use those paths as features to train a model for missing fact prediction. Due to their good performances and high model interpretability, several methods have been proposed. However, most existing methods suffer from scalability (high RAM consumption) and feature explosion (trains on an exponentially large number of features) problems. This paper proposes a Context-aware Path Ranking (C-PR) algorithm to solve these problems by introducing a selective path exploration strategy. C-PR learns global semantics of entities in the KB using word embedding and leverages the knowledge of entity semantics to enumerate contextually relevant paths using bidirectional random walk. Experimental results on three large KBs show that the path features (fewer in number) discovered by C-PR not only improve predictive performance but also are more interpretable than existing baselines.\n    ",
        "submission_date": "2017-12-20T00:00:00",
        "last_modified_date": "2017-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07794",
        "title": "The Character Thinks Ahead: creative writing with deep learning nets and its stylistic assessment",
        "authors": [
            "Roger T. Dean",
            "Hazel Smith"
        ],
        "abstract": "We discuss how to control outputs from deep learning models of text corpora so as to create contemporary poetic works. We assess whether these controls are successful in the immediate sense of creating stylo- metric distinctiveness. The specific context is our piece The Character Thinks Ahead (2016/17); the potential applications are broad.\n    ",
        "submission_date": "2017-12-21T00:00:00",
        "last_modified_date": "2017-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08207",
        "title": "Variational Attention for Sequence-to-Sequence Models",
        "authors": [
            "Hareesh Bahuleyan",
            "Lili Mou",
            "Olga Vechtomova",
            "Pascal Poupart"
        ],
        "abstract": "The variational encoder-decoder (VED) encodes source information as a set of random variables using a neural network, which in turn is decoded into target data using another neural network. In natural language processing, sequence-to-sequence (Seq2Seq) models typically serve as encoder-decoder networks. When combined with a traditional (deterministic) attention mechanism, the variational latent space may be bypassed by the attention model, and thus becomes ineffective. In this paper, we propose a variational attention mechanism for VED, where the attention vector is also modeled as Gaussian distributed random variables. Results on two experiments show that, without loss of quality, our proposed method alleviates the bypassing phenomenon as it increases the diversity of generated sentences.\n    ",
        "submission_date": "2017-12-21T00:00:00",
        "last_modified_date": "2018-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08291",
        "title": "TFW, DamnGina, Juvie, and Hotsie-Totsie: On the Linguistic and Social Aspects of Internet Slang",
        "authors": [
            "Vivek Kulkarni",
            "William Yang Wang"
        ],
        "abstract": "Slang is ubiquitous on the Internet. The emergence of new social contexts like micro-blogs, question-answering forums, and social networks has enabled slang and non-standard expressions to abound on the web. Despite this, slang has been traditionally viewed as a form of non-standard language -- a form of language that is not the focus of linguistic analysis and has largely been neglected. In this work, we use UrbanDictionary to conduct the first large-scale linguistic analysis of slang and its social aspects on the Internet to yield insights into this variety of language that is increasingly used all over the world online.\n",
        "submission_date": "2017-12-22T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08302",
        "title": "Source-side Prediction for Neural Headline Generation",
        "authors": [
            "Shun Kiyono",
            "Sho Takase",
            "Jun Suzuki",
            "Naoaki Okazaki",
            "Kentaro Inui",
            "Masaaki Nagata"
        ],
        "abstract": "The encoder-decoder model is widely used in natural language generation tasks. However, the model sometimes suffers from repeated redundant generation, misses important phrases, and includes irrelevant entities. Toward solving these problems we propose a novel source-side token prediction module. Our method jointly estimates the probability distributions over source and target vocabularies to capture a correspondence between source and target tokens. The experiments show that the proposed model outperforms the current state-of-the-art method in the headline generation task. Additionally, we show that our method has an ability to learn a reasonable token-wise correspondence without knowing any true alignments.\n    ",
        "submission_date": "2017-12-22T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08349",
        "title": "Tracking the Diffusion of Named Entities",
        "authors": [
            "Leon Derczynski",
            "Matthew Rowe"
        ],
        "abstract": "Existing studies of how information diffuses across social networks have thus far concentrated on analysing and recovering the spread of deterministic innovations such as URLs, hashtags, and group membership. However investigating how mentions of real-world entities appear and spread has yet to be explored, largely due to the computationally intractable nature of performing large-scale entity extraction. In this paper we present, to the best of our knowledge, one of the first pieces of work to closely examine the diffusion of named entities on social media, using Reddit as our case study platform. We first investigate how named entities can be accurately recognised and extracted from discussion posts. We then use these extracted entities to study the patterns of entity cascades and how the probability of a user adopting an entity (i.e. mentioning it) is associated with exposures to the entity. We put these pieces together by presenting a parallelised diffusion model that can forecast the probability of entity adoption, finding that the influence of adoption between users can be characterised by their prior interactions -- as opposed to whether the users propagated entity-adoptions beforehand. Our findings have important implications for researchers studying influence and language, and for community analysts who wish to understand entity-level influence dynamics.\n    ",
        "submission_date": "2017-12-22T00:00:00",
        "last_modified_date": "2017-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08439",
        "title": "Novel Ranking-Based Lexical Similarity Measure for Word Embedding",
        "authors": [
            "Jakub Dutkiewicz",
            "Czes\u0142aw J\u0119drzejek"
        ],
        "abstract": "Distributional semantics models derive word space from linguistic items in context. Meaning is obtained by defining a distance measure between vectors corresponding to lexical entities. Such vectors present several problems. In this paper we provide a guideline for post process improvements to the baseline vectors. We focus on refining the similarity aspect, address imperfections of the model by applying the hubness reduction method, implementing relational knowledge into the model, and providing a new ranking similarity definition that give maximum weight to the top 1 component value. This feature ranking is similar to the one used in information retrieval. All these enrichments outperform current literature results for joint ESL and TOEF sets comparison. Since single word embedding is a basic element of any semantic task one can expect a significant improvement of results for these tasks. Moreover, our improved method of text processing can be translated to continuous distributed representation of biological sequences for deep proteomics and genomics.\n    ",
        "submission_date": "2017-12-22T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08636",
        "title": "Find the Conversation Killers: a Predictive Study of Thread-ending Posts",
        "authors": [
            "Yunhao Jiao",
            "Cheng Li",
            "Fei Wu",
            "Qiaozhu Mei"
        ],
        "abstract": "How to improve the quality of conversations in online communities has attracted considerable attention recently. Having engaged, urbane, and reactive online conversations has a critical effect on the social life of Internet users. In this study, we are particularly interested in identifying a post in a multi-party conversation that is unlikely to be further replied to, which therefore kills that thread of the conversation. For this purpose, we propose a deep learning model called the ConverNet. ConverNet is attractive due to its capability of modeling the internal structure of a long conversation and its appropriate encoding of the contextual information of the conversation, through effective integration of attention mechanisms. Empirical experiments on real-world datasets demonstrate the effectiveness of the proposal model. For the widely concerned topic, our analysis also offers implications for improving the quality and user experience of online conversations.\n    ",
        "submission_date": "2017-12-22T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08647",
        "title": "Emo, Love, and God: Making Sense of Urban Dictionary, a Crowd-Sourced Online Dictionary",
        "authors": [
            "Dong Nguyen",
            "Barbara McGillivray",
            "Taha Yasseri"
        ],
        "abstract": "The Internet facilitates large-scale collaborative projects and the emergence of Web 2.0 platforms, where producers and consumers of content unify, has drastically changed the information market. On the one hand, the promise of the \"wisdom of the crowd\" has inspired successful projects such as Wikipedia, which has become the primary source of crowd-based information in many languages. On the other hand, the decentralized and often un-monitored environment of such projects may make them susceptible to low quality content. In this work, we focus on Urban Dictionary, a crowd-sourced online dictionary. We combine computational methods with qualitative annotation and shed light on the overall features of Urban Dictionary in terms of growth, coverage and types of content. We measure a high presence of opinion-focused entries, as opposed to the meaning-focused entries that we expect from traditional dictionaries. Furthermore, Urban Dictionary covers many informal, unfamiliar words as well as proper nouns. Urban Dictionary also contains offensive content, but highly offensive content tends to receive lower scores through the dictionary's voting system. The low threshold to include new material in Urban Dictionary enables quick recording of new words and new meanings, but the resulting heterogeneous content can pose challenges in using Urban Dictionary as a source to study language innovation.\n    ",
        "submission_date": "2017-12-22T00:00:00",
        "last_modified_date": "2018-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08793",
        "title": "Are words easier to learn from infant- than adult-directed speech? A quantitative corpus-based investigation",
        "authors": [
            "Adriana Guevara-Rukoz",
            "Alejandrina Cristia",
            "Bogdan Ludusan",
            "Roland Thiolli\u00e8re",
            "Andrew Martin",
            "Reiko Mazuka",
            "Emmanuel Dupoux"
        ],
        "abstract": "We investigate whether infant-directed speech (IDS) could facilitate word form learning when compared to adult-directed speech (ADS). To study this, we examine the distribution of word forms at two levels, acoustic and phonological, using a large database of spontaneous speech in Japanese. At the acoustic level we show that, as has been documented before for phonemes, the realizations of words are more variable and less discriminable in IDS than in ADS. At the phonological level, we find an effect in the opposite direction: the IDS lexicon contains more distinctive words (such as onomatopoeias) than the ADS counterpart. Combining the acoustic and phonological metrics together in a global discriminability score reveals that the bigger separation of lexical categories in the phonological space does not compensate for the opposite effect observed at the acoustic level. As a result, IDS word forms are still globally less discriminable than ADS word forms, even though the effect is numerically small. We discuss the implication of these findings for the view that the functional role of IDS is to improve language learnability.\n    ",
        "submission_date": "2017-12-23T00:00:00",
        "last_modified_date": "2017-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08819",
        "title": "A Framework for Enriching Lexical Semantic Resources with Distributional Semantics",
        "authors": [
            "Chris Biemann",
            "Stefano Faralli",
            "Alexander Panchenko",
            "Simone Paolo Ponzetto"
        ],
        "abstract": "We present an approach to combining distributional semantic representations induced from text corpora with manually constructed lexical-semantic networks. While both kinds of semantic resources are available with high lexical coverage, our aligned resource combines the domain specificity and availability of contextual information from distributional models with the conciseness and high quality of manually crafted lexical networks. We start with a distributional representation of induced senses of vocabulary terms, which are accompanied with rich context information given by related lexical items. We then automatically disambiguate such representations to obtain a full-fledged proto-conceptualization, i.e. a typed graph of induced word senses. In a final step, this proto-conceptualization is aligned to a lexical ontology, resulting in a hybrid aligned resource. Moreover, unmapped induced senses are associated with a semantic type in order to connect them to the core resource. Manual evaluations against ground-truth judgments for different stages of our method as well as an extrinsic evaluation on a knowledge-based Word Sense Disambiguation benchmark all indicate the high quality of the new hybrid resource. Additionally, we show the benefits of enriching top-down lexical knowledge resources with bottom-up distributional information from text for addressing high-end knowledge acquisition tasks such as cleaning hypernym graphs and learning taxonomies from scratch.\n    ",
        "submission_date": "2017-12-23T00:00:00",
        "last_modified_date": "2017-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08841",
        "title": "Dual Long Short-Term Memory Networks for Sub-Character Representation Learning",
        "authors": [
            "Han He",
            "Lei Wu",
            "Xiaokun Yang",
            "Hua Yan",
            "Zhimin Gao",
            "Yi Feng",
            "George Townsend"
        ],
        "abstract": "Characters have commonly been regarded as the minimal processing unit in Natural Language Processing (NLP). But many non-latin languages have hieroglyphic writing systems, involving a big alphabet with thousands or millions of characters. Each character is composed of even smaller parts, which are often ignored by the previous work. In this paper, we propose a novel architecture employing two stacked Long Short-Term Memory Networks (LSTMs) to learn sub-character level representation and capture deeper level of semantic meanings. To build a concrete study and substantiate the efficiency of our neural architecture, we take Chinese Word Segmentation as a research case example. Among those languages, Chinese is a typical case, for which every character contains several components called radicals. Our networks employ a shared radical level embedding to solve both Simplified and Traditional Chinese Word Segmentation, without extra Traditional to Simplified Chinese conversion, in such a highly end-to-end way the word segmentation can be significantly simplified compared to the previous work. Radical level embeddings can also capture deeper semantic meaning below character level and improve the system performance of learning. By tying radical and character embeddings together, the parameter count is reduced whereas semantic knowledge is shared and transferred between two levels, boosting the performance largely. On 3 out of 4 Bakeoff 2005 datasets, our method surpassed state-of-the-art results by up to 0.4%. Our results are reproducible, source codes and corpora are available on GitHub.\n    ",
        "submission_date": "2017-12-23T00:00:00",
        "last_modified_date": "2018-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08917",
        "title": "Building a Sentiment Corpus of Tweets in Brazilian Portuguese",
        "authors": [
            "Henrico Bertini Brum",
            "Maria das Gra\u00e7as Volpe Nunes"
        ],
        "abstract": "The large amount of data available in social media, forums and websites motivates researches in several areas of Natural Language Processing, such as sentiment analysis. The popularity of the area due to its subjective and semantic characteristics motivates research on novel methods and approaches for classification. Hence, there is a high demand for datasets on different domains and different languages. This paper introduces TweetSentBR, a sentiment corpora for Brazilian Portuguese manually annotated with 15.000 sentences on TV show domain. The sentences were labeled in three classes (positive, neutral and negative) by seven annotators, following literature guidelines for ensuring reliability on the annotation. We also ran baseline experiments on polarity classification using three machine learning methods, reaching 80.99% on F-Measure and 82.06% on accuracy in binary classification, and 59.85% F-Measure and 64.62% on accuracy on three point classification.\n    ",
        "submission_date": "2017-12-24T00:00:00",
        "last_modified_date": "2017-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08933",
        "title": "Semi-automatic definite description annotation: a first report",
        "authors": [
            "Danillo da Silva Rocha",
            "Alex Gwo Jen Lan",
            "Ivandre Paraboni"
        ],
        "abstract": "Studies in Referring Expression Generation (REG) often make use of corpora of definite descriptions produced by human subjects in controlled experiments. Experiments of this kind, which are essential for the study of reference phenomena and many others, may however include a considerable amount of noise. Human subjects may easily lack attention, or may simply misunderstand the task at hand and, as a result, the elicited data may include large proportions of ambiguous or ill-formed descriptions. In addition to that, REG corpora are usually collected for the study of semantics-related phenomena, and it is often the case that the elicited descriptions (and their input contexts) need to be annotated with their corresponding semantic properties. This, as in many other fields, may require considerable time and skilled annotators. As a means to tackle both kinds of difficulties - poor data quality and high annotation costs - this work discusses a semi-automatic method for the annotation of definite descriptions produced by human subjects in REG data collection experiments. The method makes use of simple rules to establish associations between words and meanings, and is intended to facilitate the design of experiments that produce REG corpora.\n    ",
        "submission_date": "2017-12-24T00:00:00",
        "last_modified_date": "2017-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08992",
        "title": "Leveraging Native Language Speech for Accent Identification using Deep Siamese Networks",
        "authors": [
            "Aditya Siddhant",
            "Preethi Jyothi",
            "Sriram Ganapathy"
        ],
        "abstract": "The problem of automatic accent identification is important for several applications like speaker profiling and recognition as well as for improving speech recognition systems. The accented nature of speech can be primarily attributed to the influence of the speaker's native language on the given speech recording. In this paper, we propose a novel accent identification system whose training exploits speech in native languages along with the accented speech. Specifically, we develop a deep Siamese network-based model which learns the association between accented speech recordings and the native language speech recordings. The Siamese networks are trained with i-vector features extracted from the speech recordings using either an unsupervised Gaussian mixture model (GMM) or a supervised deep neural network (DNN) model. We perform several accent identification experiments using the CSLU Foreign Accented English (FAE) corpus. In these experiments, our proposed approach using deep Siamese networks yield significant relative performance improvements of 15.4 percent on a 10-class accent identification task, over a baseline DNN-based classification system that uses GMM i-vectors. Furthermore, we present a detailed error analysis of the proposed accent identification system.\n    ",
        "submission_date": "2017-12-25T00:00:00",
        "last_modified_date": "2018-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09127",
        "title": "Generative Adversarial Nets for Multiple Text Corpora",
        "authors": [
            "Baiyang Wang",
            "Diego Klabjan"
        ],
        "abstract": "Generative adversarial nets (GANs) have been successfully applied to the artificial generation of image data. In terms of text data, much has been done on the artificial generation of natural language from a single corpus. We consider multiple text corpora as the input data, for which there can be two applications of GANs: (1) the creation of consistent cross-corpus word embeddings given different word embeddings per corpus; (2) the generation of robust bag-of-words document embeddings for each corpora. We demonstrate our GAN models on real-world text data sets from different corpora, and show that embeddings from both models lead to improvements in supervised learning problems.\n    ",
        "submission_date": "2017-12-25T00:00:00",
        "last_modified_date": "2017-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09185",
        "title": "Actionable Email Intent Modeling with Reparametrized RNNs",
        "authors": [
            "Chu-Cheng Lin",
            "Dongyeop Kang",
            "Michael Gamon",
            "Madian Khabsa",
            "Ahmed Hassan Awadallah",
            "Patrick Pantel"
        ],
        "abstract": "Emails in the workplace are often intentional calls to action for its recipients. We propose to annotate these emails for what action its recipient will take. We argue that our approach of action-based annotation is more scalable and theory-agnostic than traditional speech-act-based email intent annotation, while still carrying important semantic and pragmatic information. We show that our action-based annotation scheme achieves good inter-annotator agreement. We also show that we can leverage threaded messages from other domains, which exhibit comparable intents in their conversation, with domain adaptive RAINBOW (Recurrently AttentIve Neural Bag-Of-Words). On a collection of datasets consisting of IRC, Reddit, and email, our reparametrized RNNs outperform common multitask/multidomain approaches on several speech act related tasks. We also experiment with a minimally supervised scenario of email recipient action classification, and find the reparametrized RNNs learn a useful representation.\n    ",
        "submission_date": "2017-12-26T00:00:00",
        "last_modified_date": "2017-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09391",
        "title": "Mapping to Declarative Knowledge for Word Problem Solving",
        "authors": [
            "Subhro Roy",
            "Dan Roth"
        ],
        "abstract": "Math word problems form a natural abstraction to a range of quantitative reasoning problems, such as understanding financial news, sports results, and casualties of war. Solving such problems requires the understanding of several mathematical concepts such as dimensional analysis, subset relationships, etc. In this paper, we develop declarative rules which govern the translation of natural language description of these concepts to math expressions. We then present a framework for incorporating such declarative knowledge into word problem solving. Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression. This provides a way to handle multiple concepts in the same problem while, at the same time, support interpretability of the answer expression. Our method models the mapping to declarative knowledge as a latent variable, thus removing the need for expensive annotations. Experimental evaluation suggests that our domain knowledge based solver outperforms all other systems, and that it generalizes better in the realistic case where the training data it is exposed to is biased in a different way than the test data.\n    ",
        "submission_date": "2017-12-26T00:00:00",
        "last_modified_date": "2017-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09405",
        "title": "Advances in Pre-Training Distributed Word Representations",
        "authors": [
            "Tomas Mikolov",
            "Edouard Grave",
            "Piotr Bojanowski",
            "Christian Puhrsch",
            "Armand Joulin"
        ],
        "abstract": "Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.\n    ",
        "submission_date": "2017-12-26T00:00:00",
        "last_modified_date": "2017-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09444",
        "title": "Letter-Based Speech Recognition with Gated ConvNets",
        "authors": [
            "Vitaliy Liptchinsky",
            "Gabriel Synnaeve",
            "Ronan Collobert"
        ],
        "abstract": "In the recent literature, \"end-to-end\" speech systems often refer to letter-based acoustic models trained in a sequence-to-sequence manner, either via a recurrent model or via a structured output learning approach (such as CTC). In contrast to traditional phone (or senone)-based approaches, these \"end-to-end'' approaches alleviate the need of word pronunciation modeling, and do not require a \"forced alignment\" step at training time. Phone-based approaches remain however state of the art on classical benchmarks. In this paper, we propose a letter-based speech recognition system, leveraging a ConvNet acoustic model. Key ingredients of the ConvNet are Gated Linear Units and high dropout. The ConvNet is trained to map audio sequences to their corresponding letter transcriptions, either via a classical CTC approach, or via a recent variant called ASG. Coupled with a simple decoder at inference time, our system matches the best existing letter-based systems on WSJ (in word error rate), and shows near state of the art performance on LibriSpeech.\n    ",
        "submission_date": "2017-12-22T00:00:00",
        "last_modified_date": "2019-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09509",
        "title": "A Gap-Based Framework for Chinese Word Segmentation via Very Deep Convolutional Networks",
        "authors": [
            "Zhiqing Sun",
            "Gehui Shen",
            "Zhihong Deng"
        ],
        "abstract": "Most previous approaches to Chinese word segmentation can be roughly classified into character-based and word-based methods. The former regards this task as a sequence-labeling problem, while the latter directly segments character sequence into words. However, if we consider segmenting a given sentence, the most intuitive idea is to predict whether to segment for each gap between two consecutive characters, which in comparison makes previous approaches seem too complex. Therefore, in this paper, we propose a gap-based framework to implement this intuitive idea. Moreover, very deep convolutional neural networks, namely, ResNets and DenseNets, are exploited in our experiments. Results show that our approach outperforms the best character-based and word-based methods on 5 benchmarks, without any further post-processing module (e.g. Conditional Random Fields) nor beam search.\n    ",
        "submission_date": "2017-12-27T00:00:00",
        "last_modified_date": "2017-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09518",
        "title": "Improving Text Normalization by Optimizing Nearest Neighbor Matching",
        "authors": [
            "Salman Ahmad Ansari",
            "Usman Zafar",
            "Asim Karim"
        ],
        "abstract": "Text normalization is an essential task in the processing and analysis of social media that is dominated with informal writing. It aims to map informal words to their intended standard forms. Previously proposed text normalization approaches typically require manual selection of parameters for improved performance. In this paper, we present an automatic optimizationbased nearest neighbor matching approach for text normalization. This approach is motivated by the observation that text normalization is essentially a matching problem and nearest neighbor matching with an adaptive similarity function is the most direct procedure for it. Our similarity function incorporates weighted contributions of contextual, string, and phonetic similarity, and the nearest neighbor matching involves a minimum similarity threshold. These four parameters are tuned efficiently using grid search. We evaluate the performance of our approach on two benchmark datasets. The results demonstrate that parameter tuning on small sized labeled datasets produce state-of-the-art text normalization performances. Thus, this approach allows practically easy construction of evolving domain-specific normalization lexicons\n    ",
        "submission_date": "2017-12-27T00:00:00",
        "last_modified_date": "2017-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09662",
        "title": "CNN Is All You Need",
        "authors": [
            "Qiming Chen",
            "Ren Wu"
        ],
        "abstract": "The Convolution Neural Network (CNN) has demonstrated the unique advantage in audio, image and text learning; recently it has also challenged Recurrent Neural Networks (RNNs) with long short-term memory cells (LSTM) in sequence-to-sequence learning, since the computations involved in CNN are easily parallelizable whereas those involved in RNN are mostly sequential, leading to a performance bottleneck. However, unlike RNN, the native CNN lacks the history sensitivity required for sequence transformation; therefore enhancing the sequential order awareness, or position-sensitivity, becomes the key to make CNN the general deep learning model. In this work we introduce an extended CNN model with strengthen position-sensitivity, called PoseNet. A notable feature of PoseNet is the asymmetric treatment of position information in the encoder and the decoder. Experiments shows that PoseNet allows us to improve the accuracy of CNN based sequence-to-sequence learning significantly, achieving around 33-36 BLEU scores on the WMT 2014 English-to-German translation task, and around 44-46 BLEU scores on the English-to-French translation task.\n    ",
        "submission_date": "2017-12-27T00:00:00",
        "last_modified_date": "2017-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09827",
        "title": "A Syntactic Approach to Domain-Specific Automatic Question Generation",
        "authors": [
            "Guy Danon",
            "Mark Last"
        ],
        "abstract": "Factoid questions are questions that require short fact-based answers. Automatic generation (AQG) of factoid questions from a given text can contribute to educational activities, interactive question answering systems, search engines, and other applications. The goal of our research is to generate factoid source-question-answer triplets based on a specific domain. We propose a four-component pipeline, which obtains as input a training corpus of domain-specific documents, along with a set of declarative sentences from the same domain, and generates as output a set of factoid questions that refer to the source sentences but are slightly different from them, so that a question-answering system or a person can be asked a question that requires a deeper understanding and knowledge than a simple word-matching. Contrary to existing domain-specific AQG systems that utilize the template-based approach to question generation, we propose to transform each source sentence into a set of questions by applying a series of domain-independent rules (a syntactic-based approach). Our pipeline was evaluated in the domain of cyber security using a series of experiments on each component of the pipeline separately and on the end-to-end system. The proposed approach generated a higher percentage of acceptable questions than a prior state-of-the-art AQG system.\n    ",
        "submission_date": "2017-12-28T00:00:00",
        "last_modified_date": "2017-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09943",
        "title": "Toward Continual Learning for Conversational Agents",
        "authors": [
            "Sungjin Lee"
        ],
        "abstract": "While end-to-end neural conversation models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, they typically require an enormous amount of data due to the lack of modularity. Previous studies adopted a hybrid approach with knowledge-based components either to abstract out domain-specific information or to augment data to cover more diverse patterns. On the contrary, we propose to directly address the problem using recent developments in the space of continual learning for neural models. Specifically, we adopt a domain-independent neural conversational model and introduce a novel neural continual learning algorithm that allows a conversational agent to accumulate skills across different tasks in a data-efficient way. To the best of our knowledge, this is the first work that applies continual learning to conversation systems. We verified the efficacy of our method through a conversational skill transfer from either synthetic dialogs or human-human dialogs to human-computer conversations in a customer support domain.\n    ",
        "submission_date": "2017-12-28T00:00:00",
        "last_modified_date": "2018-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.10054",
        "title": "Corpus specificity in LSA and Word2vec: the role of out-of-domain documents",
        "authors": [
            "Edgar Altszyler",
            "Mariano Sigman",
            "Diego Fernandez Slezak"
        ],
        "abstract": "Latent Semantic Analysis (LSA) and Word2vec are some of the most widely used word embeddings. Despite the popularity of these techniques, the precise mechanisms by which they acquire new semantic relations between words remain unclear. In the present article we investigate whether LSA and Word2vec capacity to identify relevant semantic dimensions increases with size of corpus. One intuitive hypothesis is that the capacity to identify relevant dimensions should increase as the amount of data increases. However, if corpus size grow in topics which are not specific to the domain of interest, signal to noise ratio may weaken. Here we set to examine and distinguish these alternative hypothesis. To investigate the effect of corpus specificity and size in word-embeddings we study two ways for progressive elimination of documents: the elimination of random documents vs. the elimination of documents unrelated to a specific task. We show that Word2vec can take advantage of all the documents, obtaining its best performance when it is trained with the whole corpus. On the contrary, the specialization (removal of out-of-domain documents) of the training corpus, accompanied by a decrease of dimensionality, can increase LSA word-representation quality while speeding up the processing time. Furthermore, we show that the specialization without the decrease in LSA dimensionality can produce a strong performance reduction in specific tasks. From a cognitive-modeling point of view, we point out that LSA's word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations, whereas Word2vec does.\n    ",
        "submission_date": "2017-12-28T00:00:00",
        "last_modified_date": "2017-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.10066",
        "title": "Disentangled Representations for Manipulation of Sentiment in Text",
        "authors": [
            "Maria Larsson",
            "Amanda Nilsson",
            "Mikael K\u00e5geb\u00e4ck"
        ],
        "abstract": "The ability to change arbitrary aspects of a text while leaving the core message intact could have a strong impact in fields like marketing and politics by enabling e.g. automatic optimization of message impact and personalized language adapted to the receiver's profile. In this paper we take a first step towards such a system by presenting an algorithm that can manipulate the sentiment of a text while preserving its semantics using disentangled representations. Validation is performed by examining trajectories in embedding space and analyzing transformed sentences for semantic preservation while expression of desired sentiment shift.\n    ",
        "submission_date": "2017-12-22T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.10190",
        "title": "Detecting Cross-Lingual Plagiarism Using Simulated Word Embeddings",
        "authors": [
            "Victor Thompson"
        ],
        "abstract": "Cross-lingual plagiarism (CLP) occurs when texts written in one language are translated into a different language and used without acknowledging the original sources. One of the most common methods for detecting CLP requires online machine translators (such as Google or Microsoft translate) which are not always available, and given that plagiarism detection typically involves large document comparison, the amount of translations required would overwhelm an online machine translator, especially when detecting plagiarism over the web. In addition, when translated texts are replaced with their synonyms, using online machine translators to detect CLP would result in poor performance. This paper addresses the problem of cross-lingual plagiarism detection (CLPD) by proposing a model that uses simulated word embeddings to reproduce the predictions of an online machine translator (Google translate) when detecting CLP. The simulated embeddings comprise of translated words in different languages mapped in a common space, and replicated to increase the prediction probability of retrieving the translations of a word (and their synonyms) from the model. Unlike most existing models, the proposed model does not require parallel corpora, and accommodates multiple languages (multi-lingual). We demonstrated the effectiveness of the proposed model in detecting CLP in standard datasets that contain CLP cases, and evaluated its performance against a state-of-the-art baseline that relies on online machine translator (T+MA model). Evaluation results revealed that the proposed model is not only effective in detecting CLP, it outperformed the baseline. The results indicate that CLP could be detected with state-of-the-art performances by leveraging the prediction accuracy of an internet translator with word embeddings, without relying on internet translators.\n    ",
        "submission_date": "2017-12-29T00:00:00",
        "last_modified_date": "2018-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.10224",
        "title": "Scalable Multi-Domain Dialogue State Tracking",
        "authors": [
            "Abhinav Rastogi",
            "Dilek Hakkani-Tur",
            "Larry Heck"
        ],
        "abstract": "Dialogue state tracking (DST) is a key component of task-oriented dialogue systems. DST estimates the user's goal at each user turn given the interaction until then. State of the art approaches for state tracking rely on deep learning methods, and represent dialogue state as a distribution over all possible slot values for each slot present in the ontology. Such a representation is not scalable when the set of possible values are unbounded (e.g., date, time or location) or dynamic (e.g., movies or usernames). Furthermore, training of such models requires labeled data, where each user turn is annotated with the dialogue state, which makes building models for new domains challenging. In this paper, we present a scalable multi-domain deep learning based approach for DST. We introduce a novel framework for state tracking which is independent of the slot value set, and represent the dialogue state as a distribution over a set of values of interest (candidate set) derived from the dialogue history or knowledge. Restricting these candidate sets to be bounded in size addresses the problem of slot-scalability. Furthermore, by leveraging the slot-independent architecture and transfer learning, we show that our proposed approach facilitates quick adaptation to new domains.\n    ",
        "submission_date": "2017-12-29T00:00:00",
        "last_modified_date": "2018-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00185",
        "title": "Self-Taught Convolutional Neural Networks for Short Text Clustering",
        "authors": [
            "Jiaming Xu",
            "Bo Xu",
            "Peng Wang",
            "Suncong Zheng",
            "Guanhua Tian",
            "Jun Zhao",
            "Bo Xu"
        ],
        "abstract": "Short text clustering is a challenging problem due to its sparseness of text representation. Here we propose a flexible Self-Taught Convolutional neural network framework for Short Text Clustering (dubbed STC^2), which can flexibly and successfully incorporate more useful semantic features and learn non-biased deep text representation in an unsupervised manner. In our framework, the original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction methods. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations. Extensive experimental results demonstrate that the proposed framework is effective, flexible and outperform several popular clustering methods when tested on three public short text datasets.\n    ",
        "submission_date": "2017-01-01T00:00:00",
        "last_modified_date": "2017-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00289",
        "title": "Integrating sentiment and social structure to determine preference alignments: The Irish Marriage Referendum",
        "authors": [
            "David J.P. O'Sullivan",
            "Guillermo Gardu\u00f1o-Hern\u00e1ndez",
            "James P. Gleeson",
            "Mariano Beguerisse-D\u00edaz"
        ],
        "abstract": "We examine the relationship between social structure and sentiment through the analysis of a large collection of tweets about the Irish Marriage Referendum of 2015. We obtain the sentiment of every tweet with the hashtags #marref and #marriageref that was posted in the days leading to the referendum, and construct networks to aggregate sentiment and use it to study the interactions among users. Our results show that the sentiment of mention tweets posted by users is correlated with the sentiment of received mentions, and there are significantly more connections between users with similar sentiment scores than among users with opposite scores in the mention and follower networks. We combine the community structure of the two networks with the activity level of the users and sentiment scores to find groups of users who support voting `yes' or `no' in the referendum. There were numerous conversations between users on opposing sides of the debate in the absence of follower connections, which suggests that there were efforts by some users to establish dialogue and debate across ideological divisions. Our analysis shows that social structure can be integrated successfully with sentiment to analyse and understand the disposition of social media users. These results have potential applications in the integration of data and meta-data to study opinion dynamics, public opinion modelling, and polling.\n    ",
        "submission_date": "2017-01-01T00:00:00",
        "last_modified_date": "2017-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00660",
        "title": "Ambiguity and Incomplete Information in Categorical Models of Language",
        "authors": [
            "Dan Marsden"
        ],
        "abstract": "We investigate notions of ambiguity and partial information in categorical distributional models of natural language. Probabilistic ambiguity has previously been studied using Selinger's CPM construction. This construction works well for models built upon vector spaces, as has been shown in quantum computational applications. Unfortunately, it doesn't seem to provide a satisfactory method for introducing mixing in other compact closed categories such as the category of sets and binary relations. We therefore lack a uniform strategy for extending a category to model imprecise linguistic information.\n",
        "submission_date": "2017-01-03T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00749",
        "title": "Pyndri: a Python Interface to the Indri Search Engine",
        "authors": [
            "Christophe Van Gysel",
            "Evangelos Kanoulas",
            "Maarten de Rijke"
        ],
        "abstract": "We introduce pyndri, a Python interface to the Indri search engine. Pyndri allows to access Indri indexes from Python at two levels: (1) dictionary and tokenized document collection, (2) evaluating queries on the index. We hope that with the release of pyndri, we will stimulate reproducible, open and fast-paced IR research.\n    ",
        "submission_date": "2017-01-03T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.00991",
        "title": "World Literature According to Wikipedia: Introduction to a DBpedia-Based Framework",
        "authors": [
            "Christoph Hube",
            "Frank Fischer",
            "Robert J\u00e4schke",
            "Gerhard Lauer",
            "Mads Rosendahl Thomsen"
        ],
        "abstract": "Among the manifold takes on world literature, it is our goal to contribute to the discussion from a digital point of view by analyzing the representation of world literature in Wikipedia with its millions of articles in hundreds of languages. As a preliminary, we introduce and compare three different approaches to identify writers on Wikipedia using data from DBpedia, a community project with the goal of extracting and providing structured information from Wikipedia. Equipped with our basic set of writers, we analyze how they are represented throughout the 15 biggest Wikipedia language versions. We combine intrinsic measures (mostly examining the connectedness of articles) with extrinsic ones (analyzing how often articles are frequented by readers) and develop methods to evaluate our results. The better part of our findings seems to convey a rather conservative, old-fashioned version of world literature, but a version derived from reproducible facts revealing an implicit literary canon based on the editing and reading behavior of millions of people. While still having to solve some known issues, the introduced methods will help us build an observatory of world literature to further investigate its representativeness and biases.\n    ",
        "submission_date": "2017-01-04T00:00:00",
        "last_modified_date": "2017-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.01417",
        "title": "Exploration of Proximity Heuristics in Length Normalization",
        "authors": [
            "Pranav Agrawal"
        ],
        "abstract": "Ranking functions used in information retrieval are primarily used in the search engines and they are often adopted for various language processing applications. However, features used in the construction of ranking functions should be analyzed before applying it on a data set. This paper gives guidelines on construction of generalized ranking functions with application-dependent features. The paper prescribes a specific case of a generalized function for recommendation system using feature engineering guidelines on the given data set. The behavior of both generalized and specific functions are studied and implemented on the unstructured textual data. The proximity feature based ranking function has outperformed by 52% from regular BM25.\n    ",
        "submission_date": "2017-01-05T00:00:00",
        "last_modified_date": "2017-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.02163",
        "title": "Just an Update on PMING Distance for Web-based Semantic Similarity in Artificial Intelligence and Data Mining",
        "authors": [
            "Valentina Franzoni"
        ],
        "abstract": "One of the main problems that emerges in the classic approach to semantics is the difficulty in acquisition and maintenance of ontologies and semantic annotations. On the other hand, the Internet explosion and the massive diffusion of mobile smart devices lead to the creation of a worldwide system, which information is daily checked and fueled by the contribution of millions of users who interacts in a collaborative way. Search engines, continually exploring the Web, are a natural source of information on which to base a modern approach to semantic annotation. A promising idea is that it is possible to generalize the semantic similarity, under the assumption that semantically similar terms behave similarly, and define collaborative proximity measures based on the indexing information returned by search engines. The PMING Distance is a proximity measure used in data mining and information retrieval, which collaborative information express the degree of relationship between two terms, using only the number of documents returned as result for a query on a search engine. In this work, the PMINIG Distance is updated, providing a novel formal algebraic definition, which corrects previous works. The novel point of view underlines the features of the PMING to be a locally normalized linear combination of the Pointwise Mutual Information and Normalized Google Distance. The analyzed measure dynamically reflects the collaborative change made on the web resources.\n    ",
        "submission_date": "2017-01-09T00:00:00",
        "last_modified_date": "2017-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03051",
        "title": "Efficient Twitter Sentiment Classification using Subjective Distant Supervision",
        "authors": [
            "Tapan Sahni",
            "Chinmay Chandak",
            "Naveen Reddy Chedeti",
            "Manish Singh"
        ],
        "abstract": "As microblogging services like Twitter are becoming more and more influential in today's globalised world, its facets like sentiment analysis are being extensively studied. We are no longer constrained by our own opinion. Others opinions and sentiments play a huge role in shaping our perspective. In this paper, we build on previous works on Twitter sentiment analysis using Distant Supervision. The existing approach requires huge computation resource for analysing large number of tweets. In this paper, we propose techniques to speed up the computation process for sentiment analysis. We use tweet subjectivity to select the right training samples. We also introduce the concept of EFWS (Effective Word Score) of a tweet that is derived from polarity scores of frequently used words, which is an additional heuristic that can be used to speed up the sentiment classification with standard machine learning algorithms. We performed our experiments using 1.6 million tweets. Experimental evaluations show that our proposed technique is more efficient and has higher accuracy compared to previously proposed methods. We achieve overall accuracies of around 80% (EFWS heuristic gives an accuracy around 85%) on a training dataset of 100K tweets, which is half the size of the dataset used for the baseline model. The accuracy of our proposed model is 2-3% higher than the baseline model, and the model effectively trains at twice the speed of the baseline model.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03126",
        "title": "Attention-Based Multimodal Fusion for Video Description",
        "authors": [
            "Chiori Hori",
            "Takaaki Hori",
            "Teng-Yok Lee",
            "Kazuhiro Sumi",
            "John R. Hershey",
            "Tim K. Marks"
        ],
        "abstract": "Currently successful methods for video description are based on encoder-decoder sentence generation using recur-rent neural networks (RNNs). Recent work has shown the advantage of integrating temporal and/or spatial attention mechanisms into these models, in which the decoder net-work predicts each word in the description by selectively giving more weight to encoded features from specific time frames (temporal attention) or to features from specific spatial regions (spatial attention). In this paper, we propose to expand the attention model to selectively attend not just to specific times or spatial regions, but to specific modalities of input such as image features, motion features, and audio features. Our new modality-dependent attention mechanism, which we call multimodal attention, provides a natural way to fuse multimodal information for video description. We evaluate our method on the Youtube2Text dataset, achieving results that are competitive with current state of the art. More importantly, we demonstrate that our model incorporating multimodal attention as well as temporal attention significantly outperforms the model that uses temporal attention alone.\n    ",
        "submission_date": "2017-01-11T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03577",
        "title": "Kernel Approximation Methods for Speech Recognition",
        "authors": [
            "Avner May",
            "Alireza Bagheri Garakani",
            "Zhiyun Lu",
            "Dong Guo",
            "Kuan Liu",
            "Aur\u00e9lien Bellet",
            "Linxi Fan",
            "Michael Collins",
            "Daniel Hsu",
            "Brian Kingsbury",
            "Michael Picheny",
            "Fei Sha"
        ],
        "abstract": "We study large-scale kernel methods for acoustic modeling in speech recognition and compare their performance to deep neural networks (DNNs). We perform experiments on four speech recognition datasets, including the TIMIT and Broadcast News benchmark tasks, and compare these two types of models on frame-level performance metrics (accuracy, cross-entropy), as well as on recognition metrics (word/character error rate). In order to scale kernel methods to these large datasets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, in order to reduce the number of random features required by kernel models, we propose a simple but effective method for feature selection. The method is able to explore a large number of non-linear features while maintaining a compact model more efficiently than existing approaches. Second, we present a number of frame-level metrics which correlate very strongly with recognition performance when computed on the heldout set; we take advantage of these correlations by monitoring these metrics during training in order to decide when to stop learning. This technique can noticeably improve the recognition performance of both DNN and kernel models, while narrowing the gap between them. Additionally, we show that the linear bottleneck method of Sainath et al. (2013) improves the performance of our kernel models significantly, in addition to speeding up training and making the models more compact. Together, these three methods dramatically improve the performance of kernel acoustic models, making their performance comparable to DNNs on the tasks we explored.\n    ",
        "submission_date": "2017-01-13T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03947",
        "title": "Balancing Novelty and Salience: Adaptive Learning to Rank Entities for Timeline Summarization of High-impact Events",
        "authors": [
            "Tuan Tran",
            "Claudia Nieder\u00e9e",
            "Nattiya Kanhabua",
            "Ujwal Gadiraju",
            "Avishek Anand"
        ],
        "abstract": "Long-running, high-impact events such as the Boston Marathon bombing often develop through many stages and involve a large number of entities in their unfolding. Timeline summarization of an event by key sentences eases story digestion, but does not distinguish between what a user remembers and what she might want to re-check. In this work, we present a novel approach for timeline summarization of high-impact events, which uses entities instead of sentences for summarizing the event at each individual point in time. Such entity summaries can serve as both (1) important memory cues in a retrospective event consideration and (2) pointers for personalized event exploration. In order to automatically create such summaries, it is crucial to identify the \"right\" entities for inclusion. We propose to learn a ranking function for entities, with a dynamically adapted trade-off between the in-document salience of entities and the informativeness of entities across documents, i.e., the level of new information associated with an entity for a time point under consideration. Furthermore, for capturing collective attention for an entity we use an innovative soft labeling approach based on Wikipedia. Our experiments on a real large news datasets confirm the effectiveness of the proposed methods.\n    ",
        "submission_date": "2017-01-14T00:00:00",
        "last_modified_date": "2017-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.03980",
        "title": "DyNet: The Dynamic Neural Network Toolkit",
        "authors": [
            "Graham Neubig",
            "Chris Dyer",
            "Yoav Goldberg",
            "Austin Matthews",
            "Waleed Ammar",
            "Antonios Anastasopoulos",
            "Miguel Ballesteros",
            "David Chiang",
            "Daniel Clothiaux",
            "Trevor Cohn",
            "Kevin Duh",
            "Manaal Faruqui",
            "Cynthia Gan",
            "Dan Garrette",
            "Yangfeng Ji",
            "Lingpeng Kong",
            "Adhiguna Kuncoro",
            "Gaurav Kumar",
            "Chaitanya Malaviya",
            "Paul Michel",
            "Yusuke Oda",
            "Matthew Richardson",
            "Naomi Saphra",
            "Swabha Swayamdipta",
            "Pengcheng Yin"
        ],
        "abstract": "We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet's dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet's speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license and available at ",
        "submission_date": "2017-01-15T00:00:00",
        "last_modified_date": "2017-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04039",
        "title": "The Birth of Collective Memories: Analyzing Emerging Entities in Text Streams",
        "authors": [
            "David Graus",
            "Daan Odijk",
            "Maarten de Rijke"
        ],
        "abstract": "We study how collective memories are formed online. We do so by tracking entities that emerge in public discourse, that is, in online text streams such as social media and news streams, before they are incorporated into Wikipedia, which, we argue, can be viewed as an online place for collective memory. By tracking how entities emerge in public discourse, i.e., the temporal patterns between their first mention in online text streams and subsequent incorporation into collective memory, we gain insights into how the collective remembrance process happens online. Specifically, we analyze nearly 80,000 entities as they emerge in online text streams before they are incorporated into Wikipedia. The online text streams we use for our analysis comprise of social media and news streams, and span over 579 million documents in a timespan of 18 months. We discover two main emergence patterns: entities that emerge in a \"bursty\" fashion, i.e., that appear in public discourse without a precedent, blast into activity and transition into collective memory. Other entities display a \"delayed\" pattern, where they appear in public discourse, experience a period of inactivity, and then resurface before transitioning into our cultural collective memory.\n    ",
        "submission_date": "2017-01-15T00:00:00",
        "last_modified_date": "2017-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.04292",
        "title": "Semantic classifier approach to document classification",
        "authors": [
            "Piotr Borkowski",
            "Krzysztof Ciesielski",
            "Mieczys\u0142aw A. K\u0142opotek"
        ],
        "abstract": "In this paper we propose a new document classification method, bridging discrepancies (so-called semantic gap) between the training set and the application sets of textual data. We demonstrate its superiority over classical text classification approaches, including traditional classifier ensembles. The method consists in combining a document categorization technique with a single classifier or a classifier ensemble (SEMCOM algorithm - Committee with Semantic Categorizer).\n    ",
        "submission_date": "2017-01-16T00:00:00",
        "last_modified_date": "2017-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.05311",
        "title": "Semantic Evolutionary Concept Distances for Effective Information Retrieval in Query Expansion",
        "authors": [
            "Valentina Franzoni",
            "Yuanxi Li",
            "Clement H.C.Leung",
            "Alfredo Milani"
        ],
        "abstract": "In this work several semantic approaches to concept-based query expansion and reranking schemes are studied and compared with different ontology-based expansion methods in web document search and retrieval. In particular, we focus on concept-based query expansion schemes, where, in order to effectively increase the precision of web document retrieval and to decrease the users browsing time, the main goal is to quickly provide users with the most suitable query expansion. Two key tasks for query expansion in web document retrieval are to find the expansion candidates, as the closest concepts in web document domain, and to rank the expanded queries properly. The approach we propose aims at improving the expansion phase for better web document retrieval and precision. The basic idea is to measure the distance between candidate concepts using the PMING distance, a collaborative semantic proximity measure, i.e. a measure which can be computed by using statistical results from web search engine. Experiments show that the proposed technique can provide users with more satisfying expansion results and improve the quality of web document retrieval.\n    ",
        "submission_date": "2017-01-19T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.05334",
        "title": "Fuzzy Ontology-Based Sentiment Analysis of Transportation and City Feature Reviews for Safe Traveling",
        "authors": [
            "Farman Ali",
            "D. Kwak",
            "Pervez Khan",
            "S.M. Riazul Islam",
            "K.H. Kim",
            "K.S. Kwak"
        ],
        "abstract": "Traffic congestion is rapidly increasing in urban areas, particularly in mega cities. To date, there exist a few sensor network based systems to address this problem. However, these techniques are not suitable enough in terms of monitoring an entire transportation system and delivering emergency services when needed. These techniques require real-time data and intelligent ways to quickly determine traffic activity from useful information. In addition, these existing systems and websites on city transportation and travel rely on rating scores for different factors (e.g., safety, low crime rate, cleanliness, etc.). These rating scores are not efficient enough to deliver precise information, whereas reviews or tweets are significant, because they help travelers and transportation administrators to know about each aspect of the city. However, it is difficult for travelers to read, and for transportation systems to process, all reviews and tweets to obtain expressive sentiments regarding the needs of the city. The optimum solution for this kind of problem is analyzing the information available on social network platforms and performing sentiment analysis. On the other hand, crisp ontology-based frameworks cannot extract blurred information from tweets and reviews; therefore, they produce inadequate results. In this regard, this paper proposes fuzzy ontology-based sentiment analysis and SWRL rule-based decision-making to monitor transportation activities and to make a city- feature polarity map for travelers. This system retrieves reviews and tweets related to city features and transportation activities. The feature opinions are extracted from these retrieved data, and then fuzzy ontology is used to determine the transportation and city-feature polarity. A fuzzy ontology and an intelligent system prototype are developed by using Prot\u00e9g\u00e9 OWL and Java, respectively.\n    ",
        "submission_date": "2017-01-19T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.05847",
        "title": "End-To-End Visual Speech Recognition With LSTMs",
        "authors": [
            "Stavros Petridis",
            "Zuwei Li",
            "Maja Pantic"
        ],
        "abstract": "Traditional visual speech recognition systems consist of two stages, feature extraction and classification. Recently, several deep learning approaches have been presented which automatically extract features from the mouth images and aim to replace the feature extraction stage. However, research on joint learning of features and classification is very limited. In this work, we present an end-to-end visual speech recognition system based on Long-Short Memory (LSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and perform classification and also achieves state-of-the-art performance in visual speech classification. The model consists of two streams which extract features directly from the mouth and difference images, respectively. The temporal dynamics in each stream are modelled by an LSTM and the fusion of the two streams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement of 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the CUAVE database when compared with other methods which use a similar visual front-end.\n    ",
        "submission_date": "2017-01-20T00:00:00",
        "last_modified_date": "2017-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.06233",
        "title": "What the Language You Tweet Says About Your Occupation",
        "authors": [
            "Tianran Hu",
            "Haoyuan Xiao",
            "Thuy-vy Thi Nguyen",
            "Jiebo Luo"
        ],
        "abstract": "Many aspects of people's lives are proven to be deeply connected to their jobs. In this paper, we first investigate the distinct characteristics of major occupation categories based on tweets. From multiple social media platforms, we gather several types of user information. From users' LinkedIn webpages, we learn their proficiencies. To overcome the ambiguity of self-reported information, a soft clustering approach is applied to extract occupations from crowd-sourced data. Eight job categories are extracted, including Marketing, Administrator, Start-up, Editor, Software Engineer, Public Relation, Office Clerk, and Designer. Meanwhile, users' posts on Twitter provide cues for understanding their linguistic styles, interests, and personalities. Our results suggest that people of different jobs have unique tendencies in certain language styles and interests. Our results also clearly reveal distinctive levels in terms of Big Five Traits for different jobs. Finally, a classifier is built to predict job types based on the features extracted from tweets. A high accuracy indicates a strong discrimination power of language features for job prediction task.\n    ",
        "submission_date": "2017-01-22T00:00:00",
        "last_modified_date": "2017-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.06279",
        "title": "dna2vec: Consistent vector representations of variable-length k-mers",
        "authors": [
            "Patrick Ng"
        ],
        "abstract": "One of the ubiquitous representation of long DNA sequence is dividing it into shorter k-mer components. Unfortunately, the straightforward vector encoding of k-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse yet, the distance between any pair of one-hot vectors is equidistant. This is particularly problematic when applying the latest machine learning algorithms to solve problems in biological sequence analysis. In this paper, we propose a novel method to train distributed representations of variable-length k-mers. Our method is based on the popular word embedding model word2vec, which is trained on a shallow two-layer neural network. Our experiments provide evidence that the summing of dna2vec vectors is akin to nucleotides concatenation. We also demonstrate that there is correlation between Needleman-Wunsch similarity score and cosine similarity of dna2vec vectors.\n    ",
        "submission_date": "2017-01-23T00:00:00",
        "last_modified_date": "2017-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.06538",
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
        "authors": [
            "Noam Shazeer",
            "Azalia Mirhoseini",
            "Krzysztof Maziarz",
            "Andy Davis",
            "Quoc Le",
            "Geoffrey Hinton",
            "Jeff Dean"
        ],
        "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.\n    ",
        "submission_date": "2017-01-23T00:00:00",
        "last_modified_date": "2017-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.07795",
        "title": "Match-Tensor: a Deep Relevance Model for Search",
        "authors": [
            "Aaron Jaech",
            "Hetunandan Kamisetty",
            "Eric Ringger",
            "Charlie Clarke"
        ],
        "abstract": "The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models.\n    ",
        "submission_date": "2017-01-26T00:00:00",
        "last_modified_date": "2017-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.07955",
        "title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic and Visualize Its Change Over Time",
        "authors": [
            "Syed Mehedi Hasan Nirob",
            "Md. Kazi Nayeem",
            "Md. Saiful Islam"
        ],
        "abstract": "Trending topic of newspapers is an indicator to understand the situation of a country and also a way to evaluate the particular newspaper. This paper represents a model describing few techniques to select trending topics from Bangla Newspaper. Topics that are discussed more frequently than other in Bangla newspaper will be marked and how a very famous topic loses its importance with the change of time and another topic takes its place will be demonstrated. Data from two popular Bangla Newspaper with date and time were collected. Statistical analysis was performed after on these data after preprocessing. Popular and most used keywords were extracted from the stream of Bangla keyword with this analysis. This model can also cluster category wise news trend or a list of news trend in daily or weekly basis with enough data. A pattern can be found on their news trend too. Comparison among past news trend of Bangla newspapers will give a visualization of the situation of Bangladesh. This visualization will be helpful to predict future trending topics of Bangla Newspaper.\n    ",
        "submission_date": "2017-01-27T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08156",
        "title": "A Comprehensive Survey on Bengali Phoneme Recognition",
        "authors": [
            "Sadia Tasnim Swarna",
            "Shamim Ehsan",
            "Md. Saiful Islam",
            "Marium E Jannat"
        ],
        "abstract": "Hidden Markov model based various phoneme recognition methods for Bengali language is reviewed. Automatic phoneme recognition for Bengali language using multilayer neural network is reviewed. Usefulness of multilayer neural network over single layer neural network is discussed. Bangla phonetic feature table construction and enhancement for Bengali speech recognition is also discussed. Comparison among these methods is discussed.\n    ",
        "submission_date": "2017-01-27T00:00:00",
        "last_modified_date": "2018-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08229",
        "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health",
        "authors": [
            "Danielle Mowery",
            "Craig Bryan",
            "Mike Conway"
        ],
        "abstract": "The utility of Twitter data as a medium to support population-level mental health monitoring is not well understood. In an effort to better understand the predictive power of supervised machine learning classifiers and the influence of feature sets for efficiently classifying depression-related tweets on a large-scale, we conducted two feature study experiments. In the first experiment, we assessed the contribution of feature groups such as lexical information (e.g., unigrams) and emotions (e.g., strongly negative) using a feature ablation study. In the second experiment, we determined the percentile of top ranked features that produced the optimal classification performance by applying a three-step feature elimination approach. In the first experiment, we observed that lexical features are critical for identifying depressive symptoms, specifically for depressed mood (-35 points) and for disturbed sleep (-43 points). In the second experiment, we observed that the optimal F1-score performance of top ranked features in percentiles variably ranged across classes e.g., fatigue or loss of energy (5th percentile, 288 features) to depressed mood (55th percentile, 3,168 features) suggesting there is no consistent count of features for predicting depressive-related tweets. We conclude that simple lexical features and reduced feature sets can produce comparable results to larger feature sets.\n    ",
        "submission_date": "2017-01-28T00:00:00",
        "last_modified_date": "2017-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08269",
        "title": "Systems of natural-language-facilitated human-robot cooperation: A review",
        "authors": [
            "Rui Liu",
            "Xiaoli Zhang"
        ],
        "abstract": "Natural-language-facilitated human-robot cooperation (NLC), in which natural language (NL) is used to share knowledge between a human and a robot for conducting intuitive human-robot cooperation (HRC), is continuously developing in the recent decade. Currently, NLC is used in several robotic domains such as manufacturing, daily assistance and health caregiving. It is necessary to summarize current NLC-based robotic systems and discuss the future developing trends, providing helpful information for future NLC research. In this review, we first analyzed the driving forces behind the NLC research. Regarding to a robot s cognition level during the cooperation, the NLC implementations then were categorized into four types {NL-based control, NL-based robot training, NL-based task execution, NL-based social companion} for comparison and discussion. Last based on our perspective and comprehensive paper review, the future research trends were discussed.\n    ",
        "submission_date": "2017-01-28T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08706",
        "title": "Document Decomposition of Bangla Printed Text",
        "authors": [
            "Md. Fahad Hasan",
            "Tasmin Afroz",
            "Sabir Ismail",
            "Md. Saiful Islam"
        ],
        "abstract": "Today all kind of information is getting digitized and along with all this digitization, the huge archive of various kinds of documents is being digitized too. We know that, Optical Character Recognition is the method through which, newspapers and other paper documents convert into digital resources. But, it is a fact that this method works on texts only. As a result, if we try to process any document which contains non-textual zones, then we will get garbage texts as output. That is why; in order to digitize documents properly they should be prepossessed carefully. And while preprocessing, segmenting document in different regions according to the category properly is most important. But, the Optical Character Recognition processes available for Bangla language have no such algorithm that can categorize a newspaper/book page fully. So we worked to decompose a document into its several parts like headlines, sub headlines, columns, images etc. And if the input is skewed and rotated, then the input was also deskewed and de-rotated. To decompose any Bangla document we found out the edges of the input image. Then we find out the horizontal and vertical area of every pixel where it lies in. Later on the input image was cut according to these areas. Then we pick each and every sub image and found out their height-width ratio, line height. Then according to these values the sub images were categorized. To deskew the image we found out the skew angle and de skewed the image according to this angle. To de-rotate the image we used the line height, matra line, pixel ratio of matra line.\n    ",
        "submission_date": "2017-01-27T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08756",
        "title": "A Review of Methodologies for Natural-Language-Facilitated Human-Robot Cooperation",
        "authors": [
            "Rui Liu",
            "Xiaoli Zhang"
        ],
        "abstract": "Natural-language-facilitated human-robot cooperation (NLC) refers to using natural language (NL) to facilitate interactive information sharing and task executions with a common goal constraint between robots and humans. Recently, NLC research has received increasing attention. Typical NLC scenarios include robotic daily assistance, robotic health caregiving, intelligent manufacturing, autonomous navigation, and robot social accompany. However, a thorough review, that can reveal latest methodologies to use NL to facilitate human-robot cooperation, is missing. In this review, a comprehensive summary about methodologies for NLC is presented. NLC research includes three main research focuses: NL instruction understanding, NL-based execution plan generation, and knowledge-world mapping. In-depth analyses on theoretical methods, applications, and model advantages and disadvantages are made. Based on our paper review and perspective, potential research directions of NLC are summarized.\n    ",
        "submission_date": "2017-01-30T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08888",
        "title": "Integrating Reviews into Personalized Ranking for Cold Start Recommendation",
        "authors": [
            "Guang-Neng Hu",
            "Xin-Yu Dai"
        ],
        "abstract": "Item recommendation task predicts a personalized ranking over a set of items for each individual user. One paradigm is the rating-based methods that concentrate on explicit feedbacks and hence face the difficulties in collecting them. Meanwhile, the ranking-based methods are presented with rated items and then rank the rated above the unrated. This paradigm takes advantage of widely available implicit feedback. It, however, usually ignores a kind of important information: item reviews. Item reviews not only justify the preferences of users, but also help alleviate the cold-start problem that fails the collaborative filtering. In this paper, we propose two novel and simple models to integrate item reviews into Bayesian personalized ranking. In each model, we make use of text features extracted from item reviews using word embeddings. On top of text features we uncover the review dimensions that explain the variation in users' feedback and these review factors represent a prior preference of users. Experiments on six real-world data sets show the benefits of leveraging item reviews on ranking prediction. We also conduct analyses to understand the proposed models.\n    ",
        "submission_date": "2017-01-31T00:00:00",
        "last_modified_date": "2018-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1701.08954",
        "title": "CommAI: Evaluating the first steps towards a useful general AI",
        "authors": [
            "Marco Baroni",
            "Armand Joulin",
            "Allan Jabri",
            "Germ\u00e0n Kruszewski",
            "Angeliki Lazaridou",
            "Klemen Simonic",
            "Tomas Mikolov"
        ],
        "abstract": "With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal. However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.\n    ",
        "submission_date": "2017-01-31T00:00:00",
        "last_modified_date": "2017-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00210",
        "title": "Foreign-language Reviews: Help or Hindrance?",
        "authors": [
            "Scott A. Hale",
            "Irene Eleta"
        ],
        "abstract": "The number and quality of user reviews greatly affects consumer purchasing decisions. While reviews in all languages are increasing, it is still often the case (especially for non-English speakers) that there are only a few reviews in a person's first language. Using an online experiment, we examine the value that potential purchasers receive from interfaces showing additional reviews in a second language. The results paint a complicated picture with both positive and negative reactions to the inclusion of foreign-language reviews. Roughly 26-28% of subjects clicked to see translations of the foreign-language content when given the opportunity, and those who did so were more likely to select the product with foreign-language reviews than those who did not.\n    ",
        "submission_date": "2017-02-01T00:00:00",
        "last_modified_date": "2017-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00523",
        "title": "Deep Learning the Indus Script",
        "authors": [
            "Satish Palaniappan",
            "Ronojoy Adhikari"
        ],
        "abstract": "Standardized corpora of undeciphered scripts, a necessary starting point for computational epigraphy, requires laborious human effort for their preparation from raw archaeological records. Automating this process through machine learning algorithms can be of significant aid to epigraphical research. Here, we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered Indus script, as found in archaeological artifacts, and returns as output a string of graphemes, suitable for inclusion in a standard corpus. The image is first decomposed into regions using Selective Search and these regions are classified as containing textual and/or graphical information using a convolutional neural network. Regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information. The remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes. This set is finally passed to a second convolutional neural network to classify the graphemes, based on a standard corpus. The classifier can identify the presence or absence of the most frequent Indus grapheme, the \"jar\" sign, with an accuracy of 92%. Our results demonstrate the great potential of deep learning approaches in computational epigraphy and, more generally, in the digital humanities.\n    ",
        "submission_date": "2017-02-02T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00564",
        "title": "Modelling dependency completion in sentence comprehension as a Bayesian hierarchical mixture process: A case study involving Chinese relative clauses",
        "authors": [
            "Shravan Vasishth",
            "Nicolas Chopin",
            "Robin Ryder",
            "Bruno Nicenboim"
        ],
        "abstract": "We present a case-study demonstrating the usefulness of Bayesian hierarchical mixture modelling for investigating cognitive processes. In sentence comprehension, it is widely assumed that the distance between linguistic co-dependents affects the latency of dependency resolution: the longer the distance, the longer the retrieval time (the distance-based account). An alternative theory, direct-access, assumes that retrieval times are a mixture of two distributions: one distribution represents successful retrievals (these are independent of dependency distance) and the other represents an initial failure to retrieve the correct dependent, followed by a reanalysis that leads to successful retrieval. We implement both models as Bayesian hierarchical models and show that the direct-access model explains Chinese relative clause reading time data better than the distance account.\n    ",
        "submission_date": "2017-02-02T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.00956",
        "title": "KU-ISPL Speaker Recognition Systems under Language mismatch condition for NIST 2016 Speaker Recognition Evaluation",
        "authors": [
            "Suwon Shon",
            "Hanseok Ko"
        ],
        "abstract": "Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed speaker recognition system for SRE16 fixed training condition. Data for evaluation trials are collected from outside North America, spoken in Tagalog and Cantonese while training data only is spoken English. Thus, main issue for SRE16 is compensating the discrepancy between different languages. As development dataset which is spoken in Cebuano and Mandarin, we could prepare the evaluation trials through preliminary experiments to compensate the language mismatched condition. Our team developed 4 different approaches to extract i-vectors and applied state-of-the-art techniques as backend. To compensate language mismatch, we investigated and endeavored unique method such as unsupervised language clustering, inter language variability compensation and gender/language dependent score normalization.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.01090",
        "title": "Multi-level computational methods for interdisciplinary research in the HathiTrust Digital Library",
        "authors": [
            "Jaimie Murdock",
            "Colin Allen",
            "Katy B\u00f6rner",
            "Robert Light",
            "Simon McAlister",
            "Andrew Ravenscroft",
            "Robert Rose",
            "Doori Rose",
            "Jun Otsuka",
            "David Bourget",
            "John Lawrence",
            "Chris Reed"
        ],
        "abstract": "We show how faceted search using a combination of traditional classification systems and mixed-membership topic models can go beyond keyword search to inform resource discovery, hypothesis formulation, and argument extraction for interdisciplinary research. Our test domain is the history and philosophy of scientific work on animal mind and cognition. The methods can be generalized to other research areas and ultimately support a system for semi-automatic identification of argument structures. We provide a case study for the application of the methods to the problem of identifying and extracting arguments about anthropomorphism during a critical period in the development of comparative psychology. We show how a combination of classification systems and mixed-membership models trained over large digital libraries can inform resource discovery in this domain. Through a novel approach of \"drill-down\" topic modeling---simultaneously reducing both the size of the corpus and the unit of analysis---we are able to reduce a large collection of fulltext volumes to a much smaller set of pages within six focal volumes containing arguments of interest to historians and philosophers of comparative psychology. The volumes identified in this way did not appear among the first ten results of the keyword search in the HathiTrust digital library and the pages bear the kind of \"close reading\" needed to generate original interpretations that is the heart of scholarly work in the humanities. Zooming back out, we provide a way to place the books onto a map of science originally constructed from very different data and for different purposes. The multilevel approach advances understanding of the intellectual and societal contexts in which writings are interpreted.\n    ",
        "submission_date": "2017-02-03T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02287",
        "title": "Name Disambiguation in Anonymized Graphs using Network Embedding",
        "authors": [
            "Baichuan Zhang",
            "Mohammad Al Hasan"
        ],
        "abstract": "In real-world, our DNA is unique but many people share names. This phenomenon often causes erroneous aggregation of documents of multiple persons who are namesake of one another. Such mistakes deteriorate the performance of document retrieval, web search, and more seriously, cause improper attribution of credit or blame in digital forensic. To resolve this issue, the name disambiguation task is designed which aims to partition the documents associated with a name reference such that each partition contains documents pertaining to a unique real-life person. Existing solutions to this task substantially rely on feature engineering, such as biographical feature extraction, or construction of auxiliary features from Wikipedia. However, for many scenarios, such features may be costly to obtain or unavailable due to the risk of privacy violation. In this work, we propose a novel name disambiguation method. Our proposed method is non-intrusive of privacy because instead of using attributes pertaining to a real-life person, our method leverages only relational data in the form of anonymized graphs. In the methodological aspect, the proposed method uses a novel representation learning model to embed each document in a low dimensional vector space where name disambiguation can be solved by a hierarchical agglomerative clustering algorithm. Our experimental results demonstrate that the proposed method is significantly better than the existing name disambiguation methods working in a similar setting.\n    ",
        "submission_date": "2017-02-08T00:00:00",
        "last_modified_date": "2017-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.02737",
        "title": "Mining User/Movie Preferred Features Based on Reviews for Video Recommendation System",
        "authors": [
            "Xuan-Son Vu",
            "Seong-Bae Park"
        ],
        "abstract": "In this work, we present an approach for mining user preferences and recommendation based on reviews. There have been various studies worked on recommendation problem. However, most of the studies beyond one aspect user generated- content such as user ratings, user feedback and so on to state user preferences. There is a prob- lem in one aspect mining is lacking for stating user preferences. As a demonstration, in collaborative filter recommendation, we try to figure out the preference trend of crowded users, then use that trend to predict current user preference. Therefore, there is a gap between real user preferences and the trend of the crowded people. Additionally, user preferences can be addressed from mining user reviews since user often comment about various aspects of products. To solve this problem, we mainly focus on mining product aspects and user aspects inside user reviews to directly state user preferences. We also take into account Social Network Analysis for cold-start item problem. With cold-start user problem, collaborative filter algorithm is employed in our work. The framework is general enough to be applied to different recommendation domains. Theoretically, our method would achieve a significant enhancement.\n    ",
        "submission_date": "2017-02-09T00:00:00",
        "last_modified_date": "2017-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03274",
        "title": "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning",
        "authors": [
            "Jason D. Williams",
            "Kavosh Asadi",
            "Geoffrey Zweig"
        ],
        "abstract": "End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset, and outperform two commercially deployed customer-facing dialog systems.\n    ",
        "submission_date": "2017-02-10T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03402",
        "title": "Parallel Long Short-Term Memory for Multi-stream Classification",
        "authors": [
            "Mohamed Bouaziz",
            "Mohamed Morchid",
            "Richard Dufour",
            "Georges Linar\u00e8s",
            "Renato De Mori"
        ],
        "abstract": "Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-the-art architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach.\n    ",
        "submission_date": "2017-02-11T00:00:00",
        "last_modified_date": "2017-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.03814",
        "title": "Bilateral Multi-Perspective Matching for Natural Language Sentences",
        "authors": [
            "Zhiguo Wang",
            "Wael Hamza",
            "Radu Florian"
        ],
        "abstract": "Natural language sentence matching is a fundamental technology for a variety of tasks. Previous approaches either match sentences from a single direction or only apply single granular (word-by-word or sentence-by-sentence) matching. In this work, we propose a bilateral multi-perspective matching (BiMPM) model under the \"matching-aggregation\" framework. Given two sentences $P$ and $Q$, our model first encodes them with a BiLSTM encoder. Next, we match the two encoded sentences in two directions $P \\rightarrow Q$ and $P \\leftarrow Q$. In each matching direction, each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fix-length matching vector. Finally, based on the matching vector, the decision is made through a fully connected layer. We evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks.\n    ",
        "submission_date": "2017-02-13T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.05512",
        "title": "soc2seq: Social Embedding meets Conversation Model",
        "authors": [
            "Parminder Bhatia",
            "Marsal Gavalda",
            "Arash Einolghozati"
        ],
        "abstract": "While liking or upvoting a post on a mobile app is easy to do, replying with a written note is much more difficult, due to both the cognitive load of coming up with a meaningful response as well as the mechanics of entering the text. Here we present a novel textual reply generation model that goes beyond the current auto-reply and predictive text entry models by taking into account the content preferences of the user, the idiosyncrasies of their conversational style, and even the structure of their social graph. Specifically, we have developed two types of models for personalized user interactions: a content-based conversation model, which makes use of location together with user information, and a social-graph-based conversation model, which combines content-based conversation models with social graphs.\n    ",
        "submission_date": "2017-02-17T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06467",
        "title": "Efficient Social Network Multilingual Classification using Character, POS n-grams and Dynamic Normalization",
        "authors": [
            "Carlos-Emiliano Gonz\u00e1lez-Gallardo",
            "Juan-Manuel Torres-Moreno",
            "Azucena Montes Rend\u00f3n",
            "Gerardo Sierra"
        ],
        "abstract": "In this paper we describe a dynamic normalization process applied to social network multilingual documents (Facebook and Twitter) to improve the performance of the Author profiling task for short texts. After the normalization process, $n$-grams of characters and n-grams of POS tags are obtained to extract all the possible stylistic information encoded in the documents (emoticons, character flooding, capital letters, references to other users, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of performance.\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06510",
        "title": "Algorithmes de classification et d'optimisation: participation du LIA/ADOC \u00e1 DEFT'14",
        "authors": [
            "Luis Adri\u00e1n Cabrera-Diego",
            "St\u00e9phane Huet",
            "Bassam Jabaian",
            "Alejandro Molina",
            "Juan-Manuel Torres-Moreno",
            "Marc El-B\u00e8ze",
            "Barth\u00e9l\u00e9my Durette"
        ],
        "abstract": "This year, the DEFT campaign (D\u00e9fi Fouilles de Textes) incorporates a task which aims at identifying the session in which articles of previous TALN conferences were presented. We describe the three statistical systems developed at LIA/ADOC for this task. A fusion of these systems enables us to obtain interesting results (micro-precision score of 0.76 measured on the test corpus)\n    ",
        "submission_date": "2017-02-21T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06677",
        "title": "Discussion quality diffuses in the digital public square",
        "authors": [
            "George Berry",
            "Sean J. Taylor"
        ],
        "abstract": "Studies of online social influence have demonstrated that friends have important effects on many types of behavior in a wide variety of settings. However, we know much less about how influence works among relative strangers in digital public squares, despite important conversations happening in such spaces. We present the results of a study on large public Facebook pages where we randomly used two different methods--most recent and social feedback--to order comments on posts. We find that the social feedback condition results in higher quality viewed comments and response comments. After measuring the average quality of comments written by users before the study, we find that social feedback has a positive effect on response quality for both low and high quality commenters. We draw on a theoretical framework of social norms to explain this empirical result. In order to examine the influence mechanism further, we measure the similarity between comments viewed and written during the study, finding that similarity increases for the highest quality contributors under the social feedback condition. This suggests that, in addition to norms, some individuals may respond with increased relevance to high-quality comments.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.06700",
        "title": "Task-driven Visual Saliency and Attention-based Visual Question Answering",
        "authors": [
            "Yuetan Lin",
            "Zhangyang Pang",
            "Donghui Wang",
            "Yueting Zhuang"
        ],
        "abstract": "Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.\n    ",
        "submission_date": "2017-02-22T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07186",
        "title": "Stability of Topic Modeling via Matrix Factorization",
        "authors": [
            "Mark Belford",
            "Brian Mac Namee",
            "Derek Greene"
        ],
        "abstract": "Topic models can provide us with an insight into the underlying latent structure of a large corpus of documents. A range of methods have been proposed in the literature, including probabilistic topic models and techniques based on matrix factorization. However, in both cases, standard implementations rely on stochastic elements in their initialization phase, which can potentially lead to different results being generated on the same corpus when using the same parameter values. This corresponds to the concept of \"instability\" which has previously been studied in the context of $k$-means clustering. In many applications of topic modeling, this problem of instability is not considered and topic models are treated as being definitive, even though the results may change considerably if the initialization process is altered. In this paper we demonstrate the inherent instability of popular topic modeling approaches, using a number of new measures to assess stability. To address this issue in the context of matrix factorization for topic modeling, we propose the use of ensemble learning strategies. Based on experiments performed on annotated text corpora, we show that a K-Fold ensemble strategy, combining both ensembles and structured initialization, can significantly reduce instability, while simultaneously yielding more accurate topic models.\n    ",
        "submission_date": "2017-02-23T00:00:00",
        "last_modified_date": "2017-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07826",
        "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations",
        "authors": [
            "Upol Ehsan",
            "Brent Harrison",
            "Larry Chan",
            "Mark O. Riedl"
        ],
        "abstract": "We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.\n    ",
        "submission_date": "2017-02-25T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.07983",
        "title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks",
        "authors": [
            "Tong Che",
            "Yanran Li",
            "Ruixiang Zhang",
            "R Devon Hjelm",
            "Wenjie Li",
            "Yangqiu Song",
            "Yoshua Bengio"
        ],
        "abstract": "Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator's output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2017-02-26T00:00:00",
        "last_modified_date": "2017-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1702.08139",
        "title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions",
        "authors": [
            "Zichao Yang",
            "Zhiting Hu",
            "Ruslan Salakhutdinov",
            "Taylor Berg-Kirkpatrick"
        ],
        "abstract": "Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.\n    ",
        "submission_date": "2017-02-27T00:00:00",
        "last_modified_date": "2017-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00050",
        "title": "SceneSeer: 3D Scene Design with Natural Language",
        "authors": [
            "Angel X. Chang",
            "Mihail Eric",
            "Manolis Savva",
            "Christopher D. Manning"
        ],
        "abstract": "Designing 3D scenes is currently a creative task that requires significant expertise and effort in using complex 3D design interfaces. This effortful design process starts in stark contrast to the easiness with which people can use language to describe real and imaginary environments. We present SceneSeer: an interactive text to 3D scene generation system that allows a user to design 3D scenes using natural language. A user provides input text from which we extract explicit constraints on the objects that should appear in the scene. Given these explicit constraints, the system then uses a spatial knowledge base learned from an existing database of 3D scenes and 3D object models to infer an arrangement of the objects forming a natural scene matching the input description. Using textual commands the user can then iteratively refine the created scene by adding, removing, replacing, and manipulating objects. We evaluate the quality of 3D scenes generated by SceneSeer in a perceptual evaluation experiment where we compare against manually designed scenes and simpler baselines for 3D scene generation. We demonstrate how the generated scenes can be iteratively refined through simple natural language commands.\n    ",
        "submission_date": "2017-02-28T00:00:00",
        "last_modified_date": "2017-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00203",
        "title": "Frequency patterns of semantic change: Corpus-based evidence of a near-critical dynamics in language change",
        "authors": [
            "Quentin Feltgen",
            "Benjamin Fagard",
            "Jean-Pierre Nadal"
        ],
        "abstract": "It is generally believed that, when a linguistic item acquires a new meaning, its overall frequency of use in the language rises with time with an S-shaped growth curve. Yet, this claim has only been supported by a limited number of case studies. In this paper, we provide the first corpus-based quantitative confirmation of the genericity of the S-curve in language change. Moreover, we uncover another generic pattern, a latency phase of variable duration preceding the S-growth, during which the frequency of use of the semantically expanding word remains low and more or less constant. We also propose a usage-based model of language change supported by cognitive considerations, which predicts that both phases, the latency and the fast S-growth, take place. The driving mechanism is a stochastic dynamics, a random walk in the space of frequency of use. The underlying deterministic dynamics highlights the role of a control parameter, the strength of the cognitive impetus governing the onset of change, which tunes the system at the vicinity of a saddle-node bifurcation. In the neighborhood of the critical point, the latency phase corresponds to the diffusion time over the critical region, and the S-growth to the fast convergence that follows. The duration of the two phases is computed as specific first passage times of the random walk process, leading to distributions that fit well the ones extracted from our dataset. We argue that our results are not specific to the studied corpus, but apply to semantic change in general.\n    ",
        "submission_date": "2017-03-01T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00948",
        "title": "DAWT: Densely Annotated Wikipedia Texts across multiple languages",
        "authors": [
            "Nemanja Spasojevic",
            "Preeti Bhargava",
            "Guoning Hu"
        ],
        "abstract": "In this work, we open up the DAWT dataset - Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic. We also present the methodology used to generate the dataset which enriches Wikipedia markup in order to increase number of links. In addition to the main dataset, we open up several derived datasets including mention entity co-occurrence counts and entity embeddings, as well as mappings between Freebase ids and Wikidata item ids. We also discuss two applications of these datasets and hope that opening them up would prove useful for the Natural Language Processing and Information Retrieval communities, as well as facilitate multi-lingual research.\n    ",
        "submission_date": "2017-03-02T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.00955",
        "title": "Toward Controlled Generation of Text",
        "authors": [
            "Zhiting Hu",
            "Zichao Yang",
            "Xiaodan Liang",
            "Ruslan Salakhutdinov",
            "Eric P. Xing"
        ],
        "abstract": "Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.\n    ",
        "submission_date": "2017-03-02T00:00:00",
        "last_modified_date": "2018-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01557",
        "title": "Using Graphs of Classifiers to Impose Declarative Constraints on Semi-supervised Learning",
        "authors": [
            "Lidong Bing",
            "William W. Cohen",
            "Bhuwan Dhingra"
        ],
        "abstract": "We propose a general approach to modeling semi-supervised learning (SSL) algorithms. Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction. We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains.\n    ",
        "submission_date": "2017-03-05T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01671",
        "title": "Controlling for Unobserved Confounds in Classification Using Correlational Constraints",
        "authors": [
            "Virgile Landeiro",
            "Aron Culotta"
        ],
        "abstract": "As statistical classifiers become integrated into real-world applications, it is important to consider not only their accuracy but also their robustness to changes in the data distribution. In this paper, we consider the case where there is an unobserved confounding variable $z$ that influences both the features $\\mathbf{x}$ and the class variable $y$. When the influence of $z$ changes from training to testing data, we find that the classifier accuracy can degrade rapidly. In our approach, we assume that we can predict the value of $z$ at training time with some error. The prediction for $z$ is then fed to Pearl's back-door adjustment to build our model. Because of the attenuation bias caused by measurement error in $z$, standard approaches to controlling for $z$ are ineffective. In response, we propose a method to properly control for the influence of $z$ by first estimating its relationship with the class variable $y$, then updating predictions for $z$ to match that estimated relationship. By adjusting the influence of $z$, we show that we can build a model that exceeds competing baselines on accuracy as well as on robustness over a range of confounding relationships.\n    ",
        "submission_date": "2017-03-05T00:00:00",
        "last_modified_date": "2018-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01725",
        "title": "Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity",
        "authors": [
            "Jack Hessel",
            "Lillian Lee",
            "David Mimno"
        ],
        "abstract": "The content of today's social media is becoming more and more rich, increasingly mixing text, images, videos, and audio. It is an intriguing research question to model the interplay between these different modes in attracting user attention and engagement. But in order to pursue this study of multimodal content, we must also account for context: timing effects, community preferences, and social factors (e.g., which authors are already popular) also affect the amount of feedback and reaction that social-media posts receive. In this work, we separate out the influence of these non-content factors in several ways. First, we focus on ranking pairs of submissions posted to the same community in quick succession, e.g., within 30 seconds, this framing encourages models to focus on time-agnostic and community-specific content features. Within that setting, we determine the relative performance of author vs. content features. We find that victory usually belongs to \"cats and captions,\" as visual and textual features together tend to outperform identity-based features. Moreover, our experiments show that when considered in isolation, simple unigram text features and deep neural network visual features yield the highest accuracy individually, and that the combination of the two modalities generally leads to the best accuracies overall.\n    ",
        "submission_date": "2017-03-06T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.01898",
        "title": "Generative and Discriminative Text Classification with Recurrent Neural Networks",
        "authors": [
            "Dani Yogatama",
            "Chris Dyer",
            "Wang Ling",
            "Phil Blunsom"
        ],
        "abstract": "We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors (e.g., they account for conditional dependencies across words in a document), they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts---the same pattern that Ng & Jordan (2001) proved holds for linear classification models that make more naive conditional independence assumptions. Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models.\n    ",
        "submission_date": "2017-03-06T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02573",
        "title": "Data Noising as Smoothing in Neural Network Language Models",
        "authors": [
            "Ziang Xie",
            "Sida I. Wang",
            "Jiwei Li",
            "Daniel L\u00e9vy",
            "Aiming Nie",
            "Dan Jurafsky",
            "Andrew Y. Ng"
        ],
        "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in $n$-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.\n    ",
        "submission_date": "2017-03-07T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.02819",
        "title": "Introduction to Formal Concept Analysis and Its Applications in Information Retrieval and Related Fields",
        "authors": [
            "Dmitry I. Ignatov"
        ],
        "abstract": "This paper is a tutorial on Formal Concept Analysis (FCA) and its applications. FCA is an applied branch of Lattice Theory, a mathematical discipline which enables formalisation of concepts as basic units of human thinking and analysing data in the object-attribute form. Originated in early 80s, during the last three decades, it became a popular human-centred tool for knowledge representation and data analysis with numerous applications. Since the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics include Information Retrieval with a focus on visualisation aspects, Machine Learning, Data Mining and Knowledge Discovery, Text Mining and several others.\n    ",
        "submission_date": "2017-03-08T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03386",
        "title": "Loyalty in Online Communities",
        "authors": [
            "William L. Hamilton",
            "Justine Zhang",
            "Cristian Danescu-Niculescu-Mizil",
            "Dan Jurafsky",
            "Jure Leskovec"
        ],
        "abstract": "Loyalty is an essential component of multi-community engagement. When users have the choice to engage with a variety of different communities, they often become loyal to just one, focusing on that community at the expense of others. However, it is unclear how loyalty is manifested in user behavior, or whether loyalty is encouraged by certain community characteristics.\n",
        "submission_date": "2017-03-09T00:00:00",
        "last_modified_date": "2017-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03429",
        "title": "What can you do with a rock? Affordance extraction via word embeddings",
        "authors": [
            "Nancy Fulda",
            "Daniel Ricks",
            "Ben Murdoch",
            "David Wingate"
        ],
        "abstract": "Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance detection is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance most of the time. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose.\n    ",
        "submission_date": "2017-03-09T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03609",
        "title": "NetSpam: a Network-based Spam Detection Framework for Reviews in Online Social Media",
        "authors": [
            "Saeedreza Shehnepoor",
            "Mostafa Salehi",
            "Reza Farahbakhsh",
            "Noel Crespi"
        ],
        "abstract": "Nowadays, a big part of people rely on available content in social media in their decisions (e.g. reviews and feedback on a topic or product). The possibility that anybody can leave a review provide a golden opportunity for spammers to write spam reviews about products and services for different interests. Identifying these spammers and the spam content is a hot topic of research and although a considerable number of studies have been done recently toward this end, but so far the methodologies put forth still barely detect spam reviews, and none of them show the importance of each extracted feature type. In this study, we propose a novel framework, named NetSpam, which utilizes spam features for modeling review datasets as heterogeneous information networks to map spam detection procedure into a classification problem in such networks. Using the importance of spam features help us to obtain better results in terms of different metrics experimented on real-world review datasets from Yelp and Amazon websites. The results show that NetSpam outperforms the existing methods and among four categories of features; including review-behavioral, user-behavioral, reviewlinguistic, user-linguistic, the first type of features performs better than the other categories.\n    ",
        "submission_date": "2017-03-10T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.03923",
        "title": "A German Corpus for Text Similarity Detection Tasks",
        "authors": [
            "Juan-Manuel Torres-Moreno",
            "Gerardo Sierra",
            "Peter Peinl"
        ],
        "abstract": "Text similarity detection aims at measuring the degree of similarity between a pair of texts. Corpora available for text similarity detection are designed to evaluate the algorithms to assess the paraphrase level among documents. In this paper we present a textual German corpus for similarity detection. The purpose of this corpus is to automatically assess the similarity between a pair of texts and to evaluate different similarity measures, both for whole documents or for individual sentences. Therefore we have calculated several simple measures on our corpus based on a library of similarity functions.\n    ",
        "submission_date": "2017-03-11T00:00:00",
        "last_modified_date": "2017-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04081",
        "title": "Feature overwriting as a finite mixture process: Evidence from comprehension data",
        "authors": [
            "Shravan Vasishth",
            "Lena A. J\u00e4ger",
            "Bruno Nicenboim"
        ],
        "abstract": "The ungrammatical sentence \"The key to the cabinets are on the table\" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence \"The key to the cabinet are on the table\". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts.\n    ",
        "submission_date": "2017-03-12T00:00:00",
        "last_modified_date": "2018-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04247",
        "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
        "authors": [
            "Huifeng Guo",
            "Ruiming Tang",
            "Yunming Ye",
            "Zhenguo Li",
            "Xiuqiang He"
        ],
        "abstract": "Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04336",
        "title": "A Visual Representation of Wittgenstein's Tractatus Logico-Philosophicus",
        "authors": [
            "Anca Bucur",
            "Sergiu Nisioi"
        ],
        "abstract": "In this paper we present a data visualization method together with its potential usefulness in digital humanities and philosophy of language. We compile a multilingual parallel corpus from different versions of Wittgenstein's Tractatus Logico-Philosophicus, including the original in German and translations into English, Spanish, French, and Russian. Using this corpus, we compute a similarity measure between propositions and render a visual network of relations for different languages.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04498",
        "title": "High-Throughput and Language-Agnostic Entity Disambiguation and Linking on User Generated Data",
        "authors": [
            "Preeti Bhargava",
            "Nemanja Spasojevic",
            "Guoning Hu"
        ],
        "abstract": "The Entity Disambiguation and Linking (EDL) task matches entity mentions in text to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase id. It plays a critical role in the construction of a high quality information network, and can be further leveraged for a variety of information retrieval and NLP tasks such as text categorization and document tagging. EDL is a complex and challenging problem due to ambiguity of the mentions and real world text being multi-lingual. Moreover, EDL systems need to have high throughput and should be lightweight in order to scale to large datasets and run on off-the-shelf machines. More importantly, these systems need to be able to extract and disambiguate dense annotations from the data in order to enable an Information Retrieval or Extraction task running on the data to be more efficient and accurate. In order to address all these challenges, we present the Lithium EDL system and algorithm - a high-throughput, lightweight, language-agnostic EDL system that extracts and correctly disambiguates 75% more entities than state-of-the-art EDL systems and is significantly faster than them.\n    ",
        "submission_date": "2017-03-13T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04512",
        "title": "Normalisation de la langue et de lecriture arabe : enjeux culturels regionaux et mondiaux",
        "authors": [
            "Henri Hudrisier",
            "Ben Henda Mokhtar"
        ],
        "abstract": "Arabic language and writing are now facing a resurgence of international normative solutions that challenge most of their local or network based operating principles. Even if the multilingual digital coding solutions, especially those proposed by Unicode, have solved many difficulties of Arabic writing, the linguistic aspect is still in search of more adapted solutions. Terminology is one of the sectors in which the Arabic language requires a deep modernization of its classical productivity models. The normative approach, in particular that of the ISO TC37, is proposed as one of the solutions that would allow it to combine with international standards to better integrate the knowledge society under construction.\n",
        "submission_date": "2017-02-24T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04783",
        "title": "Multichannel End-to-end Speech Recognition",
        "authors": [
            "Tsubasa Ochiai",
            "Shinji Watanabe",
            "Takaaki Hori",
            "John R. Hershey"
        ],
        "abstract": "The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.\n    ",
        "submission_date": "2017-03-14T00:00:00",
        "last_modified_date": "2017-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04854",
        "title": "Distributed-Representation Based Hybrid Recommender System with Short Item Descriptions",
        "authors": [
            "Junhua He",
            "Hankz Hankui Zhuo",
            "Jarvan Law"
        ],
        "abstract": "Collaborative filtering (CF) aims to build a model from users' past behaviors and/or similar decisions made by other users, and use the model to recommend items for users. Despite of the success of previous collaborative filtering approaches, they are all based on the assumption that there are sufficient rating scores available for building high-quality recommendation models. In real world applications, however, it is often difficult to collect sufficient rating scores, especially when new items are introduced into the system, which makes the recommendation task challenging. We find that there are often \"short\" texts describing features of items, based on which we can approximate the similarity of items and make recommendation together with rating scores. In this paper we \"borrow\" the idea of vector representation of words to capture the information of short texts and embed it into a matrix factorization framework. We empirically show that our approach is effective by comparing it with state-of-the-art approaches.\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.04908",
        "title": "Emergence of Grounded Compositional Language in Multi-Agent Populations",
        "authors": [
            "Igor Mordatch",
            "Pieter Abbeel"
        ],
        "abstract": "By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.\n    ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2018-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05123",
        "title": "Character-based Neural Embeddings for Tweet Clustering",
        "authors": [
            "Svitlana Vakulenko",
            "Lyndon Nixon",
            "Mihai Lupu"
        ],
        "abstract": "In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line at ",
        "submission_date": "2017-03-15T00:00:00",
        "last_modified_date": "2017-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05706",
        "title": "Improving Document Clustering by Eliminating Unnatural Language",
        "authors": [
            "Myungha Jang",
            "Jinho D. Choi",
            "James Allan"
        ],
        "abstract": "Technical documents contain a fair amount of unnatural language, such as tables, formulas, pseudo-codes, etc. Unnatural language can be an important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 15%. Our corpus and tool are publicly available.\n    ",
        "submission_date": "2017-03-16T00:00:00",
        "last_modified_date": "2017-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05851",
        "title": "Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture",
        "authors": [
            "Yuanliang Meng",
            "Anna Rumshisky",
            "Alexey Romanov"
        ],
        "abstract": "In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A \"double-checking\" technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin.\n    ",
        "submission_date": "2017-03-17T00:00:00",
        "last_modified_date": "2017-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.05908",
        "title": "Learning Robust Visual-Semantic Embeddings",
        "authors": [
            "Yao-Hung Hubert Tsai",
            "Liang-Kang Huang",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.\n    ",
        "submission_date": "2017-03-17T00:00:00",
        "last_modified_date": "2017-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06108",
        "title": "Global Entity Ranking Across Multiple Languages",
        "authors": [
            "Prantik Bhattacharyya",
            "Nemanja Spasojevic"
        ],
        "abstract": "We present work on building a global long-tailed ranking of entities across multiple languages using Wikipedia and Freebase knowledge bases. We identify multiple features and build a model to rank entities using a ground-truth dataset of more than 10 thousand labels. The final system ranks 27 million entities with 75% precision and 48% F1 score. We provide performance evaluation and empirical evidence of the quality of ranking across languages, and open the final ranked lists for future research.\n    ",
        "submission_date": "2017-03-17T00:00:00",
        "last_modified_date": "2017-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06492",
        "title": "VQABQ: Visual Question Answering by Basic Questions",
        "authors": [
            "Jia-Hong Huang",
            "Modar Alfadly",
            "Bernard Ghanem"
        ],
        "abstract": "Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset and yields state-of-the-art accuracy, 60.34% in open-ended task.\n    ",
        "submission_date": "2017-03-19T00:00:00",
        "last_modified_date": "2017-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06585",
        "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning",
        "authors": [
            "Abhishek Das",
            "Satwik Kottur",
            "Jos\u00e9 M. F. Moura",
            "Stefan Lee",
            "Dhruv Batra"
        ],
        "abstract": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.\n",
        "submission_date": "2017-03-20T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06630",
        "title": "Automatic Text Summarization Approaches to Speed up Topic Model Learning Process",
        "authors": [
            "Mohamed Morchid",
            "Juan-Manuel Torres-Moreno",
            "Richard Dufour",
            "Javier Ram\u00edrez-Rodr\u00edguez",
            "Georges Linar\u00e8s"
        ],
        "abstract": "The number of documents available into Internet moves each day up. For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists. Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate. To deal with this issue, we propose to build topic spaces from summarized documents. In this paper, we present a study of topic space representation in the context of big data. The topic space representation behavior is analyzed on different languages. Experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete documents. The real advantage of such an approach is the processing time gain: we showed that the processing time can be drastically reduced using summarized documents (more than 60\\% in general). This study finally points out the differences between thematic representations of documents depending on the targeted languages such as English or latin languages.\n    ",
        "submission_date": "2017-03-20T00:00:00",
        "last_modified_date": "2017-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06642",
        "title": "Towards a Quantum World Wide Web",
        "authors": [
            "Diederik Aerts",
            "Jonito Aerts Arguelles",
            "Lester Beltran",
            "Lyneth Beltran",
            "Isaac Distrito",
            "Massimiliano Sassoli de Bianchi",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "We elaborate a quantum model for the meaning associated with corpora of written documents, like the pages forming the World Wide Web. To that end, we are guided by how physicists constructed quantum theory for microscopic entities, which unlike classical objects cannot be fully represented in our spatial theater. We suggest that a similar construction needs to be carried out by linguists and computational scientists, to capture the full meaning carried by collections of documental entities. More precisely, we show how to associate a quantum-like 'entity of meaning' to a 'language entity formed by printed documents', considering the latter as the collection of traces that are left by the former, in specific results of search actions that we describe as measurements. In other words, we offer a perspective where a collection of documents, like the Web, is described as the space of manifestation of a more complex entity - the QWeb - which is the object of our modeling, drawing its inspiration from previous studies on operational-realistic approaches to quantum physics and quantum modeling of human cognition and decision-making. We emphasize that a consistent QWeb model needs to account for the observed correlations between words appearing in printed documents, e.g., co-occurrences, as the latter would depend on the 'meaning connections' existing between the concepts that are associated with these words. In that respect, we show that both 'context and interference (quantum) effects' are required to explain the probabilities calculated by counting the relative number of documents containing certain words and co-ocurrrences of words.\n    ",
        "submission_date": "2017-03-20T00:00:00",
        "last_modified_date": "2018-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.06676",
        "title": "I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation",
        "authors": [
            "Hao Dong",
            "Jingqing Zhang",
            "Douglas McIlwraith",
            "Yike Guo"
        ],
        "abstract": "Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that %the capability of our method to understand the sentence descriptions, so as to I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose\n    ",
        "submission_date": "2017-03-20T00:00:00",
        "last_modified_date": "2017-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.07588",
        "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and Its Correlation with Phoneme Boundaries",
        "authors": [
            "Yu-Hsuan Wang",
            "Cheng-Tao Chung",
            "Hung-yi Lee"
        ],
        "abstract": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.\n    ",
        "submission_date": "2017-03-22T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08052",
        "title": "Dynamic Bernoulli Embeddings for Language Evolution",
        "authors": [
            "Maja Rudolph",
            "David Blei"
        ],
        "abstract": "Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. (2016) developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.\n    ",
        "submission_date": "2017-03-23T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08314",
        "title": "Interacting Conceptual Spaces I : Grammatical Composition of Concepts",
        "authors": [
            "Joe Bolt",
            "Bob Coecke",
            "Fabrizio Genovese",
            "Martha Lewis",
            "Dan Marsden",
            "Robin Piedeleu"
        ],
        "abstract": "The categorical compositional approach to meaning has been successfully applied in natural language processing, outperforming other models in mainstream empirical language processing tasks. We show how this approach can be generalized to conceptual space models of cognition. In order to do this, first we introduce the category of convex relations as a new setting for categorical compositional semantics, emphasizing the convex structure important to conceptual space applications. We then show how to construct conceptual spaces for various types such as nouns, adjectives and verbs. Finally we show by means of examples how concepts can be systematically combined to establish the meanings of composite phrases from the meanings of their constituent parts. This provides the mathematical underpinnings of a new compositional approach to cognition.\n    ",
        "submission_date": "2017-03-24T00:00:00",
        "last_modified_date": "2017-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08324",
        "title": "Are crossing dependencies really scarce?",
        "authors": [
            "Ramon Ferrer-i-Cancho",
            "Carlos Gomez-Rodriguez",
            "J.L. Esteban"
        ],
        "abstract": "The syntactic structure of a sentence can be modelled as a tree, where vertices correspond to words and edges indicate syntactic dependencies. It has been claimed recurrently that the number of edge crossings in real sentences is small. However, a baseline or null hypothesis has been lacking. Here we quantify the amount of crossings of real sentences and compare it to the predictions of a series of baselines. We conclude that crossings are really scarce in real sentences. Their scarcity is unexpected by the hubiness of the trees. Indeed, real sentences are close to linear trees, where the potential number of crossings is maximized.\n    ",
        "submission_date": "2017-03-24T00:00:00",
        "last_modified_date": "2017-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08428",
        "title": "Calendar.help: Designing a Workflow-Based Scheduling Agent with Humans in the Loop",
        "authors": [
            "Justin Cranshaw",
            "Emad Elwany",
            "Todd Newman",
            "Rafal Kocielnik",
            "Bowen Yu",
            "Sandeep Soni",
            "Jaime Teevan",
            "Andr\u00e9s Monroy-Hern\u00e1ndez"
        ],
        "abstract": "Although information workers may complain about meetings, they are an essential part of their work life. Consequently, busy people spend a significant amount of time scheduling meetings. We present ",
        "submission_date": "2017-03-24T00:00:00",
        "last_modified_date": "2017-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.08544",
        "title": "Data-Mining Textual Responses to Uncover Misconception Patterns",
        "authors": [
            "Joshua J. Michalenko",
            "Andrew S. Lan",
            "Richard G. Baraniuk"
        ],
        "abstract": "An important, yet largely unstudied, problem in student data analysis is to detect misconceptions from students' responses to open-response questions. Misconception detection enables instructors to deliver more targeted feedback on the misconceptions exhibited by many students in their class, thus improving the quality of instruction. In this paper, we propose a new natural language processing-based framework to detect the common misconceptions among students' textual responses to short-answer questions. We propose a probabilistic model for students' textual responses involving misconceptions and experimentally validate it on a real-world student-response dataset. Experimental results show that our proposed framework excels at classifying whether a response exhibits one or more misconceptions. More importantly, it can also automatically detect the common misconceptions exhibited across responses from multiple students to multiple questions; this property is especially important at large scale, since instructors will no longer need to manually specify all possible misconceptions that students might exhibit.\n    ",
        "submission_date": "2017-03-24T00:00:00",
        "last_modified_date": "2017-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09046",
        "title": "Bootstrapping a Lexicon for Emotional Arousal in Software Engineering",
        "authors": [
            "Mika V. M\u00e4ntyl\u00e4",
            "Nicole Novielli",
            "Filippo Lanubile",
            "Ma\u00eblick Claes",
            "Miikka Kuutila"
        ],
        "abstract": "Emotional arousal increases activation and performance but may also lead to burnout in software development. We present the first version of a Software Engineering Arousal lexicon (SEA) that is specifically designed to address the problem of emotional arousal in the software developer ecosystem. SEA is built using a bootstrapping approach that combines word embedding model trained on issue-tracking data and manual scoring of items in the lexicon. We show that our lexicon is able to differentiate between issue priorities, which are a source of emotional activation and then act as a proxy for arousal. The best performance is obtained by combining SEA (428 words) with a previously created general purpose lexicon by Warriner et al. (13,915 words) and it achieves Cohen's d effect sizes up to 0.5.\n    ",
        "submission_date": "2017-03-27T00:00:00",
        "last_modified_date": "2017-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09137",
        "title": "Where to put the Image in an Image Caption Generator",
        "authors": [
            "Marc Tanti",
            "Albert Gatt",
            "Kenneth P. Camilleri"
        ],
        "abstract": "When a recurrent neural network language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in the RNN -- conditioning the language model by `injecting' image features -- or in a layer following the RNN -- conditioning the language model by `merging' image features. While both options are attested in the literature, there is as yet no systematic comparison between the two. In this paper we empirically show that it is not especially detrimental to performance whether one architecture is used or another. The merge architecture does have practical advantages, as conditioning by merging allows the RNN's hidden state vector to shrink in size by up to four times. Our results suggest that the visual and linguistic modalities for caption generation need not be jointly encoded by the RNN as that yields large, memory-intensive models with few tangible advantages in performance; rather, the multimodal integration should be delayed to a subsequent stage.\n    ",
        "submission_date": "2017-03-27T00:00:00",
        "last_modified_date": "2018-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09398",
        "title": "This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire than Real News",
        "authors": [
            "Benjamin D. Horne",
            "Sibel Adali"
        ],
        "abstract": "The problem of fake news has gained a lot of attention as it is claimed to have had a significant impact on 2016 US Presidential Elections. Fake news is not a new problem and its spread in social networks is well-studied. Often an underlying assumption in fake news discussion is that it is written to look like real news, fooling the reader who does not check for reliability of the sources or the arguments in its content. Through a unique study of three data sets and features that capture the style and the language of articles, we show that this assumption is not true. Fake news in most cases is more similar to satire than to real news, leading us to conclude that persuasion in fake news is achieved through heuristics rather than the strength of arguments. We show overall title structure and the use of proper nouns in titles are very significant in differentiating fake from real. This leads us to conclude that fake news is targeted for audiences who are not likely to read beyond titles and is aimed at creating mental associations between entities and claims.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09400",
        "title": "Diving Deep into Clickbaits: Who Use Them to What Extents in Which Topics with What Effects?",
        "authors": [
            "Md Main Uddin Rony",
            "Naeemul Hassan",
            "Mohammad Yousuf"
        ],
        "abstract": "The use of alluring headlines (clickbait) to tempt the readers has become a growing practice nowadays. For the sake of existence in the highly competitive media industry, most of the on-line media including the mainstream ones, have started following this practice. Although the wide-spread practice of clickbait makes the reader's reliability on media vulnerable, a large scale analysis to reveal this fact is still absent. In this paper, we analyze 1.67 million Facebook posts created by 153 media organizations to understand the extent of clickbait practice, its impact and user engagement by using our own developed clickbait detection model. The model uses distributed sub-word embeddings learned from a large corpus. The accuracy of the model is 98.3%. Powered with this model, we further study the distribution of topics in clickbait and non-clickbait contents.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09684",
        "title": "An Analysis of Visual Question Answering Algorithms",
        "authors": [
            "Kushal Kafle",
            "Christopher Kanan"
        ],
        "abstract": "In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.\n    ",
        "submission_date": "2017-03-28T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.09749",
        "title": "Developpement de Methodes Automatiques pour la Reutilisation des Composants Logiciels",
        "authors": [
            "Kouakou Ive Arsene Koffi",
            "Konan Marcellin Brou",
            "Souleymane Oumtanaga"
        ],
        "abstract": "The large amount of information and the increasing complexity of applications constrain developers to have stand-alone and reusable components from libraries and component ",
        "submission_date": "2017-03-21T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10339",
        "title": "Finding News Citations for Wikipedia",
        "authors": [
            "Besnik Fetahu",
            "Katja Markert",
            "Wolfgang Nejdl",
            "Avishek Anand"
        ],
        "abstract": "An important editing policy in Wikipedia is to provide citations for added statements in Wikipedia pages, where statements can be arbitrary pieces of text, ranging from a sentence to a paragraph. In many cases citations are either outdated or missing altogether.\n",
        "submission_date": "2017-03-30T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10344",
        "title": "Automated News Suggestions for Populating Wikipedia Entity Pages",
        "authors": [
            "Besnik Fetahu",
            "Katja Markert",
            "Avishek Anand"
        ],
        "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources.\n",
        "submission_date": "2017-03-30T00:00:00",
        "last_modified_date": "2017-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10356",
        "title": "Simplified End-to-End MMI Training and Voting for ASR",
        "authors": [
            "Lior Fritz",
            "David Burshtein"
        ],
        "abstract": "A simplified speech recognition system that uses the maximum mutual information (MMI) criterion is considered. End-to-end training using gradient descent is suggested, similarly to the training of connectionist temporal classification (CTC). We use an MMI criterion with a simple language model in the training stage, and a standard HMM decoder. Our method compares favorably to CTC in terms of performance, robustness, decoding time, disk footprint and quality of alignments. The good alignments enable the use of a straightforward ensemble method, obtained by simply averaging the predictions of several neural network models, that were trained separately end-to-end. The ensemble method yields a considerable reduction in the word error rate.\n    ",
        "submission_date": "2017-03-30T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1703.10476",
        "title": "Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training",
        "authors": [
            "Rakshith Shetty",
            "Marcus Rohrbach",
            "Lisa Anne Hendricks",
            "Mario Fritz",
            "Bernt Schiele"
        ],
        "abstract": "While strong progress has been made in image captioning over the last years, machine and human captions are still quite distinct. A closer look reveals that this is due to the deficiencies in the generated word distribution, vocabulary size, and strong bias in the generators towards frequent captions. Furthermore, humans -- rightfully so -- generate multiple, diverse captions, due to the inherent ambiguity in the captioning task which is not considered in today's systems.\n",
        "submission_date": "2017-03-30T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00135",
        "title": "Topic modeling of public repositories at scale using names in source code",
        "authors": [
            "Vadim Markovtsev",
            "Eiso Kant"
        ],
        "abstract": "Programming languages themselves have a limited number of reserved keywords and character based tokens that define the language specification. However, programmers have a rich use of natural language within their code through comments, text literals and naming entities. The programmer defined names that can be found in source code are a rich source of information to build a high level understanding of the project. The goal of this paper is to apply topic modeling to names used in over 13.6 million repositories and perceive the inferred topics. One of the problems in such a study is the occurrence of duplicate repositories not officially marked as forks (obscure forks). We show how to address it using the same identifiers which are extracted for topic modeling.\n",
        "submission_date": "2017-04-01T00:00:00",
        "last_modified_date": "2017-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00717",
        "title": "It Takes Two to Tango: Towards Theory of AI's Mind",
        "authors": [
            "Arjun Chandrasekaran",
            "Deshraj Yadav",
            "Prithvijit Chattopadhyay",
            "Viraj Prabhu",
            "Devi Parikh"
        ],
        "abstract": "Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one's own. Theory of Mind is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, there has been much emphasis on research to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds. In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. We further evaluate the role existing explanation (or interpretability) modalities play in helping humans build ToAIM. Explainable AI has received considerable scientific and popular attention in recent times. Surprisingly, we find that having access to the model's internal states - its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image - do not help people better predict its behavior.\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2017-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.00784",
        "title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments",
        "authors": [
            "Colin Raffel",
            "Minh-Thang Luong",
            "Peter J. Liu",
            "Ron J. Weiss",
            "Douglas Eck"
        ],
        "abstract": "Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.\n    ",
        "submission_date": "2017-04-03T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01444",
        "title": "Learning to Generate Reviews and Discovering Sentiment",
        "authors": [
            "Alec Radford",
            "Rafal Jozefowicz",
            "Ilya Sutskever"
        ],
        "abstract": "We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.\n    ",
        "submission_date": "2017-04-05T00:00:00",
        "last_modified_date": "2017-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.01599",
        "title": "Rhetorical relations for information retrieval",
        "authors": [
            "Christina Lioma",
            "Birger Larsen",
            "Wei Lu"
        ],
        "abstract": "Typically, every part in most coherent text has some plausible reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, cause, explanation, describe how the parts of a text are linked to each other. Knowledge about this socalled discourse structure has been applied successfully to several natural language processing tasks. This work studies the use of rhetorical relations for Information Retrieval (IR): Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR? We present a language model modification that considers rhetorical relations when estimating the relevance of a document to a query. Empirical evaluation of different versions of our model on TREC settings shows that certain rhetorical relations can benefit retrieval effectiveness notably (> 10% in mean average precision over a state-of-the-art baseline).\n    ",
        "submission_date": "2017-04-05T00:00:00",
        "last_modified_date": "2017-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02298",
        "title": "TransNets: Learning to Transform for Recommendation",
        "authors": [
            "Rose Catherine",
            "William Cohen"
        ],
        "abstract": "Recently, deep learning methods have been shown to improve the performance of recommender systems over traditional methods, especially when review text is available. For example, a recent model, DeepCoNN, uses neural nets to learn one latent representation for the text of all reviews written by a target user, and a second latent representation for the text of all reviews for a target item, and then combines these latent representations to obtain state-of-the-art performance on recommendation tasks. We show that (unsurprisingly) much of the predictive value of review text comes from reviews of the target user for the target item. We then introduce a way in which this information can be used in recommendation, even when the target user's review for the target item is not available. Our model, called TransNets, extends the DeepCoNN model by introducing an additional latent layer representing the target user-target item pair. We then regularize this layer, at training time, to be similar to another latent representation of the target user's review of the target item. We show that TransNets and extensions of it improve substantially over the previous state-of-the-art.\n    ",
        "submission_date": "2017-04-07T00:00:00",
        "last_modified_date": "2017-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02360",
        "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities",
        "authors": [
            "Hiroyuki Miyoshi",
            "Yuki Saito",
            "Shinnosuke Takamichi",
            "Hiroshi Saruwatari"
        ],
        "abstract": "Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02686",
        "title": "Word Embeddings via Tensor Factorization",
        "authors": [
            "Eric Bailey",
            "Shuchin Aeron"
        ],
        "abstract": "Most popular word embedding techniques involve implicit or explicit factorization of a word co-occurrence based matrix into low rank factors. In this paper, we aim to generalize this trend by using numerical methods to factor higher-order word co-occurrence based arrays, or \\textit{tensors}. We present four word embeddings using tensor factorization and analyze their advantages and disadvantages. One of our main contributions is a novel joint symmetric tensor factorization technique related to the idea of coupled tensor factorization. We show that embeddings based on tensor factorization can be used to discern the various meanings of polysemous words without being explicitly trained to do so, and motivate the intuition behind why this works in a way that doesn't with existing methods. We also modify an existing word embedding evaluation metric known as Outlier Detection [Camacho-Collados and Navigli, 2016] to evaluate the quality of the order-$N$ relations that a word embedding captures, and show that tensor-based methods outperform existing matrix-based methods at this task. Experimentally, we show that all of our word embeddings either outperform or are competitive with state-of-the-art baselines commonly used today on a variety of recent datasets. Suggested applications of tensor factorization-based word embeddings are given, and all source code and pre-trained vectors are publicly available online.\n    ",
        "submission_date": "2017-04-10T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.02841",
        "title": "From Modal to Multimodal Ambiguities: a Classification Approach",
        "authors": [
            "Maria Chiara Caschera",
            "Fernando Ferri",
            "Patrizia Grifoni"
        ],
        "abstract": "This paper deals with classifying ambiguities for Multimodal Languages. It evolves the classifications and the methods of the literature on ambiguities for Natural Language and Visual Language, empirically defining an original classification of ambiguities for multimodal interaction using a linguistic perspective. This classification distinguishes between Semantic and Syntactic multimodal ambiguities and their subclasses, which are intercepted using a rule-based method implemented in a software module. The experimental results have achieved an accuracy of the obtained classification compared to the expected one, which are defined by the human judgment, of 94.6% for the semantic ambiguities classes, and 92.1% for the syntactic ambiguities classes.\n    ",
        "submission_date": "2017-04-04T00:00:00",
        "last_modified_date": "2017-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03520",
        "title": "Unsupervised Event Abstraction using Pattern Abstraction and Local Process Models",
        "authors": [
            "Felix Mannhardt",
            "Niek Tax"
        ],
        "abstract": "Process mining analyzes business processes based on events stored in event logs. However, some recorded events may correspond to activities on a very low level of abstraction. When events are recorded on a too low level of granularity, process discovery methods tend to generate overgeneralizing process models. Grouping low-level events to higher level activities, i.e., event abstraction, can be used to discover better process models. Existing event abstraction methods are mainly based on common sub-sequences and clustering techniques. In this paper, we propose to first discover local process models and then use those models to lift the event log to a higher level of abstraction. Our conjecture is that process models discovered on the obtained high-level event log return process models of higher quality: their fitness and precision scores are more balanced. We show this with preliminary results on several real-life event logs.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2017-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03543",
        "title": "Leveraging Term Banks for Answering Complex Questions: A Case for Sparse Vectors",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "While open-domain question answering (QA) systems have proven effective for answering simple questions, they struggle with more complex questions. Our goal is to answer more complex questions reliably, without incurring a significant cost in knowledge resource construction to support the QA. One readily available knowledge resource is a term bank, enumerating the key concepts in a domain. We have developed an unsupervised learning approach that leverages a term bank to guide a QA system, by representing the terminological knowledge with thousands of specialized vector spaces. In experiments with complex science questions, we show that this approach significantly outperforms several state-of-the-art QA systems, demonstrating that significant leverage can be gained from continuous vector representations of domain terminology.\n    ",
        "submission_date": "2017-04-11T00:00:00",
        "last_modified_date": "2017-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03627",
        "title": "Real-time On-Demand Crowd-powered Entity Extraction",
        "authors": [
            "Ting-Hao 'Kenneth' Huang",
            "Yun-Nung Chen",
            "Jeffrey P. Bigham"
        ],
        "abstract": "Output-agreement mechanisms such as ESP Game have been widely used in human computation to obtain reliable human-generated labels. In this paper, we argue that a \"time-limited\" output-agreement mechanism can be used to create a fast and robust crowd-powered component in interactive systems, particularly dialogue systems, to extract key information from user utterances on the fly. Our experiments on Amazon Mechanical Turk using the Airline Travel Information System (ATIS) dataset showed that the proposed approach achieves high-quality results with an average response time shorter than 9 seconds.\n    ",
        "submission_date": "2017-04-12T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03809",
        "title": "A Neural Parametric Singing Synthesizer",
        "authors": [
            "Merlijn Blaauw",
            "Jordi Bonada"
        ],
        "abstract": "We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that's competitive in both speed and quality.\n    ",
        "submission_date": "2017-04-12T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.03940",
        "title": "PACRR: A Position-Aware Neural IR Model for Relevance Matching",
        "authors": [
            "Kai Hui",
            "Andrew Yates",
            "Klaus Berberich",
            "Gerard de Melo"
        ],
        "abstract": "In order to adopt deep learning for information retrieval, models are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored. In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document. Extensive experiments on six years' TREC Web Track data confirm that the proposed model yields better results under multiple benchmarks.\n    ",
        "submission_date": "2017-04-12T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.04664",
        "title": "Online Spatial Concept and Lexical Acquisition with Simultaneous Localization and Mapping",
        "authors": [
            "Akira Taniguchi",
            "Yoshinobu Hagiwara",
            "Tadahiro Taniguchi",
            "Tetsunari Inamura"
        ],
        "abstract": "In this paper, we propose an online learning algorithm based on a Rao-Blackwellized particle filter for spatial concept acquisition and mapping. We have proposed a nonparametric Bayesian spatial concept acquisition model (SpCoA). We propose a novel method (SpCoSLAM) integrating SpCoA and FastSLAM in the theoretical framework of the Bayesian generative model. The proposed method can simultaneously learn place categories and lexicons while incrementally generating an environmental map. Furthermore, the proposed method has scene image features and a language model added to SpCoA. In the experiments, we tested online learning of spatial concepts and environmental maps in a novel environment of which the robot did not have a map. Then, we evaluated the results of online learning of spatial concepts and lexical acquisition. The experimental results demonstrated that the robot was able to more accurately learn the relationships between words and the place in the environmental map incrementally by using the proposed method.\n    ",
        "submission_date": "2017-04-15T00:00:00",
        "last_modified_date": "2018-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05119",
        "title": "Exploring Sparsity in Recurrent Neural Networks",
        "authors": [
            "Sharan Narang",
            "Erich Elsen",
            "Gregory Diamos",
            "Shubho Sengupta"
        ],
        "abstract": "Recurrent Neural Networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8x and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse matrix multiply. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2x to 7x.\n    ",
        "submission_date": "2017-04-17T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05135",
        "title": "Does Neural Machine Translation Benefit from Larger Context?",
        "authors": [
            "Sebastien Jean",
            "Stanislas Lauly",
            "Orhan Firat",
            "Kyunghyun Cho"
        ],
        "abstract": "We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine translation is well suited for pronoun prediction and compares favorably with other approaches that were specifically designed for this task.\n    ",
        "submission_date": "2017-04-17T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05295",
        "title": "Semantic Similarity from Natural Language and Ontology Analysis",
        "authors": [
            "S\u00e9bastien Harispe",
            "Sylvie Ranwez",
            "Stefan Janaqi",
            "Jacky Montmain"
        ],
        "abstract": "Artificial Intelligence federates numerous scientific fields in the aim of developing machines able to assist human operators performing complex treatments -- most of which demand high cognitive skills (e.g. learning or decision processes). Central to this quest is to give machines the ability to estimate the likeness or similarity between things in the way human beings estimate the similarity between stimuli.\n",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05393",
        "title": "Mining Worse and Better Opinions. Unsupervised and Agnostic Aggregation of Online Reviews",
        "authors": [
            "Michela Fazzolari",
            "Marinella Petrocchi",
            "Alessandro Tommasi",
            "Cesare Zavattari"
        ],
        "abstract": "In this paper, we propose a novel approach for aggregating online reviews, according to the opinions they express. Our methodology is unsupervised - due to the fact that it does not rely on pre-labeled reviews - and it is agnostic - since it does not make any assumption about the domain or the language of the review content. We measure the adherence of a review content to the domain terminology extracted from a review set. First, we demonstrate the informativeness of the adherence metric with respect to the score associated with a review. Then, we exploit the metric values to group reviews, according to the opinions they express. Our experimental campaign has been carried out on two large datasets collected from Booking and Amazon, respectively.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05513",
        "title": "25 Tweets to Know You: A New Model to Predict Personality with Social Media",
        "authors": [
            "Pierre-Hadrien Arnoux",
            "Anbang Xu",
            "Neil Boyette",
            "Jalal Mahmud",
            "Rama Akkiraju",
            "Vibha Sinha"
        ],
        "abstract": "Predicting personality is essential for social applications supporting human-centered activities, yet prior modeling methods with users written text require too much input data to be realistically used in the context of social media. In this work, we aim to drastically reduce the data requirement for personality modeling and develop a model that is applicable to most users on Twitter. Our model integrates Word Embedding features with Gaussian Processes regression. Based on the evaluation of over 1.3K users on Twitter, we find that our model achieves comparable or better accuracy than state of the art techniques with 8 times fewer data.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05543",
        "title": "Coordinating Collaborative Chat in Massive Open Online Courses",
        "authors": [
            "Gaurav Singh Tomar",
            "Sreecharan Sankaranarayanan",
            "Xu Wang",
            "Carolyn Penstein Ros\u00e9"
        ],
        "abstract": "An earlier study of a collaborative chat intervention in a Massive Open Online Course (MOOC) identified negative effects on attrition stemming from a requirement for students to be matched with exactly one partner prior to beginning the activity. That study raised questions about how to orchestrate a collaborative chat intervention in a MOOC context in order to provide the benefit of synchronous social engagement without the coordination difficulties. In this paper we present a careful analysis of an intervention designed to overcome coordination difficulties by welcoming students into the chat on a rolling basis as they arrive rather than requiring them to be matched with a partner before beginning. The results suggest the most positive impact when experiencing a chat with exactly one partner rather than more or less. A qualitative analysis of the chat data reveals differential experiences between these configurations that suggests a potential explanation for the effect and raises questions for future research.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.05572",
        "title": "Answering Complex Questions Using Open Information Extraction",
        "authors": [
            "Tushar Khot",
            "Ashish Sabharwal",
            "Peter Clark"
        ],
        "abstract": "While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.\n    ",
        "submission_date": "2017-04-19T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06485",
        "title": "Attend to You: Personalized Image Captioning with Context Sequence Memory Networks",
        "authors": [
            "Cesc Chunseong Park",
            "Byeongchang Kim",
            "Gunhee Kim"
        ],
        "abstract": "We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user's active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.\n    ",
        "submission_date": "2017-04-21T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.06497",
        "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning",
        "authors": [
            "Julia Kreutzer",
            "Artem Sokolov",
            "Stefan Riezler"
        ],
        "abstract": "Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.\n    ",
        "submission_date": "2017-04-21T00:00:00",
        "last_modified_date": "2018-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07468",
        "title": "GaKCo: a Fast GApped k-mer string Kernel using COunting",
        "authors": [
            "Ritambhara Singh",
            "Arshdeep Sekhon",
            "Kamran Kowsari",
            "Jack Lanchantin",
            "Beilun Wang",
            "Yanjun Qi"
        ],
        "abstract": "String Kernel (SK) techniques, especially those using gapped $k$-mers as features (gk), have obtained great success in classifying sequences like DNA, protein, and text. However, the state-of-the-art gk-SK runs extremely slow when we increase the dictionary size ($\\Sigma$) or allow more mismatches ($M$). This is because current gk-SK uses a trie-based algorithm to calculate co-occurrence of mismatched substrings resulting in a time cost proportional to $O(\\Sigma^{M})$. We propose a \\textbf{fast} algorithm for calculating \\underline{Ga}pped $k$-mer \\underline{K}ernel using \\underline{Co}unting (GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of substrings using cumulative counting. This algorithm is fast, scalable to larger $\\Sigma$ and $M$, and naturally parallelizable. We provide a rigorous asymptotic analysis that compares GaKCo with the state-of-the-art gk-SK. Theoretically, the time cost of GaKCo is independent of the $\\Sigma^{M}$ term that slows down the trie-based approach. Experimentally, we observe that GaKCo achieves the same accuracy as the state-of-the-art and outperforms its speed by factors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein (12 datasets), and character-based English text (2 datasets), respectively.\n",
        "submission_date": "2017-04-24T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07626",
        "title": "Taxonomy Induction using Hypernym Subsequences",
        "authors": [
            "Amit Gupta",
            "R\u00e9mi Lebret",
            "Hamza Harkous",
            "Karl Aberer"
        ],
        "abstract": "We propose a novel, semi-supervised approach towards domain taxonomy induction from an input vocabulary of seed terms. Unlike all previous approaches, which typically extract direct hypernym edges for terms, our approach utilizes a novel probabilistic framework to extract hypernym subsequences. Taxonomy induction from extracted subsequences is cast as an instance of the minimumcost flow problem on a carefully designed directed graph. Through experiments, we demonstrate that our approach outperforms stateof- the-art taxonomy induction approaches across four languages. Importantly, we also show that our approach is robust to the presence of noise in the input vocabulary. To the best of our knowledge, no previous approaches have been empirically proven to manifest noise-robustness in the input vocabulary.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07734",
        "title": "DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning",
        "authors": [
            "Xiaodong Gu",
            "Hongyu Zhang",
            "Dongmei Zhang",
            "Sunghun Kim"
        ],
        "abstract": "Computer programs written in one language are often required to be ported to other languages to support multiple devices and environments. When programs use language specific APIs (Application Programming Interfaces), it is very challenging to migrate these APIs to the corresponding APIs written in other languages. Existing approaches mine API mappings from projects that have corresponding versions in two languages. They rely on the sparse availability of bilingual projects, thus producing a limited number of API mappings. In this paper, we propose an intelligent system called DeepAM for automatically mining API mappings from a large-scale code corpus without bilingual projects. The key component of DeepAM is based on the multimodal sequence to sequence learning architecture that aims to learn joint semantic representations of bilingual API sequences from big source code data. Experimental results indicate that DeepAM significantly increases the accuracy of API mappings as well as the number of API mappings, when compared with the state-of-the-art approaches.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07759",
        "title": "Email Babel: Does Language Affect Criminal Activity in Compromised Webmail Accounts?",
        "authors": [
            "Emeric Bernard-Jones",
            "Jeremiah Onaolapo",
            "Gianluca Stringhini"
        ],
        "abstract": "We set out to understand the effects of differing language on the ability of cybercriminals to navigate webmail accounts and locate sensitive information in them. To this end, we configured thirty Gmail honeypot accounts with English, Romanian, and Greek language settings. We populated the accounts with email messages in those languages by subscribing them to selected online newsletters. We hid email messages about fake bank accounts in fifteen of the accounts to mimic real-world webmail users that sometimes store sensitive information in their accounts. We then leaked credentials to the honey accounts via paste sites on the Surface Web and the Dark Web, and collected data for fifteen days. Our statistical analyses on the data show that cybercriminals are more likely to discover sensitive information (bank account information) in the Greek accounts than the remaining accounts, contrary to the expectation that Greek ought to constitute a barrier to the understanding of non-Greek visitors to the Greek accounts. We also extracted the important words among the emails that cybercriminals accessed (as an approximation of the keywords that they searched for within the honey accounts), and found that financial terms featured among the top words. In summary, we show that language plays a significant role in the ability of cybercriminals to access sensitive information hidden in compromised webmail accounts.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.07828",
        "title": "Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts",
        "authors": [
            "Chenhao Tan",
            "Dallas Card",
            "Noah A. Smith"
        ],
        "abstract": "Understanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statistics --- cooccurrence within documents and prevalence correlation over time --- our approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other's prevalence over time, and yet rarely cooccur, almost like a \"cold war\" scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.\n    ",
        "submission_date": "2017-04-25T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08243",
        "title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset",
        "authors": [
            "Aishwarya Agrawal",
            "Aniruddha Kembhavi",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality -- the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.\n    ",
        "submission_date": "2017-04-26T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08424",
        "title": "Multimodal Word Distributions",
        "authors": [
            "Ben Athiwaratkun",
            "Andrew Gordon Wilson"
        ],
        "abstract": "Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.\n    ",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2019-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08619",
        "title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks",
        "authors": [
            "Panagiotis Tzirakis",
            "George Trigeorgis",
            "Mihalis A. Nicolaou",
            "Bj\u00f6rn Schuller",
            "Stefanos Zafeiriou"
        ],
        "abstract": "Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to extract features from the speech, while for the visual modality a deep residual network (ResNet) of 50 layers. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, Long Short-Term Memory (LSTM) networks are utilized. The system is then trained in an end-to-end fashion where - by also taking advantage of the correlations of the each of the streams - we manage to significantly outperform the traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.\n    ",
        "submission_date": "2017-04-27T00:00:00",
        "last_modified_date": "2017-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1704.08803",
        "title": "Neural Ranking Models with Weak Supervision",
        "authors": [
            "Mostafa Dehghani",
            "Hamed Zamani",
            "Aliaksei Severyn",
            "Jaap Kamps",
            "W. Bruce Croft"
        ],
        "abstract": "Despite the impressive improvements achieved by unsupervised deep neural networks in computer vision and NLP tasks, such improvements have not yet been observed in ranking for information retrieval. The reason may be the complexity of the ranking problem, as it is not obvious how to learn from queries and documents when no supervised signal is available. Hence, in this paper, we propose to train a neural ranking model using weak supervision, where labels are obtained automatically without human annotators or any external resources (e.g., click data). To this aim, we use the output of an unsupervised ranking model, such as BM25, as a weak supervision signal. We further train a set of simple yet effective ranking models based on feed-forward neural networks. We study their effectiveness under various learning scenarios (point-wise and pair-wise models) and using different input representations (i.e., from encoding query-document pairs into dense/sparse vectors to using word embedding representation). We train our networks using tens of millions of training instances and evaluate it on two standard collections: a homogeneous news collection(Robust) and a heterogeneous large-scale web collection (ClueWeb). Our experiments indicate that employing proper objective functions and letting the networks to learn the input representation based on weakly supervised data leads to impressive performance, with over 13% and 35% MAP improvements over the BM25 model on the Robust and the ClueWeb collections. Our findings also suggest that supervised neural ranking models can greatly benefit from pre-training on large amounts of weakly labeled data that can be easily obtained from unsupervised IR models.\n    ",
        "submission_date": "2017-04-28T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00105",
        "title": "Representation Learning and Pairwise Ranking for Implicit Feedback in Recommendation Systems",
        "authors": [
            "Sumit Sidana",
            "Mikhail Trofimov",
            "Oleg Horodnitskii",
            "Charlotte Laclau",
            "Yury Maximov",
            "Massih-Reza Amini"
        ],
        "abstract": "In this paper, we propose a novel ranking framework for collaborative filtering with the overall aim of learning user preferences over items by minimizing a pairwise ranking loss. We show the minimization problem involves dependent random variables and provide a theoretical analysis by proving the consistency of the empirical risk minimization in the worst case where all users choose a minimal number of positive and negative items. We further derive a Neural-Network model that jointly learns a new representation of users and items in an embedded space as well as the preference relation of users over the pairs of items. The learning objective is based on three scenarios of ranking losses that control the ability of the model to maintain the ordering over the items induced from the users' preferences, as well as, the capacity of the dot-product defined in the learned embedded space to produce the ordering. The proposed model is by nature suitable for implicit feedback and involves the estimation of only very few parameters. Through extensive experiments on several real-world benchmarks on implicit data, we show the interest of learning the preference and the embedding simultaneously when compared to learning those separately. We also demonstrate that our approach is very competitive with the best state-of-the-art collaborative filtering techniques proposed for implicit feedback.\n    ",
        "submission_date": "2017-04-29T00:00:00",
        "last_modified_date": "2018-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00321",
        "title": "Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation",
        "authors": [
            "Ganbin Zhou",
            "Ping Luo",
            "Rongyu Cao",
            "Yijun Xiao",
            "Fen Lin",
            "Bo Chen",
            "Qing He"
        ],
        "abstract": "Different from other sequential data, sentences in natural language are structured by linguistic grammars. Previous generative conversational models with chain-structured decoder ignore this structure in human language and might generate plausible responses with less satisfactory relevance and fluency. In this study, we aim to incorporate the results from linguistic analysis into the process of sentence generation for high-quality conversation generation. Specifically, we use a dependency parser to transform each response sentence into a dependency tree and construct a training corpus of sentence-tree pairs. A tree-structured decoder is developed to learn the mapping from a sentence to its tree, where different types of hidden states are used to depict the local dependencies from an internal tree node to its children. For training acceleration, we propose a tree canonicalization method, which transforms trees into equivalent ternary trees. Then, with a proposed tree-structured search method, the model is able to generate the most probable responses in the form of dependency trees, which are finally flattened into sequences as the system output. Experimental results demonstrate that the proposed X2Tree framework outperforms baseline methods over 11.15% increase of acceptance ratio.\n    ",
        "submission_date": "2017-04-30T00:00:00",
        "last_modified_date": "2018-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00581",
        "title": "Query-adaptive Video Summarization via Quality-aware Relevance Estimation",
        "authors": [
            "Arun Balajee Vasudevan",
            "Michael Gygli",
            "Anna Volokitin",
            "Luc Van Gool"
        ],
        "abstract": "Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture query-independent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00601",
        "title": "The Promise of Premise: Harnessing Question Premises in Visual Question Answering",
        "authors": [
            "Aroma Mahendru",
            "Viraj Prabhu",
            "Akrit Mohapatra",
            "Dhruv Batra",
            "Stefan Lee"
        ],
        "abstract": "In this paper, we make a simple observation that questions about images often contain premises - objects and relationships implied by the question - and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions. When presented with a question that is irrelevant to an image, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in non-sensical or even misleading answers. We note that a visual question is irrelevant to an image if at least one of its premises is false (i.e. not depicted in the image). We leverage this observation to construct a dataset for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not. We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00694",
        "title": "A polynomial time algorithm for the Lambek calculus with brackets of bounded order",
        "authors": [
            "Max Kanovich",
            "Stepan Kuznetsov",
            "Glyn Morrill",
            "Andre Scedrov"
        ],
        "abstract": "Lambek calculus is a logical foundation of categorial grammar, a linguistic paradigm of grammar as logic and parsing as deduction. Pentus (2010) gave a polynomial-time algorithm for determ- ining provability of bounded depth formulas in the Lambek calculus with empty antecedents allowed. Pentus' algorithm is based on tabularisation of proof nets. Lambek calculus with brackets is a conservative extension of Lambek calculus with bracket modalities, suitable for the modeling of syntactical domains. In this paper we give an algorithm for provability the Lambek calculus with brackets allowing empty antecedents. Our algorithm runs in polynomial time when both the formula depth and the bracket nesting depth are bounded. It combines a Pentus-style tabularisation of proof nets with an automata-theoretic treatment of bracketing.\n    ",
        "submission_date": "2017-05-01T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.00995",
        "title": "Fuzzy Approach Topic Discovery in Health and Medical Corpora",
        "authors": [
            "Amir Karami",
            "Aryya Gangopadhyay",
            "Bin Zhou",
            "Hadi Kharrazi"
        ],
        "abstract": "The majority of medical documents and electronic health records (EHRs) are in text format that poses a challenge for data processing and finding relevant documents. Looking for ways to automatically retrieve the enormous amount of health and medical knowledge has always been an intriguing topic. Powerful methods have been developed in recent years to make the text processing automatic. One of the popular approaches to retrieve information based on discovering the themes in health & medical corpora is topic modeling, however, this approach still needs new perspectives. In this research we describe fuzzy latent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy perspective. FLSA can handle health & medical corpora redundancy issue and provides a new method to estimate the number of topics. The quantitative evaluations show that FLSA produces superior performance and features to latent Dirichlet allocation (LDA), the most popular topic model.\n    ",
        "submission_date": "2017-05-02T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01253",
        "title": "The Forgettable-Watcher Model for Video Question Answering",
        "authors": [
            "Hongyang Xue",
            "Zhou Zhao",
            "Deng Cai"
        ],
        "abstract": "A number of visual question answering approaches have been proposed recently, aiming at understanding the visual scenes by answering the natural language questions. While the image question answering has drawn significant attention, video question answering is largely unexplored.\n",
        "submission_date": "2017-05-03T00:00:00",
        "last_modified_date": "2017-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.01359",
        "title": "FOIL it! Find One mismatch between Image and Language caption",
        "authors": [
            "Ravi Shekhar",
            "Sandro Pezzelle",
            "Yauhen Klimovich",
            "Aurelie Herbelot",
            "Moin Nabi",
            "Enver Sangineto",
            "Raffaella Bernardi"
        ],
        "abstract": "In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates images with both correct and \"foil\" captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (\"foil word\"). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.\n    ",
        "submission_date": "2017-05-03T00:00:00",
        "last_modified_date": "2017-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02203",
        "title": "Analysis of Computational Science Papers from ICCS 2001-2016 using Topic Modeling and Graph Theory",
        "authors": [
            "Tesfamariam M. Abuhay",
            "Sergey V. Kovalchuk",
            "Klavdiya O. Bochenina",
            "George Kampis",
            "Valeria V. Krzhizhanovskaya",
            "Michael H. Lees"
        ],
        "abstract": "This paper presents results of topic modeling and network models of topics using the International Conference on Computational Science corpus, which contains domain-specific (computational science) papers over sixteen years (a total of 5695 papers). We discuss topical structures of International Conference on Computational Science, how these topics evolve over time in response to the topicality of various problems, technologies and methods, and how all these topics relate to one another. This analysis illustrates multidisciplinary research and collaborations among scientific communities, by constructing static and dynamic networks from the topic modeling results and the keywords of authors. The results of this study give insights about the past and future trends of core discussion topics in computational science. We used the Non-negative Matrix Factorization topic modeling algorithm to discover topics and labeled and grouped results hierarchically.\n    ",
        "submission_date": "2017-04-18T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02315",
        "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases",
        "authors": [
            "Xiaosong Wang",
            "Yifan Peng",
            "Le Lu",
            "Zhiyong Lu",
            "Mohammadhadi Bagheri",
            "Ronald M. Summers"
        ],
        "abstract": "The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems.\n",
        "submission_date": "2017-05-05T00:00:00",
        "last_modified_date": "2017-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02426",
        "title": "Analogical Inference for Multi-Relational Embeddings",
        "authors": [
            "Hanxiao Liu",
            "Yuexin Wu",
            "Yiming Yang"
        ],
        "abstract": "Large-scale multi-relational embedding refers to the task of learning the latent representations for entities and relations in large knowledge graphs. An effective and scalable solution for this problem is crucial for the true success of knowledge-based inference in a broad range of applications. This paper proposes a novel framework for optimizing the latent representations with respect to the \\textit{analogical} properties of the embedded entities and relations. By formulating the learning objective in a differentiable fashion, our model enjoys both theoretical power and computational scalability, and significantly outperformed a large number of representative baseline methods on benchmark datasets. Furthermore, the model offers an elegant unification of several well-known methods in multi-relational embedding, which can be proven to be special instantiations of our framework.\n    ",
        "submission_date": "2017-05-06T00:00:00",
        "last_modified_date": "2017-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02518",
        "title": "Exploring Latent Semantic Factors to Find Useful Product Reviews",
        "authors": [
            "Subhabrata Mukherjee",
            "Kashyap Popat",
            "Gerhard Weikum"
        ],
        "abstract": "Online reviews provided by consumers are a valuable asset for e-Commerce platforms, influencing potential consumers in making purchasing decisions. However, these reviews are of varying quality, with the useful ones buried deep within a heap of non-informative reviews. In this work, we attempt to automatically identify review quality in terms of its helpfulness to the end consumers. In contrast to previous works in this domain exploiting a variety of syntactic and community-level features, we delve deep into the semantics of reviews as to what makes them useful, providing interpretable explanation for the same. We identify a set of consistency and semantic factors, all from the text, ratings, and timestamps of user-generated reviews, making our approach generalizable across all communities and domains. We explore review semantics in terms of several latent factors like the expertise of its author, his judgment about the fine-grained facets of the underlying product, and his writing style. These are cast into a Hidden Markov Model -- Latent Dirichlet Allocation (HMM-LDA) based model to jointly infer: (i) reviewer expertise, (ii) item facets, and (iii) review helpfulness. Large-scale experiments on five real-world datasets from Amazon show significant improvement over state-of-the-art baselines in predicting and ranking useful reviews.\n    ",
        "submission_date": "2017-05-06T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02519",
        "title": "Item Recommendation with Evolving User Preferences and Experience",
        "authors": [
            "Subhabrata Mukherjee",
            "Hemank Lamba",
            "Gerhard Weikum"
        ],
        "abstract": "Current recommender systems exploit user and item similarities by collaborative filtering. Some advanced methods also consider the temporal evolution of item ratings as a global background process. However, all prior methods disregard the individual evolution of a user's experience level and how this is expressed in the user's writing in a review community. In this paper, we model the joint evolution of user experience, interest in specific item facets, writing style, and rating behavior. This way we can generate individual recommendations that take into account the user's maturity level (e.g., recommending art movies rather than blockbusters for a cinematography expert). As only item ratings and review texts are observables, we capture the user's experience and interests in a latent model learned from her reviews, vocabulary and writing style. We develop a generative HMM-LDA model to trace user evolution, where the Hidden Markov Model (HMM) traces her latent experience progressing over time -- with solely user reviews and ratings as observables over time. The facets of a user's interest are drawn from a Latent Dirichlet Allocation (LDA) model derived from her reviews, as a function of her (again latent) experience level. In experiments with five real-world datasets, we show that our model improves the rating prediction over state-of-the-art baselines, by a substantial margin. We also show, in a use-case study, that our model performs well in the assessment of user experience levels.\n    ",
        "submission_date": "2017-05-06T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02522",
        "title": "People on Drugs: Credibility of User Statements in Health Communities",
        "authors": [
            "Subhabrata Mukherjee",
            "Gerhard Weikum",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Online health communities are a valuable source of information for patients and physicians. However, such user-generated resources are often plagued by inaccuracies and misinformation. In this work we propose a method for automatically establishing the credibility of user-generated medical statements and the trustworthiness of their authors by exploiting linguistic cues and distant supervision from expert sources. To this end we introduce a probabilistic graphical model that jointly learns user trustworthiness, statement credibility, and language objectivity. We apply this methodology to the task of extracting rare or unknown side-effects of medical drugs --- this being one of the problems where large scale non-expert data has the potential to complement expert medical knowledge. We show that our method can reliably extract side-effects and filter out false statements, while identifying trustworthy users that are likely to contribute valuable medical information.\n    ",
        "submission_date": "2017-05-06T00:00:00",
        "last_modified_date": "2017-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02667",
        "title": "People on Media: Jointly Identifying Credible News and Trustworthy Citizen Journalists in Online Communities",
        "authors": [
            "Subhabrata Mukherjee",
            "Gerhard Weikum"
        ],
        "abstract": "Media seems to have become more partisan, often providing a biased coverage of news catering to the interest of specific groups. It is therefore essential to identify credible information content that provides an objective narrative of an event. News communities such as digg, reddit, or newstrust offer recommendations, reviews, quality ratings, and further insights on journalistic works. However, there is a complex interaction between different factors in such online communities: fairness and style of reporting, language clarity and objectivity, topical perspectives (like political viewpoint), expertise and bias of community members, and more. This paper presents a model to systematically analyze the different interactions in a news community between users, news, and sources. We develop a probabilistic graphical model that leverages this joint interaction to identify 1) highly credible news articles, 2) trustworthy news sources, and 3) expert users who perform the role of \"citizen journalists\" in the community. Our method extends CRF models to incorporate real-valued ratings, as some communities have very fine-grained scales that cannot be easily discretized without losing information. To the best of our knowledge, this paper is the first full-fledged analysis of credibility, trust, and expertise in news communities.\n    ",
        "submission_date": "2017-05-07T00:00:00",
        "last_modified_date": "2017-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02668",
        "title": "Credible Review Detection with Limited Information using Consistency Analysis",
        "authors": [
            "Subhabrata Mukherjee",
            "Sourav Dutta",
            "Gerhard Weikum"
        ],
        "abstract": "Online reviews provide viewpoints on the strengths and shortcomings of products/services, influencing potential customers' purchasing decisions. However, the proliferation of non-credible reviews -- either fake (promoting/ demoting an item), incompetent (involving irrelevant aspects), or biased -- entails the problem of identifying credible reviews. Prior works involve classifiers harnessing rich information about items/users -- which might not be readily available in several domains -- that provide only limited interpretability as to why a review is deemed non-credible. This paper presents a novel approach to address the above issues. We utilize latent topic models leveraging review texts, item ratings, and timestamps to derive consistency features without relying on item/user histories, unavailable for \"long-tail\" items/users. We develop models, for computing review credibility scores to provide interpretable evidence for non-credible reviews, that are also transferable to other domains -- addressing the scarcity of labeled data. Experiments on real-world datasets demonstrate improvements over state-of-the-art baselines.\n    ",
        "submission_date": "2017-05-07T00:00:00",
        "last_modified_date": "2017-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.02669",
        "title": "Item Recommendation with Continuous Experience Evolution of Users using Brownian Motion",
        "authors": [
            "Subhabrata Mukherjee",
            "Stephan Guennemann",
            "Gerhard Weikum"
        ],
        "abstract": "Online review communities are dynamic as users join and leave, adopt new vocabulary, and adapt to evolving trends. Recent work has shown that recommender systems benefit from explicit consideration of user experience. However, prior work assumes a fixed number of discrete experience levels, whereas in reality users gain experience and mature continuously over time. This paper presents a new model that captures the continuous evolution of user experience, and the resulting language model in reviews and other posts. Our model is unsupervised and combines principles of Geometric Brownian Motion, Brownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal progression of user experience and language model respectively. We develop practical algorithms for estimating the model parameters from data and for inference with our model (e.g., to recommend items). Extensive experiments with five real-world datasets show that our model not only fits data better than discrete-model baselines, but also outperforms state-of-the-art methods for predicting item ratings.\n    ",
        "submission_date": "2017-05-07T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03487",
        "title": "A neural network system for transformation of regional cuisine style",
        "authors": [
            "Masahiro Kazama",
            "Minami Sugimoto",
            "Chizuru Hosokawa",
            "Keisuke Matsushima",
            "Lav R. Varshney",
            "Yoshiki Ishikawa"
        ],
        "abstract": "We propose a novel system which can transform a recipe into any selected regional style (e.g., Japanese, Mediterranean, or Italian). This system has two characteristics. First the system can identify the degree of regional cuisine style mixture of any selected recipe and visualize such regional cuisine style mixtures using barycentric Newton diagrams. Second, the system can suggest ingredient substitutions through an extended word2vec model, such that a recipe becomes more authentic for any selected regional cuisine style. Drawing on a large number of recipes from Yummly, an example shows how the proposed system can transform a traditional Japanese recipe, Sukiyaki, into French style.\n    ",
        "submission_date": "2017-05-06T00:00:00",
        "last_modified_date": "2018-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03556",
        "title": "Relevance-based Word Embedding",
        "authors": [
            "Hamed Zamani",
            "W. Bruce Croft"
        ],
        "abstract": "Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently attracted much attention in natural language processing and information retrieval tasks. The embedding vectors are typically learned based on term proximity in a large corpus. This means that the objective in well-known word embedding algorithms, e.g., word2vec, is to accurately predict adjacent word(s) for a given word or context. However, this objective is not necessarily equivalent to the goal of many information retrieval (IR) tasks. The primary objective in various IR tasks is to capture relevance instead of term proximity, syntactic, or even semantic similarity. This is the motivation for developing unsupervised relevance-based word embedding models that learn word representations based on query-document relevance information. In this paper, we propose two learning models with different objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classifies each term as belonging to the relevant or non-relevant class for each query. To train our models, we used over six million unique queries and the top ranked documents retrieved in response to each query, which are assumed to be relevant to the query. We extrinsically evaluate our learned word representation models using two IR tasks: query expansion and query classification. Both query expansion experiments on four TREC collections and query classification experiments on the KDD Cup 2005 dataset suggest that the relevance-based word embedding models significantly outperform state-of-the-art proximity-based embedding models, such as word2vec and GloVe.\n    ",
        "submission_date": "2017-05-09T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03633",
        "title": "Inferring and Executing Programs for Visual Reasoning",
        "authors": [
            "Justin Johnson",
            "Bharath Hariharan",
            "Laurens van der Maaten",
            "Judy Hoffman",
            "Li Fei-Fei",
            "C. Lawrence Zitnick",
            "Ross Girshick"
        ],
        "abstract": "Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03670",
        "title": "Deep Speaker Feature Learning for Text-independent Speaker Verification",
        "authors": [
            "Lantian Li",
            "Yixiang Chen",
            "Ying Shi",
            "Zhiyuan Tang",
            "Dong Wang"
        ],
        "abstract": "Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification, just as with raw features. This paper presents a convolutional time-delay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce high-quality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames.\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03751",
        "title": "A Survey of Distant Supervision Methods using PGMs",
        "authors": [
            "Gagan Madan"
        ],
        "abstract": "Relation Extraction refers to the task of populating a database with tuples of the form $r(e_1, e_2)$, where $r$ is a relation and $e_1$, $e_2$ are entities. Distant supervision is one such technique which tries to automatically generate training examples based on an existing KB such as Freebase. This paper is a survey of some of the techniques in distant supervision which primarily rely on Probabilistic Graphical Models (PGMs).\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.03773",
        "title": "Flexible and Creative Chinese Poetry Generation Using Neural Memory",
        "authors": [
            "Jiyuan Zhang",
            "Yang Feng",
            "Dong Wang",
            "Yang Wang",
            "Andrew Abel",
            "Shiyue Zhang",
            "Andi Zhang"
        ],
        "abstract": "It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.\n    ",
        "submission_date": "2017-05-10T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.04146",
        "title": "Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems",
        "authors": [
            "Wang Ling",
            "Dani Yogatama",
            "Chris Dyer",
            "Phil Blunsom"
        ],
        "abstract": "Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.\n    ",
        "submission_date": "2017-05-11T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05311",
        "title": "Using Titles vs. Full-text as Source for Automated Semantic Document Annotation",
        "authors": [
            "Lukas Galke",
            "Florian Mai",
            "Alan Schelten",
            "Dennis Brunsch",
            "Ansgar Scherp"
        ],
        "abstract": "A significant part of the largest Knowledge Graph today, the Linked Open Data cloud, consists of metadata about documents such as publications, news reports, and other media articles. While the widespread access to the document metadata is a tremendous advancement, it is yet not so easy to assign semantic annotations and organize the documents along semantic concepts. Providing semantic annotations like concepts in SKOS thesauri is a classical research topic, but typically it is conducted on the full-text of the documents. For the first time, we offer a systematic comparison of classification approaches to investigate how far semantic annotations can be conducted using just the metadata of the documents such as titles published as labels on the Linked Open Data cloud. We compare the classifications obtained from analyzing the documents' titles with semantic annotations obtained from analyzing the full-text. Apart from the prominent text classification baselines kNN and SVM, we also compare recent techniques of Learning to Rank and neural networks and revisit the traditional methods logistic regression, Rocchio, and Naive Bayes. The results show that across three of our four datasets, the performance of the classifications using only titles reaches over 90% of the quality compared to the classification performance when using the full-text. Thus, conducting document classification by just using the titles is a reasonable approach for automated semantic annotation and opens up new possibilities for enriching Knowledge Graphs.\n    ",
        "submission_date": "2017-05-15T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05742",
        "title": "Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs",
        "authors": [
            "Rakshit Trivedi",
            "Hanjun Dai",
            "Yichen Wang",
            "Le Song"
        ],
        "abstract": "The availability of large scale event data with time stamps has given rise to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood. To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact (edge) is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embeddings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting.\n    ",
        "submission_date": "2017-05-16T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.05762",
        "title": "A decentralized route to the origins of scaling in human language",
        "authors": [
            "Felipe Urbina",
            "Javier Vera"
        ],
        "abstract": "The Zipf's law establishes that if the words of a (large) text are ordered by decreasing frequency, the frequency versus the rank decreases as a power law with exponent close to $-1$. Previous work has stressed that this pattern arises from a conflict of interests of the participants of communication. The challenge here is to define a computational multi-agent language game, mainly based on a parameter that measures the relative participant's interests. Numerical simulations suggest that at critical values of the parameter a human-like vocabulary, exhibiting scaling properties, seems to appear. The appearance of an intermediate distribution of frequencies at some critical values of the parameter suggests that on a population of artificial agents the emergence of scaling partly arises as a self-organized process only from local interactions between agents.\n    ",
        "submission_date": "2017-05-16T00:00:00",
        "last_modified_date": "2019-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06400",
        "title": "Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks",
        "authors": [
            "Matthias Plappert",
            "Christian Mandery",
            "Tamim Asfour"
        ],
        "abstract": "Linking human whole-body motion and natural language is of great interest for the generation of semantic representations of observed human behaviors as well as for the generation of robot behaviors based on natural language input. While there has been a large body of research in this area, most approaches that exist today require a symbolic representation of motions (e.g. in the form of motion primitives), which have to be defined a-priori or require complex segmentation algorithms. In contrast, recent advances in the field of neural networks and especially deep learning have demonstrated that sub-symbolic representations that can be learned end-to-end usually outperform more traditional approaches, for applications such as machine translation. In this paper we propose a generative model that learns a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks (RNNs) and sequence-to-sequence learning. Our approach does not require any segmentation or manual feature engineering and learns a distributed representation, which is shared for all motions and descriptions. We evaluate our approach on 2,846 human whole-body motions and 6,187 natural language descriptions thereof from the KIT Motion-Language Dataset. Our results clearly demonstrate the effectiveness of the proposed model: We show that our model generates a wide variety of realistic motions only from descriptions thereof in form of a single sentence. Conversely, our model is also capable of generating correct and detailed natural language descriptions from human motions.\n    ",
        "submission_date": "2017-05-18T00:00:00",
        "last_modified_date": "2018-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06510",
        "title": "Entropic selection of concepts unveils hidden topics in documents corpora",
        "authors": [
            "Andrea Martini",
            "Alessio Cardillo",
            "Paolo De Los Rios"
        ],
        "abstract": "The organization and evolution of science has recently become itself an object of scientific quantitative investigation, thanks to the wealth of information that can be extracted from scientific documents, such as citations between papers and co-authorship between researchers. However, only few studies have focused on the concepts that characterize full documents and that can be extracted and analyzed, revealing the deeper organization of scientific knowledge. Unfortunately, several concepts can be so common across documents that they hinder the emergence of the underlying topical structure of the document corpus, because they give rise to a large amount of spurious and trivial relations among documents. To identify and remove common concepts, we introduce a method to gauge their relevance according to an objective information-theoretic measure related to the statistics of their occurrence across the document corpus. After progressively removing concepts that, according to this metric, can be considered as generic, we find that the topic organization displays a correspondingly more refined structure.\n    ",
        "submission_date": "2017-05-18T00:00:00",
        "last_modified_date": "2018-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.06824",
        "title": "Learning Convolutional Text Representations for Visual Question Answering",
        "authors": [
            "Zhengyang Wang",
            "Shuiwang Ji"
        ],
        "abstract": "Visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. In deep learning, images are typically modeled through convolutional neural networks, and texts are typically modeled through recurrent neural networks. While the requirement for modeling images is similar to traditional computer vision tasks, such as object recognition and image classification, visual question answering raises a different need for textual representation as compared to other natural language processing tasks. In this work, we perform a detailed analysis on natural language questions in visual question answering. Based on the analysis, we propose to rely on convolutional neural networks for learning textual representations. By exploring the various properties of convolutional neural networks specialized for text data, such as width and depth, we present our \"CNN Inception + Gate\" model. We show that our model improves question representations and thus the overall accuracy of visual question answering models. We also show that the text representation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks, making it a better task to evaluate textual representation methods. Shallow models like fastText, which can obtain comparable results with deep learning models in tasks like text classification, are not suitable in visual question answering.\n    ",
        "submission_date": "2017-05-18T00:00:00",
        "last_modified_date": "2018-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07136",
        "title": "Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML",
        "authors": [
            "Xuezhe Ma",
            "Pengcheng Yin",
            "Jingzhou Liu",
            "Graham Neubig",
            "Eduard Hovy"
        ],
        "abstract": "Reward augmented maximum likelihood (RAML), a simple and effective learning framework to directly optimize towards the reward function in structured prediction tasks, has led to a number of impressive empirical successes. RAML incorporates task-specific reward by performing maximum-likelihood updates on candidate outputs sampled according to an exponentiated payoff distribution, which gives higher probabilities to candidates that are close to the reference output. While RAML is notable for its simplicity, efficiency, and its impressive empirical successes, the theoretical properties of RAML, especially the behavior of the exponentiated payoff distribution, has not been examined thoroughly. In this work, we introduce softmax Q-distribution estimation, a novel theoretical interpretation of RAML, which reveals the relation between RAML and Bayesian decision theory. The softmax Q-distribution can be regarded as a smooth approximation of the Bayes decision boundary, and the Bayes decision rule is achieved by decoding with this Q-distribution. We further show that RAML is equivalent to approximately estimating the softmax Q-distribution, with the temperature $\\tau$ controlling approximation error. We perform two experiments, one on synthetic data of multi-class classification and one on real data of image captioning, to demonstrate the relationship between RAML and the proposed softmax Q-distribution estimation method, verifying our theoretical analysis. Additional experiments on three structured prediction tasks with rewards defined on sequential (named entity recognition), tree-based (dependency parsing) and irregular (machine translation) structures show notable improvements over maximum likelihood baselines.\n    ",
        "submission_date": "2017-05-19T00:00:00",
        "last_modified_date": "2017-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07704",
        "title": "A Regularized Framework for Sparse and Structured Neural Attention",
        "authors": [
            "Vlad Niculae",
            "Mathieu Blondel"
        ],
        "abstract": "Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in more interpretable attention mechanisms, that focus on entire segments or groups of an input. We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing ones, we evaluate our attention mechanisms on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the standard attention mechanisms based on softmax and sparsemax.\n    ",
        "submission_date": "2017-05-22T00:00:00",
        "last_modified_date": "2019-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07860",
        "title": "On-the-fly Operation Batching in Dynamic Computation Graphs",
        "authors": [
            "Graham Neubig",
            "Yoav Goldberg",
            "Chris Dyer"
        ],
        "abstract": "Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.\n    ",
        "submission_date": "2017-05-22T00:00:00",
        "last_modified_date": "2017-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.07962",
        "title": "pix2code: Generating Code from a Graphical User Interface Screenshot",
        "authors": [
            "Tony Beltramelli"
        ],
        "abstract": "Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).\n    ",
        "submission_date": "2017-05-22T00:00:00",
        "last_modified_date": "2017-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08094",
        "title": "TwiInsight: Discovering Topics and Sentiments from Social Media Datasets",
        "authors": [
            "Zhengkui Wang",
            "Guangdong Bai",
            "Soumyadeb Chowdhury",
            "Quanqing Xu",
            "Zhi Lin Seow"
        ],
        "abstract": "Social media platforms contain a great wealth of information which provides opportunities for us to explore hidden patterns or unknown correlations, and understand people's satisfaction with what they are discussing. As one showcase, in this paper, we present a system, TwiInsight which explores the insight of Twitter data. Different from other Twitter analysis systems, TwiInsight automatically extracts the popular topics under different categories (e.g., healthcare, food, technology, sports and transport) discussed in Twitter via topic modeling and also identifies the correlated topics across different categories. Additionally, it also discovers the people's opinions on the tweets and topics via the sentiment analysis. The system also employs an intuitive and informative visualization to show the uncovered insight. Furthermore, we also develop and compare six most popular algorithms - three for sentiment analysis and three for topic modeling.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2017-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08142",
        "title": "Latent Multi-task Architecture Learning",
        "authors": [
            "Sebastian Ruder",
            "Joachim Bingel",
            "Isabelle Augenstein",
            "Anders S\u00f8gaard"
        ],
        "abstract": "Multi-task learning (MTL) allows deep neural networks to learn from related tasks by sharing parameters with other networks. In practice, however, MTL involves searching an enormous space of possible parameter sharing architectures to find (a) the layers or subspaces that benefit from sharing, (b) the appropriate amount of sharing, and (c) the appropriate relative weights of the different task losses. Recent work has addressed each of the above problems in isolation. In this work we present an approach that learns a latent multi-task architecture that jointly addresses (a)--(c). We present experiments on synthetic data and data from OntoNotes 5.0, including four different tasks and seven different domains. Our extension consistently outperforms previous approaches to learning latent architectures for multi-task problems and achieves up to 15% average error reductions over common approaches to MTL.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2018-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08321",
        "title": "Increasing Papers' Discoverability with Precise Semantic Labeling: the sci.AI Platform",
        "authors": [
            "Roman Gurinovich",
            "Alexander Pashuk",
            "Yuriy Petrovskiy",
            "Alex Dmitrievskij",
            "Oleg Kuryan",
            "Alexei Scerbacov",
            "Antonia Tiggre",
            "Elena Moroz",
            "Yuri Nikolsky"
        ],
        "abstract": "The number of published findings in biomedicine increases continually. At the same time, specifics of the domain's terminology complicates the task of relevant publications retrieval. In the current research, we investigate influence of terms' variability and ambiguity on a paper's likelihood of being retrieved. We obtained statistics that demonstrate significance of the issue and its challenges, followed by presenting the ",
        "submission_date": "2017-05-02T00:00:00",
        "last_modified_date": "2017-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08557",
        "title": "Grounded Recurrent Neural Networks",
        "authors": [
            "Ankit Vani",
            "Yacine Jernite",
            "David Sontag"
        ],
        "abstract": "In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process \"grounding\"). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2017-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.08992",
        "title": "Matroids Hitting Sets and Unsupervised Dependency Grammar Induction",
        "authors": [
            "Nicholas Harvey",
            "Vahab Mirrokni",
            "David Karger",
            "Virginia Savova",
            "Leonid Peshkin"
        ],
        "abstract": "This paper formulates a novel problem on graphs: find the minimal subset of edges in a fully connected graph, such that the resulting graph contains all spanning trees for a set of specifed sub-graphs. This formulation is motivated by an un-supervised grammar induction problem from computational linguistics. We present a reduction to some known problems and algorithms from graph theory, provide computational complexity results, and describe an approximation algorithm.\n    ",
        "submission_date": "2017-05-24T00:00:00",
        "last_modified_date": "2017-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09037",
        "title": "Deriving Neural Architectures from Sequence and Graph Kernels",
        "authors": [
            "Tao Lei",
            "Wengong Jin",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ],
        "abstract": "The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.\n    ",
        "submission_date": "2017-05-25T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09222",
        "title": "Towards a Knowledge Graph based Speech Interface",
        "authors": [
            "Ashwini Jaya Kumar",
            "S\u00f6ren Auer",
            "Christoph Schmidt",
            "Joachim k\u00f6hler"
        ],
        "abstract": "Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.\n    ",
        "submission_date": "2017-05-23T00:00:00",
        "last_modified_date": "2017-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09296",
        "title": "Neural Models for Documents with Metadata",
        "authors": [
            "Dallas Card",
            "Chenhao Tan",
            "Noah A. Smith"
        ],
        "abstract": "Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.\n    ",
        "submission_date": "2017-05-25T00:00:00",
        "last_modified_date": "2018-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09665",
        "title": "Community Identity and User Engagement in a Multi-Community Landscape",
        "authors": [
            "Justine Zhang",
            "William L. Hamilton",
            "Cristian Danescu-Niculescu-Mizil",
            "Dan Jurafsky",
            "Jure Leskovec"
        ],
        "abstract": "A community's identity defines and shapes its internal dynamics. Our current understanding of this interplay is mostly limited to glimpses gathered from isolated studies of individual communities. In this work we provide a systematic exploration of the nature of this relation across a wide variety of online communities. To this end we introduce a quantitative, language-based typology reflecting two key aspects of a community's identity: how distinctive, and how temporally dynamic it is. By mapping almost 300 Reddit communities into the landscape induced by this typology, we reveal regularities in how patterns of user engagement vary with the characteristics of a community.\n",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09731",
        "title": "Multiplex model of mental lexicon reveals explosive learning in humans",
        "authors": [
            "Massimo Stella",
            "Nicole M. Beckage",
            "Markus Brede",
            "Manlio De Domenico"
        ],
        "abstract": "Word similarities affect language acquisition and use in a multi-relational way barely accounted for in the literature. We propose a multiplex network representation of this mental lexicon of word similarities as a natural framework for investigating large-scale cognitive patterns. Our representation accounts for semantic, taxonomic, and phonological interactions and it identifies a cluster of words which are used with greater frequency, are identified, memorised, and learned more easily, and have more meanings than expected at random. This cluster emerges around age 7 through an explosive transition not reproduced by null models. We relate this explosive emergence to polysemy -- redundancy in word meanings. Results indicate that the word cluster acts as a core for the lexicon, increasing both lexical navigability and robustness to linguistic degradation. Our findings provide quantitative confirmation of existing conjectures about core structure in the mental lexicon and the importance of integrating multi-relational word-word interactions in psycholinguistic frameworks.\n    ",
        "submission_date": "2017-05-26T00:00:00",
        "last_modified_date": "2018-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.09975",
        "title": "A Deep Multi-View Learning Framework for City Event Extraction from Twitter Data Streams",
        "authors": [
            "Nazli Farajidavar",
            "Sefki Kolozali",
            "Payam Barnaghi"
        ],
        "abstract": "Cities have been a thriving place for citizens over the centuries due to their complex infrastructure. The emergence of the Cyber-Physical-Social Systems (CPSS) and context-aware technologies boost a growing interest in analysing, extracting and eventually understanding city events which subsequently can be utilised to leverage the citizen observations of their cities. In this paper, we investigate the feasibility of using Twitter textual streams for extracting city events. We propose a hierarchical multi-view deep learning approach to contextualise citizen observations of various city systems and services. Our goal has been to build a flexible architecture that can learn representations useful for tasks, thus avoiding excessive task-specific feature engineering. We apply our approach on a real-world dataset consisting of event reports and tweets of over four months from San Francisco Bay Area dataset and additional datasets collected from London. The results of our evaluations show that our proposed solution outperforms the existing models and can be used for extracting city related events with an averaged accuracy of 81% over all classes. To further evaluate the impact of our Twitter event extraction model, we have used two sources of authorised reports through collecting road traffic disruptions data from Transport for London API, and parsing the Time Out London website for sociocultural events. The analysis showed that 49.5% of the Twitter traffic comments are reported approximately five hours prior to the authorities official records. Moreover, we discovered that amongst the scheduled sociocultural event topics; tweets reporting transportation, cultural and social events are 31.75% more likely to influence the distribution of the Twitter comments than sport, weather and crime topics.\n    ",
        "submission_date": "2017-05-28T00:00:00",
        "last_modified_date": "2017-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10369",
        "title": "Emergent Communication in a Multi-Modal, Multi-Step Referential Game",
        "authors": [
            "Katrina Evtimova",
            "Andrew Drozdov",
            "Douwe Kiela",
            "Kyunghyun Cho"
        ],
        "abstract": "Inspired by previous work on emergent communication in referential games, we propose a novel multi-modal, multi-step referential game, where the sender and receiver have access to distinct modalities of an object, and their information exchange is bidirectional and of arbitrary duration. The multi-modal multi-step setting allows agents to develop an internal communication significantly closer to natural language, in that they share a single set of messages, and that the length of the conversation may vary according to the difficulty of the task. We examine these properties empirically using a dataset consisting of images and textual descriptions of mammals, where the agents are tasked with identifying the correct object. Our experiments indicate that a robust and efficient communication protocol emerges, where gradual information exchange informs better predictions and higher communication bandwidth improves generalization.\n    ",
        "submission_date": "2017-05-29T00:00:00",
        "last_modified_date": "2018-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.10874",
        "title": "Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments",
        "authors": [
            "Zixing Zhang",
            "J\u00fcrgen Geiger",
            "Jouni Pohjalainen",
            "Amr El-Desoky Mousa",
            "Wenyu Jin",
            "Bj\u00f6rn Schuller"
        ],
        "abstract": "Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition that stills remains an important challenge. Data-driven supervised approaches, including ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks.\n    ",
        "submission_date": "2017-05-30T00:00:00",
        "last_modified_date": "2018-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.11122",
        "title": "Controllable Invariance through Adversarial Feature Learning",
        "authors": [
            "Qizhe Xie",
            "Zihang Dai",
            "Yulun Du",
            "Eduard Hovy",
            "Graham Neubig"
        ],
        "abstract": "Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.\n    ",
        "submission_date": "2017-05-31T00:00:00",
        "last_modified_date": "2018-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1705.11192",
        "title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols",
        "authors": [
            "Serhii Havrylov",
            "Ivan Titov"
        ],
        "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.\n    ",
        "submission_date": "2017-05-31T00:00:00",
        "last_modified_date": "2017-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00286",
        "title": "Learning to Compute Word Embeddings On the Fly",
        "authors": [
            "Dzmitry Bahdanau",
            "Tom Bosc",
            "Stanis\u0142aw Jastrz\u0119bski",
            "Edward Grefenstette",
            "Pascal Vincent",
            "Yoshua Bengio"
        ],
        "abstract": "Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the \"long tail\" of this distribution requires enormous amounts of data. Representations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2018-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00290",
        "title": "Transfer Learning for Speech Recognition on a Budget",
        "authors": [
            "Julius Kunze",
            "Louis Kirsch",
            "Ilia Kurenkov",
            "Andreas Krug",
            "Jens Johannsmeier",
            "Sebastian Stober"
        ],
        "abstract": "End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.\n    ",
        "submission_date": "2017-06-01T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.00887",
        "title": "Wikipedia Vandal Early Detection: from User Behavior to User Embedding",
        "authors": [
            "Shuhan Yuan",
            "Panpan Zheng",
            "Xintao Wu",
            "Yang Xiang"
        ],
        "abstract": "Wikipedia is the largest online encyclopedia that allows anyone to edit articles. In this paper, we propose the use of deep learning to detect vandals based on their edit history. In particular, we develop a multi-source long-short term memory network (M-LSTM) to model user behaviors by using a variety of user edit aspects as inputs, including the history of edit reversion information, edit page titles and categories. With M-LSTM, we can encode each user into a low dimensional real vector, called user embedding. Meanwhile, as a sequential model, M-LSTM updates the user embedding each time after the user commits a new edit. Thus, we can predict whether a user is benign or vandal dynamically based on the up-to-date user embedding. Furthermore, those user embeddings are crucial to discover collaborative vandals.\n    ",
        "submission_date": "2017-06-03T00:00:00",
        "last_modified_date": "2017-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01038",
        "title": "Improving Legal Information Retrieval by Distributional Composition with Term Order Probabilities",
        "authors": [
            "Danilo S. Carvalho",
            "Duc-Vu Tran",
            "Van-Khanh Tran",
            "Le-Nguyen Minh"
        ],
        "abstract": "Legal professionals worldwide are currently trying to get up-to-pace with the explosive growth in legal document availability through digital means. This drives a need for high efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods. The IR task in particular has a set of unique challenges that invite the use of semantic motivated NLP techniques. In this work, a two-stage method for Legal Information Retrieval is proposed, combining lexical statistics and distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done with the use of disambiguation rules, applied over the rankings obtained through n-gram statistics. After the ranking is done, its results are evaluated for ambiguity, and disambiguation is done if a result is decided to be unreliable for a given query. Competition and experimental results indicate small gains in overall retrieval performance using the proposed approach. Additionally, an analysis of error and improvement cases is presented for a better understanding of the contributions.\n    ",
        "submission_date": "2017-06-04T00:00:00",
        "last_modified_date": "2017-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01084",
        "title": "Joint Text Embedding for Personalized Content-based Recommendation",
        "authors": [
            "Ting Chen",
            "Liangjie Hong",
            "Yue Shi",
            "Yizhou Sun"
        ],
        "abstract": "Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned effectively from user interaction data, in many cases, such data is not available, especially for newly emerged items.\n",
        "submission_date": "2017-06-04T00:00:00",
        "last_modified_date": "2017-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.01554",
        "title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model",
        "authors": [
            "Jiasen Lu",
            "Anitha Kannan",
            "Jianwei Yang",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses (\"I don't know\", \"I can't tell\"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it cannot be deployed to have real conversations with users.\n",
        "submission_date": "2017-06-05T00:00:00",
        "last_modified_date": "2017-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02222",
        "title": "Gated Recurrent Neural Tensor Network",
        "authors": [
            "Andros Tjandra",
            "Sakriani Sakti",
            "Ruli Manurung",
            "Mirna Adriani",
            "Satoshi Nakamura"
        ],
        "abstract": "Recurrent Neural Networks (RNNs), which are a powerful scheme for modeling temporal and sequential data need to capture long-term dependencies on datasets and represent them in hidden layers with a powerful model to capture more information from inputs. For modeling long-term dependencies in a dataset, the gating mechanism concept can help RNNs remember and forget previous information. Representing the hidden layers of an RNN with more expressive operations (i.e., tensor products) helps it learn a more complex relationship between the current input and the previous hidden layer information. These ideas can generally improve RNN performances. In this paper, we proposed a novel RNN architecture that combine the concepts of gating mechanism and the tensor product into a single model. By combining these two concepts into a single RNN, our proposed models learn long-term dependencies by modeling with gating units and obtain more expressive and direct interaction between input and hidden layers using a tensor product on 3-dimensional array (tensor) weight parameters. We use Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit (GRU) RNN and combine them with a tensor product inside their formulations. Our proposed RNNs, which are called a Long-Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN) and Gated Recurrent Unit Recurrent Neural Tensor Network (GRURNTN), are made by combining the LSTM and GRU RNN models with the tensor product. We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models.\n    ",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2017-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02490",
        "title": "Where is my forearm? Clustering of body parts from simultaneous tactile and linguistic input using sequential mapping",
        "authors": [
            "Karla Stepanova",
            "Matej Hoffmann",
            "Zdenek Straka",
            "Frederico B. Klein",
            "Angelo Cangelosi",
            "Michal Vavrecka"
        ],
        "abstract": "Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. At the same time, they form more compressed representations like concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established. There is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts (cross-situational learning). In existing models, the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences. In this paper, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available Matlab implementation. For demonstration, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body representations. A humanoid robot is receiving tactile stimulations on its body, while at the same time listening to utterances of the body part names (e.g., hand, forearm and torso). With the goal at arriving at the correct \"body categories\", we demonstrate how a sequential mapping algorithm outperforms one-step mapping. In addition, the effect of data set size and noise in the linguistic input are studied.\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02496",
        "title": "Context encoders as a simple but powerful extension of word2vec",
        "authors": [
            "Franziska Horn"
        ],
        "abstract": "With a simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings. Additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for a word with multiple meanings can be created based on the word's local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02757",
        "title": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction",
        "authors": [
            "Jekaterina Novikova",
            "Christian Dondrup",
            "Ioannis Papaioannou",
            "Oliver Lemon"
        ],
        "abstract": "Recognition of social signals, from human facial expressions or prosody of speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users' impressions of the robot after a conversation. We find that happiness in the user's recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learning based adaptive human-robot dialogue systems.\n    ",
        "submission_date": "2017-06-08T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.02901",
        "title": "Characterizing Types of Convolution in Deep Convolutional Recurrent Neural Networks for Robust Speech Emotion Recognition",
        "authors": [
            "Che-Wei Huang",
            "Shrikanth. S. Narayanan"
        ],
        "abstract": "Deep convolutional neural networks are being actively investigated in a wide range of speech and audio processing applications including speech recognition, audio event detection and computational paralinguistics, owing to their ability to reduce factors of variations, for learning from speech. However, studies have suggested to favor a certain type of convolutional operations when building a deep convolutional neural network for speech applications although there has been promising results using different types of convolutional operations. In this work, we study four types of convolutional operations on different input features for speech emotion recognition under noisy and clean conditions in order to derive a comprehensive understanding. Since affective behavioral information has been shown to reflect temporally varying of mental state and convolutional operation are applied locally in time, all deep neural networks share a deep recurrent sub-network architecture for further temporal modeling. We present detailed quantitative module-wise performance analysis to gain insights into information flows within the proposed architectures. In particular, we demonstrate the interplay of affective information and the other irrelevant information during the progression from one module to another. Finally we show that all of our deep neural networks provide state-of-the-art performance on the eNTERFACE'05 corpus.\n    ",
        "submission_date": "2017-06-07T00:00:00",
        "last_modified_date": "2018-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03196",
        "title": "Online Learning for Neural Machine Translation Post-editing",
        "authors": [
            "\u00c1lvaro Peris",
            "Luis Cebri\u00e1n",
            "Francisco Casacuberta"
        ],
        "abstract": "Neural machine translation has meant a revolution of the field. Nevertheless, post-editing the outputs of the system is mandatory for tasks requiring high translation quality. Post-editing offers a unique opportunity for improving neural machine translation systems, using online learning techniques and treating the post-edited translations as new, fresh training data. We review classical learning methods and propose a new optimization algorithm. We thoroughly compare online learning algorithms in a post-editing scenario. Results show significant improvements in translation quality and effort reduction.\n    ",
        "submission_date": "2017-06-10T00:00:00",
        "last_modified_date": "2017-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.03850",
        "title": "Adversarial Feature Matching for Text Generation",
        "authors": [
            "Yizhe Zhang",
            "Zhe Gan",
            "Kai Fan",
            "Zhi Chen",
            "Ricardo Henao",
            "Dinghan Shen",
            "Lawrence Carin"
        ],
        "abstract": "The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long short-term memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.\n    ",
        "submission_date": "2017-06-12T00:00:00",
        "last_modified_date": "2017-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04223",
        "title": "Adversarially Regularized Autoencoders",
        "authors": [
            "Jake Zhao",
            "Yoon Kim",
            "Kelly Zhang",
            "Alexander M. Rush",
            "Yann LeCun"
        ],
        "abstract": "Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently-proposed Wasserstein autoencoder (WAE) which formalizes the adversarial autoencoder (AAE) as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. This adversarially regularized autoencoder (ARAE) allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic/human evaluation compared to existing methods.\n    ",
        "submission_date": "2017-06-13T00:00:00",
        "last_modified_date": "2018-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04432",
        "title": "Is Natural Language a Perigraphic Process? The Theorem about Facts and Words Revisited",
        "authors": [
            "\u0141ukasz D\u0119bowski"
        ],
        "abstract": "As we discuss, a stationary stochastic process is nonergodic when a random persistent topic can be detected in the infinite random text sampled from the process, whereas we call the process strongly nonergodic when an infinite sequence of independent random bits, called probabilistic facts, is needed to describe this topic completely. Replacing probabilistic facts with an algorithmically random sequence of bits, called algorithmic facts, we adapt this property back to ergodic processes. Subsequently, we call a process perigraphic if the number of algorithmic facts which can be inferred from a finite text sampled from the process grows like a power of the text length. We present a simple example of such a process. Moreover, we demonstrate an assertion which we call the theorem about facts and words. This proposition states that the number of probabilistic or algorithmic facts which can be inferred from a text drawn from a process must be roughly smaller than the number of distinct word-like strings detected in this text by means of the PPM compression algorithm. We also observe that the number of the word-like strings for a sample of plays by Shakespeare follows an empirical stepwise power law, in a stark contrast to Markov processes. Hence we suppose that natural language considered as a process is not only non-Markov but also perigraphic.\n    ",
        "submission_date": "2017-06-14T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.04922",
        "title": "DSRIM: A Deep Neural Information Retrieval Model Enhanced by a Knowledge Resource Driven Representation of Documents",
        "authors": [
            "Gia-Hung Nguyen",
            "Laure Soulier",
            "Lynda Tamine",
            "Nathalie Bricon-Souf"
        ],
        "abstract": "The state-of-the-art solutions to the vocabulary mismatch in information retrieval (IR) mainly aim at leveraging either the relational semantics provided by external resources or the distributional semantics, recently investigated by deep neural approaches. Guided by the intuition that the relational semantics might improve the effectiveness of deep neural approaches, we propose the Deep Semantic Resource Inference Model (DSRIM) that relies on: 1) a representation of raw-data that models the relational semantics of text by jointly considering objects and relations expressed in a knowledge resource, and 2) an end-to-end neural architecture that learns the query-document relevance by leveraging the distributional and relational semantics of documents and queries. The experimental evaluation carried out on two TREC datasets from TREC Terabyte and TREC CDS tracks relying respectively on WordNet and MeSH resources, indicates that our model outperforms state-of-the-art semantic and deep neural IR models.\n    ",
        "submission_date": "2017-06-15T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05125",
        "title": "Deal or No Deal? End-to-End Learning for Negotiation Dialogues",
        "authors": [
            "Mike Lewis",
            "Denis Yarats",
            "Yann N. Dauphin",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (",
        "submission_date": "2017-06-16T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05349",
        "title": "Active learning in annotating micro-blogs dealing with e-reputation",
        "authors": [
            "Jean-Val\u00e8re Cossu",
            "Alejandro Molina-Villegas",
            "Mariana Tello-Signoret"
        ],
        "abstract": "Elections unleash strong political views on Twitter, but what do people really think about politics? Opinion and trend mining on micro blogs dealing with politics has recently attracted researchers in several fields including Information Retrieval and Machine Learning (ML). Since the performance of ML and Natural Language Processing (NLP) approaches are limited by the amount and quality of data available, one promising alternative for some tasks is the automatic propagation of expert annotations. This paper intends to develop a so-called active learning process for automatically annotating French language tweets that deal with the image (i.e., representation, web reputation) of politicians. Our main focus is on the methodology followed to build an original annotated dataset expressing opinion from two French politicians over time. We therefore review state of the art NLP-based ML algorithms to automatically annotate tweets using a manual initiation step as bootstrap. This paper focuses on key issues about active learning while building a large annotated data set from noise. This will be introduced by human annotators, abundance of data and the label distribution across data and entities. In turn, we show that Twitter characteristics such as the author's name or hashtags can be considered as the bearing point to not only improve automatic systems for Opinion Mining (OM) and Topic Classification but also to reduce noise in human annotations. However, a later thorough analysis shows that reducing noise might induce the loss of crucial information.\n    ",
        "submission_date": "2017-06-16T00:00:00",
        "last_modified_date": "2017-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05549",
        "title": "A Large-Scale CNN Ensemble for Medication Safety Analysis",
        "authors": [
            "Liliya Akhtyamova",
            "Andrey Ignatov",
            "John Cardiff"
        ],
        "abstract": "Revealing Adverse Drug Reactions (ADR) is an essential part of post-marketing drug surveillance, and data from health-related forums and medical communities can be of a great significance for estimating such effects. In this paper, we propose an end-to-end CNN-based method for predicting drug safety on user comments from healthcare discussion forums. We present an architecture that is based on a vast ensemble of CNNs with varied structural parameters, where the prediction is determined by the majority vote. To evaluate the performance of the proposed solution, we present a large-scale dataset collected from a medical website that consists of over 50 thousand reviews for more than 4000 drugs. The results demonstrate that our model significantly outperforms conventional approaches and predicts medicine safety with an accuracy of 87.17% for binary and 62.88% for multi-classification tasks.\n    ",
        "submission_date": "2017-06-17T00:00:00",
        "last_modified_date": "2017-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05656",
        "title": "Lexical representation explains cortical entrainment during speech comprehension",
        "authors": [
            "Stefan Frank",
            "Jinbiao Yang"
        ],
        "abstract": "Results from a recent neuroimaging study on spoken sentence comprehension have been interpreted as evidence for cortical entrainment to hierarchical syntactic structure. We present a simple computational model that predicts the power spectra from this study, even though the model's linguistic knowledge is restricted to the lexical level, and word-level representations are not combined into higher-level units (phrases or sentences). Hence, the cortical entrainment results can also be explained from the lexical properties of the stimuli, without recourse to hierarchical syntax.\n    ",
        "submission_date": "2017-06-18T00:00:00",
        "last_modified_date": "2018-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.05719",
        "title": "Towards the Improvement of Automated Scientific Document Categorization by Deep Learning",
        "authors": [
            "Thomas Krause"
        ],
        "abstract": "This master thesis describes an algorithm for automated categorization of scientific documents using deep learning techniques and compares the results to the results of existing classification algorithms. As an additional goal a reusable API is to be developed allowing the automation of classification tasks in existing software. A design will be proposed using a convolutional neural network as a classifier and integrating this into a REST based API. This is then used as the basis for an actual proof of concept implementation presented as well in this thesis. It will be shown that the deep learning classifier provides very good result in the context of multi-class document categorization and that it is feasible to integrate such classifiers into a larger ecosystem using REST based services.\n    ",
        "submission_date": "2017-06-18T00:00:00",
        "last_modified_date": "2017-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06197",
        "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting",
        "authors": [
            "Xu Sun",
            "Xuancheng Ren",
            "Shuming Ma",
            "Houfeng Wang"
        ],
        "abstract": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1-4% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The code is available at ",
        "submission_date": "2017-06-19T00:00:00",
        "last_modified_date": "2019-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.06613",
        "title": "End-to-End Neural Ad-hoc Ranking with Kernel Pooling",
        "authors": [
            "Chenyan Xiong",
            "Zhuyun Dai",
            "Jamie Callan",
            "Zhiyuan Liu",
            "Russell Power"
        ],
        "abstract": "This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.\n    ",
        "submission_date": "2017-06-20T00:00:00",
        "last_modified_date": "2017-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07230",
        "title": "Gated-Attention Architectures for Task-Oriented Language Grounding",
        "authors": [
            "Devendra Singh Chaplot",
            "Kanthashree Mysore Sathyendra",
            "Rama Kumar Pasumarthi",
            "Dheeraj Rajagopal",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2018-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07859",
        "title": "Deep Speaker Verification: Do We Need End to End?",
        "authors": [
            "Dong Wang",
            "Lantian Li",
            "Zhiyuan Tang",
            "Thomas Fang Zheng"
        ],
        "abstract": "End-to-end learning treats the entire system as a whole adaptable black box, which, if sufficient data are available, may learn a system that works very well for the target task. This principle has recently been applied to several prototype research on speaker verification (SV), where the feature learning and classifier are learned together with an objective function that is consistent with the evaluation metric. An opposite approach to end-to-end is feature learning, which firstly trains a feature learning model, and then constructs a back-end classifier separately to perform SV. Recently, both approaches achieved significant performance gains on SV, mainly attributed to the smart utilization of deep neural networks. However, the two approaches have not been carefully compared, and their respective advantages have not been well discussed. In this paper, we compare the end-to-end and feature learning approaches on a text-independent SV task. Our experiments on a dataset sampled from the Fisher database and involving 5,000 speakers demonstrated that the feature learning approach outperformed the end-to-end approach. This is a strong support for the feature learning approach, at least with data and computation resources similar to ours.\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2017-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07860",
        "title": "Speaker Recognition with Cough, Laugh and \"Wei\"",
        "authors": [
            "Miao Zhang",
            "Yixiang Chen",
            "Lantian Li",
            "Dong Wang"
        ],
        "abstract": "This paper proposes a speaker recognition (SRE) task with trivial speech events, such as cough and laugh. These trivial events are ubiquitous in conversations and less subjected to intentional change, therefore offering valuable particularities to discover the genuine speaker from disguised speech. However, trivial events are often short and idiocratic in spectral patterns, making SRE extremely difficult. Fortunately, we found a very powerful deep feature learning structure that can extract highly speaker-sensitive features. By employing this tool, we studied the SRE performance on three types of trivial events: cough, laugh and \"Wei\" (a short Chinese \"Hello\"). The results show that there is rich speaker information within these trivial events, even for cough that is intuitively less speaker distinguishable. With the deep feature approach, the EER can reach 10%-14% with the three trivial events, despite their extremely short durations (0.2-1.0 seconds).\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2017-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07861",
        "title": "Cross-lingual Speaker Verification with Deep Feature Learning",
        "authors": [
            "Lantian Li",
            "Dong Wang",
            "Askar Rozi",
            "Thomas Fang Zheng"
        ],
        "abstract": "Existing speaker verification (SV) systems often suffer from performance degradation if there is any language mismatch between model training, speaker enrollment, and test. A major cause of this degradation is that most existing SV methods rely on a probabilistic model to infer the speaker factor, so any significant change on the distribution of the speech signal will impact the inference. Recently, we proposed a deep learning model that can learn how to extract the speaker factor by a deep neural network (DNN). By this feature learning, an SV system can be constructed with a very simple back-end model. In this paper, we investigate the robustness of the feature-based SV system in situations with language mismatch. Our experiments were conducted on a complex cross-lingual scenario, where the model training was in English, and the enrollment and test were in Chinese or Uyghur. The experiments demonstrated that the feature-based system outperformed the i-vector system with a large margin, particularly with language mismatch between enrollment and test.\n    ",
        "submission_date": "2017-06-22T00:00:00",
        "last_modified_date": "2017-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07912",
        "title": "Cluster Based Symbolic Representation for Skewed Text Categorization",
        "authors": [
            "Lavanya Narayana Raju",
            "Mahamad Suhil",
            "D S Guru",
            "Harsha S Gowda"
        ],
        "abstract": "In this work, a problem associated with imbalanced text corpora is addressed. A method of converting an imbalanced text corpus into a balanced one is presented. The presented method employs a clustering algorithm for conversion. Initially to avoid curse of dimensionality, an effective representation scheme based on term class relevancy measure is adapted, which drastically reduces the dimension to the number of classes in the corpus. Subsequently, the samples of larger sized classes are grouped into a number of subclasses of smaller sizes to make the entire corpus balanced. Each subclass is then given a single symbolic vector representation by the use of interval valued features. This symbolic representation in addition to being compact helps in reducing the space requirement and also the classification time. The proposed model has been empirically demonstrated for its superiority on bench marking datasets viz., Reuters 21578 and TDT2. Further, it has been compared against several other existing contemporary models including model based on support vector machine. The comparative analysis indicates that the proposed model outperforms the other existing models.\n    ",
        "submission_date": "2017-06-24T00:00:00",
        "last_modified_date": "2017-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.07913",
        "title": "Semi-supervised Text Categorization Using Recursive K-means Clustering",
        "authors": [
            "Harsha S. Gowda",
            "Mahamad Suhil",
            "D.S. Guru",
            "Lavanya Narayana Raju"
        ],
        "abstract": "In this paper, we present a semi-supervised learning algorithm for classification of text documents. A method of labeling unlabeled text documents is presented. The presented method is based on the principle of divide and conquer strategy. It uses recursive K-means algorithm for partitioning both labeled and unlabeled data collection. The K-means algorithm is applied recursively on each partition till a desired level partition is achieved such that each partition contains labeled documents of a single class. Once the desired clusters are obtained, the respective cluster centroids are considered as representatives of the clusters and the nearest neighbor rule is used for classifying an unknown text document. Series of experiments have been conducted to bring out the superiority of the proposed model over other recent state of the art models on 20Newsgroups dataset.\n    ",
        "submission_date": "2017-06-24T00:00:00",
        "last_modified_date": "2017-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.08746",
        "title": "DE-PACRR: Exploring Layers Inside the PACRR Model",
        "authors": [
            "Andrew Yates",
            "Kai Hui"
        ],
        "abstract": "Recent neural IR models have demonstrated deep learning's utility in ad-hoc information retrieval. However, deep models have a reputation for being black boxes, and the roles of a neural IR model's components may not be obvious at first glance. In this work, we attempt to shed light on the inner workings of a recently proposed neural IR model, namely the PACRR model, by visualizing the output of intermediate layers and by investigating the relationship between intermediate weights and the ultimate relevance score produced. We highlight several insights, hoping that such insights will be generally applicable.\n    ",
        "submission_date": "2017-06-27T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09055",
        "title": "Acoustic Modeling Using a Shallow CNN-HTSVM Architecture",
        "authors": [
            "Christopher Dane Shulby",
            "Martha Dais Ferreira",
            "Rodrigo F. de Mello",
            "Sandra Maria Aluisio"
        ],
        "abstract": "High-accuracy speech recognition is especially challenging when large datasets are not available. It is possible to bridge this gap with careful and knowledge-driven parsing combined with the biologically inspired CNN and the learning guarantees of the Vapnik Chervonenkis (VC) theory. This work presents a Shallow-CNN-HTSVM (Hierarchical Tree Support Vector Machine classifier) architecture which uses a predefined knowledge-based set of rules with statistical machine learning techniques. Here we show that gross errors present even in state-of-the-art systems can be avoided and that an accurate acoustic model can be built in a hierarchical fashion. The CNN-HTSVM acoustic model outperforms traditional GMM-HMM models and the HTSVM structure outperforms a MLP multi-class classifier. More importantly we isolate the performance of the acoustic model and provide results on both the frame and phoneme level considering the true robustness of the model. We show that even with a small amount of data accurate and robust recognition rates can be obtained.\n    ",
        "submission_date": "2017-06-27T00:00:00",
        "last_modified_date": "2017-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.09588",
        "title": "Multi-scale Multi-band DenseNets for Audio Source Separation",
        "authors": [
            "Naoya Takahashi",
            "Yuki Mitsufuji"
        ],
        "abstract": "This paper deals with the problem of audio source separation. To handle the complex and ill-posed nature of the problems of audio source separation, the current state-of-the-art approaches employ deep neural networks to obtain instrumental spectra from a mixture. In this study, we propose a novel network architecture that extends the recently developed densely connected convolutional network (DenseNet), which has shown excellent results on image classification tasks. To deal with the specific problem of audio source separation, an up-sampling layer, block skip connection and band-dedicated dense blocks are incorporated on top of DenseNet. The proposed approach takes advantage of long contextual information and outperforms state-of-the-art results on SiSEC 2016 competition by a large margin in terms of signal-to-distortion ratio. Moreover, the proposed architecture requires significantly fewer parameters and considerably less training time compared with other methods.\n    ",
        "submission_date": "2017-06-29T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.10006",
        "title": "Automated Audio Captioning with Recurrent Neural Networks",
        "authors": [
            "Konstantinos Drossos",
            "Sharath Adavanne",
            "Tuomas Virtanen"
        ],
        "abstract": "We present the first approach to automated audio captioning. We employ an encoder-decoder scheme with an alignment model in between. The input to the encoder is a sequence of log mel-band energies calculated from an audio file, while the output is a sequence of words, i.e. a caption. The encoder is a multi-layered, bi-directional gated recurrent unit (GRU) and the decoder a multi-layered GRU with a classification layer connected to the last GRU of the decoder. The classification layer and the alignment model are fully connected layers with shared weights between timesteps. The proposed method is evaluated using data drawn from a commercial sound effects library, ProSound Effects. The resulting captions were rated through metrics utilized in machine translation and image captioning fields. Results from metrics show that the proposed method can predict words appearing in the original caption, but not always correctly ordered.\n    ",
        "submission_date": "2017-06-30T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1706.10192",
        "title": "Co-PACRR: A Context-Aware Neural IR Model for Ad-hoc Retrieval",
        "authors": [
            "Kai Hui",
            "Andrew Yates",
            "Klaus Berberich",
            "Gerard de Melo"
        ],
        "abstract": "Neural IR models, such as DRMM and PACRR, have achieved strong results by successfully capturing relevance matching signals. We argue that the context of these matching signals is also important. Intuitively, when extracting, modeling, and combining matching signals, one would like to consider the surrounding text (local context) as well as other signals from the same document that can contribute to the overall relevance score. In this work, we highlight three potential shortcomings caused by not considering context information and propose three neural ingredients to address them: a disambiguation component, cascade k-max pooling, and a shuffling combination layer. Incorporating these components into the PACRR model yields Co-PACRR, a novel context-aware neural IR model. Extensive comparisons with established models on Trec Web Track data confirm that the proposed model can achieve superior search results. In addition, an ablation analysis is conducted to gain insights into the impact of and interactions between different components. We release our code to enable future comparisons.\n    ",
        "submission_date": "2017-06-30T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00061",
        "title": "Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English",
        "authors": [
            "Su Lin Blodgett",
            "Brendan O'Connor"
        ],
        "abstract": "We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.\n    ",
        "submission_date": "2017-06-30T00:00:00",
        "last_modified_date": "2017-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00189",
        "title": "Content-Based Weak Supervision for Ad-Hoc Re-Ranking",
        "authors": [
            "Sean MacAvaney",
            "Andrew Yates",
            "Kai Hui",
            "Ophir Frieder"
        ],
        "abstract": "One challenge with neural ranking is the need for a large amount of manually-labeled relevance judgments for training. In contrast with prior work, we examine the use of weak supervision sources for training that yield pseudo query-document pairs that already exhibit relevance (e.g., newswire headline-content pairs and encyclopedic heading-paragraph pairs). We also propose filtering techniques to eliminate training samples that are too far out of domain using two techniques: a heuristic-based approach and novel supervised filter that re-purposes a neural ranker. Using several leading neural ranking architectures and multiple weak supervision datasets, we show that these sources of training pairs are effective on their own (outperforming prior weak supervision techniques), and that filtering can further improve performance.\n    ",
        "submission_date": "2017-07-01T00:00:00",
        "last_modified_date": "2019-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00201",
        "title": "Rank-1 Constrained Multichannel Wiener Filter for Speech Recognition in Noisy Environments",
        "authors": [
            "Ziteng Wang",
            "Emmanuel Vincent",
            "Romain Serizel",
            "Yonghong Yan"
        ],
        "abstract": "Multichannel linear filters, such as the Multichannel Wiener Filter (MWF) and the Generalized Eigenvalue (GEV) beamformer are popular signal processing techniques which can improve speech recognition performance. In this paper, we present an experimental study on these linear filters in a specific speech recognition task, namely the CHiME-4 challenge, which features real recordings in multiple noisy environments. Specifically, the rank-1 MWF is employed for noise reduction and a new constant residual noise power constraint is derived which enhances the recognition performance. To fulfill the underlying rank-1 assumption, the speech covariance matrix is reconstructed based on eigenvectors or generalized eigenvectors. Then the rank-1 constrained MWF is evaluated with alternative multichannel linear filters under the same framework, which involves a Bidirectional Long Short-Term Memory (BLSTM) network for mask estimation. The proposed filter outperforms alternative ones, leading to a 40% relative Word Error Rate (WER) reduction compared with the baseline Weighted Delay and Sum (WDAS) beamformer on the real test set, and a 15% relative WER reduction compared with the GEV-BAN method. The results also suggest that the speech recognition accuracy correlates more with the Mel-frequency cepstral coefficients (MFCC) feature variance than with the noise reduction or the speech distortion level.\n    ",
        "submission_date": "2017-07-01T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00206",
        "title": "Efficient Correlated Topic Modeling with Topic Embedding",
        "authors": [
            "Junxian He",
            "Zhiting Hu",
            "Taylor Berg-Kirkpatrick",
            "Ying Huang",
            "Eric P. Xing"
        ],
        "abstract": "Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.\n    ",
        "submission_date": "2017-07-01T00:00:00",
        "last_modified_date": "2017-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00683",
        "title": "Modulating early visual processing by language",
        "authors": [
            "Harm de Vries",
            "Florian Strub",
            "J\u00e9r\u00e9mie Mary",
            "Hugo Larochelle",
            "Olivier Pietquin",
            "Aaron Courville"
        ],
        "abstract": "It is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where visual and linguistic input are mostly processed independently before being fused into a single representation. In this paper, we deviate from this classic pipeline and propose to modulate the \\emph{entire visual processing} by linguistic input. Specifically, we condition the batch normalization parameters of a pretrained residual network (ResNet) on a language embedding. This approach, which we call MOdulated RESnet (\\MRN), significantly improves strong baselines on two visual question answering tasks. Our ablation study shows that modulating from the early stages of the visual processing is beneficial.\n    ",
        "submission_date": "2017-07-02T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.00836",
        "title": "DeepStory: Video Story QA by Deep Embedded Memory Networks",
        "authors": [
            "Kyung-Min Kim",
            "Min-Oh Heo",
            "Seong-Ho Choi",
            "Byoung-Tak Zhang"
        ],
        "abstract": "Question-answering (QA) on video contents is a significant challenge for achieving human-level intelligence as it involves both vision and language in real-world settings. Here we demonstrate the possibility of an AI agent performing video story QA by learning from a large amount of cartoon videos. We develop a video-story learning model, i.e. Deep Embedded Memory Networks (DEMN), to reconstruct stories from a joint scene-dialogue video stream using a latent embedding space of observed data. The video stories are stored in a long-term memory component. For a given question, an LSTM-based attention model uses the long-term memory to recall the best question-story-answer triplet by focusing on specific words containing key information. We trained the DEMN on a novel QA dataset of children's cartoon video series, Pororo. The dataset contains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained sentences for scene description, and 8,913 story-related QA pairs. Our experimental results show that the DEMN outperforms other QA models. This is mainly due to 1) the reconstruction of video stories in a scene-dialogue combined form that utilize the latent embedding and 2) attention. DEMN also achieved state-of-the-art results on the MovieQA benchmark.\n    ",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01090",
        "title": "Hidden-Markov-Model Based Speech Enhancement",
        "authors": [
            "Daniel Dzibela",
            "Armin Sehr"
        ],
        "abstract": "The goal of this contribution is to use a parametric speech synthesis system for reducing background noise and other interferences from recorded speech signals. In a first step, Hidden Markov Models of the synthesis system are trained.\n",
        "submission_date": "2017-07-04T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01425",
        "title": "Determining sentiment in citation text and analyzing its impact on the proposed ranking index",
        "authors": [
            "Souvick Ghosh",
            "Dipankar Das",
            "Tanmoy Chakraborty"
        ],
        "abstract": "Whenever human beings interact with each other, they exchange or express opinions, emotions, and sentiments. These opinions can be expressed in text, speech or images. Analysis of these sentiments is one of the popular research areas of present day researchers. Sentiment analysis, also known as opinion mining tries to identify or classify these sentiments or opinions into two broad categories - positive and negative. In recent years, the scientific community has taken a lot of interest in analyzing sentiment in textual data available in various social media platforms. Much work has been done on social media conversations, blog posts, newspaper articles and various narrative texts. However, when it comes to identifying emotions from scientific papers, researchers have faced some difficulties due to the implicit and hidden nature of opinion. By default, citation instances are considered inherently positive in emotion. Popular ranking and indexing paradigms often neglect the opinion present while citing. In this paper, we have tried to achieve three objectives. First, we try to identify the major sentiment in the citation text and assign a score to the instance. We have used a statistical classifier for this purpose. Secondly, we have proposed a new index (we shall refer to it hereafter as M-index) which takes into account both the quantitative and qualitative factors while scoring a paper. Thirdly, we developed a ranking of research papers based on the M-index. We also try to explain how the M-index impacts the ranking of scientific papers.\n    ",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01450",
        "title": "The Complex Negotiation Dialogue Game",
        "authors": [
            "Romain Laroche"
        ],
        "abstract": "This position paper formalises an abstract model for complex negotiation dialogue. This model is to be used for the benchmark of optimisation algorithms ranging from Reinforcement Learning to Stochastic Games, through Transfer Learning, One-Shot Learning or others.\n    ",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01477",
        "title": "Like trainer, like bot? Inheritance of bias in algorithmic content moderation",
        "authors": [
            "Reuben Binns",
            "Michael Veale",
            "Max Van Kleek",
            "Nigel Shadbolt"
        ],
        "abstract": "The internet has become a central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated content moderation aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encourage more civil debate, they must navigate inherently normatively contestable boundaries, and are subject to the idiosyncratic norms of the human raters who provide the training data. An important objective for platforms implementing such measures might be to ensure that they are not unduly biased towards or against particular norms of offence. This paper provides some exploratory methods by which the normative biases of algorithmic content moderation systems can be measured, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various test sets. We conclude by discussing some of the ethical choices facing the implementers of algorithmic moderation systems, given various desired levels of diversity of viewpoints amongst discussion participants.\n    ",
        "submission_date": "2017-07-05T00:00:00",
        "last_modified_date": "2017-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.01890",
        "title": "An Interactive Tool for Natural Language Processing on Clinical Text",
        "authors": [
            "Gaurav Trivedi",
            "Phuong Pham",
            "Wendy Chapman",
            "Rebecca Hwa",
            "Janyce Wiebe",
            "Harry Hochheiser"
        ],
        "abstract": "Natural Language Processing (NLP) systems often make use of machine learning techniques that are unfamiliar to end-users who are interested in analyzing clinical records. Although NLP has been widely used in extracting information from clinical text, current systems generally do not support model revision based on feedback from domain experts.\n",
        "submission_date": "2017-07-06T00:00:00",
        "last_modified_date": "2017-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02363",
        "title": "Towards Zero-Shot Frame Semantic Parsing for Domain Scaling",
        "authors": [
            "Ankur Bapna",
            "Gokhan Tur",
            "Dilek Hakkani-Tur",
            "Larry Heck"
        ],
        "abstract": "State-of-the-art slot filling models for goal-oriented human/machine conversational language understanding systems rely on deep learning methods. While multi-task training of such models alleviates the need for large in-domain annotated datasets, bootstrapping a semantic parsing model for a new domain using only the semantic frame, such as the back-end API or knowledge graph schema, is still one of the holy grail tasks of language understanding for dialogue systems. This paper proposes a deep learning based approach that can utilize only the slot description in context without the need for any labeled or unlabeled in-domain examples, to quickly bootstrap a new domain. The main idea of this paper is to leverage the encoding of the slot names and descriptions within a multi-task deep learned slot filling model, to implicitly align slots across domains. The proposed approach is promising for solving the domain scaling problem and eliminating the need for any manually annotated data or explicit schema alignment. Furthermore, our experiments on multiple domains show that this approach results in significantly better slot-filling performance when compared to using only in-domain data, especially in the low data regime.\n    ",
        "submission_date": "2017-07-07T00:00:00",
        "last_modified_date": "2017-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.02812",
        "title": "Towards Crafting Text Adversarial Samples",
        "authors": [
            "Suranjana Samanta",
            "Sameep Mehta"
        ],
        "abstract": "Adversarial samples are strategically modified samples, which are crafted with the purpose of fooling a classifier at hand. An attacker introduces specially crafted adversarial samples to a deployed classifier, which are being mis-classified by the classifier. However, the samples are perceived to be drawn from entirely different classes and thus it becomes hard to detect the adversarial samples. Most of the prior works have been focused on synthesizing adversarial samples in the image domain. In this paper, we propose a new method of crafting adversarial text samples by modification of the original samples. Modifications of the original text samples are done by deleting or replacing the important or salient words in the text or by introducing new words in the text sample. Our algorithm works best for the datasets which have sub-categories within each of the classes of examples. While crafting adversarial samples, one of the key constraint is to generate meaningful sentences which can at pass off as legitimate from language (English) viewpoint. Experimental results on IMDB movie review dataset for sentiment analysis and Twitter dataset for gender detection show the efficiency of our proposed method.\n    ",
        "submission_date": "2017-07-10T00:00:00",
        "last_modified_date": "2017-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03017",
        "title": "Learning Visual Reasoning Without Strong Priors",
        "authors": [
            "Ethan Perez",
            "Harm de Vries",
            "Florian Strub",
            "Vincent Dumoulin",
            "Aaron Courville"
        ],
        "abstract": "Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step, high-level process - is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% error rate. We outperform the next best end-to-end method (4.5%) and even methods that use extra supervision (3.1%). We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively.\n    ",
        "submission_date": "2017-07-10T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03457",
        "title": "Multiple Context-Free Tree Grammars: Lexicalization and Characterization",
        "authors": [
            "Joost Engelfriet",
            "Andreas Maletti",
            "Sebastian Maneth"
        ],
        "abstract": "Multiple (simple) context-free tree grammars are investigated, where \"simple\" means \"linear and nondeleting\". Every multiple context-free tree grammar that is finitely ambiguous can be lexicalized; i.e., it can be transformed into an equivalent one (generating the same tree language) in which each rule of the grammar contains a lexical symbol. Due to this transformation, the rank of the nonterminals increases at most by 1, and the multiplicity (or fan-out) of the grammar increases at most by the maximal rank of the lexical symbols; in particular, the multiplicity does not increase when all lexical symbols have rank 0. Multiple context-free tree grammars have the same tree generating power as multi-component tree adjoining grammars (provided the latter can use a root-marker). Moreover, every multi-component tree adjoining grammar that is finitely ambiguous can be lexicalized. Multiple context-free tree grammars have the same string generating power as multiple context-free (string) grammars and polynomial time parsing algorithms. A tree language can be generated by a multiple context-free tree grammar if and only if it is the image of a regular tree language under a deterministic finite-copying macro tree transducer. Multiple context-free tree grammars can be used as a synchronous translation device.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.03569",
        "title": "Multitask Learning for Fine-Grained Twitter Sentiment Analysis",
        "authors": [
            "Georgios Balikas",
            "Simon Moura",
            "Massih-Reza Amini"
        ],
        "abstract": "Traditional sentiment analysis approaches tackle problems like ternary (3-category) and fine-grained (5-category) classification by learning the tasks separately. We argue that such classification tasks are correlated and we propose a multitask approach based on a recurrent neural network that benefits by jointly learning them. Our study demonstrates the potential of multitask models on this type of problems and improves the state-of-the-art results in the fine-grained sentiment classification problem.\n    ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04242",
        "title": "Neural Networks for Information Retrieval",
        "authors": [
            "Tom Kenter",
            "Alexey Borisov",
            "Christophe Van Gysel",
            "Mostafa Dehghani",
            "Maarten de Rijke",
            "Bhaskar Mitra"
        ],
        "abstract": "Machine learning plays a role in many aspects of modern IR systems, and deep learning is applied in all of them. The fast pace of modern-day research has given rise to many different approaches for many different IR problems. The amount of information available can be overwhelming both for junior students and for experienced researchers looking for new research topics and directions. Additionally, it is interesting to see what key insights into IR problems the new technologies are able to give us. The aim of this full-day tutorial is to give a clear overview of current tried-and-trusted neural methods in IR and how they benefit IR research. It covers key architectures, as well as the most promising future directions.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04244",
        "title": "Lithium NLP: A System for Rich Information Extraction from Noisy User Generated Text on Social Media",
        "authors": [
            "Preeti Bhargava",
            "Nemanja Spasojevic",
            "Guoning Hu"
        ],
        "abstract": "In this paper, we describe the Lithium Natural Language Processing (NLP) system - a resource-constrained, high- throughput and language-agnostic system for information extraction from noisy user generated text on social media. Lithium NLP extracts a rich set of information including entities, topics, hashtags and sentiment from text. We discuss several real world applications of the system currently incorporated in Lithium products. We also compare our system with existing commercial and academic NLP systems in terms of performance, information extracted and languages supported. We show that Lithium NLP is at par with and in some cases, outperforms state- of-the-art commercial NLP systems.\n    ",
        "submission_date": "2017-07-13T00:00:00",
        "last_modified_date": "2017-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04678",
        "title": "Lyrics-Based Music Genre Classification Using a Hierarchical Attention Network",
        "authors": [
            "Alexandros Tsaptsinos"
        ],
        "abstract": "Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure - in which words combine to form lines, lines form segments, and segments form a complete song - we adapt a hierarchical attention network (HAN) to exploit these layers and in addition learn the importance of the words, lines, and segments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher number of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres.\n    ",
        "submission_date": "2017-07-15T00:00:00",
        "last_modified_date": "2017-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04724",
        "title": "Memoisation: Purely, Left-recursively, and with (Continuation Passing) Style",
        "authors": [
            "Samer Abdallah"
        ],
        "abstract": "Memoisation, or tabling, is a well-known technique that yields large improvements in the performance of some recursive computations. Tabled resolution in Prologs such as XSB and B-Prolog can transform so called left-recursive predicates from non-terminating computations into finite and well-behaved ones. In the functional programming literature, memoisation has usually been implemented in a way that does not handle left-recursion, requiring supplementary mechanisms to prevent non-termination. A notable exception is Johnson's (1995) continuation passing approach in Scheme. This, however, relies on mutation of a memo table data structure and coding in explicit continuation passing style. We show how Johnson's approach can be implemented purely functionally in a modern, strongly typed functional language (OCaml), presented via a monadic interface that hides the implementation details, yet providing a way to return a compact represention of the memo tables at the end of the computation.\n    ",
        "submission_date": "2017-07-15T00:00:00",
        "last_modified_date": "2017-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04935",
        "title": "Automatized Generation of Alphabets of Symbols",
        "authors": [
            "Serhii Hamotskyi",
            "Anis Rojbi",
            "Sergii Stirenko",
            "Yuri Gordienko"
        ],
        "abstract": "In this paper, we discuss the generation of symbols (and alphabets) based on specific user requirements (medium, priorities, type of information that needs to be conveyed). A framework for the generation of alphabets is proposed, and its use for the generation of a shorthand writing system is explored. We discuss the possible use of machine learning and genetic algorithms to gather inputs for generation of such alphabets and for optimization of already generated ones. The alphabets generated using such methods may be used in very different fields, from the creation of synthetic languages and constructed scripts to the creation of sensible commands for multimodal interaction through Human-Computer Interfaces, such as mouse gestures, touchpads, body gestures, eye-tracking cameras, and brain-computing Interfaces, especially in applications for elderly care and people with disabilities.\n    ",
        "submission_date": "2017-07-16T00:00:00",
        "last_modified_date": "2017-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.04968",
        "title": "Visual Question Answering with Memory-Augmented Networks",
        "authors": [
            "Chao Ma",
            "Chunhua Shen",
            "Anthony Dick",
            "Qi Wu",
            "Peng Wang",
            "Anton van den Hengel",
            "Ian Reid"
        ],
        "abstract": "In this paper, we exploit a memory-augmented neural network to predict accurate answers to visual questions, even when those answers occur rarely in the training set. The memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar. We show that memory-augmented neural networks are able to maintain a relatively long-term memory of scarce training exemplars, which is important for visual question answering due to the heavy-tailed distribution of answers in a general VQA setting. Experimental results on two large-scale benchmark datasets show the favorable performance of the proposed algorithm with a comparison to state of the art.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2018-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05005",
        "title": "graph2vec: Learning Distributed Representations of Graphs",
        "authors": [
            "Annamalai Narayanan",
            "Mahinthan Chandramohan",
            "Rajasekar Venkatesan",
            "Lihui Chen",
            "Yang Liu",
            "Shantanu Jaiswal"
        ],
        "abstract": "Recent works on representation learning for graph structured data predominantly focus on learning distributed representations of graph substructures such as nodes and subgraphs. However, many graph analytics tasks such as graph classification and clustering require representing entire graphs as fixed length feature vectors. While the aforementioned approaches are naturally unequipped to learn such representations, graph kernels remain as the most effective way of obtaining them. However, these graph kernels use handcrafted features (e.g., shortest paths, graphlets, etc.) and hence are hampered by problems such as poor generalization. To address this limitation, in this work, we propose a neural embedding framework named graph2vec to learn data-driven distributed representations of arbitrary sized graphs. graph2vec's embeddings are learnt in an unsupervised manner and are task agnostic. Hence, they could be used for any downstream task such as graph classification, clustering and even seeding supervised representation learning approaches. Our experiments on several benchmark and large real-world datasets show that graph2vec achieves significant improvements in classification and clustering accuracies over substructure representation learning approaches and are competitive with state-of-the-art graph kernels.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05015",
        "title": "Iris: A Conversational Agent for Complex Tasks",
        "authors": [
            "Ethan Fast",
            "Binbin Chen",
            "Julia Mendelsohn",
            "Jonathan Bassen",
            "Michael Bernstein"
        ],
        "abstract": "Today's conversational agents are restricted to simple standalone commands. In this paper, we present Iris, an agent that draws on human conversational strategies to combine commands, allowing it to perform more complex tasks that it has not been explicitly designed to support: for example, composing one command to \"plot a histogram\" with another to first \"log-transform the data\". To enable this complexity, we introduce a domain specific language that transforms commands into automata that Iris can compose, sequence, and execute dynamically by interacting with a user through natural language, as well as a conversational type system that manages what kinds of commands can be combined. We have designed Iris to help users with data science tasks, a domain that requires support for command combination. In evaluation, we find that data scientists complete a predictive modeling task significantly faster (2.6 times speedup) with Iris than a modern non-conversational programming environment. Iris supports the same kinds of commands as today's agents, but empowers users to weave together these commands to accomplish complex goals.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05115",
        "title": "The Power of Constraint Grammars Revisited",
        "authors": [
            "Anssi Yli-Jyr\u00e4"
        ],
        "abstract": "Sequential Constraint Grammar (SCG) (Karlsson, 1990) and its extensions have lacked clear connections to formal language theory. The purpose of this article is to lay a foundation for these connections by simplifying the definition of strings processed by the grammar and by showing that Nonmonotonic SCG is undecidable and that derivations similar to the Generative Phonology exist. The current investigations propose resource bounds that restrict the generative power of SCG to a subset of context sensitive languages and present a strong finite-state condition for grammars as wholes. We show that a grammar is equivalent to a finite-state transducer if it is implemented with a Turing machine that runs in o(n log n) time. This condition opens new finite-state hypotheses and avenues for deeper analysis of SCG instances in the way inspired by Finite-State Phonology.\n    ",
        "submission_date": "2017-07-17T00:00:00",
        "last_modified_date": "2017-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05254",
        "title": "Explainable Entity-based Recommendations with Knowledge Graphs",
        "authors": [
            "Rose Catherine",
            "Kathryn Mazaitis",
            "Maxine Eskenazi",
            "William Cohen"
        ],
        "abstract": "Explainable recommendation is an important task. Many methods have been proposed which generate explanations from the content and reviews written for items. When review text is unavailable, generating explanations is still a hard problem. In this paper, we illustrate how explanations can be generated in such a scenario by leveraging external knowledge in the form of knowledge graphs. Our method jointly ranks items and knowledge graph entities using a Personalized PageRank procedure to produce recommendations together with their explanations.\n    ",
        "submission_date": "2017-07-12T00:00:00",
        "last_modified_date": "2017-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05612",
        "title": "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives",
        "authors": [
            "Fartash Faghri",
            "David J. Fleet",
            "Jamie Ryan Kiros",
            "Sanja Fidler"
        ],
        "abstract": "We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. Inspired by hard negative mining, the use of hard negatives in structured prediction, and ranking loss functions, we introduce a simple change to common loss functions used for multi-modal embeddings. That, combined with fine-tuning and use of augmented data, yields significant gains in retrieval performance. We showcase our approach, VSE++, on MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval and 11.3% in image retrieval (at R@1).\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2018-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.05720",
        "title": "Grounding Spatio-Semantic Referring Expressions for Human-Robot Interaction",
        "authors": [
            "Mohit Shridhar",
            "David Hsu"
        ],
        "abstract": "The human language is one of the most natural interfaces for humans to interact with robots. This paper presents a robot system that retrieves everyday objects with unconstrained natural language descriptions. A core issue for the system is semantic and spatial grounding, which is to infer objects and their spatial relationships from images and natural language expressions. We introduce a two-stage neural-network grounding pipeline that maps natural language referring expressions directly to objects in the images. The first stage uses visual descriptions in the referring expressions to generate a candidate set of relevant objects. The second stage examines all pairwise relationships between the candidates and predicts the most likely referred object according to the spatial descriptions in the referring expressions. A key feature of our system is that by leveraging a large dataset of images labeled with text descriptions, it allows unrestricted object types and natural language referring expressions. Preliminary results indicate that our system outperforms a near state-of-the-art object comprehension system on standard benchmark datasets. We also present a robot system that follows voice commands to pick and place previously unseen objects.\n    ",
        "submission_date": "2017-07-18T00:00:00",
        "last_modified_date": "2017-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06163",
        "title": "Metrical-accent Aware Vocal Onset Detection in Polyphonic Audio",
        "authors": [
            "Georgi Dzhambazov",
            "Andre Holzapfel",
            "Ajay Srinivasamurthy",
            "Xavier Serra"
        ],
        "abstract": "The goal of this study is the automatic detection of onsets of the singing voice in polyphonic audio recordings. Starting with a hypothesis that the knowledge of the current position in a metrical cycle (i.e. metrical accent) can improve the accuracy of vocal note onset detection, we propose a novel probabilistic model to jointly track beats and vocal note onsets. The proposed model extends a state of the art model for beat and meter tracking, in which a-priori probability of a note at a specific metrical accent interacts with the probability of observing a vocal note onset. We carry out an evaluation on a varied collection of multi-instrument datasets from two music traditions (English popular music and Turkish makam) with different types of metrical cycles and singing styles. Results confirm that the proposed model reasonably improves vocal note onset detection accuracy compared to a baseline model that does not take metrical position into account.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06209",
        "title": "Crowdsourcing Multiple Choice Science Questions",
        "authors": [
            "Johannes Welbl",
            "Nelson F. Liu",
            "Matt Gardner"
        ],
        "abstract": "We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions (Dataset available at ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06355",
        "title": "Video Question Answering via Attribute-Augmented Attention Network Learning",
        "authors": [
            "Yunan Ye",
            "Zhou Zhao",
            "Yimeng Li",
            "Long Chen",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "abstract": "Video Question Answering is a challenging problem in visual information retrieval, which provides the answer to the referenced video content according to the question. However, the existing visual question answering approaches mainly tackle the problem of static image question, which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video question answering dataset. We conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06527",
        "title": "Single-Channel Multi-talker Speech Recognition with Permutation Invariant Training",
        "authors": [
            "Yanmin Qian",
            "Xuankai Chang",
            "Dong Yu"
        ],
        "abstract": "Although great progresses have been made in automatic speech recognition (ASR), significant performance degradation is still observed when recognizing multi-talker mixed speech. In this paper, we propose and evaluate several architectures to address this problem under the assumption that only a single channel of mixed signal is available. Our technique extends permutation invariant training (PIT) by introducing the front-end feature separation module with the minimum mean square error (MSE) criterion and the back-end recognition module with the minimum cross entropy (CE) criterion. More specifically, during training we compute the average MSE or CE over the whole utterance for each possible utterance-level output-target assignment, pick the one with the minimum MSE or CE, and optimize for that assignment. This strategy elegantly solves the label permutation problem observed in the deep learning based multi-talker mixed speech separation and recognition systems. The proposed architectures are evaluated and compared on an artificially mixed AMI dataset with both two- and three-talker mixed speech. The experimental results indicate that our proposed architectures can cut the word error rate (WER) by 45.0% and 25.0% relatively against the state-of-the-art single-talker speech recognition system across all speakers when their energies are comparable, for two- and three-talker mixed speech, respectively. To our knowledge, this is the first work on the multi-talker mixed speech recognition on the challenging speaker-independent spontaneous large vocabulary continuous speech task.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06562",
        "title": "From Task Classification Towards Similarity Measures for Recommendation in Crowdsourcing Systems",
        "authors": [
            "Steffen Schnitzer",
            "Svenja Neitzel",
            "Christoph Rensing"
        ],
        "abstract": "Task selection in micro-task markets can be supported by recommender systems to help individuals to find appropriate tasks. Previous work showed that for the selection process of a micro-task the semantic aspects, such as the required action and the comprehensibility, are rated more important than factual aspects, such as the payment or the required completion time. This work gives a foundation to create such similarity measures. Therefore, we show that an automatic classification based on task descriptions is possible. Additionally, we propose similarity measures to cluster micro-tasks according to semantic aspects.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06588",
        "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop",
        "authors": [
            "Yaniv Taigman",
            "Lior Wolf",
            "Adam Polyak",
            "Eliya Nachmani"
        ],
        "abstract": "We present a new neural text to speech (TTS) method that is able to transform text to speech in voices that are sampled in the wild. Unlike other systems, our solution is able to deal with unconstrained voice samples and without requiring aligned phonemes or linguistic features. The network architecture is simpler than those in the existing literature and is based on a novel shifting buffer working memory. The same buffer is used for estimating the attention, computing the output audio, and for updating the buffer itself. The input sentence is encoded using a context-free lookup table that contains one entry per character or phoneme. The speakers are similarly represented by a short vector that can also be fitted to new identities, even with only a few samples. Variability in the generated speech is achieved by priming the buffer prior to generating the audio. Experimental results on several datasets demonstrate convincing capabilities, making TTS accessible to a wider range of applications. In order to promote reproducibility, we release our source code and models.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2018-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06598",
        "title": "Toward Incorporation of Relevant Documents in word2vec",
        "authors": [
            "Navid Rekabsaz",
            "Bhaskar Mitra",
            "Mihai Lupu",
            "Allan Hanbury"
        ],
        "abstract": "Recent advances in neural word embedding provide significant benefit to various information retrieval tasks. However as shown by recent studies, adapting the embedding models for the needs of IR tasks can bring considerable further improvements. The embedding models in general define the term relatedness by exploiting the terms' co-occurrences in short-window contexts. An alternative (and well-studied) approach in IR for related terms to a query is using local information i.e. a set of top-retrieved documents. In view of these two methods of term relatedness, in this work, we report our study on incorporating the local information of the query in the word embeddings. One main challenge in this direction is that the dense vectors of word embeddings and their estimation of term-to-term relatedness remain difficult to interpret and hard to analyze. As an alternative, explicit word representations propose vectors whose dimensions are easily interpretable, and recent methods show competitive performance to the dense vectors. We introduce a neural-based explicit representation, rooted in the conceptual ideas of the word2vec Skip-Gram model. The method provides interpretable explicit vectors while keeping the effectiveness of the Skip-Gram model. The evaluation of various explicit representations on word association collections shows that the newly proposed method out- performs the state-of-the-art explicit representations when tasked with ranking highly similar terms. Based on the introduced ex- plicit representation, we discuss our approaches on integrating local documents in globally-trained embedding models and discuss the preliminary results.\n    ",
        "submission_date": "2017-07-20T00:00:00",
        "last_modified_date": "2018-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.06939",
        "title": "Autocompletion interfaces make crowd workers slower, but their use promotes response diversity",
        "authors": [
            "Xipei Liu",
            "James P. Bagrow"
        ],
        "abstract": "Creative tasks such as ideation or question proposal are powerful applications of crowdsourcing, yet the quantity of workers available for addressing practical problems is often insufficient. To enable scalable crowdsourcing thus requires gaining all possible efficiency and information from available workers. One option for text-focused tasks is to allow assistive technology, such as an autocompletion user interface (AUI), to help workers input text responses. But support for the efficacy of AUIs is mixed. Here we designed and conducted a randomized experiment where workers were asked to provide short text responses to given questions. Our experimental goal was to determine if an AUI helps workers respond more quickly and with improved consistency by mitigating typos and misspellings. Surprisingly, we found that neither occurred: workers assigned to the AUI treatment were slower than those assigned to the non-AUI control and their responses were more diverse, not less, than those of the control. Both the lexical and semantic diversities of responses were higher, with the latter measured using word2vec. A crowdsourcer interested in worker speed may want to avoid using an AUI, but using an AUI to boost response diversity may be valuable to crowdsourcers interested in receiving as much novel information from workers as possible.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07066",
        "title": "Ultraslow diffusion in language: Dynamics of appearance of already popular adjectives on Japanese blogs",
        "authors": [
            "Hayafumi Watanabe"
        ],
        "abstract": "What dynamics govern a time series representing the appearance of words in social media data? In this paper, we investigate an elementary dynamics, from which word-dependent special effects are segregated, such as breaking news, increasing (or decreasing) concerns, or seasonality. To elucidate this problem, we investigated approximately three billion Japanese blog articles over a period of six years, and analysed some corresponding solvable mathematical models. From the analysis, we found that a word appearance can be explained by the random diffusion model based on the power-law forgetting process, which is a type of long memory point process related to ARFIMA(0,0.5,0). In particular, we confirmed that ultraslow diffusion (where the mean squared displacement grows logarithmically), which the model predicts in an approximate manner, reproduces the actual data. In addition, we also show that the model can reproduce other statistical properties of a time series: (i) the fluctuation scaling, (ii) spectrum density, and (iii) shapes of the probability density functions.\n    ",
        "submission_date": "2017-07-21T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07102",
        "title": "OBJ2TEXT: Generating Visually Descriptive Language from Object Layouts",
        "authors": [
            "Xuwang Yin",
            "Vicente Ordonez"
        ],
        "abstract": "Generating captions for images is a task that has recently received considerable attention. In this work we focus on caption generation for abstract scenes, or object layouts where the only information provided is a set of objects and their locations. We propose OBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show that our model, despite encoding object layouts as a sequence, can represent spatial relationships between objects, and generate descriptions that are globally coherent and semantically relevant. We test our approach in a task of object-layout captioning by using only object annotations as inputs. We additionally show that our model, combined with a state-of-the-art object detector, improves an image captioning model from 0.863 to 0.950 (CIDEr score) in the test benchmark of the standard MS-COCO Captioning task.\n    ",
        "submission_date": "2017-07-22T00:00:00",
        "last_modified_date": "2017-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07270",
        "title": "MatchZoo: A Toolkit for Deep Text Matching",
        "authors": [
            "Yixing Fan",
            "Liang Pang",
            "JianPeng Hou",
            "Jiafeng Guo",
            "Yanyan Lan",
            "Xueqi Cheng"
        ],
        "abstract": "In recent years, deep neural models have been widely adopted for text matching tasks, such as question answering and information retrieval, showing improved performance as compared with previous methods. In this paper, we introduce the MatchZoo toolkit that aims to facilitate the designing, comparing and sharing of deep text matching models. Specifically, the toolkit provides a unified data preparation module for different text matching problems, a flexible layer-based model construction process, and a variety of training objectives and evaluation metrics. In addition, the toolkit has implemented two schools of representative deep text matching models, namely representation-focused models and interaction-focused models. Finally, users can easily modify existing models, create and share their own models for text matching in MatchZoo.\n    ",
        "submission_date": "2017-07-23T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07585",
        "title": "Stock Prediction: a method based on extraction of news features and recurrent neural networks",
        "authors": [
            "Zeya Zhang",
            "Weizheng Chen",
            "Hongfei Yan"
        ],
        "abstract": "This paper proposed a method for stock prediction. In terms of feature extraction, we extract the features of stock-related news besides stock prices. We first select some seed words based on experience which are the symbols of good news and bad news. Then we propose an optimization method and calculate the positive polar of all words. After that, we construct the features of news based on the positive polar of their words. In consideration of sequential stock prices and continuous news effects, we propose a recurrent neural network model to help predict stock prices. Compared to SVM classifier with price features, we find our proposed method has an over 5% improvement on stock prediction accuracy in experiments.\n    ",
        "submission_date": "2017-07-19T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07605",
        "title": "Share your Model instead of your Data: Privacy Preserving Mimic Learning for Ranking",
        "authors": [
            "Mostafa Dehghani",
            "Hosein Azarbonyad",
            "Jaap Kamps",
            "Maarten de Rijke"
        ],
        "abstract": "Deep neural networks have become a primary tool for solving problems in many fields. They are also used for addressing information retrieval problems and show strong performance in several tasks. Training these models requires large, representative datasets and for most IR tasks, such data contains sensitive information from users. Privacy and confidentiality concerns prevent many data owners from sharing the data, thus today the research community can only benefit from research on large-scale datasets in a limited manner. In this paper, we discuss privacy preserving mimic learning, i.e., using predictions from a privacy preserving trained model instead of labels from the original sensitive training data as a supervision signal. We present the results of preliminary experiments in which we apply the idea of mimic learning and privacy preserving mimic learning for the task of document re-ranking as one of the core IR tasks. This research is a step toward laying the ground for enabling researchers from data-rich environments to share knowledge learned from actual users' data, which should facilitate research collaborations.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07660",
        "title": "Thread Reconstruction in Conversational Data using Neural Coherence Models",
        "authors": [
            "Dat Tien Nguyen",
            "Shafiq Joty",
            "Basma El Amel Boussaha",
            "Maarten de Rijke"
        ],
        "abstract": "Discussion forums are an important source of information. They are often used to answer specific questions a user might have and to discover more about a topic of interest. Discussions in these forums may evolve in intricate ways, making it difficult for users to follow the flow of ideas. We propose a novel approach for automatically identifying the underlying thread structure of a forum discussion. Our approach is based on a neural model that computes coherence scores of possible reconstructions and then selects the highest scoring, i.e., the most coherent one. Preliminary experiments demonstrate promising results outperforming a number of strong baseline methods.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07678",
        "title": "Extracting Core Claims from Scientific Articles",
        "authors": [
            "Tom Jansen",
            "Tobias Kuhn"
        ],
        "abstract": "The number of scientific articles has grown rapidly over the years and there are no signs that this growth will slow down in the near future. Because of this, it becomes increasingly difficult to keep up with the latest developments in a scientific field. To address this problem, we present here an approach to help researchers learn about the latest developments and findings by extracting in a normalized form core claims from scientific articles. This normalized representation is a controlled natural language of English sentences called AIDA, which has been proposed in previous work as a method to formally structure and organize scientific findings and discourse. We show how such AIDA sentences can be automatically extracted by detecting the core claim of an article, checking for AIDA compliance, and - if necessary - transforming it into a compliant sentence. While our algorithm is still far from perfect, our results indicate that the different steps are feasible and they support the claim that AIDA sentences might be a promising approach to improve scientific communication in the future.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07792",
        "title": "Integrating Lexical and Temporal Signals in Neural Ranking Models for Searching Social Media Streams",
        "authors": [
            "Jinfeng Rao",
            "Hua He",
            "Haotian Zhang",
            "Ferhan Ture",
            "Royal Sequiera",
            "Salman Mohammed",
            "Jimmy Lin"
        ],
        "abstract": "Time is an important relevance signal when searching streams of social media posts. The distribution of document timestamps from the results of an initial query can be leveraged to infer the distribution of relevant documents, which can then be used to rerank the initial results. Previous experiments have shown that kernel density estimation is a simple yet effective implementation of this idea. This paper explores an alternative approach to mining temporal signals with recurrent neural networks. Our intuition is that neural networks provide a more expressive framework to capture the temporal coherence of neighboring documents in time. To our knowledge, we are the first to integrate lexical and temporal signals in an end-to-end neural network architecture, in which existing neural ranking models are used to generate query-document similarity vectors that feed into a bidirectional LSTM layer for temporal modeling. Our results are mixed: existing neural models for document ranking alone yield limited improvements over simple baselines, but the integration of lexical and temporal signals yield significant improvements over competitive temporal baselines.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07804",
        "title": "Exploring the Effectiveness of Convolutional Neural Networks for Answer Selection in End-to-End Question Answering",
        "authors": [
            "Royal Sequiera",
            "Gaurav Baruah",
            "Zhucheng Tu",
            "Salman Mohammed",
            "Jinfeng Rao",
            "Haotian Zhang",
            "Jimmy Lin"
        ],
        "abstract": "Most work on natural language question answering today focuses on answer selection: given a candidate list of sentences, determine which contains the answer. Although important, answer selection is only one stage in a standard end-to-end question answering pipeline. This paper explores the effectiveness of convolutional neural networks (CNNs) for answer selection in an end-to-end context using the standard TrecQA dataset. We observe that a simple idf-weighted word overlap algorithm forms a very strong baseline, and that despite substantial efforts by the community in applying deep learning to tackle answer selection, the gains are modest at best on this dataset. Furthermore, it is unclear if a CNN is more effective than the baseline in an end-to-end context based on standard retrieval metrics. To further explore this finding, we conducted a manual user evaluation, which confirms that answers from the CNN are detectably better than those from idf-weighted word overlap. This result suggests that users are sensitive to relatively small differences in answer selection quality.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07835",
        "title": "Towards Semantic Query Segmentation",
        "authors": [
            "Ajinkya Kale",
            "Thrivikrama Taula",
            "Sanjika Hewavitharana",
            "Amit Srivastava"
        ],
        "abstract": "Query Segmentation is one of the critical components for understanding users' search intent in Information Retrieval tasks. It involves grouping tokens in the search query into meaningful phrases which help downstream tasks like search relevance and query understanding. In this paper, we propose a novel approach to segment user queries using distributed query embeddings. Our key contribution is a supervised approach to the segmentation task using low-dimensional feature vectors for queries, getting rid of traditional hand tuned and heuristic NLP features which are quite expensive.\n",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07847",
        "title": "Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering",
        "authors": [
            "Yi Tay",
            "Luu Anh Tuan",
            "Siu Cheung Hui"
        ],
        "abstract": "The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, \\textsc{HyperQA}, is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple QA benchmarks. The novelty behind \\textsc{HyperQA} is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space. This empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks.\n    ",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.07930",
        "title": "Structural Regularities in Text-based Entity Vector Spaces",
        "authors": [
            "Christophe Van Gysel",
            "Maarten de Rijke",
            "Evangelos Kanoulas"
        ],
        "abstract": "Entity retrieval is the task of finding entities such as people or products in response to a query, based solely on the textual documents they are associated with. Recent semantic entity retrieval algorithms represent queries and experts in finite-dimensional vector spaces, where both are constructed from text sequences.\n",
        "submission_date": "2017-07-25T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08209",
        "title": "On the letter frequencies and entropy of written Marathi",
        "authors": [
            "Jaydeep Chipalkatti",
            "Mihir Kulkarni"
        ],
        "abstract": "We carry out a comprehensive analysis of letter frequencies in contemporary written Marathi. We determine sets of letters which statistically predominate any large generic Marathi text, and use these sets to estimate the entropy of Marathi.\n    ",
        "submission_date": "2017-07-11T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08309",
        "title": "Probabilistic Graphical Models for Credibility Analysis in Evolving Online Communities",
        "authors": [
            "Subhabrata Mukherjee"
        ],
        "abstract": "One of the major hurdles preventing the full exploitation of information from online communities is the widespread concern regarding the quality and credibility of user-contributed content. Prior works in this domain operate on a static snapshot of the community, making strong assumptions about the structure of the data (e.g., relational tables), or consider only shallow features for text classification.\n",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08616",
        "title": "Guiding Reinforcement Learning Exploration Using Natural Language",
        "authors": [
            "Brent Harrison",
            "Upol Ehsan",
            "Mark O. Riedl"
        ],
        "abstract": "In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. We evaluate this technique using the popular arcade game, Frogger, under ideal and non-ideal conditions. This evaluation shows that our modified policy shaping algorithm improves over a Q-learning agent as well as a baseline version of policy shaping.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.08668",
        "title": "A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting Action-Oriented and Goal-Oriented Instructions",
        "authors": [
            "Siddharth Karamcheti",
            "Edward C. Williams",
            "Dilip Arumugam",
            "Mina Rhee",
            "Nakul Gopalan",
            "Lawson L. S. Wong",
            "Stefanie Tellex"
        ],
        "abstract": "Robots operating alongside humans in diverse, stochastic environments must be able to accurately interpret natural language commands. These instructions often fall into one of two categories: those that specify a goal condition or target state, and those that specify explicit actions, or how to perform a given task. Recent approaches have used reward functions as a semantic representation of goal-based commands, which allows for the use of a state-of-the-art planner to find a policy for the given task. However, these reward functions cannot be directly used to represent action-oriented commands. We introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding Network (DRAGGN), for task grounding and execution that handles natural language from either category as input, and generalizes to unseen environments. Our robot-simulation results demonstrate that a system successfully interpreting both goal-oriented and action-oriented task specifications brings us closer to robust natural language understanding for human-robot interaction.\n    ",
        "submission_date": "2017-07-26T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09098",
        "title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension",
        "authors": [
            "Boyuan Pan",
            "Hao Li",
            "Zhou Zhao",
            "Bin Cao",
            "Deng Cai",
            "Xiaofei He"
        ],
        "abstract": "Machine comprehension(MC) style question answering is a representative problem in natural language processing. Previous methods rarely spend time on the improvement of encoding layer, especially the embedding of syntactic information and name entity of the words, which are very crucial to the quality of encoding. Moreover, existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence, neither of them can handle the proper weight of the key words in query sentence. In this paper, we introduce a novel neural network architecture called Multi-layer Embedding with Memory Network(MEMEN) for machine reading task. In the encoding layer, we employ classic skip-gram model to the syntactic and semantic information of the words to train a new kind of embedding layer. We also propose a memory network of full-orientation matching of the query and passage to catch more pivotal information. Experiments show that our model has competitive results both from the perspectives of precision and efficiency in Stanford Question Answering Dataset(SQuAD) among all published results and achieves the state-of-the-art results on TriviaQA dataset.\n    ",
        "submission_date": "2017-07-28T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09118",
        "title": "Counterfactual Learning from Bandit Feedback under Deterministic Logging: A Case Study in Statistical Machine Translation",
        "authors": [
            "Carolin Lawrence",
            "Artem Sokolov",
            "Stefan Riezler"
        ],
        "abstract": "The goal of counterfactual learning for statistical machine translation (SMT) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system. A challenge arises by the fact that risk-averse commercial SMT systems deterministically log the most probable translation. The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning. We show that counterfactual learning from deterministic bandit logs is possible nevertheless by smoothing out deterministic components in learning. This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization. Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback.\n    ",
        "submission_date": "2017-07-28T00:00:00",
        "last_modified_date": "2017-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09457",
        "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
        "authors": [
            "Jieyu Zhao",
            "Tianlu Wang",
            "Mark Yatskar",
            "Vicente Ordonez",
            "Kai-Wei Chang"
        ],
        "abstract": "Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.\n    ",
        "submission_date": "2017-07-29T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09538",
        "title": "Benchmarking Multimodal Sentiment Analysis",
        "authors": [
            "Erik Cambria",
            "Devamanyu Hazarika",
            "Soujanya Poria",
            "Amir Hussain",
            "R.B.V. Subramaanyam"
        ],
        "abstract": "We propose a framework for multimodal sentiment analysis and emotion recognition using convolutional neural network-based feature extraction from text and visual modalities. We obtain a performance improvement of 10% over the state of the art by combining visual, text and audio features. We also discuss some major issues frequently ignored in multimodal sentiment analysis research: the role of speaker-independent models, importance of the modalities and generalizability. The paper thus serve as a new benchmark for further research in multimodal sentiment analysis and also demonstrates the different facets of analysis to be considered while performing such tasks.\n    ",
        "submission_date": "2017-07-29T00:00:00",
        "last_modified_date": "2017-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09823",
        "title": "Familia: An Open-Source Toolkit for Industrial Topic Modeling",
        "authors": [
            "Di Jiang",
            "Zeyu Chen",
            "Rongzhong Lian",
            "Siqi Bao",
            "Chen Li"
        ],
        "abstract": "Familia is an open-source toolkit for pragmatic topic modeling in industry. Familia abstracts the utilities of topic modeling in industry as two paradigms: semantic representation and semantic matching. Efficient implementations of the two paradigms are made publicly available for the first time. Furthermore, we provide off-the-shelf topic models trained on large-scale industrial corpora, including Latent Dirichlet Allocation (LDA), SentenceLDA and Topical Word Embedding (TWE). We further describe typical applications which are successfully powered by topic modeling, in order to ease the confusions and difficulties of software engineers during topic model selection and utilization.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1707.09872",
        "title": "Full-Network Embedding in a Multimodal Embedding Pipeline",
        "authors": [
            "Armand Vilalta",
            "Dario Garcia-Gasulla",
            "Ferran Par\u00e9s",
            "Eduard Ayguad\u00e9",
            "Jesus Labarta",
            "Ulises Cort\u00e9s",
            "Toyotaro Suzumura"
        ],
        "abstract": "The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations. To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding. These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.\n    ",
        "submission_date": "2017-07-24T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00077",
        "title": "Bayesian Sparsification of Recurrent Neural Networks",
        "authors": [
            "Ekaterina Lobacheva",
            "Nadezhda Chirkova",
            "Dmitry Vetrov"
        ],
        "abstract": "Recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights. Recently proposed Sparse Variational Dropout eliminates the majority of the weights in a feed-forward neural network without significant loss of quality. We apply this technique to sparsify recurrent neural networks. To account for recurrent specifics we also rely on Binary Variational Dropout for RNN. We report 99.5% sparsity level on sentiment analysis task without a quality drop and up to 87% sparsity level on language modeling task with slight loss of accuracy.\n    ",
        "submission_date": "2017-07-31T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00111",
        "title": "A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models",
        "authors": [
            "Kartik Goyal",
            "Graham Neubig",
            "Chris Dyer",
            "Taylor Berg-Kirkpatrick"
        ],
        "abstract": "Beam search is a desirable choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when compared with greedy decoding. In order to train models that can more effectively make use of beam search, we propose a new training procedure that focuses on the final loss metric (e.g. Hamming loss) evaluated on the output of beam search. While well-defined, this \"direct loss\" objective is itself discontinuous and thus difficult to optimize. Hence, in our approach, we form a sub-differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure. In experiments, we show that optimizing this new training objective yields substantially better results on two sequence tasks (Named Entity Recognition and CCG Supertagging) when compared with both cross entropy trained greedy decoding and cross entropy trained beam decoding baselines.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00112",
        "title": "Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations",
        "authors": [
            "Benjamin J. Lengerich",
            "Andrew L. Maas",
            "Christopher Potts"
        ],
        "abstract": "Knowledge graphs are a versatile framework to encode richly structured data relationships, but it can be challenging to combine these graphs with unstructured data. Methods for retrofitting pre-trained entity representations to the structure of a knowledge graph typically assume that entities are embedded in a connected space and that relations imply similarity. However, useful knowledge graphs often contain diverse entities and relations (with potentially disjoint underlying corpora) which do not accord with these assumptions. To overcome these limitations, we present Functional Retrofitting, a framework that generalizes current retrofitting methods by explicitly modeling pairwise relations. Our framework can directly incorporate a variety of pairwise penalty functions previously developed for knowledge graph completion. Further, it allows users to encode, learn, and extract information about relation semantics. We present both linear and neural instantiations of the framework. Functional Retrofitting significantly outperforms existing retrofitting methods on complex knowledge graphs and loses no accuracy on simpler graphs (in which relations do imply similarity). Finally, we demonstrate the utility of the framework by predicting new drug--disease treatment pairs in a large, complex health knowledge graph.\n    ",
        "submission_date": "2017-08-01T00:00:00",
        "last_modified_date": "2018-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.00667",
        "title": "Deep Reinforcement Learning for Inquiry Dialog Policies with Logical Formula Embeddings",
        "authors": [
            "Takuya Hiraoka",
            "Masaaki Tsuchida",
            "Yotaro Watanabe"
        ],
        "abstract": "This paper is the first attempt to learn the policy of an inquiry dialog system (IDS) by using deep reinforcement learning (DRL). Most IDS frameworks represent dialog states and dialog acts with logical formulae. In order to make learning inquiry dialog policies more effective, we introduce a logical formula embedding framework based on a recursive neural network. The results of experiments to evaluate the effect of 1) the DRL and 2) the logical formula embedding framework show that the combination of the two are as effective or even better than existing rule-based methods for inquiry dialog policies.\n    ",
        "submission_date": "2017-08-02T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01336",
        "title": "MemexQA: Visual Memex Question Answering",
        "authors": [
            "Lu Jiang",
            "Junwei Liang",
            "Liangliang Cao",
            "Yannis Kalantidis",
            "Sachin Farfade",
            "Alexander Hauptmann"
        ],
        "abstract": "This paper proposes a new task, MemexQA: given a collection of photos or videos from a user, the goal is to automatically answer questions that help users recover their memory about events captured in the collection. Towards solving the task, we 1) present the MemexQA dataset, a large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers, 2) propose MemexNet, a unified, end-to-end trainable network architecture for image, text and video question answering. Experimental results on the MemexQA dataset demonstrate that MemexNet outperforms strong baselines and yields the state-of-the-art on this novel and challenging task. The promising results on TextQA and VideoQA suggest MemexNet's efficacy and scalability across various QA tasks.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01565",
        "title": "Improving Speaker-Independent Lipreading with Domain-Adversarial Training",
        "authors": [
            "Michael Wand",
            "Juergen Schmidhuber"
        ],
        "abstract": "We present a Lipreading system, i.e. a speech recognition system using only visual features, which uses domain-adversarial training for speaker independence. Domain-adversarial training is integrated into the optimization of a lipreader based on a stack of feedforward and LSTM (Long Short-Term Memory) recurrent neural networks, yielding an end-to-end trainable system which only requires a very small number of frames of untranscribed target data to substantially improve the recognition accuracy on the target speaker. On pairs of different source and target speakers, we achieve a relative accuracy improvement of around 40% with only 15 to 20 seconds of untranscribed target speech data. On multi-speaker training setups, the accuracy improvements are smaller but still substantial.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2017-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01677",
        "title": "A network approach to topic models",
        "authors": [
            "Martin Gerlach",
            "Tiago P. Peixoto",
            "Eduardo G. Altmann"
        ],
        "abstract": "One of the main computational and scientific challenges in the modern age is to extract useful information from unstructured texts. Topic models are one popular machine-learning approach which infers the latent topical structure of a collection of documents. Despite their success --- in particular of its most widely used variant called Latent Dirichlet Allocation (LDA) --- and numerous applications in sociology, history, and linguistics, topic models are known to suffer from severe conceptual and practical problems, e.g. a lack of justification for the Bayesian priors, discrepancies with statistical properties of real texts, and the inability to properly choose the number of topics. Here we obtain a fresh view on the problem of identifying topical structures by relating it to the problem of finding communities in complex networks. This is achieved by representing text corpora as bipartite networks of documents and words. By adapting existing community-detection methods -- using a stochastic block model (SBM) with non-parametric priors -- we obtain a more versatile and principled framework for topic modeling (e.g., it automatically detects the number of topics and hierarchically clusters both the words and documents). The analysis of artificial and real corpora demonstrates that our SBM approach leads to better topic models than LDA in terms of statistical model selection. More importantly, our work shows how to formally relate methods from community detection and topic modeling, opening the possibility of cross-fertilization between these two fields.\n    ",
        "submission_date": "2017-08-04T00:00:00",
        "last_modified_date": "2018-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01776",
        "title": "e-QRAQ: A Multi-turn Reasoning Dataset and Simulator with Explanations",
        "authors": [
            "Clemens Rosenbaum",
            "Tian Gao",
            "Tim Klinger"
        ],
        "abstract": "In this paper we present a new dataset and user simulator e-QRAQ (explainable Query, Reason, and Answer Question) which tests an Agent's ability to read an ambiguous text; ask questions until it can answer a challenge question; and explain the reasoning behind its questions and answer. The User simulator provides the Agent with a short, ambiguous story and a challenge question about the story. The story is ambiguous because some of the entities have been replaced by variables. At each turn the Agent may ask for the value of a variable or try to answer the challenge question. In response the User simulator provides a natural language explanation of why the Agent's query or answer was useful in narrowing down the set of possible answers, or not. To demonstrate one potential application of the e-QRAQ dataset, we train a new neural architecture based on End-to-End Memory Networks to successfully generate both predictions and partial explanations of its current understanding of the problem. We observe a strong correlation between the quality of the prediction and explanation.\n    ",
        "submission_date": "2017-08-05T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.01944",
        "title": "Rookie: A unique approach for exploring news archives",
        "authors": [
            "Abram Handler",
            "Brendan O'Connor"
        ],
        "abstract": "News archives are an invaluable primary source for placing current events in historical context. But current search engine tools do a poor job at uncovering broad themes and narratives across documents. We present Rookie: a practical software system which uses natural language processing (NLP) to help readers, reporters and editors uncover broad stories in news archives. Unlike prior work, Rookie's design emerged from 18 months of iterative development in consultation with editors and computational journalists. This process lead to a dramatically different approach from previous academic systems with similar goals. Our efforts offer a generalizable case study for others building real-world journalism software using NLP.\n    ",
        "submission_date": "2017-08-06T00:00:00",
        "last_modified_date": "2017-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02214",
        "title": "LitStoryTeller: An Interactive System for Visual Exploration of Scientific Papers Leveraging Named entities and Comparative Sentences",
        "authors": [
            "Qing Ping",
            "Chaomei Chen"
        ],
        "abstract": "The present study proposes LitStoryTeller, an interactive system for visually exploring the semantic structure of a scientific article. We demonstrate how LitStoryTeller could be used to answer some of the most fundamental research questions, such as how a new method was built on top of existing methods, based on what theoretical proof and experimental evidences. More importantly, LitStoryTeller can assist users to understand the full and interesting story a scientific paper, with a concise outline and important details. The proposed system borrows a metaphor from screen play, and visualizes the storyline of a scientific paper by arranging its characters (scientific concepts or terminologies) and scenes (paragraphs/sentences) into a progressive and interactive storyline. Such storylines help to preserve the semantic structure and logical thinking process of a scientific paper. Semantic structures, such as scientific concepts and comparative sentences, are extracted using existing named entity recognition APIs and supervised classifiers, from a scientific paper automatically. Two supplementary views, ranked entity frequency view and entity co-occurrence network view, are provided to help users identify the \"main plot\" of such scientific storylines. When collective documents are ready, LitStoryTeller also provides a temporal entity evolution view and entity community view for collection digestion.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02255",
        "title": "Generative Statistical Models with Self-Emergent Grammar of Chord Sequences",
        "authors": [
            "Hiroaki Tsushima",
            "Eita Nakamura",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii"
        ],
        "abstract": "Generative statistical models of chord sequences play crucial roles in music processing. To capture syntactic similarities among certain chords (e.g. in C major key, between G and G7 and between F and Dm), we study hidden Markov models and probabilistic context-free grammar models with latent variables describing syntactic categories of chord symbols and their unsupervised learning techniques for inducing the latent grammar from data. Surprisingly, we find that these models often outperform conventional Markov models in predictive power, and the self-emergent categories often correspond to traditional harmonic functions. This implies the need for chord categories in harmony models from the informatics perspective.\n    ",
        "submission_date": "2017-08-07T00:00:00",
        "last_modified_date": "2018-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02702",
        "title": "Neural Vector Spaces for Unsupervised Information Retrieval",
        "authors": [
            "Christophe Van Gysel",
            "Maarten de Rijke",
            "Evangelos Kanoulas"
        ],
        "abstract": "We propose the Neural Vector Space Model (NVSM), a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that NVSM performs better at document ranking than existing latent semantic vector space methods. The addition of NVSM to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, NVSM adds a complementary relevance signal. Next to semantic matching, we find that NVSM performs well in cases where lexical matching is needed.\n",
        "submission_date": "2017-08-09T00:00:00",
        "last_modified_date": "2018-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.02711",
        "title": "Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge",
        "authors": [
            "Damien Teney",
            "Peter Anderson",
            "Xiaodong He",
            "Anton van den Hengel"
        ],
        "abstract": "This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.\n    ",
        "submission_date": "2017-08-09T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03044",
        "title": "\"Is there anything else I can help you with?\": Challenges in Deploying an On-Demand Crowd-Powered Conversational Agent",
        "authors": [
            "Ting-Hao Kenneth Huang",
            "Walter S. Lasecki",
            "Amos Azaria",
            "Jeffrey P. Bigham"
        ],
        "abstract": "Intelligent conversational assistants, such as Apple's Siri, Microsoft's Cortana, and Amazon's Echo, have quickly become a part of our digital life. However, these assistants have major limitations, which prevents users from conversing with them as they would with human dialog partners. This limits our ability to observe how users really want to interact with the underlying system. To address this problem, we developed a crowd-powered conversational assistant, Chorus, and deployed it to see how users and workers would interact together when mediated by the system. Chorus sophisticatedly converses with end users over time by recruiting workers on demand, which in turn decide what might be the best response for each user sentence. Up to the first month of our deployment, 59 users have held conversations with Chorus during 320 conversational sessions. In this paper, we present an account of Chorus' deployment, with a focus on four challenges: (i) identifying when conversations are over, (ii) malicious users and workers, (iii) on-demand recruiting, and (iv) settings in which consensus is not enough. Our observations could assist the deployment of crowd-powered conversation systems and crowd-powered systems in general.\n    ",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03052",
        "title": "Communication-Free Parallel Supervised Topic Models",
        "authors": [
            "Lee Gao",
            "Ronghuo Zheng"
        ],
        "abstract": "Embarrassingly (communication-free) parallel Markov chain Monte Carlo (MCMC) methods are commonly used in learning graphical models. However, MCMC cannot be directly applied in learning topic models because of the quasi-ergodicity problem caused by multimodal distribution of topics. In this paper, we develop an embarrassingly parallel MCMC algorithm for sLDA. Our algorithm works by switching the order of sampled topics combination and labeling variable prediction in sLDA, in which it overcomes the quasi-ergodicity problem because high-dimension topics that follow a multimodal distribution are projected into one-dimension document labels that follow a unimodal distribution. Our empirical experiments confirm that the out-of-sample prediction performance using our embarrassingly parallel algorithm is comparable to non-parallel sLDA while the computation time is significantly reduced.\n    ",
        "submission_date": "2017-08-10T00:00:00",
        "last_modified_date": "2017-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03418",
        "title": "Learning to Attend, Copy, and Generate for Session-Based Query Suggestion",
        "authors": [
            "Mostafa Dehghani",
            "Sascha Rothe",
            "Enrique Alfonseca",
            "Pascal Fleury"
        ],
        "abstract": "Users try to articulate their complex information needs during search sessions by reformulating their queries. To make this process more effective, search engines provide related queries to help users in specifying the information need in their search process. In this paper, we propose a customized sequence-to-sequence model for session-based query suggestion. In our model, we employ a query-aware attention mechanism to capture the structure of the session context. is enables us to control the scope of the session from which we infer the suggested next query, which helps not only handle the noisy data but also automatically detect session boundaries. Furthermore, we observe that, based on the user query reformulation behavior, within a single session a large portion of query terms is retained from the previously submitted queries and consists of mostly infrequent or unseen terms that are usually not included in the vocabulary. We therefore empower the decoder of our model to access the source words from the session context during decoding by incorporating a copy mechanism. Moreover, we propose evaluation metrics to assess the quality of the generative models for query suggestion. We conduct an extensive set of experiments and analysis. e results suggest that our model outperforms the baselines both in terms of the generating queries and scoring candidate queries for the task of query suggestion.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03569",
        "title": "Semantic Word Clouds with Background Corpus Normalization and t-distributed Stochastic Neighbor Embedding",
        "authors": [
            "Erich Schubert",
            "Andreas Spitz",
            "Michael Weiler",
            "Johanna Gei\u00df",
            "Michael Gertz"
        ],
        "abstract": "Many word clouds provide no semantics to the word placement, but use a random layout optimized solely for aesthetic purposes. We propose a novel approach to model word significance and word affinity within a document, and in comparison to a large background corpus. We demonstrate its usefulness for generating more meaningful word clouds as a visual summary of a given document. We then select keywords based on their significance and construct the word cloud based on the derived affinity. Based on a modified t-distributed stochastic neighbor embedding (t-SNE), we generate a semantic word placement. For words that cooccur significantly, we include edges, and cluster the words according to their cooccurrence. For this we designed a scalable and memory-efficient sketch-based approach usable on commodity hardware to aggregate the required corpus statistics needed for normalization, and for identifying keywords as well as significant cooccurences. We empirically validate our approch using a large Wikipedia corpus.\n    ",
        "submission_date": "2017-08-11T00:00:00",
        "last_modified_date": "2017-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.03892",
        "title": "EmoTxt: A Toolkit for Emotion Recognition from Text",
        "authors": [
            "Fabio Calefato",
            "Filippo Lanubile",
            "Nicole Novielli"
        ],
        "abstract": "We present EmoTxt, a toolkit for emotion recognition from text, trained and tested on a gold standard of about 9K question, answers, and comments from online interactions. We provide empirical evidence of the performance of EmoTxt. To the best of our knowledge, EmoTxt is the first open-source toolkit supporting both emotion recognition from text and training of custom emotion classification models.\n    ",
        "submission_date": "2017-08-13T00:00:00",
        "last_modified_date": "2018-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04120",
        "title": "Putting Self-Supervised Token Embedding on the Tables",
        "authors": [
            "Marc Szafraniec",
            "Gautier Marti",
            "Philippe Donnat"
        ],
        "abstract": "Information distribution by electronic messages is a privileged means of transmission for many businesses and individuals, often under the form of plain-text tables. As their number grows, it becomes necessary to use an algorithm to extract text and numbers instead of a human. Usual methods are focused on regular expressions or on a strict structure in the data, but are not efficient when we have many variations, fuzzy structure or implicit labels. In this paper we introduce SC2T, a totally self-supervised model for constructing vector representations of tokens in semi-structured messages by using characters and context levels that address these issues. It can then be used for an unsupervised labeling of tokens, or be the basis for a semi-supervised information extraction system.\n    ",
        "submission_date": "2017-07-28T00:00:00",
        "last_modified_date": "2017-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04686",
        "title": "VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation",
        "authors": [
            "Chuang Gan",
            "Yandong Li",
            "Haoxiang Li",
            "Chen Sun",
            "Boqing Gong"
        ],
        "abstract": "Rich and dense human labeled datasets are among the main enabling factors for the recent advance on vision-language understanding. Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal different levels and perspectives of human understandings about the same visual scenes --- and even the same set of images (e.g., of COCO). The popularity of COCO correlates those annotations and tasks. Explicitly linking them up may significantly benefit both individual tasks and the unified vision and language modeling. We present the preliminary work of linking the instance segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected links visual questions and segmentation answers (VQS). They transfer human supervision between the previously separate tasks, offer more effective leverage to existing problems, and also open the door for new research problems and models. We study two applications of the VQS data in this paper: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting the multilayer perceptrons with some attention features that are learned using the segmentation-QA links as explicit supervision. To put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that the instance segmentations are given at the test stage.\n    ",
        "submission_date": "2017-08-15T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04776",
        "title": "Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network",
        "authors": [
            "Yuxin Peng",
            "Jinwei Qi",
            "Yuxin Yuan"
        ],
        "abstract": "Nowadays, cross-modal retrieval plays an indispensable role to flexibly find information across different modalities of data. Effectively measuring the similarity between different modalities of data is the key of cross-modal retrieval. Different modalities such as image and text have imbalanced and complementary relationships, which contain unequal amount of information when describing the same semantics. For example, images often contain more details that cannot be demonstrated by textual descriptions and vice versa. Existing works based on Deep Neural Network (DNN) mostly construct one common space for different modalities to find the latent alignments between them, which lose their exclusive modality-specific characteristics. Different from the existing works, we propose modality-specific cross-modal similarity measurement (MCSM) approach by constructing independent semantic space for each modality, which adopts end-to-end framework to directly generate modality-specific cross-modal similarity without explicit common representation. For each semantic space, modality-specific characteristics within one modality are fully exploited by recurrent attention network, while the data of another modality is projected into this space with attention based joint embedding to utilize the learned attention weights for guiding the fine-grained cross-modal correlation learning, which can capture the imbalanced and complementary relationships between different modalities. Finally, the complementarity between the semantic spaces for different modalities is explored by adaptive fusion of the modality-specific cross-modal similarities to perform cross-modal retrieval. Experiments on the widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset verify the effectiveness of our proposed approach, outperforming 9 state-of-the-art methods.\n    ",
        "submission_date": "2017-08-16T00:00:00",
        "last_modified_date": "2017-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.04968",
        "title": "Fault in your stars: An Analysis of Android App Reviews",
        "authors": [
            "Rahul Aralikatte",
            "Giriprasad Sridhara",
            "Neelamadhav Gantayat",
            "Senthil Mani"
        ],
        "abstract": "Mobile app distribution platforms such as Google Play Store allow users to share their feedback about downloaded apps in the form of a review comment and a corresponding star rating. Typically, the star rating ranges from one to five stars, with one star denoting a high sense of dissatisfaction with the app and five stars denoting a high sense of satisfaction.\n",
        "submission_date": "2017-08-16T00:00:00",
        "last_modified_date": "2018-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05122",
        "title": "Evaluating Visual Conversational Agents via Cooperative Human-AI Games",
        "authors": [
            "Prithvijit Chattopadhyay",
            "Deshraj Yadav",
            "Viraj Prabhu",
            "Arjun Chandrasekaran",
            "Abhishek Das",
            "Stefan Lee",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "As AI continues to advance, human-AI teams are inevitable. However, progress in AI is routinely measured in isolation, without a human in the loop. It is crucial to benchmark progress in AI, not just in isolation, but also in terms of how it translates to helping humans perform certain tasks, i.e., the performance of human-AI teams.\n",
        "submission_date": "2017-08-17T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05271",
        "title": "Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects",
        "authors": [
            "Ting Yao",
            "Yingwei Pan",
            "Yehao Li",
            "Tao Mei"
        ],
        "abstract": "Image captioning often requires a large set of training image-sentence pairs. In practice, however, acquiring sufficient training pairs is always expensive, making the recent captioning models limited in their ability to describe objects outside of training corpora (i.e., novel objects). In this paper, we present Long Short-Term Memory with Copying Mechanism (LSTM-C) --- a new architecture that incorporates copying into the Convolutional Neural Networks (CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for describing novel objects in captions. Specifically, freely available object recognition datasets are leveraged to develop classifiers for novel objects. Our LSTM-C then nicely integrates the standard word-by-word sentence generation by a decoder RNN with copying mechanism which may instead select words from novel objects at proper places in the output sentence. Extensive experiments are conducted on both MSCOCO image captioning and ImageNet datasets, demonstrating the ability of our proposed LSTM-C architecture to describe novel objects. Furthermore, superior results are reported when compared to state-of-the-art deep models.\n    ",
        "submission_date": "2017-08-17T00:00:00",
        "last_modified_date": "2017-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05565",
        "title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online Auctions",
        "authors": [
            "Yu Wang",
            "Jiayi Liu",
            "Yuxiang Liu",
            "Jun Hao",
            "Yang He",
            "Jinghe Hu",
            "Weipeng P. Yan",
            "Mantian Li"
        ],
        "abstract": "We present LADDER, the first deep reinforcement learning agent that can successfully learn control policies for large-scale real-world problems directly from raw inputs composed of high-level semantic information. The agent is based on an asynchronous stochastic variant of DQN (Deep Q Network) named DASQN. The inputs of the agent are plain-text descriptions of states of a game of incomplete information, i.e. real-time large scale online auctions, and the rewards are auction profits of very large scale. We apply the agent to an essential portion of JD's online RTB (real-time bidding) advertising business and find that it easily beats the former state-of-the-art bidding policy that had been carefully engineered and calibrated by human experts: during ",
        "submission_date": "2017-08-18T00:00:00",
        "last_modified_date": "2017-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.05963",
        "title": "Neural Networks Compression for Language Modeling",
        "authors": [
            "Artem M. Grachev",
            "Dmitry I. Ignatov",
            "Andrey V. Savchenko"
        ],
        "abstract": "In this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks (RNNs). It is known that conventional RNNs, e.g, LSTM-based networks in language modeling, are characterized with either high space complexity or substantial inference time. This problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. By using the Penn Treebank (PTB) dataset we compare pruning, quantization, low-rank factorization, tensor train decomposition for LSTM networks in terms of model size and suitability for fast inference.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06000",
        "title": "Efficient Online Inference for Infinite Evolutionary Cluster models with Applications to Latent Social Event Discovery",
        "authors": [
            "Wei Wei",
            "Kennth Joseph",
            "Kathleen Carley"
        ],
        "abstract": "The Recurrent Chinese Restaurant Process (RCRP) is a powerful statistical method for modeling evolving clusters in large scale social media data. With the RCRP, one can allow both the number of clusters and the cluster parameters in a model to change over time. However, application of the RCRP has largely been limited due to the non-conjugacy between the cluster evolutionary priors and the Multinomial likelihood. This non-conjugacy makes inference di cult and restricts the scalability of models which use the RCRP, leading to the RCRP being applied only in simple problems, such as those that can be approximated by a single Gaussian emission. In this paper, we provide a novel solution for the non-conjugacy issues for the RCRP and an example of how to leverage our solution for one speci c problem - the social event discovery problem. By utilizing Sequential Monte Carlo methods in inference, our approach can be massively paralleled and is highly scalable, to the extent it can work on tens of millions of documents. We are able to generate high quality topical and location distributions of the clusters that can be directly interpreted as real social events, and our experimental results suggest that the approaches proposed achieve much better predictive performance than techniques reported in prior work. We also demonstrate how the techniques we develop can be used in a much more general ways toward similar problems.\n    ",
        "submission_date": "2017-08-20T00:00:00",
        "last_modified_date": "2017-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06266",
        "title": "Probabilistic Relation Induction in Vector Space Embeddings",
        "authors": [
            "Zied Bouraoui",
            "Shoaib Jameel",
            "Steven Schockaert"
        ],
        "abstract": "Word embeddings have been found to capture a surprisingly rich amount of syntactic and semantic knowledge. However, it is not yet sufficiently well-understood how the relational knowledge that is implicitly encoded in word embeddings can be extracted in a reliable way. In this paper, we propose two probabilistic models to address this issue. The first model is based on the common relations-as-translations view, but is cast in a probabilistic setting. Our second model is based on the much weaker assumption that there is a linear relationship between the vector representations of related words. Compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what can and cannot be extracted from the word embedding.\n    ",
        "submission_date": "2017-08-21T00:00:00",
        "last_modified_date": "2017-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.06872",
        "title": "Discovering Political Topics in Facebook Discussion threads with Graph Contextualization",
        "authors": [
            "Yilin Zhang",
            "Marie Poux-Berthe",
            "Chris Wells",
            "Karolina Koc-Michalska",
            "Karl Rohe"
        ],
        "abstract": "We propose a graph contextualization method, pairGraphText, to study political engagement on Facebook during the 2012 French presidential election. It is a spectral algorithm that contextualizes graph data with text data for online discussion thread. In particular, we examine the Facebook posts of the eight leading candidates and the comments beneath these posts. We find evidence of both (i) candidate-centered structure, where citizens primarily comment on the wall of one candidate and (ii) issue-centered structure (i.e. on political topics), where citizens' attention and expression is primarily directed towards a specific set of issues (e.g. economics, immigration, etc). To identify issue-centered structure, we develop pairGraphText, to analyze a network with high-dimensional features on the interactions (i.e. text). This technique scales to hundreds of thousands of nodes and thousands of unique words. In the Facebook data, spectral clustering without the contextualizing text information finds a mixture of (i) candidate and (ii) issue clusters. The contextualized information with text data helps to separate these two structures. We conclude by showing that the novel methodology is consistent under a statistical model.\n    ",
        "submission_date": "2017-08-23T00:00:00",
        "last_modified_date": "2018-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07903",
        "title": "Nationality Classification Using Name Embeddings",
        "authors": [
            "Junting Ye",
            "Shuchu Han",
            "Yifan Hu",
            "Baris Coskun",
            "Meizhu Liu",
            "Hong Qin",
            "Steven Skiena"
        ],
        "abstract": "Nationality identification unlocks important demographic information, with many applications in biomedical and sociological research. Existing name-based nationality classifiers use name substrings as features and are trained on small, unrepresentative sets of labeled names, typically extracted from Wikipedia. As a result, these methods achieve limited performance and cannot support fine-grained classification.\n",
        "submission_date": "2017-08-25T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.07918",
        "title": "Robust Task Clustering for Deep Many-Task Learning",
        "authors": [
            "Mo Yu",
            "Xiaoxiao Guo",
            "Jinfeng Yi",
            "Shiyu Chang",
            "Saloni Potdar",
            "Gerald Tesauro",
            "Haoyu Wang",
            "Bowen Zhou"
        ],
        "abstract": "We investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting. We propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario. Although this matrix provides us critical information regarding similarity between tasks, its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely. Additionally, the uncertain task-pairs, i.e., the ones with extremely asymmetric transfer scores, may collectively mislead clustering algorithms to output an inaccurate task-partition. To overcome these limitations, we propose a novel task-clustering algorithm by using the matrix completion technique. The proposed algorithm constructs a partially-observed similarity matrix based on the certainty of cluster membership of the task-pairs. We then use a matrix completion algorithm to complete the similarity matrix. Our theoretical analysis shows that under mild constraints, the proposed algorithm will perfectly recover the underlying \"true\" similarity matrix with a high probability. Our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi-task learning setup for sentiment classification and dialog intent classification tasks. Our task clustering approach also extends metric-based few-shot learning methods to adapt multiple metrics, which demonstrates empirical advantages when the tasks are diverse.\n    ",
        "submission_date": "2017-08-26T00:00:00",
        "last_modified_date": "2018-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08123",
        "title": "Impact of Feature Selection on Micro-Text Classification",
        "authors": [
            "Ankit Vadehra",
            "Maura R. Grossman",
            "Gordon V. Cormack"
        ],
        "abstract": "Social media datasets, especially Twitter tweets, are popular in the field of text classification. Tweets are a valuable source of micro-text (sometimes referred to as \"micro-blogs\"), and have been studied in domains such as sentiment analysis, recommendation systems, spam detection, clustering, among others. Tweets often include keywords referred to as \"Hashtags\" that can be used as labels for the tweet. Using tweets encompassing 50 labels, we studied the impact of word versus character-level feature selection and extraction on different learners to solve a multi-class classification task. We show that feature extraction of simple character-level groups performs better than simple word groups and pre-processing methods like normalizing using Porter's Stemming and Part-of-Speech (\"POS\")-Lemmatization.\n    ",
        "submission_date": "2017-08-27T00:00:00",
        "last_modified_date": "2017-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08289",
        "title": "Generating Query Suggestions to Support Task-Based Search",
        "authors": [
            "Dar\u00edo Garigliotti",
            "Krisztian Balog"
        ],
        "abstract": "We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the first place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model.\n    ",
        "submission_date": "2017-08-28T00:00:00",
        "last_modified_date": "2017-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.08291",
        "title": "On Type-Aware Entity Retrieval",
        "authors": [
            "Dar\u00edo Garigliotti",
            "Krisztian Balog"
        ],
        "abstract": "Today, the practice of returning entities from a knowledge base in response to search queries has become widespread. One of the distinctive characteristics of entities is that they are typed, i.e., assigned to some hierarchically organized type system (type taxonomy). The primary objective of this paper is to gain a better understanding of how entity type information can be utilized in entity retrieval. We perform this investigation in an idealized \"oracle\" setting, assuming that we know the distribution of target types of the relevant entities for a given query. We perform a thorough analysis of three main aspects: (i) the choice of type taxonomy, (ii) the representation of hierarchical type information, and (iii) the combination of type-based and term-based similarity in the retrieval model. Using a standard entity search test collection based on DBpedia, we find that type information proves most useful when using large type taxonomies that provide very specific types. We provide further insights on the extensional coverage of entities and on the utility of target types.\n    ",
        "submission_date": "2017-08-28T00:00:00",
        "last_modified_date": "2017-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09040",
        "title": "Modelling Protagonist Goals and Desires in First-Person Narrative",
        "authors": [
            "Elahe Rahimtoroghi",
            "Jiaqi Wu",
            "Ruimin Wang",
            "Pranav Anand",
            "Marilyn A Walker"
        ],
        "abstract": "Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on computational models for this problem. We introduce a new dataset, DesireDB, which includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09492",
        "title": "Automatically Generating Commit Messages from Diffs using Neural Machine Translation",
        "authors": [
            "Siyuan Jiang",
            "Ameer Armaly",
            "Collin McMillan"
        ],
        "abstract": "Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically \"translate\" diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.\n    ",
        "submission_date": "2017-08-30T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09516",
        "title": "Leveraging Deep Neural Network Activation Entropy to cope with Unseen Data in Speech Recognition",
        "authors": [
            "Vikramjit Mitra",
            "Horacio Franco"
        ],
        "abstract": "Unseen data conditions can inflict serious performance degradation on systems relying on supervised machine learning algorithms. Because data can often be unseen, and because traditional machine learning algorithms are trained in a supervised manner, unsupervised adaptation techniques must be used to adapt the model to the unseen data conditions. However, unsupervised adaptation is often challenging, as one must generate some hypothesis given a model and then use that hypothesis to bootstrap the model to the unseen data conditions. Unfortunately, reliability of such hypotheses is often poor, given the mismatch between the training and testing datasets. In such cases, a model hypothesis confidence measure enables performing data selection for the model adaptation. Underlying this approach is the fact that for unseen data conditions, data variability is introduced to the model, which the model propagates to its output decision, impacting decision reliability. In a fully connected network, this data variability is propagated as distortions from one layer to the next. This work aims to estimate the propagation of such distortion in the form of network activation entropy, which is measured over a short- time running window on the activation from each neuron of a given hidden layer, and these measurements are then used to compute summary entropy. This work demonstrates that such an entropy measure can help to select data for unsupervised model adaptation, resulting in performance gains in speech recognition tasks. Results from standard benchmark speech recognition tasks show that the proposed approach can alleviate the performance degradation experienced under unseen data conditions by iteratively adapting the model to the unseen datas acoustic condition.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09666",
        "title": "Generating Video Descriptions with Topic Guidance",
        "authors": [
            "Shizhe Chen",
            "Jia Chen",
            "Qin Jin"
        ],
        "abstract": "Generating video descriptions in natural language (a.k.a. video captioning) is a more challenging task than image captioning as the videos are intrinsically more complicated than images in two aspects. First, videos cover a broader range of topics, such as news, music, sports and so on. Second, multiple topics could coexist in the same video. In this paper, we propose a novel caption model, topic-guided model (TGM), to generate topic-oriented descriptions for videos in the wild via exploiting topic information. In addition to predefined topics, i.e., category tags crawled from the web, we also mine topics in a data-driven way based on training captions by an unsupervised topic mining model. We show that data-driven topics reflect a better topic schema than the predefined topics. As for testing video topic prediction, we treat the topic mining model as teacher to train the student, the topic prediction model, by utilizing the full multi-modalities in the video especially the speech modality. We propose a series of caption models to exploit topic guidance, including implicitly using the topics as input features to generate words related to the topic and explicitly modifying the weights in the decoder with topics to function as an ensemble of topic-aware language decoders. Our comprehensive experimental results on the current largest video caption dataset MSR-VTT prove the effectiveness of our topic-guided model, which significantly surpasses the winning performance in the 2016 MSR video to language challenge.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1708.09667",
        "title": "Video Captioning with Guidance of Multimodal Latent Topics",
        "authors": [
            "Shizhe Chen",
            "Jia Chen",
            "Qin Jin",
            "Alexander Hauptmann"
        ],
        "abstract": "The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an unified caption framework, M&M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-defined topics, the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better. We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction, in addition to the caption task. For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos. The topic prediction provides intermediate supervision to the learning process. As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics. The entire learning procedure is end-to-end and it optimizes both tasks simultaneously. The results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the effectiveness of our proposed model. M&M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also achieves better generalization ability.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2023-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00071",
        "title": "Weather impacts expressed sentiment",
        "authors": [
            "Patrick Baylis",
            "Nick Obradovich",
            "Yury Kryvasheyeu",
            "Haohui Chen",
            "Lorenzo Coviello",
            "Esteban Moro",
            "Manuel Cebrian",
            "James H. Fowler"
        ],
        "abstract": "We conduct the largest ever investigation into the relationship between meteorological conditions and the sentiment of human expressions. To do this, we employ over three and a half billion social media posts from tens of millions of individuals from both Facebook and Twitter between 2009 and 2016. We find that cold temperatures, hot temperatures, precipitation, narrower daily temperature ranges, humidity, and cloud cover are all associated with worsened expressions of sentiment, even when excluding weather-related posts. We compare the magnitude of our estimates with the effect sizes associated with notable historical events occurring within our data.\n    ",
        "submission_date": "2017-08-31T00:00:00",
        "last_modified_date": "2017-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00149",
        "title": "Learning what to read: Focused machine reading",
        "authors": [
            "Enrique Noriega-Atala",
            "Marco A. Valenzuela-Escarcega",
            "Clayton T. Morrison",
            "Mihai Surdeanu"
        ],
        "abstract": "Recent efforts in bioinformatics have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at today's scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and processing overhead. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of algorithms for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the baseline, while being more efficient, i.e., reading fewer documents.\n    ",
        "submission_date": "2017-09-01T00:00:00",
        "last_modified_date": "2017-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00661",
        "title": "Topic Independent Identification of Agreement and Disagreement in Social Media Dialogue",
        "authors": [
            "Amita Misra",
            "Marilyn Walker"
        ],
        "abstract": "Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community's ability to develop better theories, as well as good off the shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%.\n    ",
        "submission_date": "2017-09-03T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00662",
        "title": "Using Summarization to Discover Argument Facets in Online Ideological Dialog",
        "authors": [
            "Amita Misra",
            "Pranav Anand",
            "Jean E Fox Tree",
            "Marilyn Walker"
        ],
        "abstract": "More and more of the information available on the web is dialogic, and a significant portion of it takes place in online forum conversations about current social and political topics. We aim to develop tools to summarize what these conversations are about. What are the CENTRAL PROPOSITIONS associated with different stances on an issue, what are the abstract objects under discussion that are central to a speaker's argument? How can we recognize that two CENTRAL PROPOSITIONS realize the same FACET of the argument? We hypothesize that the CENTRAL PROPOSITIONS are exactly those arguments that people find most salient, and use human summarization as a probe for discovering them. We describe our corpus of human summaries of opinionated dialogs, then show how we can identify similar repeated arguments, and group them into FACETS across many discussions of a topic. We define a new task, ARGUMENT FACET SIMILARITY (AFS), and show that we can predict AFS with a .54 correlation score, versus an ngram system baseline of .39 and a semantic textual similarity system baseline of .45.\n    ",
        "submission_date": "2017-09-03T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00728",
        "title": "Formalising Type-Logical Grammars in Agda",
        "authors": [
            "Wen Kokke"
        ],
        "abstract": "In recent years, the interest in using proof assistants to formalise and reason about mathematics and programming languages has grown. Type-logical grammars, being closely related to type theories and systems used in functional programming, are a perfect candidate to next apply this curiosity to. The advantages of using proof assistants is that they allow one to write formally verified proofs about one's type-logical systems, and that any theory, once implemented, can immediately be computed with. The downside is that in many cases the formal proofs are written as an afterthought, are incomplete, or use obtuse syntax. This makes it that the verified proofs are often much more difficult to read than the pen-and-paper proofs, and almost never directly published. In this paper, we will try to remedy that by example.\n",
        "submission_date": "2017-09-03T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00893",
        "title": "Interactive Attention Networks for Aspect-Level Sentiment Classification",
        "authors": [
            "Dehong Ma",
            "Sujian Li",
            "Xiaodong Zhang",
            "Houfeng Wang"
        ],
        "abstract": "Aspect-level sentiment classification aims at identifying the sentiment polarity of specific target in its context. Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target-specific representations. However, these studies always ignore the separate modeling of targets. In this paper, we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning. Then, we propose the interactive attention networks (IAN) to interactively learn attentions in the contexts and targets, and generate the representations for targets and contexts separately. With this design, the IAN model can well represent a target and its collocative context, which is helpful to sentiment classification. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our model.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.00917",
        "title": "Using Optimal Ratio Mask as Training Target for Supervised Speech Separation",
        "authors": [
            "Shasha Xia",
            "Hao Li",
            "Xueliang Zhang"
        ],
        "abstract": "Supervised speech separation uses supervised learning algorithms to learn a mapping from an input noisy signal to an output target. With the fast development of deep learning, supervised separation has become the most important direction in speech separation area in recent years. For the supervised algorithm, training target has a significant impact on the performance. Ideal ratio mask is a commonly used training target, which can improve the speech intelligibility and quality of the separated speech. However, it does not take into account the correlation between noise and clean speech. In this paper, we use the optimal ratio mask as the training target of the deep neural network (DNN) for speech separation. The experiments are carried out under various noise environments and signal to noise ratio (SNR) conditions. The results show that the optimal ratio mask outperforms other training targets in general.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01144",
        "title": "Information Theoretic Analysis of DNN-HMM Acoustic Modeling",
        "authors": [
            "Pranay Dighe",
            "Afsaneh Asaei",
            "Herv\u00e9 Bourlard"
        ],
        "abstract": "We propose an information theoretic framework for quantitative assessment of acoustic modeling for hidden Markov model (HMM) based automatic speech recognition (ASR). Acoustic modeling yields the probabilities of HMM sub-word states for a short temporal window of speech acoustic features. We cast ASR as a communication channel where the input sub-word probabilities convey the information about the output HMM state sequence. The quality of the acoustic model is thus quantified in terms of the information transmitted through this channel. The process of inferring the most likely HMM state sequence from the sub-word probabilities is known as decoding. HMM based decoding assumes that an acoustic model yields accurate state-level probabilities and the data distribution given the underlying hidden state is independent of any other state in the sequence. We quantify 1) the acoustic model accuracy and 2) its robustness to mismatch between data and the HMM conditional independence assumption in terms of some mutual information quantities. In this context, exploiting deep neural network (DNN) posterior probabilities leads to a simple and straightforward analysis framework to assess shortcomings of the acoustic model for HMM based decoding. This analysis enables us to evaluate the Gaussian mixture acoustic model (GMM) and the importance of many hidden layers in DNNs without any need of explicit speech recognition. In addition, it sheds light on the contribution of low-dimensional models to enhance acoustic modeling for better compliance with the HMM based decoding requirements.\n    ",
        "submission_date": "2017-08-29T00:00:00",
        "last_modified_date": "2017-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01188",
        "title": "Storytelling Agents with Personality and Adaptivity",
        "authors": [
            "Zhichao Hu",
            "Marilyn A. Walker",
            "Michael Neff",
            "Jean E. Fox Tree"
        ],
        "abstract": "We explore the expression of personality and adaptivity through the gestures of virtual agents in a storytelling task. We conduct two experiments using four different dialogic stories. We manipulate agent personality on the extraversion scale, whether the agents adapt to one another in their gestural performance and agent gender. Our results show that subjects are able to perceive the intended variation in extraversion between different virtual agents, independently of the story they are telling and the gender of the agent. A second study shows that subjects also prefer adaptive to nonadaptive virtual agents.\n    ",
        "submission_date": "2017-09-04T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01256",
        "title": "Semantic Document Distance Measures and Unsupervised Document Revision Detection",
        "authors": [
            "Xiaofeng Zhu",
            "Diego Klabjan",
            "Patrick Bless"
        ],
        "abstract": "In this paper, we model the document revision detection problem as a minimum cost branching problem that relies on computing document distances. Furthermore, we propose two new document distance measures, word vector-based Dynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED). Our revision detection system is designed for a large scale corpus and implemented in Apache Spark. We demonstrate that our system can more precisely detect revisions than state-of-the-art methods by utilizing the Wikipedia revision dumps ",
        "submission_date": "2017-09-05T00:00:00",
        "last_modified_date": "2017-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01687",
        "title": "Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction Mention Extraction",
        "authors": [
            "Shashank Gupta",
            "Sachin Pawar",
            "Nitin Ramrakhiyani",
            "Girish Palshikar",
            "Vasudeva Varma"
        ],
        "abstract": "Social media is an useful platform to share health-related information due to its vast reach. This makes it a good candidate for public-health monitoring tasks, specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from twitter. Medical information extraction from social media is challenging, mainly due to short and highly information nature of text, as compared to more technical and formal medical reports.\n",
        "submission_date": "2017-09-06T00:00:00",
        "last_modified_date": "2017-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.01991",
        "title": "Semi-Automatic Terminology Ontology Learning Based on Topic Modeling",
        "authors": [
            "Monika Rani",
            "Amit Kumar Dhar",
            "O. P. Vyas"
        ],
        "abstract": "Ontologies provide features like a common vocabulary, reusability, machine-readable content, and also allows for semantic search, facilitate agent interaction and ordering & structuring of knowledge for the Semantic Web (Web 3.0) application. However, the challenge in ontology engineering is automatic learning, i.e., the there is still a lack of fully automatic approach from a text corpus or dataset of various topics to form ontology using machine learning techniques. In this paper, two topic modeling algorithms are explored, namely LSI & SVD and ",
        "submission_date": "2017-08-05T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02076",
        "title": "Composition by Conversation",
        "authors": [
            "Donya Quick",
            "Clayton T. Morrison"
        ],
        "abstract": "Most musical programming languages are developed purely for coding virtual instruments or algorithmic compositions. Although there has been some work in the domain of musical query languages for music information retrieval, there has been little attempt to unify the principles of musical programming and query languages with cognitive and natural language processing models that would facilitate the activity of composition by conversation. We present a prototype framework, called MusECI, that merges these domains, permitting score-level algorithmic composition in a text editor while also supporting connectivity to existing natural language processing frameworks.\n    ",
        "submission_date": "2017-09-07T00:00:00",
        "last_modified_date": "2017-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02968",
        "title": "Matrix and Graph Operations for Relationship Inference: An Illustration with the Kinship Inference in the China Biographical Database",
        "authors": [
            "Chao-Lin Liu",
            "Hongsu Wang"
        ],
        "abstract": "Biographical databases contain diverse information about individuals. Person names, birth information, career, friends, family and special achievements are some possible items in the record for an individual. The relationships between individuals, such as kinship and friendship, provide invaluable insights about hidden communities which are not directly recorded in databases. We show that some simple matrix and graph-based operations are effective for inferring relationships among individuals, and illustrate the main ideas with the China Biographical Database (CBDB).\n    ",
        "submission_date": "2017-09-09T00:00:00",
        "last_modified_date": "2017-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.02984",
        "title": "Sentiment Polarity Detection for Software Development",
        "authors": [
            "Fabio Calefato",
            "Filippo Lanubile",
            "Federico Maiorano",
            "Nicole Novielli"
        ],
        "abstract": "The role of sentiment analysis is increasingly emerging to study software developers' emotions by mining crowd-generated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers' communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexicon- and keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines.\n    ",
        "submission_date": "2017-09-09T00:00:00",
        "last_modified_date": "2017-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03406",
        "title": "Social Media Text Processing and Semantic Analysis for Smart Cities",
        "authors": [
            "Jo\u00e3o Filipe Figueiredo Pereira"
        ],
        "abstract": "With the rise of Social Media, people obtain and share information almost instantly on a 24/7 basis. Many research areas have tried to gain valuable insights from these large volumes of freely available user generated content.\n",
        "submission_date": "2017-09-11T00:00:00",
        "last_modified_date": "2017-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.03742",
        "title": "Dependencies: Formalising Semantic Catenae for Information Retrieval",
        "authors": [
            "Christina Lioma"
        ],
        "abstract": "Building machines that can understand text like humans is an AI-complete problem. A great deal of research has already gone into this, with astounding results, allowing everyday people to discuss with their telephones, or have their reading materials analysed and classified by computers. A prerequisite for processing text semantics, common to the above examples, is having some computational representation of text as an abstract object. Operations on this representation practically correspond to making semantic inferences, and by extension simulating understanding text. The complexity and granularity of semantic processing that can be realised is constrained by the mathematical and computational robustness, expressiveness, and rigour of the tools used.\n",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04071",
        "title": "Variational Reasoning for Question Answering with Knowledge Graph",
        "authors": [
            "Yuyu Zhang",
            "Hanjun Dai",
            "Zornitsa Kozareva",
            "Alexander J. Smola",
            "Le Song"
        ],
        "abstract": "Knowledge graph (KG) is known to be helpful for the task of question answering (QA), since it provides well-structured relational information between entities, and allows one to further infer indirect facts. However, it is challenging to build QA systems which can learn to reason over knowledge graphs based on question-answer pairs alone. First, when people ask questions, their expressions are noisy (for example, typos in texts, or variations in pronunciations), which is non-trivial for the QA system to match those mentioned entities to the knowledge graph. Second, many questions require multi-hop logic reasoning over the knowledge graph to retrieve the answers. To address these challenges, we propose a novel and unified deep learning architecture, and an end-to-end variational learning algorithm which can handle noise in questions, and learn multi-hop reasoning simultaneously. Our method achieves state-of-the-art performance on a recent benchmark dataset in the literature. We also derive a series of new benchmark datasets, including questions for multi-hop reasoning, questions paraphrased by neural translation model, and questions in human voice. Our method yields very promising results on all these challenging datasets.\n    ",
        "submission_date": "2017-09-12T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04380",
        "title": "Neural Network Based Nonlinear Weighted Finite Automata",
        "authors": [
            "Tianyu Li",
            "Guillaume Rabusseau",
            "Doina Precup"
        ],
        "abstract": "Weighted finite automata (WFA) can expressively model functions defined over strings but are inherently linear models. Given the recent successes of nonlinear models in machine learning, it is natural to wonder whether ex-tending WFA to the nonlinear setting would be beneficial. In this paper, we propose a novel model of neural network based nonlinearWFA model (NL-WFA) along with a learning algorithm. Our learning algorithm is inspired by the spectral learning algorithm for WFAand relies on a nonlinear decomposition of the so-called Hankel matrix, by means of an auto-encoder network. The expressive power of NL-WFA and the proposed learning algorithm are assessed on both synthetic and real-world data, showing that NL-WFA can lead to smaller model sizes and infer complex grammatical structures from data.\n    ",
        "submission_date": "2017-09-13T00:00:00",
        "last_modified_date": "2017-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04625",
        "title": "Robustness Analysis of Visual QA Models by Basic Questions",
        "authors": [
            "Jia-Hong Huang",
            "Cuong Duc Dao",
            "Modar Alfadly",
            "C. Huck Yang",
            "Bernard Ghanem"
        ],
        "abstract": "Visual Question Answering (VQA) models should have both high robustness and accuracy. Unfortunately, most of the current VQA research only focuses on accuracy because there is a lack of proper methods to measure the robustness of VQA models. There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the ranked basic questions, with similarity scores, of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question about the given image. We claim that a robust VQA model is one, whose performance is not changed much when related basic questions as also made available to it as input. We formulate the basic questions generation problem as a LASSO optimization, and also propose a large scale Basic Question Dataset (BQD) and Rscore (novel robustness measure), for analyzing the robustness of VQA models. We hope our BQD will be used as a benchmark for to evaluate the robustness of VQA models, so as to help the community build more robust and accurate VQA models.\n    ",
        "submission_date": "2017-09-14T00:00:00",
        "last_modified_date": "2018-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.04710",
        "title": "Embedded-Graph Theory",
        "authors": [
            "Atsushi Yokoyama"
        ],
        "abstract": "In this paper, we propose a new type of graph, denoted as \"embedded-graph\", and its theory, which employs a distributed representation to describe the relations on the graph edges. Embedded-graphs can express linguistic and complicated relations, which cannot be expressed by the existing edge-graphs or weighted-graphs. We introduce the mathematical definition of embedded-graph, translation, edge distance, and graph similarity. We can transform an embedded-graph into a weighted-graph and a weighted-graph into an edge-graph by the translation method and by threshold calculation, respectively. The edge distance of an embedded-graph is a distance based on the components of a target vector, and it is calculated through cosine similarity with the target vector. The graph similarity is obtained considering the relations with linguistic complexity. In addition, we provide some examples and data structures for embedded-graphs in this paper.\n    ",
        "submission_date": "2017-09-14T00:00:00",
        "last_modified_date": "2017-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05027",
        "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
        "authors": [
            "Wei Wen",
            "Yuxiong He",
            "Samyam Rajbhandari",
            "Minjia Zhang",
            "Wenhan Wang",
            "Fang Liu",
            "Bin Hu",
            "Yiran Chen",
            "Hai Li"
        ],
        "abstract": "Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is publicly available at ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2018-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05036",
        "title": "Query-based Attention CNN for Text Similarity Map",
        "authors": [
            "Tzu-Chien Liu",
            "Yu-Hsueh Wu",
            "Hung-Yi Lee"
        ],
        "abstract": "In this paper, we introduce Query-based Attention CNN(QACNN) for Text Similarity Map, an end-to-end neural network for question answering. This network is composed of compare mechanism, two-staged CNN architecture with attention mechanism, and a prediction layer. First, the compare mechanism compares between the given passage, query, and multiple answer choices to build similarity maps. Then, the two-staged CNN architecture extracts features through word-level and sentence-level. At the same time, attention mechanism helps CNN focus more on the important part of the passage based on the query information. Finally, the prediction layer find out the most possible answer choice. We conduct this model on the MovieQA dataset using Plot Synopses only, and achieve 79.99% accuracy which is the state of the art on the dataset.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05038",
        "title": "Self-Guiding Multimodal LSTM - when we do not have a perfect training dataset for image captioning",
        "authors": [
            "Yang Xian",
            "Yingli Tian"
        ],
        "abstract": "In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning model is proposed to handle uncontrolled imbalanced real-world image-sentence dataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165 images and the original text descriptions uploaded by the users are utilized as the ground truth for training. Descriptions in FlickrNYC dataset vary dramatically ranging from short term-descriptions to long paragraph-descriptions and can describe any visual aspects, or even refer to objects that are not depicted. To deal with the imbalanced and noisy situation and to fully explore the dataset itself, we propose a novel guiding textual feature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of m-LSTM is based on the portion of data in which the image content and the corresponding descriptions are strongly bonded. Afterwards, during the training of sg-LSTM on the rest training data, this guiding information serves as additional input to the network along with the image representations and the ground-truth descriptions. By integrating these input components into a multimodal block, we aim to form a training scheme with the textual information tightly coupled with the image content. The experimental results demonstrate that the proposed sg-LSTM model outperforms the traditional state-of-the-art multimodal RNN captioning framework in successfully describing the key components of the input images.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05278",
        "title": "Algorithms and Architecture for Real-time Recommendations at News UK",
        "authors": [
            "Dion Bailey",
            "Tom Pajak",
            "Daoud Clarke",
            "Carlos Rodriguez"
        ],
        "abstract": "Recommendation systems are recognised as being hugely important in industry, and the area is now well understood. At News UK, there is a requirement to be able to quickly generate recommendations for users on news items as they are published. However, little has been published about systems that can generate recommendations in response to changes in recommendable items and user behaviour in a very short space of time. In this paper we describe a new algorithm for updating collaborative filtering models incrementally, and demonstrate its effectiveness on clickstream data from The Times. We also describe the architecture that allows recommendations to be generated on the fly, and how we have made each component scalable. The system is currently being used in production at News UK.\n    ",
        "submission_date": "2017-09-15T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05453",
        "title": "Augmenting End-to-End Dialog Systems with Commonsense Knowledge",
        "authors": [
            "Tom Young",
            "Erik Cambria",
            "Iti Chaturvedi",
            "Minlie Huang",
            "Hao Zhou",
            "Subham Biswas"
        ],
        "abstract": "Building dialog agents that can converse naturally with humans is a challenging yet intriguing problem of artificial intelligence. In open-domain human-computer conversation, where the conversational agent is expected to respond to human responses in an interesting and engaging way, commonsense knowledge has to be integrated into the model effectively. In this paper, we investigate the impact of providing commonsense knowledge about the concepts covered in the dialog. Our model represents the first attempt to integrating a large commonsense knowledge base into end-to-end conversational models. In the retrieval-based scenario, we propose the Tri-LSTM model to jointly take into account message and commonsense for selecting an appropriate response. Our experiments suggest that the knowledge-augmented models are superior to their knowledge-free counterparts in automatic evaluation.\n    ",
        "submission_date": "2017-09-16T00:00:00",
        "last_modified_date": "2018-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05576",
        "title": "SKOS Concepts and Natural Language Concepts: an Analysis of Latent Relationships in KOSs",
        "authors": [
            "Anna Mastora",
            "Manolis Peponakis",
            "Sarantos Kapidakis"
        ],
        "abstract": "The vehicle to represent Knowledge Organization Systems (KOSs) in the environment of the Semantic Web and linked data is the Simple Knowledge Organization System (SKOS). SKOS provides a way to assign a URI to each concept, and this URI functions as a surrogate for the concept. This fact makes of main concern the need to clarify the URIs' ontological meaning. The aim of this study is to investigate the relation between the ontological substance of KOS concepts and concepts revealed through the grammatical and syntactic formalisms of natural language. For this purpose, we examined the dividableness of concepts in specific KOSs (i.e. a thesaurus, a subject headings system and a classification scheme) by applying Natural Language Processing (NLP) techniques (i.e. morphosyntactic analysis) to the lexical representations (i.e. RDF literals) of SKOS concepts. The results of the comparative analysis reveal that, despite the use of multi-word units, thesauri tend to represent concepts in a way that can hardly be further divided conceptually, while Subject Headings and Classification Schemes - to a certain extent - comprise terms that can be decomposed into more conceptual constituents. Consequently, SKOS concepts deriving from thesauri are more likely to represent atomic conceptual units and thus be more appropriate tools for inference and reasoning. Since identifiers represent the meaning of a concept, complex concepts are neither the most appropriate nor the most efficient way of modelling a KOS for the Semantic Web.\n    ",
        "submission_date": "2017-09-16T00:00:00",
        "last_modified_date": "2017-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05700",
        "title": "Morphology-based Entity and Relational Entity Extraction Framework for Arabic",
        "authors": [
            "Amin Jaber",
            "Fadi A. Zaraket"
        ],
        "abstract": "Rule-based techniques to extract relational entities from documents allow users to specify desired entities with natural language questions, finite state automata, regular expressions and structured query language. They require linguistic and programming expertise and lack support for Arabic morphological analysis. We present a morphology-based entity and relational entity extraction framework for Arabic (MERF). MERF requires basic knowledge of linguistic features and regular expressions, and provides the ability to interactively specify Arabic morphological and synonymity features, tag types associated with regular expressions, and relations and code actions defined over matches of subexpressions. MERF constructs entities and relational entities from matches of the specifications. We evaluated MERF with several case studies. The results show that MERF requires shorter development time and effort compared to existing application specific techniques and produces reasonably accurate results within a reasonable overhead in run time.\n    ",
        "submission_date": "2017-09-17T00:00:00",
        "last_modified_date": "2018-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.05743",
        "title": "Towards Building a Knowledge Base of Monetary Transactions from a News Collection",
        "authors": [
            "Jan R. Benetka",
            "Krisztian Balog",
            "Kjetil N\u00f8rv\u00e5g"
        ],
        "abstract": "We address the problem of extracting structured representations of economic events from a large corpus of news articles, using a combination of natural language processing and machine learning techniques. The developed techniques allow for semi-automatic population of a financial knowledge base, which, in turn, may be used to support a range of data mining and exploration tasks. The key challenge we face in this domain is that the same event is often reported multiple times, with varying correctness of details. We address this challenge by first collecting all information pertinent to a given event from the entire corpus, then considering all possible representations of the event, and finally, using a supervised learning method, to rank these representations by the associated confidence scores. A main innovative element of our approach is that it jointly extracts and stores all attributes of the event as a single representation (quintuple). Using a purpose-built test set we demonstrate that our supervised learning approach can achieve 25% improvement in F1-score over baseline methods that consider the earliest, the latest or the most frequent reporting of the event.\n    ",
        "submission_date": "2017-09-18T00:00:00",
        "last_modified_date": "2017-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06907",
        "title": "Doctoral Advisor or Medical Condition: Towards Entity-specific Rankings of Knowledge Base Properties [Extended Version]",
        "authors": [
            "Simon Razniewski",
            "Vevake Balaraman",
            "Werner Nutt"
        ],
        "abstract": "In knowledge bases such as Wikidata, it is possible to assert a large set of properties for entities, ranging from generic ones such as name and place of birth to highly profession-specific or background-specific ones such as doctoral advisor or medical condition. Determining a preference or ranking in this large set is a challenge in tasks such as prioritisation of edits or natural-language generation. Most previous approaches to ranking knowledge base properties are purely data-driven, that is, as we show, mistake frequency for interestingness.\n",
        "submission_date": "2017-09-20T00:00:00",
        "last_modified_date": "2017-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.06990",
        "title": "Text Compression for Sentiment Analysis via Evolutionary Algorithms",
        "authors": [
            "Emmanuel Dufourq",
            "Bruce A. Bassett"
        ],
        "abstract": "Can textual data be compressed intelligently without losing accuracy in evaluating sentiment? In this study, we propose a novel evolutionary compression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression), which makes use of Parts-of-Speech tags to compress text in a way that sacrifices minimal classification accuracy when used in conjunction with sentiment analysis algorithms. An analysis of PARSEC with eight commercial and non-commercial sentiment analysis algorithms on twelve English sentiment data sets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss in sentiment classification accuracy for (20%, 50%, 75%) data compression with PARSEC using LingPipe, the most accurate of the sentiment algorithms. Other sentiment analysis algorithms are more severely affected by compression. We conclude that significant compression of text data is possible for sentiment analysis depending on the accuracy demands of the specific application and the specific sentiment analysis algorithm used.\n    ",
        "submission_date": "2017-09-20T00:00:00",
        "last_modified_date": "2017-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07432",
        "title": "Dynamic Evaluation of Neural Sequence Models",
        "authors": [
            "Ben Krause",
            "Emmanuel Kahembwe",
            "Iain Murray",
            "Steve Renals"
        ],
        "abstract": "We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.\n    ",
        "submission_date": "2017-09-21T00:00:00",
        "last_modified_date": "2017-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07642",
        "title": "Code Attention: Translating Code to Comments by Exploiting Domain Features",
        "authors": [
            "Wenhao Zheng",
            "Hong-Yu Zhou",
            "Ming Li",
            "Jianxin Wu"
        ],
        "abstract": "Appropriate comments of code snippets provide insight for code functionality, which are helpful for program comprehension. However, due to the great cost of authoring with the comments, many code projects do not contain adequate comments. Automatic comment generation techniques have been proposed to generate comments from pieces of code in order to alleviate the human efforts in annotating the code. Most existing approaches attempt to exploit certain correlations (usually manually given) between code and generated comments, which could be easily violated if the coding patterns change and hence the performance of comment generation declines. In this paper, we first build C2CGit, a large dataset from open projects in GitHub, which is more than 20$\\times$ larger than existing datasets. Then we propose a new attention module called Code Attention to translate code to comments, which is able to utilize the domain features of code snippets, such as symbols and identifiers. We make ablation studies to determine effects of different parts in Code Attention. Experimental results demonstrate that the proposed module has better performance over existing approaches in both BLEU and METEOR.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07871",
        "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
        "authors": [
            "Ethan Perez",
            "Florian Strub",
            "Harm de Vries",
            "Vincent Dumoulin",
            "Aaron Courville"
        ],
        "abstract": "We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07902",
        "title": "Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data",
        "authors": [
            "Wei-Ning Hsu",
            "Yu Zhang",
            "James Glass"
        ],
        "abstract": "We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07915",
        "title": "Computational Content Analysis of Negative Tweets for Obesity, Diet, Diabetes, and Exercise",
        "authors": [
            "George Shaw Jr.",
            "Amir Karami"
        ],
        "abstract": "Social media based digital epidemiology has the potential to support faster response and deeper understanding of public health related threats. This study proposes a new framework to analyze unstructured health related textual data via Twitter users' post (tweets) to characterize the negative health sentiments and non-health related concerns in relations to the corpus of negative sentiments, regarding Diet Diabetes Exercise, and Obesity (DDEO). Through the collection of 6 million Tweets for one month, this study identified the prominent topics of users as it relates to the negative sentiments. Our proposed framework uses two text mining methods, sentiment analysis and topic modeling, to discover negative topics. The negative sentiments of Twitter users support the literature narratives and the many morbidity issues that are associated with DDEO and the linkage between obesity and diabetes. The framework offers a potential method to understand the publics' opinions and sentiments regarding DDEO. More importantly, this research provides new opportunities for computational social scientists, medical experts, and public health professionals to collectively address DDEO-related issues.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.07916",
        "title": "Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter",
        "authors": [
            "Amir Karami",
            "Alicia A. Dahl",
            "Gabrielle Turner-McGrievy",
            "Hadi Kharrazi Jr.",
            "George Shaw"
        ],
        "abstract": "Social media provide a platform for users to express their opinions and share information. Understanding public health opinions on social media, such as Twitter, offers a unique approach to characterizing common health issues such as diabetes, diet, exercise, and obesity (DDEO), however, collecting and analyzing a large scale conversational public health data set is a challenging research task. The goal of this research is to analyze the characteristics of the general public's opinions in regard to diabetes, diet, exercise and obesity (DDEO) as expressed on Twitter. A multi-component semantic and linguistic framework was developed to collect Twitter data, discover topics of interest about DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8% of tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity. The strongest correlation among the topics was determined between exercise and obesity. Other notable correlations were: diabetes and obesity, and diet and obesity DDEO terms were also identified as subtopics of each of the DDEO topics. The frequent subtopics discussed along with Diabetes, excluding the DDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer. The non-DDEO subtopics for Diet included vegetarian, pregnancy, celebrities, weight loss, religious, and mental health, while subtopics for Exercise included computer games, brain, fitness, and daily plan. Non-DDEO subtopics for Obesity included Alzheimer, cancer, and children. With 2.67 billion social media users in 2016, publicly available data such as Twitter posts can be utilized to support clinical providers, public health experts, and social scientists in better understanding common public opinions in regard to diabetes, diet, exercise, and obesity.\n    ",
        "submission_date": "2017-09-22T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08267",
        "title": "HDLTex: Hierarchical Deep Learning for Text Classification",
        "authors": [
            "Kamran Kowsari",
            "Donald E. Brown",
            "Mojtaba Heidarysafa",
            "Kiana Jafari Meimandi",
            "Matthew S. Gerber",
            "Laura E. Barnes"
        ],
        "abstract": "The continually increasing number of documents produced each year necessitates ever improving information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of these traditional classifiers has degraded as the number of documents has increased. This is because along with this growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.\n    ",
        "submission_date": "2017-09-24T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08366",
        "title": "\"Let me convince you to buy my product ... \": A Case Study of an Automated Persuasive System for Fashion Products",
        "authors": [
            "Vitobha Munigala",
            "Srikanth Tamilselvam",
            "Anush Sankaran"
        ],
        "abstract": "Persuasivenes is a creative art aimed at making people believe in certain set of beliefs. Many a times, such creativity is about adapting richness of one domain into another to strike a chord with the target audience. In this research, we present PersuAIDE! - A persuasive system based on linguistic creativity to transform given sentence to generate various forms of persuading sentences. These various forms cover multiple focus of persuasion such as memorability and sentiment. For a given simple product line, the algorithm is composed of several steps including: (i) select an appropriate well-known expression for the target domain to add memorability, (ii) identify keywords and entities in the given sentence and expression and transform it to produce creative persuading sentence, and (iii) adding positive or negative sentiment for further persuasion. The persuasive conversion were manually verified using qualitative results and the effectiveness of the proposed approach is empirically discussed.\n    ",
        "submission_date": "2017-09-25T00:00:00",
        "last_modified_date": "2017-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08448",
        "title": "Extracting Ontological Knowledge from Textual Descriptions",
        "authors": [
            "Kevin Alex Mathews",
            "P Sreenivasa Kumar"
        ],
        "abstract": "Authoring of OWL-DL ontologies is intellectually challenging and to make this process simpler, many systems accept natural language text as input. A text-based ontology authoring approach can be successful only when it is combined with an effective method for extracting ontological axioms from text. Extracting axioms from unrestricted English input is a substantially challenging task due to the richness of the language. Controlled natural languages (CNLs) have been proposed in this context and these tend to be highly restrictive. In this paper, we propose a new CNL called TEDEI (TExtual DEscription Identifier) whose grammar is inspired by the different ways OWL-DL constructs are expressed in English. We built a system that transforms TEDEI sentences into corresponding OWL-DL axioms. Now, ambiguity due to different possible lexicalizations of sentences and semantic ambiguity present in sentences are challenges in this context. We find that the best way to handle these challenges is to construct axioms corresponding to alternative formalizations of the sentence so that the end-user can make an appropriate choice. The output is compared against human-authored axioms and in substantial number of cases, human-authored axiom is indeed one of the alternatives given by the system. The proposed system substantially enhances the types of sentence structures that can be used for ontology authoring.\n    ",
        "submission_date": "2017-09-25T00:00:00",
        "last_modified_date": "2017-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08853",
        "title": "Object-oriented Neural Programming (OONP) for Document Understanding",
        "authors": [
            "Zhengdong Lu",
            "Xianggen Liu",
            "Haotian Cui",
            "Yukun Yan",
            "Daqi Zheng"
        ],
        "abstract": "We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure (referred to as ontology in this paper) that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and during the process it builds and updates an intermediate ontology to summarize its partial understanding of the text it covers. OONP supports a rich family of operations (both symbolic and differentiable) for composing the ontology, and a big variety of forms (both symbolic and differentiable) for representing the state and the document. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL) , reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2018-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.08858",
        "title": "Polysemy Detection in Distributed Representation of Word Sense",
        "authors": [
            "Kana Oomoto",
            "Haruka Oikawa",
            "Eiko Yamamoto",
            "Mitsuo Yoshida",
            "Masayuki Okabe",
            "Kyoji Umemura"
        ],
        "abstract": "In this paper, we propose a statistical test to determine whether a given word is used as a polysemic word or not. The statistic of the word in this test roughly corresponds to the fluctuation in the senses of the neighboring words a nd the word itself. Even though the sense of a word corresponds to a single vector, we discuss how polysemy of the words affects the position of vectors. Finally, we also explain the method to detect this effect.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2017-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09118",
        "title": "Tensor Product Generation Networks for Deep NLP Modeling",
        "authors": [
            "Qiuyuan Huang",
            "Paul Smolensky",
            "Xiaodong He",
            "Li Deng",
            "Dapeng Wu"
        ],
        "abstract": "We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture --- the Tensor Product Generation Network (TPGN) --- is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.\n    ",
        "submission_date": "2017-09-26T00:00:00",
        "last_modified_date": "2017-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09345",
        "title": "A Read-Write Memory Network for Movie Story Understanding",
        "authors": [
            "Seil Na",
            "Sangho Lee",
            "Jisung Kim",
            "Gunhee Kim"
        ],
        "abstract": "We propose a novel memory network model named Read-Write Memory Network (RWMN) to perform question and answering tasks for large-scale, multimodal movie story understanding. The key focus of our RWMN model is to design the read network and the write network that consist of multiple convolutional layers, which enable memory read and write operations to have high capacity and flexibility. While existing memory-augmented network models treat each memory slot as an independent block, our use of multi-layered CNNs allows the model to read and write sequential memory cells as chunks, which is more reasonable to represent a sequential story because adjacent memory blocks often have strong correlations. For evaluation, we apply our model to all the six tasks of the MovieQA benchmark, and achieve the best accuracies on several tasks, especially on the visual QA task. Our model shows a potential to better understand not only the content in the story, but also more abstract information, such as relationships between characters and the reasons for their actions.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2018-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09741",
        "title": "WHY: Natural Explanations from a Robot Navigator",
        "authors": [
            "Raj Korpan",
            "Susan L. Epstein",
            "Anoop Aroor",
            "Gil Dekel"
        ],
        "abstract": "Effective collaboration between a robot and a person requires natural communication. When a robot travels with a human companion, the robot should be able to explain its navigation behavior in natural language. This paper explains how a cognitively-based, autonomous robot navigation system produces informative, intuitive explanations for its decisions. Language generation here is based upon the robot's commonsense, its qualitative reasoning, and its learned spatial model. This approach produces natural explanations in real time for a robot as it navigates in a large, complex indoor environment.\n    ",
        "submission_date": "2017-09-27T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1709.09927",
        "title": "Inference of Personal Attributes from Tweets Using Machine Learning",
        "authors": [
            "Take Yo",
            "Kazutoshi Sasahara"
        ],
        "abstract": "Using machine learning algorithms, including deep learning, we studied the prediction of personal attributes from the text of tweets, such as gender, occupation, and age groups. We applied word2vec to construct word vectors, which were then used to vectorize tweet blocks. The resulting tweet vectors were used as inputs for training models, and the prediction accuracy of those models was examined as a function of the dimension of the tweet vectors and the size of the tweet blacks. The results showed that the machine learning algorithms could predict the three personal attributes of interest with 60-70% accuracy.\n    ",
        "submission_date": "2017-09-28T00:00:00",
        "last_modified_date": "2017-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00284",
        "title": "Efficient and Effective Single-Document Summarizations and A Word-Embedding Measurement of Quality",
        "authors": [
            "Liqun Shao",
            "Hao Zhang",
            "Ming Jia",
            "Jie Wang"
        ],
        "abstract": "Our task is to generate an effective summary for a given document with specific realtime requirements. We use the softplus function to enhance keyword rankings to favor important sentences, based on which we present a number of summarization algorithms using various keyword extraction and topic clustering methods. We show that our algorithms meet the realtime requirements and yield the best ROUGE recall scores on DUC-02 over all previously-known algorithms. We show that our algorithms meet the realtime requirements and yield the best ROUGE recall scores on DUC-02 over all previously-known algorithms. To evaluate the quality of summaries without human-generated benchmarks, we define a measure called WESM based on word-embedding using Word Mover's Distance. We show that the orderings of the ROUGE and WESM scores of our algorithms are highly comparable, suggesting that WESM may serve as a viable alternative for measuring the quality of a summary.\n    ",
        "submission_date": "2017-10-01T00:00:00",
        "last_modified_date": "2017-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00286",
        "title": "DTATG: An Automatic Title Generator based on Dependency Trees",
        "authors": [
            "Liqun Shao",
            "Jie Wang"
        ],
        "abstract": "We study automatic title generation for a given block of text and present a method called DTATG to generate titles. DTATG first extracts a small number of central sentences that convey the main meanings of the text and are in a suitable structure for conversion into a title. DTATG then constructs a dependency tree for each of these sentences and removes certain branches using a Dependency Tree Compression Model we devise. We also devise a title test to determine if a sentence can be used as a title. If a trimmed sentence passes the title test, then it becomes a title candidate. DTATG selects the title candidate with the highest ranking score as the final title. Our experiments showed that DTATG can generate adequate titles. We also showed that DTATG-generated titles have higher F1 scores than those generated by the previous methods.\n    ",
        "submission_date": "2017-10-01T00:00:00",
        "last_modified_date": "2017-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.00888",
        "title": "Sentiment Perception of Readers and Writers in Emoji use",
        "authors": [
            "Jose Berengueres",
            "Dani Castro"
        ],
        "abstract": "Previous research has traditionally analyzed emoji sentiment from the point of view of the reader of the content not the author. Here, we analyze emoji sentiment from the point of view of the author and present a emoji sentiment benchmark that was built from an employee happiness dataset where emoji happen to be annotated with daily happiness of the author of the comment. The data spans over 3 years, and 4k employees of 56 companies based in Barcelona. We compare sentiment of writers to readers. Results indicate that, there is an 82% agreement in how emoji sentiment is perceived by readers and writers. Finally, we report that when authors use emoji they report higher levels of happiness. Emoji use was not found to be correlated with differences in author moodiness.\n    ",
        "submission_date": "2017-10-02T00:00:00",
        "last_modified_date": "2018-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01093",
        "title": "Which phoneme-to-viseme maps best improve visual-only computer lip-reading?",
        "authors": [
            "Helen L. Bear",
            "Richard W. Harvey",
            "Barry-John Theobald",
            "Yuxuan Lan"
        ],
        "abstract": "A critical assumption of all current visual speech recognition systems is that there are visual speech units called visemes which can be mapped to units of acoustic speech, the phonemes. Despite there being a number of published maps it is infrequent to see the effectiveness of these tested, particularly on visual-only lip-reading (many works use audio-visual speech). Here we examine 120 mappings and consider if any are stable across talkers. We show a method for devising maps based on phoneme confusions from an automated lip-reading system, and we present new mappings that show improvements for individual talkers.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01142",
        "title": "Finding phonemes: improving machine lip-reading",
        "authors": [
            "Helen L. Bear",
            "Richard W. Harvey",
            "Yuxuan Lan"
        ],
        "abstract": "In machine lip-reading there is continued debate and research around the correct classes to be used for recognition. In this paper we use a structured approach for devising speaker-dependent viseme classes, which enables the creation of a set of phoneme-to-viseme maps where each has a different quantity of visemes ranging from two to 45. Viseme classes are based upon the mapping of articulated phonemes, which have been confused during phoneme recognition, into viseme groups. Using these maps, with the LiLIR dataset, we show the effect of changing the viseme map size in speaker-dependent machine lip-reading, measured by word recognition correctness and so demonstrate that word recognition with phoneme classifiers is not just possible, but often better than word recognition with viseme classifiers. Furthermore, there are intermediate units between visemes and phonemes which are better still.\n    ",
        "submission_date": "2017-10-03T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.01507",
        "title": "Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks",
        "authors": [
            "Vaibhav Kumar",
            "Dhruv Khattar",
            "Siddhartha Gairola",
            "Yash Kumar Lal",
            "Vasudeva Varma"
        ],
        "abstract": "Online media outlets, in a bid to expand their reach and subsequently increase revenue through ad monetisation, have begun adopting clickbait techniques to lure readers to click on articles. The article fails to fulfill the promise made by the headline. Traditional methods for clickbait detection have relied heavily on feature engineering which, in turn, is dependent on the dataset it is built for. The application of neural networks for this task has only been explored partially. We propose a novel approach considering all information found in a social media post. We train a bidirectional LSTM with an attention mechanism to learn the extent to which a word contributes to the post's clickbait score in a differential manner. We also employ a Siamese net to capture the similarity between source and target information. Information gleaned from images has not been considered in previous approaches. We learn image embeddings from large amounts of data using Convolutional Neural Networks to add another layer of complexity to our model. Finally, we concatenate the outputs from the three separate components, serving it as input to a fully connected layer. We conduct experiments over a test corpus of 19538 social media posts, attaining an F1 score of 65.37% on the dataset bettering the previous state-of-the-art, as well as other proposed approaches, feature engineering or otherwise.\n    ",
        "submission_date": "2017-10-04T00:00:00",
        "last_modified_date": "2018-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02560",
        "title": "The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments",
        "authors": [
            "Mirco Ravanelli",
            "Maurizio Omologo"
        ],
        "abstract": "This paper introduces the contents and the possible usage of the DIRHA-ENGLISH multi-microphone corpus, recently realized under the EC DIRHA project. The reference scenario is a domestic environment equipped with a large number of microphones and microphone arrays distributed in space.\n",
        "submission_date": "2017-10-06T00:00:00",
        "last_modified_date": "2017-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.02973",
        "title": "LD-SDS: Towards an Expressive Spoken Dialogue System based on Linked-Data",
        "authors": [
            "Alexandros Papangelis",
            "Panagiotis Papadakos",
            "Margarita Kotti",
            "Yannis Stylianou",
            "Yannis Tzitzikas",
            "Dimitris Plexousakis"
        ],
        "abstract": "In this work we discuss the related challenges and describe an approach towards the fusion of state-of-the-art technologies from the Spoken Dialogue Systems (SDS) and the Semantic Web and Information Retrieval domains. We envision a dialogue system named LD-SDS that will support advanced, expressive, and engaging user requests, over multiple, complex, rich, and open-domain data sources that will leverage the wealth of the available Linked Data. Specifically, we focus on: a) improving the identification, disambiguation and linking of entities occurring in data sources and user input; b) offering advanced query services for exploiting the semantics of the data, with reasoning and exploratory capabilities; and c) expanding the typical information seeking dialogue model (slot filling) to better reflect real-world conversational search scenarios.\n    ",
        "submission_date": "2017-10-09T00:00:00",
        "last_modified_date": "2017-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.03538",
        "title": "Contaminated speech training methods for robust DNN-HMM distant speech recognition",
        "authors": [
            "Mirco Ravanelli",
            "Maurizio Omologo"
        ],
        "abstract": "Despite the significant progress made in the last years, state-of-the-art speech recognition technologies provide a satisfactory performance only in the close-talking condition. Robustness of distant speech recognition in adverse acoustic conditions, on the other hand, remains a crucial open issue for future applications of human-machine interaction. To this end, several advances in speech enhancement, acoustic scene analysis as well as acoustic modeling, have recently contributed to improve the state-of-the-art in the field. One of the most effective approaches to derive a robust acoustic modeling is based on using contaminated speech, which proved helpful in reducing the acoustic mismatch between training and testing conditions.\n",
        "submission_date": "2017-10-10T00:00:00",
        "last_modified_date": "2017-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04099",
        "title": "Wembedder: Wikidata entity embedding web service",
        "authors": [
            "Finn \u00c5rup Nielsen"
        ],
        "abstract": "I present a web service for querying an embedding of entities in the Wikidata knowledge graph. The embedding is trained on the Wikidata dump using Gensim's Word2Vec implementation and a simple graph walk. A REST API is implemented. Together with the Wikidata API the web service exposes a multilingual resource for over 600'000 Wikidata items and properties.\n    ",
        "submission_date": "2017-10-11T00:00:00",
        "last_modified_date": "2017-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04142",
        "title": "Bollywood Movie Corpus for Text, Images and Videos",
        "authors": [
            "Nishtha Madaan",
            "Sameep Mehta",
            "Mayank Saxena",
            "Aditi Aggarwal",
            "Taneea S Agrawaal",
            "Vrinda Malhotra"
        ],
        "abstract": "In past few years, several data-sets have been released for text and images. We present an approach to create the data-set for use in detecting and removing gender bias from text. We also include a set of challenges we have faced while creating this corpora. In this work, we have worked with movie data from Wikipedia plots and movie trailers from YouTube. Our Bollywood Movie corpus contains 4000 movies extracted from Wikipedia and 880 trailers extracted from YouTube which were released from 1970-2017. The corpus contains csv files with the following data about each movie - Wikipedia title of movie, cast, plot text, co-referenced plot text, soundtrack information, link to movie poster, caption of movie poster, number of males in poster, number of females in poster. In addition to that, corresponding to each cast member the following data is available - cast name, cast gender, cast verbs, cast adjectives, cast relations, cast centrality, cast mentions. We present some preliminary results on the task of bias removal which suggest that the data-set is quite useful for performing such tasks.\n    ",
        "submission_date": "2017-10-11T00:00:00",
        "last_modified_date": "2017-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.04312",
        "title": "Measurement Context Extraction from Text: Discovering Opportunities and Gaps in Earth Science",
        "authors": [
            "Kyle Hundman",
            "Chris A. Mattmann"
        ],
        "abstract": "We propose Marve, a system for extracting measurement values, units, and related words from natural language text. Marve uses conditional random fields (CRF) to identify measurement values and units, followed by a rule-based system to find related entities, descriptors and modifiers within a sentence. Sentence tokens are represented by an undirected graphical model, and rules are based on part-of-speech and word dependency patterns connecting values and units to contextual words. Marve is unique in its focus on measurement context and early experimentation demonstrates Marve's ability to generate high-precision extractions with strong recall. We also discuss Marve's role in refining measurement requirements for NASA's proposed HyspIRI mission, a hyperspectral infrared imaging satellite that will study the world's ecosystems. In general, our work with HyspIRI demonstrates the value of semantic measurement extractions in characterizing quantitative discussion contained in large corpuses of natural language text. These extractions accelerate broad, cross-cutting research and expose scientists new algorithmic approaches and experimental nuances. They also facilitate identification of scientific opportunities enabled by HyspIRI leading to more efficient scientific investment and research.\n    ",
        "submission_date": "2017-10-11T00:00:00",
        "last_modified_date": "2017-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.05298",
        "title": "Text2Action: Generative Adversarial Synthesis from Language to Action",
        "authors": [
            "Hyemin Ahn",
            "Timothy Ha",
            "Yunho Choi",
            "Hwiyeon Yoo",
            "Songhwai Oh"
        ],
        "abstract": "In this paper, we propose a generative model which learns the relationship between language and human action in order to generate a human action sequence given a sentence describing human behavior. The proposed generative model is a generative adversarial network (GAN), which is based on the sequence to sequence (SEQ2SEQ) model. Using the proposed generative network, we can synthesize various actions for a robot or a virtual agent using a text encoder recurrent neural network (RNN) and an action decoder RNN. The proposed generative network is trained from 29,770 pairs of actions and sentence annotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video dataset. We demonstrate that the network can generate human-like actions which can be transferred to a Baxter robot, such that the robot performs an action based on a provided sentence. Results show that the proposed generative network correctly models the relationship between language and action and can generate a diverse set of actions from the same sentence.\n    ",
        "submission_date": "2017-10-15T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06280",
        "title": "Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions",
        "authors": [
            "Jun Hatori",
            "Yuta Kikuchi",
            "Sosuke Kobayashi",
            "Kuniyuki Takahashi",
            "Yuta Tsuboi",
            "Yuya Unno",
            "Wilson Ko",
            "Jethro Tan"
        ],
        "abstract": "Comprehension of spoken natural language is an essential component for robots to communicate with human effectively. However, handling unconstrained spoken instructions is challenging due to (1) complex structures including a wide variety of expressions used in spoken language and (2) inherent ambiguity in interpretation of human instructions. In this paper, we propose the first comprehensive system that can handle unconstrained spoken language and is able to effectively resolve ambiguity in spoken instructions. Specifically, we integrate deep-learning-based object detection together with natural language processing technologies to handle unconstrained spoken instructions, and propose a method for robots to resolve instruction ambiguity through dialogue. Through our experiments on both a simulated environment as well as a physical industrial robot arm, we demonstrate the ability of our system to understand natural instructions from human operators effectively, and how higher success rates of the object picking task can be achieved through an interactive clarification process.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2018-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06303",
        "title": "Describing Natural Images Containing Novel Objects with Knowledge Guided Assitance",
        "authors": [
            "Aditya Mogadala",
            "Umanga Bista",
            "Lexing Xie",
            "Achim Rettinger"
        ],
        "abstract": "Images in the wild encapsulate rich knowledge about varied abstract concepts and cannot be sufficiently described with models built only using image-caption pairs containing selected objects. We propose to handle such a task with the guidance of a knowledge base that incorporate many abstract concepts. Our method is a two-step process where we first build a multi-entity-label image recognition model to predict abstract concepts as image labels and then leverage them in the second step as an external semantic attention and constrained inference in the caption generation model for describing images that depict unseen/novel objects. Evaluations show that our models outperform most of the prior work for out-of-domain captioning on MSCOCO and are useful for integration of knowledge and vision in general.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.06390",
        "title": "Fishing for Clickbaits in Social Images and Texts with Linguistically-Infused Neural Network Models",
        "authors": [
            "Maria Glenski",
            "Ellyn Ayton",
            "Dustin Arendt",
            "Svitlana Volkova"
        ],
        "abstract": "This paper presents the results and conclusions of our participation in the Clickbait Challenge 2017 on automatic clickbait detection in social media. We first describe linguistically-infused neural network models and identify informative representations to predict the level of clickbaiting present in Twitter posts. Our models allow to answer the question not only whether a post is a clickbait or not, but to what extent it is a clickbait post e.g., not at all, slightly, considerably, or heavily clickbaity using a score ranging from 0 to 1. We evaluate the predictive power of models trained on varied text and image representations extracted from tweets. Our best performing model that relies on the tweet text and linguistic markers of biased language extracted from the tweet and the corresponding page yields mean squared error (MSE) of 0.04, mean absolute error (MAE) of 0.16 and R2 of 0.43 on the held-out test data. For the binary classification setup (clickbait vs. non-clickbait), our model achieved F1 score of 0.69. We have not found that image representations combined with text yield significant performance improvement yet. Nevertheless, this work is the first to present preliminary analysis of objects extracted using Google Tensorflow object detection API from images in clickbait vs. non-clickbait Twitter posts. Finally, we outline several steps to improve model performance as a part of the future work.\n    ",
        "submission_date": "2017-10-17T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07551",
        "title": "Spoken Language Biomarkers for Detecting Cognitive Impairment",
        "authors": [
            "Tuka Alhanai",
            "Rhoda Au",
            "James Glass"
        ],
        "abstract": "In this study we developed an automated system that evaluates speech and language features from audio recordings of neuropsychological examinations of 92 subjects in the Framingham Heart Study. A total of 265 features were used in an elastic-net regularized binomial logistic regression model to classify the presence of cognitive impairment, and to select the most predictive features. We compared performance with a demographic model from 6,258 subjects in the greater study cohort (0.79 AUC), and found that a system that incorporated both audio and text features performed the best (0.92 AUC), with a True Positive Rate of 29% (at 0% False Positive Rate) and a good model fit (Hosmer-Lemeshow test > 0.05). We also found that decreasing pitch and jitter, shorter segments of speech, and responses phrased as questions were positively associated with cognitive impairment.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2017-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07654",
        "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
        "authors": [
            "Wei Ping",
            "Kainan Peng",
            "Andrew Gibiansky",
            "Sercan O. Arik",
            "Ajay Kannan",
            "Sharan Narang",
            "Jonathan Raiman",
            "John Miller"
        ],
        "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2018-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07728",
        "title": "A Computational Framework for Multi-Modal Social Action Identification",
        "authors": [
            "Jason Anastasopoulos",
            "Jake Ryland Williams"
        ],
        "abstract": "We create a computational framework for understanding social action and demonstrate how this framework can be used to build an open-source event detection tool with scalable statistical machine learning algorithms and a subsampled database of over 600 million geo-tagged Tweets from around the world. These Tweets were collected between April 1st, 2014 and April 30th, 2015, most notably when the Black Lives Matter movement began. We demonstrate how these methods can be used diagnostically-by researchers, government officials and the public-to understand peaceful and violent collective action at very fine-grained levels of time and geography.\n    ",
        "submission_date": "2017-10-20T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.07868",
        "title": "Deep Triphone Embedding Improves Phoneme Recognition",
        "authors": [
            "Mohit Yadav",
            "Vivek Tyagi"
        ],
        "abstract": "In this paper, we present a novel Deep Triphone Embedding (DTE) representation derived from Deep Neural Network (DNN) to encapsulate the discriminative information present in the adjoining speech frames. DTEs are generated using a four hidden layer DNN with 3000 nodes in each hidden layer at the first-stage. This DNN is trained with the tied-triphone classification accuracy as an optimization criterion. Thereafter, we retain the activation vectors (3000) of the last hidden layer, for each speech MFCC frame, and perform dimension reduction to further obtain a 300 dimensional representation, which we termed as DTE. DTEs along with MFCC features are fed into a second-stage four hidden layer DNN, which is subsequently trained for the task of tied-triphone classification. Both DNNs are trained using tri-phone labels generated from a tied-state triphone HMM-GMM system, by performing a forced-alignment between the transcriptions and MFCC feature frames. We conduct the experiments on publicly available TED-LIUM speech corpus. The results show that the proposed DTE method provides an improvement of absolute 2.11% in phoneme recognition, when compared with a competitive hybrid tied-state triphone HMM-DNN system.\n    ",
        "submission_date": "2017-10-22T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08528",
        "title": "A Two-Level Classification Approach for Detecting Clickbait Posts using Text-Based Features",
        "authors": [
            "Olga Papadopoulou",
            "Markos Zampoglou",
            "Symeon Papadopoulos",
            "Ioannis Kompatsiaris"
        ],
        "abstract": "The emergence of social media as news sources has led to the rise of clickbait posts attempting to attract users to click on article links without informing them on the actual article content. This paper presents our efforts to create a clickbait detector inspired by fake news detection algorithms, and our submission to the Clickbait Challenge 2017. The detector is based almost exclusively on text-based features taken from previous work on clickbait detection, our own work on fake post detection, and features we designed specifically for the challenge. We use a two-level classification approach, combining the outputs of 65 first-level classifiers in a second-level feature vector. We present our exploratory results with individual features and their combinations, taken from the post text and the target article title, as well as feature selection. While our own blind tests with the dataset led to an F-score of 0.63, our final evaluation in the Challenge only achieved an F-score of 0.43. We explore the possible causes of this, and lay out potential future steps to achieve more successful results.\n    ",
        "submission_date": "2017-10-23T00:00:00",
        "last_modified_date": "2017-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08634",
        "title": "Using Multi-Label Classification for Improved Question Answering",
        "authors": [
            "Ricardo Usbeck",
            "Michael Hoffmann",
            "Michael R\u00f6der",
            "Jens Lehmann",
            "Axel-Cyrille Ngonga Ngomo"
        ],
        "abstract": "A plethora of diverse approaches for question answering over RDF data have been developed in recent years. While the accuracy of these systems has increased significantly over time, most systems still focus on particular types of questions or particular challenges in question answering. What is a curse for single systems is a blessing for the combination of these systems. We show in this paper how machine learning techniques can be applied to create a more accurate question answering metasystem by reusing existing systems. In particular, we develop a multi-label classification-based metasystem for question answering over 6 existing systems using an innovative set of 14 question features. The metasystem outperforms the best single system by 14% F-measure on the recent QALD-6 benchmark. Furthermore, we analyzed the influence and correlation of the underlying features on the metasystem quality.\n    ",
        "submission_date": "2017-10-24T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.08963",
        "title": "Scaling Text with the Class Affinity Model",
        "authors": [
            "Patrick O. Perry",
            "Kenneth Benoit"
        ],
        "abstract": "Probabilistic methods for classifying text form a rich tradition in machine learning and natural language processing. For many important problems, however, class prediction is uninteresting because the class is known, and instead the focus shifts to estimating latent quantities related to the text, such as affect or ideology. We focus on one such problem of interest, estimating the ideological positions of 55 Irish legislators in the 1991 D\u00e1il confidence vote. To solve the D\u00e1il scaling problem and others like it, we develop a text modeling framework that allows actors to take latent positions on a \"gray\" spectrum between \"black\" and \"white\" polar opposites. We are able to validate results from this model by measuring the influences exhibited by individual words, and we are able to quantify the uncertainty in the scaling estimates by using a sentence-level block bootstrap. Applying our method to the D\u00e1il debate, we are able to scale the legislators between extreme pro-government and pro-opposition in a way that reveals nuances in their speeches not captured by their votes or party affiliations.\n    ",
        "submission_date": "2017-10-24T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09026",
        "title": "Trace norm regularization and faster inference for embedded speech recognition RNNs",
        "authors": [
            "Markus Kliegl",
            "Siddharth Goyal",
            "Kexin Zhao",
            "Kavya Srinet",
            "Mohammad Shoeybi"
        ],
        "abstract": "We propose and evaluate new techniques for compressing and speeding up dense matrix multiplications as found in the fully connected and recurrent layers of neural networks for embedded large vocabulary continuous speech recognition (LVCSR). For compression, we introduce and study a trace norm regularization technique for training low rank factored versions of matrix multiplications. Compared to standard low rank training, we show that our method leads to good accuracy versus number of parameter trade-offs and can be used to speed up training of large models. For speedup, we enable faster inference on ARM processors through new open sourced kernels optimized for small batch sizes, resulting in 3x to 7x speed ups over the widely used gemmlowp library. Beyond LVCSR, we expect our techniques and kernels to be more generally applicable to embedded neural networks with large fully connected or recurrent layers.\n    ",
        "submission_date": "2017-10-25T00:00:00",
        "last_modified_date": "2018-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09085",
        "title": "Re-evaluating the need for Modelling Term-Dependence in Text Classification Problems",
        "authors": [
            "Sounak Banerjee",
            "Prasenjit Majumder",
            "Mandar Mitra"
        ],
        "abstract": "A substantial amount of research has been carried out in developing machine learning algorithms that account for term dependence in text classification. These algorithms offer acceptable performance in most cases but they are associated with a substantial cost. They require significantly greater resources to operate. This paper argues against the justification of the higher costs of these algorithms, based on their performance in text classification problems. In order to prove the conjecture, the performance of one of the best dependence models is compared to several well established algorithms in text classification. A very specific collection of datasets have been designed, which would best reflect the disparity in the nature of text data, that are present in real world applications. The results show that even one of the best term dependence models, performs decent at best when compared to other independence models. Coupled with their substantially greater requirement for hardware resources for operation, this makes them an impractical choice for being used in real world scenarios.\n    ",
        "submission_date": "2017-10-25T00:00:00",
        "last_modified_date": "2017-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.09805",
        "title": "Improving Negative Sampling for Word Representation using Self-embedded Features",
        "authors": [
            "Long Chen",
            "Fajie Yuan",
            "Joemon M. Jose",
            "Weinan Zhang"
        ],
        "abstract": "Although the word-popularity based negative sampler has shown superb performance in the skip-gram model, the theoretical motivation behind oversampling popular (non-observed) words as negative samples is still not well understood. In this paper, we start from an investigation of the gradient vanishing issue in the skipgram model without a proper negative sampler. By performing an insightful analysis from the stochastic gradient descent (SGD) learning perspective, we demonstrate that, both theoretically and intuitively, negative samples with larger inner product scores are more informative than those with lower scores for the SGD learner in terms of both convergence rate and accuracy. Understanding this, we propose an alternative sampling algorithm that dynamically selects informative negative samples during each SGD update. More importantly, the proposed sampler accounts for multi-dimensional self-embedded features during the sampling process, which essentially makes it more effective than the original popularity-based (one-dimensional) sampler. Empirical experiments further verify our observations, and show that our fine-grained samplers gain significant improvement over the existing ones without increasing computational complexity.\n    ",
        "submission_date": "2017-10-26T00:00:00",
        "last_modified_date": "2018-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10380",
        "title": "Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding",
        "authors": [
            "Shuai Tang",
            "Hailin Jin",
            "Chen Fang",
            "Zhaowen Wang",
            "Virginia R. de Sa"
        ],
        "abstract": "Context plays an important role in human language understanding, thus it may also be useful for machines learning vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. We carefully designed experiments to show that neither an autoregressive decoder nor an RNN decoder is required. After that, we designed a model which still keeps an RNN as the encoder, while using a non-autoregressive convolutional decoder. We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabelled corpora, and in both cases the transferability is evaluated on a set of downstream NLP tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2018-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10393",
        "title": "Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks",
        "authors": [
            "Xu Sun",
            "Bingzhen Wei",
            "Xuancheng Ren",
            "Shuming Ma"
        ],
        "abstract": "We propose a method, called Label Embedding Network, which can learn label representation (label embedding) during the training process of deep networks. With the proposed method, the label embedding is adaptively and automatically learned through back propagation. The original one-hot represented loss function is converted into a new loss function with soft distributions, such that the originally unrelated labels have continuous interactions with each other during the training process. As a result, the trained model can achieve substantially higher accuracy and with faster convergence speed. Experimental results based on competitive tasks demonstrate the effectiveness of the proposed method, and the learned label embedding is reasonable and interpretable. The proposed method achieves comparable or even better results than the state-of-the-art systems. The source code is available at \\url{",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2017-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.10467",
        "title": "Generalized End-to-End Loss for Speaker Verification",
        "authors": [
            "Li Wan",
            "Quan Wang",
            "Alan Papir",
            "Ignacio Lopez Moreno"
        ],
        "abstract": "In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as well as multiple dialects.\n    ",
        "submission_date": "2017-10-28T00:00:00",
        "last_modified_date": "2020-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11027",
        "title": "Named Entity Recognition in Twitter using Images and Text",
        "authors": [
            "Diego Esteves",
            "Rafael Peres",
            "Jens Lehmann",
            "Giulio Napolitano"
        ],
        "abstract": "Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities. Despite recent achievements, we still face limitations with correctly detecting and classifying entities, prominently in short and noisy text, such as Twitter. An important negative aspect in most of NER approaches is the high dependency on hand-crafted features and domain-specific knowledge, necessary to achieve state-of-the-art results. Thus, devising models to deal with such linguistically complex contexts is still challenging. In this paper, we propose a novel multi-level architecture that does not rely on any specific linguistic resource or encoded rule. Unlike traditional approaches, we use features extracted from images and text to classify named entities. Experimental tests against state-of-the-art NER for Twitter on the Ritter dataset present competitive results (0.59 F-measure), indicating that this approach may lead towards better NER models.\n    ",
        "submission_date": "2017-10-30T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1710.11342",
        "title": "Generating Natural Adversarial Examples",
        "authors": [
            "Zhengli Zhao",
            "Dheeru Dua",
            "Sameer Singh"
        ],
        "abstract": "Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.\n    ",
        "submission_date": "2017-10-31T00:00:00",
        "last_modified_date": "2018-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.00313",
        "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision",
        "authors": [
            "Mostafa Dehghani",
            "Aliaksei Severyn",
            "Sascha Rothe",
            "Jaap Kamps"
        ],
        "abstract": "Training deep neural networks requires massive amounts of training data, but for many tasks only limited labeled data is available. This makes weak supervision attractive, using weak or noisy signals like the output of heuristic methods or user click-through data for training. In a semi-supervised setting, we can use a large set of data with weak labels to pretrain a neural network and then fine-tune the parameters with a small amount of data with true labels. This feels intuitively sub-optimal as these two independent stages leave the model unaware about the varying label quality. What if we could somehow inform the model about the label quality? In this paper, we propose a semi-supervised learning method where we train two neural networks in a multi-task fashion: a \"target network\" and a \"confidence network\". The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to weight the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model. We evaluate our learning strategy on two different tasks: document ranking and sentiment classification. The results demonstrate that our approach not only enhances the performance compared to the baselines but also speeds up the learning process from weak labels.\n    ",
        "submission_date": "2017-11-01T00:00:00",
        "last_modified_date": "2017-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01563",
        "title": "Multi-label Dataless Text Classification with Topic Modeling",
        "authors": [
            "Daochen Zha",
            "Chenliang Li"
        ],
        "abstract": "Manually labeling documents is tedious and expensive, but it is essential for training a traditional text classifier. In recent years, a few dataless text classification techniques have been proposed to address this problem. However, existing works mainly center on single-label classification problems, that is, each document is restricted to belonging to a single category. In this paper, we propose a novel Seed-guided Multi-label Topic Model, named SMTM. With a few seed words relevant to each category, SMTM conducts multi-label classification for a collection of documents without any labeled document. In SMTM, each category is associated with a single category-topic which covers the meaning of the category. To accommodate with multi-labeled documents, we explicitly model the category sparsity in SMTM by using spike and slab prior and weak smoothing prior. That is, without using any threshold tuning, SMTM automatically selects the relevant categories for each document. To incorporate the supervision of the seed words, we propose a seed-guided biased GPU (i.e., generalized Polya urn) sampling procedure to guide the topic inference of SMTM. Experiments on two public datasets show that SMTM achieves better classification accuracy than state-of-the-art alternatives and even outperforms supervised solutions in some scenarios.\n    ",
        "submission_date": "2017-11-05T00:00:00",
        "last_modified_date": "2017-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01694",
        "title": "Multilingual Speech Recognition With A Single End-To-End Model",
        "authors": [
            "Shubham Toshniwal",
            "Tara N. Sainath",
            "Ron J. Weiss",
            "Bo Li",
            "Pedro Moreno",
            "Eugene Weinstein",
            "Kanishka Rao"
        ],
        "abstract": "Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2018-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.01921",
        "title": "$A^{4}NT$: Author Attribute Anonymity by Adversarial Training of Neural Machine Translation",
        "authors": [
            "Rakshith Shetty",
            "Bernt Schiele",
            "Mario Fritz"
        ],
        "abstract": "Text-based analysis methods allow to reveal privacy relevant author attributes such as gender, age and identify of the text's author. Such methods can compromise the privacy of an anonymous author even when the author tries to remove privacy sensitive content. In this paper, we propose an automatic method, called Adversarial Author Attribute Anonymity Neural Translation ($A^4NT$), to combat such text-based adversaries. We combine sequence-to-sequence language models used in machine translation and generative adversarial networks to obfuscate author attributes. Unlike machine translation techniques which need paired data, our method can be trained on unpaired corpora of text containing different authors. Importantly, we propose and evaluate techniques to impose constraints on our $A^4NT$ to preserve the semantics of the input text. $A^4NT$ learns to make minimal changes to the input text to successfully fool author attribute classifiers, while aiming to maintain the meaning of the input. We show through experiments on two different datasets and three settings that our proposed method is effective in fooling the author attribute classifiers and thereby improving the anonymity of authors.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2018-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02132",
        "title": "Weighted Transformer Network for Machine Translation",
        "authors": [
            "Karim Ahmed",
            "Nitish Shirish Keskar",
            "Richard Socher"
        ],
        "abstract": "State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion. Vaswani et al. (2017) propose a new architecture that avoids recurrence and convolution completely. Instead, it uses only self-attention and feed-forward layers. While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge. We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15-40% faster. Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process. Our model improves the state-of-the-art performance by 0.5 BLEU points on the WMT 2014 English-to-German translation task and by 0.4 on the English-to-French translation task.\n    ",
        "submission_date": "2017-11-06T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02295",
        "title": "Quality-Efficiency Trade-offs in Machine Learning for Text Processing",
        "authors": [
            "Ricardo Baeza-Yates",
            "Zeinab Liaghat"
        ],
        "abstract": "Data mining, machine learning, and natural language processing are powerful techniques that can be used together to extract information from large texts. Depending on the task or problem at hand, there are many different approaches that can be used. The methods available are continuously being optimized, but not all these methods have been tested and compared in a set of problems that can be solved using supervised machine learning algorithms. The question is what happens to the quality of the methods if we increase the training data size from, say, 100 MB to over 1 GB? Moreover, are quality gains worth it when the rate of data processing diminishes? Can we trade quality for time efficiency and recover the quality loss by just being able to process more data? We attempt to answer these questions in a general way for text processing tasks, considering the trade-offs involving training data size, learning time, and quality obtained. We propose a performance trade-off framework and apply it to three important text processing problems: Named Entity Recognition, Sentiment Analysis and Document Classification. These problems were also chosen because they have different levels of object granularity: words, paragraphs, and documents. For each problem, we selected several supervised machine learning algorithms and we evaluated the trade-offs of them on large publicly available data sets (news, reviews, patents). To explore these trade-offs, we use different data subsets of increasing size ranging from 50 MB to several GB. We also consider the impact of the data set and the evaluation technique. We find that the results do not change significantly and that most of the time the best algorithms is the fastest. However, we also show that the results for small data (say less than 100 MB) are different from the results for big data and in those cases the best algorithm is much harder to determine.\n    ",
        "submission_date": "2017-11-07T00:00:00",
        "last_modified_date": "2017-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02604",
        "title": "Unbounded cache model for online language modeling with open vocabulary",
        "authors": [
            "Edouard Grave",
            "Moustapha Cisse",
            "Armand Joulin"
        ],
        "abstract": "Recently, continuous cache models were proposed as extensions to recurrent neural network language models, to adapt their predictions to local changes in the data distribution. These models only capture the local context, of up to a few thousands tokens. In this paper, we propose an extension of continuous cache models, which can scale to larger contexts. In particular, we use a large scale non-parametric memory component that stores all the hidden activations seen in the past. We leverage recent advances in approximate nearest neighbor search and quantization algorithms to store millions of representations while searching them efficiently. We conduct extensive experiments showing that our approach significantly improves the perplexity of pre-trained language models on new distributions, and can scale efficiently to much larger contexts than previously proposed local cache models.\n    ",
        "submission_date": "2017-11-07T00:00:00",
        "last_modified_date": "2017-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.02799",
        "title": "Fidelity-Weighted Learning",
        "authors": [
            "Mostafa Dehghani",
            "Arash Mehrjou",
            "Stephan Gouws",
            "Jaap Kamps",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing. This creates a fundamental quality versus-quantity trade-off in the learning process. Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data? We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds. To this end, we propose \"fidelity-weighted learning\" (FWL), a semi-supervised student-teacher approach for training deep neural networks using weakly-labeled data. FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels). Both student and teacher are learned from the data. We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.\n    ",
        "submission_date": "2017-11-08T00:00:00",
        "last_modified_date": "2018-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03147",
        "title": "On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog system: declarative semantics, implementation and applications",
        "authors": [
            "Clemente Rubio-Manzano",
            "Martin Pereira-Fari\u00f1a"
        ],
        "abstract": "In this paper we analyse the benefits of incorporating interval-valued fuzzy sets into the Bousi-Prolog system. A syntax, declarative semantics and im- plementation for this extension is presented and formalised. We show, by using potential applications, that fuzzy logic programming frameworks enhanced with them can correctly work together with lexical resources and ontologies in order to improve their capabilities for knowledge representation and reasoning.\n    ",
        "submission_date": "2017-11-08T00:00:00",
        "last_modified_date": "2017-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03373",
        "title": "SemRe-Rank: Improving Automatic Term Extraction By Incorporating Semantic Relatedness With Personalised PageRank",
        "authors": [
            "Ziqi Zhang",
            "Jie Gao",
            "Fabio Ciravegna"
        ],
        "abstract": "Automatic Term Extraction deals with the extraction of terminology from a domain specific corpus, and has long been an established research area in data and knowledge acquisition. ATE remains a challenging task as it is known that there is no existing ATE methods that can consistently outperform others in any domain. This work adopts a refreshed perspective to this problem: instead of searching for such a 'one-size-fit-all' solution that may never exist, we propose to develop generic methods to 'enhance' existing ATE methods. We introduce SemRe-Rank, the first method based on this principle, to incorporate semantic relatedness - an often overlooked venue - into an existing ATE method to further improve its performance. SemRe-Rank incorporates word embeddings into a personalised PageRank process to compute 'semantic importance' scores for candidate terms from a graph of semantically related words (nodes), which are then used to revise the scores of candidate terms computed by a base ATE algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on four datasets of diverse nature, it is shown to have achieved widespread improvement over all base methods and across all datasets, with up to 15 percentage points when measured by the Precision in the top ranked K candidate terms (the average for a set of K's), or up to 28 percentage points in F1 measured at a K that equals to the expected real terms in the candidates (F1 in short). Compared to an alternative approach built on the well-known TextRank algorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision at top K, or up to 17 points in F1.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2018-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03438",
        "title": "Open-World Knowledge Graph Completion",
        "authors": [
            "Baoxu Shi",
            "Tim Weninger"
        ],
        "abstract": "Knowledge Graphs (KGs) have been applied to many tasks including Web search, link prediction, recommendation, natural language processing, and entity linking. However, most KGs are far from complete and are growing at a rapid pace. To address these problems, Knowledge Graph Completion (KGC) has been proposed to improve KGs by filling in its missing connections. Unlike existing methods which hold a closed-world assumption, i.e., where KGs are fixed and new entities cannot be easily added, in the present work we relax this assumption and propose a new open-world KGC task. As a first attempt to solve this task we introduce an open-world KGC model called ConMask. This model learns embeddings of the entity's name and parts of its text-description to connect unseen entities to the KG. To mitigate the presence of noisy text descriptions, ConMask uses a relationship-dependent content masking to extract relevant snippets and then trains a fully convolutional neural network to fuse the extracted snippets with entities in the KG. Experiments on large data sets, both old and new, show that ConMask performs well in the open-world KGC task and even outperforms existing KGC models on the standard closed-world KGC task.\n    ",
        "submission_date": "2017-11-09T00:00:00",
        "last_modified_date": "2017-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.03800",
        "title": "Object Referring in Visual Scene with Spoken Language",
        "authors": [
            "Arun Balajee Vasudevan",
            "Dengxin Dai",
            "Luc Van Gool"
        ],
        "abstract": "Object referring has important applications, especially for human-machine interaction. While having received great attention, the task is mainly attacked with written language (text) as input rather than spoken language (speech), which is more natural. This paper investigates Object Referring with Spoken Language (ORSpoken) by presenting two datasets and one novel approach. Objects are annotated with their locations in images, text descriptions and speech descriptions. This makes the datasets ideal for multi-modality learning. The approach is developed by carefully taking down ORSpoken problem into three sub-problems and introducing task-specific vision-language interactions at the corresponding levels. Experiments show that our method outperforms competing methods consistently and significantly. The approach is also evaluated in the presence of audio noise, showing the efficacy of the proposed vision-language interaction methods in counteracting background noise.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04434",
        "title": "Faithful to the Original: Fact Aware Neural Abstractive Summarization",
        "authors": [
            "Ziqiang Cao",
            "Furu Wei",
            "Wenjie Li",
            "Sujian Li"
        ],
        "abstract": "Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04498",
        "title": "Targeted Advertising Based on Browsing History",
        "authors": [
            "Yong Zhang",
            "Hongming Zhou",
            "Nganmeng Tan",
            "Saeed Bagheri",
            "Meng Joo Er"
        ],
        "abstract": "Audience interest, demography, purchase behavior and other possible classifications are ex- tremely important factors to be carefully studied in a targeting campaign. This information can help advertisers and publishers deliver advertisements to the right audience group. How- ever, it is not easy to collect such information, especially for the online audience with whom we have limited interaction and minimum deterministic knowledge. In this paper, we pro- pose a predictive framework that can estimate online audience demographic attributes based on their browsing histories. Under the proposed framework, first, we retrieve the content of the websites visited by audience, and represent the content as website feature vectors; second, we aggregate the vectors of websites that audience have visited and arrive at feature vectors representing the users; finally, the support vector machine is exploited to predict the audience demographic attributes. The key to achieving good prediction performance is preparing representative features of the audience. Word Embedding, a widely used tech- nique in natural language processing tasks, together with term frequency-inverse document frequency weighting scheme is used in the proposed method. This new representation ap- proach is unsupervised and very easy to implement. The experimental results demonstrate that the new audience feature representation method is more powerful than existing baseline methods, leading to a great improvement in prediction accuracy.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04564",
        "title": "Phonemic and Graphemic Multilingual CTC Based Speech Recognition",
        "authors": [
            "Markus M\u00fcller",
            "Sebastian St\u00fcker",
            "Alex Waibel"
        ],
        "abstract": "Training automatic speech recognition (ASR) systems requires large amounts of data in the target language in order to achieve good performance. Whereas large training corpora are readily available for languages like English, there exists a long tail of languages which do suffer from a lack of resources. One method to handle data sparsity is to use data from additional source languages and build a multilingual system. Recently, ASR systems based on recurrent neural networks (RNNs) trained with connectionist temporal classification (CTC) have gained substantial research interest. In this work, we extended our previous approach towards training CTC-based systems multilingually. Our systems feature a global phone set, based on the joint phone sets of each source language. We evaluated the use of different language combinations as well as the addition of Language Feature Vectors (LFVs). As contrastive experiment, we built systems based on graphemes as well. Systems having a multilingual phone set are known to suffer in performance compared to their monolingual counterparts. With our proposed approach, we could reduce the gap between these mono- and multilingual setups, using either graphemes or phonemes.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04569",
        "title": "Multilingual Adaptation of RNN Based ASR Systems",
        "authors": [
            "Markus M\u00fcller",
            "Sebastian St\u00fcker",
            "Alex Waibel"
        ],
        "abstract": "In this work, we focus on multilingual systems based on recurrent neural networks (RNNs), trained using the Connectionist Temporal Classification (CTC) loss function. Using a multilingual set of acoustic units poses difficulties. To address this issue, we proposed Language Feature Vectors (LFVs) to train language adaptive multilingual systems. Language adaptation, in contrast to speaker adaptation, needs to be applied not only on the feature level, but also to deeper layers of the network. In this work, we therefore extended our previous approach by introducing a novel technique which we call \"modulation\". Based on this method, we modulated the hidden layers of RNNs using LFVs. We evaluated this approach in both full and low resource conditions, as well as for grapheme and phone based systems. Lower error rates throughout the different conditions could be achieved by the use of the modulation.\n    ",
        "submission_date": "2017-11-13T00:00:00",
        "last_modified_date": "2018-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.04981",
        "title": "SkipFlow: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring",
        "authors": [
            "Yi Tay",
            "Minh C. Phan",
            "Luu Anh Tuan",
            "Siu Cheung Hui"
        ],
        "abstract": "Deep learning has demonstrated tremendous potential for Automatic Text Scoring (ATS) tasks. In this paper, we describe a new neural architecture that enhances vanilla neural network models with auxiliary neural coherence features. Our new method proposes a new \\textsc{SkipFlow} mechanism that models relationships between snapshots of the hidden representations of a long short-term memory (LSTM) network as it reads. Subsequently, the semantic relationships between multiple snapshots are used as auxiliary features for prediction. This has two main benefits. Firstly, essays are typically long sequences and therefore the memorization capability of the LSTM network may be insufficient. Implicit access to multiple snapshots can alleviate this problem by acting as a protection against vanishing gradients. The parameters of the \\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly, modeling relationships between multiple positions allows our model to learn features that represent and approximate textual coherence. In our model, we call this \\textit{neural coherence} features. Overall, we present a unified deep learning architecture that generates neural coherence features as it reads in an end-to-end fashion. Our approach demonstrates state-of-the-art performance on the benchmark ASAP dataset, outperforming not only feature engineering baselines but also other deep learning models.\n    ",
        "submission_date": "2017-11-14T00:00:00",
        "last_modified_date": "2017-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05408",
        "title": "Recurrent Neural Networks as Weighted Language Recognizers",
        "authors": [
            "Yining Chen",
            "Sorcha Gilroy",
            "Andreas Maletti",
            "Jonathan May",
            "Kevin Knight"
        ],
        "abstract": "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete and APX-hard. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2018-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05443",
        "title": "Human and Machine Speaker Recognition Based on Short Trivial Events",
        "authors": [
            "Miao Zhang",
            "Xiaofei Kang",
            "Yanqing Wang",
            "Lantian Li",
            "Zhiyuan Tang",
            "Haisheng Dai",
            "Dong Wang"
        ],
        "abstract": "Trivial events are ubiquitous in human to human conversations, e.g., cough, laugh and sniff. Compared to regular speech, these trivial events are usually short and unclear, thus generally regarded as not speaker discriminative and so are largely ignored by present speaker recognition research. However, these trivial events are highly valuable in some particular circumstances such as forensic examination, as they are less subjected to intentional change, so can be used to discover the genuine speaker from disguised speech. In this paper, we collect a trivial event speech database that involves 75 speakers and 6 types of events, and report preliminary speaker recognition results on this database, by both human listeners and machines. Particularly, the deep feature learning technique recently proposed by our group is utilized to analyze and recognize the trivial events, which leads to acceptable equal error rates (EERs) despite the extremely short durations (0.2-0.5 seconds) of these events. Comparing different types of events, 'hmm' seems more speaker discriminative.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2018-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05447",
        "title": "Emotional End-to-End Neural Speech Synthesizer",
        "authors": [
            "Younggun Lee",
            "Azam Rabiee",
            "Soo-Young Lee"
        ],
        "abstract": "In this paper, we introduce an emotional speech synthesizer based on the recent end-to-end neural model, named Tacotron. Despite its benefits, we found that the original Tacotron suffers from the exposure bias problem and irregularity of the attention alignment. Later, we address the problem by utilization of context vector and residual connection at recurrent neural networks (RNNs). Our experiments showed that the model could successfully train and generate speech for given emotion labels.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05448",
        "title": "Lattice Rescoring Strategies for Long Short Term Memory Language Models in Speech Recognition",
        "authors": [
            "Shankar Kumar",
            "Michael Nirschl",
            "Daniel Holtmann-Rice",
            "Hank Liao",
            "Ananda Theertha Suresh",
            "Felix Yu"
        ],
        "abstract": "Recurrent neural network (RNN) language models (LMs) and Long Short Term Memory (LSTM) LMs, a variant of RNN LMs, have been shown to outperform traditional N-gram LMs on speech recognition tasks. However, these models are computationally more expensive than N-gram LMs for decoding, and thus, challenging to integrate into speech recognizers. Recent research has proposed the use of lattice-rescoring algorithms using RNNLMs and LSTMLMs as an efficient strategy to integrate these models into a speech recognition system. In this paper, we evaluate existing lattice rescoring algorithms along with new variants on a YouTube speech recognition task. Lattice rescoring using LSTMLMs reduces the word error rate (WER) for this task by 8\\% relative to the WER obtained using an N-gram LM.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05472",
        "title": "Can clone detection support quality assessments of requirements specifications?",
        "authors": [
            "Elmar Juergens",
            "Florian Deissenboeck",
            "Martin Feilkas",
            "Benjamin Hummel",
            "Bernhard Schaetz",
            "Stefan Wagner",
            "Christoph Domann",
            "Jonathan Streit"
        ],
        "abstract": "Due to their pivotal role in software engineering, considerable effort is spent on the quality assurance of software requirements specifications. As they are mainly described in natural language, relatively few means of automated quality assessment exist. However, we found that clone detection, a technique widely applied to source code, is promising to assess one important quality aspect in an automated way, namely redundancy that stems from copy&paste operations. This paper describes a large-scale case study that applied clone detection to 28 requirements specifications with a total of 8,667 pages. We report on the amount of redundancy found in real-world specifications, discuss its nature as well as its consequences and evaluate in how far existing code clone detection approaches can be applied to assess the quality of requirements specifications in practice.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05557",
        "title": "Phrase-based Image Captioning with Hierarchical LSTM Model",
        "authors": [
            "Ying Hua Tan",
            "Chee Seng Chan"
        ],
        "abstract": "Automatic generation of caption to describe the content of an image has been gaining a lot of research interests recently, where most of the existing works treat the image caption as pure sequential data. Natural language, however possess a temporal hierarchy structure, with complex dependencies between each subsequence. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. In contrast to the conventional solutions that generate caption in a pure sequential manner, our proposed model decodes image caption from phrase to sentence. It consists of a phrase decoder at the bottom hierarchy to decode noun phrases of variable length, and an abbreviated sentence decoder at the upper hierarchy to decode an abbreviated form of the image description. A complete image caption is formed by combining the generated phrases with sentence during the inference stage. Empirically, our proposed model shows a better or competitive result on the Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the art models. We also show that our proposed model is able to generate more novel captions (not seen in the training data) which are richer in word contents in all these three datasets.\n    ",
        "submission_date": "2017-11-11T00:00:00",
        "last_modified_date": "2017-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.05715",
        "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems",
        "authors": [
            "Zachary Lipton",
            "Xiujun Li",
            "Jianfeng Gao",
            "Lihong Li",
            "Faisal Ahmed",
            "Li Deng"
        ],
        "abstract": "We present a new algorithm that significantly improves the efficiency of exploration for deep Q-learning agents in dialogue systems. Our agents explore via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop neural network. Our algorithm learns much faster than common exploration strategies such as \\epsilon-greedy, Boltzmann, bootstrapping, and intrinsic-reward-based ones. Additionally, we show that spiking the replay buffer with experiences from just a few successful episodes can make Q-learning feasible when it might otherwise fail.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06004",
        "title": "Remedies against the Vocabulary Gap in Information Retrieval",
        "authors": [
            "Christophe Van Gysel"
        ],
        "abstract": "Search engines rely heavily on term-based approaches that represent queries and documents as bags of words. Text---a document or a query---is represented by a bag of its words that ignores grammar and word order, but retains word frequency counts. When presented with a search query, the engine then ranks documents according to their relevance scores by computing, among other things, the matching degrees between query and document terms. While term-based approaches are intuitive and effective in practice, they are based on the hypothesis that documents that exactly contain the query terms are highly relevant regardless of query semantics. Inversely, term-based approaches assume documents that do not contain query terms as irrelevant. However, it is known that a high matching degree at the term level does not necessarily mean high relevance and, vice versa, documents that match null query terms may still be relevant. Consequently, there exists a vocabulary gap between queries and documents that occurs when both use different words to describe the same concepts. It is the alleviation of the effect brought forward by this vocabulary gap that is the topic of this dissertation. More specifically, we propose (1) methods to formulate an effective query from complex textual structures and (2) latent vector space models that circumvent the vocabulary gap in information retrieval.\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2017-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06095",
        "title": "Depression Severity Estimation from Multiple Modalities",
        "authors": [
            "Evgeny Stepanov",
            "Stephane Lathuiliere",
            "Shammur Absar Chowdhury",
            "Arindam Ghosh",
            "Radu-Laurentiu Vieriu",
            "Nicu Sebe",
            "Giuseppe Riccardi"
        ],
        "abstract": "Depression is a major debilitating disorder which can affect people from all ages. With a continuous increase in the number of annual cases of depression, there is a need to develop automatic techniques for the detection of the presence and extent of depression. In this AVEC challenge we explore different modalities (speech, language and visual features extracted from face) to design and develop automatic methods for the detection of depression. In psychology literature, the PHQ-8 questionnaire is well established as a tool for measuring the severity of depression. In this paper we aim to automatically predict the PHQ-8 scores from features extracted from the different modalities. We show that visual features extracted from facial landmarks obtain the best performance in terms of estimating the PHQ-8 results with a mean absolute error (MAE) of 4.66 on the development set. Behavioral characteristics from speech provide an MAE of 4.73. Language features yield a slightly higher MAE of 5.17. When switching to the test set, our Turn Features derived from audio transcriptions achieve the best performance, scoring an MAE of 4.11 (corresponding to an RMSE of 4.94), which makes our system the winner of the AVEC 2017 depression sub-challenge.\n    ",
        "submission_date": "2017-11-10T00:00:00",
        "last_modified_date": "2017-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06232",
        "title": "A Novel Framework for Robustness Analysis of Visual QA Models",
        "authors": [
            "Jia-Hong Huang",
            "Cuong Duc Dao",
            "Modar Alfadly",
            "Bernard Ghanem"
        ],
        "abstract": "Deep neural networks have been playing an essential role in many computer vision tasks including Visual Question Answering (VQA). Until recently, the study of their accuracy was the main focus of research but now there is a trend toward assessing the robustness of these models against adversarial attacks by evaluating their tolerance to varying noise levels. In VQA, adversarial attacks can target the image and/or the proposed main question and yet there is a lack of proper analysis of the later. In this work, we propose a flexible framework that focuses on the language part of VQA that uses semantically relevant questions, dubbed basic questions, acting as controllable noise to evaluate the robustness of VQA models. We hypothesize that the level of noise is positively correlated to the similarity of a basic question to the main question. Hence, to apply noise on any given main question, we rank a pool of basic questions based on their similarity by casting this ranking task as a LASSO optimization problem. Then, we propose a novel robustness measure, R_score, and two large-scale basic question datasets (BQDs) in order to standardize robustness analysis for VQA models.\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2018-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06288",
        "title": "Language-Based Image Editing with Recurrent Attentive Models",
        "authors": [
            "Jianbo Chen",
            "Yelong Shen",
            "Jianfeng Gao",
            "Jingjing Liu",
            "Xiaodong Liu"
        ],
        "abstract": "We investigate the problem of Language-Based Image Editing (LBIE). Given a source image and a natural language description, we want to generate a target image by editing the source image based on the description. We propose a generic modeling framework for two sub-tasks of LBIE: language-based image segmentation and image colorization. The framework uses recurrent attentive models to fuse image and language features. Instead of using a fixed step size, we introduce for each region of the image a termination gate to dynamically determine after each inference step whether to continue extrapolating additional information from the textual description. The effectiveness of the framework is validated on three datasets. First, we introduce a synthetic dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE system. Second, we show that the framework leads to state-of-the-art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on the Oxford-102 Flowers dataset.\n    ",
        "submission_date": "2017-11-16T00:00:00",
        "last_modified_date": "2018-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06794",
        "title": "Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering",
        "authors": [
            "Pan Lu",
            "Hongsheng Li",
            "Wei Zhang",
            "Jianyong Wang",
            "Xiaogang Wang"
        ],
        "abstract": "Recently, the Visual Question Answering (VQA) task has gained increasing attention in artificial intelligence. Existing VQA methods mainly adopt the visual attention mechanism to associate the input question with corresponding image regions for effective question answering. The free-form region based and the detection-based visual attention mechanisms are mostly investigated, with the former ones attending free-form image regions and the latter ones attending pre-specified detection-box regions. We argue that the two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve the VQA problem. In this paper, we propose a novel deep neural network for VQA that integrates both attention mechanisms. Our proposed framework effectively fuses features from free-form image regions, detection boxes, and question representations via a multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes for more accurate question answering. The proposed method is extensively evaluated on two publicly available datasets, COCO-QA and VQA, and outperforms state-of-the-art approaches. Source code is available at ",
        "submission_date": "2017-11-18T00:00:00",
        "last_modified_date": "2017-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06821",
        "title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates",
        "authors": [
            "Guillem Collell",
            "Luc Van Gool",
            "Marie-Francine Moens"
        ],
        "abstract": "Spatial understanding is a fundamental problem with wide-reaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.\n    ",
        "submission_date": "2017-11-18T00:00:00",
        "last_modified_date": "2020-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06899",
        "title": "The Cultural Evolution of National Constitutions",
        "authors": [
            "Daniel N. Rockmore",
            "Chen Fang",
            "Nicholas J. Foti",
            "Tom Ginsburg",
            "David C. Krakauer"
        ],
        "abstract": "We explore how ideas from infectious disease and genetics can be used to uncover patterns of cultural inheritance and innovation in a corpus of 591 national constitutions spanning 1789 - 2008. Legal \"Ideas\" are encoded as \"topics\" - words statistically linked in documents - derived from topic modeling the corpus of constitutions. Using these topics we derive a diffusion network for borrowing from ancestral constitutions back to the US Constitution of 1789 and reveal that constitutions are complex cultural recombinants. We find systematic variation in patterns of borrowing from ancestral texts and \"biological\"-like behavior in patterns of inheritance with the distribution of \"offspring\" arising through a bounded preferential-attachment process. This process leads to a small number of highly innovative (influential) constitutions some of which have yet to have been identified as so in the current literature. Our findings thus shed new light on the critical nodes of the constitution-making network. The constitutional network structure reflects periods of intense constitution creation, and systematic patterns of variation in constitutional life-span and temporal influence.\n    ",
        "submission_date": "2017-11-18T00:00:00",
        "last_modified_date": "2017-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.06968",
        "title": "Intelligent Word Embeddings of Free-Text Radiology Reports",
        "authors": [
            "Imon Banerjee",
            "Sriraman Madhavan",
            "Roger Eric Goldman",
            "Daniel L. Rubin"
        ],
        "abstract": "Radiology reports are a rich resource for advancing deep learning applications in medicine by leveraging the large volume of data continuously being updated, integrated, and shared. However, there are significant challenges as well, largely due to the ambiguity and subtlety of natural language. We propose a hybrid strategy that combines semantic-dictionary mapping and word2vec modeling for creating dense vector embeddings of free-text radiology reports. Our method leverages the benefits of both semantic-dictionary mapping as well as unsupervised learning. Using the vector representation, we automatically classify the radiology reports into three classes denoting confidence in the diagnosis of intracranial hemorrhage by the interpreting radiologist. We performed experiments with varying hyperparameter settings of the word embeddings and a range of different classifiers. Best performance achieved was a weighted precision of 88% and weighted recall of 90%. Our work offers the potential to leverage unstructured electronic health record data by allowing direct analysis of narrative clinical notes.\n    ",
        "submission_date": "2017-11-19T00:00:00",
        "last_modified_date": "2017-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07128",
        "title": "Hello Edge: Keyword Spotting on Microcontrollers",
        "authors": [
            "Yundong Zhang",
            "Naveen Suda",
            "Liangzhen Lai",
            "Vikas Chandra"
        ],
        "abstract": "Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4%, which is ~10% higher than the DNN model with similar number of parameters.\n    ",
        "submission_date": "2017-11-20T00:00:00",
        "last_modified_date": "2018-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07280",
        "title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
        "authors": [
            "Peter Anderson",
            "Qi Wu",
            "Damien Teney",
            "Jake Bruce",
            "Mark Johnson",
            "Niko S\u00fcnderhauf",
            "Ian Reid",
            "Stephen Gould",
            "Anton van den Hengel"
        ],
        "abstract": "A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.\n    ",
        "submission_date": "2017-11-20T00:00:00",
        "last_modified_date": "2018-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07613",
        "title": "Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning",
        "authors": [
            "Qi Wu",
            "Peng Wang",
            "Chunhua Shen",
            "Ian Reid",
            "Anton van den Hengel"
        ],
        "abstract": "The Visual Dialogue task requires an agent to engage in a conversation about an image with a human. It represents an extension of the Visual Question Answering task in that the agent needs to answer a question about an image, but it needs to do so in light of the previous dialogue that has taken place. The key challenge in Visual Dialogue is thus maintaining a consistent, and natural dialogue while continuing to answer questions correctly. We present a novel approach that combines Reinforcement Learning and Generative Adversarial Networks (GANs) to generate more human-like responses to questions. The GAN helps overcome the relative paucity of training data, and the tendency of the typical MLE-based approach to generate overly terse answers. Critically, the GAN is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer. This means that the discriminative model of the GAN has the task of assessing whether a candidate answer is generated by a human or not, given the provided reason. This is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning. The method also generates the state-of-the-art results on the primary benchmark.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.07614",
        "title": "Asking the Difficult Questions: Goal-Oriented Visual Question Generation via Intermediate Rewards",
        "authors": [
            "Junjie Zhang",
            "Qi Wu",
            "Chunhua Shen",
            "Jian Zhang",
            "Jianfeng Lu",
            "Anton van den Hengel"
        ],
        "abstract": "Despite significant progress in a variety of vision-and-language problems, developing a method capable of asking intelligent, goal-oriented questions about images is proven to be an inscrutable challenge. Towards this end, we propose a Deep Reinforcement Learning framework based on three new intermediate rewards, namely goal-achieved, progressive and informativeness that encourage the generation of succinct questions, which in turn uncover valuable information towards the overall goal. By directly optimizing for questions that work quickly towards fulfilling the overall goal, we avoid the tendency of existing methods to generate long series of insane queries that add little value. We evaluate our model on the GuessWhat?! dataset and show that the resulting questions can help a standard Guesser identify a specific object in an image at a much higher success rate.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08016",
        "title": "Deep Long Short-Term Memory Adaptive Beamforming Networks For Multichannel Robust Speech Recognition",
        "authors": [
            "Zhong Meng",
            "Shinji Watanabe",
            "John R. Hershey",
            "Hakan Erdogan"
        ],
        "abstract": "Far-field speech recognition in noisy and reverberant conditions remains a challenging problem despite recent deep learning breakthroughs. This problem is commonly addressed by acquiring a speech signal from multiple microphones and performing beamforming over them. In this paper, we propose to use a recurrent neural network with long short-term memory (LSTM) architecture to adaptively estimate real-time beamforming filter coefficients to cope with non-stationary environmental noise and dynamic nature of source and microphones positions which results in a set of timevarying room impulse responses. The LSTM adaptive beamformer is jointly trained with a deep LSTM acoustic model to predict senone labels. Further, we use hidden units in the deep LSTM acoustic model to assist in predicting the beamforming filter coefficients. The proposed system achieves 7.97% absolute gain over baseline systems with no beamforming on CHiME-3 real evaluation set.\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08058",
        "title": "Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio",
        "authors": [
            "Ahmad AbdulKader",
            "Kareem Nassar",
            "Mohamed El-Geish",
            "Daniel Galvez",
            "Chetan Patil"
        ],
        "abstract": "We propose using cascaded classifiers for a keyword spotting (KWS) task on narrow-band (NB), 8kHz audio acquired in non-IID environments -- a more challenging task than most state-of-the-art KWS systems face. We present a model that incorporates Deep Neural Networks (DNNs), cascading, multiple-feature representations, and multiple-instance learning. The cascaded classifiers handle the task's class imbalance and reduce power consumption on computationally-constrained devices via early termination. The KWS system achieves a false negative rate of 6% at an hourly false positive rate of 0.75\n    ",
        "submission_date": "2017-11-21T00:00:00",
        "last_modified_date": "2025-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08521",
        "title": "EMFET: E-mail Features Extraction Tool",
        "authors": [
            "Wadi' Hijawi",
            "Hossam Faris",
            "Ja'far Alqatawna",
            "Ibrahim Aljarah",
            "Ala' M. Al-Zoubi",
            "Maria Habib"
        ],
        "abstract": "EMFET is an open source and flexible tool that can be used to extract a large number of features from any email corpus with emails saved in EML format. The extracted features can be categorized into three main groups: header features, payload (body) features, and attachment features. The purpose of the tool is to help practitioners and researchers to build datasets that can be used for training machine learning models for spam detection. So far, 140 features can be extracted using EMFET. EMFET is extensible and easy to use. The source code of EMFET is publicly available at GitHub (",
        "submission_date": "2017-11-22T00:00:00",
        "last_modified_date": "2017-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08621",
        "title": "Counterfactual Learning for Machine Translation: Degeneracies and Solutions",
        "authors": [
            "Carolin Lawrence",
            "Pratik Gajane",
            "Stefan Riezler"
        ],
        "abstract": "Counterfactual learning is a natural scenario to improve web-based machine translation services by offline learning from feedback logged during user interactions. In order to avoid the risk of showing inferior translations to users, in such scenarios mostly exploration-free deterministic logging policies are in place. We analyze possible degeneracies of inverse and reweighted propensity scoring estimators, in stochastic and deterministic settings, and relate them to recently proposed techniques for counterfactual learning under deterministic logging.\n    ",
        "submission_date": "2017-11-23T00:00:00",
        "last_modified_date": "2017-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08870",
        "title": "Continuous Semantic Topic Embedding Model Using Variational Autoencoder",
        "authors": [
            "Namkyu Jung",
            "Hyeong In Choi"
        ],
        "abstract": "This paper proposes the continuous semantic topic embedding model (CSTEM) which finds latent topic variables in documents using continuous semantic distance function between the topics and the words by means of the variational autoencoder(VAE). The semantic distance could be represented by any symmetric bell-shaped geometric distance function on the Euclidean space, for which the Mahalanobis distance is used in this paper. In order for the semantic distance to perform more properly, we newly introduce an additional model parameter for each word to take out the global factor from this distance indicating how likely it occurs regardless of its topic. It certainly improves the problem that the Gaussian distribution which is used in previous topic model with continuous word embedding could not explain the semantic relation correctly and helps to obtain the higher topic coherence. Through the experiments with the dataset of 20 Newsgroup, NIPS papers and CNN/Dailymail corpus, the performance of the recent state-of-the-art models is accomplished by our model as well as generating topic embedding vectors which makes possible to observe where the topic vectors are embedded with the word vectors in the real Euclidean space and how the topics are related each other semantically.\n    ",
        "submission_date": "2017-11-24T00:00:00",
        "last_modified_date": "2017-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.08992",
        "title": "Self-Supervised Vision-Based Detection of the Active Speaker as Support for Socially-Aware Language Acquisition",
        "authors": [
            "Kalin Stefanov",
            "Jonas Beskow",
            "Giampiero Salvi"
        ],
        "abstract": "This paper presents a self-supervised method for visual detection of the active speaker in a multi-person spoken interaction scenario. Active speaker detection is a fundamental prerequisite for any artificial cognitive system attempting to acquire language in social settings. The proposed method is intended to complement the acoustic detection of the active speaker, thus improving the system robustness in noisy conditions. The method can detect an arbitrary number of possibly overlapping active speakers based exclusively on visual information about their face. Furthermore, the method does not rely on external annotations, thus complying with cognitive development. Instead, the method uses information from the auditory modality to support learning in the visual domain. This paper reports an extensive evaluation of the proposed method using a large multi-person face-to-face interaction dataset. The results show good performance in a speaker dependent setting. However, in a speaker independent setting the proposed method yields a significantly lower performance. We believe that the proposed method represents an essential component of any artificial cognitive system or robotic platform engaging in social interactions.\n    ",
        "submission_date": "2017-11-24T00:00:00",
        "last_modified_date": "2019-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09055",
        "title": "Interactive Robot Learning of Gestures, Language and Affordances",
        "authors": [
            "Giovanni Saponaro",
            "Lorenzo Jamone",
            "Alexandre Bernardino",
            "Giampiero Salvi"
        ],
        "abstract": "A growing field in robotics and Artificial Intelligence (AI) research is human-robot collaboration, whose target is to enable effective teamwork between humans and robots. However, in many situations human teams are still superior to human-robot teams, primarily because human teams can easily agree on a common goal with language, and the individual members observe each other effectively, leveraging their shared motor repertoire and sensorimotor resources. This paper shows that for cognitive robots it is possible, and indeed fruitful, to combine knowledge acquired from interacting with elements of the environment (affordance exploration) with the probabilistic observation of another agent's actions.\n",
        "submission_date": "2017-11-24T00:00:00",
        "last_modified_date": "2017-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.09714",
        "title": "Language Bootstrapping: Learning Word Meanings From Perception-Action Association",
        "authors": [
            "Giampiero Salvi",
            "Luis Montesano",
            "Alexandre Bernardino",
            "Jos\u00e9 Santos-Victor"
        ],
        "abstract": "We address the problem of bootstrapping language acquisition for an artificial system similarly to what is observed in experiments with human infants. Our method works by associating meanings to words in manipulation tasks, as a robot interacts with objects and listens to verbal descriptions of the interactions. The model is based on an affordance network, i.e., a mapping between robot actions, robot perceptions, and the perceived effects of these actions upon objects. We extend the affordance model to incorporate spoken words, which allows us to ground the verbal symbols to the execution of actions and the perception of the environment. The model takes verbal descriptions of a task as the input and uses temporal co-occurrence to create links between speech utterances and the involved objects, actions, and effects. We show that the robot is able form useful word-to-meaning associations, even without considering grammatical structure in the learning process and in the presence of recognition errors. These word-to-meaning associations are embedded in the robot's own understanding of its actions. Thus, they can be directly used to instruct the robot to perform tasks and also allow to incorporate context in the speech recognition task. We believe that the encouraging results with our approach may afford robots with a capacity to acquire language descriptors in their operation's environment as well as to shed some light as to how this challenging process develops with human infants.\n    ",
        "submission_date": "2017-11-27T00:00:00",
        "last_modified_date": "2017-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10307",
        "title": "Semantic Technology-Assisted Review (STAR) Document analysis and monitoring using random vectors",
        "authors": [
            "Jean-Fran\u00e7ois Delpech"
        ],
        "abstract": "The review and analysis of large collections of documents and the periodic monitoring of new additions thereto has greatly benefited from new developments in computer software. This paper demonstrates how using random vectors to construct a low-dimensional Euclidean space embedding words and documents enables fast and accurate computation of semantic similarities between them. With this technique of Semantic Technology-Assisted Review (STAR), documents can be selected, compared, classified, summarized and evaluated very quickly with minimal expert involvement and high-quality results.\n    ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10327",
        "title": "Generative Interest Estimation for Document Recommendations",
        "authors": [
            "Danijar Hafner",
            "Alexander Immer",
            "Willi Raschkowski",
            "Fabian Windheuser"
        ],
        "abstract": "Learning distributed representations of documents has pushed the state-of-the-art in several natural language processing tasks and was successfully applied to the field of recommender systems recently. In this paper, we propose a novel content-based recommender system based on learned representations and a generative model of user interest. Our method works as follows: First, we learn representations on a corpus of text documents. Then, we capture a user's interest as a generative model in the space of the document representations. In particular, we model the distribution of interest for each user as a Gaussian mixture model (GMM). Recommendations can be obtained directly by sampling from a user's generative model. Using Latent semantic analysis (LSA) as comparison, we compute and explore document representations on the Delicious bookmarks dataset, a standard benchmark for recommender systems. We then perform density estimation in both spaces and show that learned representations outperform LSA in terms of predictive performance.\n    ",
        "submission_date": "2017-11-28T00:00:00",
        "last_modified_date": "2017-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10331",
        "title": "Complex Structure Leads to Overfitting: A Structure Regularization Decoding Method for Natural Language Processing",
        "authors": [
            "Xu Sun",
            "Weiwei Sun",
            "Shuming Ma",
            "Xuancheng Ren",
            "Yi Zhang",
            "Wenjie Li",
            "Houfeng Wang"
        ],
        "abstract": "Recent systems on structured prediction focus on increasing the level of structural dependencies within the model. However, our study suggests that complex structures entail high overfitting risks. To control the structure-based overfitting, we propose to conduct structure regularization decoding (SR decoding). The decoding of the complex structure model is regularized by the additionally trained simple structure model. We theoretically analyze the quantitative relations between the structural complexity and the overfitting risk. The analysis shows that complex structure models are prone to the structure-based overfitting. Empirical evaluations show that the proposed method improves the performance of the complex structure models by reducing the structure-based overfitting. On the sequence labeling tasks, the proposed method substantially improves the performance of the complex neural network models. The maximum F1 error rate reduction is 36.4% for the third-order model. The proposed method also works for the parsing task. The maximum UAS improvement is 5.5% for the tri-sibling model. The results are competitive with or better than the state-of-the-art results.\n    ",
        "submission_date": "2017-11-25T00:00:00",
        "last_modified_date": "2017-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.10377",
        "title": "Sentiment analysis of twitter data",
        "authors": [
            "Hamid Bagheri",
            "Md Johirul Islam"
        ],
        "abstract": "Social networks are the main resources to gather information about people's opinion and sentiments towards different topics as they spend hours daily on social media and share their opinion. In this technical paper, we show the application of sentimental analysis and how to connect to Twitter and run sentimental analysis queries. We run experiments on different queries from politics to humanity and show the interesting results. We realized that the neutral sentiments for tweets are significantly high which clearly shows the limitations of the current works.\n    ",
        "submission_date": "2017-11-15T00:00:00",
        "last_modified_date": "2017-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11017",
        "title": "HoME: a Household Multimodal Environment",
        "authors": [
            "Simon Brodeur",
            "Ethan Perez",
            "Ankesh Anand",
            "Florian Golemo",
            "Luca Celotti",
            "Florian Strub",
            "Jean Rouat",
            "Hugo Larochelle",
            "Aaron Courville"
        ],
        "abstract": "We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11023",
        "title": "A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management",
        "authors": [
            "I\u00f1igo Casanueva",
            "Pawe\u0142 Budzianowski",
            "Pei-Hao Su",
            "Nikola Mrk\u0161i\u0107",
            "Tsung-Hsien Wen",
            "Stefan Ultes",
            "Lina Rojas-Barahona",
            "Steve Young",
            "Milica Ga\u0161i\u0107"
        ],
        "abstract": "Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid the significant effort needed to hand-craft the required dialogue flow, the Dialogue Management (DM) module can be cast as a continuous Markov Decision Process (MDP) and trained through Reinforcement Learning (RL). Several RL models have been investigated over recent years. However, the lack of a common benchmarking framework makes it difficult to perform a fair comparison between different models and their capability to generalise to different environments. Therefore, this paper proposes a set of challenging simulated environments for dialogue model development and evaluation. To provide some baselines, we investigate a number of representative parametric algorithms, namely deep reinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and compare them to a non-parametric model, GP-SARSA. Both the environments and policy models are implemented using the publicly available PyDial toolkit and released on-line, in order to establish a testbed framework for further experiments and to facilitate experimental reproducibility.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2018-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11135",
        "title": "Video Captioning via Hierarchical Reinforcement Learning",
        "authors": [
            "Xin Wang",
            "Wenhu Chen",
            "Jiawei Wu",
            "Yuan-Fang Wang",
            "William Yang Wang"
        ],
        "abstract": "Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2018-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11383",
        "title": "Learning to Learn from Weak Supervision by Full Supervision",
        "authors": [
            "Mostafa Dehghani",
            "Aliaksei Severyn",
            "Sascha Rothe",
            "Jaap Kamps"
        ],
        "abstract": "In this paper, we propose a method for training neural networks when we have a large set of data with weak labels and a small amount of data with true labels. In our proposed model, we train two neural networks: a target network, the learner and a confidence network, the meta-learner. The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to control the magnitude of the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11486",
        "title": "Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy Optimisation",
        "authors": [
            "Christopher Tegho",
            "Pawe\u0142 Budzianowski",
            "Milica Ga\u0161i\u0107"
        ],
        "abstract": "In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are sample efficient, leading to better user experience, but on the expense of a greater computational complexity. This paper examines approaches to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We perform an extensive benchmark of deep Bayesian methods to extract uncertainty estimates, namely Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and alpha-divergences, combining it with DQN algorithm.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1711.11543",
        "title": "Embodied Question Answering",
        "authors": [
            "Abhishek Das",
            "Samyak Datta",
            "Georgia Gkioxari",
            "Stefan Lee",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (\"What color is the car?\"). In order to answer, the agent must first intelligently navigate to explore the environment, gather information through first-person (egocentric) vision, and then answer the question (\"orange\").\n",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00334",
        "title": "Enabling Embodied Analogies in Intelligent Music Systems",
        "authors": [
            "Fabio Paolizzo"
        ],
        "abstract": "The present methodology is aimed at cross-modal machine learning and uses multidisciplinary tools and methods drawn from a broad range of areas and disciplines, including music, systematic musicology, dance, motion capture, human-computer interaction, computational linguistics and audio signal processing. Main tasks include: (1) adapting wisdom-of-the-crowd approaches to embodiment in music and dance performance to create a dataset of music and music lyrics that covers a variety of emotions, (2) applying audio/language-informed machine learning techniques to that dataset to identify automatically the emotional content of the music and the lyrics, and (3) integrating motion capture data from a Vicon system and dancers performing on that music.\n    ",
        "submission_date": "2017-11-30T00:00:00",
        "last_modified_date": "2017-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00377",
        "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
        "authors": [
            "Aishwarya Agrawal",
            "Dhruv Batra",
            "Devi Parikh",
            "Aniruddha Kembhavi"
        ],
        "abstract": "A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.\n    ",
        "submission_date": "2017-12-01T00:00:00",
        "last_modified_date": "2018-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.00733",
        "title": "Incorporating External Knowledge to Answer Open-Domain Visual Questions with Dynamic Memory Networks",
        "authors": [
            "Guohao Li",
            "Hang Su",
            "Wenwu Zhu"
        ],
        "abstract": "Visual Question Answering (VQA) has attracted much attention since it offers insight into the relationships between the multi-modal analysis of images and natural language. Most of the current algorithms are incapable of answering open-domain questions that require to perform reasoning beyond the image contents. To address this issue, we propose a novel framework which endows the model capabilities in answering more complex questions by leveraging massive external knowledge with dynamic memory networks. Specifically, the questions along with the corresponding images trigger a process to retrieve the relevant information in external knowledge bases, which are embedded into a continuous vector space by preserving the entity-relation structures. Afterwards, we employ dynamic memory networks to attend to the large body of facts in the knowledge graph and images, and then perform reasoning over these facts to generate corresponding answers. Extensive experiments demonstrate that our model not only achieves the state-of-the-art performance in the visual question answering task, but can also answer open-domain questions effectively by leveraging the external knowledge.\n    ",
        "submission_date": "2017-12-03T00:00:00",
        "last_modified_date": "2017-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01238",
        "title": "Learning by Asking Questions",
        "authors": [
            "Ishan Misra",
            "Ross Girshick",
            "Rob Fergus",
            "Martial Hebert",
            "Abhinav Gupta",
            "Laurens van der Maaten"
        ],
        "abstract": "We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA). We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions.\n    ",
        "submission_date": "2017-12-04T00:00:00",
        "last_modified_date": "2017-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01329",
        "title": "Examining Cooperation in Visual Dialog Models",
        "authors": [
            "Mircea Mironenco",
            "Dana Kianfar",
            "Ke Tran",
            "Evangelos Kanoulas",
            "Efstratios Gavves"
        ],
        "abstract": "In this work we propose a blackbox intervention method for visual dialog models, with the aim of assessing the contribution of individual linguistic or visual components. Concretely, we conduct structured or randomized interventions that aim to impair an individual component of the model, and observe changes in task performance. We reproduce a state-of-the-art visual dialog model and demonstrate that our methodology yields surprising insights, namely that both dialog and image information have minimal contributions to task performance. The intervention method presented here can be applied as a sanity check for the strength and robustness of each component in visual dialog systems.\n    ",
        "submission_date": "2017-12-04T00:00:00",
        "last_modified_date": "2017-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01455",
        "title": "Multimodal Storytelling via Generative Adversarial Imitation Learning",
        "authors": [
            "Zhiqian Chen",
            "Xuchao Zhang",
            "Arnold P. Boedihardjo",
            "Jing Dai",
            "Chang-Tien Lu"
        ],
        "abstract": "Deriving event storylines is an effective summarization method to succinctly organize extensive information, which can significantly alleviate the pain of information overload. The critical challenge is the lack of widely recognized definition of storyline metric. Prior studies have developed various approaches based on different assumptions about users' interests. These works can extract interesting patterns, but their assumptions do not guarantee that the derived patterns will match users' preference. On the other hand, their exclusiveness of single modality source misses cross-modality information. This paper proposes a method, multimodal imitation learning via generative adversarial networks(MIL-GAN), to directly model users' interests as reflected by various data. In particular, the proposed model addresses the critical challenge by imitating users' demonstrated storylines. Our proposed model is designed to learn the reward patterns given user-provided storylines and then applies the learned policy to unseen data. The proposed approach is demonstrated to be capable of acquiring the user's implicit intent and outperforming competing methods by a substantial margin with a user study.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.01996",
        "title": "An analysis of incorporating an external language model into a sequence-to-sequence model",
        "authors": [
            "Anjuli Kannan",
            "Yonghui Wu",
            "Patrick Nguyen",
            "Tara N. Sainath",
            "Zhifeng Chen",
            "Rohit Prabhavalkar"
        ],
        "abstract": "Attention-based sequence-to-sequence models for automatic speech recognition jointly train an acoustic model, language model, and alignment mechanism. Thus, the language model component is only trained on transcribed audio-text pairs. This leads to the use of shallow fusion with an external language model at inference time. Shallow fusion refers to log-linear interpolation with a separately trained language model at each step of the beam search. In this work, we investigate the behavior of shallow fusion across a range of conditions: different types of language models, different decoding units, and different tasks. On Google Voice Search, we demonstrate that the use of shallow fusion with a neural LM with wordpieces yields a 9.1% relative word error rate reduction (WERR) over our competitive attention-based sequence-to-sequence model, obviating the need for second-pass rescoring.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02034",
        "title": "SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for Predicting Chemical Properties",
        "authors": [
            "Garrett B. Goh",
            "Nathan O. Hodas",
            "Charles Siegel",
            "Abhinav Vishnu"
        ],
        "abstract": "Chemical databases store information in text representations, and the SMILES format is a universal standard used in many cheminformatics software. Encoded in each SMILES string is structural information that can be used to predict complex chemical properties. In this work, we develop SMILES2vec, a deep RNN that automatically learns features from SMILES to predict chemical properties, without the need for additional explicit feature engineering. Using Bayesian optimization methods to tune the network architecture, we show that an optimized SMILES2vec model can serve as a general-purpose neural network for predicting distinct chemical properties including toxicity, activity, solubility and solvation energy, while also outperforming contemporary MLP neural networks that uses engineered features. Furthermore, we demonstrate proof-of-concept of interpretability by developing an explanation mask that localizes on the most important characters used in making a prediction. When tested on the solubility dataset, it identified specific parts of a chemical that is consistent with established first-principles knowledge with an accuracy of 88%. Our work demonstrates that neural networks can learn technically accurate chemical concept and provide state-of-the-art accuracy, making interpretable deep neural networks a useful tool of relevance to the chemical industry.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2018-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02259",
        "title": "An innovative solution for breast cancer textual big data analysis",
        "authors": [
            "Nicolas Thiebaut",
            "Antoine Simoulin",
            "Karl Neuberger",
            "Issam Ibnouhsein",
            "Nicolas Bousquet",
            "Nathalie Reix",
            "S\u00e9bastien Moli\u00e8re",
            "Carole Mathelin"
        ],
        "abstract": "The digitalization of stored information in hospitals now allows for the exploitation of medical data in text format, as electronic health records (EHRs), initially gathered for other purposes than epidemiology. Manual search and analysis operations on such data become tedious. In recent years, the use of natural language processing (NLP) tools was highlighted to automatize the extraction of information contained in EHRs, structure it and perform statistical analysis on this structured information. The main difficulties with the existing approaches is the requirement of synonyms or ontology dictionaries, that are mostly available in English only and do not include local or custom notations. In this work, a team composed of oncologists as domain experts and data scientists develop a custom NLP-based system to process and structure textual clinical reports of patients suffering from breast cancer. The tool relies on the combination of standard text mining techniques and an advanced synonym detection method. It allows for a global analysis by retrieval of indicators such as medical history, tumor characteristics, therapeutic responses, recurrences and prognosis. The versatility of the method allows to obtain easily new indicators, thus opening up the way for retrospective studies with a substantial reduction of the amount of manual work. With no need for biomedical annotators or pre-defined ontologies, this language-agnostic method reached an good extraction accuracy for several concepts of interest, according to a comparison with a manually structured file, without requiring any existing corpus with local or new notations.\n    ",
        "submission_date": "2017-12-06T00:00:00",
        "last_modified_date": "2017-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02820",
        "title": "A Deep Network Model for Paraphrase Detection in Short Text Messages",
        "authors": [
            "Basant Agarwal",
            "Heri Ramampiaro",
            "Helge Langseth",
            "Massimiliano Ruocco"
        ],
        "abstract": "This paper is concerned with paraphrase detection. The ability to detect similar sentences written in natural language is crucial for several applications, such as text mining, text summarization, plagiarism detection, authorship authentication and question answering. Given two sentences, the objective is to detect whether they are semantically identical. An important insight from this work is that existing paraphrase systems perform well when applied on clean texts, but they do not necessarily deliver good performance against noisy texts. Challenges with paraphrase detection on user generated short texts, such as Twitter, include language irregularity and noise. To cope with these challenges, we propose a novel deep neural network-based approach that relies on coarse-grained sentence modeling using a convolutional neural network and a long short-term memory model, combined with a specific fine-grained word-level similarity matching model. Our experimental results show that the proposed approach outperforms existing state-of-the-art approaches on user-generated noisy social media data, such as Twitter texts, and achieves highly competitive performance on a cleaner corpus.\n    ",
        "submission_date": "2017-12-07T00:00:00",
        "last_modified_date": "2017-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02838",
        "title": "End-to-End Offline Goal-Oriented Dialog Policy Learning via Policy Gradient",
        "authors": [
            "Li Zhou",
            "Kevin Small",
            "Oleg Rokhlenko",
            "Charles Elkan"
        ],
        "abstract": "Learning a goal-oriented dialog policy is generally performed offline with supervised learning algorithms or online with reinforcement learning (RL). Additionally, as companies accumulate massive quantities of dialog transcripts between customers and trained human agents, encoder-decoder methods have gained popularity as agent utterances can be directly treated as supervision without the need for utterance-level annotations. However, one potential drawback of such approaches is that they myopically generate the next agent utterance without regard for dialog-level considerations. To resolve this concern, this paper describes an offline RL method for learning from unannotated corpora that can optimize a goal-oriented policy at both the utterance and dialog level. We introduce a novel reward function and use both on-policy and off-policy policy gradient to learn a policy offline without requiring online user interaction or an explicit state space definition.\n    ",
        "submission_date": "2017-12-07T00:00:00",
        "last_modified_date": "2017-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.02896",
        "title": "Audio-Visual Sentiment Analysis for Learning Emotional Arcs in Movies",
        "authors": [
            "Eric Chu",
            "Deb Roy"
        ],
        "abstract": "Stories can have tremendous power -- not only useful for entertainment, they can activate our interests and mobilize our actions. The degree to which a story resonates with its audience may be in part reflected in the emotional journey it takes the audience upon. In this paper, we use machine learning methods to construct emotional arcs in movies, calculate families of arcs, and demonstrate the ability for certain arcs to predict audience engagement. The system is applied to Hollywood films and high quality shorts found on the web. We begin by using deep convolutional neural networks for audio and visual sentiment analysis. These models are trained on both new and existing large-scale datasets, after which they can be used to compute separate audio and visual emotional arcs. We then crowdsource annotations for 30-second video clips extracted from highs and lows in the arcs in order to assess the micro-level precision of the system, with precision measured in terms of agreement in polarity between the system's predictions and annotators' ratings. These annotations are also used to combine the audio and visual predictions. Next, we look at macro-level characterizations of movies by investigating whether there exist `universal shapes' of emotional arcs. In particular, we develop a clustering approach to discover distinct classes of emotional arcs. Finally, we show on a sample corpus of short web videos that certain emotional arcs are statistically significant predictors of the number of comments a video receives. These results suggest that the emotional arcs learned by our approach successfully represent macroscopic aspects of a video story that drive audience engagement. Such machine understanding could be used to predict audience reactions to video stories, ultimately improving our ability as storytellers to communicate with each other.\n    ",
        "submission_date": "2017-12-08T00:00:00",
        "last_modified_date": "2017-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03086",
        "title": "FlagIt: A System for Minimally Supervised Human Trafficking Indicator Mining",
        "authors": [
            "Mayank Kejriwal",
            "Jiayuan Ding",
            "Runqi Shao",
            "Anoop Kumar",
            "Pedro Szekely"
        ],
        "abstract": "In this paper, we describe and study the indicator mining problem in the online sex advertising domain. We present an in-development system, FlagIt (Flexible and adaptive generation of Indicators from text), which combines the benefits of both a lightweight expert system and classical semi-supervision (heuristic re-labeling) with recently released state-of-the-art unsupervised text embeddings to tag millions of sentences with indicators that are highly correlated with human trafficking. The FlagIt technology stack is open source. On preliminary evaluations involving five indicators, FlagIt illustrates promising performance compared to several alternatives. The system is being actively developed, refined and integrated into a domain-specific search system used by over 200 law enforcement agencies to combat human trafficking, and is being aggressively extended to mine at least six more indicators with minimal programming effort. FlagIt is a good example of a system that operates in limited label settings, and that requires creative combinations of established machine learning techniques to produce outputs that could be used by real-world non-technical analysts.\n    ",
        "submission_date": "2017-12-05T00:00:00",
        "last_modified_date": "2017-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03249",
        "title": "Social Emotion Mining Techniques for Facebook Posts Reaction Prediction",
        "authors": [
            "Florian Krebs",
            "Bruno Lubascher",
            "Tobias Moers",
            "Pieter Schaap",
            "Gerasimos Spanakis"
        ],
        "abstract": "As of February 2016 Facebook allows users to express their experienced emotions about a post by using five so-called `reactions'. This research paper proposes and evaluates alternative methods for predicting these reactions to user posts on public pages of firms/companies (like supermarket chains). For this purpose, we collected posts (and their reactions) from Facebook pages of large supermarket chains and constructed a dataset which is available for other researches. In order to predict the distribution of reactions of a new post, neural network architectures (convolutional and recurrent neural networks) were tested using pretrained word embeddings. Results of the neural networks were improved by introducing a bootstrapping approach for sentiment and emotion mining on the comments for each post. The final model (a combination of neural network and a baseline emotion miner) is able to predict the reaction distribution on Facebook posts with a mean squared error (or misclassification rate) of 0.135.\n    ",
        "submission_date": "2017-12-08T00:00:00",
        "last_modified_date": "2017-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03512",
        "title": "Comparative analysis of criteria for filtering time series of word usage frequencies",
        "authors": [
            "Inna A. Belashova",
            "Vladimir V. Bochkarev"
        ],
        "abstract": "This paper describes a method of nonlinear wavelet thresholding of time series. The Ramachandran-Ranganathan runs test is used to assess the quality of approximation. To minimize the objective function, it is proposed to use genetic algorithms - one of the stochastic optimization methods. The suggested method is tested both on the model series and on the word frequency series using the Google Books Ngram data. It is shown that method of filtering which uses the runs criterion shows significantly better results compared with the standard wavelet thresholding. The method can be used when quality of filtering is of primary importance but not the speed of calculations.\n    ",
        "submission_date": "2017-12-10T00:00:00",
        "last_modified_date": "2017-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.03897",
        "title": "Learning Modality-Invariant Representations for Speech and Images",
        "authors": [
            "Kenneth Leidal",
            "David Harwath",
            "James Glass"
        ],
        "abstract": "In this paper, we explore the unsupervised learning of a semantic embedding space for co-occurring sensory inputs. Specifically, we focus on the task of learning a semantic vector space for both spoken and handwritten digits using the TIDIGITs and MNIST datasets. Current techniques encode image and audio/textual inputs directly to semantic embeddings. In contrast, our technique maps an input to the mean and log variance vectors of a diagonal Gaussian from which sample semantic embeddings are drawn. In addition to encouraging semantic similarity between co-occurring inputs,our loss function includes a regularization term borrowed from variational autoencoders (VAEs) which drives the posterior distributions over embeddings to be unit Gaussian. We can use this regularization term to filter out modality information while preserving semantic information. We speculate this technique may be more broadly applicable to other areas of cross-modality/domain information retrieval and transfer learning.\n    ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2017-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04046",
        "title": "Character-Based Handwritten Text Transcription with Attention Networks",
        "authors": [
            "Jason Poulos",
            "Rafael Valle"
        ],
        "abstract": "The paper approaches the task of handwritten text recognition (HTR) with attentional encoder-decoder networks trained on sequences of characters, rather than words. We experiment on lines of text from popular handwriting datasets and compare different activation functions for the attention mechanism used for aligning image pixels and target characters. We find that softmax attention focuses heavily on individual characters, while sigmoid attention focuses on multiple characters at each step of the decoding. When the sequence alignment is one-to-one, softmax attention is able to learn a more precise alignment at each step of the decoding, whereas the alignment generated by sigmoid attention is much less precise. When a linear function is used to obtain attention weights, the model predicts a character by looking at the entire sequence of characters and performs poorly because it lacks a precise alignment between the source and target. Future research may explore HTR in natural scene images, since the model is capable of transcribing handwritten text without the need for producing segmentations or bounding boxes of text in images.\n    ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2021-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04048",
        "title": "Cavs: A Vertex-centric Programming Interface for Dynamic Neural Networks",
        "authors": [
            "Hao Zhang",
            "Shizhen Xu",
            "Graham Neubig",
            "Wei Dai",
            "Qirong Ho",
            "Guangwen Yang",
            "Eric P. Xing"
        ],
        "abstract": "Recent deep learning (DL) models have moved beyond static network architectures to dynamic ones, handling data where the network structure changes every example, such as sequences of variable lengths, trees, and graphs. Existing dataflow-based programming models for DL---both static and dynamic declaration---either cannot readily express these dynamic models, or are inefficient due to repeated dataflow graph construction and processing, and difficulties in batched execution. We present Cavs, a vertex-centric programming interface and optimized system implementation for dynamic DL models. Cavs represents dynamic network structure as a static vertex function $\\mathcal{F}$ and a dynamic instance-specific graph $\\mathcal{G}$, and performs backpropagation by scheduling the execution of $\\mathcal{F}$ following the dependencies in $\\mathcal{G}$. Cavs bypasses expensive graph construction and preprocessing overhead, allows for the use of static graph optimization techniques on pre-defined operations in $\\mathcal{F}$, and naturally exposes batched execution opportunities over different graphs. Experiments comparing Cavs to two state-of-the-art frameworks for dynamic NNs (TensorFlow Fold and DyNet) demonstrate the efficacy of this approach: Cavs achieves a near one order of magnitude speedup on training of various dynamic NN architectures, and ablations demonstrate the contribution of our proposed batching and memory management strategies.\n    ",
        "submission_date": "2017-12-11T00:00:00",
        "last_modified_date": "2017-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04314",
        "title": "Generating and Estimating Nonverbal Alphabets for Situated and Multimodal Communications",
        "authors": [
            "Serhii Hamotskyi",
            "Sergii Stirenko",
            "Yuri Gordienko",
            "Anis Rojbi"
        ],
        "abstract": "In this paper, we discuss the formalized approach for generating and estimating symbols (and alphabets), which can be communicated by the wide range of non-verbal means based on specific user requirements (medium, priorities, type of information that needs to be conveyed). The short characterization of basic terms and parameters of such symbols (and alphabets) with approaches to generate them are given. Then the framework, experimental setup, and some machine learning methods to estimate usefulness and effectiveness of the nonverbal alphabets and systems are presented. The previous results demonstrate that usage of multimodal data sources (like wearable accelerometer, heart monitor, muscle movements sensors, braincomputer interface) along with machine learning approaches can provide the deeper understanding of the usefulness and effectiveness of such alphabets and systems for nonverbal and situated communication. The symbols (and alphabets) generated and estimated by such methods may be useful in various applications: from synthetic languages and constructed scripts to multimodal nonverbal and situated interaction between people and artificial intelligence systems through Human-Computer Interfaces, such as mouse gestures, touchpads, body gestures, eyetracking cameras, wearables, and brain-computing interfaces, especially in applications for elderly care and people with disabilities.\n    ",
        "submission_date": "2017-12-12T00:00:00",
        "last_modified_date": "2017-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04753",
        "title": "Learning Spontaneity to Improve Emotion Recognition In Speech",
        "authors": [
            "Karttikeya Mangalam",
            "Tanaya Guha"
        ],
        "abstract": "We investigate the effect and usefulness of spontaneity (i.e. whether a given speech is spontaneous or not) in speech in the context of emotion recognition. We hypothesize that emotional content in speech is interrelated with its spontaneity, and use spontaneity classification as an auxiliary task to the problem of emotion recognition. We propose two supervised learning settings that utilize spontaneity to improve speech emotion recognition: a hierarchical model that performs spontaneity detection before performing emotion recognition, and a multitask learning model that jointly learns to recognize both spontaneity and emotion. Through various experiments on the well known IEMOCAP database, we show that by using spontaneity detection as an additional task, significant improvement can be achieved over emotion recognition systems that are unaware of spontaneity. We achieve state-of-the-art emotion recognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming several relevant and competitive baselines.\n    ",
        "submission_date": "2017-12-12T00:00:00",
        "last_modified_date": "2018-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.04798",
        "title": "A Multimodal Corpus of Expert Gaze and Behavior during Phonetic Segmentation Tasks",
        "authors": [
            "Arif Khan",
            "Ingmar Steiner",
            "Yusuke Sugano",
            "Andreas Bulling",
            "Ross Macdonald"
        ],
        "abstract": "Phonetic segmentation is the process of splitting speech into distinct phonetic units. Human experts routinely perform this task manually by analyzing auditory and visual cues using analysis software, which is an extremely time-consuming process. Methods exist for automatic segmentation, but these are not always accurate enough. In order to improve automatic segmentation, we need to model it as close to the manual segmentation as possible. This corpus is an effort to capture the human segmentation behavior by recording experts performing a segmentation task. We believe that this data will enable us to highlight the important aspects of manual segmentation, which can be used in automatic segmentation to improve its accuracy.\n    ",
        "submission_date": "2017-12-13T00:00:00",
        "last_modified_date": "2018-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05558",
        "title": "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication",
        "authors": [
            "Jin-Hwa Kim",
            "Nikita Kitaev",
            "Xinlei Chen",
            "Marcus Rohrbach",
            "Byoung-Tak Zhang",
            "Yuandong Tian",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel \"crosstalk\" evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.\n    ",
        "submission_date": "2017-12-15T00:00:00",
        "last_modified_date": "2019-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.05997",
        "title": "Taming Wild High Dimensional Text Data with a Fuzzy Lash",
        "authors": [
            "Amir Karami"
        ],
        "abstract": "The bag of words (BOW) represents a corpus in a matrix whose elements are the frequency of words. However, each row in the matrix is a very high-dimensional sparse vector. Dimension reduction (DR) is a popular method to address sparsity and high-dimensionality issues. Among different strategies to develop DR method, Unsupervised Feature Transformation (UFT) is a popular strategy to map all words on a new basis to represent BOW. The recent increase of text data and its challenges imply that DR area still needs new perspectives. Although a wide range of methods based on the UFT strategy has been developed, the fuzzy approach has not been considered for DR based on this strategy. This research investigates the application of fuzzy clustering as a DR method based on the UFT strategy to collapse BOW matrix to provide a lower-dimensional representation of documents instead of the words in a corpus. The quantitative evaluation shows that fuzzy clustering produces superior performance and features to Principal Components Analysis (PCA) and Singular Value Decomposition (SVD), two popular DR methods based on the UFT strategy.\n    ",
        "submission_date": "2017-12-16T00:00:00",
        "last_modified_date": "2017-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06204",
        "title": "Probabilistic Semantic Retrieval for Surveillance Videos with Activity Graphs",
        "authors": [
            "Yuting Chen",
            "Joseph Wang",
            "Yannan Bai",
            "Gregory Casta\u00f1\u00f3n",
            "Venkatesh Saligrama"
        ],
        "abstract": "We present a novel framework for finding complex activities matching user-described queries in cluttered surveillance videos. The wide diversity of queries coupled with unavailability of annotated activity data limits our ability to train activity models. To bridge the semantic gap we propose to let users describe an activity as a semantic graph with object attributes and inter-object relationships associated with nodes and edges, respectively. We learn node/edge-level visual predictors during training and, at test-time, propose to retrieve activity by identifying likely locations that match the semantic graph. We formulate a novel CRF based probabilistic activity localization objective that accounts for mis-detections, mis-classifications and track-losses, and outputs a likelihood score for a candidate grounded location of the query in the video. We seek groundings that maximize overall precision and recall. To handle the combinatorial search over all high-probability groundings, we propose a highest precision subgraph matching algorithm. Our method outperforms existing retrieval methods on benchmarked datasets.\n    ",
        "submission_date": "2017-12-17T00:00:00",
        "last_modified_date": "2018-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06414",
        "title": "The Wisdom of Polarized Crowds",
        "authors": [
            "Feng Shi",
            "Misha Teplitskiy",
            "Eamon Duede",
            "James Evans"
        ],
        "abstract": "As political polarization in the United States continues to rise, the question of whether polarized individuals can fruitfully cooperate becomes pressing. Although diversity of individual perspectives typically leads to superior team performance on complex tasks, strong political perspectives have been associated with conflict, misinformation and a reluctance to engage with people and perspectives beyond one's echo chamber. It is unclear whether self-selected teams of politically diverse individuals will create higher or lower quality outcomes. In this paper, we explore the effect of team political composition on performance through analysis of millions of edits to Wikipedia's Political, Social Issues, and Science articles. We measure editors' political alignments by their contributions to conservative versus liberal articles. A survey of editors validates that those who primarily edit liberal articles identify more strongly with the Democratic party and those who edit conservative ones with the Republican party. Our analysis then reveals that polarized teams---those consisting of a balanced set of politically diverse editors---create articles of higher quality than politically homogeneous teams. The effect appears most strongly in Wikipedia's Political articles, but is also observed in Social Issues and even Science articles. Analysis of article \"talk pages\" reveals that politically polarized teams engage in longer, more constructive, competitive, and substantively focused but linguistically diverse debates than political moderates. More intense use of Wikipedia policies by politically diverse teams suggests institutional design principles to help unleash the power of politically polarized teams.\n    ",
        "submission_date": "2017-11-29T00:00:00",
        "last_modified_date": "2017-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06682",
        "title": "Synthesizing Novel Pairs of Image and Text",
        "authors": [
            "Jason Xie",
            "Tingwen Bao"
        ],
        "abstract": "Generating novel pairs of image and text is a problem that combines computer vision and natural language processing. In this paper, we present strategies for generating novel image and caption pairs based on existing captioning datasets. The model takes advantage of recent advances in generative adversarial networks and sequence-to-sequence modeling. We make generalizations to generate paired samples from multiple domains. Furthermore, we study cycles -- generating from image to text then back to image and vise versa, as well as its connection with autoencoders.\n    ",
        "submission_date": "2017-12-18T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.06704",
        "title": "Multilingual Topic Models",
        "authors": [
            "Kriste Krstovski",
            "Michael J. Kurtz",
            "David A. Smith",
            "Alberto Accomazzi"
        ],
        "abstract": "Scientific publications have evolved several features for mitigating vocabulary mismatch when indexing, retrieving, and computing similarity between articles. These mitigation strategies range from simply focusing on high-value article sections, such as titles and abstracts, to assigning keywords, often from controlled vocabularies, either manually or through automatic annotation. Various document representation schemes possess different cost-benefit tradeoffs. In this paper, we propose to model different representations of the same article as translations of each other, all generated from a common latent representation in a multilingual topic model. We start with a methodological overview on latent variable models for parallel document representations that could be used across many information science tasks. We then show how solving the inference problem of mapping diverse representations into a shared topic space allows us to evaluate representations based on how topically similar they are to the original article. In addition, our proposed approach provides means to discover where different concept vocabularies require improvement.\n    ",
        "submission_date": "2017-12-18T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.07199",
        "title": "Cognitive Database: A Step towards Endowing Relational Databases with Artificial Intelligence Capabilities",
        "authors": [
            "Rajesh Bordawekar",
            "Bortik Bandyopadhyay",
            "Oded Shmueli"
        ],
        "abstract": "We propose Cognitive Databases, an approach for transparently enabling Artificial Intelligence (AI) capabilities in relational databases. A novel aspect of our design is to first view the structured data source as meaningful unstructured text, and then use the text to build an unsupervised neural network model using a Natural Language Processing (NLP) technique called word embedding. This model captures the hidden inter-/intra-column relationships between database tokens of different types. For each database token, the model includes a vector that encodes contextual semantic relationships. We seamlessly integrate the word embedding model into existing SQL query infrastructure and use it to enable a new class of SQL-based analytics queries called cognitive intelligence (CI) queries. CI queries use the model vectors to enable complex queries such as semantic matching, inductive reasoning queries such as analogies, predictive queries using entities not present in a database, and, more generally, using knowledge from external sources. We demonstrate unique capabilities of Cognitive Databases using an Apache Spark based prototype to execute inductive reasoning CI queries over a multi-modal database containing text and images. We believe our first-of-a-kind system exemplifies using AI functionality to endow relational databases with capabilities that were previously very hard to realize in practice.\n    ",
        "submission_date": "2017-12-19T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08697",
        "title": "Interpretable Counting for Visual Question Answering",
        "authors": [
            "Alexander Trott",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "Questions that require counting a variety of objects in images remain a major challenge in visual question answering (VQA). The most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image. In contrast, we treat counting as a sequential decision process and force our model to make discrete choices of what to count. Specifically, the model sequentially selects from detected objects and learns interactions between objects that influence subsequent selections. A distinction of our approach is its intuitive and interpretable output, as discrete counts are automatically grounded in the image. Furthermore, our method outperforms the state of the art architecture for VQA on multiple metrics that evaluate counting.\n    ",
        "submission_date": "2017-12-23T00:00:00",
        "last_modified_date": "2018-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08862",
        "title": "Neural Network Multitask Learning for Traffic Flow Forecasting",
        "authors": [
            "Feng Jin",
            "Shiliang Sun"
        ],
        "abstract": "Traditional neural network approaches for traffic flow forecasting are usually single task learning (STL) models, which do not take advantage of the information provided by related tasks. In contrast to STL, multitask learning (MTL) has the potential to improve generalization by transferring information in training signals of extra tasks. In this paper, MTL based neural networks are used for traffic flow forecasting. For neural network MTL, a backpropagation (BP) network is constructed by incorporating traffic flows at several contiguous time instants into an output layer. Nodes in the output layer can be seen as outputs of different but closely related STL tasks. Comprehensive experiments on urban vehicular traffic flow data and comparisons with STL show that MTL in BP neural networks is a promising and effective approach for traffic flow forecasting.\n    ",
        "submission_date": "2017-12-24T00:00:00",
        "last_modified_date": "2017-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.08941",
        "title": "Comparative Opinion Mining: A Review",
        "authors": [
            "Kasturi Dewi Varathan",
            "Anastasia Giachanou",
            "Fabio Crestani"
        ],
        "abstract": "Opinion mining refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in textual material. Opinion mining, also known as sentiment analysis, has received a lot of attention in recent times, as it provides a number of tools to analyse the public opinion on a number of different topics. Comparative opinion mining is a subfield of opinion mining that deals with identifying and extracting information that is expressed in a comparative form (e.g.~\"paper X is better than the Y\"). Comparative opinion mining plays a very important role when ones tries to evaluate something, as it provides a reference point for the comparison. This paper provides a review of the area of comparative opinion mining. It is the first review that cover specifically this topic as all previous reviews dealt mostly with general opinion mining. This survey covers comparative opinion mining from two different angles. One from perspective of techniques and the other from perspective of comparative opinion elements. It also incorporates preprocessing tools as well as dataset that were used by the past researchers that can be useful to the future researchers in the field of comparative opinion mining.\n    ",
        "submission_date": "2017-12-24T00:00:00",
        "last_modified_date": "2017-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09359",
        "title": "Basic concepts and tools for the Toki Pona minimal and constructed language: description of the language and main issues; analysis of the vocabulary; text synthesis and syntax highlighting; Wordnet synsets",
        "authors": [
            "Renato Fabbri"
        ],
        "abstract": "A minimal constructed language (conlang) is useful for experiments and comfortable for making tools. The Toki Pona (TP) conlang is minimal both in the vocabulary (with only 14 letters and 124 lemmas) and in the (about) 10 syntax rules. The language is useful for being a used and somewhat established minimal conlang with at least hundreds of fluent speakers. This article exposes current concepts and resources for TP, and makes available Python (and Vim) scripted routines for the analysis of the language, synthesis of texts, syntax highlighting schemes, and the achievement of a preliminary TP Wordnet. Focus is on the analysis of the basic vocabulary, as corpus analyses were found. The synthesis is based on sentence templates, relates to context by keeping track of used words, and renders larger texts by using a fixed number of phonemes (e.g. for poems) and number of sentences, words and letters (e.g. for paragraphs). Syntax highlighting reflects morphosyntactic classes given in the official dictionary and different solutions are described and implemented in the well-established Vim text editor. The tentative TP Wordnet is made available in three patterns of relations between synsets and word lemmas. In summary, this text holds potentially novel conceptualizations about, and tools and results in analyzing, synthesizing and syntax highlighting the TP language.\n    ",
        "submission_date": "2017-12-26T00:00:00",
        "last_modified_date": "2018-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09687",
        "title": "Combining Representation Learning with Logic for Language Processing",
        "authors": [
            "Tim Rockt\u00e4schel"
        ],
        "abstract": "The current state-of-the-art in many natural language processing and automated knowledge base completion tasks is held by representation learning methods which learn distributed vector representations of symbols via gradient-based optimization. They require little or no hand-crafted features, thus avoiding the need for most preprocessing steps and task-specific assumptions. However, in many cases representation learning requires a large amount of annotated training data to generalize well to unseen data. Such labeled training data is provided by human annotators who often use formal logic as the language for specifying annotations. This thesis investigates different combinations of representation learning methods with logic for reducing the need for annotated training data, and for improving generalization.\n    ",
        "submission_date": "2017-12-27T00:00:00",
        "last_modified_date": "2017-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09783",
        "title": "Topic Compositional Neural Language Model",
        "authors": [
            "Wenlin Wang",
            "Zhe Gan",
            "Wenqi Wang",
            "Dinghan Shen",
            "Jiaji Huang",
            "Wei Ping",
            "Sanjeev Satheesh",
            "Lawrence Carin"
        ],
        "abstract": "We propose a Topic Compositional Neural Language Model (TCNLM), a novel method designed to simultaneously capture both the global semantic meaning and the local word ordering structure in a document. The TCNLM learns the global semantic coherence of a document via a neural topic model, and the probability of each learned latent topic is further used to build a Mixture-of-Experts (MoE) language model, where each expert (corresponding to one topic) is a recurrent neural network (RNN) that accounts for learning the local structure of a word sequence. In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices. The degree to which each member of the ensemble is used is tied to the document-dependent probability of the corresponding topics. Experimental results on several corpora show that the proposed approach outperforms both a pure RNN-based model and other topic-guided language models. Further, our model yields sensible topics, and also has the capacity to generate meaningful sentences conditioned on given topics.\n    ",
        "submission_date": "2017-12-28T00:00:00",
        "last_modified_date": "2018-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.09929",
        "title": "On the Challenges of Detecting Rude Conversational Behaviour",
        "authors": [
            "Karan Grewal",
            "Khai N. Truong"
        ],
        "abstract": "In this study, we aim to identify moments of rudeness between two individuals. In particular, we segment all occurrences of rudeness in conversations into three broad, distinct categories and try to identify each. We show how machine learning algorithms can be used to identify rudeness based on acoustic and semantic signals extracted from conversations. Furthermore, we make note of our shortcomings in this task and highlight what makes this problem inherently difficult. Finally, we provide next steps which are needed to ensure further success in identifying rudeness in conversations.\n    ",
        "submission_date": "2017-12-28T00:00:00",
        "last_modified_date": "2017-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1712.10309",
        "title": "Methods for Detecting Paraphrase Plagiarism",
        "authors": [
            "Victor Thompson"
        ],
        "abstract": "Paraphrase plagiarism is one of the difficult challenges facing plagiarism detection systems. Paraphrasing occur when texts are lexically or syntactically altered to look different, but retain their original meaning. Most plagiarism detection systems (many of which are commercial based) are designed to detect word co-occurrences and light modifications, but are unable to detect severe semantic and structural alterations such as what is seen in many academic documents. Hence many paraphrase plagiarism cases go undetected. In this paper, we approached the problem of paraphrase plagiarism by proposing methods for detecting the most common techniques (phenomena) used in paraphrasing texts (namely; lexical substitution, insertion/deletion and word and phrase reordering), and combined the methods into a paraphrase detection model. We evaluated our proposed methods and model on collections containing paraphrase texts. Experimental results show significant improvement in performance when the methods were combined (the proposed model) as opposed to running them individually. The results also show that the proposed paraphrase detection model outperformed a standard baseline (based on greedy string tilling), and previous studies.\n    ",
        "submission_date": "2017-12-29T00:00:00",
        "last_modified_date": "2017-12-29T00:00:00"
    }
]